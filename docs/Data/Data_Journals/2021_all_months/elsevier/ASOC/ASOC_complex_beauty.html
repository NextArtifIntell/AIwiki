<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ASOC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="asoc---1054">ASOC - 1054</h2>
<ul>
<li><details>
<summary>
(2021). Multi-criteria weighted aggregated sum product assessment
method for sustainable biomass crop selection problem using
single-valued neutrosophic sets. <em>ASOC</em>, <em>113</em>, 108038.
(<a href="https://doi.org/10.1016/j.asoc.2021.108038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the most optimum biomass crop alternative for biofuel production is a strategic decision due to inherent uncertainty, subjective human mind and involvement of numerous criteria. As the single-valued neutrosophic set (SVNS) is a valuable tool to tackle the uncertain, indeterminate and inconsistent information arises in real-life decision-making problems. Thus, the aim of the present work is to develop a novel weighted aggregated sum product assessment (WASPAS) framework for solving multi-criteria group decision-making (MCGDM) problems with single-valued neutrosophic information. In this framework, single-valued neutrosophic subjective and objective weight integrated approach (SVN-SOWIA) is discussed to estimate the weights of criteria. This method is based on the similarity measure, aggregation operator and score value, which provides more realistic and exact weights. In this regard, new similarity measures are discussed for SVNSs with their desirable characteristics. Further, to illustrate the potentiality and feasibility of the developed approach, a case study of sustainable biomass crop alternative selection is presented under SVNSs context. The sensitivity analysis shows that the alternative ‘Miscanthus’ constantly obtains its top ranking in spite of how criteria weights vary. Lastly, a comparison is made to verify the robustness of the obtained outcome. The result of this work shows that the developed method can suggest more feasible performance while facing multiple influencing factors and input uncertainties, and thus, lends itself to a broader range of applications.},
  archive      = {J_ASOC},
  author       = {Arunodaya Raj Mishra and Pratibha Rani and Ravi Sundar Prajapati},
  doi          = {10.1016/j.asoc.2021.108038},
  journal      = {Applied Soft Computing},
  pages        = {108038},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria weighted aggregated sum product assessment method for sustainable biomass crop selection problem using single-valued neutrosophic sets},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-performance and ultra-compact spike-based architecture
for real-time acoustic echo cancellation. <em>ASOC</em>, <em>113</em>,
108037. (<a href="https://doi.org/10.1016/j.asoc.2021.108037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the implementation of advanced adaptive filters in embedded devices for acoustic echo cancellation has increased because most of them are used in portable devices, especially in Internet of Things systems, in which high-performance and low-area digital hardware implementations are required. In this work, we present a high-performance and ultra-compact spike-based hardware architecture to efficiently compute adaptive filters to be used as an acoustic echo canceller (AEC). To achieve this architecture, we address two factors. (1) We propose a new data-selective adaptive filter along with subband decomposition least mean square (LMS) method to reduce the computational cost by minimizing the number of operations required to efficiently update the filter coefficients . The proposed method requires approximately 40\% fewer updates when compared with conventional subband adaptive filters. As a consequence, the spike-based hardware architecture simulates the proposed adaptive filter at high processing speeds. (2) We propose a compact and high-precision neural multiplier since acoustic echo cancellers require a large number of high-precision multiplications to efficiently identify the echo path. The proposed neural multiplier expends up to 15 times fewer synapses when compared with existing approaches, which represents a significant improvement in terms of area. In addition, this improvement avoids routing problems by implementing large-scale synapse connectivity in advanced FPGAs . Herein, we carry out exhaustive testing by simulating several acoustic echo scenarios. The results demonstrate that the features of the model and the hardware implementation techniques potentially allow easy integration into portable devices for use in real-world acoustic echo cancellation scenarios.},
  archive      = {J_ASOC},
  author       = {Juan-Gerardo Avalos and Giovanny Sanchez and Carlos Trejo and Luis Garcia and Eduardo Pichardo and Angel Vazquez and Esteban Anides and Juan-Carlos Sanchez and Hector Perez},
  doi          = {10.1016/j.asoc.2021.108037},
  journal      = {Applied Soft Computing},
  pages        = {108037},
  shortjournal = {Appl. Soft. Comput.},
  title        = {High-performance and ultra-compact spike-based architecture for real-time acoustic echo cancellation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In-air hand gesture signature using transfer learning and
its forgery attack. <em>ASOC</em>, <em>113</em>, 108033. (<a
href="https://doi.org/10.1016/j.asoc.2021.108033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-air hand gesture signature (HGS) has become a new and emerging technique for dynamic signature recognition due to its advantageous touchless acquisition procedure. Unlike the conventional dynamic signature, HGS creates general images that carrying the spatial and temporal information of the signing action. Deep learning algorithms are prominent to learn these image features . However, they require tremendous amount of data to reach an optimal model which make the collection process computationally expensive. Transfer learning becomes an alternative solution for small sample size problems. This paper aims to investigate the feasibility of transfer learning in classifying a hand gesture-based signature. In our system, the hand region is detected and segmented from each depth image. Then, the salient spatial and temporal features are formed from various images. The knowledge of a pre-trained model is transferred and reused to classify the new seen image features. In this paper, we further investigate the robustness of the proposed approach against two common forgery attacks , (1) random forgeries and (2) skilled forgeries. Empirical results demonstrate that the proposed approach can achieves 99.03\% of precision and 98.89\% of recall in classifying HGS. On top of this, the proposed approach also manifests its robustness in handling different kinds of forgery attacks, i.e. achieving low error rates of 0.78\% in random forgery attack and 4.88\% in skilled forgery attack.},
  archive      = {J_ASOC},
  author       = {Wee How Khoh and Ying Han Pang and Andrew Beng Jin Teoh and Shih Yin Ooi},
  doi          = {10.1016/j.asoc.2021.108033},
  journal      = {Applied Soft Computing},
  pages        = {108033},
  shortjournal = {Appl. Soft. Comput.},
  title        = {In-air hand gesture signature using transfer learning and its forgery attack},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting crude oil prices based on variational mode
decomposition and random sparse bayesian learning. <em>ASOC</em>,
<em>113</em>, 108032. (<a
href="https://doi.org/10.1016/j.asoc.2021.108032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately forecasting crude oil prices has drawn much attention from researchers, investors, producers, and consumers. However, the complexity of crude oil prices makes it a very challenging task. To this end, this paper presents a novel scheme by integrating variational mode decomposition (VMD) and random sparse Bayesian learning (RSBL, SBL-based prediction with random lags and random samples), namely VMD-RSBL, for the forecasting task. The proposed VMD-RSBL contains three stages. First, crude oil price series is decomposed into a couple of components by VMD. The decomposed components exhibit simpler characteristics than the raw prices and hence are easy to forecast. Second, RSBL is employed to predict each component individually. Specifically, for each component, the proposed scheme builds a group of predictors with SBL on different subsets of samples (random samples) and random lags, and then the average of all the predictors is taken as the forecasting result of the individual component. At last, the forecasting results of all the components are added as the final forecasting prices. We perform extensive experiments, and the results demonstrate that the proposed VMD-RSBL significantly outperforms many state-of-the-art schemes in terms of several evaluation indicators.},
  archive      = {J_ASOC},
  author       = {Taiyong Li and Zijie Qian and Wu Deng and Duzhong Zhang and Huihui Lu and Shuheng Wang},
  doi          = {10.1016/j.asoc.2021.108032},
  journal      = {Applied Soft Computing},
  pages        = {108032},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting crude oil prices based on variational mode decomposition and random sparse bayesian learning},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topology and size optimization for a flexure hinge using an
integration of SIMP, deep artificial neural network, and water cycle
algorithm. <em>ASOC</em>, <em>113</em>, 108031. (<a
href="https://doi.org/10.1016/j.asoc.2021.108031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a multistage optimization method for designing a new flexure hinge (FH). The proposed method is a combination of the topology optimization , the deep artificial neural network (DANN)-based modeling, and the water cycle algorithm-based size optimization. Firstly, solid isotropic material with penalization is employed to topologize the FH. Then, the topological FH is modified to transform into a compliant configuration. Finite element method is used to collect the output datasets of the hinge. Subsequently, the architectures of DANN are optimized to formulate the objective functions and constraints of the hinge. The results showed that the prediction accuracy of the developed DANN is better than that of the multivariate general linear model. Lastly, the geometrical sizes of the hinge are optimized by hybridizing the optimal DANN and the water cycle algorithm. The results found that the optimal solutions found from the proposed method are greater than those obtained from the other metaheuristic algorithms . Based on the results of Wilcoxon, Friedman, and Post-hoc tests, the proposed method outperforms the other methods. Besides, the results indicated that the performances of the FH are superior to the conventional hinges. The proposed optimization framework can be considered as a systematic design method for compliant mechanisms and related engineering areas.},
  archive      = {J_ASOC},
  author       = {Ngoc Le Chau and Ngoc Thoai Tran and Thanh-Phong Dao},
  doi          = {10.1016/j.asoc.2021.108031},
  journal      = {Applied Soft Computing},
  pages        = {108031},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Topology and size optimization for a flexure hinge using an integration of SIMP, deep artificial neural network, and water cycle algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic expression recognition of face image sequence
based on key-frame generation and differential emotion feature.
<em>ASOC</em>, <em>113</em>, 108029. (<a
href="https://doi.org/10.1016/j.asoc.2021.108029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using two key-frames with the neutral expression and maximum expression intensity respectively in face image sequence to alleviate inter-subject variations for Facial Expression Recognition (FER) has become the research focus of computer vision . To determine the two key-frames, an automatic Generation Difference Convolutional Neural Network (GDCNN) framework was presented to reduce the influences of high inter-subject variations caused by individual differences. Firstly, for any given input expression, a trained conditional Generative Adversarial Network (cGAN) is utilized to generate the frame with neutral expression while keeping the identity-related information. Secondly, the fine-tuned triplet distance model is used to detect the frame with the maximum expression intensity. Finally, an optimized two-stream CNN model is designed to reduce the influences of inter-subject variations and trained to extract the differential emotion features for FER. Extensive comparisons were performed on the CK+, MMI, and Beihang University databases and the results demonstrated the superiority of the proposed GDCNN framework over the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Yi Sima and Jizheng Yi and Aibin Chen and Ze Jin},
  doi          = {10.1016/j.asoc.2021.108029},
  journal      = {Applied Soft Computing},
  pages        = {108029},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic expression recognition of face image sequence based on key-frame generation and differential emotion feature},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical hashing-based multi-source image retrieval
method for image denoising. <em>ASOC</em>, <em>113</em>, 108028. (<a
href="https://doi.org/10.1016/j.asoc.2021.108028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a fundamental problem in image processing . One prominent image denoising approach is to exploit the non-local self-similarity (NSS) prior of natural images. How to find and utilize similar image patches is the key to the NSS-based method. However, most NSS-based methods only use noisy patches within the noisy image itself, whose number and quality are quite limited. To exploit the similar clean patches in external image datasets, in this paper, we propose a hierarchical hashing-based multi-source image retrieval method . First, based on deep hashing, a multi-source image-level retrieval method is proposed to search clean appearance-alike images from the external dataset. Second, on the basis of NSS prior and image coherence hypothesis, an improved continuous sensitive hashing method is designed from the image patch level to create a dense mapping between the external similar patches and the target patches. Finally, using the searched similar clean image patches, a simplified denoising method is developed based on the 3D transformation and thresholding technique. Experiments on two kinds of image datasets have demonstrated the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Yan Huang and Zihan Zhou and Xiangyu Sai and Yong Xu and Yuexian Zou},
  doi          = {10.1016/j.asoc.2021.108028},
  journal      = {Applied Soft Computing},
  pages        = {108028},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical hashing-based multi-source image retrieval method for image denoising},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of modified beetle antennae search algorithm and
BP power flow prediction model on multi-objective optimal active power
dispatch. <em>ASOC</em>, <em>113</em>, 108027. (<a
href="https://doi.org/10.1016/j.asoc.2021.108027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-objective optimal active power dispatch (MOAPD), an important part to realize the high-efficient and economic operation of power systems , widely considers the reduction of power loss, fuel cost and emission. To successfully solve the nonlinear MOAPD problems with contradictory objectives, the modified hybrid beetle antennae search (MHBAS) algorithm with adaptively-adjusted step factor is proposed in this paper. The suggested MHBAS algorithm integrates the preliminary optimization and mutation-crossover mechanisms of multi-objective differential evolution (MDE) algorithm, and has better population diversity. Furthermore, the novel fuzzy-object sorting method (FOSM) is put forward to determine the feasible Pareto fronts (PFs) of MOAPD. Seven testing cases on IEEE 30, 57, and 118-bus systems certify that, the best compromise solution (BCS) achieved by the MHBAS algorithm with FOSM is more desirable than traditional MDE and NSGA-II algorithms. Besides, the great superiority of MHBAS algorithm on PF diversity and PF uniformity are quantitatively validated by the hypervolume (HV) and inverted generational distance (IGD) indexes. Even more importantly, a feasible BP power flow prediction model which takes the basic fuel cost as the fulcrum is put forward for the first time. The innovative MHBAS-BP method, the combination of BP prediction model and MHBAS algorithm, provides great convenience for exploring the higher quality dispatching schemes near the current BCS. These multiple alternative power flow solutions obtained by NMBAS-BP method can meet the various requirements of decision makers with less time cost. In general, the proposed MHBAS-BP provides a representative application of computer technologies in solving complex MOAPD problems.},
  archive      = {J_ASOC},
  author       = {Jie Qian and Ping Wang and Chenggen Pu and Xiaoli Peng and Gonggui Chen},
  doi          = {10.1016/j.asoc.2021.108027},
  journal      = {Applied Soft Computing},
  pages        = {108027},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of modified beetle antennae search algorithm and BP power flow prediction model on multi-objective optimal active power dispatch},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient GBDTRSO control strategy for PV connected
h-bridge nine level MLI system with quasi-z-source inverter.
<em>ASOC</em>, <em>113</em>, 108026. (<a
href="https://doi.org/10.1016/j.asoc.2021.108026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this manuscript, an extended topology based switched coupled inductor quasi-z-source with nine-level inverter is proposed for three-phase grid-tie photovoltaic power system using hybrid technique. The proposed method is the combined execution of Gradient Boosting Decision Tree (GBDT) and Rat Swarm Optimizer (RSO), hence it is called GBDTRSO method. The major objective of the GBDTRSO method is “compute the performance of photovoltaic system by the maximal power extraction”. Here, the designing of the proposed inverter is optimized to supply the maximal power from photovoltaic power generating system . Besides, the higher voltage gain diminished current ripple switched coupled inductor quasi-Z-source inverter i.e extended boost switched coupled inductor quasi-Z-source inverter topology is proposed. The vital aspects of proposed topology is higher voltage gain, ripple free continual source current, typical ground amid the direct current source and inverter circuit, therefore it is optimized for photovoltaic utilizations. The objective function is deemed depending upon its controller parameters with limits like, power, voltages, current, modulation index . These parameters have been employed to the inputs of proposed method. Ensure the maximal power supply to the load using Gradient Boosting Decision Tree technique in terms of maximum power point tracking . Furthermore, the Rat Swarm Optimizer diminished the feed power and controls the direct current link current, voltage and frequency levels. The proposed technique is activated in Matrix Laboratory (MATLAB)/Simulink platform, also the efficiency is likened with different existing methods. The statistical analysis of proposed and existing methods using energy sources is also analyzed. Finally, the experimental outcomes demonstrate that the proposed method is more efficient than the other existing methods.},
  archive      = {J_ASOC},
  author       = {P. Meenalochini and E. Sakthivel},
  doi          = {10.1016/j.asoc.2021.108026},
  journal      = {Applied Soft Computing},
  pages        = {108026},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient GBDTRSO control strategy for PV connected H-bridge nine level MLI system with quasi-Z-source inverter},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online streaming feature selection based on neighborhood
rough set. <em>ASOC</em>, <em>113</em>, 108025. (<a
href="https://doi.org/10.1016/j.asoc.2021.108025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is considered as a necessary and significant pre-processing step in many fields, especially in machine learning . However, in some real problems, in which features flow one by one, many existing approaches do not work well on the online streaming features, and most online streaming feature selection (OSFS) methods face the challenge of requiring domain knowledge before setting optimal parameters in advance. Therefore, an effective feature selection method for online streaming features, named OFS-Gapknn, is proposed in this paper. A new neighborhood rough set relation is firstly defined, which combines the advantages of both the k -nearest and the Gap neighborhood. The proposed neighborhood relation can not only work well on the unevenly distributed sample space, but also need not any parameters and domain knowledge. Then, the relevance and redundancy features are analyzed by using the dependency based on the neighborhood rough set. Finally, one of the optimal feature subsets is obtained. To validate the effectiveness of the proposed algorithm, four traditional methods and three OSFS methods are compared with it on 11 datasets. Experimental results indicate the dominance and significance of the proposed method.},
  archive      = {J_ASOC},
  author       = {Shuangjie Li and Kaixiang Zhang and Yali Li and Shuqin Wang and Shaoqiang Zhang},
  doi          = {10.1016/j.asoc.2021.108025},
  journal      = {Applied Soft Computing},
  pages        = {108025},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online streaming feature selection based on neighborhood rough set},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective covering salesman problem with 2-coverage.
<em>ASOC</em>, <em>113</em>, 108024. (<a
href="https://doi.org/10.1016/j.asoc.2021.108024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a wide range of contexts including military operations, environment monitoring, surveillance in border areas, health care, public safety [1, 2], disaster management, humanitarian relief and blood supply chain, a robust solution of the Covering Salesman Problem (CSP) is necessary. These applications require more than one facilities to cover a given customer (region of interest (ROI)). In this paper, we consider the coverage radius to be fixed and same for each node. Then we propose a multi-objective algorithm based on NSGA-II, in which minimization of tour length and maximization of number of nodes with 2-coverage are considered as the objectives. For implementing the algorithm, an individual chromosome is represented using a one-dimensional array with variable length, and developed a new crossover and a new mutation operator based on the problem and variable length chromosome representation . Numerical examples taken from TSP instances (TSPLIB [3]) with number of nodes ranging from 51 to 150 are solved using the algorithm. For each numerical example, the tour corresponding to the solution with 2-coverage of customer nodes is presented, which shows that the proposed algorithm is effective. Finally, a potential future research direction of this problem is discussed.},
  archive      = {J_ASOC},
  author       = {Siba Prasada Tripathy and Amiya Biswas and Tandra Pal},
  doi          = {10.1016/j.asoc.2021.108024},
  journal      = {Applied Soft Computing},
  pages        = {108024},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective covering salesman problem with 2-coverage},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast neighborhood search scheme for identical parallel
machine scheduling problems under general learning curves.
<em>ASOC</em>, <em>113</em>, 108023. (<a
href="https://doi.org/10.1016/j.asoc.2021.108023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a limited number of results concerning fast and efficient solution algorithms for scheduling problems with general learning models. Therefore, we develop a fast insert neighborhood search (FINS) for a group of identical parallel machine scheduling problems with general learning curves under the following minimization objectives: the maximum completion time (makespan) and the maximum lateness. Our fast search scheme allows to calculate criterion values in a constant time per solution in a neighborhood. The application is presented on the basis of Nawaz–Enscore–Ham’s method (NEH), iterative local search, tabu search , particle swarm optimization and hybrid simulated annealing — tabu search algorithms , where the efficiency of their standard implementations are compared to their versions enhanced by our fast insert neighborhood search. The computational experiments confirm the theoretical analysis that our method essentially overwhelms the well known approaches. The metaheuristics augmented by our FINS are about 120 times faster than their standard versions and improve the criterion values by up to 25\% for analyzed instances with 100–800 jobs and 5–80 machines.},
  archive      = {J_ASOC},
  author       = {Radosław Rudek},
  doi          = {10.1016/j.asoc.2021.108023},
  journal      = {Applied Soft Computing},
  pages        = {108023},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fast neighborhood search scheme for identical parallel machine scheduling problems under general learning curves},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classifiers selection based on analytic hierarchy process
and similarity score for spam identification. <em>ASOC</em>,
<em>113</em>, 108022. (<a
href="https://doi.org/10.1016/j.asoc.2021.108022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The overrun of images and corpus spam on the web constitutes a threat to users producing more troubles, diminishing internet bandwidth, and increasing network overload. In this paper, we introduce a multi-criteria decision-making (MCDM) framework for spam classification based on an analytic hierarchy process. The proposed approach permits the selection of the most optimal classifier among six machine learning algorithms according to a set of performance metrics. The ranking consistency of classifiers has been evaluated with a consistency ratio almost inferior to 0.1 on all dataset. The proposed framework can recommend top-K classifiers with high accuracy by using Euclidean distance on basic characteristics of test and train data that was already trained on a set of classifiers and fed into the MCDM framework. The experiment has been performed on eight publicly available datasets and tested on four datasets using only known features from the dataset and using Euclidean distance measure to identify from the most performant classifier algorithm. The followed methodology has accurately identified the most suited classifier on training datasets with average accuracy 95\% for both images and corpus spam out and achieved 100\% accuracy on a personalimageham+spamarchive_jml dataset that performs better than existing methods. We performed Friedman statistical test on the dataset that showed significance, then a post hoc Conover test performed on each data to make a pairwise comparison that demonstrated the effectiveness of the proposed approach based AHP framework. The proposed approach outperforms existing research works in term of spam identification on both image and corpus spam data and was successful in recommending classifiers to unknown datasets.},
  archive      = {J_ASOC},
  author       = {Soufiana Mekouar},
  doi          = {10.1016/j.asoc.2021.108022},
  journal      = {Applied Soft Computing},
  pages        = {108022},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classifiers selection based on analytic hierarchy process and similarity score for spam identification},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A niching differential evolution algorithm for the
large-scale combined heat and power economic dispatch problem.
<em>ASOC</em>, <em>113</em>, 108017. (<a
href="https://doi.org/10.1016/j.asoc.2021.108017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combined heat and power economic dispatch (CHPED) problem, which aims at minimizing system cost to supply system demand under various constraints, is one of the most challenging problems in the power system . Numerous researches are concentrated on solving the CHPED problem with small size, ignoring implementation of the large-scale CHPED problem with lots of local optimal solutions . This paper proposes a niching differential evolution algorithm (NDE), which incorporates niching methods into differential evolution algorithm (DE), to solve the large-scale CHPED problem. In NDE, a modified DE/best/1 strategy inspired by the neighborhood concept of niching methods, called DE/best/1/nh, is developed to generate diverse solutions. A two-phase selection that utilizes the deterministic crowding of niching methods and greedy selection is designed to remain promising solutions. The two strategies help to balance the global and local search capabilities of the algorithm. The performance of NDE is evaluated by a comparative analysis with other existing approaches on the five CHPED systems with 7, 24, 84, 96 and 192 units. The numerical results demonstrate the feasibility and effectiveness of NDE in solving the large-scale CHPED problem.},
  archive      = {J_ASOC},
  author       = {Di Liu and Zhongbo Hu and Qinghua Su and Mianfang Liu},
  doi          = {10.1016/j.asoc.2021.108017},
  journal      = {Applied Soft Computing},
  pages        = {108017},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A niching differential evolution algorithm for the large-scale combined heat and power economic dispatch problem},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differential evolution with rankings-based fitness function
for constrained optimization problems. <em>ASOC</em>, <em>113</em>,
108016. (<a href="https://doi.org/10.1016/j.asoc.2021.108016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When evolutionary algorithms are employed to solve constrained optimization problems (COPs), how to efficiently make use of the information of some promising infeasible solutions is very important in the process of searching for the optimal feasible solution. In this paper, for selecting and making full use of some better infeasible solutions, a rankings-based fitness function method is designed. Specifically, the final fitness function of each individual is obtained by weighting two rankings, which are got after sorting the population based on the ɛ ɛ ɛ constraint technique and only based on the objective function, respectively. Furthermore, the weight is dynamically adjusted by considering the proportion of feasible solutions and generation information. By doing this, the tradeoff in constraints and objective can be addressed. Moreover, the promising offspring are generated by three differential evolution strategies with distinct characters to balance diversity and convergence. In addition, 116 benchmark problems from three test suites are used to evaluate the performance of the proposed method. Nine commonly used practical problems are selected to test the potential of the algorithm to solve real-world problems. Experimental results indicate that the proposed method shows superior or competitive to other state-of-the-art methods tailored for COPs. Moreover, the effectiveness of each introduced component in the proposed algorithm is investigated by the ablation study.},
  archive      = {J_ASOC},
  author       = {Jing Liang and Xuanxuan Ban and Kunjie Yu and Boyang Qu and Kangjia Qiao},
  doi          = {10.1016/j.asoc.2021.108016},
  journal      = {Applied Soft Computing},
  pages        = {108016},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution with rankings-based fitness function for constrained optimization problems},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization of the distance-constrained multi-based
multi-UAV routing problem with simulated annealing and local
search-based matheuristic to detect forest fires: The case of turkey.
<em>ASOC</em>, <em>113</em>, 108015. (<a
href="https://doi.org/10.1016/j.asoc.2021.108015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forests cover nearly a third of the Earth’s land area, and they are a key factor for all life on Earth, but unfortunately, forest fires are the greatest danger to their presence. The wildfires jeopardize general wellbeing, security, and require high levels of government resources. They also lead to noteworthy debasement of nature, property loss, and high rates of human death and injury. This paper proposes an algorithm to use and route unmanned aerial vehicles (UAVs) to mitigate forest fire risks. The developed matheuristic algorithm hybridizes simulated annealing and local search metaheuristics with an integer linear programming model. The mathematical model was developed to solve the distance-constrained multi-based multi-UAV routing problem, and because of the complexity of the problem, the generated metaheuristics helps the model to find better solutions. The effectiveness of the proposed matheuristic is tested with a real-life case study for Turkey and is also compared with a genetic algorithm . The Turkish State Meteorological Service generates forest fire-risk maps countrywide every day to predict fire risks 3 days later by using meteorological data . These maps are used to generate the risky regions to be visited by the UAVs, and the existing airports are considered for the UAVs to take off and land. The algorithm is coded using MATLAB and ILOG. The metaheuristics are designed with problem-based operators, and their parameters are tuned by experiments. Computational results demonstrate the effectiveness of the algorithm and the hybridization procedures. Results demonstrate that the CPU times for the methods are acceptable.},
  archive      = {J_ASOC},
  author       = {Omer Ozkan},
  doi          = {10.1016/j.asoc.2021.108015},
  journal      = {Applied Soft Computing},
  pages        = {108015},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of the distance-constrained multi-based multi-UAV routing problem with simulated annealing and local search-based matheuristic to detect forest fires: The case of turkey},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multicriteria decision making methodology based on
two-dimensional uncertainty by hesitant z-fuzzy linguistic terms with an
application for blockchain risk evaluation. <em>ASOC</em>, <em>113</em>,
108014. (<a href="https://doi.org/10.1016/j.asoc.2021.108014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology (BT), which provides to record transactions among many computers as a distributed ledger, is commonly utilized by companies to develop and manage business processes. Since it is a multi-process and multi-agent complex system, the implementation process of BT contains many risk factors. Identifying and analyzing these risks in order to minimize their adverse effects during the implementation process is also completely critical for companies. Since evaluating these risks must be handled by considering many different perspectives, the evaluation of risk factors for BT can be considered as a multi-criteria decision-making (MCDM) problem. To cope with uncertainty in this process, the fuzzy set theory (FST) can be effectively utilized with MCDM methods to increase their ability to obtain more concrete and realistic results. Therefore, this paper proposes an MCDM methodology consisting of Decision-Making Trail and Evaluating Laboratory (DEMATEL) and cognitive mapping methods based on FST. These methods have been reconstructed with hesitant fuzzy Z-numbers that enable not only to state impreciseness of the assessments but also to consider indeterminacy and hesitancy of the experts to evaluate risk factors with respect to BT implementation. While the dependencies of risk factors have been determined via the hesitant fuzzy Z-DEMATEL method, their weights have been obtained through the hesitant fuzzy Z-cognitive mapping method. As a result, this study aims to prioritize risk factors related to the BT implementation process with a novel fuzzy decision-making methodology and provide a road map for companies. With this study, which is the first paper in the literature in terms of the subject of the study and the method adopted, an integrated, comprehensive fuzzy approach is introduced for the first time in evaluating BT risks. As a result of detailed research, risk factors that companies may encounter in BT use and the degrees of their importance have been determined. Through that, essential decision analysis has been put forward that companies can apply for their strategic decisions. A sensitivity analysis based on changes in the initial vector values has also been adapted to analyze the effects of the risk factors.},
  archive      = {J_ASOC},
  author       = {Ali Karaşan and İhsan Kaya and Melike Erdoğan and Murat Çolak},
  doi          = {10.1016/j.asoc.2021.108014},
  journal      = {Applied Soft Computing},
  pages        = {108014},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multicriteria decision making methodology based on two-dimensional uncertainty by hesitant Z-fuzzy linguistic terms with an application for blockchain risk evaluation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative coevolutionary algorithm with resource
allocation strategies to minimize unnecessary computations.
<em>ASOC</em>, <em>113</em>, 108013. (<a
href="https://doi.org/10.1016/j.asoc.2021.108013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new computational resource allocation (CRA)-based cooperative coevolutionary (CC) algorithm, called ECCRA. To effectively allocate the computational resources into the sub-problems, ECCRA compensatively utilizes various strategies: (i) evaluate a degree of contribution for each sub-problem; (ii) extricate the stagnant sub-problems from any local minimums; (iii) allocate individuals adaptively according to a size of each sub-problem and its contribution; (iv) prune the unpromising sub-problems from the evolution process; and (v) utilize the multi-armed bandit (MAB)-based selection method to choose various sub-problems extensively. In the experiments, ECCRA achieved best optimization results for the CEC’2010 benchmark problems. Moreover, ECCRA showed notable optimization performance for imbalanced problems in the CEC’2013 benchmark suite. Thus, we found that ECCRA could considerably outperform the existing CRA-based CC algorithms in terms of the optimization quality and convergence.},
  archive      = {J_ASOC},
  author       = {Kyung Soo Kim and Yong Suk Choi},
  doi          = {10.1016/j.asoc.2021.108013},
  journal      = {Applied Soft Computing},
  pages        = {108013},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperative coevolutionary algorithm with resource allocation strategies to minimize unnecessary computations},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved harris hawks optimization algorithm based on random
unscented sigma point mutation strategy. <em>ASOC</em>, <em>113</em>,
108012. (<a href="https://doi.org/10.1016/j.asoc.2021.108012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Harris hawks optimization algorithm (HHO) is a new swarm intelligence algorithm for simulating the capture and attack of the hawks which has good global exploration and local exploitation capabilities. In order to further improve the optimization performance of the algorithm, quasi-opposite learning and quasi-reflection learning strategies performed according to probability are involved in the attack phase to enhance the diversity of the population and accelerate the convergence rate of HHO while a logarithmic nonlinear convergence factor is designed to balance the ability of global search and local optimization of the algorithm. Furthermore, in order to avoid the algorithm falling into a local optimum, using the characteristics of the unscented transform (UT) to estimate the mean and variance of a random variable function can achieve second-order accuracy, a new strategy for generating random symmetric sigma points is designed to mutate the current best individual in the visible range, at last, an improved Harris hawk algorithm (IHHO) based on random unscented sigma point mutation is proposed. The new stochastic UT ensures the random exploitation of the algorithm and has a certain theoretical support, which overcomes the theoretical deficiency of the stochastic optimization algorithm to some extent. The numerical optimization ability of IHHO is verified by CEC2017 benchmark functions , CEC2020 benchmark function and fifteen standard test functions. Finally, the practicality and effectiveness of the IHHO algorithm are verified by three engineering constraint optimization and the discounted { { 0-1 } } knapsack problem .},
  archive      = {J_ASOC},
  author       = {Wenyan Guo and Peng Xu and Fang Dai and Fengqun Zhao and Mingfei Wu},
  doi          = {10.1016/j.asoc.2021.108012},
  journal      = {Applied Soft Computing},
  pages        = {108012},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved harris hawks optimization algorithm based on random unscented sigma point mutation strategy},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using fuzzy clustering to address imprecision and
uncertainty present in deterministic components of time series.
<em>ASOC</em>, <em>113</em>, 108011. (<a
href="https://doi.org/10.1016/j.asoc.2021.108011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series analysis models, understands, and predicts phenomena from different domains such as meteorology, medicine, and economics. In this context, Fuzzy Time Series has been standing out due to its capacity of using mathematical functions to represent linguistic variables , resulting in interpretative and more accurate models. Several studies aim at improving time series forecasting using fuzzy set theory , however such efforts were executed solely to obtain modeling improvements in the fuzzification stage, disregarding the stochastic and deterministic influences composing time series to assist the modeling process. In an attempt to fill out such gap, this manuscript employs the Empirical Mode Decomposition (EMD) to extract the deterministic influences which are next modeled using fuzzy clustering to improve time series forecasting. EMD reduces the data imprecision and uncertainty thus helping the fuzzy clustering stage that automatically finds an adequate space partitioning to produce the fuzzy sets. We use Fuzzy C-means to generate fuzzy sets with different characteristics, contributing to improvements in the definition of the number of sets, usually barely explored in the fuzzification stage. Our method was assessed using different validation indices while forecasting time series, which has confirmed promising results and supported the interpretability of data ranges along time.},
  archive      = {J_ASOC},
  author       = {Marcos Vinícius dos Santos Ferreira and Ricardo Rios and Rodrigo Mello and Tatiane Nogueira Rios},
  doi          = {10.1016/j.asoc.2021.108011},
  journal      = {Applied Soft Computing},
  pages        = {108011},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using fuzzy clustering to address imprecision and uncertainty present in deterministic components of time series},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cultural evolution of probabilistic aggregation in synthetic
swarms. <em>ASOC</em>, <em>113</em>, 108010. (<a
href="https://doi.org/10.1016/j.asoc.2021.108010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local interactions and communication are key features in swarm robotics, but they are most often fixed at design time, limiting flexibility and causing a stiff and inefficient response to changing environments. Motivated by the need for higher adaptation abilities, we propose that information about emergent collective structures should percolate onto the individual behavior, modifying it in a way that determines suitable responses in the face of new working conditions and organizational challenges. Indeed, complex societies are driven by an evolving set of individual and social norms subject to cultural propagation, which contribute to determining the individual behaviors. We leverage ideas from the evolution of natural language – an undoubtedly efficient cultural trait – and exploit the resulting social dynamics to select and propagate microscopic behavioral parameters that adapt continuously to macroscopic conditions, which in turn affect the agents’ communication topography, and, therefore, feed back onto the social dynamics. This concept is demonstrated on a self-organized aggregation behavior, which is a building block for most swarm robotics behaviors and a striking example of how collective dynamics are sensitive to experimental parameters. By means of experiments with simulated and real robots, we show that the cultural evolution of aggregation rules outperforms conventional approaches in terms of adaptivity to multiple experimental settings.},
  archive      = {J_ASOC},
  author       = {Nicolas Cambier and Dario Albani and Vincent Frémont and Vito Trianni and Eliseo Ferrante},
  doi          = {10.1016/j.asoc.2021.108010},
  journal      = {Applied Soft Computing},
  pages        = {108010},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cultural evolution of probabilistic aggregation in synthetic swarms},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating a softened multi-interval loss function into
neural networks for wind power prediction. <em>ASOC</em>, <em>113</em>,
108009. (<a href="https://doi.org/10.1016/j.asoc.2021.108009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality wind power interval prediction is an effective means to ensure the economic and stable operation of the electric power system . Comparing with single-interval prediction, multi-interval prediction is conducive to providing more uncertainty information to decision-makers for risk quantification. Existing multi-interval prediction methods require several independent forecasting models to generate prediction intervals (PIs) at different prediction interval nominal confidence (PINC) levels, which would lead to long training time and cross-bound phenomenon. This paper constructs a novel framework to simultaneously generate multiple PIs for wind power by integrating a proposed softened multi-interval loss function into neural networks . Firstly, the effectiveness of the proposed loss function is verified via simulation data, and the suitable training method and softening factor range are found. Then, five widely used neural networks are employed with both single-interval and multi-interval loss functions to carry out multiple interval prediction on two real-world wind power datasets. The results indicate that the proposed loss function can effectively avoid the cross-bound phenomenon and decrease the mean prediction interval width of PIs. In addition, the echo state network (ESN) with the proposed loss function exhibits the best forecasting performance among the investigated models for both point prediction and interval prediction.},
  archive      = {J_ASOC},
  author       = {Jianming Hu and Weigang Zhao and Jingwei Tang and Qingxi Luo},
  doi          = {10.1016/j.asoc.2021.108009},
  journal      = {Applied Soft Computing},
  pages        = {108009},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating a softened multi-interval loss function into neural networks for wind power prediction},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimized support vector regression for prediction of
bearing degradation. <em>ASOC</em>, <em>113</em>, 108008. (<a
href="https://doi.org/10.1016/j.asoc.2021.108008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning and deep learning models are gradually being applied to predict the remaining useful life of bearings. But, still extracting effective features from the bearing signals and enhancing the prediction accuracy is a major concern. Therefore, an optimized support vector regression (SVR) model for prediction of bearing degradation is proposed. Firstly, the original time domain and frequency domain features extracted from the bearing vibration signal are learned by using the deep neural network (DNN) to improve the quality of degradation features. Secondly, a novel multi-population fruit fly optimization algorithm (MPFOA) is proposed by introducing multi-population mechanism. Thirdly, MPFOA is employed to choose the parameters of SVR, then we use MPFOA-SVR to predict the bearing remaining useful life through the degradation features learned by DNN. At last, CEC 2017 unconstrained benchmark functions and a real bearing dataset (IEEE 2012 PHM) are used to verify the performance of MPFOA and optimized SVR models respectively. Numerical experimental results show that MPFOA has a better optimization ability than the compared meta-heuristic algorithms. The optimized SVR model has a higher prediction accuracy in predicting the bearing remaining useful life.},
  archive      = {J_ASOC},
  author       = {Chenglong Zhang and Shifei Ding and Yuting Sun and Zichen Zhang},
  doi          = {10.1016/j.asoc.2021.108008},
  journal      = {Applied Soft Computing},
  pages        = {108008},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized support vector regression for prediction of bearing degradation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose control of constrained redundant arm using recurrent
neural networks and one-iteration computing algorithm. <em>ASOC</em>,
<em>113</em>, 108007. (<a
href="https://doi.org/10.1016/j.asoc.2021.108007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accuracy and safety are two important factors in redundant arm control. Generally, physical constraints should be taken into account for safe operation of redundant arm. By investigating the kinematics of redundant arm, the formulation of end-effector’s position and orientation is presented in detail. By utilizing zeroing neural network method, a pose control scheme considering physical constraints is proposed. For solving the pose control scheme, it is reformulated as a standard quadratic programming at joint angular velocity level. Then, a projection neural network solver is developed to handle the quadratic programming . Further, for better computer operation and real-time requirements, the discrete algorithm is desired. As a result, a one-iteration computing algorithm is developed for pose control of redundant arm by discretizing the projection neural network solver. Theoretical results are presented to guarantee the properties of the proposed scheme and the one-iteration computing algorithm. In addition, plenty of comparative experiments on the basis of UR5 arm verify the efficacy and superiority of the one-iteration computing algorithm, and substantiate theoretical results.},
  archive      = {J_ASOC},
  author       = {Min Yang and Yunong Zhang and Xuefeng Zhou and Haifeng Hu},
  doi          = {10.1016/j.asoc.2021.108007},
  journal      = {Applied Soft Computing},
  pages        = {108007},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pose control of constrained redundant arm using recurrent neural networks and one-iteration computing algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble learning via feature selection and multiple
transformed subsets: Application to image classification. <em>ASOC</em>,
<em>113</em>, 108006. (<a
href="https://doi.org/10.1016/j.asoc.2021.108006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the machine learning field, especially in classification tasks , the model’s design and construction are very important. Constructing the model via a limited set of features may sometimes bound the classification performance and lead to non-optimal performances that some algorithms can provide. To this end, Ensemble learning methods were proposed in the literature. These methods’ main goal is to learn a set of models that provide features or predictions whose joint use could lead to a performance better than that obtained by the single model. In this paper, we propose three variants of a new efficient ensemble learning approach that was able to enhance the classification performance of a linear discriminant embedding method. As a case study we consider the efficient “Inter-class sparsity discriminative least square regression” method. We seek the estimation of an enhanced data representation. Instead of deploying multiple classifiers on top of the transformed features, we target the estimation of multiple extracted feature subsets obtained by multiple learned linear embeddings. These are associated with subsets of ranked original features. Multiple feature subsets were used for estimating the transformations. The derived extracted feature subsets were concatenated to form a single data representation vector that is used in the classification process. Many factors were studied and investigated in this paper including (Parameter combinations, number of models, different training percentages, feature selection methods combinations, etc.). Our proposed approach has been benchmarked on different image datasets of various sizes and types (faces, objects and scenes). The proposed scheme achieved competitive performance on four face image datasets (Extended Yale B, LFW-a, Gorgia and FEI) as well as on the COIL20 object dataset and the Outdoor Scene dataset. We measured the performance of our proposed schemes in comparison to (the single model ICS_DLSR, RDA_GD, RSLDA , PCE, LDE, LDA, SVM as well as the KNN algorithm) The conducted experiments showed that the proposed approach can enhance the classification performance in an efficient manner compared to the single-model based learning and was able to outperform its competing methods.},
  archive      = {J_ASOC},
  author       = {A. Khoder and F. Dornaika},
  doi          = {10.1016/j.asoc.2021.108006},
  journal      = {Applied Soft Computing},
  pages        = {108006},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble learning via feature selection and multiple transformed subsets: Application to image classification},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CGFFCM: Cluster-weight and group-local feature-weight
learning in fuzzy c-means clustering algorithm for color image
segmentation. <em>ASOC</em>, <em>113</em>, 108005. (<a
href="https://doi.org/10.1016/j.asoc.2021.108005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy c-means (FCM) algorithm is a popular method for data clustering and image segmentation . However, the main problem of this algorithm is that it is very sensitive to the initialization of primary clusters , so it may not perform well in segmenting complex images. Another problem with the FCM is the equal importance of the image features used during the segmentation process , which causes unstable performance on different images. In this paper, we propose an FCM-based color image segmentation approach, termed CGFFCM , applying an automatic cluster weighting scheme to reduce the sensitivity to the initialization, and a group-local feature weighting strategy to better image segmentation. Also, we combine the proposed clustering algorithm with the Imperialist Competitive Algorithm (ICA) to optimize the feature weighting process. In addition, we apply an efficient combination of image features to increase the segmentation quality . The performance of CGFFCM is evaluated and compared with state-of-the-art methods (such as SMKIFC (semi-supervised surrogate-assisted multi-objective kernel intuitionistic fuzzy clustering), and A-PSO-IT2IFCM (alternate particle swarm optimization based adaptive interval type-2 intuitionistic FCM clustering algorithm)) using the Berkeley benchmark dataset. The results obtained by CGFFCM are 95\%, 79\%, and 91\%, in terms of average Accuracy , NMI , and F-score metrics, respectively, which all are better than the competitors. The implementation source code of CGFFCM is made publicly available at https://github.com/Amin-Golzari-Oskouei/CGFFCM .},
  archive      = {J_ASOC},
  author       = {Amin Golzari Oskouei and Mahdi Hashemzadeh and Bahareh Asheghi and Mohammad Ali Balafar},
  doi          = {10.1016/j.asoc.2021.108005},
  journal      = {Applied Soft Computing},
  pages        = {108005},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CGFFCM: Cluster-weight and group-local feature-weight learning in fuzzy C-means clustering algorithm for color image segmentation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-population and dynamic-iterative cuckoo search
algorithm for linear antenna array synthesis. <em>ASOC</em>,
<em>113</em>, 108004. (<a
href="https://doi.org/10.1016/j.asoc.2021.108004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nature inspired algorithms have served as the backbone of modern computing technology and over the past three decades, the field has grown enormously. CS is a recent addition to optimization computing and suffers from problem of local optima stagnation and poor exploration. This paper presents a new version of CS namely extended CS (ECS) by employing the concept of multi-population adaptation, adaptive switching, dynamic iterative search, and GWO inspired global search phase, for solving benchmark problems and real-world design problem of linear antenna array (LAA) optimization. The algorithm aims at providing a better framework for optimization problems by keeping the original structure of CS intact. For performance evaluation, the algorithm has been firstly applied on two highly challenging datasets namely CEC 2015 and CEC 2017 benchmark problems and performance evaluation is done in comparison with other algorithms. Further to test the ECS algorithm on real world application , it is applied for synthesis of uniform and non-uniform LAA. Experimentally, the ECS algorithm is found to provide better performance in comparison to basic CS and others state-of-the-art algorithms. Statistical tests and radiation patterns further validate the results.},
  archive      = {J_ASOC},
  author       = {Rohit Salgotra and Mohamed Abouhawwash and Urvinder Singh and Sriparna Saha and Nitin Mittal and Shubham Mahajan and Amit Kant Pandit},
  doi          = {10.1016/j.asoc.2021.108004},
  journal      = {Applied Soft Computing},
  pages        = {108004},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-population and dynamic-iterative cuckoo search algorithm for linear antenna array synthesis},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Observer-based adaptive neural network control for PEMFC
air-feed subsystem. <em>ASOC</em>, <em>113</em>, 108003. (<a
href="https://doi.org/10.1016/j.asoc.2021.108003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oxygen excess ratio (OER) tracking of polymer electrolyte membrane fuel cell (PEMFC) air-feed subsystem is an interesting topic to obtain a high energy conversion ratio. The control purpose is challenging due to the unknown nonlinear function , time-varying external disturbance and unmeasured variable. To deal with the above problems, a neural network control scheme based on tracking error observer and disturbance observer is proposed. Firstly, a canonical form of the PEMFC air-feed subsystem is developed based on the input–output feedback linearization method. Meanwhile, a tracking error observer which only uses the system output is developed for the reconstruction of the unmeasured variable. Secondly, the neural network based on a novel adaptive learning law is proposed to approximate the unknown nonlinear function . An identification model with a second-order low-pass filter is designed such that the adaptive learning law is adjusted by the observer error and modeling error, simultaneously. Finally, to reduce the effect caused by compound disturbance including the external disturbance and neural network approximation error, a nonlinear disturbance observer is proposed. The main contribution of this study is not only avoiding the prior knowledge of system uncertainty, external disturbance and the unmeasured variable but also improving the OER tracking performance under the measurement noise. Lyapunov theory analysis shows that the system tracking error is uniformly ultimately bounded. Numerical simulations and hardware-in-loop (HIL) experiments show that the proposed neural network control algorithm can maintain OER at the target value even under external disturbance. The experimental results show that the performance indexes including the mean absolute error (MAE), the root mean square error (RMSE) and the standard deviation (SD) of the proposed controller during the whole process are 0.0213, 0.0547 and 0.0538, respectively. Compared with the proportional–integral–derivative (PID) controller (MAE 0.0466, RMSE 0.0762 and SD 0.0745), robust adaptive radial basis function neural network (RBFNN-H ∞ ∞ ) controller (MAE 0.0841, RMSE 0.0839 and SD 0.0813) and hybrid robust adaptive radial basis function neural network (HARBFNN-1st) controller (MAE 0.0255, RMSE 0.0613 and SD 0.0603), the proposed controller performs a better performance.},
  archive      = {J_ASOC},
  author       = {Yunlong Wang and Yongfu Wang and Jing Zhao and Jianfeng Xu},
  doi          = {10.1016/j.asoc.2021.108003},
  journal      = {Applied Soft Computing},
  pages        = {108003},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Observer-based adaptive neural network control for PEMFC air-feed subsystem},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting the seasonal natural gas consumption in the US
using a gray model with dummy variables. <em>ASOC</em>, <em>113</em>,
108002. (<a href="https://doi.org/10.1016/j.asoc.2021.108002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To forecast the seasonal fluctuations of US natural gas consumption accurately, a novel gray model based on seasonal dummy variables and its derived model are separately established. Then, the approximate time response formula in the proposed seasonal forecasting model is optimally calculated by using particle swarm optimization algorithm. On this basis, empirical analysis is conducted using data pertaining to natural gas consumption in the US during 2010–2019. The results show that gray model based on seasonal dummy variables and its derived model can recognize seasonal fluctuations in US natural gas consumption, whose prediction performances are much better than that of a traditional gray model, autoregressive integrated moving average (ARIMA), support vector regression (SVR), recurrent neural network (RNN), transformer, and fuzzy time series forecasting models. The mean absolute percentage errors (MAPEs) of the proposed seasonal gray forecasting model and its derived model, the classic gray model, ARIMA, SVR, RNN, Transformer, and Fuzzy time series forecasting models are 3.46\%, 2.37\%, 12.23\%, 3.39\%, 2.38\%, 3.08\%, 3.84\%, and 5.02\% in the training set, while those are 4.57\%, 4.42\%, 12.44\%, 7.9\%, 8.09\%, 11.33\%, 35.19\% and 13.19\% in the test set, respectively. The predicted and empirical results obtained by utilizing the proposed gray model implied that natural gas consumption in the US from 2020 to 2022 will maintain its seasonal growth and periodic changes, with the highest and the lowest values in the first and second quarters, respectively.},
  archive      = {J_ASOC},
  author       = {Zheng-Xin Wang and Ling-Yang He and Yu-Feng Zhao},
  doi          = {10.1016/j.asoc.2021.108002},
  journal      = {Applied Soft Computing},
  pages        = {108002},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting the seasonal natural gas consumption in the US using a gray model with dummy variables},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comprehensive learning particle swarm optimization enabled
modeling framework for multi-step-ahead influenza prediction.
<em>ASOC</em>, <em>113</em>, 107994. (<a
href="https://doi.org/10.1016/j.asoc.2021.107994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemics of influenza are major public health concerns. Since influenza prediction always relies on the weekly clinical or laboratory surveillance data, typically the weekly Influenza-like illness (ILI) rate series, accurate multi-step-ahead influenza predictions using ILI series is of great importance, especially, to the potential coming influenza outbreaks. This study proposes Comprehensive Learning Particle Swarm Optimization based Machine Learning (CLPSO-ML) framework incorporating support vector regression (SVR) and multilayer perceptron (MLP) for multi-step-ahead influenza prediction. A comprehensive examination and comparison of the performance and potential of three commonly used multi-step-ahead prediction modeling strategies, including iterated strategy, direct strategy and multiple-input multiple-output (MIMO) strategy, was conducted using the weekly ILI rate series from both the Southern and Northern China. The results show that: (1) The MIMO strategy achieves the best multi-step-ahead prediction, and is potentially more adaptive for longer horizon; (2) The iterated strategy demonstrates special potentials for deriving the least time difference between the occurrence of the predicted peak value and the true peak value of an influenza outbreak; (3) For ILI in the Northern China, SVR model implemented with MIMO strategy performs best, and SVR with iterated strategy also shows remarkable performance especially during outbreak periods; while for ILI in the Southern China, both SVR and MLP models with MIMO strategy have competitive prediction performance},
  archive      = {J_ASOC},
  author       = {Siyue Yang and Yukun Bao},
  doi          = {10.1016/j.asoc.2021.107994},
  journal      = {Applied Soft Computing},
  pages        = {107994},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comprehensive learning particle swarm optimization enabled modeling framework for multi-step-ahead influenza prediction},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online early terminated streaming feature selection based on
rough set theory. <em>ASOC</em>, <em>113</em>, 107993. (<a
href="https://doi.org/10.1016/j.asoc.2021.107993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a vital dimensionality reduction technology for machine learning and data mining that aims to select a minimal subset from the original feature space. Traditional feature selection methods assume that all features can be required before learning, while features may exist in a stream mode for some real-world applications. Therefore, online streaming feature selection was proposed to handle streaming features on the fly. When the feature dimension is extraordinarily high or even infinite, it is time-consuming or impractical to wait for all the streaming features to arrive. Motivated by this, we study and solve the exciting issue of whether we can terminate the online streaming feature selection early for efficiency while maintaining satisfactory performance for the first time. Specifically, we first formally define the problem of online early terminated streaming feature selection and summary two properties that the early terminated mapping function should satisfy. Then we choose the dependency degree function in Rough Set theory as our early terminated mapping function and demonstrate that it satisfies the two properties. Based on this, we propose a novel Early Terminated Online Streaming Feature Selection framework, named OSFS-ET, which could terminate the streaming feature selection early before the end of streaming features and guarantee a competing performance with the currently selected features. Extensive experiments on twelve real-world datasets demonstrate that OSFS-ET can be far faster than state-of-the-art streaming feature selection methods while maintaining excellent performance on predictive accuracy .},
  archive      = {J_ASOC},
  author       = {Peng Zhou and Peipei Li and Shu Zhao and Yanping Zhang},
  doi          = {10.1016/j.asoc.2021.107993},
  journal      = {Applied Soft Computing},
  pages        = {107993},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online early terminated streaming feature selection based on rough set theory},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A heterogeneous guided ant colony algorithm based on space
explosion and long–short memory. <em>ASOC</em>, <em>113</em>, 107991.
(<a href="https://doi.org/10.1016/j.asoc.2021.107991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ant colony optimization (ACO) has a good performance in solving discrete problems, but it inevitably has some disadvantages. Although it has good stability, it has some shortcomings in the convergence speed and solution accuracy when dealing with a large amount of data. Therefore, to solve the above problems, we proposed a heterogeneous guided ant colony algorithm based on space explosion and long–short memory. First, the difference between the current optimal path and the globally optimal path is used to search for the solution, which could accelerate the convergence of the algorithm. Second, the space explosion strategy is used to recombine the solution in different directions to avoid the algorithm falling into the local optimum. In this paper, 37 TSP data sets are selected to carry out simulation experiments. From the results and the rank-sum test, it could be concluded that the improved ant colony algorithm improved significantly in terms of convergence speed and solution accuracy.},
  archive      = {J_ASOC},
  author       = {Jin Yu and Xiaoming You and Sheng Liu},
  doi          = {10.1016/j.asoc.2021.107991},
  journal      = {Applied Soft Computing},
  pages        = {107991},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A heterogeneous guided ant colony algorithm based on space explosion and long–short memory},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An assessment of the banking industry performance based on
intuitionistic fuzzy best-worst method and fuzzy inference system.
<em>ASOC</em>, <em>113</em>, 107990. (<a
href="https://doi.org/10.1016/j.asoc.2021.107990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19’s impact on individuals, enterprises, and organizations is undeniable. Many markets, in particular, financial markets, have experienced considerable shock and suffered massive losses. Supply chain networks have encountered significant problems, especially in terms of financing. Supply chain finance (SCF), responsible for financing the supply chain components and improving supply chain performance, received a significant impact of COVID-19 consequences. Financial providers are the leading resource for supply chain finance. The banking sector is known as the main element of financing among financial providers. Any disruption in the banking system procedures can have a significant impact on the financing process. In this paper, we have tried to understand the essential effects of the COVID-19 pandemic on the banking sector. A hybrid approach consists of a novel and extended intuitionistic fuzzy best-worst method (IFBWM) to determine the critical factors’ weights, and a fuzzy inference system to get the performance index of the banking sector is utilized. The proposed method investigates the banking sector’s role in supply chain finance, specifies the principal risks of this sector in emergencies such as pandemics. The Turkish banking sector’s performance in three different periods is considered to demonstrate the validity and applicability of the proposed approach. The results depict that credit risk, loss of income, and liquidity were the most critical factors of the COVID-19 outbreak in the banking sector. Also, the Turkish banking sector’s performance has worsened with the spread and continuation of the COVID-19. Moreover, the cogency of the proposed method is verified through a brief comparative analysis.},
  archive      = {J_ASOC},
  author       = {Seyed Amin Seyfi-Shishavan and Fatma Kutlu Gündoğdu and Elmira Farrokhizadeh},
  doi          = {10.1016/j.asoc.2021.107990},
  journal      = {Applied Soft Computing},
  pages        = {107990},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An assessment of the banking industry performance based on intuitionistic fuzzy best-worst method and fuzzy inference system},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-stage stacking heterogeneous ensemble learning method
for gasoline octane number loss prediction. <em>ASOC</em>, <em>113</em>,
107989. (<a href="https://doi.org/10.1016/j.asoc.2021.107989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gasoline is the main fuel for small vehicles, and the exhaust emissions from its combustion have a major impact on the atmospheric environment. In the cumbersome process of gasoline refining , the aim is to reduce the sulfur and olefin contents within the raw material while maintaining its octane number as much as possible. In general, the research octane number (RON) loss is measured by using an instrument in the laboratory, which is time-consuming and expensive. Therefore, the use of algorithms to build RON loss prediction models has become a hot topic. Considering that machine learning has a good ability in fitting the non-linear complex data, we propose a stacking-based heterogeneous ensemble method for RON prediction. First, we propose a fusion algorithm of sequence forward search (SFS) and feature importance score to reduce the dimension of data set. Later, the data after feature selection will be used to construct a two-stage stacking heterogeneous ensemble learning model. Finally, the differential evolution (DE) algorithm is used to optimize multiple sensitive parameters involved in the model. Experiments with data obtained in the actual gasoline refining process show that the proposed method can accurately predict the RON loss in the product. Compared to the popular machine learning methods such as support vector machines , random forests , and XGBoost , the proposed method achieves the smallest mean square error . Furthermore, we analyze the important features that affect the RON loss to promote the development of the gasoline refining industry.},
  archive      = {J_ASOC},
  author       = {Shaoze Cui and Huaxin Qiu and Sutong Wang and Yanzhang Wang},
  doi          = {10.1016/j.asoc.2021.107989},
  journal      = {Applied Soft Computing},
  pages        = {107989},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage stacking heterogeneous ensemble learning method for gasoline octane number loss prediction},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting hourly PM2.5 based on deep temporal
convolutional neural network and decomposition method. <em>ASOC</em>,
<em>113</em>, 107988. (<a
href="https://doi.org/10.1016/j.asoc.2021.107988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For hourly PM 2.5 2.5 concentration prediction, accurately capturing the data patterns of external factors that affect PM 2.5 2.5 concentration changes, and constructing a forecasting model is one of efficient means to improve forecasting accuracy . In this study, a novel hybrid forecasting model based on complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) and deep temporal convolutional neural network (DeepTCN) is developed to predict PM 2.5 2.5 concentration, by modeling the data patterns of historical pollutant concentrations data, meteorological data , and discrete time variables’ data. Taking PM 2.5 2.5 concentration of Beijing as the sample, experimental results showed that the forecasting accuracy of the proposed CEEMDAN-DeepTCN model is verified to be the highest when compared with the statistics-based models, traditional machine learning models, the popular deep learning models and several existing hybrid models. The new model has improved the capability to model the PM 2.5 2.5 -related factor data patterns, and can be used as a promising tool for forecasting PM 2.5 2.5 concentrations.},
  archive      = {J_ASOC},
  author       = {Fuxin Jiang and Chengyuan Zhang and Shaolong Sun and Jingyun Sun},
  doi          = {10.1016/j.asoc.2021.107988},
  journal      = {Applied Soft Computing},
  pages        = {107988},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting hourly PM2.5 based on deep temporal convolutional neural network and decomposition method},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Consensus kernel subspace clustering based on coefficient
discriminant information. <em>ASOC</em>, <em>113</em>, 107987. (<a
href="https://doi.org/10.1016/j.asoc.2021.107987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, multiple kernel subspace clustering technology has been extensively used in various fields of machine learning and computer vision . However, the traditional multiple kernel subspace clustering method still has two problems. On the one hand, the combination of kernel functions is uncertain. Choosing inappropriate kernel functions from different sources or data with different characteristics will make the results inaccurate. On the other hand, data sources are extensive and subspace division is not accurate. In this paper, we unify coefficient discriminant information and multiple kernel subspace clustering into one process. Firstly, we combine the consensus Hilbert space to propose a consensus kernel method . This method reconstructs a set of different kernel matrices into kernel matrices more in line with the data characteristics, which can effectively solve the uncertainty of the combination of kernel functions. Secondly, the diagonal structure of blocks is constructed by subspace clustering, which can effectively solve the problems of different data sources. In the process, we propose a coefficient discriminant information which can constrain the data in the coefficient space to group as many of the same types of data as possible, and separate different types of data as much as possible. Finally, we propose a new iterative optimization method based on the Alternating Direction Multiplier Method (ADMM) to solve the final optimization goal. Our experiment is conducted on twenty different data sets. According to the final experimental results, our proposed algorithm has higher ACC, NMI and ARI than the latest clustering methods.},
  archive      = {J_ASOC},
  author       = {Zhongyuan Wang and Jinglei Liu},
  doi          = {10.1016/j.asoc.2021.107987},
  journal      = {Applied Soft Computing},
  pages        = {107987},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consensus kernel subspace clustering based on coefficient discriminant information},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequency stabilization in interconnected power system using
bat and harmony search algorithm with coordinated controllers.
<em>ASOC</em>, <em>113</em>, 107986. (<a
href="https://doi.org/10.1016/j.asoc.2021.107986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern power system faces excessive frequency aberrations due to the intermittent renewable generations and persistently changing load demands. To avoid any possible blackout, an efficient and robust control strategy is obligatory to minimize deviations in the system frequency and tie-line. Hence, to achieve this target, a new two-degree of freedom-tilted integral derivative with filter (2DOF–TIDN) controller is proposed in this work for a two-area wind-hydro-diesel power system . To enhance the outcome of the proposed 2DOF–TIDN controller, its gain parameters are optimized with the use of a newly designed hybrid bat algorithm-harmony search algorithm (hybrid BA–HSA) technique. The effectiveness and superiority of hybrid BA–HSA tuned 2DOF–TIDN is validated over various existing optimization techniques like cuckoo search (CS), particle swarm optimization (PSO), HSA, BA and teaching learning-based optimization (TLBO). To further refine the system outcome in the dynamic conditions, several flexible AC transmission systems (FACTS) and superconducting magnetic energy storage (SMES) units are adopted for enriching the frequency and tie-line responses. The FACTS controllers like static synchronous series compensator (SSSC), thyristor-controlled phase shifter (TCPS), unified power flow controller (UPFC) and interline power flow controller (IPFC) are employed with SMES simultaneously. The simulation results disclose that the hybrid BA–HSA based 2DOF–TIDN shows superior dynamic performance with IPFC–SMES than other studied approaches. A sensitivity analysis is examined to verify the robustness of proposed controller under ± ± 25\% changes in loading and system parameters.},
  archive      = {J_ASOC},
  author       = {K. Peddakapu and M.R. Mohamed and P. Srinivasarao and P.K. Leung},
  doi          = {10.1016/j.asoc.2021.107986},
  journal      = {Applied Soft Computing},
  pages        = {107986},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Frequency stabilization in interconnected power system using bat and harmony search algorithm with coordinated controllers},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic segmentation of tea geometrid in natural scene
images using discriminative pyramid network. <em>ASOC</em>,
<em>113</em>, 107984. (<a
href="https://doi.org/10.1016/j.asoc.2021.107984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A discriminative pyramid (DP) network-based method is presented in this paper, which aims to perform semantic segmentation on tea geometrid in natural scene images. The method uses image flipping, translation, mirroring, random zooming, and other techniques for training sample augmentation and adopts local histogram equalization to reduce nonuniform light influence on segmentation. Moreover, this method constructs a DP network to improve segmentation accuracy . The DP network contains two sub networks : pyramid attention and border networks. The pyramid attention network can capture the global context information of targets at different scales, increase the receptive fields to focus on small targets, and solve the problems of large shape change, small size, and difficult detection of tea geometrids. The border network can increase the differences between tea geometrids and tea tree stalks, diseased leaves, and other types of backgrounds with similar appearances by extracting and supervising semantic boundaries to help the network learn additional discriminative features . Experiment results show that the proposed method has outstanding accuracy for semantic segmentation of tea geometrid in natural scene images.},
  archive      = {J_ASOC},
  author       = {Gensheng Hu and Suqing Li and Mingzhu Wan and Wenxia Bao},
  doi          = {10.1016/j.asoc.2021.107984},
  journal      = {Applied Soft Computing},
  pages        = {107984},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semantic segmentation of tea geometrid in natural scene images using discriminative pyramid network},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A sparse regression and neural network approach for
financial factor modeling. <em>ASOC</em>, <em>113</em>, 107983. (<a
href="https://doi.org/10.1016/j.asoc.2021.107983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor models are central to understanding risk-return trade-offs in finance. Since Fama and French (1993), hundreds of factors have been found to have explanatory power for asset pricing. To construct a factor model, two tasks have to be performed: Feature Selection, selecting a small subset given a large number of factors to overcome overfitting in regression, and Feature Engineering, determining the interactions between the factors. In this work, the process of constructing factor models (not the factors themselves) is examined. A unified, two-step process of dimensionality reduction and nonlinear transformation that produces parsimonious, general factor models is proposed. Comparisons between frameworks implementing linear feature selection models as well as non-linear feature reduction techniques are conducted. A second stage generalizes the models by learning nonlinear interactions . The framework attempts to strike a balance between accuracy and interpretability . Results of computational experiments on historical financial data, on three models of varying degrees of non-linearity and interpretability suggest that mixed-integer-programming-based formulations are suitable for the task of linear financial factor selection and that the second-stage nonlinearity due to neural networks improves accuracy.},
  archive      = {J_ASOC},
  author       = {Hassan T. Anis and Roy H. Kwon},
  doi          = {10.1016/j.asoc.2021.107983},
  journal      = {Applied Soft Computing},
  pages        = {107983},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A sparse regression and neural network approach for financial factor modeling},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconfiguration with DG location and capacity optimization
using crossover mutation based harris hawk optimization algorithm
(CMBHHO). <em>ASOC</em>, <em>113</em>, 107982. (<a
href="https://doi.org/10.1016/j.asoc.2021.107982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a crossover mutation based Harris Hawk Optimization (CMBHHO) technique has been proposed for the reconfiguration of feeder network by optimal location and sizing of distribution generator (DG) in power system . The novelty of the proposed method is the combined performance of both Harris Hawk Optimization (HHO) and crossover mutation operator. The mentioned technique is the optimization technique, which is used for optimizing the optimum location and capacity of the DG for radial distribution network . The CMBHHO algorithm requires radial distribution network voltage, real and reactive power for determining the optimum location and capacity of the DG. Here, the CMBHHO input parameters are classified into sub parameters and allowed as crossover mutation operator process. The crossover, mutation operator synthesis the problem and develops the sub solution with the help of sub parameters. The CMBHHO is applied for identifying the optimum location and capacity of the DG. Then the proposed technique is implemented in the MATLAB/Simulink platform and the effectiveness is analyzed by comparing it with various existing techniques such as GSA, LSA, ALO, HHO, HSA , GA , RGA, FWA, GA*, PSO , PSO*, TLBO and TLBO*. In addition, the percentage power loss reduction and voltage deviation are also analyzed. The obtained value for the percentage power loss reduced using proposed technique under case 1 is 70.87\%, case 2 is 88.70\% and case 3 is 92.81\%. The obtained value for the voltage deviation index reduced using proposed technique under case 1 is 0.02062, case 2 is 0.00051 and case 3 is 0.02062.},
  archive      = {J_ASOC},
  author       = {Pradeep Mohandas and Sudhakar Thirumalaivasal Devanathan},
  doi          = {10.1016/j.asoc.2021.107982},
  journal      = {Applied Soft Computing},
  pages        = {107982},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reconfiguration with DG location and capacity optimization using crossover mutation based harris hawk optimization algorithm (CMBHHO)},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A soft computing based multi-objective optimization approach
for automatic prediction of software cost models. <em>ASOC</em>,
<em>113</em>, 107981. (<a
href="https://doi.org/10.1016/j.asoc.2021.107981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tries to extend the idea of single-objective differential evolution (DE) algorithm to a multi-objective algorithm. Most of the existing algorithms face the problem of diversity loss and convergence rate. In this paper, we propose a novel multi-objective DE algorithm to deal with this problem. In the validation process, the proposed method is validated in two steps. Firstly, the new homeostasis factor-based mutation operator incorporates multi-objective differential evolution algorithms (MODE). In this method, we use the Pareto optimality principle. We incorporate a new adaptive-based mutation operator (MODE) to create more diversity and enhance convergence rate among candidate solutions which provide better solutions to help the evolution. The effectiveness of the proposed method is evaluated on eight benchmarks of bi-objective and tri-objective test functions. Our proposed method performed well compared to the latest variants of multi-objective evolutionary algorithms (MOEAs). Secondly, the proposed method is used for an application-based test by applying it for software cost estimation . This method also incorporates multi-objective parameters, i.e., two objectives-based software cost estimation and three objectives-based software cost estimation. The proposed approach achieves better results in most software projects in terms of reducing effort and minimum error.},
  archive      = {J_ASOC},
  author       = {Shailendra Pratap Singh and Gaurav Dhiman and Prayag Tiwari and Rutvij H. Jhaveri},
  doi          = {10.1016/j.asoc.2021.107981},
  journal      = {Applied Soft Computing},
  pages        = {107981},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A soft computing based multi-objective optimization approach for automatic prediction of software cost models},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatically interactive group VIKOR decision making
mechanism based on BSO-SNA. <em>ASOC</em>, <em>113</em>, 107979. (<a
href="https://doi.org/10.1016/j.asoc.2021.107979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the reliability of alternative ranking and group consensus adaptively, this paper develops an automatically interactive group decision-making (GDM) framework under the VIKOR environment. The proposed framework contains an interaction network mechanism based on social network analysis (SNA) and an opinion interaction mechanism based on Brain Strom Optimization (BSO) algorithm. Firstly, to connect isolated decision-makers (DMs), interaction relationships among DMs are built based on the SNA and similarity measurement. Secondly, the BSO algorithm is introduced to design the opinion interaction mechanism, including subgroup identification model and preference iteration model. The preference iteration model contains an adaptive preference update rule (UR), iteration stop rule (ISR), and solution selection rule (SSR) to enhance group consensus adaptively rather than force DMs to change the preference. Thirdly, the proposed interactive group VIKOR (G-VIKOR) method is applied in SARS-cov-2 nucleic acid detection sites evaluation. Finally, consistency and comparison analysis are developed to illustrate the validity and robustness of evaluation results.},
  archive      = {J_ASOC},
  author       = {Xiwen Tao and Wenqi Jiang},
  doi          = {10.1016/j.asoc.2021.107979},
  journal      = {Applied Soft Computing},
  pages        = {107979},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatically interactive group VIKOR decision making mechanism based on BSO-SNA},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-phase quasi-affine transformation evolution with
feedback for parameter identification of photovoltaic models.
<em>ASOC</em>, <em>113</em>, 107978. (<a
href="https://doi.org/10.1016/j.asoc.2021.107978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithm is a prestigious technique for solving optimization problems . QUATRE is a simple but powerful algorithm. However, QUATRE also shows premature convergence and is easily trapped in local optima for complex optimization problems . This work presents a novel algorithm named two-phase QUasi-Affine Transformation Evolution with feedback (tfQUATRE). The proposed tfQUATRE is an enhanced quasi-affine transformation evolution algorithm. In tfQUATRE, a two-phase approach is introduced to improve the exploration and exploitation abilities by adjusting the search tendency at different phases. Moreover, the historical population is employed for the feedback approach to guide the search towards promising areas to maintain population diversity, which boosts the exploration ability. The comprehensive performance of tfQUATRE is evaluated in the simulations. First, the performance of tfQUATRE is evaluated under the CEC2017 test suite. The simulations prove that tfQUATRE is superior to 12 state-of-the-art algorithms. In addition, tfQUATRE is applied to extract the parameters of photovoltaic (PV) systems in real application. The experimental results confirm that the proposed tfQUATRE is more competitive than 17 recent counterparts.},
  archive      = {J_ASOC},
  author       = {Xiaopeng Wang and Shu-Chuan Chu and Václav Snášel and Lingping Kong and Jeng-Shyang Pan and Hisham A. Shehadeh},
  doi          = {10.1016/j.asoc.2021.107978},
  journal      = {Applied Soft Computing},
  pages        = {107978},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-phase quasi-affine transformation evolution with feedback for parameter identification of photovoltaic models},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The landscape of soft computing applications for terrorism
analysis: A review. <em>ASOC</em>, <em>113</em>, 107977. (<a
href="https://doi.org/10.1016/j.asoc.2021.107977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terrorism is a globally prevalent dreaded form of crime against humanity in modern civil society. The nature of surprise, casualties caused, and the panic involved in terrorist activities compels improvisation of efforts to counter them. These counter-terrorism efforts require precise and reliable techniques to analyze the patterns existing in data of previous terrorist activities. Such patterns can reveal vital information for predicting details of upcoming attacks. Structures of terrorist networks and their operational specifics are among such attack details that deserve critical analysis by specialized applications. Most of these applications used for analyzing terrorism data are based on computational methods articulated under the broad term of soft computing techniques . In this paper, we review various aspects of soft computing applications developed for the analysis of terrorism data. Initiating with an in-depth discussion on the databases of terrorist event data, we propose 6 criteria for their quality evaluation. We proceed by elaborating the utilities of a prospective terrorism analysis application. These utilities include forecasting, detection and link mapping of terrorist activities. In the core of this review, we present a categorization of soft computing techniques into 3 major components; approximate reasoning, metaheuristic optimization and machine learning . A rich volume of applications for terrorism analysis has been discussed and compared on the scale of these techniques and their subcategories. Among these applications, while metaheuristic approaches present results to a precision of 90\%, machine learning classifier methods also depict a classification accuracy of up to 93\% in their outputs. Later, we discuss the perceived challenges in current literature, their consequential inclinations of research, and suggestions for directions of possible future developments. Finally, we conclude this review with a summary of the current state-of-art and critical comment on open opportunities in terrorism analysis.},
  archive      = {J_ASOC},
  author       = {Saurabh Ranjan Srivastava and Yogesh Kumar Meena and Girdhari Singh},
  doi          = {10.1016/j.asoc.2021.107977},
  journal      = {Applied Soft Computing},
  pages        = {107977},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The landscape of soft computing applications for terrorism analysis: A review},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum fractional order darwinian particle swarm
optimization for hyperspectral multi-level image thresholding.
<em>ASOC</em>, <em>113</em>, 107976. (<a
href="https://doi.org/10.1016/j.asoc.2021.107976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Hyperspectral Image (HSI) is a data cube consisting of hundreds of spatial images . Each captured spatial band is an image at a particular wavelength. Thresholding of these images is itself a tedious task. Two procedures, viz., Qubit Fractional Order Particle Swarm Optimization and Qutrit Fractional Order Particle Swarm Optimization are proposed in this paper for HSI thresholding. The Improved Subspace Decomposition Algorithm, Principal Component Analysis, and a Band Selection Convolutional Neural Network are used in the preprocessing stage for band reduction or informative band selection. For optimal segmentation of the HSI, modified Otsu’s criterion, Masi entropy and Tsallis entropy are used. A new method for quantum disaster operation is implemented to prevent the algorithm from getting stuck into local optima. The implementations are carried out on three well known datasets viz., the Indian Pines, the Pavia University and the Xuzhou HYSPEX. The proposed methods are compared with state-of-the-art methods viz., Particle Swarm Optimization (PSO), Ant Colony Optimization , Darwinian Particle Swarm Optimization, Fractional Order Particle Swarm Optimization, Exponential Decay Weight PSO and Heterogeneous Comprehensive Learning PSO concerning the optimal thresholds, best fitness value, computational time, mean and standard deviation of fitness values. Furthermore, the performance of each method is validated with Peak signal-to-noise ratio and Sørensen–Dice Similarity Index. The Kruskal–Wallis test, a statistical significance test, is conducted to establish the superiority in favor of the proposed methods. The proposed algorithms are also implemented on some benchmark functions and real life images to establish their universality.},
  archive      = {J_ASOC},
  author       = {Tulika Dutta and Sandip Dey and Siddhartha Bhattacharyya and Somnath Mukhopadhyay},
  doi          = {10.1016/j.asoc.2021.107976},
  journal      = {Applied Soft Computing},
  pages        = {107976},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum fractional order darwinian particle swarm optimization for hyperspectral multi-level image thresholding},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StaResGRU-CNN with CMedLMs: A stacked residual GRU-CNN with
pre-trained biomedical language models for predictive intelligence.
<em>ASOC</em>, <em>113</em>, 107975. (<a
href="https://doi.org/10.1016/j.asoc.2021.107975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a task requiring strong professional experience as supports, predictive biomedical intelligence cannot be separated from the support of a large amount of external domain knowledge. By using transfer learning to obtain sufficient prior experience from massive biomedical text data, it is essential to promote the performance of specific downstream predictive and decision-making task models. This is an efficient and convenient method, but it has not been fully developed for Chinese Natural Language Processing (NLP) in the biomedical field . This study proposes a Stacked Residual Gated Recurrent Unit-Convolutional Neural Networks (StaResGRU-CNN) combined with the pre-trained language models (PLMs) for biomedical text-based predictive tasks. Exploring related paradigms in biomedical NLP based on transfer learning of external expert knowledge and comparing some Chinese and English language models . We have identified some key issues that have not been developed or those present difficulties of application in the field of Chinese biomedicine. Therefore, we also propose a series of Chinese bioMedical Language Models (CMedLMs) with detailed evaluations of downstream tasks. By using transfer learning, language models are introduced with prior knowledge to improve the performance of downstream tasks and solve specific predictive NLP tasks related to the Chinese biomedical field to serve the predictive medical system better. Additionally, a free-form text Electronic Medical Record (EMR)-based Disease Diagnosis Prediction task is proposed, which is used in the evaluation of the analyzed language models together with Clinical Named Entity Recognition , Biomedical Text Classification tasks. Our experiments prove that the introduction of biomedical knowledge in the analyzed models significantly improves their performance in the predictive biomedical NLP tasks with different granularity . And our proposed model also achieved competitive performance in these predictive intelligence tasks.},
  archive      = {J_ASOC},
  author       = {Pin Ni and Gangmin Li and Patrick C.K. Hung and Victor Chang},
  doi          = {10.1016/j.asoc.2021.107975},
  journal      = {Applied Soft Computing},
  pages        = {107975},
  shortjournal = {Appl. Soft. Comput.},
  title        = {StaResGRU-CNN with CMedLMs: A stacked residual GRU-CNN with pre-trained biomedical language models for predictive intelligence},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel hybrid ANN and clustering inspired load balancing
algorithm in cloud environment. <em>ASOC</em>, <em>113</em>, 107963. (<a
href="https://doi.org/10.1016/j.asoc.2021.107963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load balancing in the cloud environment is a NP-hard problem. To address this problem, a new hybridized soft computing inspired technique named Clustering based Artificial Neural Network for Dynamic Load Balancing (CANN-DLB) is introduced. Artificial Neural Network aims to achieve an optimized load of VMs by training the cloud environment. Back-propagation ANN is performed to calculate the optimized Virtual Machine (VM) load in cloud systems for improving QoS. The CANN-DLB technique uses K-means clustering algorithm over calculated VM loads and clusters them into under-loaded and over-loaded VMs. The scheduling of homogeneous independent non-preemptive tasks is performed using Particle Swarm Optimization (PSO) technique. CloudSim (Cloud Simulator) tool has been used to implement the proposed algorithm. The performance of the CANN-DLB technique has been achieved for Space Shared and Time Shared VM task scheduler methods. The simulation results of the CANN-DLB algorithm are compared with existing load balancing and scheduling algorithms with the objectives to improve load balance fairness. Results of CANN-DLB have achieved 69.5\%, 96.9\%, 96.0\% and 97.4\% less processing cost and 87.12\%, 87.12\%, 87.57\% and 81.78\% less degree of imbalance compared to CM-eFCFS, Static RR, MinMin and MaxMin method respectively. The system fairness with respect to system load is calculated to analyze CANN-DLB algorithm performance on heavy load. The proposed algorithm achieved 12\% higher load fairness than the existing Two-level method and proves that the introduced model works better in dynamic scenarios. Plotted graphs show that the proposed idea is innovative for load balancing in a dynamic cloud environment. This hybridization of the ANN and K-means clustering method produces remarkable results as compared to existing algorithms for different cases.},
  archive      = {J_ASOC},
  author       = {Sarita Negi and Neelam Panwar and Man Mohan Singh Rauthan and Kunwar Singh Vaisla},
  doi          = {10.1016/j.asoc.2021.107963},
  journal      = {Applied Soft Computing},
  pages        = {107963},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Novel hybrid ANN and clustering inspired load balancing algorithm in cloud environment},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The categorized orienteering problem with count-dependent
profits. <em>ASOC</em>, <em>113</em>, 107962. (<a
href="https://doi.org/10.1016/j.asoc.2021.107962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the Categorized Orienteering Problem with Count-Dependent Profits (COPCDP), in which nodes are clustered into different categories, and each category is assigned an interest rate. The goal is to find a tour among all nodes from various categories, which maximizes the total collected profit. However, different from the basic Orienteering Problem (OP), the profit of each node is not fixed, but decreases based on the number of nodes selected from the same category. This way, it is encouraged to visit fewer nodes from the same category. The COPCDP may have different exciting applications, but in this research, it is focused on its application in personal tourist trip planning. Two meta-heuristic algorithms based on the combination of a Genetic Algorithm with a Variable Neighborhood Descent structure (GA-VND), as well as a Simulated Annealing algorithm combined with a Variable Neighborhood Search (SA-VNS), are proposed. Computational experiments over a large set of instances show the efficiency of both algorithms. However, the GA-VND is proved to perform better in terms of solution quality. Additionally, a real-size problem instance based on the real data from the megacity of Tehran is generated and solved by using the proposed GA-VND to prove the usability of the method in practice.},
  archive      = {J_ASOC},
  author       = {Hossein Jandaghi and Ali Divsalar and Saeed Emami},
  doi          = {10.1016/j.asoc.2021.107962},
  journal      = {Applied Soft Computing},
  pages        = {107962},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The categorized orienteering problem with count-dependent profits},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The network loan risk prediction model based on
convolutional neural network and stacking fusion model. <em>ASOC</em>,
<em>113</em>, 107961. (<a
href="https://doi.org/10.1016/j.asoc.2021.107961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to establish a more suitable risk prediction system for the network loan platform and reduce the loan risk of the network platform, this paper proposes a loan risk prediction model Stacking+CNN, which is based on the convolutional neural network (CNN) and the integration model of Stacking. Using CNN to extract the local spatial features hidden in the results of Stacking algorithm, improve the generalization ability of the model, give full play to the generalization ability of Stacking algorithm and the feature extraction ability of CNN algorithm. Firstly, this paper uses wrapper method and variance inflation coefficient (VIF) to extract the features from the original data. Then, the processed data is brought into the first layer of Stacking algorithm base learner for training. The predicted results are taken into CNN as a new data set for feature extraction. Finally, the extracted data are brought into the SVM for risk prediction. Empirical results show that the prediction model proposed in this paper is better than the single model and other integrated models in predicting accuracy and recall rate.},
  archive      = {J_ASOC},
  author       = {Meixuan Li and Chun Yan and Wei Liu},
  doi          = {10.1016/j.asoc.2021.107961},
  journal      = {Applied Soft Computing},
  pages        = {107961},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The network loan risk prediction model based on convolutional neural network and stacking fusion model},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simulation-based modified backtracking search algorithm
for multi-objective stochastic flexible job shop scheduling problem with
worker flexibility. <em>ASOC</em>, <em>113</em>, 107960. (<a
href="https://doi.org/10.1016/j.asoc.2021.107960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flexible shop scheduling problem (FJSSP) has been an extensively studied and applied manufacturing systems in recent years. However, research has mostly focused on addressing the FJSSP in its basic form by relaxing several practical constraints. In this work, we consider FJSSP with worker flexibility and uncertain processing times. A simheuristic approach addresses the multi-objective stochastic FJSSP with worker flexibility, minimizing the expected makespan and standard deviation of makespan. This approach integrates the Monte Carlo simulation (MCS) into the proposed multi-objective modified Backtracking Search Algorithm (MBSA) framework. MBSA employs an effective population initialization strategy, dynamic mutation and crossover operators , and a transfer criterion. MCS evaluates’ promising’ solutions generated by MBSA to guide the search process. Extensive experiments are performed considering three FJSSP benchmark data sets. At first, the best parameters for the MBSA are identified using the Taguchi method . Then, for the deterministic variant of the problem, MBSA is compared with two other metaheuristics to demonstrate its effectiveness. Finally, the stochastic variant is investigated considering two factors: variation level and the probability distribution of the processing times. Computational results statistically show a significant effect of these factors on the objectives, demonstrating the importance of selecting an appropriate probability distribution in an uncertain environment.},
  archive      = {J_ASOC},
  author       = {A. Gnanavelbabu and Rylan H. Caldeira and T. Vaidyanathan},
  doi          = {10.1016/j.asoc.2021.107960},
  journal      = {Applied Soft Computing},
  pages        = {107960},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A simulation-based modified backtracking search algorithm for multi-objective stochastic flexible job shop scheduling problem with worker flexibility},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning automata based particle swarm optimization for
solving class imbalance problem. <em>ASOC</em>, <em>113</em>, 107959.
(<a href="https://doi.org/10.1016/j.asoc.2021.107959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is an important problem in many domains such as disease classification, network intrusion detection , fraud detection, and spam filtering . While dealing with imbalanced datasets, traditional supervised machine learning algorithms do not often provide acceptable results. Several approaches are used to handle the class imbalance problem . Of these, undersampling approaches are mostly followed by the researchers in which the number of instances in the majority class gets reduced. Selection of instances from the majority class can be considered as an optimization problem . To this end, in this paper, we present an undersampling approach based on widely-used Particle Swarm Optimization (PSO). The majority class samples are first clustered to form the initial undersampled set. The samples to be selected are then optimized using PSO to give the best model. The parameters of PSO are fine tuned using Learning Automata . Appropriate metrics suitable for class imbalance problems have been used to construct the fitness function for optimizing the undersampled training set. The proposed method has achieved 2\% to 10\% performance improvement over most of the contemporary methods on various datasets with imbalance ratios ranging from 5 to 130, thus showing that the method is robust and useful in practical scenarios. The code of the proposed method can be accessed via https://github.com/kkg1999/Undersampling .},
  archive      = {J_ASOC},
  author       = {Anuran Chakraborty and Kushal Kanti Ghosh and Rajonya De and Erik Cuevas and Ram Sarkar},
  doi          = {10.1016/j.asoc.2021.107959},
  journal      = {Applied Soft Computing},
  pages        = {107959},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning automata based particle swarm optimization for solving class imbalance problem},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature-guided dictionary learning for patch-and-group
sparse representations in single image deraining. <em>ASOC</em>,
<em>113</em>, 107958. (<a
href="https://doi.org/10.1016/j.asoc.2021.107958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image deraining as a fundamental task in computer vision is important in improving the visual quality of images and videos. In this paper, we propose a feature-guided dictionary learning method for patch-and-group sparse representation in single image deraining. Specifically, we develop an external dictionary that learns the generic feature representation from a lot of natural images using Gaussian mixture models (GMMs), and a novel strategy is designed to learn an internal dictionary using the given rainy image guiding by the generic features. The external dictionary and the internal dictionary are adaptively tailored to produce a coherent dictionary, which can extend the representation abilities of the learned dictionary for the group-based sparse representation . Moreover, we present a novel patch-and-group sparse representation framework to reconstruct an image tending to be free of rain by simultaneously combining the local sparsity and the nonlocal self-similarity property. This framework can integrate their advantages of the two representative methods capable of capturing more effective features for the single image deraining. The results of experiments on both the synthetic and the real-world rainy images demonstrate that the proposed method delivers more favorable visual effects and superior quality results, and it outperforms several other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Ting Liu and Hongzhong Tang and Dongbo Zhang and Shuying Zeng and Biao Luo and Zhaoyang Ai},
  doi          = {10.1016/j.asoc.2021.107958},
  journal      = {Applied Soft Computing},
  pages        = {107958},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature-guided dictionary learning for patch-and-group sparse representations in single image deraining},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring and evaluating the home health care scheduling
problem with simultaneous pick-up and delivery with time window using a
tabu search metaheuristic solution. <em>ASOC</em>, <em>113</em>, 107957.
(<a href="https://doi.org/10.1016/j.asoc.2021.107957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Home Health Care Scheduling Problem (HCSP) with Simultaneous Pick-up and Delivery and Time Window (HCSPSPDTW) consists of finding the optimal crew composition and the best routes to follow in order to satisfy some logistics actions such as the delivery of medical services from the hospital to patients, and the pickup of unused drugs and biological samples from patients to the hospital in a transportation network. The HCSPSPDTW is a bi-objective problem where the traveled time and distance should be minimized. In this paper, we present a hierarchical approach for solving the HCSPSPDTW: the inner optimization problem solved by the k-means++ clustering algorithm, asks for finding the closest visits in terms of distance to avoid long travels. The outer problem is a Vehicle Routing Problem (VRP) modeled by a mixed-integer mathematical model and solved using Tabu Search (TS) metaheuristic . In order to evaluate the performance of the proposed algorithm, this study used some modified test problems from Solomon’s benchmark. The obtained results show that the proposed approach was able to generate better solutions than similar algorithms while using less computation time. For instance, the proposed approach outperforms other algorithms by generating less routing cost for 83.93\% instances, less used vehicles for 98.73\% instances, and by spending less computational time for 98.7\% instances. The proposed HCSPSPDTW solving approach is the solver module of a decision support system that will help Home Health Care companies to make appropriate decisions on scheduling their daily plans which will lead to can lead to a significant reduction of transportation costs and improvement of staff satisfaction.},
  archive      = {J_ASOC},
  author       = {Marouene Chaieb and Dhekra Ben Sassi},
  doi          = {10.1016/j.asoc.2021.107957},
  journal      = {Applied Soft Computing},
  pages        = {107957},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Measuring and evaluating the home health care scheduling problem with simultaneous pick-up and delivery with time window using a tabu search metaheuristic solution},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recursive elimination–election algorithms for wrapper
feature selection. <em>ASOC</em>, <em>113</em>, 107956. (<a
href="https://doi.org/10.1016/j.asoc.2021.107956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For classification tasks in machine learning , this paper proposes a brand-new wrapper feature selection algorithm prototype named recursive elimination–election (REE), which is conceived in a simple but exquisite structure inspired by the recursion technique in computer science. Prevalent metaheuristic methods such as differential evolution (DE), particle swarm optimization (PSO), etc., from evolutionary computation (EC) and swarm intelligence (SI) communities have recently been widely applied to feature selection research, but suffer from severe drawbacks including but not limited to low-efficient binary representation transformation, poor population diversity, excessive control parameter adjustments and sophisticated mechanisms. Instead, REE is organically constructed with an ordinary subset representation of feature indexes, simple operators, getting rid of extra control parameters. Specifically, REE is assembled of two basic recursive sub-algorithms, i.e., recursive random bisection elimination (RRBE) and recursive greedy binary election (RGBE), which somewhat embody the idea of “divide-and-conquer”. By inspecting smaller and potential feature subsets in recursive ways, better subsets are returned automatically. A comprehensive experimental study was conducted on 14 UCI and ASU benchmark datasets with feature sizes ranging from dozens to thousands by using REE together with 6 state-of-the-art metaheuristic algorithms for comparison. The results show that the proposed REE has competitive search ability for feature selection problems, and it is especially prominent in handling high-dimensional datasets. Therefore, REE is promising to become a wrapper feature selection search paradigm with low solution cost and high efficiency.},
  archive      = {J_ASOC},
  author       = {Wei Liu and Jianyu Wang},
  doi          = {10.1016/j.asoc.2021.107956},
  journal      = {Applied Soft Computing},
  pages        = {107956},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recursive elimination–election algorithms for wrapper feature selection},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An entropy minimization based multilevel colour thresholding
technique for analysis of breast thermograms using equilibrium slime
mould algorithm. <em>ASOC</em>, <em>113</em>, 107955. (<a
href="https://doi.org/10.1016/j.asoc.2021.107955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the leading causes of death in women due to the abnormal growth of cells known as a tumour. Thermography is an imaging technique used to capture infrared radiation and generate a thermogram. The major advantages of thermography are contactless, radiation-free, painless, and real-time screening. The thermogram is used further in a pathological investigation. However, in the modern-day, machine intelligence technologies are used for the analysis of breast thermograms, which assist the expert in decision making. The machine intelligence technique requires a thresholding image during the preprocessing stage , which warrants an efficient thresholding method. Here, we investigate the multilevel thresholding objective function to minimize the information regarding the entropic dependence on various classes. A new Equilibrium Slime Mould Algorithm (ESMA) is proposed for colour image thresholding, an improvement of the Slime Mould Algorithm (SMA), by integrating the equilibrium practice in updating the slime mould positions from an equilibrium pool concept of Equilibrium Optimizer (EO). The ESMA performance is compared with well know optimization algorithms and ranked one based on Friedman’s mean rank, when evaluated using 53 test problems. Further, the ESMA is used for the development of an entropy minimization based multilevel colour thresholding method for the analysis of breast thermograms. It is applied in two sets of experiments using grey components and the RGB components of breast thermograms. The encouraging results on the thermogram image analysis are presented. Even more interesting results are seen while evaluating our proposal in terms of different metrics—the peak signal to noise ratio (PSNR), the feature similarity (FSIM), and the structure similarity (SSIM). Statistical a nalysis provided reveals the suitability of the technique for the analysis of breast thermograms. The method may assist the medical practitioners, as an additional tool. The ESMA may be useful for solving different optimization problems in the world of engineering.},
  archive      = {J_ASOC},
  author       = {Manoj Kumar Naik and Rutuparna Panda and Ajith Abraham},
  doi          = {10.1016/j.asoc.2021.107955},
  journal      = {Applied Soft Computing},
  pages        = {107955},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An entropy minimization based multilevel colour thresholding technique for analysis of breast thermograms using equilibrium slime mould algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiobjective bilevel evolutionary approach for off-grid
direction-of-arrival estimation. <em>ASOC</em>, <em>113</em>, 107954.
(<a href="https://doi.org/10.1016/j.asoc.2021.107954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The source number identification is an essential step in direction-of-arrival (DOA) estimation. Existing methods may provide a wrong source number due to modeling errors caused by relaxing sparse penalties, especially in impulsive noise. This paper proposes a novel idea of simultaneous source number identification and DOA estimation to address this issue. We formulate a multiobjective off-grid DOA estimation model to realize this idea, by which the source number can be automatically identified together with DOA estimation. In particular, the source number is correctly exploited by the l 0 l0 norm of impinging signals without relaxations, guaranteeing accuracy. We further design a multiobjective bilevel evolutionary algorithm to solve this model. The source number identification and sparse recovery are simultaneously optimized at the on-grid (lower) level. A forward search strategy is developed to further refine the grid at the off-grid (upper) level. This strategy does not need linear approximations and can eliminate the off-grid gap with low computational complexity . Simulation results demonstrate the outperformance of our method in terms of source number and root mean square error .},
  archive      = {J_ASOC},
  author       = {Bai Yan and Qi Zhao and Jin Zhang and J. Andrew Zhang and Xin Yao},
  doi          = {10.1016/j.asoc.2021.107954},
  journal      = {Applied Soft Computing},
  pages        = {107954},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective bilevel evolutionary approach for off-grid direction-of-arrival estimation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed energy resource placement considering hosting
capacity by combining teaching–learning-based and honey-bee-mating
optimisation algorithms. <em>ASOC</em>, <em>113</em>, 107953. (<a
href="https://doi.org/10.1016/j.asoc.2021.107953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An emerging challenge of most electric utilities is to expand hosting capacity constraints and maximise the interconnection of distributed energy resources (DER). DERs have proliferated in distribution systems in most jurisdictions, and this proliferation has not occurred in an orderly fashion. Reports of degradation in power quality and operations are common. With the continuing adoption of DERs, it is imperative to develop methods to unlock the potential of DERs in a way that they are used to support system needs, rather than degrading them. This concept’s advantages are varied in different power system scenarios based on the considered models for owning and operating DER. With this objective in mind, this paper proposes a new algorithm to identify the optimal locations for new DERs. The proposed method is a novel combination of teaching–learning-based optimisation (TLBO) and honey-bee-mating optimisation (HBMO) algorithms, and it capitalises on both techniques’ benefits. A major novelty of this method is that it fully accounts for current hosting capacity and all existing DERs and incorporates these parameters in the optimisation algorithm. The results reveal that adding fuzzy clustering to the multiobjective process improves the DER placement problem. The DERs considered in this optimisation problem are fuel cell units, and the objective function includes cost, losses, and voltage deviation . The proposed technique is adopted in the IEEE 70-bus-radial test system, and its performance is compared with those of other optimisation algorithms . The results reveal the proposed algorithm’s superiority in accuracy and computational speed when reaching the optimal solution. As a case in point, the calculation time of the proposed algorithm is about\%9 and\%22 faster than TLBO and HBMO, respectively.},
  archive      = {J_ASOC},
  author       = {Seyed Iman Taheri and Mauricio B.C. Salles and Alexandre B. Nassif},
  doi          = {10.1016/j.asoc.2021.107953},
  journal      = {Applied Soft Computing},
  pages        = {107953},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distributed energy resource placement considering hosting capacity by combining teaching–learning-based and honey-bee-mating optimisation algorithms},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bitcoin transaction strategy construction based on deep
reinforcement learning. <em>ASOC</em>, <em>113</em>, 107952. (<a
href="https://doi.org/10.1016/j.asoc.2021.107952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging cryptocurrency market has lately received great attention for asset allocation due to its decentralization uniqueness. However, its volatility and brand new trading mode has made it challenging to devising an acceptable automatically-generating strategy. This study proposes a framework for automatic high-frequency bitcoin transactions based on a deep reinforcement learning algorithm — proximal policy optimization (PPO). The framework creatively regards the transaction process as actions, returns as awards and prices as states to align with the idea of reinforcement learning . It compares advanced machine learning-based models for static price predictions including support vector machine (SVM), multi-layer perceptron (MLP), long short-term memory (LSTM), temporal convolutional network (TCN), and Transformer by applying them to the real-time bitcoin price and the experimental results demonstrate that LSTM outperforms. Then an automatically-generating transaction strategy is constructed building on PPO with LSTM as the basis to construct the policy. Extensive empirical studies validate that the proposed method perform superiorly to various common trading strategy benchmarks for a single financial product. The approach is able to trade bitcoins in a simulated environment with synchronous data and obtains a 31.67\% more return than that of the best benchmark, improving the benchmark by 12.75\%. The proposed framework can earn excess returns through both the period of volatility and surge, which opens the door to research on building a single cryptocurrency trading strategy based on deep learning . Visualizations of trading the process show how the model handles high-frequency transactions to provide inspiration and demonstrate that it can be expanded to other financial products.},
  archive      = {J_ASOC},
  author       = {Fengrui Liu and Yang Li (Ph.D.) and Baitong Li and Jiaxin Li and Huiyang Xie},
  doi          = {10.1016/j.asoc.2021.107952},
  journal      = {Applied Soft Computing},
  pages        = {107952},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bitcoin transaction strategy construction based on deep reinforcement learning},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Niche-based cooperative co-evolutionary ensemble neural
network for classification. <em>ASOC</em>, <em>113</em>, 107951. (<a
href="https://doi.org/10.1016/j.asoc.2021.107951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, artificial neural networks have been widely used for classification. It is important to optimize the weight parameters and topological structure of the neural network simultaneously. These two tasks are interdependent and should be solved at the same time to achieve a better result. However, existing works cannot balance the accuracy and diversity of neural networks very well. In this paper, a cooperative co-evolutionary algorithm is proposed to simultaneously evolve artificial neural network topology, neuron attributes, and connection weights. In the proposed algorithm, two effective strategies are proposed. First, the niche-based strategy is used in the evolutionary and cooperative process to refine the local search ability. In this way, a set of candidate networks with a higher level of output diversity is obtained. Second, a two-step comparison scheme is designed to acquire a compact ensemble network. Moreover, a fully connected weights matrix crossover scheme is used to avoid destroying the network structure. The proposed algorithm is tested on the benchmark classification problems in the UCI machine learning repository and compared with other state-of-the-art methods. The experimental results show that the proposed niche-based cooperative co-evolutionary ensemble neural network has a higher capability of generalization compared with other methods in six of nine kinds of classification problems. Furthermore, the proposed ensemble neural network has relatively low complexity.},
  archive      = {J_ASOC},
  author       = {Jing Liang and Guanlin Chen and Boyang Qu and Caitong Yue and Kunjie Yu and Kangjia Qiao},
  doi          = {10.1016/j.asoc.2021.107951},
  journal      = {Applied Soft Computing},
  pages        = {107951},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Niche-based cooperative co-evolutionary ensemble neural network for classification},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Road pollution estimation from vehicle tracking in
surveillance videos by deep convolutional neural networks.
<em>ASOC</em>, <em>113</em>, 107950. (<a
href="https://doi.org/10.1016/j.asoc.2021.107950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air quality and reduction of emissions in the transport sector are determinant factors in achieving a sustainable global climate. The monitoring of emissions in traffic routes can help to improve route planning and to design strategies that may make the pollution levels to be reduced. In this work, a method which detects the pollution levels of transport vehicles from the images of IP cameras by means of computer vision techniques and neural networks is proposed. Specifically, for each sequence of images, a homography is calculated to correct the camera perspective and determine the real distance for each pixel. Subsequently, the trajectory of each vehicle is computed by applying convolutional neural networks for object detection and tracking algorithms. Finally, the speed in each frame and the pollution emitted by each vehicle are determined. Experimental results on several datasets available in the literature support the feasibility and scalability of the system as an emission control strategy.},
  archive      = {J_ASOC},
  author       = {Jorge García-González and Miguel A. Molina-Cabello and Rafael M. Luque-Baena and Juan M. Ortiz-de-Lazcano-Lobato and Ezequiel López-Rubio},
  doi          = {10.1016/j.asoc.2021.107950},
  journal      = {Applied Soft Computing},
  pages        = {107950},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Road pollution estimation from vehicle tracking in surveillance videos by deep convolutional neural networks},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soft sensor of bath temperature in an electric arc furnace
based on a data-driven takagi–sugeno fuzzy model. <em>ASOC</em>,
<em>113</em>, 107949. (<a
href="https://doi.org/10.1016/j.asoc.2021.107949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric arc furnaces (EAFs) are intended for the recycling of steel scrap . One of the more important variables in the recycling process is the tapping temperature of the steel. Due to the nature of the process, continuous measurement of the melt temperature is complicated and requires sophisticated measuring equipment; therefore, for most EAFs, separate temperature samples are taken several times before the melt is tapped, to verify whether the melt temperature is within the prescribed range. The measurements are obtained using disposable probes; when measurement is performed, the furnace must be switched off, leading to increased tap-to-tap time, unnecessary energy losses , and consequently, lower efficiency. The following paper presents a novel approach to EAF bath temperature estimation using a fuzzy model soft sensor obtained using Gustafson–Kessel input data clustering and particle swarm optimization of model parameters. The model uses the first temperature measurement as an initial condition, and measurements of the necessary EAF inputs to estimate continuously the bath temperature throughout the refining stage of the recycling process. The results have shown that the prediction accuracy of the proposed model is very high and that it fulfils the required tolerance band. The model is intended for parallel implementation in the EAF process , with the aim of achieving fewer temperature measurements, shorter tap-to-tap times, and decreased energy losses. Furthermore, if information about bath temperature is accessible in a continuous manner, operators can adjust the control of the EAF to achieve optimal tapping temperature and thus higher EAF efficiency.},
  archive      = {J_ASOC},
  author       = {Aljaž Blažič and Igor Škrjanc and Vito Logar},
  doi          = {10.1016/j.asoc.2021.107949},
  journal      = {Applied Soft Computing},
  pages        = {107949},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft sensor of bath temperature in an electric arc furnace based on a data-driven Takagi–Sugeno fuzzy model},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust mean-risk portfolio optimization using machine
learning-based trade-off parameter. <em>ASOC</em>, <em>113</em>, 107948.
(<a href="https://doi.org/10.1016/j.asoc.2021.107948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conservatism is the notorious problem of the worst-case robust portfolio optimization, and this issue has raised broad discussion in academia. To this end, we propose the hybrid robust portfolio models under ellipsoidal uncertainty sets in this paper, where both the best-case and the worst-case counterparts are involved. In the suggested models, we introduce a trade-off parameter to adjust the portfolio optimism level. Machine learning algorithms including Long Short-Term Memory (LSTM) and eXtreme Gradient Boosting (XGBoost) are used to evaluate the potential market movements and provide forecasting information to generate the hyperparameter for modeling. Additionally, we develop a clustering-based algorithm for properly constructing joint ellipsoidal uncertainty sets to reduce conservatism further. In the modeling phase, we design the hybrid portfolios based on variance (HRMV) and value at risk (VaR) and prove the equivalent relationship between the hybrid robust mean-VaR model (HRMVaR) and the hybrid robust mean-CVaR (conditional value at risk) according to the existing research. The US 12 industry portfolio data set retrieved from Kenneth R. French is employed for the in-sample and out-of-sample numerical experiments. The experimental results demonstrate the effectiveness and robustness of the proposed portfolios, where HRMV models have better Sharpe ratios and Calmar ratios than the corresponding nominal mean–variance model, and HRMVaR models outperform the baseline VaR-based portfolios in terms of returns. Sensitivity analysis supports the superiority of the joint ellipsoidal uncertainty set U δ 2 Uδ2 , where the proposed portfolios constrained with U δ 2 Uδ2 show stable risk characteristics.},
  archive      = {J_ASOC},
  author       = {Liangyu Min and Jiawei Dong and Jiangwei Liu and Xiaomin Gong},
  doi          = {10.1016/j.asoc.2021.107948},
  journal      = {Applied Soft Computing},
  pages        = {107948},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust mean-risk portfolio optimization using machine learning-based trade-off parameter},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evolvable adversarial network with gradient penalty for
COVID-19 infection segmentation. <em>ASOC</em>, <em>113</em>, 107947.
(<a href="https://doi.org/10.1016/j.asoc.2021.107947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 infection segmentation has essential applications in determining the severity of a COVID-19 patient and can provide a necessary basis for doctors to adopt a treatment scheme. However, in clinical applications, infection segmentation is performed by human beings, which is time-consuming and generally introduces bias. In this paper, we developed a novel evolvable adversarial framework for COVID-19 infection segmentation. Three generator networks compose an evolutionary population to accommodate the current discriminator , i.e., generator networks evolved with different mutations instead of the single adversarial objective to provide sufficient gradient feedback. Compared with the existing work that enforces a Lipschitz constraint by weight clipping, which may lead to gradient exploding or vanishing, the proposed model also incorporates the gradient penalty into the network, penalizing the discriminator’s gradient norm input. Experiments on several COVID-19 CT scan datasets verified that the proposed method achieved superior effectiveness and stability for COVID-19 infection segmentation.},
  archive      = {J_ASOC},
  author       = {Juanjuan He and Qi Zhu and Kai Zhang and Piaoyao Yu and Jinshan Tang},
  doi          = {10.1016/j.asoc.2021.107947},
  journal      = {Applied Soft Computing},
  pages        = {107947},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolvable adversarial network with gradient penalty for COVID-19 infection segmentation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-layer nested heterogeneous ensemble learning
predictive method for COVID-19 mortality. <em>ASOC</em>, <em>113</em>,
107946. (<a href="https://doi.org/10.1016/j.asoc.2021.107946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 epidemic has had a great adverse impact on the world, having taken a heavy toll, killing hundreds of thousands of people. In order to help the world better combat COVID-19 and reduce its death toll, this study focuses on the COVID-19 mortality. First, using the multiple stepwise regression analysis method, the factors from eight aspects (economy, society, climate etc.) that may affect the mortality rates of COVID-19 in various countries is examined. In addition, a two-layer nested heterogeneous ensemble learning-based prediction method that combines linear regression (LR), support vector machine (SVM), and extreme learning machine (ELM) is developed to predict the development trends of COVID-19 mortality in various countries. Based on data from 79 countries, the experiment proves that age structure (proportion of the population over 70 years old) and medical resources (number of beds) are the main factors affecting the mortality of COVID-19 in each country. In addition, it is found that the number of nucleic acid tests and climatic factors are correlated with COVID-19 mortality. At the same time, when predicting COVID-19 mortality, the proposed heterogeneous ensemble learning-based prediction method shows better prediction ability than state-of-the-art machine learning methods such as LR, SVM, ELM, random forest (RF), long short-term memory (LSTM) etc.},
  archive      = {J_ASOC},
  author       = {Shaoze Cui and Yanzhang Wang and Dujuan Wang and Qian Sai and Ziheng Huang and T.C.E. Cheng},
  doi          = {10.1016/j.asoc.2021.107946},
  journal      = {Applied Soft Computing},
  pages        = {107946},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-layer nested heterogeneous ensemble learning predictive method for COVID-19 mortality},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning ensemble approach to prioritize antiviral
drugs against novel coronavirus SARS-CoV-2 for COVID-19 drug
repurposing. <em>ASOC</em>, <em>113</em>, 107945. (<a
href="https://doi.org/10.1016/j.asoc.2021.107945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alarming pandemic situation of Coronavirus infectious disease COVID-19, caused by the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2), has become a critical threat to public health. The unexpected outbreak and unrealistic progression of COVID-19 have generated an utmost need to realize promising therapeutic strategies to fight the pandemic. Drug repurposing-an efficient drug discovery technique from approved drugs is an emerging tactic to face the immediate global challenge. It​ offers a time-efficient and cost-effective way to find potential therapeutic agents for the disease. Artificial Intelligence-empowered deep learning models enable the rapid identification of potentially repurposable drug candidates against diseases. This study presents a deep learning ensemble model to prioritize clinically validated anti-viral drugs for their potential efficacy against SARS-CoV-2. The method integrates the similarities of drug chemical structures and virus genome sequences to generate feature vectors. The best combination of features is retrieved by the convolutional neural network in a deep learning manner. The extracted deep features are classified by the extreme gradient boosting classifier to infer potential virus–drug associations. The method could achieve an AUC of 0.8897 with 0.8571 prediction accuracy and 0.8394 sensitivity under the fivefold cross-validation. The experimental results and case studies demonstrate the suggested deep learning ensemble system yields competitive results compared with the state-of-the-art approaches. The top-ranked drugs are released for further wet-lab researches.},
  archive      = {J_ASOC},
  author       = {Deepthi K. and Jereesh A.S. and Yuansheng Liu},
  doi          = {10.1016/j.asoc.2021.107945},
  journal      = {Applied Soft Computing},
  pages        = {107945},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep learning ensemble approach to prioritize antiviral drugs against novel coronavirus SARS-CoV-2 for COVID-19 drug repurposing},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic construction of accurate bioacoustics workflows
under time constraints using a surrogate model. <em>ASOC</em>,
<em>113</em>, 107944. (<a
href="https://doi.org/10.1016/j.asoc.2021.107944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated bioacoustics analysis is being increasingly used to describe environmental phenomena such as species abundance and biodiversity. Within this research area, many algorithms have been proposed. These achieve different sub-objectives within bioacoustics processes and can be combined to form workflows. However, these algorithms are typically evaluated in a limited number of scenarios and are rarely evaluated with different combinations of other tasks. This can result in workflows that are not well optimised to serve a given scenario, particularly under resource and time constraints, which ultimately leads to inaccurate bioacoustics analyses. This work examines the problem of bioacoustics workflow construction by searching and ordering combinations of tasks to determine which produce the most accurate output while remaining under user-defined time constraints. Workflow construction is investigated within a scenario where species need to be classified within synthetically generated soundscapes with different numbers of species, noise levels, and densities of species. A search algorithm is created that applies Particle Swarm Optimisation (PSO) to a neural network-based surrogate model . This algorithm is used to efficiently search for candidate workflow structures. This is compared to a random search, a genetic algorithm , and a PSO algorithm without the surrogate model , as well as existing workflows based on previous research. It is found that for all scenarios, the surrogate model-based search method can quickly find effective workflows in a low number of searches. Furthermore, it is found that workflow effectiveness varies depending on the scenarios and recordings used.},
  archive      = {J_ASOC},
  author       = {Alexander Brown and James Montgomery and Saurabh Garg},
  doi          = {10.1016/j.asoc.2021.107944},
  journal      = {Applied Soft Computing},
  pages        = {107944},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic construction of accurate bioacoustics workflows under time constraints using a surrogate model},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pareto based ant lion optimizer for energy efficient
scheduling in cloud environment. <em>ASOC</em>, <em>113</em>, 107943.
(<a href="https://doi.org/10.1016/j.asoc.2021.107943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tremendous growth of cloud-based data centres leads to significant amount of energy. Thus, the prime concern for the cloud service providers is to generate environment friendly solution by minimizing the energy consumption. Further, due to increased demand of scientific workflow applications , cloud providers face a challenging issue of efficient workflow scheduling by minimizing the makespan. To address the above-mentioned contradictory issues, we proposed a Pareto based multi-objective discrete ant lion optimization algorithm (PBMO-DALO) to solve workflow-scheduling problem in cloud data centres with minimizing the conflicting objectives of makespan and energy consumption simultaneously. Distinguished from the original ant lion optimization algorithm , the proposed algorithm involves new encoding scheme for ants and antlions, their random walk and selection of fitter antlion for trap building to address the discrete nature of the workflow scheduling problem. Subsequently, the PBMO-DALO uses the Pareto dominance and crowding distance approach to tackle optimization of multiple objectives and to achieve the optimal solutions. The simulation results indicate that the proposed PBMO-DALO algorithm overwhelms other competing algorithms and generates good trade-off solutions with better convergence and uniform diversity.},
  archive      = {J_ASOC},
  author       = {Rama Rani and Ritu Garg},
  doi          = {10.1016/j.asoc.2021.107943},
  journal      = {Applied Soft Computing},
  pages        = {107943},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pareto based ant lion optimizer for energy efficient scheduling in cloud environment},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dual opposition-based learning for differential evolution
with protective mechanism for engineering optimization problems.
<em>ASOC</em>, <em>113</em>, 107942. (<a
href="https://doi.org/10.1016/j.asoc.2021.107942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opposition-based learning (OBL), which plays an important role in soft computing, has recently drawn attention. The paramount challenge of OBL is to design and find an OBL strategy that is suitable for the problem structure. Besides, for the OBL variants proposed so far, there is no clear taxonomy guideline. To solve these issues, this paper proposes a novel opposition, called dual opposition-based learning (DOBL), which contains two opposition strategies and a protective mechanism. Firstly, a diversity-based taxonomy is proposed, which categorizes existing state-of-the-art OBL variants according to dimension-wise diversity. The subpopulation strategy is used and embedded in the classified OBL variants to generate explorative opposition and exploitative opposition. Secondly, for a successful algorithm, a good ratio between exploration and exploitation is required. Therefore, a protective mechanism is designed to obtain a good equilibrium between exploration and exploitation. Finally, the performance of DOBL is compared with eight state-of-the-art OBL variants on DE and advanced DE named jSO to find the CEC 2017 test suite’s best solution. Besides, DOBL is applied to CEC 2011 as well as CEC 2020 real-world optimization problems , and compared with nine novel metaheuristic algorithms as well as the top three algorithms in CEC 2020, respectively. Two statistical tests, the Wilcoxon rank-sum test and the Friedman test, are used to analyze the experiment results. The experiment results of 29 functions and 60 real-world problems demonstrate that the proposed DOBL is better than its competitors on CEC2011, CEC 2017, and CEC 2020 test suites.},
  archive      = {J_ASOC},
  author       = {Jiahang Li and Yuelin Gao and Kaiguang Wang and Ying Sun},
  doi          = {10.1016/j.asoc.2021.107942},
  journal      = {Applied Soft Computing},
  pages        = {107942},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual opposition-based learning for differential evolution with protective mechanism for engineering optimization problems},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A regional pretraining-classification-selection forecasting
system for wind power point forecasting and interval forecasting.
<em>ASOC</em>, <em>113</em>, 107941. (<a
href="https://doi.org/10.1016/j.asoc.2021.107941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind power forecasting is extremely crucial for power market transactions and power system operation. Although a lot of researches have concentrated on wind power forecasting, the forecasting performances were confined and the forecasting models were only suitable for providing good performance at a few sites of investigation. To bridge these gaps, a novel regional pretraining-classification-selection wind power forecasting system is proposed in this paper based on four modules-pretraining module, classification module, point forecasting module, and interval forecasting module, which effectively improves forecasting performance and extends the applicability to different data characteristics. 10-min wind power data obtained from 20 datasets are used to verify the forecasting ability of the proposed forecasting system. The experimental analyses and discussions reveal that the proposed forecasting system is accurate and reliable for achieving high-quality wind power point and interval forecasting results. Thus, it could provide useful references for wind producers and managers in power system dispatch and operation.},
  archive      = {J_ASOC},
  author       = {Jianzhou Wang and Lifang Zhang and Chen Wang and Zhenkun Liu},
  doi          = {10.1016/j.asoc.2021.107941},
  journal      = {Applied Soft Computing},
  pages        = {107941},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A regional pretraining-classification-selection forecasting system for wind power point forecasting and interval forecasting},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). System for adaptive extraction of non-invasive fetal
electrocardiogram. <em>ASOC</em>, <em>113</em>, 107940. (<a
href="https://doi.org/10.1016/j.asoc.2021.107940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aimed to find the most suitable combination of adaptive and non-adaptive methods for extraction of non-invasive fetal electrocardiogram (NI-fECG) using signals recorded from the mother’s abdomen. Among the nine methods considered, the combination of independent component analysis (ICA), fast transversal filter (FTF), and complementary ensemble empirical mode decomposition with adaptive noise (CEEMDAN) proved to be the most effective for the extraction of fECG from abdominal recordings. This combined method was suitable due to both being effective in extracting fECG and being less computationally complex. Further, so far, FTF and CEEMDAN methods have not been extensively tested for fECG extraction, and in particular, have not been examined as a hybrid method . The ICA-FTF-CEEMDAN hybrid algorithm was tested on two patient databases: Fetal Electrocardiograms, Direct and Abdominal with Reference Heartbeats Annotations (FECGDARHA) and PhysioNet Challenge 2013. The evaluation of the accuracy of fQRS complexes detection was performed using the following parameters: accuracy (ACC), sensitivity (SE), positive predictive value (PPV), and F1 score. The fetal heart rate (fHR) determination accuracy was evaluated using Bland–Altman plots and fHR traces. When testing on the FECGDARHA database, average values of ACC = 92 . 98 =92.98\%, SE = 95 . 33 =95.33\%, PPV = 96 . 4 =96.4\% and F1 = 95 . 86 =95.86\% for detection fQRS were achieved. The error in estimating the fHR was − 1 . 02 ± 7 . 02 −1.02±7.02 ( μ ± 1 . 96 σ μ±1.96σ ) bpm. When testing on the Challenge 2013 database, average values of ACC = 78 . 47 =78.47\%, SE = 82 . 06 =82.06\%, PPV = 87 . 90 =87.90\% and F1 = 84 . 62 =84.62\% for fQRS detection were achieved, and the error in estimating the fHR was − 6 . 62 ± 10 . 33 −6.62±10.33 ( μ ± 1 . 96 σ μ±1.96σ ) bpm. In addition, a non-invasive morphological analysis (ST analysis) was performed on the records from the FECGDARHA database, which was accurate in 7 of 12 records with values of μ μ&amp;lt; 0.03 and values of ± 1 . 96 σ ±1.96σ&amp;lt; 0.04 .},
  archive      = {J_ASOC},
  author       = {Katerina Barnova and Radek Martinek and Rene Jaros and Radana Kahankova and Khosrow Behbehani and Vaclav Snasel},
  doi          = {10.1016/j.asoc.2021.107940},
  journal      = {Applied Soft Computing},
  pages        = {107940},
  shortjournal = {Appl. Soft. Comput.},
  title        = {System for adaptive extraction of non-invasive fetal electrocardiogram},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gait based parkinson’s disease diagnosis and severity rating
using multi-class support vector machine. <em>ASOC</em>, <em>113</em>,
107939. (<a href="https://doi.org/10.1016/j.asoc.2021.107939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To diagnose Parkinson’s disease (PD) in a clinical setting, generally clinicians utilize several clinical manifestations such as motor impairments and non-motor symptoms and predict the severity level based on unified Parkinson’s disease rating scale (UPDRS). Such a clinical evaluation highly depends on the expertise and experience of the clinicians and results in variation in assessment among clinicians. Hence, to assist the clinicians to diagnose the PD and rate the severity level , we present a gait classification based decision support system using multi-class support vector machine (MCSVM). As the gait alterations are the initial manifestations of PD, we utilize the publicly available vertical ground reaction force (VGRF) dataset and perform the kinematic analysis to extract the spatiotemporal features. To identify the prominent gait biomarkers, this work utilizes a correlation based feature selection approach and employs multi-regression approach to normalize the gait time series data . Moreover, transforming the multi-class classification problem into multiple binary classification problem using one-versus-one (OVO) strategy, the proposed PD severity rating framework tests the performance of four SVM kernel functions for three different walking tests. Experimental results highlight that the quadratic SVM classifier offers an average accuracy of 98.65\% and outperforms several other state-of-the-art methods that utilized gait dataset for PD diagnosis.},
  archive      = {J_ASOC},
  author       = {B. Vidya and Sasikumar P},
  doi          = {10.1016/j.asoc.2021.107939},
  journal      = {Applied Soft Computing},
  pages        = {107939},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gait based parkinson’s disease diagnosis and severity rating using multi-class support vector machine},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparison of data selection methods for modeling chemical
processes with artificial neural networks. <em>ASOC</em>, <em>113</em>,
107938. (<a href="https://doi.org/10.1016/j.asoc.2021.107938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance selection aims at selecting model training data in a way that the performance of the trained models is maximized. In the context of modeling chemical processes by artificial neural networks , it can serve as an essential preprocessing step since measurement data of such processes are commonly highly clustered and thus far away from being ideally normally distributed. In this paper, four filter methods from literature and a newly proposed method for data selection are tested and combined with a convex hull data selection algorithm , which results in ten different selection approaches. These approaches are applied to five selected datasets by training feed-forward artificial neural networks with the produced split datasets. The final mean model deviation is used to quantify the algorithms’ performance and their standard deviation to provide information about their reproducibility. It is found that the convex hull extended algorithms self-organizing maps based stratified sampling with a proportional allocation rule and the newly proposed self-information-based subset selection perform best for real-world chemical engineering data.},
  archive      = {J_ASOC},
  author       = {Fabian Zapf and Thomas Wallek},
  doi          = {10.1016/j.asoc.2021.107938},
  journal      = {Applied Soft Computing},
  pages        = {107938},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comparison of data selection methods for modeling chemical processes with artificial neural networks},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive multi-objective particle swarm optimization with
multi-strategy based on energy conversion and explosive mutation.
<em>ASOC</em>, <em>113</em>, 107937. (<a
href="https://doi.org/10.1016/j.asoc.2021.107937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convergence and diversity are crucial in designing a multi-objective optimization algorithm, which are related to whether an accurate and well-distributed Pareto front can be obtained. Although the multi-objective particle swarm optimization (MOPSO) has achieved successful development, there are still some innovative and desirable schemes to further improve its performance. An adaptive MOPSO with multi-strategy based on energy conversion and explosive mutation (ecemAMOPSO) is proposed for solving multi-objective optimization problems. Dissipative energy of particles, defined from the perspective of force analysis and energy conversion, can be used as the feedback information to detect the evolutionary environment and formulate an adaptive strategy. The population is divided into three classes and the particles in different classes are optimized by the customized strategies. A novel mechanism inspired by the explosion of fireworks is proposed to design a multi-strategy mutation operator . Moreover, the particles is equipped with memory interval to select the personal best, and fusion index is formed to maintain the external archive . Experimental studies are conducted on ZDT, DTLZ, and WFG benchmark suits and several state of the art algorithms are employed as competitors. Experimental results show that the proposed algorithm is highly competitive in multi-aspect.},
  archive      = {J_ASOC},
  author       = {Weimin Huang and Wei Zhang},
  doi          = {10.1016/j.asoc.2021.107937},
  journal      = {Applied Soft Computing},
  pages        = {107937},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive multi-objective particle swarm optimization with multi-strategy based on energy conversion and explosive mutation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new approach to online training for the fuzzy ARTMAP
artificial neural network. <em>ASOC</em>, <em>113</em>, 107936. (<a
href="https://doi.org/10.1016/j.asoc.2021.107936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of internet resources has led to an increase in the flow of data and, consequently, the need for classification or forecasting models that support online learning. The Fuzzy ARTMAP neural network has been used in the most areas of knowledge; however, few have explored real-time applications that require continuous training. In this work, a Fuzzy ARTMAP neural network with continuous training is proposed. This new network can acquire knowledge via classification or prediction. Modifications made to the architecture and learning algorithm enable online learning from the first sample of data and perform the classification or forecasting at any time during training. To validate the proposed model, three experiments were performed, one for forecasting and two for classification. Each experiment used benchmark databases and compared its final results with the results of the original Fuzzy ARTMAP neural network. The results demonstrate the ability of the proposed model to acquire knowledge from the first data samples in a stable and efficient way. Thus, this study contributes to the evolution of the Fuzzy ARTMAP neural network and introduces the continuous training method as an effective alternative to real-time applications.},
  archive      = {J_ASOC},
  author       = {Carlos R. Santos-Junior and Thays Abreu and Mara L.M. Lopes and Anna D.P. Lotufo},
  doi          = {10.1016/j.asoc.2021.107936},
  journal      = {Applied Soft Computing},
  pages        = {107936},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new approach to online training for the fuzzy ARTMAP artificial neural network},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of autonomous vehicle driving systems for risk
assessment based on three-dimensional uncertain linguistic variables.
<em>ASOC</em>, <em>113</em>, 107934. (<a
href="https://doi.org/10.1016/j.asoc.2021.107934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous vehicle driving systems (AVDSs) have independent decision structures from the users to manage and control the operations of the vehicles both in normal conditions and unexpected situations. Although there are some advantages, such as decreasing accidents reasoned by human errors and efficient energy usage, it is obvious that some risks are rooted in this technology usage . Therefore, it will be beneficial to realize a risk assessment application for autonomous vehicles (AVs) and/or driving systems (DSs) since whose risks are crucial to test and solve. In this paper, a multi-criteria decision making (MCDM) methodology integrating DEcision MAking Trial and Evaluation Laboratory (DEMATEL), Analytical Network Process (ANP), and VlseKriterijuska Optimizacija I Komoromisno Resenje (VIKOR) techniques under spherical fuzzy environment have been suggested to evaluate AVDS alternatives in terms of considered risk criteria. Spherical fuzzy sets (SFS), which are the extension of the ordinary fuzzy sets, have been used to consider the hesitancy of experts and decision-makers as well as uncertainty and impreciseness in the available data. In the application, six AVDS alternatives have been evaluated in terms of seven main criteria and forty sub-criteria. The factors “Software Specifications” and “Reliability” have been determined as the most important main and sub-criteria with the weights 0.193 and 0.066, respectively. Additionally, comparative and sensitivity analyses have been applied to present flexibility, validation and verification of the proposed methodology together with the sensitivity of the given decisions. Based on the application results and conducted analyses, possible implications by views of theoretical, managerial, and policy context have been discussed.},
  archive      = {J_ASOC},
  author       = {Melike Erdoğan and İhsan Kaya and Ali Karaşan and Murat Çolak},
  doi          = {10.1016/j.asoc.2021.107934},
  journal      = {Applied Soft Computing},
  pages        = {107934},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of autonomous vehicle driving systems for risk assessment based on three-dimensional uncertain linguistic variables},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy least squares projection twin support vector machines
for class imbalance learning. <em>ASOC</em>, <em>113</em>, 107933. (<a
href="https://doi.org/10.1016/j.asoc.2021.107933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel fuzzy least squares projection twin support vector machines for class imbalance learning (FLSPTSVM-CIL). Unlike twin support vector machine (TSVM) which solves two dual problems, we solve two modified primal formulations by solving two systems of linear equations . The proposed FLSPTSVM-CIL model seeks two projection directions such that the samples of two classes are well separated in the projected space. To avoid the singularity issues, we incorporate an extra regularization term to make the optimization problem positive definite. As the real world data may be imbalanced, we assign appropriate fuzzy weights to the samples such that the classifier is not biased towards the samples of the majority class. The statistical analysis and experimental results on the publicly available UCI benchmark datasets show that the proposed FLSPTSVM-CIL performs better as compared to the baseline models . To show the applications of the proposed FLSPTSVM-CIL model on real world datasets, we performed classification of Alzheimer’s disease and breast cancer patients . Experimental results show that the generalization performance of the proposed FLSPTSVM-CIL model for the classification of the breast cancer patients and the mild cognitive impairment versus Alzheimer’s disease subjects is better as compared to the baseline models .},
  archive      = {J_ASOC},
  author       = {M.A. Ganaie and M. Tanveer and for the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.asoc.2021.107933},
  journal      = {Applied Soft Computing},
  pages        = {107933},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy least squares projection twin support vector machines for class imbalance learning},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating long-term impacts of tunnel infrastructure
development on urban sustainability using granular computing.
<em>ASOC</em>, <em>113</em>, 107932. (<a
href="https://doi.org/10.1016/j.asoc.2021.107932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of metro construction causes adverse effects and even threats; thus, the sustainability assessment becomes extremely important but complex. A novel framework integrating System Dynamics (SD) and Granular Computing (GrC) is proposed to estimate the long-term impacts of tunnel infrastructure development on urban sustainability. An integrated system , consisting of social, economic, and environmental sub-systems, is developed to model urban sustainability. A comprehensive method that integrates Analytic Hierarchy Process (AHP) and entropy weighting methods is proposed to provide a more reliable and objective way to sort information and identify the weight of each criterion. The SD model is established to interpret the relationships of the sub-systems and predict future development. The GrC approach is employed to model and reveal the uncertainties in the SD model and improve reliability. A project case is selected to verify the proposed approach. The results show that the economic system (S2) has better performance than the other two systems, namely social (S1) and environmental systems (S3), which indicates the development of metro lines can boost economic growth. The long-term impact of tunnel infrastructure development on urban sustainability fluctuates over time. The developed approach can be used as a decision tool to assess and estimate the long-term impacts of tunnel infrastructure development on urban sustainability with the consideration of uncertainties in factor measurement and provide decision support for enhancing sustainable urban development .},
  archive      = {J_ASOC},
  author       = {Limao Zhang and Yan Zhang and Hong Xian Li and Zhen Lei},
  doi          = {10.1016/j.asoc.2021.107932},
  journal      = {Applied Soft Computing},
  pages        = {107932},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Estimating long-term impacts of tunnel infrastructure development on urban sustainability using granular computing},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A principal component analysis dominance mechanism based
many-objective scheduling optimization. <em>ASOC</em>, <em>113</em>,
107931. (<a href="https://doi.org/10.1016/j.asoc.2021.107931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many-objective scheduling optimization problems (MOSOPs) are NP hard problems. It is a great challenge for an algorithm to solve them. In order to solve a many-objective scheduling optimization model aiming at minimizing total production cost, makespan, earliness and tardiness penalty and carbon emissions in manufacturing processes , algorithms with high performances on convergences and diversities are needed. Currently most algorithms might suffer insufficient selection pressures and cause low search efficiency for MOSOPs. As a dominance mechanism based on principal component analysis has shown good performances on reducing dimensionalities of a data set with a large number of interrelated variables and sorting non-dominated individuals, a generally frame of algorithms based on a principal component analysis dominance mechanism has been proposed. Performances of the principal component analysis based algorithm is verified by benchmark problems. Results show that the proposed principal component analysis based algorithm outperforms other algorithms in both convergences and diversities.},
  archive      = {J_ASOC},
  author       = {Qiong Liu and Zhui Gui and Shuping Xiong and Mengmeng Zhan},
  doi          = {10.1016/j.asoc.2021.107931},
  journal      = {Applied Soft Computing},
  pages        = {107931},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A principal component analysis dominance mechanism based many-objective scheduling optimization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification of facial expression using a multiple
impression feedback recognition model. <em>ASOC</em>, <em>113</em>,
107930. (<a href="https://doi.org/10.1016/j.asoc.2021.107930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) plays a vital role in the automatic detection of human emotions with intelligent machine. Since the FER is an interdisciplinary technique involving biology, computer science and even psychology, more challenges will be encountered as we pursue a high recognition accuracy of facial expressions. Inspired by the progressive enhancing procedure of face recognition of human, we proposed a multiple impression feedback recognition model (MIFR) for the identification of facial expression. Different from current deep learning techniques, the MIFR realizes a quick facial expression recognition through cascade feedback recognition cycles. Multiple impression features of an FER image are firstly obtained by the discrete wavelet decomposition (DWT). Each recognition cycle is implemented by inputting wavelet impression features in a specific decomposition scale into a classifier set of parallel Support Vector Machines (SVMs). In terms of coarse-to-fine wavelet features on multiple scales, the recognition results are gradually improved through integrating classification probability vectors of multiple cycles. In order to validate the performance of the MIFR_SVM for face expression recognition, we also conducted an experiment of facial multi-view expression with occlusion ( FMEO ) in the laboratory. Three traditional schemes and seven deep learning schemes have been chosen for the FER comparison of three public datasets and one lab dataset FMEO . The classification results show that the MIFR_SVM is not only superior than traditional schemes, but also performs better than deep learning techniques, including both neural-network-based ones and random-forest-based ones. The average recognition accuracy of MIFR_SVM reaches to 92.31\% for four datasets. Furthermore, the MIFR_SVM has fewer parameters, less complexity and adaptable cascade structure, which is more suitable for the image datasets with small size or with diverse qualities.},
  archive      = {J_ASOC},
  author       = {Hong He and Shuda Chen},
  doi          = {10.1016/j.asoc.2021.107930},
  journal      = {Applied Soft Computing},
  pages        = {107930},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identification of facial expression using a multiple impression feedback recognition model},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Twin k-class support vector classification with pinball
loss. <em>ASOC</em>, <em>113</em>, 107929. (<a
href="https://doi.org/10.1016/j.asoc.2021.107929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The twin K K -class support vector classification (TKSVC) is an effective and efficient algorithm for multi-class classification problem. However, due to the use of hinge loss function , it is very sensitive to feature noises and not stable for re-sampling. To avoid the above flaws, we present a novel twin K K -class support vector classification with pinball loss (Pin-TKSVC) in this paper. The structure of Pin-TKSVC is analogous to that of TKSVC. Namely, it adopts “One-vs.-One-vs.-Rest” structure to do multi-class classification and solves two smaller sized quadratic programming problems to save the running time. Besides, the introduction of pinball loss does not increase the additional computational complexity . More importantly, the Pin-TKSVC maximizes the quantile distance between different categories, making it a more robust classifier. To testify its performance, we do numerical experiments on twenty datasets with different noises, and compare it with five state-of-the-art algorithms. The experimental results confirm the validity of our algorithm.},
  archive      = {J_ASOC},
  author       = {Huiru Wang and Qing Zhang},
  doi          = {10.1016/j.asoc.2021.107929},
  journal      = {Applied Soft Computing},
  pages        = {107929},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Twin K-class support vector classification with pinball loss},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ramp sparse support matrix machine and its application in
roller bearing fault diagnosis. <em>ASOC</em>, <em>113</em>, 107928. (<a
href="https://doi.org/10.1016/j.asoc.2021.107928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an efficient matrix classifier, support matrix machine (SMM) can make full use of the spatial structure of the input matrix and show superior diagnostic performance. However, the input feature matrix may be contaminated by noise to form some outliers, which will affect the classification accuracy due to excessive loss. Therefore, this paper proposes a new matrix classification method, called Ramp sparse support matrix machine (RSSMM). In RSSMM, it compulsorily limits a loss threshold under the Ramp loss function, which solves the problem of model generalization performance degradation caused by excessive loss. Meanwhile, the generalized forward–backward algorithm (GFB) is introduced into RSSMM as a solver, and a generalized smooth Ramp loss function is designed to solve the problem that the Ramp loss function itself does not have a continuous gradient. Two roller bearing fault data sets are used to prove the effectiveness of the RSSMM method, and the analysis results show the superiority of the proposed RSSMM method in the classification of roller bearing fault signal.},
  archive      = {J_ASOC},
  author       = {Mingen Gu and Jinde Zheng and Haiyang Pan and Jinyu Tong},
  doi          = {10.1016/j.asoc.2021.107928},
  journal      = {Applied Soft Computing},
  pages        = {107928},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ramp sparse support matrix machine and its application in roller bearing fault diagnosis},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new type-2 fuzzy multi-criteria hybrid method for rail
transit operation safety assessment. <em>ASOC</em>, <em>113</em>,
107927. (<a href="https://doi.org/10.1016/j.asoc.2021.107927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rail transit plays a very important role in the social and economic development of big cities. Its operational safety is of great significance for ensuring people’s lives and maintaining social stability. In this paper, the rail transit operation safety management framework is proposed and analyzed by considering the factors of organizational management, personnel management, equipment and facilities and operating environment together. Considering the fuzzy uncertainty in the process of the safety assessment of rail transit operation (SARTO) and the interrelationship and conflict among the criteria, a new hybrid model combining DEMATEL, ANP and VIKOR with type-2 fuzzy linguistic variables is proposed. The model can present the effect-cause relationship among the different main criteria by the type-2 fuzzy DEMATEL, obtain the weights of the sub-criteria by the type-2 fuzzy ANP and the risk assessment value of each metro line by the type-2 fuzzy VIKOR and give further suggestions for rail transit operation safety management. This novel hybrid methodology is used to evaluate operation safety of metro lines in Beijing, and the sensitivity analysis is also applied to analyze the robustness. For the accuracy and consistency with the actual situation check, the proposed type-2 fuzzy VIKOR method is compared with both TOPSIS method and MOORA method in interval type-2 fuzzy sets. The application of Beijing metro lines proves the feasibility and the practicality of the new hybrid method , which provides effective support for decision-makers in practice.},
  archive      = {J_ASOC},
  author       = {Shilian Han and Wei Wang and Xinwang Liu},
  doi          = {10.1016/j.asoc.2021.107927},
  journal      = {Applied Soft Computing},
  pages        = {107927},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new type-2 fuzzy multi-criteria hybrid method for rail transit operation safety assessment},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social class particle swarm optimization for variable-length
wireless sensor network deployment. <em>ASOC</em>, <em>113</em>, 107926.
(<a href="https://doi.org/10.1016/j.asoc.2021.107926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Network Deployment (WSND) is an active research topic. This mechanism involves optimal placement of a wireless sensor network in a 2D environment for optimizing a set of metrics, such as coverage and cost. The topic of WSND has two challenging parts. First, it has a Multiobjective Optimization (MOO) nature instead of a single optimal solution due to the existing set of nondominated solutions . Second, its Variable length (V-length) decision space obtains nonhomogeneous solutions in terms of length. These challenging concepts cause traditional MOO algorithms to become insufficient to solve WSND; thus, developing an MOO algorithm with a V-length nature is required. In this study, Social Class Multiobjective Particle Swarm Optimization (SC-MOPSO) was developed for solving difficult optimization problems with MOO and V-length nature. The algorithm extends the concept of social interaction of Particle Swarm Optimization by decomposing the solution space into classes on the basis of their dimension. Furthermore, it incorporates intra and inter class operators for assuring the required dynamics of solution changes to reach the Pareto front . A set of mathematical optimization problems with two and three objectives based on different dimensions of mathematical functions was tested for evaluation. In addition, SC-MOPSO and the benchmarks were evaluated for accomplishing WSND. Experimental results show that SC-MOPSO outperforms all benchmarks in terms of domination for WSND with maximum percentage of 100\% for Weighted Sum Variable Length Particle Swarm Optimization (WS-VLPSO) and minimum percentage of 68\% for Nondominated Sorting Genetic Algorithm (NSGA-II).},
  archive      = {J_ASOC},
  author       = {Ahmed Mahdi Jubair and Rosilah Hassan and Azana Hafizah Mohd Aman and Hasimi Sallehudin},
  doi          = {10.1016/j.asoc.2021.107926},
  journal      = {Applied Soft Computing},
  pages        = {107926},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Social class particle swarm optimization for variable-length wireless sensor network deployment},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Potential bias when creating a differential-vector movement
algorithm. <em>ASOC</em>, <em>113</em>, 107925. (<a
href="https://doi.org/10.1016/j.asoc.2021.107925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating natural biological processes has been used widely to create new algorithms to optimize complex problems. Hundreds of new algorithms have been proposed in this growing area of research, with many of these using differential-vector movement to generate new solutions. However, the relationship between the natural inspiration for an algorithm and its algorithmic behavior is sometimes tenuous. Unfortunately, high levels of solution quality and speed on narrowly defined benchmarks are often prioritized over general theoretical understanding. Several algorithms, including teaching–learning-based optimization, symbiotic organisms search, sine cosine algorithm, forensic-based investigation optimization, and grey wolf optimizer were examined to explore the search regions achieved by their suggested strategies. Interestingly, all of the five algorithms were found to frequently consider the origin of coordinates when generating new solutions. Accordingly, these algorithms sometimes obtained extraordinary results when applied to benchmark functions where the optimum solutions reside at the origin. The results highlight the importance of properly checking how and from which regions points are sampled when developing new metaheuristics . Furthermore, providing clear descriptions of the underlying designs of strategies and parameter settings is encouraged.},
  archive      = {J_ASOC},
  author       = {Hsing-Chih Tsai},
  doi          = {10.1016/j.asoc.2021.107925},
  journal      = {Applied Soft Computing},
  pages        = {107925},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Potential bias when creating a differential-vector movement algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Customer segmentation using k-means clustering and the
adaptive particle swarm optimization algorithm. <em>ASOC</em>,
<em>113</em>, 107924. (<a
href="https://doi.org/10.1016/j.asoc.2021.107924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The improvement of enterprise competitiveness depends on the ability to match segmented customers in a competitive market. In this study, we propose a customer segmentation method based on the improved K-means algorithm and the adaptive particle swarm optimization (PSO) algorithm. The current PSO algorithm can easily fall into a local extremum; thus, adaptive learning PSO (ALPSO) is proposed to improve the optimization accuracy. On the basis of the analysis of population-based optimization, the inertia weight, learning factors, and the position update method are redesigned. To prevent the K-means clustering algorithm from depending on initial cluster centres, the ALPSO algorithm is used to optimize the K-means cluster centres (KM-ALPSO). Aimed at the issue of clustering the actual grape-customer consumption mixed dataset, factor analysis is used to extract numerical variables. We then propose a dissimilarity measurement method to cluster the mixed data. We compare ALPSO with several parameter update methods. We also conduct comparative experiments to compare KM-ALPSO on five UCI datasets. Finally, the improved KM-ALPSO (IKM-ALPSO) clustering algorithm is applied in customer segmentation . All results show that the three proposed methods outperform existing models. The experimental results also demonstrate the effectiveness and practicability of IKM-ALPSO for customer segmentation.},
  archive      = {J_ASOC},
  author       = {Yue Li and Xiaoquan Chu and Dong Tian and Jianying Feng and Weisong Mu},
  doi          = {10.1016/j.asoc.2021.107924},
  journal      = {Applied Soft Computing},
  pages        = {107924},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Customer segmentation using K-means clustering and the adaptive particle swarm optimization algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing sustainability of supply chains by fuzzy malmquist
network data envelopment analysis: Incorporating double frontier and
common set of weights. <em>ASOC</em>, <em>113</em>, 107923. (<a
href="https://doi.org/10.1016/j.asoc.2021.107923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable supply chain management (SSCM) is one of the most significant topics. Today, assessing sustainability and choosing the best supply chain is an important goal for companies. One of the techniques that can be used for evaluating the sustainability of supply chains is data envelopment analysis (DEA). This study contributes to the knowledge of sustainability assessment by (i) proposing a double frontier network DEA (NDEA) model with common set of weights (CSW), (ii) dealing with fuzzy data to assess sustainability, and (iii) deriving a CSW model for Malmquist productivity index (MPI). The CSW models consider the same set of weights for all decision making units (DMUs) so that all DMUs get the highest sustainability score. The fuzzy MPI (FMPI) is used to assess the sustainability of supply chains with the trapezoidal fuzzy number. The FMPI reflects the productivity change over time. The proposed model can assess the sustainability of supply chains and takes into account different confidence levels in two periods and can fully rank DMUs. Our model can measure the sustainability of supply chains via fuzzy MPI. Using our proposed approach, the decision-makers can determine the optimistic, pessimistic, and double frontier sustainability of supply chains. To prove the applicability of the proposed model, the sustainability of customs centers is assessed.},
  archive      = {J_ASOC},
  author       = {Amirali Fathi and Reza Farzipoor Saen},
  doi          = {10.1016/j.asoc.2021.107923},
  journal      = {Applied Soft Computing},
  pages        = {107923},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessing sustainability of supply chains by fuzzy malmquist network data envelopment analysis: Incorporating double frontier and common set of weights},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy clustering algorithms with distance metric learning
and entropy regularization. <em>ASOC</em>, <em>113</em>, 107922. (<a
href="https://doi.org/10.1016/j.asoc.2021.107922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering has been used in various fields, such as image processing , data mining, pattern recognition, and statistical analysis. Generally, clustering algorithms consider all variables equally relevant or not correlated. Nevertheless, the pattern of data samples in the multidimensional space can be geometrically complicated, e.g., clusters may exist in different subsets of features. In this regard, new soft subspace clustering algorithms have been proposed, in which the correlation and relevance of variables are considered to improve their performance. Since regularization-based methods are robust for initializations, the approaches proposed introduce an entropy regularization term for controlling the membership degree of the objects. Such regularizations are popular due to high performance in large-scale data clustering and low computational complexity . These three-step iterative algorithms provide a fuzzy partition , a representative for each cluster, and the relevance weight of the variables or their correlation by minimizing a suitable objective function. Several experiments on synthetic and real datasets, including their application to the segmentation of noisy image textures, demonstrate the usefulness of the proposed clustering methods .},
  archive      = {J_ASOC},
  author       = {Sara I.R. Rodríguez and Francisco de A.T. de Carvalho},
  doi          = {10.1016/j.asoc.2021.107922},
  journal      = {Applied Soft Computing},
  pages        = {107922},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy clustering algorithms with distance metric learning and entropy regularization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A relation b-cell network used for data identification and
fault diagnosis. <em>ASOC</em>, <em>113</em>, 107921. (<a
href="https://doi.org/10.1016/j.asoc.2021.107921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current artificial immune algorithms , the process of activation calculation considers the affinity only, but fails to take into account the relationship between sample features. In response to this deficiency, this paper proposes a new Relation B-cell and Relation B-cell Network Model (RBNM), which is basing on the immune mechanism of clone, mutation and network inhibition. In the method, this relation B-cells were calculated by the relationship between sample features. As a result, in the training and diagnosis process, combining with the relation B-cells, the algorithm can effectively improve the ability of cell association monitoring and finally generate efficient detectors. According to the following performance tests, RBNM classification result is not sensitive to its five parameters and results of the method are competitive among comparison methods. Finally, the algorithm was also applied to the reciprocating compressor faults dataset, and the result reached 99.7\%.},
  archive      = {J_ASOC},
  author       = {Hongli Zhang and Haihua Xiao and Shulin Liu and Wenhui Jiao and Chao Lan and Zhongyuan Ren and Yuan Wei},
  doi          = {10.1016/j.asoc.2021.107921},
  journal      = {Applied Soft Computing},
  pages        = {107921},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A relation B-cell network used for data identification and fault diagnosis},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An introduction of a reward-based time-series forecasting
model and its application in predicting the dynamic and complicated
behavior of the earth rotation (delta-t values). <em>ASOC</em>,
<em>113</em>, 107920. (<a
href="https://doi.org/10.1016/j.asoc.2021.107920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aimed to propose a reward-based time-series forecasting model (RBTM) as a novel method to model and forecast time series with no learning algorithm. This method extracts features of time series as distinctive rules and calculates their proper relevant rewards. These rules and rewards are kept in the knowledge base part. The next value is estimated by using rules, rewards, and the prior value of a time series. To evaluate RBTM, it was run to forecast four different time series, and the results were compared with those of MLP , TDNN , NARX, ANFIS , and LSTM models. The findings showed that errors in RBTM decreased by 0.6\% to 84.4\% compared to the other methods. Then, RBTM was used to forecast the dynamic and complicated behavior of the Earth rotation (DT values), as a real-world application. In this case, the data from 1800–2000​ and 2000–2018 were selected to model and test the parts, respectively. The mean absolute error (MAE) and root mean square error (RMSE) of the test part were 1.04 and 1.40, respectively. To evaluate the accuracy of the forecasts, the RBTM results were compared with the prediction results of the previous models. The findings indicated that the MAE and RMSE of RBTM decreased by 48\% and 39\%, respectively, as compared to those of the other methods. Finally, the results of forecasting DT values using RBTM from 2019–2030 were reported.},
  archive      = {J_ASOC},
  author       = {Alireza Hakimi and S. Amirhassan Monadjemi and Saeed Setayeshi},
  doi          = {10.1016/j.asoc.2021.107920},
  journal      = {Applied Soft Computing},
  pages        = {107920},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An introduction of a reward-based time-series forecasting model and its application in predicting the dynamic and complicated behavior of the earth rotation (Delta-T values)},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ecotourism supply chain during the COVID-19 pandemic: A real
case study. <em>ASOC</em>, <em>113</em>, 107919. (<a
href="https://doi.org/10.1016/j.asoc.2021.107919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus (COVID-19) disease has caused serious and irreversible damage to the ecotourism industry, posing serious challenges to all parts of the ecotourism supply chain. The ecotourism supply chain is made up of various components, the most important of which are ecotourism centers. During these pandemic times, the primary concerns of these centers are to improve their deplorable economic conditions and retain customers for the post-coronavirus era. As a result, an investigation should be conducted to address these concerns and provide appropriate solutions to help them overcome the challenges that have emerged. To achieve the research goal, a bi-objective mathematical model for the ecotourism supply chain in an uncertain environment is developed, accounting for the effects of COVID-19. The first objective function minimizes the total cost of the supply chain, while the second maximizes customer satisfaction. The proposed mathematical model is solved using a fuzzy goal programming (FGP) method. A sensitivity analysis study is also carried out to examine the performance of some basic parameters. Furthermore, the model is tested in a real case study to determine its efficacy. Finally, some effective managerial insights are proposed to improve the situation of the centers during the pandemic.},
  archive      = {J_ASOC},
  author       = {Seyyed Mehdi Hosseini and Mohammad Mahdi Paydar and Mehdi Alizadeh and Chefi Triki},
  doi          = {10.1016/j.asoc.2021.107919},
  journal      = {Applied Soft Computing},
  pages        = {107919},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ecotourism supply chain during the COVID-19 pandemic: A real case study},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A CNN-LSTM network with multi-level feature extraction-based
approach for automated detection of coronavirus from CT scan and x-ray
images. <em>ASOC</em>, <em>113</em>, 107918. (<a
href="https://doi.org/10.1016/j.asoc.2021.107918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auto-detection of diseases has become a prime issue in medical sciences as population density is fast growing. An intelligent framework for disease detection helps physicians identify illnesses, give reliable and consistent results, and reduce death rates. Coronavirus (Covid-19) has recently been one of the most severe and acute diseases in the world. An automatic detection framework should therefore be introduced as the fastest diagnostic alternative to avoid Covid-19 spread. In this paper, an automatic Covid-19 identification in the CT scan and chest X-ray is obtained with the help of a combined deep learning and multi-level feature extraction methodology. In this method, the multi-level feature extraction approach comprises GIST, Scale Invariant Feature Transform (SIFT), and Convolutional Neural Network (CNN) extract features from CT scans and chest X-rays. The objective of multi-level feature extraction is to reduce the training complexity of CNN network, which significantly assists in accurate and robust Covid-19 identification. Finally, Long Short-Term Memory (LSTM) along the CNN network is used to detect the extracted Covid-19 features. The Kaggle SARS-CoV-2 CT scan dataset and the Italian SIRM Covid-19 CT scan and chest X-ray dataset were employed for testing purposes. Experimental outcomes show that proposed approach obtained 98.94\% accuracy with the SARS-CoV-2 CT scan dataset and 83.03\% accuracy with the SIRM Covid-19 CT scan and chest X-ray dataset. The proposed approach helps radiologists and practitioners to detect and treat Covid-19 cases effectively over the pandemic.},
  archive      = {J_ASOC},
  author       = {Hamad Naeem and Ali Abdulqader Bin-Salem},
  doi          = {10.1016/j.asoc.2021.107918},
  journal      = {Applied Soft Computing},
  pages        = {107918},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A CNN-LSTM network with multi-level feature extraction-based approach for automated detection of coronavirus from CT scan and X-ray images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep features based convolutional neural network model for
text and non-text region segmentation from document images.
<em>ASOC</em>, <em>113</em>, 107917. (<a
href="https://doi.org/10.1016/j.asoc.2021.107917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep convolutional neural network model is presented here which uses deep learning features for text and non-text region segmentation from document images . The key objective is to extract text regions from the complex layout document images without any prior knowledge of segmentation. In a real-world scenario, a document or magazine images contain various text information along with non-text regions such as symbols, logos, pictures, and graphics. Extraction of text regions from non-text regions is challenging. To mitigate these issues, an efficient and robust segmentation technique has been proposed in this paper. The implementation of the proposed model is divided into three phases: (a) a method for pre-processing of document images using different patch sizes is employed to handle the situations for variants of text fonts and sizes in mage; (b) a deep convolutional neural network model is proposed to predict the text or non-text or ambiguous region within the image; (c) a method for post-processing of document image is proposed to handle the situation where the image has complex ambiguous regions by utilizing the recursive partitioning of those regions into their proper classes (i.e. text or non-text) and then the system accumulates the responses of those predictive patches with varying resolutions for handling the situation of text fonts variations within the image. Extensive computer simulations have been conducted using a collection of complex layout magazine images from Google sites and the ICDAR 2015 database. Results are collected and compared with state-of-the-art methods. It reveals that the proposed model is robust and more effective as compared to state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Saiyed Umer and Ranjan Mondal and Hari Mohan Pandey and Ranjeet Kumar Rout},
  doi          = {10.1016/j.asoc.2021.107917},
  journal      = {Applied Soft Computing},
  pages        = {107917},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep features based convolutional neural network model for text and non-text region segmentation from document images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal coordination of over-current relay in a power
distribution network using opposition based learning fractional order
class topper optimization (OBL-FOCTO) algorithm. <em>ASOC</em>,
<em>113</em>, 107916. (<a
href="https://doi.org/10.1016/j.asoc.2021.107916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the appropriate service of the power system network, the protection system which is equipped with relays and circuit breakers must operate in proper and efficient manner. In the power protection system , relays act as the brain of the circuit breaker which sense the fault situation and send the command to trip the circuit breaker to isolate the faulty part for reliable operation of rest part of the power system network. The relay performance may be affected by the Plug Setting (PS) and Time Dial Setting (TDS) due to the complex distribution network . The performance of the relay operation can be improved by proper tuning of TDS and PS. Therefore, to optimize the PS and TDS and to improve the performance of relay operating time, an opposition based learning fractional order class topper optimization algorithm called (OBL-FOCTO) is presented. The proposed method is a modified version of classical Class Topper Optimization (CTO, in which an Opposition Based Learning (OBL) concept as well as Fractional Order (FO) updating concept is introduced to improve the explorative and exploitive properties. The exploitation, exploration and convergence behavior of OBL-FOCTO is validated using CEC-2020 benchmark functions . The performance of the proposed method is evaluated using six test systems like IEEE 3 Bus, IEEE 4 Bus and IEEE 30 Bus system respectively. The superiority of the proposed method is shown by comparing the obtained results with some other existing results obtained using some existing methods. The results of the presented method for the relay coordination problem shows that the total operating time of the relays is minimized ( taking 0.037 to 10.432 percent less operating time to perform the relay operation for different cases) while compared to existing results for all six test systems with maintaining coordinations between each relays and also fulfill other constraints. Further, a non-parametric statistical hypothesis test has been applied to all test systems of the simulation results to show their significance.},
  archive      = {J_ASOC},
  author       = {Pankaj Kumar Choudhary and Dushmanta Kumar Das},
  doi          = {10.1016/j.asoc.2021.107916},
  journal      = {Applied Soft Computing},
  pages        = {107916},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal coordination of over-current relay in a power distribution network using opposition based learning fractional order class topper optimization (OBL-FOCTO) algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment-oriented query-focused text summarization
addressed with a multi-objective optimization approach. <em>ASOC</em>,
<em>113</em>, 107915. (<a
href="https://doi.org/10.1016/j.asoc.2021.107915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the automatic text summarization is a highly relevant task in many contexts. In particular, query-focused summarization consists of generating a summary from one or multiple documents according to a query given by the user. Additionally, sentiment analysis and opinion mining analyze the polarity of the opinions contained in texts. These two issues are integrated in an approach to produce an opinionated summary according to the user’s query. Thereby, the query-focused sentiment-oriented extractive multi-document text summarization problem entails the optimization of different criteria, specifically, query relevance, redundancy reduction, and sentiment relevance. An adaptation of the metaheuristic population-based crow search algorithm has been designed, implemented, and tested to solve this multi-objective problem. Experiments have been carried out by using datasets from the Text Analysis Conference (TAC) datasets. Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics and the Pearson correlation coefficient have been used for the performance assessment. The results have reported that the proposed approach outperforms the existing methods in the scientific literature, with a percentage improvement of 75.5\% for ROUGE-1 score and 441.3\% for ROUGE-2 score. It also has been obtained a Pearson correlation coefficient of +0.841, reporting a strong linear positive correlation between the sentiment scores of the generated summaries and the sentiment scores of the queries of the topics.},
  archive      = {J_ASOC},
  author       = {Jesus M. Sanchez-Gomez and Miguel A. Vega-Rodríguez and Carlos J. Pérez},
  doi          = {10.1016/j.asoc.2021.107915},
  journal      = {Applied Soft Computing},
  pages        = {107915},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentiment-oriented query-focused text summarization addressed with a multi-objective optimization approach},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel random matrix particle swarm optimization
scheduling algorithms with budget constraints on cloud computing
systems. <em>ASOC</em>, <em>113</em>, 107914. (<a
href="https://doi.org/10.1016/j.asoc.2021.107914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, increasing number of Internet of Things and mobile Internet application services are migrated to cloud computing systems . One of the most important cloud challenges for this business is to optimize services cost. The efficient way to deal with this challenge is to improve the performance of resource management and task scheduling in cloud systems . However, this kind of service scheduling is a typical combinatorial optimization problem , and it is difficult to obtain the optimal solution. In this study, we first formalize this cloud services on virtual machines (VMs) with budget constraints scheduling problem. Then, we propose a random matrix particle swarm optimization scheduling algorithm (RMPSO), which uses the random integer matrix to represent its position and a feasible task scheduling scheme, to achieve the optimal total cost of cloud services. However, the drawback of this solution for large-scale systems is its high time complexity. Therefore, we propose two parallel RMPSO algorithms: CPU parallel algorithm (M-RMPSO) on multi-core system with shared memory and manycore GPU-accelerated strategy (G-RMPSO) to reduce its time complexity. Finally, the rigorous performance evaluation results clearly show that our proposed G-RMPSO outperforms M-RMPSO and existing FMPSO, HYBRID (MPSO+MCSO) in terms of cloud services total cost and algorithm execution time. Therefore, our proposed GPU-accelerated G-RMPSO algorithm is very suitable for cloud service scheduling.},
  archive      = {J_ASOC},
  author       = {Xiaoyong Tang and Cheng Shi and Tan Deng and Zhiqiang Wu and Li Yang},
  doi          = {10.1016/j.asoc.2021.107914},
  journal      = {Applied Soft Computing},
  pages        = {107914},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel random matrix particle swarm optimization scheduling algorithms with budget constraints on cloud computing systems},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised textile defect detection using convolutional
neural networks. <em>ASOC</em>, <em>113</em>, 107913. (<a
href="https://doi.org/10.1016/j.asoc.2021.107913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel motif-based approach for unsupervised textile anomaly detection that combines the benefits of traditional convolutional neural networks with those of an unsupervised learning paradigm. It consists of five main steps: preprocessing, automatic pattern period extraction, patch extraction, features selection and anomaly detection . This proposed approach uses a new dynamic and heuristic method for feature selection which avoids the drawbacks of initialization of the number of filters (neurons) and their weights, and those of the backpropagation mechanism such as the vanishing gradients, which are common practice in the state-of-the-art methods. The design and training of the network are performed in a dynamic and input domain-based manner and, thus, no ad-hoc configurations are required. Before building the model, only the number of layers and the stride are defined. We do not initialize the weights randomly nor do we define the filter size or number of filters as conventionally done in CNN-based approaches. This reduces effort and time spent on hyper-parameter initialization and fine-tuning. Only one defect-free sample is required for training and no further labeled data is needed. The trained network is then used to detect anomalies on defective fabric samples. We demonstrate the effectiveness of our approach on the Patterned Fabrics benchmark dataset. Our algorithm yields reliable and competitive results (on recall, precision, accuracy and f1-measure) compared to state-of-the-art unsupervised approaches, in less time, with efficient training in a single epoch and a lower computational cost.},
  archive      = {J_ASOC},
  author       = {Imane Koulali and M. Taner Eskil},
  doi          = {10.1016/j.asoc.2021.107913},
  journal      = {Applied Soft Computing},
  pages        = {107913},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised textile defect detection using convolutional neural networks},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Punishment-driven consensus reaching model in social network
large-scale decision-making with application to social capital
selection. <em>ASOC</em>, <em>113</em>, 107912. (<a
href="https://doi.org/10.1016/j.asoc.2021.107912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering and consensus reaching are two crucial processes for solving social network large-scale decision-making (SN-LSDM) problems. This paper explores the moderating effect of trust on the decision-making process, which is decomposed into examining the influence of trust constraint on clustering and the influence of trust loss on consensus reaching. We hold that low-trust experts are not suitable to be assigned to the same subgroup, even though their preferences are sufficiently similar. We propose a punishment-driven consensus reaching model for SN-LSDM problems. The model contains two main stages. In Stage 1, a trust-constrained K-means clustering algorithm is presented, which can overcome the defect of grouping low-trust experts caused by traditional clustering methods based on preference similarity. Then, a weight-determining method combining the size, cohesion, and overall trust degree of a subgroup is proposed. In Stage 2, a punishment-driven consensus reaching process is designed, which identifies four categories of consensus scenarios (high–high, high–low, low–high, low–low) through consensus measures and trust measures. Different adjustment strategies are established, and the moderating effect of trust loss on consensus reaching and consensus cost is investigated. Finally, both a case study on social capital selection in public–private partnership projects and a comparative analysis are implemented to reveal the feasibility and advantages of the clustering algorithm and consensus reaching model proposed in this paper.},
  archive      = {J_ASOC},
  author       = {Su-min Yu and Zhi-jiao Du and Xue-yang Zhang and Han-yang Luo and Xu-dong Lin},
  doi          = {10.1016/j.asoc.2021.107912},
  journal      = {Applied Soft Computing},
  pages        = {107912},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Punishment-driven consensus reaching model in social network large-scale decision-making with application to social capital selection},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative semantic representation network for metaphor
detection. <em>ASOC</em>, <em>113</em>, 107911. (<a
href="https://doi.org/10.1016/j.asoc.2021.107911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaphorical expressions are widely present in natural language, which brings huge challenges to various natural language processing tasks such as machine translation. Metaphor detection identifies the metaphorical usage of each word at the lexical level. How to effectively represent metaphor words and context semantics is the core element of this task. In this study, we propose a deep learning model that combines hierarchical feature representation and semantic interaction. Specifically, we extract the word-level and sentence-level representations of sentences as hierarchical feature representations. In this model, multi-attention combining word embedding , part-of-speech and position is proposed to focus on the importance of words from multiple levels. To integrate hierarchical features, efficient collaborative attention was developed to fuse word-level and sentence-level representations. This strategy helps to process features at different levels and makes the fusion vector more complementary and complete, which brings a natural perspective to feature extraction. Experimental results on the VU Amsterdam (VUA) Metaphor Corpus and TroFi show that our method outperforms the state-of-the-art models.},
  archive      = {J_ASOC},
  author       = {Qimeng Yang and Long Yu and Shengwei Tian and Jinmiao Song},
  doi          = {10.1016/j.asoc.2021.107911},
  journal      = {Applied Soft Computing},
  pages        = {107911},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Collaborative semantic representation network for metaphor detection},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binary searching iterative algorithm for generating test
cases to cover paths. <em>ASOC</em>, <em>113</em>, 107910. (<a
href="https://doi.org/10.1016/j.asoc.2021.107910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar paths are usually covered by similar test cases, which is one of the characteristics of automated test case generation for path coverage. Based on this characteristic, this paper proposes a novel search-based algorithm for generating test cases to satisfy path coverage criterion, called binary searching iterative algorithm . The proposed algorithm first selects an uncovered path as a target path, which is most similar to the path covered by a discovered test case. Then it performs a binary search in both the left and right regions of each element of the discovered test case under the guidance of a fitness function for the target path. Binary searching iterative algorithm can quickly find undiscovered test case covering the target path because of making full use of the characteristic of automated test case generation for path coverage. Experimental studies on six fog computing benchmark programs and six natural language processing benchmark programs show that the proposed algorithm can achieve the highest path coverage for all the twelve benchmark programs, and the average number of test cases obtained by the proposed algorithm is significantly less than those obtained by a number of state-of-the-art algorithms for eleven out of the twelve benchmark programs. Moreover, binary searching iterative algorithm is more appropriate for ALBD-based fitness function than BD-based fitness function.},
  archive      = {J_ASOC},
  author       = {Gaocheng Cai and Qinghua Su and Zhongbo Hu},
  doi          = {10.1016/j.asoc.2021.107910},
  journal      = {Applied Soft Computing},
  pages        = {107910},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Binary searching iterative algorithm for generating test cases to cover paths},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MO-PaDGAN: Reparameterizing engineering designs for
augmented multi-objective optimization. <em>ASOC</em>, <em>113</em>,
107909. (<a href="https://doi.org/10.1016/j.asoc.2021.107909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective optimization is key to solving many Engineering Design problems , where design parameters are optimized for several performance indicators. However, optimization results are highly dependent on how the designs are parameterized. Researchers have shown that deep generative models can learn compact design representations, providing a new way of parameterizing designs to achieve faster convergence and improved optimization performance . Despite their success in capturing complex distributions, existing generative models face three challenges when used for design problems: (1) generated designs have limited design space coverage, (2) the generator ignores design performance, and 3) the new parameterization is unable to represent designs beyond training data. To address these challenges, we propose MO-PaDGAN, which adds a Determinantal Point Processes based loss function to the generative adversarial network to simultaneously model diversity and (multi-variate) performance. MO-PaDGAN can thus improve the performances and coverage of generated designs, and even generate designs with performances exceeding those from training data. When using MO-PaDGAN as a new parameterization in multi-objective optimization, we can discover much better Pareto fronts even though the training data do not cover those Pareto fronts . In a real-world multi-objective airfoil design example, we demonstrate that MO-PaDGAN achieves, on average, an over 180\% improvement in the hypervolume indicator when compared to the vanilla GAN or other state-of-the-art parameterization methods.},
  archive      = {J_ASOC},
  author       = {Wei Chen and Faez Ahmed},
  doi          = {10.1016/j.asoc.2021.107909},
  journal      = {Applied Soft Computing},
  pages        = {107909},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MO-PaDGAN: Reparameterizing engineering designs for augmented multi-objective optimization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hardware implementation of metaheuristics through LabVIEW
FPGA. <em>ASOC</em>, <em>113</em>, 107908. (<a
href="https://doi.org/10.1016/j.asoc.2021.107908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic optimization methods have been implemented for solving several problems. However, when it is required to implement those algorithms in hardware to run online, there is not enough information. This paper describes how could be programmed and implemented those optimization algorithms . Moreover, a complete evaluation is shown as well as a comparative study regarding the most important metaheuristic optimization algorithms . Thus, this paper presents a comparison between five optimization algorithms implemented into a cRIO field-programmable gate array (LabVIEW FPGA) NI-9030 of National Instruments T M TM (NI). The algorithms implemented were particle swarm optimization (PSO), bat algorithm (BA), grey wolf optimizer (GWO), earthquake algorithm (EA), and Nelder–Mead algorithm (NM). To analyze hardware device utilization and execution time by each algorithm, synthesis results were presented. In addition, a set of ten benchmark functions was selected to compare performance between algorithms. Results show the feasibility of this approach for NI FPGA hardware. From device utilization results, GWO presents the lowest placed usage (29\%) while NM shows the fastest execution time (0.683 ms). Nevertheless, PSO, GWO and EA show better performance between benchmark functions due their exploration characteristics which make possible to find a better solution.},
  archive      = {J_ASOC},
  author       = {Alexandro Ortiz and Efrain Mendez and David Balderas and Pedro Ponce and Israel Macias and Arturo Molina},
  doi          = {10.1016/j.asoc.2021.107908},
  journal      = {Applied Soft Computing},
  pages        = {107908},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hardware implementation of metaheuristics through LabVIEW FPGA},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust sparse low-rank embedding for image dimension
reduction. <em>ASOC</em>, <em>113</em>, 107907. (<a
href="https://doi.org/10.1016/j.asoc.2021.107907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many methods based on matrix factorization have recently been proposed and achieve good performance in many practical applications. Latent low-rank representation (LatLRR) is a marvelous feature extraction method, and it has shown a powerful ability in extracting robust data features . However, LatLRR and the variants of LRR have some shortcomings as follows: (1) The label information of the original data are not considered, and they are usually unsupervised learning methods. (2) The local structure information is not preserved in the projected space. (3) The dimension of projection space is not reduced, and the extracted features do not have good and distinct interpretability . In order to solve the above problems, a new dimensionality reduction method based on low-rank representation termed robust sparse low-rank embedding (RSLRE) is proposed. Especially, by introducing the L 2 , 1 L2, 1 norm constraint into the projected matrix, RSLRE algorithm can adaptively select the most discriminative and robust data features . In addition, two different matrices are introduced to ensure that projected feature dimensions can be reduced, and the obtained features can simultaneously maintain most of the energy of the observed samples. A large number of experiments on five public image datasets show that the proposed method can achieve very encouraging results compared with some classical feature extraction methods.},
  archive      = {J_ASOC},
  author       = {Zhonghua Liu and Yue Lu and Zhihui Lai and Weihua Ou and Kaibing Zhang},
  doi          = {10.1016/j.asoc.2021.107907},
  journal      = {Applied Soft Computing},
  pages        = {107907},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust sparse low-rank embedding for image dimension reduction},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classifying the degree of exposure of customers to COVID-19
in the restaurant industry: A novel intuitionistic fuzzy set extension
of the TOPSIS-sort. <em>ASOC</em>, <em>113</em>, 107906. (<a
href="https://doi.org/10.1016/j.asoc.2021.107906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the rigid public safety protocols of the restaurant sector amid the COVID-19 pandemic in an effort to restart economic activities, customers do not feel secure eating at a sit-in restaurant, which is associated with prolonged restrictions on movement. As a mitigating initiative, holistically evaluating customers’ perceived degree of exposure to COVID-19 in restaurants is deemed relevant in the design of mitigation measures. Such an agenda is associated with multiple attributes under decision-making uncertainty within the framework of multiple criteria sorting (MCS). Thus, this work addresses this problem domain by proposing an intuitionistic fuzzy set extension of the previously developed TOPSIS-Sort (i.e., IF TOPSIS-Sort). As a case demonstration, 40 restaurants are evaluated under six attributes that define exposure to COVID-19. With 250 survey participants, the IF TOPSIS-Sort assigns 10, 13, and 17 restaurants to low, moderate, and high exposure classes, respectively. With this classification, crucial insights are offered to the restaurant industry for planning and policy formulation. To determine its effectiveness, a comparative analysis was carried with other distance-based MCS methods. Findings reveal that the proposed method is pessimistic and that other methods tend to underestimate the assignments, which may be counterintuitive, especially in applications related to public health. These sorting differences may be associated with addressing the vagueness and uncertainty in decision-making within the IF TOPSIS-Sort platform. The proposed novel IF TOPSIS-Sort is sufficiently generic for other domain sorting applications and contributes to the MCS literature.},
  archive      = {J_ASOC},
  author       = {Lanndon Ocampo and Reciel Ann Tanaid and Ann Myril Tiu and Egberto Selerio Jr. and Kafferine Yamagishi},
  doi          = {10.1016/j.asoc.2021.107906},
  journal      = {Applied Soft Computing},
  pages        = {107906},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classifying the degree of exposure of customers to COVID-19 in the restaurant industry: A novel intuitionistic fuzzy set extension of the TOPSIS-sort},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Many-objective multilevel thresholding image segmentation
for infrared images of power equipment with boost marine predators
algorithm. <em>ASOC</em>, <em>113</em>, 107905. (<a
href="https://doi.org/10.1016/j.asoc.2021.107905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of power systems , fault diagnosis in infrared images has become increasingly important for ensuring the stability of these systems. In this paper, we propose a multi-objective multilevel threshold image segmentation method based on the boost marine predators algorithm (BMPA) for infrared-image fault diagnosis. As fault points are small, it is difficult to detect the target in an infrared image using existing segmentation methods . To address this, we use 9DKapur as the fitness function and obtain a many-objective optimization problems (MaOPs). Then, we use adaptive weights and opposition-based learning to boost the optimization ability of the MPA and solve the MOP so that a Pareto front is obtained. The DTLZ and WFG test suits are used as benchmarks to evaluate the performance of the BMPA, and electric equipment in infrared images was used to assess the fault-diagnostic ability of the proposed method. The results demonstrate that the BMPA performs better than other optimization algorithms in terms of uniformity measurement, peak signal-to-noise ratio, feature similarity, hypervolume, spacing, and CPU time. On the actual test data, the recall rate and fault detection accuracy of the proposed method were 94.29\% and 92.38\%, respectively. Insulator faults under various circumstances can be correctly detected in infrared images.},
  archive      = {J_ASOC},
  author       = {Zhikai Xing and Yigang He},
  doi          = {10.1016/j.asoc.2021.107905},
  journal      = {Applied Soft Computing},
  pages        = {107905},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Many-objective multilevel thresholding image segmentation for infrared images of power equipment with boost marine predators algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evolvable hardware method based on elite partheno-genetic
algorithm. <em>ASOC</em>, <em>113</em>, 107904. (<a
href="https://doi.org/10.1016/j.asoc.2021.107904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the shortcomings of traditional genetic algorithm (GA) in the overall evolution of hardware, such as slow speed and small scale, a method of evolvable hardware (EHW) based on elite Partheno-Genetic Algorithm (PGA) is proposed. To improve the evolution efficiency of EHW, the number of chromosomes is reduced, the length of the chromosome is increased, the elite node chain is reserved, and the connection relationship of non-elite nodes is adjusted. The experimental results show that: compared with the existing literature, the evolution speed is increased 144 times on average, the overall scale of evolution is increased 74.7 times, and the evolution efficiency is significantly improved.},
  archive      = {J_ASOC},
  author       = {Lijun Liu and Tao Wang},
  doi          = {10.1016/j.asoc.2021.107904},
  journal      = {Applied Soft Computing},
  pages        = {107904},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolvable hardware method based on elite partheno-genetic algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sustainable building material selection: An integrated
multi-criteria large group decision making framework. <em>ASOC</em>,
<em>113</em>, 107903. (<a
href="https://doi.org/10.1016/j.asoc.2021.107903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades, the rapid urbanisation has driven the prosperity of building industry, which, however, has led to the consumption of enormous amounts of energy and resources and the continued deterioration of the environment owing to its unsustainable development. These outcomes impact seriously the ecosystems and human health. In order to promote the sustainable performance of building sector, the widely adoption of sustainable building materials has been considered as one of the most promising ways toward this endeavor. This paper aims at addressing the problem of sustainable building material selection (SBMS) under uncertain context by developing an integrated multi-criteria large-group decision-making framework. In the proposed framework, the assessment information is characterised in the form of Improved Basic Uncertain Linguistic Information (IBULI), which incorporates reliable degrees of decision experts that can accurately quantify subjective assessment information provided under uncertainty. The weights of assessment criteria for SBMS are determined by the Quality Function Deployment -based method that is capable of accommodating the influences of multiple stakeholders and the fields of conflicts amongst them. Subsequently, the assessment information given by a large-group of decision experts are aggregated by IBULI-aggregation operators guided by an IBULI-based correlation- and consensus-driven clustering method , which achieves the classification of large-scale decision experts and the satisfied consensus level simultaneously. The selection of sustainable building materials is eventually accomplished by the IBULI-based Technique for Order Preference by Similarity to Ideal Solution method. Finally, an illustrative example accompanied by sensitivity and comparative analyses is performed to verify the rationality and feasibility of the proposed model.},
  archive      = {J_ASOC},
  author       = {Zhen-Song Chen and Lan-Lan Yang and Kwai-Sang Chin and Yi Yang and Witold Pedrycz and Jian-Peng Chang and Luis Martínez and Mirosław J. Skibniewski},
  doi          = {10.1016/j.asoc.2021.107903},
  journal      = {Applied Soft Computing},
  pages        = {107903},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sustainable building material selection: An integrated multi-criteria large group decision making framework},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Systems failure analysis using z-number theory-based
combined compromise solution and full consistency method. <em>ASOC</em>,
<em>113</em>, 107902. (<a
href="https://doi.org/10.1016/j.asoc.2021.107902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial and time constraints force managers to put limited failure modes in priority to implement corrective and preventive actions. Thus, how prioritization can be done more constructively is of particular importance. This study aims to introduce an improved Failure Modes and Effects Analysis (FMEA) technique based on an extension of the Combined Compromise Solution (CoCoSo) method and the Full Consistency Method (FUCOM) to assess and prioritize failures in a production process. These developed methods called Z-CoCoSo and Z-FUCOM rely on the Z-number theory. They can consider uncertainty and reliability simultaneously in determining the weights of risk factors and the value of these factors in the studied problem. The proposed approach can cover some shortcomings of the conventional FMEA technique employed in identifying potential failures before their occurrence. Applying this approach enables experts to consider different weights for risk factors and uncertainty in the risk assessment process and provides them with a reliable ranking with high separability . Implementing the introduced approach for a real case study in the automotive parts industry has been compared with FMEA and other existent versions of the used methods demonstrating its reliable prioritization.},
  archive      = {J_ASOC},
  author       = {Samuel Yousefi and Mahsa Valipour and Muhammet Gul},
  doi          = {10.1016/j.asoc.2021.107902},
  journal      = {Applied Soft Computing},
  pages        = {107902},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Systems failure analysis using Z-number theory-based combined compromise solution and full consistency method},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying crop diseases using attention embedded
MobileNet-v2 model. <em>ASOC</em>, <em>113</em>, 107901. (<a
href="https://doi.org/10.1016/j.asoc.2021.107901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various crop diseases are a major problem worldwide since their occurrence leads to a significant decrease in crop production. The image-based automatic identification of crop diseases that involves food security has attracted much attention recently. It is a challenging research topic due to the complexity of crop disease images, such as clutter field backdrops and irregular lighting strengths. A variety of deep learning networks, especially CNNs , are becoming the mainstream methods for addressing many challenges correlated with image recognition and classification. In this study, to improve the learning ability of minor lesion features, we introduced the Location-wise Soft Attention mechanism to the pre-trained MobileNet-V2, in which the general knowledge of images learned from ImageNet was migrated to our crop disease recognition mode, namely, CDRM. Further, a localization strategy was embedded in the proposed network, and the two-phase progressive strategy was executed for model training. The proposed method shows substantial efficacy in the experimental analyses. It reached a 99.71\% average accuracy on the open-source dataset, and even under cluttered background conditions, the average accuracy attained 99.13\% for the identification of crop diseases. Experimental findings deliver a competitive performance compared to other state-of-the-art methods and also indicate the efficacy and extension of the proposed method. Our code is available at https://github.com/xtu502/crop-disease-recognition-model .},
  archive      = {J_ASOC},
  author       = {Junde Chen and Defu Zhang and Md Suzauddola and Adnan Zeb},
  doi          = {10.1016/j.asoc.2021.107901},
  journal      = {Applied Soft Computing},
  pages        = {107901},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying crop diseases using attention embedded MobileNet-v2 model},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive boundary sine cosine optimizer with population
reduction for robustness analysis of finite time horizon systems.
<em>ASOC</em>, <em>113</em>, 107900. (<a
href="https://doi.org/10.1016/j.asoc.2021.107900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an adaptive boundary sine cosine optimizer with population reduction. It is designed specifically to calculate an upper bound on the worst-case gain of a known finite horizon Linear Time Varying (LTV) system and a perturbation. The input/output behavior of the perturbation is described by a time domain Integral Quadratic Constraint (IQC). The analysis condition is formulated as a parametric Riccati differential equation which depends on the IQC representation. A nonlinear optimization problem is posed that minimizes the upper bound on the worst-case gain over the IQC parameterization. For industrial size applications like a space launcher, the number of decision variables can grow arbitrarily large depending on the number of considered perturbations as well as the type and representation of the IQC. This is aggravated by nonlinear constraints on the decision variables imposed by the Riccati differential equation making it challenging to solve. Several established Meta-Heuristics (MHs) along with the proposed algorithm are applied to an industry size worst case analysis of a space launcher during its atmospheric ascend. Their respective performances are evaluated to emphasize the advantages of the developed optimizer. This work builds the foundation of applying MHs to IQC based robustness analysis of finite horizon LTV systems.},
  archive      = {J_ASOC},
  author       = {Felix Biertümpfel and Nantiwat Pholdee and Sujin Bureerat and Harald Pfifer},
  doi          = {10.1016/j.asoc.2021.107900},
  journal      = {Applied Soft Computing},
  pages        = {107900},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive boundary sine cosine optimizer with population reduction for robustness analysis of finite time horizon systems},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilinear clustering via tensor fukunaga–koontz transform
with fisher eigenspectrum regularization. <em>ASOC</em>, <em>113</em>,
107899. (<a href="https://doi.org/10.1016/j.asoc.2021.107899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental learning task with many applications in a wide range of fields. Recently proposed techniques have shown that performing clustering in a discriminative space provides reliable results. Motivated by these results, as well as by advances in subspace representation, we introduce in this paper a new learning model that performs discriminative clustering on tensor data. The proposed method exploits the inherent tensor mode representation provided by multilinear data, extracting discriminative spaces in each mode, which are further combined in a product space. In previous work, the Fukunaga–Koontz transform was extended to handle multilinear data through the use of a tensor representation. That work yielded notable results in the clustering of gestures and actions from videos. However, the model may overfit because no regularization process is applied. Therefore, an efficient regularization scheme based on the Fisher score is proposed in this paper to optimize the clustering model. In addition to a new regularization scheme and discriminative properties, the advantages of our method include (1) sufficient flexibility to adapt to hierarchical and k k -means clustering algorithms with low computational cost inherited from subspace learning, (2) a new formulation of the mean between two tensors in terms of the product of spaces, and (3) a Fisher score definition for multilinear data. Comprehensive experimental results on diverse real-world datasets confirm that the proposed method provides results that are competitive with those from current tensor clustering algorithms.},
  archive      = {J_ASOC},
  author       = {Bernardo B. Gatto and Eulanda M. dos Santos and Marco A.F. Molinetti and Kazuhiro Fukui},
  doi          = {10.1016/j.asoc.2021.107899},
  journal      = {Applied Soft Computing},
  pages        = {107899},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multilinear clustering via tensor Fukunaga–Koontz transform with fisher eigenspectrum regularization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stock index prediction and uncertainty analysis using
multi-scale nonlinear ensemble paradigm of optimal feature extraction,
two-stage deep learning and gaussian process regression. <em>ASOC</em>,
<em>113</em>, 107898. (<a
href="https://doi.org/10.1016/j.asoc.2021.107898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable prediction of stock indexes can be highly valuable for financial decision-making and risk management . The stock market is a highly complicated nonlinear system which makes it difficult to present accurate predictors. In this paper, an innovative multi-scale nonlinear ensemble paradigm is proposed for stock index prediction and uncertainty analysis, which consists of an optimal feature extraction including variational mode decomposition and auto-encoder, a two-stage deep learning based on recurrent neural network and long short-term memory, and Gaussian process regression. The optimal feature extraction is proposed to extract the optimal features of stock index fluctuations and eliminate the disturbance of illusive components. The two-stage deep learning is developed to conduct the prediction of each feature sub-signal and implement its nonlinear integration. The Gaussian process regression is utilized to construct the interval prediction of the original stock signal and analyze the uncertainties of stock market. The validity of the developed model is verified by the data from S&amp;P 500, Dow Jones index and NASDAQ. After a series of comparisons, the mean absolute percentage errors of the proposed model in S&amp;P 500, Dow Jones index and NASDAQ are 0.55\%, 0.65\% and 1.11\%, respectively. These results fully verify the effectiveness of proposed model.},
  archive      = {J_ASOC},
  author       = {Jujie Wang and Junjie He and Chunchen Feng and Liu Feng and Yang Li},
  doi          = {10.1016/j.asoc.2021.107898},
  journal      = {Applied Soft Computing},
  pages        = {107898},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stock index prediction and uncertainty analysis using multi-scale nonlinear ensemble paradigm of optimal feature extraction, two-stage deep learning and gaussian process regression},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal feature selection using modified cuckoo search for
classification of power quality disturbances. <em>ASOC</em>,
<em>113</em>, 107897. (<a
href="https://doi.org/10.1016/j.asoc.2021.107897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread usages of sensitive equipment such as computers, controllers and microelectronic devices have placed immense burden on the grid operators to deliver high quality electrical power to their customers. To achieve this end, the power quality disturbances (PQD) within the network need to be minimized. In this paper, a method to enhance the performance of the multiclass support vector machine (MSVM) classifier using the modified cuckoo search (MCS) is proposed. The wavelet packet transform is used to extract the crucial features from the PQD waveforms; these features are utilized as the input data to the classifier. In order to achieve high accuracy, robustness and speed, the MCS optimizes the number of selected features, as well as the penalty factor and slack variable of the MSVM. The proposed combinatorial algorithm (MCS-MSVM) is tested using 31 categories of PQD events; the hypothetical data for these events are generated by the IEEE 1159 Standard parametric equations . From simulation, the classification accuracy is recorded to be 100\% under the no-noise condition, while at the signal-to-noise ratios (SNR) of 10, 20, 30 and 40 dB, the accuracies are 98.40, 98.54, 99.14 and 99.64\%, respectively. Moreover, the comparative assessment concludes that the proposed method is superior to other heuristics-MSVM classification methods, namely the GA , PSO , differential evolution, harmony search and the conventional cuckoo search . The practical performance of the MCS-MSVM classifier is validated using real-time PQD data of a typical 11-kV underground distribution network , obtained from a particular electrical utility operator. For benchmarking, comparisons are made to 17 most recent PQD classification techniques published in literature. It is found that the proposed method exhibits the highest accuracies and the lowest computation times under ideal and noisy environments .},
  archive      = {J_ASOC},
  author       = {Ibrahim Mustafa Mehedi and Masoud Ahmadipour and Zainal Salam and Hussein Mohammed Ridha and Hussein Bassi and Muhyaddin Jamal Hosin Rawa and Mohammad Ajour and Abdullah Abusorrah and Md. Pauzi Abdullah},
  doi          = {10.1016/j.asoc.2021.107897},
  journal      = {Applied Soft Computing},
  pages        = {107897},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal feature selection using modified cuckoo search for classification of power quality disturbances},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RAI: Rapid, autonomous and intelligent machine learning
approach to identify fire-vulnerable bridges. <em>ASOC</em>,
<em>113</em>, 107896. (<a
href="https://doi.org/10.1016/j.asoc.2021.107896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent surveys have noted that the majority of bridges continue to serve for a prolonged period of time (+40 years) that far exceeds its intended operational lifespan. Given our limited resources to maintain and upkeep bridges, these structures become notoriously vulnerable to extreme events. Building upon the fact that bridges continue not to be specifically designed to withstand the adverse effects of fire, this study presents the development of a r apid, a utomated, and i ntelligent ( RAI ) approach that leverages machine learning to identify vulnerable bridges to fire hazard . This work also presents details on a comprehensive database comprising actual observations collected from 135 notable and international bridge fire incidents. This database was developed to train two machine learning techniques , deep learning and genetic algorithms , to quantify hidden patterns that govern the propensity of existing/new/historical bridges to undergo fire damage and/or fire-induced collapse via a multi-classification analysis. The proposed RAI approach also has the capability to pinpoint fire-vulnerable bridge components and to display its level of confidence in its predictions. As such, our approach can be of aid to bridge engineers and government officials (who may not be well-versed in fire design) with accuracy reaching 89.6\%. This approach is implemented into a software (App) with optimized architecture and reduced computational complexity and hence is easily scalable and integratable into a user-friendly framework and handheld devices. The outcome of this study shows that the RAI approach can be deployed to arrive at an instantaneous assessment of fire vulnerable bridges.},
  archive      = {J_ASOC},
  author       = {M. Abedi and M.Z. Naser},
  doi          = {10.1016/j.asoc.2021.107896},
  journal      = {Applied Soft Computing},
  pages        = {107896},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RAI: Rapid, autonomous and intelligent machine learning approach to identify fire-vulnerable bridges},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance evaluation of metaheuristics algorithms for
workload prediction in cloud environment. <em>ASOC</em>, <em>113</em>,
107895. (<a href="https://doi.org/10.1016/j.asoc.2021.107895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The smooth operation of a cloud data center along with the best user experience is one of the prime objectives of a resource management scheme that must be achieved at low cost in terms of resource wastage, electricity consumption, security and many others. The workload prediction has proved to be very useful in improving these schemes as it provides the prior estimation of upcoming demands. These predictions help a cloud system in assigning the resources to new and existing applications on low cost. Machine learning has been extensively used to design the predictive models . This article aims to study the performance of different nature-inspired based metaheuristic algorithms on workload prediction in cloud environment. We conducted an in-depth analysis using eight widely used algorithms on five different data traces. The performance of each approach is measured using Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). In addition, the statistical analysis is also carried out using Wilcoxon signed rank and Friedman with Finner post-hoc multiple comparison tests. The study finds that Blackhole Algorithm (BhA) reduced the RMSE by 23.60\%, 6.51\%, 21.21\%, 60.45\% and 38.30\% relative to the worst performing algorithm for 5 min forecasts of all five data traces correspondingly. Moreover, Friedman test confirms that the results of these approaches have a significant difference with 95\% confidence interval (CI) and ranks show that the BhA and FSA received best ranks for Google Cluster trace (CPU and Memory Requests) while second best ranks for NASA and Saskatchewan HTTP server requests.},
  archive      = {J_ASOC},
  author       = {Jitendra Kumar and Ashutosh Kumar Singh},
  doi          = {10.1016/j.asoc.2021.107895},
  journal      = {Applied Soft Computing},
  pages        = {107895},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Performance evaluation of metaheuristics algorithms for workload prediction in cloud environment},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-step wind speed forecasting based on secondary
decomposition algorithm and optimized back propagation neural network.
<em>ASOC</em>, <em>113</em>, 107894. (<a
href="https://doi.org/10.1016/j.asoc.2021.107894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improve the reliability of wind speed forecasting is a crucial task in wind power generation . Due to the stochastic and noise nature of wind, a preprocessing step is beneficial for wind speed series to get clean data. The decomposition technique is reported as the critical preprocessor to transform the unstable wind speed data into several regular components. This study proposes a hybrid forecasting system, which combines secondary decomposition algorithm and optimized back propagation (BP) neural network. For the decomposition part, the variational mode decomposition (VMD) is firstly used to extract the low-frequency part from the original wind data. Then the symplectic geometry mode decomposition (SGMD) decomposes the rest high-frequency part into clean and separate components. The BP algorithm is optimized by the differential evolution (DE) as the predictor in this study. Empirical studies with different comparison models are conducted on real wind speed data. The results affirm the competitive strength of the proposed combination strategy. And the proposed two-stage decomposition technique is applicable for nonlinear wind speed analysis.},
  archive      = {J_ASOC},
  author       = {Wei Sun and Bin Tan and Qiqi Wang},
  doi          = {10.1016/j.asoc.2021.107894},
  journal      = {Applied Soft Computing},
  pages        = {107894},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-step wind speed forecasting based on secondary decomposition algorithm and optimized back propagation neural network},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive clustering-based approach for forgery detection in
images containing similar appearing but authentic objects.
<em>ASOC</em>, <em>113</em>, 107893. (<a
href="https://doi.org/10.1016/j.asoc.2021.107893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy-move forgery is one of the well-known image forgery technique which exploits regions of the same image to create forged image by replicating or hiding authentic content of the original image. Original images can also contain similar looking but authentic objects. In such cases, identification of authentic and tampered images is a complicated task. To tackle this problem, we propose a method in which Stationary Wavelet Transform (SWT) and spatial-constrained edge preserving watershed segmentation are applied over input image in preprocessing step . Keypoint extraction and descriptor computation are performed using Cascaded Features from Accelerated Segment Test (Cascaded FAST) and Binary Robust Invariant Scalable Keypoint (BRISK) descriptor, respectively. Approximate nearest neighbor search is performed using Random Binary Search Tree (RBST) method. For keypoint clustering, Adaptive Density Peak Clustering (ADPC) technique is employed. Outlier removal is performed using Random Sample Consensus (RANSAC) technique. Further, forged regions are localized using correlation map generation. Experimental results display that the proposed approach can effectively distinguish between forged and original images containing similar appearing but authentic objects. It is also able to detect forged images sustaining different post-processing attacks. For COVERAGE dataset, proposed technique achieves high F-Measure = 86.901\% and low False Positive Rate (FPR) = 15.241\% in comparison to state-of-the-art techniques.},
  archive      = {J_ASOC},
  author       = {Anuja Dixit and Soumen Bag},
  doi          = {10.1016/j.asoc.2021.107893},
  journal      = {Applied Soft Computing},
  pages        = {107893},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive clustering-based approach for forgery detection in images containing similar appearing but authentic objects},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Elephant clan optimization: A nature-inspired metaheuristic
algorithm for the optimal design of structures. <em>ASOC</em>,
<em>113</em>, 107892. (<a
href="https://doi.org/10.1016/j.asoc.2021.107892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current study proposes a new metaheuristic algorithm based on the clan behavior of elephants, called elephant clan optimization (ECO), to solve structural optimization problems . This method is a new version of the previously developed algorithm; namely, the elephant herding optimization (EHO). While the EHO algorithm has been inspired by the behavior of elephants, the theory behind this method is based on the herding behavior of the elephants, and also the selection of random members to replace the worst members, which is far from the real-life behavior of this animal. Since elephants are animals with powerful memories and a high capability for learning, it seems that by accurately modeling the real-life behavior of this animal, a more powerful algorithm can be developed. The proposed ECO algorithm attempts to simulate the most essential individual and collective behaviors of elephants. The performance of the ECO method is evaluated by solving several structural optimization problems , including the size optimization of truss structures. The findings of the study confirm the reliable performance of the proposed ECO algorithm to expedite the convergence rate and achieve superior solutions in comparison with the EHO. Moreover, the ECO method produces better or very competitive results by consuming less computational effort compared to well-known metaheuristic methods.},
  archive      = {J_ASOC},
  author       = {Malihe Jafari and Eysa Salajegheh and Javad Salajegheh},
  doi          = {10.1016/j.asoc.2021.107892},
  journal      = {Applied Soft Computing},
  pages        = {107892},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Elephant clan optimization: A nature-inspired metaheuristic algorithm for the optimal design of structures},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained object detection method using attention
mechanism and its application in coal–gangue detection. <em>ASOC</em>,
<em>113</em>, 107891. (<a
href="https://doi.org/10.1016/j.asoc.2021.107891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the performance of object detectors is constantly improving based on benchmark assessments, how to carry out their industrial deployment remains a challenging task. Considering that fine-grained characteristic is widely existed in industrial object images, we proposed a single-shot fine-grained object detector, and applied it to coal–gangue images in coal preparation plant. Firstly, we employed grouped convolution to build a new convolutional neural network as the backbone of the detector. Secondly, a feature fusion module, which was designed based on spatial attention , was employed to optimize the features extracted by a feature pyramid network. Finally, a feature separation module, which was designed based on channel attention, was employed to alleviate the conflict in detection between the classification task and the localization task. Moreover, the effect of each component was fully demonstrated by ablation study, visualization analysis and hypothesis testing. Compared with other classical detectors, the proposed detector’s AP iou=0.5 iou=0.5 has increased by an average of approximately 10\% in coal–gangue dataset.},
  archive      = {J_ASOC},
  author       = {Ziqi Lv and Weidong Wang and Zhiqiang Xu and Kanghui Zhang and Yuhan Fan and Yang Song},
  doi          = {10.1016/j.asoc.2021.107891},
  journal      = {Applied Soft Computing},
  pages        = {107891},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fine-grained object detection method using attention mechanism and its application in coal–gangue detection},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary algorithms for multi-objective flexible job
shop cell scheduling. <em>ASOC</em>, <em>113</em>, 107890. (<a
href="https://doi.org/10.1016/j.asoc.2021.107890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-objective flexible job shop scheduling in a cellular manufacturing environment is a challenging real-world problem. This recently introduced scheduling problem variant considers exceptional parts, intercellular moves, intercellular transportation times, sequence-dependent family setup times, and recirculation requiring minimization of makespan and total tardiness, simultaneously. A previous study shows that the exact solver based on mixed-integer nonlinear programming model fails to find an optimal solution to each of the ‘medium’ to ‘large’ size instances considering even the simplified version of the problem. In this study, we present evolutionary algorithms for solving that bi-objective problem and apply genetic and memetic algorithms that use three different scalarization methods, including weighted sum, conic, and tchebycheff . The performance of all evolutionary algorithms with various configurations is investigated across forty-three benchmark instances from ‘small’ to ‘large’ size including a large real-world problem instance. The experimental results show that the transgenerational memetic algorithm using weighted sum outperforms the rest producing the best-known results for almost all bi-objective flexible job shop cell scheduling instances, in overall.},
  archive      = {J_ASOC},
  author       = {Derya Deliktaş and Ender Özcan and Ozden Ustun and Orhan Torkul},
  doi          = {10.1016/j.asoc.2021.107890},
  journal      = {Applied Soft Computing},
  pages        = {107890},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary algorithms for multi-objective flexible job shop cell scheduling},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of different dimensionality reduction methods for
the prediction of sugar content from hyperspectral images of wine grape
berries. <em>ASOC</em>, <em>113</em>, 107889. (<a
href="https://doi.org/10.1016/j.asoc.2021.107889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several dimensionality reduction techniques were applied to hyperspectral reflectance images of wine grape berries, leading a study of the machine learning models’ efficiency in the prediction of sugar content for training, validation and independent test sets, and for generalization sets with vintages not previously seen in the training phase. The results obtained across all settings were up to par, either matching or improving state-of-the-art results, and showcasing that a model capable of generalizing predictions from one vintage year to another without further training is achievable in a very accurate way. For the dimensionality reduction techniques studied, the results show that the PCA outperforms the nonlinear techniques for the case of real-world hyperspectral data while also suggesting that, for the case of hyperspectral images of wine grape berries, local nonlinear techniques more frequently have a better performance than their global nonlinear counterparts. This review highlights that more complex methods for dimensionality reduction may not be necessary for the case of hyperspectral images , since the PCA still yields the best results when using the transformed dataset for the prediction of oenological parameters.},
  archive      = {J_ASOC},
  author       = {Rui Silva and Pedro Melo-Pinto},
  doi          = {10.1016/j.asoc.2021.107889},
  journal      = {Applied Soft Computing},
  pages        = {107889},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A review of different dimensionality reduction methods for the prediction of sugar content from hyperspectral images of wine grape berries},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A spatial–temporal graph attention network approach for air
temperature forecasting. <em>ASOC</em>, <em>113</em>, 107888. (<a
href="https://doi.org/10.1016/j.asoc.2021.107888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air temperature prediction is a significant task for researchers and forecasters in the field of meteorology. In this paper, we present an innovative, deep spatial–temporal learning air temperature forecasting framework based on graph attention network and gated recurrent unit. Particularly, historical observations containing multiple environmental variables at different stations are constructed as graph signals. The original stations’ conditions and the learned attention information are all included in our model, which overcomes the flaw of the conventional graph network approach. Results of experiments on a real-world dataset demonstrate that, compared to the state-of-the-art baselines, our model achieves the best performance in terms of short-, middle- and long-term air temperature predictions.},
  archive      = {J_ASOC},
  author       = {Xuan Yu and Suixiang Shi and Lingyu Xu},
  doi          = {10.1016/j.asoc.2021.107888},
  journal      = {Applied Soft Computing},
  pages        = {107888},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A spatial–temporal graph attention network approach for air temperature forecasting},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal particle swarm optimization for feature
selection. <em>ASOC</em>, <em>113</em>, 107887. (<a
href="https://doi.org/10.1016/j.asoc.2021.107887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of feature selection (FS) is to eliminate redundant and irrelevant features and leave useful features for classification, which can not only reduce the cost of classification, but also improve the classification accuracy . Existing algorithms mainly focus on finding one best feature subset for an optimization target or some Pareto solutions that best fit multiple targets, neglecting the fact that the FS problem may have more than one best feature subset for a single target. In fact, diffident feature subsets are likely to exhibit similar classification ability, so the FS problem is also a multimodal optimization problem . This paper firstly attempts to study the FS problem from the perspective of multimodal optimization. A novel multimodal niching particle swarm optimization (MNPSO) algorithm, aiming at finding out all the best feature combinations in a FS problem is proposed. Unlike traditional niching methods, the proposed algorithm uses the Hamming distance to measure the distance between any two particles. Two niching updating strategies are adopted for multimodal FS, and the two proposed variants of MNPSO are termed MNPSO-C (using crowding clustering) and MNPSO-S (using speciation clustering) respectively. To enable the particles in the same niche to exchange information properly, the particle velocity update is modified based on the best particle in the niche instead of the traditional globally best one. An external archive is applied to store the feature subsets with the highest classification accuracy . Datasets with various dimensions of attributes have been tested. Particularly, the number of multimodal solutions and the successful rates of the proposed algorithms have been extensively analyzed and compared with the state-of-the-art algorithms. The experimental results show that the proposed algorithms can find more multimodal feature solutions and have advantages in classification accuracy.},
  archive      = {J_ASOC},
  author       = {Xiao-Min Hu and Shou-Rong Zhang and Min Li and Jeremiah D. Deng},
  doi          = {10.1016/j.asoc.2021.107887},
  journal      = {Applied Soft Computing},
  pages        = {107887},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multimodal particle swarm optimization for feature selection},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Breast cancer intelligent analysis of histopathological
data: A systematic review. <em>ASOC</em>, <em>113</em>, 107886. (<a
href="https://doi.org/10.1016/j.asoc.2021.107886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a favorable prognosis of breast cancer, early diagnosis is essential. The histopathological analysis is considered the gold standard to indicate the type of cancer. Histopathology consists of analyzing characteristics of the lesions through tissue sections stained with Hematoxylin and Eosin. During the last years, there is much interest in developing the histopathological slide analysis process. This article aims to explore recent literature related to intelligent analysis of breast cancer histopathological images , defining the taxonomy, identifying challenges, and open questions. The method is based on a systematic literature review, guided by research questions to identify relevant work and identify open problems in the literature. The present study investigates articles published in the last ten years. We are selecting and researching the most significant approaches according to pre-established criteria in the intelligent analysis of breast cancer histopathological images , resulting in a final corpus of 53 articles. As a result, we developed an updated taxonomy, identified the main challenges, public datasets, evaluation metrics , and techniques used in the studies. These results contribute to discussions about the intelligent analysis of breast cancer histopathological images and highlight some research gaps for future studies.},
  archive      = {J_ASOC},
  author       = {Felipe André Zeiser and Cristiano André da Costa and Adriana Vial Roehe and Rodrigo da Rosa Righi and Nuno Miguel Cavalheiro Marques},
  doi          = {10.1016/j.asoc.2021.107886},
  journal      = {Applied Soft Computing},
  pages        = {107886},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Breast cancer intelligent analysis of histopathological data: A systematic review},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical object detection for very high-resolution
satellite images. <em>ASOC</em>, <em>113</em>, 107885. (<a
href="https://doi.org/10.1016/j.asoc.2021.107885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection from satellite images is challenging and either computationally expensive or labor intense. Satellite images often cover large areas of more than 10 k m × 10 k m 10km×10km . They include objects of different scales, which makes it hard to detect all of them at the same image resolution. Considering that airplanes are usually located in airports , ships are often distributed in ports and sea areas, and that oil depots are typically found close to airports or ports, we propose a new hierarchical object detection framework for very high-resolution satellite images. Our framework prescribes two stages: (1) detecting airports and ports in down-sampled satellite images and (2) mapping the detected object back to the original high-resolution satellite images for detecting the smaller objects near them. In order to improve the efficiency of object detection, we further propose a contextual information based deep feature extraction approach for both of the hierarchical detection steps, as well as an inclined bounding box based arbitrarily-oriented object location mechanism suitable especially for the smaller objects. Comprehensive experiments on a public dataset and two self-assembled datasets (which we made publicly available) show the superior performance of our method compared to standalone state-of-the-art object detectors.},
  archive      = {J_ASOC},
  author       = {Zhi-Ze Wu and Xiao-Feng Wang and Le Zou and Li-Xiang Xu and Xin-Lu Li and Thomas Weise},
  doi          = {10.1016/j.asoc.2021.107885},
  journal      = {Applied Soft Computing},
  pages        = {107885},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical object detection for very high-resolution satellite images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample and feature selecting based ensemble learning for
imbalanced problems. <em>ASOC</em>, <em>113</em>, 107884. (<a
href="https://doi.org/10.1016/j.asoc.2021.107884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced problem is concerned with the performance of classifiers on the data set with severe class imbalance distribution. Traditional methods are misled by the majority samples to make the incorrect prediction and fail to make full use of minority samples. This paper is motivated to design a novel hybrid ensemble learning strategy named Sample and Feature Selection Hybrid Ensemble Learning (SFSHEL) and combine it with random forest to improve the classification performance of imbalanced data . Specifically, SFSHEL considers cluster-based stratification to undersample the majority samples and adopts sliding windows mechanism to generate a diversity of feature subsets, simultaneously. Then the weights trained through validation are assigned to different base learners and SFSHEL makes the prediction by weighted voting at last. In this manner, SFSHEL could not only guarantee the acceptable performance, but also save computational time. Furthermore, the weighting process makes SFSHEL interpret the importance of each selected feature set, which is important in the real-world scenarios. The contributions of the proposed strategy are: (1) reducing the impact of class imbalance distribution, (2) assigning based learner weights only once after the training process, and (3) generating weights of features to help interpret the importance of clinical features. In practice, the random forest is adopted as the base learner for SFSHEL, so as to build a classifier abbreviated as SFSHEL-RF. The experiments show the average performance of the proposed SFSHEL-RF on a part of KEEL dataset reaches 91.37\%, which is comparable to our previous best ECUBoost-RF method and higher than the other eleven methods. On the clinical heart failure datasets, the performance of SFSHEL-RF can stably reach the level of the top three with three indicators. The experimental results on both the standard imbalanced and clinical heart failure datasets validate the effectiveness and stability of SFSHEL-RF.},
  archive      = {J_ASOC},
  author       = {Zhe Wang and Peng Jia and Xinlei Xu and Bolu Wang and Yujin Zhu and Dongdong Li},
  doi          = {10.1016/j.asoc.2021.107884},
  journal      = {Applied Soft Computing},
  pages        = {107884},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sample and feature selecting based ensemble learning for imbalanced problems},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Surrogate-adjoint refine based global optimization method
combining with multi-stage fuzzy clustering space reduction strategy for
expensive problems. <em>ASOC</em>, <em>113</em>, 107883. (<a
href="https://doi.org/10.1016/j.asoc.2021.107883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In engineering optimization, surrogate model (SM) is widely used to replace the involved time expensive model, due to the expensive model is complex and high precise requirement caused a long calculation cycle. In traditional process of engineering optimization, the separation of the surrogate model static construction stage and dynamic optimization stage depresses the optimization accuracy and efficiency. Moreover, in order to ensure the accuracy of the surrogate model, expensive model had to be intensively invoked to get enough representative samples in the design space for the SM training. In this paper, a surrogate model adjoint refine based global optimization method combining with the multi-stage fuzzy clustering space reduction strategy (MFCPR-SGO) is proposed to improve the optimization accuracy and efficiency. Firstly, the optimal Latin hypercube design method (OLHD) is used to sample in design space to assure the initial sample set with strong space filling property. Then, the design space is subdivided into three tiered subspaces by using the space reduction strategy of multi-stage fuzzy clustering , which has the ability of space focusing, space reduction and jumping out of local optimum. On this basis, the hierarchical optimization method with ADAM gradient descent is proposed to quickly and accurately search the local minimum value of the objective function in each subspaces. At the same time, combined with the extremum sampling and the gaussian process sampling, a dynamic sampling algorithm is given to realize the synchronization of optimization and surrogate model update. Finally, the benchmark test problems in 12 different dimensions are used to verify the proposed method. The results show that the optimization accuracy can be improved by 21.3\% and expensive model invoking times are reduced by 31.5\% compared with other three heuristic optimization methods and the three recent surrogate-based optimization (SGO) algorithms. It indicated that the optimization precision and efficiency can be greatly improved by synchronizing the dynamic updating of the surrogate model with the engineering optimization search.},
  archive      = {J_ASOC},
  author       = {Kai Wu and Faping Zhang and Yun He Zhang and Yan Yan and Shahid Ikramullah Butt},
  doi          = {10.1016/j.asoc.2021.107883},
  journal      = {Applied Soft Computing},
  pages        = {107883},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-adjoint refine based global optimization method combining with multi-stage fuzzy clustering space reduction strategy for expensive problems},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying critical causal criteria of green supplier
evaluation using heterogeneous judgements: An integrated approach based
on cloud model and DEMATEL. <em>ASOC</em>, <em>113</em>, 107882. (<a
href="https://doi.org/10.1016/j.asoc.2021.107882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing awareness of environmental protection, green supplier selection as an indispensable part of green supply chain management has received extensive attention. Effective and reliable green supplier evaluation criteria are crucial to the success of green supplier selection. Therefore, identifying the critical criteria and determining the causality of these criteria is an important requirement of stakeholders. However, the existing approaches on identifying critical causal criteria suffer at least two weaknesses: firstly it assumes the same cognitive levels between different decision makers by using a pre-determined and uniformed formation to characterize evaluation judgements, which may cause potential decision bias; secondly there is a lack of effective methods to analyse and identify critical causal criteria in the face of heterogeneous judgements, potentially causing the duplication consideration of the impacts of some criteria in green supplier selection. To address these issues, this study proposes a new approach integrating cloud model and DEMATEL (decision making trial and evaluation laboratory) to determine critical causal criteria for green supplier evaluation with qualitative heterogeneous judgements. The contribution of this study is threefold. First, to address the difficulty in processing uncertain and heterogeneous judgements, the cloud model theory is utilized and further developed to convert heterogeneous qualitative judgements into homogeneous quantitative data with the form of interval integrated clouds, which realizes the flow of uncertainty from qualitative judgements to quantitative data. Second, to enable the identification of critical causal criteria, the DEMATEL method is extended to accommodate the cloud model environment to solve the identification problem. Third, a case study, followed by a comparison analysis is provided to illustrate the applicability and advantages of the proposed approach. The results indicate that the proposed approach can handle heterogeneous judgements effectively as well as that staff environmental training, green production innovation, green marketing and green corporate culture are the critical causal criteria for the given application.},
  archive      = {J_ASOC},
  author       = {Hengxia Gao and Yanbing Ju and Ernesto D.R. Santibanez Gonzalez and Xiao-Jun Zeng and Peiwu Dong and Aihua Wang},
  doi          = {10.1016/j.asoc.2021.107882},
  journal      = {Applied Soft Computing},
  pages        = {107882},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying critical causal criteria of green supplier evaluation using heterogeneous judgements: An integrated approach based on cloud model and DEMATEL},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of motor imagery electroencephalogram signals
by using a divergence based convolutional neural network. <em>ASOC</em>,
<em>113</em>, 107881. (<a
href="https://doi.org/10.1016/j.asoc.2021.107881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are observed to be successful in pattern classification. However, the high classification performances of DNNs are related to their large training sets. Unfortunately, in the literature, the datasets used to classify motor imagery (MI) electroencephalogram (EEG) signals contain a small number of samples. To achieve high performances with small-sized datasets, most of the studies have employed a transformation such as the common spatial patterns (CSP) before the classification process. However, the CSP is dependent on subjects and introduces computational load in real time applications . It is observed in the literature that data augmentation is not applied for increasing the classification performance of EEG signals. In this study, we have investigated the effect of the augmentation process on the classification performance of MI EEG signals instead of using a preceding transformation such as the CSP, and we have demonstrated that the augmentation process is able to compete with the CSP by generating high success rates for the classification of MI EEGs. In addition to the augmentation process, we have modified the DNN structure to increase the classification performance, to decrease the number of nodes in the structure, and to use less number of hyper parameters. A minimum distance network following the last layer of the convolutional neural network (CNN) was used as the classifier instead of a fully connected neural network (FCNN). By augmenting the EEG dataset and focusing solely on CNN’s training, the training algorithm of the proposed structure is strengthened without applying any transformation. We tested these improvements on brain–computer interface (BCI) competitions 2005 and 2008 databases with two and four classes, and the positive effects of the augmentation on the average accuracies are demonstrated.},
  archive      = {J_ASOC},
  author       = {Zümray Dokur and Tamer Olmez},
  doi          = {10.1016/j.asoc.2021.107881},
  journal      = {Applied Soft Computing},
  pages        = {107881},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification of motor imagery electroencephalogram signals by using a divergence based convolutional neural network},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary-mean shift algorithm for dynamic multimodal
function optimization. <em>ASOC</em>, <em>113</em>, 107880. (<a
href="https://doi.org/10.1016/j.asoc.2021.107880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many dynamic optimization algorithms based on metaheuristic methods have been proposed. Although these schemes are highly efficient in determining a single global optimum, they fail in locating multiple optimal solutions. The central goal of dynamic multimodal optimization is to detect multiple optimal solutions for an optimization problem where its objective function is modified over time. Locating many optimal solutions (global and local) in a dynamic multimodal optimization problem is particularly crucial for several applications since the best solution could not always be the best implementable alternative due to various practical limitations. In spite of its importance, the problem of dynamic multimodal optimization through evolutionary principles has been scarcely considered in the literature. On the other hand, mean shift is a non-parametric and iterative process for detecting local maxima in a density function represented by a set of samples. Mean shift maintains interesting adaptive characteristics that allow it to find local maxima under dynamic environments. In this paper, the mean shift scheme is proposed to detect global and local optima in dynamic optimization problems. In the proposed method, the search strategy of the mean shift is modified to consider not only the density but also the fitness value of the candidate solutions. A competitive memory, along with a dynamic strategy, has also been added to accelerate the convergence process by using important information from previous environments. As a result, the proposed approach can effectively identify most of the global and local optima in dynamic environments. To demonstrate the performance of the proposed algorithm, a set of comparisons with other well-known dynamic optimization methods has been conducted. The study considers the benchmark generator of the CEC competition for dynamic optimization. The experimental results suggest a very competitive performance of the proposed scheme in terms of accuracy and robustness.},
  archive      = {J_ASOC},
  author       = {Erik Cuevas and Jorge Gálvez and Miguel Toski and Karla Avila},
  doi          = {10.1016/j.asoc.2021.107880},
  journal      = {Applied Soft Computing},
  pages        = {107880},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary-mean shift algorithm for dynamic multimodal function optimization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sustainably resilient supply chains evaluation in public
transport: A fuzzy chance-constrained two-stage DEA approach.
<em>ASOC</em>, <em>113</em>, 107879. (<a
href="https://doi.org/10.1016/j.asoc.2021.107879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to today’s highly competitive market environments, substantial attention has been focused on sustainably resilient supply chains (SCs) over the last few years. Nevertheless, very few studies have focused on the efficiency evaluation analysis of the sustainability and resilience of SCs as an inevitable essential in any profitable business. This study aims to address this issue by proposing a novel fuzzy chance-constrained two-stage data envelopment analysis (DEA) model as an advanced and rigorous approach in the performance evaluation of sustainably resilient SCs. To the best of our knowledge, the current study is pioneering as it introduces a new fuzzy chance-constrained two-stage method that can be used to undertake the deterministic non-fuzzy programming of the proposed model. The proposed approach is validated and applied to evaluate a real case study including 21 major public transport providers in three megacities. The results demonstrate the advantages of the proposed approach in comparison to the existing approaches in the literature.},
  archive      = {J_ASOC},
  author       = {Mohammad Izadikhah and Majid Azadi and Mehdi Toloo and Farookh Khadeer Hussain},
  doi          = {10.1016/j.asoc.2021.107879},
  journal      = {Applied Soft Computing},
  pages        = {107879},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sustainably resilient supply chains evaluation in public transport: A fuzzy chance-constrained two-stage DEA approach},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimal cascaded recurrent neural network for intelligent
COVID-19 detection using chest x-ray images. <em>ASOC</em>,
<em>113</em>, 107878. (<a
href="https://doi.org/10.1016/j.asoc.2021.107878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, COVID-19, has a great impact on the healthcare sector and results in a wide range of respiratory illnesses. It is a type of Ribonucleic acid (RNA) virus, which affects humans as well as animals. Though several artificial intelligence-based COVID-19 diagnosis models have been presented in the literature, most of the works have not focused on the hyperparameter tuning process. Therefore, this paper proposes an intelligent COVID-19 diagnosis model using a barnacle mating optimization (BMO) algorithm with a cascaded recurrent neural network (CRNN) model, named BMO-CRNN. The proposed BMO-CRNN model aims to detect and classify the existence of COVID-19 from Chest X-ray images. Initially, pre-processing is applied to enhance the quality of the image. Next, the CRNN model is used for feature extraction, followed by hyperparameter tuning of CRNN via the BMO algorithm to improve the classification performance. The BMO algorithm determines the optimal values of the CRNN hyperparameters namely learning rate, batch size, activation function , and epoch count. The application of CRNN and hyperparameter tuning using the BMO algorithm shows the novelty of this work. A comprehensive simulation analysis is carried out to ensure the better performance of the BMO-CRNN model, and the experimental outcome is investigated using several performance metrics. The simulation results portrayed that the BMO-CRNN model has showcased optimal performance with an average sensitivity of 97.01\%, specificity of 98.15\%, accuracy of 97.31\%, and F-measure of 97.73\% compared to state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {K. Shankar and Eswaran Perumal and Vicente García Díaz and Prayag Tiwari and Deepak Gupta and Abdul Khader Jilani Saudagar and Khan Muhammad},
  doi          = {10.1016/j.asoc.2021.107878},
  journal      = {Applied Soft Computing},
  pages        = {107878},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimal cascaded recurrent neural network for intelligent COVID-19 detection using chest X-ray images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modified a* algorithm integrated with ant colony
optimization for multi-objective route-finding; case study: yazd.
<em>ASOC</em>, <em>113</em>, 107877. (<a
href="https://doi.org/10.1016/j.asoc.2021.107877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, MASA (Modified A* Algorithm) method is introduced which can merge numerous factors with different weights into a multi-weighted graph to offer the most optimal route for a pedestrian tourist. Several mobile applications support personalized tour recommendations in making feasible plans, however, tours should not only make visiting attractions possible via the shorter route but also satisfy the unique needs of individuals with different preferences. MASA integrates the Ant Colony Optimization algorithm for arranging points of interest (POI), with a modified A* algorithm for generating multi-weighted graphs through pairs of POIs to propose the most suitable tour and facilitate traversal for a tourist who does not get involved with riding vehicles. Our contribution in this research is to consider multiple factors simultaneously or customize the route based on the user’s choice, and introduce pareto front chart for selecting optimal links, which has not been solved yet for being a challenging as well as complicated case. The user in this paper is authorized to choose preferred POIs, defined the importance of each parameter to affect route finding. A historical region in Yazd is selected as the case study due to its winding streets to show the solidity of our approach, with four main factors including Length, Communion, Relaxation, and Isovist, and then three routes have been generated considering customized selections along with an overall route considering all objectives. The computational results have shown the capability of the new proposed algorithm in route finding.},
  archive      = {J_ASOC},
  author       = {Leila Pasandi and Mehrnaz Hooshmand and Morteza Rahbar},
  doi          = {10.1016/j.asoc.2021.107877},
  journal      = {Applied Soft Computing},
  pages        = {107877},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified a* algorithm integrated with ant colony optimization for multi-objective route-finding; case study: Yazd},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection in a neighborhood decision information
system with application to single cell RNA data classification.
<em>ASOC</em>, <em>113</em>, 107876. (<a
href="https://doi.org/10.1016/j.asoc.2021.107876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A neighborhood information system (NIS) deals with an information system (IS) by means of neighborhoods. Sometimes it has some advantages over an IS. A neighborhood decision information system (NDIS) means a NIS with decision attributes . Single cell RNA (scRNA) data possess the characteristics of high dimensionality , small sample, unbalanced distribution, big noise and high redundancy. It has become an important research topic to select suitable and effective genes. This paper studies feature selection in a NDIS and considers its application for scRNA data classification . We first give the distance between information values on each attribute in a NDIS. Then, we present tolerance relations on the object set of a NDIS based on this distance. Next, we define the rough approximations in a NDIS by means of the presented tolerance relations . Furthermore, we put forward the notions of δ δ -dependence degree, δ δ -information entropy, δ δ -conditional information entropy and δ δ -joint information entropy in a NDIS. Based on Kryszkiewicz’s ideal, we introduce δ δ -generalized decision and consider feature selection in a consistent NDIS by decision. Finally, we study feature selection in a consistent NDIS by using dependence degree and information entropy, and design the relevant algorithms. The experimental results conducted several scRNA data demonstrate that the designed algorithms possess excellent performance.},
  archive      = {J_ASOC},
  author       = {Jie Zhang and Gangqiang Zhang and Zhaowen Li and Liangdong Qu and Ching-Feng Wen},
  doi          = {10.1016/j.asoc.2021.107876},
  journal      = {Applied Soft Computing},
  pages        = {107876},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection in a neighborhood decision information system with application to single cell RNA data classification},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A combined intelligent and game theoretical methodology for
collaborative multicenter pickup and delivery problems with time window
assignment. <em>ASOC</em>, <em>113</em>, 107875. (<a
href="https://doi.org/10.1016/j.asoc.2021.107875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative multicenter pickup and delivery problems with time window assignment (CMPDPTWA) are introduced in this paper and formulated and solved as a two-stage optimization problem . First, the collaborative mechanism achieves optimal transportation resource configuration in a multicenter logistics network. Second, open-closed mixed pickup and delivery routes satisfying the demands and time windows of customers are proposed to enhance transportation efficiency and reduce the total logistics operating cost. A set of candidate-time windows is assigned to the corresponding customers through time window assignment (TWA) strategy to increase the operational efficiency of logistics network. A bi-objective programming model for CMPDPTWA is formulated to minimize the number of vehicles and total operating cost. An improved k -means clustering algorithm is used to cluster customers according to their proximities to pickup and delivery centers for reducing the computational complexity to solve the abovementioned problem. A self-learning non-dominated sorting genetic algorithm-II (SNSGA-II) is proposed to optimize the open-closed mixed pickup and delivery routes in CMPDPTWA. An update mechanism for the probabilities of crossover and mutation is devised to improve the efficiency of searching the solution space, and thus to accelerate the convergence of SNSGA-II. The performance of the proposed algorithm is compared with that of other algorithms, and the results indicate that it can generate near-optimal solutions for CMPDPTWA. A game theoretical method involving a cost gap allocation model and the strictly monotonic path strategy is proposed to find the best profit allocation scheme and the optimal coalition sequence for stabilizing the collaborative coalition. Finally, the proposed method is applied to a case study using the real-world logistics network in Chongqing City, China. Six scenarios with and without the collaborative mechanism and TWA strategy are simulated and compared with one another to verify the efficacy of the proposed method. The results show that the collaborative mechanism and TWA strategy can improve the utilization of transportation resources and operational efficiency. Thus, they can facilitate an efficient and sustainable urban logistics network.},
  archive      = {J_ASOC},
  author       = {Yong Wang and Xiuwen Wang and Xiangyang Guan and Qin Li and Jianxin Fan and Haizhong Wang},
  doi          = {10.1016/j.asoc.2021.107875},
  journal      = {Applied Soft Computing},
  pages        = {107875},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combined intelligent and game theoretical methodology for collaborative multicenter pickup and delivery problems with time window assignment},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time series classification and creation of 2D bifurcation
diagrams in nonlinear dynamical systems using supervised machine
learning methods. <em>ASOC</em>, <em>113</em>, 107874. (<a
href="https://doi.org/10.1016/j.asoc.2021.107874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes new methods of computing 2D bifurcation diagrams for nonlinear time series using MultiLayer Perceptrons (MLPs), LSTM Fully Convolutional Networks (LSTM-FCN), Time Series Forests (TSFs) with entropy, Gini impurity, and K-Nearest Neighbors (KNNs) algorithm with Dynamic Time Warping (DTW). The proposed algorithms can precisely compute 2D bifurcation diagrams for oscillatory time-series (periodic or chaotic) obtained either as solutions of nonlinear systems of ordinary differential equations (ODEs) or measured and recorded when a mathematical model is not known. Illustrative computational examples include chaotic electric arc RLC circuits. The obtained results confirm usefulness of the proposed methods in a creation of 2D bifurcation diagrams — color images representing dynamics of nonlinear processes, circuits or systems.},
  archive      = {J_ASOC},
  author       = {Salama Hassona and Wieslaw Marszalek and Jan Sadecki},
  doi          = {10.1016/j.asoc.2021.107874},
  journal      = {Applied Soft Computing},
  pages        = {107874},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time series classification and creation of 2D bifurcation diagrams in nonlinear dynamical systems using supervised machine learning methods},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel CFN-watchdog protocol for edge computing.
<em>ASOC</em>, <em>113</em>, 107873. (<a
href="https://doi.org/10.1016/j.asoc.2021.107873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute first networking (CFN) is a latest distributed framework that intelligently allocates computing resources for edge computing according to computing load and network status. It requires real-time visibility of available statuses of local or remote computing resources. To the best of our knowledge, this paper is the first to propose a centralized fault-detection protocol called CFN-Watchdog to well meet this CFN requirement and timely recycle resources occupied by faults. We then theoretically analyze the impact of various parameters (e.g., detection thresholds, task processing time, and network delay) on the Watchdog performance. Extensive simulations verify the effectiveness of our proposed protocol and the accuracy of our theoretical model. This study is very helpful to optimize parameter configurations and better design fault-detection protocols for edge computing .},
  archive      = {J_ASOC},
  author       = {Hong Liang and Li Feng and Fangxin Xu and Guangcheng Li and Jie Xu and Yuqiang Chen},
  doi          = {10.1016/j.asoc.2021.107873},
  journal      = {Applied Soft Computing},
  pages        = {107873},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel CFN-watchdog protocol for edge computing},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-heuristic as manager in federated learning approaches
for image processing purposes. <em>ASOC</em>, <em>113</em>, 107872. (<a
href="https://doi.org/10.1016/j.asoc.2021.107872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new form of artificial intelligence training, i.e. federated learning , is becoming more popular in the last few years. It is an optimization problem that includes additional mechanisms such as aggregation and data transmission. In this paper, we propose a hybridization of this type of training with a meta-heuristic. The meta-heuristic algorithm is adapted to manage the entire process as well as to analyze the best models to minimize attacks on this type of collaboration. The proposed solution is based on minimizing the general model error, with additional control mechanisms for incoming models, or adapting the aggregation method depending on the quality of the model. The innovative solution has been analyzed in terms of its application to the problem of image classification using classical and convolutional neural networks , and the most popular meta-heuristic algorithms. The proposal was analyzed in terms of the accuracy of the general model as well as for security against poisoning attacks. We reached 91\% of accuracy using the proposed method with the Red Fox Optimization Algorithm and 95\% in terms of detection of poisoned samples in the database.},
  archive      = {J_ASOC},
  author       = {Dawid Połap and Marcin Woźniak},
  doi          = {10.1016/j.asoc.2021.107872},
  journal      = {Applied Soft Computing},
  pages        = {107872},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Meta-heuristic as manager in federated learning approaches for image processing purposes},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic credit scoring strategy (ACSS) using memetic
evolutionary algorithm and neural architecture search. <em>ASOC</em>,
<em>113</em>, 107871. (<a
href="https://doi.org/10.1016/j.asoc.2021.107871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit scoring is playing an increasingly critical role with the rising number of lending operations for micro and small enterprises as well as individuals. A large number of research is primarily based on the combination and construction methods of credit scoring models by the experts. However, different credit data have distinct requirements for the models, and how to automatically search and construct credit scoring models according to credit data has become essential, that is the main concern of this paper. In response to the current challenges for credit scoring research, we proposed an Automatic Credit Scoring Strategy (ACSS), designed a credit assessment platform which includes data import, classification model automatic search, feature selection, hyperparameter optimization, data mining, classification output and other modules. Aiming at the problem of substantial imbalance in credit data, we propose an improved SMOTE algorithm that is capable of generating supplementary data for the lack of minority in credit data, thereby making the credit data distribution well balanced. As for classification model selection, features engineering, parameter optimization and other parts, we further incorporate automatic search ways to reduce manual interaction. We utilize public and self-owned credit data sets to conduct experiments and compare them with the latest credit assessment methods. Extensive experiments have demonstrated that our ACSS method achieves relatively noticeable performance improvements using the German credit dataset, the Taiwan credit dataset and the personal credit dataset for credit scoring. The best results achieved by our proposed method are 0.98, 0.99, 0.895 and 0.901 for MAE , RMSE , accuracy and precision respectively. In addition, the experimental results also show that our proposed improved SMOTE algorithm contributes to the credit scoring performance enhancement. The experimental findings suggest that our proposed ACSS balances automation and accuracy, which can be implemented to the consumption industry to enhance the reliability of credit assessments.},
  archive      = {J_ASOC},
  author       = {Fan Yang and Yanan Qiao and Cheng Huang and Shan Wang and Xiao Wang},
  doi          = {10.1016/j.asoc.2021.107871},
  journal      = {Applied Soft Computing},
  pages        = {107871},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An automatic credit scoring strategy (ACSS) using memetic evolutionary algorithm and neural architecture search},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inter-release defect prediction with feature selection using
temporal chunk-based learning: An empirical study. <em>ASOC</em>,
<em>113</em>, 107870. (<a
href="https://doi.org/10.1016/j.asoc.2021.107870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inter-release defect prediction (IRDP) is a practical scenario that employs the datasets of the previous release to build a prediction model and predicts defects for the current release within the same software project. A practical software project experiences several releases where data of each release appears in the form of chunks that arrive in temporal order. The evolving data of each release introduces new concept to the model known as concept drift, which negatively impacts the performance of IRDP models. In this study, we aim to examine and assess the impact of feature selection (FS) on the performance of IRDP models and the robustness of the model to concept drift. We conduct empirical experiments using 36 releases of 10 open-source projects. The Friedman and Nemenyi Post-hoc test results indicate that there were statistical differences between the prediction results with and without FS techniques. IRDP models trained on the data of most recent releases were not always the best models. Furthermore, the prediction models trained with carefully selected features could help reduce concept drifts.},
  archive      = {J_ASOC},
  author       = {Md Alamgir Kabir and Jacky Keung and Burak Turhan and Kwabena Ebo Bennin},
  doi          = {10.1016/j.asoc.2021.107870},
  journal      = {Applied Soft Computing},
  pages        = {107870},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inter-release defect prediction with feature selection using temporal chunk-based learning: An empirical study},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Many-objective evolutionary algorithm based on dominance
degree. <em>ASOC</em>, <em>113</em>, 107869. (<a
href="https://doi.org/10.1016/j.asoc.2021.107869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many-objective optimization problems , the comparability of non-dominated solutions is always an essential and fundamental issue. Due to the inefficiency of Pareto dominance for many-objective optimization problems , various improved dominance relations have been proposed to enhance the evolutionary pressure. However, these variants have one thing in common that they treat each solution in a static manner, and the relations between any two solutions are just defined as a kind of static spatial adjacencies, resulting in the unquantifiable comparability . Different from them, this paper proposes a dominance degree metric, which treats solutions as different stages of a dynamic motion process. The dynamic motion process represents the continuous changes of the degree of one solution from Pareto dominating others to being Pareto dominated by others. Based on the dominance degree, this paper proposes a Many-Objective Evolutionary Algorithm based on Dominance Degree, in which the mating selection and environmental update strategies are redesigned accordingly. The proposed method is comprehensively tested with several state-of-the-art optimizers on two popular test suites and practical multi-point distance minimization problems . Experimental results demonstrate its effectiveness and superiority over other optimizers in terms of the convergence, diversity and spread.},
  archive      = {J_ASOC},
  author       = {Maoqing Zhang and Lei Wang and Weian Guo and Wuzhao Li and Junwei Pang and Jun Min and Hanwei Liu and Qidi Wu},
  doi          = {10.1016/j.asoc.2021.107869},
  journal      = {Applied Soft Computing},
  pages        = {107869},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Many-objective evolutionary algorithm based on dominance degree},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SHADE–WOA: A metaheuristic algorithm for global
optimization. <em>ASOC</em>, <em>113</em>, 107866. (<a
href="https://doi.org/10.1016/j.asoc.2021.107866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution and its variants have already proven their worth in the field of evolutionary optimization techniques. This study further enhances the success history-based adaptive differential evolution (SHADE) by hybridizing it with a modified Whale optimization algorithm (WOA). In the new algorithm, the two algorithms, SHADE and modified WOA, carry out the search process independently and share information like the global best solution and whole population and thus guides both the algorithms to explore and exploit new promising areas in the search space. It also reduces the chance of being trapped in local optima and stagnation. The proposed algorithm (SHADE–WOA) is tested, evaluating CEC 2017 functions using dimensions 30, 50, and 100. The results are compared with modified DE algorithms , namely SaDE, SHADE, LSHADE, LSHADE-SPACMA, and LSHADE-cnEpSin, also with modified WOA algorithms, namely ACWOA, AWOA, IWOA, HIWOA, and MCSWOA. The new algorithm’s efficiency in solving real-world problems is examined by solving two unconstrained and four constrained engineering design problems . The performance is verified statistically using non-parametric statistical tests like Friedman’s test and Wilcoxon’s test. Analysis of numerical results, convergence analysis , diversity analysis, and statistical analysis ensures the enhanced performance of the proposed SHADE–WOA.},
  archive      = {J_ASOC},
  author       = {Sanjoy Chakraborty and Sushmita Sharma and Apu Kumar Saha and Sandip Chakraborty},
  doi          = {10.1016/j.asoc.2021.107866},
  journal      = {Applied Soft Computing},
  pages        = {107866},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SHADE–WOA: A metaheuristic algorithm for global optimization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Hierarchical polynomial-based fuzzy neural networks driven
with the aid of hybrid network architecture and ranking-based neuron
selection strategies. <em>ASOC</em>, <em>113</em>, 107865. (<a
href="https://doi.org/10.1016/j.asoc.2021.107865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose hierarchical polynomial-based fuzzy neural networks (HPFNN). The aim of this study is to develop the design methodologies of hierarchical model to improve the prediction accuracy of the model without sacrificing computational efficiency through combining hybrid network architecture composed of different neurons and ranking-based neuron selection strategies. The essential bullets of the proposed model can be enumerated as follows: (a) A hybrid network architecture is designed by combining the traits of fuzzy rule-based neurons with random vector functional link (FRN-RVFL) and polynomial neurons. FRN-RVFL is employed to erect the first layer of network. The entire network topology and the neurons of the remaining layers are constructed with polynomial neural network . (b) Two kinds of ranking-based neuron selection (RNS) strategies such as Linear-RNS (LRNS) and Exponential-RNS (EPNS) are presented. Compared with traditional neuron selection strategy, RNS can enrich the diversity of candidate neurons while maintaining the approximation ability of neurons, which provides opportunities for selecting neurons with predictive potential. (c) Regularization-based least square approach is applied to alleviate the possible overfitting in coefficient estimation as well as enhance generalization abilities of the model. The performance of HPFNN is verified by using a series of synthetic data and machine learning datasets. Based on the experimental results, HPFNN exhibits sound prediction accuracy and reasonable computational cost in contrast with the same type of hierarchical models. Furthermore, HPFNN achieves better generalization performance on at least 12 of 20 datasets when compared with the performance of state-of-the-art models.},
  archive      = {J_ASOC},
  author       = {Congcong Zhang and Sung-Kwun Oh and Zunwei Fu},
  doi          = {10.1016/j.asoc.2021.107865},
  journal      = {Applied Soft Computing},
  pages        = {107865},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical polynomial-based fuzzy neural networks driven with the aid of hybrid network architecture and ranking-based neuron selection strategies},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical problem decomposition — the key to the
evolutionary effectiveness in solving a large-scale non-binary discrete
real-world problem. <em>ASOC</em>, <em>113</em>, 107864. (<a
href="https://doi.org/10.1016/j.asoc.2021.107864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider an NP-hard, real-world optimization problem from the field of computer networks. The problem refers to the network survivability and may be considered hard due to its scale. Such problems are usually successfully solved by various Genetic Algorithms (GAs). The recent advances in the development of Evolutionary Algorithms (EAs) dedicated to solving discrete-encoded problems show that GAs employing modern linkage learning techniques seem to have exceptional potential in proposing high-quality results. Many of such GAs are the state-of-the-art methods for various optimization problems, including various domain types and single- and multi-objective optimization problems. These GAs have also been shown effective in solving theoretical and practical problems. Thus, linkage learning development may lead to proposing GAs that are significantly more effective than their predecessors. Therefore, we develop the recently proposed linkage learning technique, namely the linkage learning based on local optimization (3LO) that is (to the best of our knowledge) based on fundamentally different ideas than all other linkage-based problem decomposition techniques. Due to these differences, 3LO has some exceptional features — it is proven that it will never commit some of the possible problem decomposition inconsistencies. However, 3LO was only proposed for binary-encoded problems. Therefore, to fill this gap and adjust 3LO to the considered problem, we propose linkage learning based on local optimization for discrete non-binary search spaces (3LO-nb). The obtained results show that 3LO-nb allows for reaching excellent results. Additionally, our proposition is generic, and its effectiveness may be verified on other non-binary discrete problems.},
  archive      = {J_ASOC},
  author       = {Michal Witold Przewozniczek and Marcin Michal Komarnicki},
  doi          = {10.1016/j.asoc.2021.107864},
  journal      = {Applied Soft Computing},
  pages        = {107864},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Empirical problem decomposition — the key to the evolutionary effectiveness in solving a large-scale non-binary discrete real-world problem},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy surfacelet neural network evaluation model optimized
by adaptive dragonfly algorithm for pipeline network integrity
management. <em>ASOC</em>, <em>113</em>, 107862. (<a
href="https://doi.org/10.1016/j.asoc.2021.107862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve integrity management evaluation level of pipeline, the fuzzy surfacelet neural network optimized by improved dragonfly algorithm. Firstly, the domestic and foreign related research progresses of pipeline integrity management are summarized. Secondly, the pipeline integrity management evaluation index system is constructed according to management and technology characteristics of pipeline integrity management. Thirdly, the fuzzy surfacelet neural network with five layers is established by combining Surfacelet transfer, wavelet neural network and fuzzy theory. The improved dragonfly algorithm is established by improved population initialization strategy and inertia weight updating strategy. Finally, simulation analysis of pipeline integrity management is carried out based on fuzzy B-spline wavelet neural network optimized by improved particle swarm algorithm (BWNN-IPSA), fuzzy surfacelet neural network optimized by traditional dragonfly algorithm (FSNN-TDA) and fuzzy surface neural network optimized by improved dragonfly algorithm (FSNN-IDA), simulation results show that the fuzzy Surfacelet neural network optimized by improved dragonfly algorithm can achieve convergence after 500 times, it has less convergence times than other evaluation models. The mean square error of the proposed evaluation model ranges from 0.79 to 1.02\%, it has less error than other evaluation models. Therefore the proposed fuzzy surface neural network optimized by improved dragonfly algorithm has higher computing precision and efficiency. The proposed evaluation model of pipeline integrity management can improve intelligent level of pipeline management, and ensure the safety and reliability of pipeline system.},
  archive      = {J_ASOC},
  author       = {Jiaming Sun and Bin Zhao and Diankui Gao and Lizhi Xu},
  doi          = {10.1016/j.asoc.2021.107862},
  journal      = {Applied Soft Computing},
  pages        = {107862},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy surfacelet neural network evaluation model optimized by adaptive dragonfly algorithm for pipeline network integrity management},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emotion-infused deep neural network for emotionally resonant
conversation. <em>ASOC</em>, <em>113</em>, 107861. (<a
href="https://doi.org/10.1016/j.asoc.2021.107861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread development of conversational agents (chatbots) has enabled us to communicate and collaborate with different forms and functions of robots using natural language, thus facilitating a closer relationship between humans and technology. Given that chatbot services infused with domain knowledge are of great interest to not only global businesses but also academics, chatbots have in recent years become a popular research topic in the field of natural language processing . We therefore aim at improving current chatbots with the addition of natural emotions. In contrast to previous work, we intend to distinguish fine-grained emotion differences between words in order to better understand emotion expressions in sentences. Our approach infuses fine-grained emotion content into the response generation process to make the dialog more emotionally resonant. The experimental results demonstrate that this method can classify emotions more effectively. In addition, the proposed hybrid model, which consists of recurrent and convolutional neural networks with additional emotion-specific valence-arousal features, can correctly identify five emotions with a 67.89\% overall F1-score. We further evaluate the subjective quality of the responses and discover that the infusion of fine-grained emotion information substantially improves the quality and fluency of automatically generated empathetic conversation. We conclude that the proposed model can greatly improve the efficiency and usability of a conversational chatbot system.},
  archive      = {J_ASOC},
  author       = {Yung-Chun Chang and Yan-Chun Hsing},
  doi          = {10.1016/j.asoc.2021.107861},
  journal      = {Applied Soft Computing},
  pages        = {107861},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Emotion-infused deep neural network for emotionally resonant conversation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RFPruning: A retraining-free pruning method for accelerating
convolutional neural networks. <em>ASOC</em>, <em>113</em>, 107860. (<a
href="https://doi.org/10.1016/j.asoc.2021.107860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning has been developed as a remedy for accelerating the inference of deep convolutional neural networks (DCNNs). The mainstream methods retrain the pruned models, which maintain the performance of the pruned models but consume a great deal of time. While the other methods reduce the time consumption by omitting to retrain, they lose the performance. To resolve the above conflicts, we propose a two-stage R etraining- F ree pruning method, named RFPruning , which embeds the rough screening of channels into training and fine-tunes the structures during pruning, to achieve both good performance and low time consumption. In the first stage, the network training is reformulated as an optimization problem with constraints and solved by a sparse learning approach for rough channel selection. In the second stage, the pruning process is regarded as a multiobjective optimization problem , where the genetic algorithm is applied to carefully select channels for a trade-off between the performance and model size. The proposed method is evaluated against several DCNNs on CIFAR-10 and ImageNet datasets. Extensive experiments demonstrate that such a retraining-free pruning method obtains 43.0\% ∼ ∼ 88.4\% compression on model size and maintains the accuracy as the methods with retraining while achieving 3 × 3× speed up in pruning.},
  archive      = {J_ASOC},
  author       = {Zhenyu Wang and Xuemei Xie and Guangming Shi},
  doi          = {10.1016/j.asoc.2021.107860},
  journal      = {Applied Soft Computing},
  pages        = {107860},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RFPruning: A retraining-free pruning method for accelerating convolutional neural networks},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Securing smart cities using LSTM algorithm and lightweight
containers against botnet attacks. <em>ASOC</em>, <em>113</em>, 107859.
(<a href="https://doi.org/10.1016/j.asoc.2021.107859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Cities contains millions of IoT sensors supporting critical applications such as Smart Transport, Buildings, Intelligent Vehicles , and Logistics. A central administrator appointed by the government manages and maintains the security of each node. Smart City relies upon millions of sensors that are heterogeneous and do not support standard security architecture. Different manufacturers have weak protection protocols for their products and do not update their firmware upon newly identified operating systems’ vulnerabilities. Adversaries using brute force methods exploit the lack of inbuilt security systems on IoT devices to grow their bot network. Smart cities require a standard framework combining soft computing and Deep Learning (DL) for device fleet management and complete control of sensor operating systems for absolute security. This paper presents a real-world application for IoT fleet management security using a lightweight container-based botnet detection (C-BotDet) framework. Using a three-phase approach, the framework using Artificial Intelligence detects compromised IoT devices sending malicious traffic on the network. Balena Cloud revokes API keys and prevents a compromised device from infecting other devices to form a more giant botnet . VPN (Virtual Private Network) prevents inter-device communication and routes all malicious traffic through an external server . The framework quickly updates the standard Linux-based operating system IoT device fleet without relying on different manufacturers to update their system security individually. The simulation and analysis of the C-BotDet framework are presented in a practical working environment to demonstrate its implementation feasibility.},
  archive      = {J_ASOC},
  author       = {Mikail Mohammed Salim and Sushil Kumar Singh and Jong Hyuk Park},
  doi          = {10.1016/j.asoc.2021.107859},
  journal      = {Applied Soft Computing},
  pages        = {107859},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Securing smart cities using LSTM algorithm and lightweight containers against botnet attacks},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A time series forecasting based multi-criteria methodology
for air quality prediction. <em>ASOC</em>, <em>113</em>, 107850. (<a
href="https://doi.org/10.1016/j.asoc.2021.107850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a very extensive literature on the design and test of models of environmental pollution, especially in the atmosphere. Current and recent models, however, are focused on explaining the causes and their temporal relationships, but do not explore, in full detail, the performances of pure forecasting models. We consider here three years of data that contain hourly nitrogen oxides concentrations in the air; exposure to high concentrations of these pollutants has been indicated as potential cause of numerous respiratory, circulatory, and even nervous diseases. Nitrogen oxides concentrations are paired with meteorological and vehicle traffic data for each measure. We propose a methodology based on exactness and robustness criteria to compare different pollutant forecasting models and their characteristics. 1DCNN , GRU and LSTM deep learning models, along with Random Forest , Lasso Regression and Support Vector Machines regression models, are analyzed with different window sizes. As a result, our best models offer a 24-hours ahead, very reliable prediction of the concentration of pollutants in the air in the considered area, which can be used to plan, and implement, different kinds of interventions and measures to mitigate the effects on the population.},
  archive      = {J_ASOC},
  author       = {Raquel Espinosa and José Palma and Fernando Jiménez and Joanna Kamińska and Guido Sciavicco and Estrella Lucena-Sánchez},
  doi          = {10.1016/j.asoc.2021.107850},
  journal      = {Applied Soft Computing},
  pages        = {107850},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A time series forecasting based multi-criteria methodology for air quality prediction},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy jaccard index: A robust comparison of ordered lists.
<em>ASOC</em>, <em>113</em>, 107849. (<a
href="https://doi.org/10.1016/j.asoc.2021.107849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Fuzzy Jaccard Index ( Fuji ) — a scale-invariant score for similarity assessment of two ranked/ordered lists. Fuji improves upon the Jaccard index by incorporating a membership function that takes into account the particular ranks, thus producing both more stable and more accurate similarity estimates. We provide theoretical insights into the properties of the Fuji score as well as propose an efficient algorithm for computing it. We also present empirical evidence of its performance in different synthetic scenarios. Finally, we demonstrate its utility in a typical machine learning setting — comparing feature ranking lists, relevant to a given machine learning task. In many practical applications, in particular originating from high-dimensional domains, where only a small percentage of the whole feature space might be relevant, a robust and confident feature ranking leads to interpretable findings, efficient computation and good predictive performance . In such cases, Fuji correctly distinguishes between existing feature ranking approaches, while being more robust and efficient than the benchmark similarity scores.},
  archive      = {J_ASOC},
  author       = {Matej Petković and Blaž Škrlj and Dragi Kocev and Nikola Simidjievski},
  doi          = {10.1016/j.asoc.2021.107849},
  journal      = {Applied Soft Computing},
  pages        = {107849},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy jaccard index: A robust comparison of ordered lists},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wind speed interval prediction model based on variational
mode decomposition and multi-objective optimization. <em>ASOC</em>,
<em>113</em>, 107848. (<a
href="https://doi.org/10.1016/j.asoc.2021.107848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a potential new energy power generation technology, wind power is gradually developing into the world’s mainstream energy. In the research on wind power generation , wind speed prediction is an important part, which has been widely studied. The accurate wind speed prediction is a key part of wind power management to help wind power grid-tied. Currently, most research has focused on point prediction, which in fact does not facilitate the quantitative characterization of the endogenous uncertainty involved. However, interval prediction can avoid this deficiency and make better operation and scheduling of the wind power models. In this study, a novel interval prediction model based on wind speed distribution and multi-objective optimization is designed, which includes data noise reduction module, prediction module, and combination module based on a multi-objective salp swarm algorithm, to provide accurate forecast for power model operation and grid dispatching. The 10-minute wind speed data from three data sets in China were selected for prediction to evaluate the effectiveness of the proposed combined model. The results show that the model is not only better than the considered benchmark model , but also has good potential practical application value in wind power models.},
  archive      = {J_ASOC},
  author       = {Jianzhou Wang and Zishu Cheng},
  doi          = {10.1016/j.asoc.2021.107848},
  journal      = {Applied Soft Computing},
  pages        = {107848},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wind speed interval prediction model based on variational mode decomposition and multi-objective optimization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted differential evolution heuristics for improved
multilayer piezoelectric transducer design. <em>ASOC</em>, <em>113</em>,
107835. (<a href="https://doi.org/10.1016/j.asoc.2021.107835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Piezoelectric transducers are designed for wide applications of underwater sonar, energy harvesting , biomedical and nondestructive testing in terms of higher sensitivity and broad bandwidth. Multilayer transducers have well-known advantages of higher sensitivity and thus can be optimized by varying layer thicknesses of active material. The present study deals with mathematical optimization of multilayer transducers exploiting weighted differential evolution (WDE) based computational heuristics for optimized piezoelectric transducer design while satisfying the constraint of constant total thickness. Fitness evaluation is carried out on the basis of one dimensional model (ODM) for simple multilayer, multilayer with bondlines and transducer with bondlines and matching for piezoelectric materials including PZT5h, PMN-PT and piezocomposite with PMN-PT for different loading media. WDE exploited along with ODM for the transducer models has demonstrated the efficacy, reliability and applicability of the proposed scheme. Comparative study of the proposed WDE-ODM with its counterparts including simulated annealing, genetic algorithms , and particle swarm optimization , reveal better performance in terms of accuracy, complexity and convergence metrics for different number of layers for simple transducers. Applicability of WDE-ODM on piezoelectric structures with bondlines, and matching layers has also been accessed and validated.},
  archive      = {J_ASOC},
  author       = {Sidra Naz and Aneela Zameer and Muhammad Asif Zahoor Raja and Kehkesan Muhammad},
  doi          = {10.1016/j.asoc.2021.107835},
  journal      = {Applied Soft Computing},
  pages        = {107835},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weighted differential evolution heuristics for improved multilayer piezoelectric transducer design},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integration of balancing and preventive maintenance in
straight and u-shaped resource-dependent assembly lines: MILP model and
memetic algorithm. <em>ASOC</em>, <em>113</em>, 107773. (<a
href="https://doi.org/10.1016/j.asoc.2021.107773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In enterprises, machines are subject to some unavailable periods resulting in production halts. The consideration of preventive maintenance (PM) and resource assignment in straight and U-shaped assembly lines is of paramount importance. Hence, this paper aims to tackle the integration of PM and balancing problems in straight and U-shaped resource-dependent assembly lines via two new mixed-integer linear programming (MILP) models and a memetic algorithm . In the MILP models, the optimal PM intervals of all machines and all PM scenarios are determined according to the maximization of the availability. In the memetic algorithm , a multi-attribute solution representation, including heuristic rule matrix, task separator set and machine assignment permutation, is designed to build a feasible solution. For this multi-attribute solution representation, this algorithm respectively designs different crossover, mutation and local search operators to exploit the space around the solutions. Finally, five comparison experiments are conducted to prove the effectiveness of the MILP models in small-scale instances and the superiority of the improvements and memetic algorithm in all instances.},
  archive      = {J_ASOC},
  author       = {Zikai Zhang and Qiuhua Tang and Xinbo Qian},
  doi          = {10.1016/j.asoc.2021.107773},
  journal      = {Applied Soft Computing},
  pages        = {107773},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integration of balancing and preventive maintenance in straight and U-shaped resource-dependent assembly lines: MILP model and memetic algorithm},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Broad learning extreme learning machine for forecasting and
eliminating tremors in teleoperation. <em>ASOC</em>, <em>112</em>,
107863. (<a href="https://doi.org/10.1016/j.asoc.2021.107863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unwanted errors caused by hand tremors are a bottleneck for the application of teleoperation robots in space explorations, underwater explorations, and minimally invasive surgery . In order to eliminate hand tremor signals in teleoperation control systems, two tremor-filtering models based on artificial neural networks are defined. With the purpose of decreasing the errors of tremor filtering models, a novel Broad Learning Extreme Learning Machine with Improved Equilibrium Optimizer (IEO-BLELM) is proposed. Firstly, the structure of Extreme Learning Machine (ELM) is re-designed by coupling with broad learning. Time series and smoothing are introduced as feature extraction layer and enhancement layer, respectively. Secondly, different activation functions are selected to construct Broad Learning ELM (BLELM). An Improved Equilibrium Optimizer is introduced to optimize input weights, thresholds, and parameters of the BLELM model. To verify the performance of the IEO-BLELM model, the proposed model is applied to six examples and compared with other models. The results show that Mean Absolute Error (MAE) of the proposed model in six examples is at least lower than 0.253. As compared with the ELM, the MAE of the IEO-BLELM model can be decreased by 51.03\% through reasonable improvement strategies. In particular, estimation errors are mainly contributed to peak and the proposed model significantly reduces the peak errors. The forecasting performance of the proposed model is better than that of previously existing models. In general, this study provides effective models to eliminate hand tremor signals in teleoperation control systems.},
  archive      = {J_ASOC},
  author       = {Qiye Yang and Ke Liang and Tiecheng Su and Kuihua Geng and Mingzhang Pan},
  doi          = {10.1016/j.asoc.2021.107863},
  journal      = {Applied Soft Computing},
  pages        = {107863},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Broad learning extreme learning machine for forecasting and eliminating tremors in teleoperation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aggregation of nonlinearly enhanced experts with application
to electricity load forecasting. <em>ASOC</em>, <em>112</em>, 107857.
(<a href="https://doi.org/10.1016/j.asoc.2021.107857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining the predictions of different base experts is a well known approach used to improve the accuracy of time series forecasts. Forecast aggregation is becoming crucial in many fields, including electricity forecasting, as Internet of Things and Cloud technology give access to larger numbers of sensor data, time series and predictions from external providers. In this context, it is not uncommon that the failure of some experts causes relevant drops in the performances of the aggregated forecast when classical techniques based on linear averaging are applied. This might be a symptom of suboptimality of the individual experts, that do not fully exploit important predictors, e.g. calendar features that play a major role in the electrical demand profiles. In this work, we therefore present two non-linear strategies to obtain aggregated forecasts, starting from the availability of a set of base experts and the knowledge of some relevant predictor variables . The first approach, called aggregation of enhanced experts (AEE), enhances each individual expert and then feeds the enhanced forecasts into classical linear aggregation techniques. In the second approach, called enhanced aggregation of experts (EAE), the expert forecasts are nonlinearly combined with the predictor variables through an Artificial Neural Network (ANN). The case of missing expert forecasts is also considered via a statistically-based imputation method. The short-term prediction of German electrical load is used as a case study. Twelve base experts are enhanced with respect to calendar features, i.e. daytime and weekday. Compared to state-of-the-art aggregation methods applied to the not-enhanced set of experts, the proposed approaches not only improve the accuracy of aggregated forecast (up to 25\% reduction of MAPE and RMSE), but are also robust with respect to missing experts.},
  archive      = {J_ASOC},
  author       = {A. Incremona and G. De Nicolao and F. Fusco and B.J. Eck and S. Tirupathi},
  doi          = {10.1016/j.asoc.2021.107857},
  journal      = {Applied Soft Computing},
  pages        = {107857},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Aggregation of nonlinearly enhanced experts with application to electricity load forecasting},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semisupervised learning model based on fuzzy min–max
neural networks for data classification. <em>ASOC</em>, <em>112</em>,
107856. (<a href="https://doi.org/10.1016/j.asoc.2021.107856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised learning (SSL) models are useful for undertaking classification problems with a small set of labeled samples and a large number of unlabeled samples . In this regard, the family of fuzzy min–max (FMM) neural networks offers the capability of online learning for addressing both unsupervised and supervised problems. As such, this paper proposes a novel two-stage SSL model based on FMM networks, denoted SSL–FMM. The first stage employs the unlabeled samples to generate a number of hyperboxes using the unsupervised FMM algorithm, while the second stage uses the labeled samples to associate the generated hyperboxes with their target classes using the supervised FMM algorithm. In addition, a neighborhood-labeling mechanism based on the Euclidean distance and hyperbox centroids is formulated to associate the unlabeled hyperboxes with the most likely target classes. A number of benchmark problems and a real-world case study are employed to evaluate the effectiveness of the proposed SSL–FMM model. The outcome indicates that SSL–FMM is able to use unlabeled samples effectively and improve the FMM performance, producing promising results compared with other SSL methods in the literature.},
  archive      = {J_ASOC},
  author       = {Farhad Pourpanah and Di Wang and Ran Wang and Chee Peng Lim},
  doi          = {10.1016/j.asoc.2021.107856},
  journal      = {Applied Soft Computing},
  pages        = {107856},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semisupervised learning model based on fuzzy min–max neural networks for data classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous feature and instance selection in big noisy
data using memetic variable neighborhood search. <em>ASOC</em>,
<em>112</em>, 107855. (<a
href="https://doi.org/10.1016/j.asoc.2021.107855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In smart factories, the data collected by Internet-of-things sensors is enormous and includes a lot of noise and missing values. To address this big data problem , metaheuristic is one of the main approaches to data preprocessing , i.e., instance selection or feature selection before training the model. Most previous works on metaheuristic approaches rarely considered simultaneous instance selection and feature selection, and rarely focused on addressing big noisy data. Consequently, this work proposes a hybrid memetic algorithm (MA) with variable neighborhood search (VNS) to simultaneously select instances and features, in which MA performs excellently in data selection; and VNS has been shown to perform well in local search. To evaluate the performance of the proposed algorithm, this work creates simulation data by combining the datasets from the UCI with noisy data. The proposed algorithm for simultaneous feature and instance selection is adopted to reduce the simulation data, and then the reduced data is adopted to train a predictive model for later performance evaluation of model testing. As compared with other metaheuristics, the proposed algorithm achieves a balance between exploration and exploitation. Additionally, the results show that the proposed algorithm is more robust than other feature selection methods.},
  archive      = {J_ASOC},
  author       = {Chun-Cheng Lin and Jia-Rong Kang and Yu-Lin Liang and Chih-Chi Kuo},
  doi          = {10.1016/j.asoc.2021.107855},
  journal      = {Applied Soft Computing},
  pages        = {107855},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Simultaneous feature and instance selection in big noisy data using memetic variable neighborhood search},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-population improved whale optimization algorithm for
high dimensional optimization. <em>ASOC</em>, <em>112</em>, 107854. (<a
href="https://doi.org/10.1016/j.asoc.2021.107854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaheuristic algorithms do not depend on the functional form when solving the optimization problem . They have strong adaptability and are widely used in many fields. Whale Optimization Algorithm (WOA) is a metaheuristic algorithm based on the social behavior of humpback whales. Compared with other metaheuristic algorithms , WOA shows better performance. However, when solving high dimensional optimization problems, WOA tends to fall into local optima and has slow convergence speed and low accuracy of solution. Aiming at these problems, a multi-population improved WOA (MIWOA) is proposed to improve the performance of WOA when tackling high dimensional optimization. First of all, the multi-population exploitation and exploration processes are introduced. The population is divided into a better group and a worse group. The better individuals are used to improve exploitation performance, while the poorer individuals are used to improve exploration performance. Secondly, the current optimal individual and weighted center are taken to improve the learning process, which enhances the exploration ability and convergence speed. Moreover an interpolation method is introduced to enhance the search ability in the vicinity of the current optimum and further improve the exploitation performance. Finally, a control parameter is used to balance the exploitation and exploration processes. In the experimental part, MIWOA is compared with several state-of-the-art algorithms on 30 high dimensional benchmark functions with dimensions ranging from 100 to 2000. Simulation results show that MIWOA has good performance in both high dimensional single-mode and multi-mode optimization problems. MIWOA is superior to other algorithms in solution accuracy, convergence speed, and execution time.},
  archive      = {J_ASOC},
  author       = {Yongjun Sun and Yu Chen},
  doi          = {10.1016/j.asoc.2021.107854},
  journal      = {Applied Soft Computing},
  pages        = {107854},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-population improved whale optimization algorithm for high dimensional optimization},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of the multi-objective cluster head selection
problem in WSNs. <em>ASOC</em>, <em>112</em>, 107853. (<a
href="https://doi.org/10.1016/j.asoc.2021.107853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The target of wireless sensor networks is to gather data regarding certain phenomena. This is why they are meant to last as long as possible. An efficient strategy to extend the wireless sensor network lifetime is to organize sensors hierarchically into clusters and, for each cluster, select one of the sensors as the cluster head . The task of the cluster head is to collect data from the cluster members and retransmit it to the base station across the network. By means of cluster organization, the process of acquiring data is enhanced and, consequently, the lifetime of the network is increased. Nevertheless, in addition to data collection and transmission, every cluster head performs additional tasks. Thus, they spend their energy quicker than the cluster members. This is the reason why cluster head selection is a multi-objective optimization problem. In this paper, we consider the three objectives, distance, delay, and residual energy , and apply three multi-objective evolutionary algorithms, specifically NSGA-II, SMS-EMOA, and MOEA/D, for studying the problem by means of solving several instances. We analyze the conflict between the objectives, present a proper multi-objective performance comparison of the algorithms, and investigate the efficiency of the found solutions regarding network energy consumption.},
  archive      = {J_ASOC},
  author       = {Abel García-Nájera and Saúl Zapotecas-Martínez and Karen Miranda},
  doi          = {10.1016/j.asoc.2021.107853},
  journal      = {Applied Soft Computing},
  pages        = {107853},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis of the multi-objective cluster head selection problem in WSNs},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image classification in frequency domain with 2SReLU: A
second harmonics superposition activation function. <em>ASOC</em>,
<em>112</em>, 107851. (<a
href="https://doi.org/10.1016/j.asoc.2021.107851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Convolutional Neural Networks are able to identify complex patterns and perform tasks with super-human capabilities. However, besides the exceptional results, they are not completely understood and it is still impractical to hand-engineer similar solutions. In this work, an image classification Convolutional Neural Network and its building blocks are described from a frequency domain perspective. Some network layers have established counterparts in the frequency domain like the convolutional and pooling layers. We propose the 2SReLU layer, a novel non-linear activation function that preserves high frequency components in deep networks. A convolution-free network is presented, and it is demonstrated that in the frequency domain it is possible to achieve competitive results without using the computationally costly convolution operation . A source code implementation in PyTorch is provided at: https://gitlab.com/thomio/2srelu .},
  archive      = {J_ASOC},
  author       = {Thomio Watanabe and Denis F. Wolf},
  doi          = {10.1016/j.asoc.2021.107851},
  journal      = {Applied Soft Computing},
  pages        = {107851},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image classification in frequency domain with 2SReLU: A second harmonics superposition activation function},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ATCSpeechNet: A multilingual end-to-end speech recognition
framework for air traffic control systems. <em>ASOC</em>, <em>112</em>,
107847. (<a href="https://doi.org/10.1016/j.asoc.2021.107847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a multilingual end-to-end framework, called ATCSpeechNet, is proposed to tackle the issue of translating communication speech into human-readable text in air traffic control (ATC) systems. In the proposed framework, we focus on integrating multilingual automatic speech recognition (ASR) into one model, in which an end-to-end paradigm is developed to convert speech waveforms into text directly, without any feature engineering or lexicon. To compensate the deficiency of handcrafted feature engineering caused by ATC challenges, including multilingual, multispeaker dialog and unstable speech rates, a speech representation learning (SRL) network is proposed to capture robust and discriminative speech representations from raw waves. The self-supervised training strategy is adopted to optimize the SRL network from unlabeled data , and to further predict the speech features, i.e., wave-to-feature. An end-to-end architecture is improved to complete the ASR task, in which a grapheme-based modeling unit is applied to address the multilingual ASR issue. Facing the problem of small transcribed samples in the ATC domain, an unsupervised approach with mask prediction is applied to pretrain the backbone network of the ASR model on unlabeled data by a feature-to-feature process. Finally, by integrating the SRL with ASR, an end-to-end multilingual ASR framework is formulated in a supervised manner, which is able to translate the raw wave into text in one model, i.e., wave-to-text. Experimental results on the ATCSpeech corpus demonstrate that the proposed approach achieves high performance with a very small labeled corpus and less resource consumption, only a 4.20\% label error rate on the 58-hour transcribed corpus. Compared to the baseline model , the proposed approach obtains over 100\% relative performance improvement which can be further enhanced with increasing size of the transcribed samples. It is also confirmed that the proposed SRL and training strategies make significant contributions to improving the final performance. In addition, the effectiveness of the proposed framework is also validated on common corpora (AISHELL, LibriSpeech, and cv-fr). More importantly, the proposed multilingual framework not only reduces the system complexity but also obtains higher accuracy compared to that of the independent monolingual ASR models. The proposed approach can also greatly reduce the cost of annotating samples, which benefits to advance the ASR technique to industrial applications.},
  archive      = {J_ASOC},
  author       = {Yi Lin and Bo Yang and Linchao Li and Dongyue Guo and Jianwei Zhang and Hu Chen and Yi Zhang},
  doi          = {10.1016/j.asoc.2021.107847},
  journal      = {Applied Soft Computing},
  pages        = {107847},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ATCSpeechNet: A multilingual end-to-end speech recognition framework for air traffic control systems},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). An advanced YOLOv3 method for small-scale road object
detection. <em>ASOC</em>, <em>112</em>, 107846. (<a
href="https://doi.org/10.1016/j.asoc.2021.107846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road target detection is a very challenging task in the field of computer vision because it is easily affected by complex backgrounds and sparse features of small targets. YOLOv3 (You Only Look Once v3) is currently one of the state-of-the-art object detection methods of deep learning . However, because the k-means clustering algorithm is sensitive to the initial clustering center, the local fragile visual field features related to small objects in the prediction map are severely lost and the final decision-making theory (The grid located in the center of the foreground object is responsible for predicting this object) of the network ignores the detailed information of the neighboring grid, there are still many problems in object detection. In this paper, we propose an improved algorithm based on YOLOv3 for small-scale object detection. We use the improved k-medians clustering method instead of the previous k-means to improve the model instability caused by the singularity ; We propose a local enhancement method to strengthen weak features for small-scale object detection by paralleling a branch on the backbone. Besides, a flexible offset sampling structure added in parallel for information compensation is also designed. A series of experiments showing that our system has achieved good detection results on the KITTI and UA-DETRAC public datasets, and the distinguishing performance for small-scale objects is significantly improved. Therefore, our method is effective in road target detection tasks.},
  archive      = {J_ASOC},
  author       = {Kun Wang and Maozhen Liu and Zhaojun Ye},
  doi          = {10.1016/j.asoc.2021.107846},
  journal      = {Applied Soft Computing},
  pages        = {107846},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An advanced YOLOv3 method for small-scale road object detection},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast variable neighborhood search approach for
multi-objective community detection. <em>ASOC</em>, <em>112</em>,
107838. (<a href="https://doi.org/10.1016/j.asoc.2021.107838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection in social networks is becoming one of the key tasks in social network analysis , since it helps analyzing groups of users with similar interests. This task is also useful in different areas, such as biology (interactions of genes and proteins), psychology (diagnostic criteria), or criminology (fraud detection). This paper presents a metaheuristic approach based on Variable Neighborhood Search (VNS) which leverages the combination of quality and diversity of a constructive procedure inspired in Greedy Randomized Adaptative Search Procedure (GRASP) for detecting communities in social networks. In this work, the community detection problem is modeled as a bi-objective optimization problem , where the two objective functions to be optimized are the Negative Ratio Association (NRA) and Ratio Cut (RC), two objectives that have already been proven to be in conflict. To evaluate the quality of the obtained solutions, we use the Normalized Mutual Information (NMI) metric for the instances under evaluation whose optimal solution is known, and modularity for those in which the optimal solution is unknown. Furthermore, we use metrics widely used in multi-objective optimization community to evaluate solutions, such as coverage, ε ε -indicator, hypervolume, and inverted generational distance. The obtained results outperform the state-of-the-art method for community detection over a set of real-life instances in both, quality and computing time.},
  archive      = {J_ASOC},
  author       = {Sergio Pérez-Peló and Jesús Sánchez-Oro and Antonio Gonzalez-Pardo and Abraham Duarte},
  doi          = {10.1016/j.asoc.2021.107838},
  journal      = {Applied Soft Computing},
  pages        = {107838},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fast variable neighborhood search approach for multi-objective community detection},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning with small datasets: Using autoencoders to
address limited datasets in construction management. <em>ASOC</em>,
<em>112</em>, 107836. (<a
href="https://doi.org/10.1016/j.asoc.2021.107836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large datasets are necessary for deep learning as the performance of the algorithms used increases as the size of the dataset increases. Poor data management practices and the low level of digitisation of the construction industry represent a big hurdle to compiling big datasets; which in many cases can be prohibitively expensive. In other fields, such as computer vision , data augmentation techniques and synthetic data have been used successfully to address issues with limited datasets. In this study, undercomplete, sparse, deep and variational autoencoders are investigated as methods for data augmentation and generation of synthetic data. Two financial datasets of underground and overhead power transmission projects are used as case studies. The datasets were augmented using the autoencoders , and the project cost was predicted using a deep neural network regressor . All the augmented datasets yielded better results than the original dataset. On average the autoencoders provide a model score improvement of 7.2\% and 11.5\% for the underground and overhead datasets, respectively. MAE and RMSE are lower for all autoencoders as well. The average error improvement for the underground and overhead datasets is 22.9\% and 56.5\%, respectively. Variational autoencoders provided more robust results and represented better the non-linear correlations among the attributes in both datasets. The novelty of this study is that presents an approach to improve existing datasets and thus improve the generalisation of deep learning models when other approaches are not feasible. Moreover, this study provides practitioners with methods to address the limited access to big datasets, a visualisation method to extract insights from non-linear correlations in data, and a way to improve data privacy and to enable sharing sensitive data using analogous synthetic data. The main contribution to knowledge of this study is that it presents a data augmentation technique for transformation variant data. Many techniques have been developed for transformation invariant data that contributed to improving the performance of deep learning models. This study showed that autoencoders are a good option for data augmentation for transformation variant data.},
  archive      = {J_ASOC},
  author       = {Juan Manuel Davila Delgado and Lukumon Oyedele},
  doi          = {10.1016/j.asoc.2021.107836},
  journal      = {Applied Soft Computing},
  pages        = {107836},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning with small datasets: Using autoencoders to address limited datasets in construction management},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated FCM-FBWM approach to assess and manage the
readiness for blockchain incorporation in the supply chain.
<em>ASOC</em>, <em>112</em>, 107832. (<a
href="https://doi.org/10.1016/j.asoc.2021.107832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the initial hype surrounding blockchain applications in the supply chain, real-world implementation of this disruptive technology faces significant challenges. To manage these challenges and ensure a smooth implementation, supply chain organizations must perform various activities to acquire readiness. This paper proposes a readiness assessment and management approach for blockchain implementation in the supply chain. The proposed approach allows supply chain decision-makers to (1) identify the readiness-relevant activities for blockchain implementation, (2) model the causal relationships among the identified activities, (3) assess the activities’ contribution weights to the overall readiness, and (4) develop an effective readiness improvement plan by prioritizing those activities with the most impact on the overall readiness. Fuzzy cognitive maps (FCMs) are employed to model the causal relationships between the activities. The fuzzy best–worst​ method (FBWM) is adopted to establish the contribution weights of the activities to the supply chain’s overall readiness. The FCM inference process is also used to incorporate feedback loops among the activities. The proposed approach is then illustrated through an empirical study.},
  archive      = {J_ASOC},
  author       = {Mandana Irannezhad and Sajjad Shokouhyar and Sadra Ahmadi and Elpiniki I. Papageorgiou},
  doi          = {10.1016/j.asoc.2021.107832},
  journal      = {Applied Soft Computing},
  pages        = {107832},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated FCM-FBWM approach to assess and manage the readiness for blockchain incorporation in the supply chain},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective iterated greedy algorithm for PCBs grouping
problem to minimize setup times. <em>ASOC</em>, <em>112</em>, 107830.
(<a href="https://doi.org/10.1016/j.asoc.2021.107830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The printed circuit boards (PCBs) grouping problem (PGP) is an essential part of PCBs assembly that has attracted much attention in recent years. In this paper, we propose a mathematical model and an iterated greedy (IG) algorithm, called IGP, for solving the PGP with setup time criterion. Based on the problem characteristics, two speed-up theorems are proposed and applied in the IGP. In the IGP, a new solution representation consisting of PCBs assignment sequence and component assignment sequence is adapted, and a heuristic based on PCB pairs and an iterated scheme is presented to create an initial solution. Then a merge operator is introduced to further improve the initial solution. A local search method based on the shift and swap operators is applied to improve solutions from the construction phase. To ensure the diversity of solutions, an acceptance criterion with probability is presented. Additionally, a detailed design experiment is carried out to calibrate the parameters for the presented IGP algorithm. The IGP is assessed by comparing it with the state-of-the-art algorithms in the literature. The experimental results show that the proposed IGP achieves the best performance among the tested methods for PGP.},
  archive      = {J_ASOC},
  author       = {Jiang-Ping Huang and Quan-Ke Pan and Liang Gao and Ling Wang},
  doi          = {10.1016/j.asoc.2021.107830},
  journal      = {Applied Soft Computing},
  pages        = {107830},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective iterated greedy algorithm for PCBs grouping problem to minimize setup times},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An interpretable evolving fuzzy neural network based on
self-organized direction-aware data partitioning and fuzzy logic
neurons. <em>ASOC</em>, <em>112</em>, 107829. (<a
href="https://doi.org/10.1016/j.asoc.2021.107829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the definition of the architecture of an evolving fuzzy neural network based on self-organizing direction aware data partitioning through stochastic processes based on the dataset used in the model. The choice between four different types of logical neurons in the second layer and the six different types of neuron activation function that compose the artificial neural network of aggregation are defined in a procedure of generating random pairs of combinations between the two factors. The combination that obtains the maximization of training accuracy is chosen to compose the network and perform the composition of the model structure, whose components can be transferred to readable IF-THEN rules for interpretability purposes. Furthermore, the model is able to adapt its parameters and evolve its structure autonomously with new data samples through self-organized direction-aware data partitioning (SODA). In this context, we also propose a technique to measure the degree of changes of neurons (rules), which could be used for structural active learning purposes (e.g, to request user feedback in the case of significant changes). To compare the proposed approach, binary pattern classification tests were performed, and the results were compared with other models of fuzzy neural networks and neural networks, obtaining satisfactory results with the stochastic definition of elements that compose their architecture comparing the final accuracy of the model when classifying real datasets. The proposed model obtained the best result in 3 of the four synthetic bases evaluated, in addition to the best accuracy results in the classification of patterns in five of the nine evaluated real datasets. It highlights the accuracy in problems in patients who underwent breast cancer surgery (72.44\%), diabetes evaluation (67.87\%), Australian (72.27\%) and German (80.51\%) credit ratings evaluation, and finally in the classification of radar signals in the ionosphere (90.46\%). It is also noteworthy to obtain fuzzy rules in evolution extracted from a real problem of the identification of respiratory diseases through the collection of saliva with 76.67\% of accuracy. The results presented by the model were superior to traditional artificial intelligence models, while it was possible to extract knowledge in form of interpretable rules and to realize how these changed over time.},
  archive      = {J_ASOC},
  author       = {Paulo Vitor de Campos Souza and Edwin Lughofer and Augusto Junio Guimaraes},
  doi          = {10.1016/j.asoc.2021.107829},
  journal      = {Applied Soft Computing},
  pages        = {107829},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interpretable evolving fuzzy neural network based on self-organized direction-aware data partitioning and fuzzy logic neurons},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ant colony optimization-based traffic routing with
intersection negotiation for connected vehicles. <em>ASOC</em>,
<em>112</em>, 107828. (<a
href="https://doi.org/10.1016/j.asoc.2021.107828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a decentralized traffic management system that integrates dynamic traffic routing and signal-free intersection traffic control for connected vehicles (CV). In particular, we apply the ant colony optimization algorithm and the proposed concept of colored CV to solve the dynamic traffic routing problem with multi-source multi-destination traffic flows. Each CV, modeled as an ant, deposits pheromones on its path, while road sensors aggregate the pheromone information representing traffic conditions and transfer the information to the incoming CV to perform routing. Using the colored CV concept, the CV with the same destination deposit the corresponding pheromone called colored pheromone, and only these CV can interact with that pheromone. Thus, we can distinguish multi-source multi-destination traffic flows in the routing process. For implementing signal-free intersection traffic control, we propose a decentralized intersection negotiation protocol that allows the CV to pass the intersections by exchanging their trajectory information, thereby significantly reducing the time required by CV to pass the intersections. We performed extensive simulations to evaluate our system using Simulation of Urban Mobility, a microscopic traffic simulator. The results show improvement of our proposed system over the conventional traffic management systems in terms of the mean trip time and the number of running vehicles.},
  archive      = {J_ASOC},
  author       = {Tri-Hai Nguyen and Jason J. Jung},
  doi          = {10.1016/j.asoc.2021.107828},
  journal      = {Applied Soft Computing},
  pages        = {107828},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ant colony optimization-based traffic routing with intersection negotiation for connected vehicles},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time series classification using diversified ensemble deep
random vector functional link and resnet features. <em>ASOC</em>,
<em>112</em>, 107826. (<a
href="https://doi.org/10.1016/j.asoc.2021.107826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Vector Functional Link (RVFL) is popular among researchers in many areas of machine learning . RVFL is preferred by many researchers as RVFL can produce good performance with relatively little training time. Recent works extend RVFL into deep and ensemble versions. However, RVFL does not have effective feature extraction methods commonly used in time series classification. This results in poor performance of RVFL in time series classification tasks . Also, deep RVFL is a relatively new and evolving area of research. In this paper, we present a framework that extracts features from Residual Networks (Resnet) and trains Ensemble Deep Random Vector Functional Link (edRVFL). We use features extracted from every residual block to train an ensemble of edRVFLs. We propose the following enhancements to edRVFL. Firstly, we diversity the structure of edRVFL and the direct link features to encourage diversity. Secondly, we built an ensemble of edRVFLs with the top two activation functions . Thirdly, we use two-stage tuning to save computational costs. Lastly, we perform a weighted average of all decisions made by every edRVFL. Experiments on the 55 largest UCR datasets show that using features extracted from all Residual blocks improves performance. All our proposed enhancements help improve classification accuracy or computational effort. Consequently, our proposed framework outperforms all traditional and deep learning-based time series classification methods.},
  archive      = {J_ASOC},
  author       = {Wen Xin Cheng and P.N. Suganthan and Rakesh Katuwal},
  doi          = {10.1016/j.asoc.2021.107826},
  journal      = {Applied Soft Computing},
  pages        = {107826},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time series classification using diversified ensemble deep random vector functional link and resnet features},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel capacity sharing mechanism to collaborative
activities in the blood collection process during the COVID-19 outbreak.
<em>ASOC</em>, <em>112</em>, 107821. (<a
href="https://doi.org/10.1016/j.asoc.2021.107821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of government intervention, such as quarantine and cancellation of public events at the peak of the COVID-19 outbreak and donors’ health scare of exposure to the virus in medical centers, the number of blood donors has considerably decreased. In some countries, the rate of blood donation has reached lower than 30\%. Accordingly, in this study, to fill the lack of blood product during COVID-19, especially at the outbreak’s peak, we propose a novel mechanism by providing a two-stage optimization tool for coordinating activities to mitigate the shortage in this urgent situation. In the first stage, a blood collection plan considering disruption risk in supply to minimize the unmet demand will be solved. Afterward, in the second stage, the collected units will be shared between regions by applying the capacity sharing concept to avoid the blood shortage in health centers. Moreover, to tackle the uncertainty and disruption risk, a novel stochastic model combining the mixed uncertainty approach is tailored. A rolling horizon planning method is implemented under an iterative procedure to provide and share the limited blood resources to solve the proposed model. A real-world case study of Iran is investigated to examine the applicability and performance of the proposed model; it should be noted that the designed mechanism is not confined just to this case. Obtained computational results indicate the applicability of the model, the superior performance of the capacity sharing concept, and the effectiveness of the designed mechanism for mitigating the shortage and wastage during the COVID-19 outbreak.},
  archive      = {J_ASOC},
  author       = {Mohammad Reza Ghatreh Samani and Seyyed-Mahdi Hosseini-Motlagh},
  doi          = {10.1016/j.asoc.2021.107821},
  journal      = {Applied Soft Computing},
  pages        = {107821},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel capacity sharing mechanism to collaborative activities in the blood collection process during the COVID-19 outbreak},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing of higher order information granules through
clustering heterogeneous granular data. <em>ASOC</em>, <em>112</em>,
107820. (<a href="https://doi.org/10.1016/j.asoc.2021.107820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is devoted to the generalization of information granules by forming higher order, namely, order-2 information granules. Information granules are semantically meaningful entities, which play a central role in knowledge representation and system modeling in the framework of Granular Computing . The encountered information granules could exhibit significant heterogeneity because of the diversified formal formalisms. To facilitate an effective generalization of heterogeneous granular data when using clustering algorithms , an efficient scheme has been proposed to form a unified representation of various types of granular data by using Possibility–necessity measures. Once the clustering process has been completed in the possibility–necessity feature space, the higher order information granules come as results of decoding by involving the possibility–necessity metrics and fuzzy relational calculus. The extent to which the higher order information granules are supported by the granular data present at a lower level of hierarchy is quantified in terms of the membership degrees obtained in the clustering process . Experimental studies concerning a series of publicly available datasets coming from UCI and KEEL machine learning repositories are carried out in this study.},
  archive      = {J_ASOC},
  author       = {Dan Wang and Peng Nie and Xiubin Zhu and Witold Pedrycz and Zhiwu Li},
  doi          = {10.1016/j.asoc.2021.107820},
  journal      = {Applied Soft Computing},
  pages        = {107820},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Designing of higher order information granules through clustering heterogeneous granular data},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Securing private information by data perturbation using
statistical transformation with three dimensional shearing.
<em>ASOC</em>, <em>112</em>, 107819. (<a
href="https://doi.org/10.1016/j.asoc.2021.107819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy is very important in shared data for the knowledge based applications. However it causes serious privacy concerns, when the sensitive data is stored and moved to other applications. It is vital to incorporate privacy in the sensitive data for the data mining process . While preserving privacy, certain protocols allow the knowledge extraction from the modified data without revealing the original information. In this work, a series of steps like, Weight of Evidence, Information Value, Min–Max normalization and 3D shearing are applied to perturb the quasi-identifiers in the data. The classification techniques such as Decision Tree , Random Forest , Extreme Gradient Boost and Support Vector Machines are employed in adult income, bank marketing and lung cancer datasets to analyze the performance of the original and perturbed data. Accuracy, variance and sensitivity-specificity are being considered as performance measures of the classifiers. This research work is compared with 2D rotation and 3D rotation algorithms. The experimental results clearly show that the proposed work preserves the data utility with higher data transformation capacity and privacy preserving capacity than the existing geometric transformation techniques.},
  archive      = {J_ASOC},
  author       = {G. Sathish Kumar and K. Premalatha},
  doi          = {10.1016/j.asoc.2021.107819},
  journal      = {Applied Soft Computing},
  pages        = {107819},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Securing private information by data perturbation using statistical transformation with three dimensional shearing},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Order-guided deep neural network for emotion-cause pair
prediction. <em>ASOC</em>, <em>112</em>, 107818. (<a
href="https://doi.org/10.1016/j.asoc.2021.107818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion-Cause Pair Extraction (ECPE) is a prediction task aiming to extract the emotions and their corresponding causes in a target document. The existing methods for this problem mainly focus on modeling the dependence between emotion clauses and related cause clauses and the interaction among emotion-cause pairs. However, these methods ignore the order information between emotion clauses and their cause clauses, which can be proved useful for the ECPE task. In this paper, we propose an order-guided deep predictive model , which integrates different orders between emotion clauses and their cause clauses into an end-to-end framework to tackle this task. Specifically, we build an order-guided clause encoder with a three-level long-short term memory (LSTM) network to learn the different orders from forward LSTM, backward LSTM and Bi-LSTM, respectively. In this way, the deep networks with different directions can effectively capture different orders, and therefore improve the performance of our model in this prediction task. Additionally, the previous methods use only a shared word encoder to capture word-level emotion and cause information, resulting in paying more attention to emotion information and lacking the ability to capture cause information. In order to overcome this deficiency, we design both an emotion-aware word encoder and a cause-aware word encoder to enhance the ability to capture the emotion and cause information. The experiment results illustrate that our method outperforms the other baselines on two real-world datasets, and demonstrate the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Wei Fan and Yuexuan Zhu and Ziyun Wei and Tianyu Yang and W.H. Ip and Yuxiang Zhang},
  doi          = {10.1016/j.asoc.2021.107818},
  journal      = {Applied Soft Computing},
  pages        = {107818},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Order-guided deep neural network for emotion-cause pair prediction},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Remaining useful life prediction using an integrated
laplacian-LSTM network on machinery components. <em>ASOC</em>,
<em>112</em>, 107817. (<a
href="https://doi.org/10.1016/j.asoc.2021.107817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate remaining useful life (RUL) analysis of a machinery system is of great importance. Such systems work in long-term operations in which unexpected failures often occur. Due to the rapid development of computer technology, the deep learning model has supplanted physical-based RUL analysis. The data-driven approach using the deep learning model is capable of providing an accurate RUL analysis. However, an accurate analysis using deep learning comes with challenges and costs. In current RUL analysis practice, the deep learning hyperparameters are manually selected, which hinders the deep network from reaching the local optima. Additionally, current practice uses powerful signal processing methods with complicated prediction indicators. Therefore, a novel methodological step is proposed to tackle this problem by integrating the Laplacian score (LS), random search optimisation and long short-term memory (LSTM). The proposed system, called integrated Laplacian-LSTM, produced accurate RUL analyses on the IEEE PHM 2012 Competition and IMS bearing datasets, showing significant improvement in prediction accuracy. This system increases prediction accuracy by 18\% compared to other available RUL methods in similar studies.},
  archive      = {J_ASOC},
  author       = {Mohd Syahril Ramadhan Mohd Saufi and Kamarul Arifin Hassan},
  doi          = {10.1016/j.asoc.2021.107817},
  journal      = {Applied Soft Computing},
  pages        = {107817},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Remaining useful life prediction using an integrated laplacian-LSTM network on machinery components},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An intuitionistic fuzzy kernel ridge regression classifier
for binary classification. <em>ASOC</em>, <em>112</em>, 107816. (<a
href="https://doi.org/10.1016/j.asoc.2021.107816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel ridge regression (KRR) is a widely accepted efficient machine learning paradigm that has been fruitfully implemented for solving both classification and regression problems . KRR solves a set of linear equations instead of solving a quadratic programming problem. However, KRR gives equal importance to each sample which leads to giving the same significance to the important and non-important samples. That might lead to low classification accuracy . To resolve this issue, this paper suggests a novel kernel ridge regression based on intuitionistic fuzzy membership (IFKRR) for binary classification . In IFKRR, there is an intuitionistic fuzzy number linked to each training sample which is framed by either its membership or non-membership. A pattern’s membership degree considers its distance from the corresponding class center.​ However, the non-membership degree is provided by the ratio of the number of heterogeneous points to the total number of its neighborhood points. The proposed IFKRR model can efficiently reduce the influence of noise in datasets. To evaluate the efficiency of IFKRR, its performance is compared with support vector machine (SVM), twin SVM (TWSVM), intuitionistic fuzzy SVM (IFSVM), intuitionistic fuzzy TWSVM (IFTSVM), random vector functional link with univariate trees (RFL), KRR and Co-trained KRR average (CoKRR-avg) on an artificial and a few really interesting real world datasets using Gaussian kernel . Computational results reveal the efficacy of the IFKRR model on the real world as well as noisy datasets.},
  archive      = {J_ASOC},
  author       = {Barenya Bikash Hazarika and Deepak Gupta and Parashjyoti Borah},
  doi          = {10.1016/j.asoc.2021.107816},
  journal      = {Applied Soft Computing},
  pages        = {107816},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intuitionistic fuzzy kernel ridge regression classifier for binary classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of the multi-objective adaptive guided
differential evolution and optimization of the MO-ACOPF for
wind/PV/tidal energy sources. <em>ASOC</em>, <em>112</em>, 107814. (<a
href="https://doi.org/10.1016/j.asoc.2021.107814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, one of the most popular research topics is the development of a new meta-heuristic algorithm for solving multi-objective optimization problems. However, few of the proposed algorithms have been successful in finding sets that represent feasible solutions. One reason for this failure is that algorithms originally designed for single-objective problems do not provide sufficient exploitation and exploration capabilities for the multi-objective optimization process. Another reason is that a strong harmony is lacking between the Pareto-based archiving approach and the meta-heuristic search (MHS) method (the two basic elements of multi-objective optimization). This study presents the Multi-Objective Adaptive Guided Differential Evolution (MOAGDE) as a powerful and stable algorithm . The MOAGDE can effectively find Pareto optimal solutions for multi-objective optimization problems with different types of high-complexity decision/objective spaces. The proposed MOAGDE was developed by redesigning the adaptive guided differential evolution algorithm for multi-objective optimization. In addition, in the MOAGDE, non-dominated sorting strategy was integrated with crowding distance to archive Pareto optimal solutions of multiple conflicting functions. The CEC 2020, the most recent and advanced benchmark problem suite, was used along with the strongest competitors in the literature to test and verify the performance of the MOAGDE. Finally, using the proposed method, the best Pareto optimal solutions in the literature were applied to a real-world engineering problem: the multi-objective-alternating current optimal power flow (MO-ACOPF) problem involving wind/PV/tidal energy sources. Although the multi-objective version of this problem is rarely studied, the optimal power flow (OPF) is one of the major problems encountered in the planning and operation of modern power systems . The performance of the proposed approach was studied and tested for various objective functions on an IEEE 30-bus test system and the simulation results were compared with the results of the OMNI and MO_Ring_PSO_SCD algorithms.},
  archive      = {J_ASOC},
  author       = {Serhat Duman and Mustafa Akbel and Hamdi Tolga Kahraman},
  doi          = {10.1016/j.asoc.2021.107814},
  journal      = {Applied Soft Computing},
  pages        = {107814},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of the multi-objective adaptive guided differential evolution and optimization of the MO-ACOPF for wind/PV/tidal energy sources},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum parallel multi-layer monte carlo optimization
algorithm for controller parameters optimization of doubly-fed induction
generator-based wind turbines. <em>ASOC</em>, <em>112</em>, 107813. (<a
href="https://doi.org/10.1016/j.asoc.2021.107813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With larger searching space, intelligent algorithms could have insufficient global search capability and accuracy for optimization problems . This paper proposes a novel quantum parallel multi-layer Monte Carlo optimization algorithm (QPMMCOA) to optimize the rotor-side controller (RSC) parameters based on proportional–integral of a doubly-fed induction generator (DFIG) for achieving maximum power and improving generation efficiency. The QPMMCOA is proposed to find optimal solutions in an accurate small searching space. The QPMMCOA combines qubit probability amplitude with Monte Carlo random numbers to generate a diverse population and expand accurate search space. The optimization process of the QPMMCOA with strong global search ability and local development ability is divided into the rough search, precise search, and re-precise search. These three search processes are searched in the ever-shrinking feasible region. The QPMMCOA mainly seeks the optimized solution by continuously changing and narrowing the feasible region. The QPMMCOA is utilized to optimize a discontinuous step function and a multimodal function for confirming the efficacy and feasibility. With broader exploration and deeper development capabilities, the QPMMCOA can achieve the optimization result of the fitness function of the RSC is at least 0.51\% lower than other algorithms. Compared with other algorithms, the QPMMCOA-based RSC can enhance the average power coefficient of the DFIG-based wind power system by at least 0.0028\%.},
  archive      = {J_ASOC},
  author       = {Kunlun Han and Tianwei Huang and Linfei Yin},
  doi          = {10.1016/j.asoc.2021.107813},
  journal      = {Applied Soft Computing},
  pages        = {107813},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum parallel multi-layer monte carlo optimization algorithm for controller parameters optimization of doubly-fed induction generator-based wind turbines},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A combined hesitant fuzzy MCDM approach for supply chain
analytics tool evaluation. <em>ASOC</em>, <em>112</em>, 107812. (<a
href="https://doi.org/10.1016/j.asoc.2021.107812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data quantity generated in supply chains expands as supply chain processes get more complex. Companies can leverage such data and gain valuable insight into their processes with Supply Chain Analytics (SCA) tools. The selection of the most appropriate SCA tool directly affects companies’ productivity since they allow enhancing visibility, obtaining informed decision-making and developing well-planned strategies for companies. A simple, practical, efficient and robust decision-making method can help select the most suitable SCA tool. Such a selection problem can be solved with multi-criteria decision-making (MCDM) methods that consider different criteria. This paper proposes an SCA tool evaluation model which combines hesitant fuzzy linguistic term set (HFLTS), analytic hierarchy process (AHP), multi-objective optimization by ratio analysis, and the full multiplicative (MULTIMOORA). The HFLTS technique is applied for handling the uncertainty and hesitancy of experts’ views in the evaluation process. The weights of the six main selection criteria and their thirty sub-criteria are computed via the hesitant fuzzy linguistic (HFL) AHP method. Then, the HFL MULTIMOORA method is combined with the fuzzy envelope technique for the first time in the literature to rank the eight SCA tool alternatives from different companies. A case study for a logistics firm illustrates the potential of the proposed evaluation model, which underlines the most important criteria to be the statistical power, product quality improvement, organizational performance enhancement, and service cooperation. It also ranks PeopleSoft as the most appropriate SCA tool for the case company. A comparative analysis with the HFL VIKOR method demonstrated that the proposed method is robust and consistent.},
  archive      = {J_ASOC},
  author       = {Gülçin Büyüközkan and Merve Güler},
  doi          = {10.1016/j.asoc.2021.107812},
  journal      = {Applied Soft Computing},
  pages        = {107812},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combined hesitant fuzzy MCDM approach for supply chain analytics tool evaluation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Drug–disease associations prediction via multiple
kernel-based dual graph regularized least squares. <em>ASOC</em>,
<em>112</em>, 107811. (<a
href="https://doi.org/10.1016/j.asoc.2021.107811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting associations in drug–disease network provides effective information for the drug repositioning. Therefore, it is an important task to develop an effective drug–disease association prediction method. In this paper, we propose a model, called the Multiple Kernel-based Dual Graph Regularized Least Squares Model (MKDGRLS), to predict potential drug–disease associations. First, we calculate the multiple kernels of drug and disease spaces respectively. Moreover, we utilize multiple kernels and related Laplacian regularization terms to construct MKDGRLS. Finally, we solve the object function of MKDGRLS by the Alternating Least squares algorithm (ALSA), for predicting drug–disease associations. Simultaneously, we design a variant MKDGRLS-A, which is added one dimension as the bias for compensate the inexact solution of linear systems. Our proposed approach has better prediction performance than existing prediction tools under two types of cross validation on the three real drug–disease association network datasets. The results of the case studies of the two diseases prove that our model can effectively predict new associations. We also test MKDGRLS on six real-world networks and it achieves better performance than other methods. In conclusion, our research work can effectively discover potential drug–disease associations, and can provide effective help for related drug repositioning work.},
  archive      = {J_ASOC},
  author       = {Hongpeng Yang and Yijie Ding and Jijun Tang and Fei Guo},
  doi          = {10.1016/j.asoc.2021.107811},
  journal      = {Applied Soft Computing},
  pages        = {107811},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Drug–disease associations prediction via multiple kernel-based dual graph regularized least squares},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A soft computing automatic based in deep learning with use
of fine-tuning for pulmonary segmentation in computed tomography images.
<em>ASOC</em>, <em>112</em>, 107810. (<a
href="https://doi.org/10.1016/j.asoc.2021.107810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmentation of medical images is a significant challenge for computer vision and medical techniques. In the lungs, the difficulty is mainly due to the presence of lung diseases, different dimensions of the lung, and different types and configurations of medical imaging devices. Computed tomography (CT) is a tool to aid clinical diagnosis; several systems based on computer-aided diagnostics (CAD) are developed or enhanced from CT images combined with computational methods. This study proposes an innovative approach based on the generalization of models for pulmonary segmentation in CT images, using the Convolutional Neural Network Based on Mask Regions (Mask R-CNN) combined with image processing techniques, K-means clustering, region growing and Parzen window through fine-tuning. Our models achieved satisfactory results with 97\% Accuracy and 98\% Dice, 99\% Sensitivity, and 97\% Positive Predictive Value (PPV) in our best MPK model. The method was able to generalize its learning to solve different pulmonary segmentation problems in different lung CT databases. A second set of data was used, obtaining even better results. The results obtained for this second data set were: 98\% Accuracy, 99\% Dice, 100\% Sensitivity, and 98\% PPV, demonstrating the effectiveness of our method.},
  archive      = {J_ASOC},
  author       = {Yongzhao Xu and Luís F.F. Souza and Iágson C.L. Silva and Adriell G. Marques and Francisco H.S. Silva and Virgínia X. Nunes and Tao Han and Chuanyu Jia and Victor Hugo C. de Albuquerque and Pedro P. Rebouças Filho},
  doi          = {10.1016/j.asoc.2021.107810},
  journal      = {Applied Soft Computing},
  pages        = {107810},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A soft computing automatic based in deep learning with use of fine-tuning for pulmonary segmentation in computed tomography images},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disaster relief supply chain design for personal protection
equipment during the COVID-19 pandemic. <em>ASOC</em>, <em>112</em>,
107809. (<a href="https://doi.org/10.1016/j.asoc.2021.107809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global epidemic caused by novel coronavirus continues to be a crisis in the world and a matter of concern. The way the epidemic has wreaked havoc on the international level has become difficult for the healthcare systems to supply adequately personal protection equipment for medical personnel all over the globe. In this paper, considering the COVID-19 outbreak, a multi-objective, multi-product, and multi-period model for the personal protection equipment demands satisfaction aiming to optimize total cost and shortage, simultaneously, is developed. The model is embedded with instances and validated by both modern and classic multi-objective metaheuristic algorithms. Moreover, the Taguchi method is exploited to set the metaheuristic into their best performances by finding their parameters’ optimum level. Furthermore, fifteen test examples are designed to prove the established PPE supply chain model and tuned algorithms’ applicability. Among the test examples, one is related to a real case study in Iran. Finally, metaheuristics are evaluated by a series of related metrics through different statistical analyses. It can be concluded from the obtained results that solution methods are practical and valuable to achieve the efficient shortage level and cost.},
  archive      = {J_ASOC},
  author       = {Behzad Mosallanezhad and Vivek Kumar Chouhan and Mohammad Mahdi Paydar and Mostafa Hajiaghaei-Keshteli},
  doi          = {10.1016/j.asoc.2021.107809},
  journal      = {Applied Soft Computing},
  pages        = {107809},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Disaster relief supply chain design for personal protection equipment during the COVID-19 pandemic},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differential evolution method integrated with a fuzzy
decision-making mechanism and virtual mutant agent: Theory and
application. <em>ASOC</em>, <em>112</em>, 107808. (<a
href="https://doi.org/10.1016/j.asoc.2021.107808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current study deals with upgrading the Differential Evolution (DE) method by equipping it with a novel fuzzy decision-making strategy and a new auxiliary agent called Virtual Mutant. The new algorithm is called Fuzzy Differential Evolution incorporated Virtual Mutant (FDEVM) The presented fuzzy strategy employs two nine-rule mapping mechanisms to adjust the internal parameters of the algorithm considering governing conditions of the current problem. Also, the Virtual Mutant providing reasonable attractive and compulsive effects on the other agents, plays important role in increasing the efficiency of the algorithm. These two new enhancements (i.e., adding the fuzzy module and Virtual Mutant) converts the conventional DE algorithm to a self-adaptive and parameter-free approach which can dynamically adjust its search behavior during the optimization process. Consequently, the search performance of the presented FDEVM is tested on a suite of unconstrained mathematical functions and constrained engineering problems. The outcomes are reported and compared with those obtained by six other well-established search techniques. The acquired results indicate that the proposed method could improve the search process in the terms of computational cost, stability and accuracy.},
  archive      = {J_ASOC},
  author       = {Ali Mortazavi and Mahsa Moloodpoor},
  doi          = {10.1016/j.asoc.2021.107808},
  journal      = {Applied Soft Computing},
  pages        = {107808},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution method integrated with a fuzzy decision-making mechanism and virtual mutant agent: Theory and application},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local latin hypercube refinement for multi-objective design
uncertainty optimization. <em>ASOC</em>, <em>112</em>, 107807. (<a
href="https://doi.org/10.1016/j.asoc.2021.107807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the reliability and the robustness of a design is important but often unaffordable due to high sample requirements. Surrogate models based on statistical and machine learning methods are used to increase the sample efficiency. However, for higher dimensional or multi-modal systems, surrogate models may also require a large amount of samples to achieve good results. We propose a sequential sampling strategy for the surrogate based solution of multi-objective reliability based robust design optimization problems . Proposed local Latin hypercube refinement (LoLHR) strategy is model-agnostic and can be combined with any surrogate model because there is no free lunch but possibly a budget one. The proposed method is compared to stationary sampling as well as other proposed strategies from the literature. Gaussian process and support vector regression are both used as surrogate models. Empirical evidence is presented, showing that LoLHR achieves on average better results compared to other surrogate based strategies on the tested examples.},
  archive      = {J_ASOC},
  author       = {Can Bogoclu and Dirk Roos and Tamara Nestorović},
  doi          = {10.1016/j.asoc.2021.107807},
  journal      = {Applied Soft Computing},
  pages        = {107807},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local latin hypercube refinement for multi-objective design uncertainty optimization},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A band selection approach based on a modified gray wolf
optimizer and weight updating of bands for hyperspectral image.
<em>ASOC</em>, <em>112</em>, 107805. (<a
href="https://doi.org/10.1016/j.asoc.2021.107805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the curse of dimensionality, band selection is utilized to choose part of feature information from hyperspectral image (HSI) and achieve the highest classification accuracy as possible. It is considered as a non-polynomial hard problem and difficult to be fully solved within the effective period, especially for traditional swarm optimization. Gray wolf optimizer (GWO) is a newly proposed swarm intelligence algorithm based on heuristic search , and chaotic operation helps the algorithm jump out of the local optima. In the paper, a novel band selection approach based on a modified GWO (MGWO) is presented to reduce the data dimension of HSIs, and chaotic operation is utilized to set the index of gray wolves. In addition, the weight of bands is updated by the best and worst fitness values as the iteration of MGWO, and evaluates the quality of each band. Experimental results illustrate that it is better than other state-of-the-art band selection approaches, and satisfied band subsets with the combination from 10\% to 25\% total number of bands are obtained with higher classification accuracy .},
  archive      = {J_ASOC},
  author       = {Mingwei Wang and Wei Liu and Maolin Chen and Xiaohui Huang and Wei Han},
  doi          = {10.1016/j.asoc.2021.107805},
  journal      = {Applied Soft Computing},
  pages        = {107805},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A band selection approach based on a modified gray wolf optimizer and weight updating of bands for hyperspectral image},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-stage approach for multicast-oriented virtual network
function placement. <em>ASOC</em>, <em>112</em>, 107798. (<a
href="https://doi.org/10.1016/j.asoc.2021.107798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV) is an emerging network paradigm that decouples softwarized network functions from proprietary hardware. Nowadays, resource allocation has become one of the hot topics in the NFV domain. In this paper, we formulate a service function chain (SFC) mapping problem in the context of multicast, which is also referred to as the multicast-oriented virtual network function placement (MVNFP) problem. The objective function considers end-to-end delay as well as compute resource consumption, with bandwidth requirements met. A two-stage approach is proposed to address this problem. In the first stage, Dijkstra’s algorithm is adopted to construct a multicast tree . In the second stage, a novel estimation of distribution algorithm (nEDA) is developed to map a given SFC over the multicast tree. Simulation results show that the proposed two-stage approach outperforms a number of state-of-the-art evolutionary, approximation, and heuristic algorithms, in terms of the solution quality.},
  archive      = {J_ASOC},
  author       = {Xinhan Wang and Huanlai Xing and Dawei Zhan and Shouxi Luo and Penglin Dai and Muhammad Azhar Iqbal},
  doi          = {10.1016/j.asoc.2021.107798},
  journal      = {Applied Soft Computing},
  pages        = {107798},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage approach for multicast-oriented virtual network function placement},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A constructive approach to data-driven randomized learning
for feedforward neural networks. <em>ASOC</em>, <em>112</em>, 107797.
(<a href="https://doi.org/10.1016/j.asoc.2021.107797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an issue with the way in which feedforward neural networks with random hidden nodes generate random parameters in order to obtain a good projection space. Typically, random weights and biases are both drawn from the same interval, which is misguided as they have different functions. Recently, more sophisticated methods of random parameters generation have been developed, such as the data-driven approach, where the sigmoids are placed in randomly selected regions of the input space and then their slopes are adjusted to the local fluctuations of the target function. In this work, we propose a new constructive data-driven method that builds iteratively the network architecture . This method successively generates new candidate hidden nodes and accepts them when the training error falls significantly. The threshold of acceptance is adapted throughout training, accepting at the beginning of the training process only those nodes which lead to the largest reductions in error. In the next stages, the threshold is successively reduced to accept only those nodes which model the target function details more accurately. This leads to a more compact network architecture, as it includes only ”significant” nodes. It is worth noting that redundant, random nodes, which are usually generated by existing randomized learning methods, are not accepted by the proposed method. We empirically compared our approach with several alternative methods, including its predecessor, competitive randomized learning solutions, a gradient-based network and a generalized additive model . We found that our proposed approach outperformed its competitors in terms of fitting accuracy.},
  archive      = {J_ASOC},
  author       = {Grzegorz Dudek},
  doi          = {10.1016/j.asoc.2021.107797},
  journal      = {Applied Soft Computing},
  pages        = {107797},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A constructive approach to data-driven randomized learning for feedforward neural networks},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An enhanced genetic algorithm for path planning of
autonomous UAV in target coverage problems. <em>ASOC</em>, <em>112</em>,
107796. (<a href="https://doi.org/10.1016/j.asoc.2021.107796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAV) have become an important and integral part of military and civilian operations in recent years. In many UAV missions, the main purpose is to visit some predetermined checkpoints in operational space. If the number of checkpoints and constraints increases, finding a feasible solution may take up too much time. In this paper; the path planning problem of autonomous UAV in target coverage problems is solved by using artificial intelligent methods including genetic algorithm (GA), ant colony optimizer (ACO), Voronoi diagram, and clustering methods . The main contribution of this article is to propose initial population enhancement methods in GA, and thus accelerate convergence process . The first common enhancement to basic GA structure is to generate a sub-optimal path by implementing ACO. A sub-optimal path can be used to generate initial individuals. However, sub-optimal paths may have the problem that is collision with terrain. To avoid a UAV from any crash three approaches are integrated into an initial population phase of genetic algorithm . The first approach includes Voronoi vertices as additional waypoints to keep clear of trouble. The second approach consists of cluster centers which forms Voronoi vertices as supplemental waypoints. The final proposal comprises again cluster centers but based on a set of collision points . The proposed methods are tested in different three dimensional (3D) environments and the results are compared. Performance results show that collision with terrain surface is a local phenomenon and solving this issue by using the cluster center of collision points provides the best result including at least 70\% or much more decrease in the required number of objective function evaluations.},
  archive      = {J_ASOC},
  author       = {Y. Volkan Pehlivanoglu and Perihan Pehlivanoglu},
  doi          = {10.1016/j.asoc.2021.107796},
  journal      = {Applied Soft Computing},
  pages        = {107796},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced genetic algorithm for path planning of autonomous UAV in target coverage problems},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multicondition operation fault detection for chillers based
on global density-weighted support vector data description.
<em>ASOC</em>, <em>112</em>, 107795. (<a
href="https://doi.org/10.1016/j.asoc.2021.107795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault detection is essential to maintain the healthy and efficient operation of chillers. However, in actual chiller operation, due to fluctuating operating conditions, fault detection becomes very difficult, i.e., a low detection accuracy and high false alarm rate (FAR) are attained. To better reflect the distribution of data, effectively improve the detection accuracy and reduce the FAR, this study considers the global perspective of target data, and a method based on global density-weighted support vector data description (GDW-SVDD) is proposed to detect chiller faults. This method is validated against ASHRAE RP-1043 experimental data. The results show that compared to the conventional SVDD method, the GDW-SVDD method not only effectively reduces the FAR from 10.75\% to 8.25\% but also improves the fault detection accuracy by 3.75\% (under the reduced evaporator water flow fault at severity level 2). Compared to the density-weighted support vector data description (DW-SVDD) method, this method can effectively improve the accuracy of fault detection up to 6.5\% (under the excessive oil fault at severity level 2) while simultaneously maintaining the same low FAR. Therefore, the proposed method is effective for chiller fault detection.},
  archive      = {J_ASOC},
  author       = {Kuiliang Chen and Zhiwei Wang and Xiaowei Gu and Zhanwei Wang},
  doi          = {10.1016/j.asoc.2021.107795},
  journal      = {Applied Soft Computing},
  pages        = {107795},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multicondition operation fault detection for chillers based on global density-weighted support vector data description},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective feature selection approach based on
chemical reaction optimization. <em>ASOC</em>, <em>112</em>, 107794. (<a
href="https://doi.org/10.1016/j.asoc.2021.107794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is an NP-hard combinatorial optimization problem , which aims to select the most relevant features from a large number of candidates. Recently, the FS in classification has been handled as a bi-objective optimization problem, where both the classification error and the number of selected features are minimized simultaneously. Despite of the effectiveness of existing multi-objective optimization (MOO) algorithms on FS, the convergence toward the Pareto front and the distribution over the Pareto front still deserve to be improved. To this end, this paper proposes a new multi-objective chemical reaction optimization algorithm for FS, termed MOFSCRO. In the proposed MOFSCRO, the preference information related to the problem is firstly extracted from the population. Then, three evolutionary operators based on the preference information are suggested to guide the population evolution, with the purpose of enhancing the population convergence and diversity. Experimental results on 12 benchmark data sets demonstrate the superiority of the proposed MOFSCRO over seven state-of-the-art FS algorithms in terms of both the classification error and the number of selected features. In comparison to five MOO algorithms, MOFSCRO also shows better performance according to the metric of inverted generational distance.},
  archive      = {J_ASOC},
  author       = {Jianfeng Qiu and Xiaoshu Xiang and Chao Wang and Xingyi Zhang},
  doi          = {10.1016/j.asoc.2021.107794},
  journal      = {Applied Soft Computing},
  pages        = {107794},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective feature selection approach based on chemical reaction optimization},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A three-stage learning approach to cross-domain person
re-identification. <em>ASOC</em>, <em>112</em>, 107793. (<a
href="https://doi.org/10.1016/j.asoc.2021.107793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain person re-identification (Re-id) is far less discussed than supervised Re-id, since there are not labeled data that can be used to guide the person Re-id. Even though some works have been done on unsupervised Re-id, cross-domain Re-id is much challenging, and needs further research works. In this paper, without background transfer and other data enhancement of source domain training set, we propose a three-stage cross-domain Re-id model which considers the domain adaptation , self-supervised clustering re-training and joint loss training in one schema. In order to improve the adaptability of the model trained on the source data, cross-domain and cross-camera losses are considered. Then we do the self-supervised learning of clustering re-training in target dataset to learn the discriminative features in the target domain on the basis of the pre-trained model. Finally, we train the target data using the pseudo labels obtained by clustering with joint metric learning and representation learning . In particular, we replace the classic cross-entropy loss with label smoothing regularization loss so as to reduce fitting of false pseudo labels. Performance evaluation on benchmark datasets demonstrates the effectiveness of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Yao Ge and Li Liu and Huaxiang Zhang},
  doi          = {10.1016/j.asoc.2021.107793},
  journal      = {Applied Soft Computing},
  pages        = {107793},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-stage learning approach to cross-domain person re-identification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment classification using attention mechanism and
bidirectional long short-term memory network. <em>ASOC</em>,
<em>112</em>, 107792. (<a
href="https://doi.org/10.1016/j.asoc.2021.107792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data .},
  archive      = {J_ASOC},
  author       = {Peng Wu and Xiaotong Li and Chen Ling and Shengchun Ding and Si Shen},
  doi          = {10.1016/j.asoc.2021.107792},
  journal      = {Applied Soft Computing},
  pages        = {107792},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentiment classification using attention mechanism and bidirectional long short-term memory network},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantitative detection of cervical cancer based on time
series information from smear images. <em>ASOC</em>, <em>112</em>,
107791. (<a href="https://doi.org/10.1016/j.asoc.2021.107791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing cervical cancer detection methods usually screen the samples based on separated cells. Cell misclassification leads to poor robustness and accuracy, quantitative analysis is missed. Global smear information and cell relationship are also not fully utilized. Aiming at the mentioned limitations, a cervical quantitative detection framework which combines the fine-tuned Long Short-Term Memory Fully Convolutional Network and Fuzzy Nonlinear Regression is proposed. Time series-based​ screening improves the detection performance. Deoxyribonucleic Acid (DNA) value is better expressed by the rectified method using soft computing. A cervical dataset containing 657 samples is used for training and validation, accuracy, sensitivity, and specificity of 98.3\%, 98.1\% and 97.9\% are achieved with the time series features, providing an automatic and effective way for the computer-assisted screening of cervical cancer.},
  archive      = {J_ASOC},
  author       = {C.W. Zhang and D.Y. Jia and N.K. Wu and Z.G. Guo and H.R. Ge},
  doi          = {10.1016/j.asoc.2021.107791},
  journal      = {Applied Soft Computing},
  pages        = {107791},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantitative detection of cervical cancer based on time series information from smear images},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dependent tasks offloading based on particle swarm
optimization algorithm in multi-access edge computing. <em>ASOC</em>,
<em>112</em>, 107790. (<a
href="https://doi.org/10.1016/j.asoc.2021.107790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of emerging applications such as augmented reality , face recognition, and autonomous driving have stimulated the growth of demand for low-latency services, while the traditional cloud computing paradigm inevitably increases end-to-end latency. Multi-access edge computing deploys computing and storage resources to user terminals, which is expected to become an effective solution. Most of the existing researches on multi-access edge computing focus on the offloading of independent tasks, which cannot meet the challenge of a real scenario in which a task is composed of multiple interdependent subtasks. To bridge the gap, we formulate the problem of offloading multi-dependent tasks in multi-access edge computing considering both task completion time and execution cost. Since this offloading problem is NP-hard, a queue-based improved multi-objective particle swarm optimization is proposed. During the optimization process, we introduce the Pareto optimal relationship and define a transition probability to obtain the optimal solution. A large number of simulation results show that compared with other alternatives, the performance of our algorithm can be improved by about 3\%–25\%.},
  archive      = {J_ASOC},
  author       = {Shuyue Ma and Shudian Song and Lingyu Yang and Jingmei Zhao and Feng Yang and Linbo Zhai},
  doi          = {10.1016/j.asoc.2021.107790},
  journal      = {Applied Soft Computing},
  pages        = {107790},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dependent tasks offloading based on particle swarm optimization algorithm in multi-access edge computing},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Study on enhanced deep learning approaches for value-added
identification and segmentation of striation marks in bullets for
precise firearm classification. <em>ASOC</em>, <em>112</em>, 107789. (<a
href="https://doi.org/10.1016/j.asoc.2021.107789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-crime, identifying firearm types from remnants of fired bullets is a daunting task for any ballistic expert. The features used for identification are few and almost obliterated due to impact. This creates a solid foundation for applying AI to the process, the first being segmentation and enhancement of the features. However, the bullet’s metal surface makes image capturing and analysis more complicated than other common domains. In the present study, an attempt is made to extract one of the defining features of fired bullets, namely striations, using deep learning techniques, which will assist in automated firearm identification. U-net, a CNN-based semantic segmentation architecture, and two variants (the Inception U-net and Residual U-net architecture) achieve the objectives. The U-net architecture achieved 88\% accuracy with a training loss as low as 0.0231 after 700 epochs of training. The Inception U-net architecture and Residual U-net architecture achieved training accuracy of 88.30\% and 88.79\%, respectively, while their training loss reduced to as low as 0.0194 and 0.0151, respectively, with the same number of epochs. With 10 Fold Cross-Validation the accuracy of Residual U-net further enhanced to 89.70\%. One key observation from the three models’ training curve is that the convergence is significantly faster in Residual U-net than Inception U-net architecture, which, in turn, is much faster than the U-net architecture. Supported with statistical analysis, the study establishes that deep learning techniques prove valuable to segment the striation marks from the bullet images and help the ballistic experts identify firearms.},
  archive      = {J_ASOC},
  author       = {Subrat K. Dutta and Sudarshan Saikia and Abhilasa Barman and Ritumoni Roy and Kangkana Bora and Lipi B. Mahanta and R. Suresh},
  doi          = {10.1016/j.asoc.2021.107789},
  journal      = {Applied Soft Computing},
  pages        = {107789},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Study on enhanced deep learning approaches for value-added identification and segmentation of striation marks in bullets for precise firearm classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment-influenced trading system based on multimodal deep
reinforcement learning. <em>ASOC</em>, <em>112</em>, 107788. (<a
href="https://doi.org/10.1016/j.asoc.2021.107788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to advancements in deep learning , studies involving the use of deep learning techniques to solve investment decision-making problems are increasing. However, although numerous aspects of the stock market may affect trends in financial data, previous studies have only considered price fluctuations. Therefore, investors may lose out on profits because of the complicated financial market condition. In this study, a multimodal reinforcement trading system is developed, which makes use of three techniques: reinforcement learning , sentiment analysis , and multimodal learning. The agent considers not only the price fluctuations but also news information when making a trading decision. Multimodal learning which can merge different modalities of data to enhance the performance of the model, and sentiment analysis for understanding the sentiment of news are introduced. In addition, an influence model is proposed to enable our agents to gain special insights on the impact that news has on the market. The influence model considers the relationship between sentiment of news and time. The experimental results show that multimodal agents outperform price-concerned agents by at least 13.26\%. Our experimental results also indicate that the proposed influence model has the ability to shape the impact of news on the stock market. The model can aid the multimodal agents in evaluating the status of the market. The proposed multimodal reinforcement trading system is demonstrated to be robust in an experiment involving different sectors and evaluations by using various measures. In addition, because the data used are public, investors seeking to profit can easily implement the results of this paper. Therefore, it can be used in advanced research and financial applications.},
  archive      = {J_ASOC},
  author       = {Yu-Fu Chen and Szu-Hao Huang},
  doi          = {10.1016/j.asoc.2021.107788},
  journal      = {Applied Soft Computing},
  pages        = {107788},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentiment-influenced trading system based on multimodal deep reinforcement learning},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosting the oversampling methods based on differential
evolution strategies for imbalanced learning. <em>ASOC</em>,
<em>112</em>, 107787. (<a
href="https://doi.org/10.1016/j.asoc.2021.107787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class imbalance problem is a challenging problem in the data mining area. To overcome the low classification performance related to imbalanced datasets, sampling strategies are used for balancing the datasets. Oversampling is a technique that increases the minority class samples in various proportions. In this work, these 16 different DE strategies are used for oversampling the imbalanced datasets for better classification. The main aim of this work is to determine the best strategy in terms of Area Under the receiver operating characteristic (ROC) Curve (AUC) and Geometric Mean (G-Mean) metrics. 44 imbalanced datasets are used in experiments. Support Vector Machines (SVM), k-Nearest Neighbor (kNN), and Decision Tree (DT) are used as a classifier in the experiments. The best results are produced by 6th Debohid Strategy (DSt6), 1th Debohid Strategy (DSt1), and 3th Debohid Strategy (DSt3) by using kNN, DT, and SVM classifiers, respectively. The obtained results outperform the 9 state-of-the-art oversampling methods in terms of AUC and G-Mean metrics},
  archive      = {J_ASOC},
  author       = {Sedat Korkmaz and Mehmet Akif Şahman and Ahmet Cevahir Cinar and Ersin Kaya},
  doi          = {10.1016/j.asoc.2021.107787},
  journal      = {Applied Soft Computing},
  pages        = {107787},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Boosting the oversampling methods based on differential evolution strategies for imbalanced learning},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integration of SERVQUAL, analytical kano, and QFD using
fuzzy approaches to support improvement decisions in an entrepreneurial
education service. <em>ASOC</em>, <em>112</em>, 107786. (<a
href="https://doi.org/10.1016/j.asoc.2021.107786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality tools such as Kano’s model, SERVQUAL, and Quality Function Deployment (QFD) have been used to design and improve services. Although these tools can be used separately, their integration can provide a deep analysis of the service quality, generating opportunities for its improvement. However, the effective integration of these tools requires the proper handling of their variables, which contains imprecision and uncertainties since they are based on customer’s perceptions. Fuzzy approaches are feasible alternatives to overcome these limitations and integrate these tools. Nevertheless, sophisticated fuzzy methods can deal better with these limitations. This article proposes an integrative framework involving SERVQUAL, Analytical Kano (A-Kano), and QFD using fuzzy approaches (Fuzzy Inference System and 2-tuple fuzzy linguistic representation). The framework comprises four main phases concerning (i) the identification of quality attributes and service processes using the A-Kano and SERVQUAL; (ii) the integration of these two quality tools using the Fuzzy Inference System (FIS); (iii) the integration between A-Kano and SERVQUAL output and QFD matrix using 2-tuple fuzzy linguistic representation ; and (iv) the identification of improvement projects to address the opportunities identified in the previous phases. The proposed integrative framework was tested and validated in an entrepreneurial education company that provides experiential services. It contributes to providing a new method to assess the service quality perceptions, categorizing these perceptions according to their effects on customer satisfaction, prioritizing improvements, and identifying technical requirements. Mainly, this study presents a new proposal for integrating the SERVQUAL, A-Kano, and QFD using advanced fuzzy techniques, which contributes to overcoming the limitations found in the extant literature.},
  archive      = {J_ASOC},
  author       = {Fabiane L. Lizarelli and Lauro Osiro and Gilberto M.D. Ganga and Glauco H.S. Mendes and Guilherme R. Paz},
  doi          = {10.1016/j.asoc.2021.107786},
  journal      = {Applied Soft Computing},
  pages        = {107786},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integration of SERVQUAL, analytical kano, and QFD using fuzzy approaches to support improvement decisions in an entrepreneurial education service},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applying fuzzy scenarios for the measurement of operational
risk. <em>ASOC</em>, <em>112</em>, 107785. (<a
href="https://doi.org/10.1016/j.asoc.2021.107785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operational risk measurement assesses the probability to suffer financial losses in an organisation. The assessment of this risk is based primarily on the organisation’s internal data. However, other factors, such as external data and scenarios are also key elements in the assessment process. Scenarios enrich the data of operational risk events by simulating situations that still have not occurred and therefore are not part of the internal databases of an organisation but which might occur in the future or have already happened to other companies. Internal data scenarios often represent extreme risk events that increase the operational Value at Risk (OpVaR) and also the average loss. In general, OpVaR and the loss distribution are an important part of risk measurement and management. In this paper, a fuzzy method is proposed to add risk scenarios as a valuable data source to the data for operational risk measurement. We compare adding fuzzy scenarios with the possibility of adding non fuzzy or crisp scenarios. The results show that by adding fuzzy scenarios the tail of the aggregated loss distribution increases but that the effect on the expected average loss and on the OpVaR is lesser in its extent.},
  archive      = {J_ASOC},
  author       = {Isis Bonet and Alejandro Peña and Christian Lochmuller and Héctor Alejandro Patiño and Francisco Chiclana and Mario Góngora},
  doi          = {10.1016/j.asoc.2021.107785},
  journal      = {Applied Soft Computing},
  pages        = {107785},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Applying fuzzy scenarios for the measurement of operational risk},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards learning behavior modeling of military logistics
agent utilizing profit sharing reinforcement learning algorithm.
<em>ASOC</em>, <em>112</em>, 107784. (<a
href="https://doi.org/10.1016/j.asoc.2021.107784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-based modeling has become a beneficial tool in describing the complex and intelligent decision-making behaviors of military logistics entities, which is essential in exploring military logistics system . A challenging task in this field is the learning behavior modeling of military logistics agents. Profit sharing (PS) reinforcement learning algorithm is a representative exploitation-oriented method describing empirical reinforcement learning mechanism, and has been successfully applied to a variety of real-world problems. However, constructing the learning behavior model of military logistics agents is difficult by merely using the original PS algorithm. This difficulty is due to the actual characteristics of equipment support operations and military requirements, such as experience sharing, cooperative action, and hierarchical control. To address this issue, we propose an improved PS algorithm by introducing cooperative task reward correction parameters, experience sharing learning function, and superior command controlled function. We use the research methodology centering on the basic process of the improved PS algorithm as basis to construct the architecture of the learning behavior model of military logistics agents and its corresponding model of elements. Furthermore, we design the implementation algorithm of the learning behavior model. Lastly, we conduct a case study of a tactical military industrial logistics simulation system, thereby verifying the feasibility and effectiveness of the learning behavior model. We find that the improved PS algorithm and corresponding learning behavior model have more advantages than the original PS algorithm.},
  archive      = {J_ASOC},
  author       = {Xiong Li and Wei Pu and Xiaodong Zhao},
  doi          = {10.1016/j.asoc.2021.107784},
  journal      = {Applied Soft Computing},
  pages        = {107784},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards learning behavior modeling of military logistics agent utilizing profit sharing reinforcement learning algorithm},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Malware detection in edge devices with fuzzy oversampling
and dynamic class weighting. <em>ASOC</em>, <em>112</em>, 107783. (<a
href="https://doi.org/10.1016/j.asoc.2021.107783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Internet-of-things (IoT) domain, edge devices are used increasingly for data accumulation, preprocessing, and analytics. Intelligent integration of edge devices with Artificial Intelligence (AI) facilitates real-time analysis and decision making. However, these devices simultaneously provide additional attack opportunities for malware developers, potentially leading to information and financial loss. Machine learning approaches can detect such attacks but their performance degrades when benign samples substantially outnumber malware samples in training data. Existing approaches for such imbalanced data assume samples represented as continuous features and thus can generate invalid samples when malware applications are represented by binary features . We propose a novel malware oversampling technique that addresses this issue. Further, we propose two approaches for malware detection. Our first approach uses fuzzy set theory , while the second approach dynamically assigns higher priority to malware samples using a novel loss function. Combining our oversampling technique with these approaches, the proposed approach attains over 9\% improvement over competing methods in terms of F1_score. Our approaches can, therefore, result in enhanced privacy and security in edge computing services.},
  archive      = {J_ASOC},
  author       = {Mahbub E Khoda and Joarder Kamruzzaman and Iqbal Gondal and Tasadduq Imam and Ashfaqur Rahman},
  doi          = {10.1016/j.asoc.2021.107783},
  journal      = {Applied Soft Computing},
  pages        = {107783},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Malware detection in edge devices with fuzzy oversampling and dynamic class weighting},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A frequency count approach to multi-criteria recommender
system based on criteria weighting using particle swarm optimization.
<em>ASOC</em>, <em>112</em>, 107782. (<a
href="https://doi.org/10.1016/j.asoc.2021.107782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering (CF) is the most successful and extensively used recommendation technique in the field of Recommender Systems (RS). Generally, the collaborative filtering technique suffers from sparsity and cold start problems because of the absence of appropriate rating information for efficient similarity computation among users. To handle such issues and to identify similar users more efficiently, it is necessary to include other useful user–item details into the system through efficient fusion methods. The multi-criteria information has been proved as a beneficial and influential factor for improving the performance of the classical CF technique. In this research, we incorporate multi-criteria ratings into traditional CF technique using a frequency count approach. The similarity among users is computed using a newly derived common rating weight similarity (CRS) measure. Further, different users show different importance to various criteria of an item. Therefore, a particle swarm optimization algorithm is used to learn optimal weights on different criteria’s in the process of global similarity computation. The extensive experimental study driven on a benchmark dataset demonstrates significant improvements in prediction and recommendation qualities compared to the most commonly used similarity and heuristic approaches.},
  archive      = {J_ASOC},
  author       = {Mohammed Wasid and Rashid Ali},
  doi          = {10.1016/j.asoc.2021.107782},
  journal      = {Applied Soft Computing},
  pages        = {107782},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A frequency count approach to multi-criteria recommender system based on criteria weighting using particle swarm optimization},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedding regularized nonnegative matrix factorization for
structural reduction in multi-layer networks. <em>ASOC</em>,
<em>112</em>, 107781. (<a
href="https://doi.org/10.1016/j.asoc.2021.107781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of complex systems from nature and society, ranging from biological to technological systems, can be effectively modeled by networks. The multiplex character of real-world systems leads to distinct types of interactions that are categorized as edges belonging to various layers. And, the topological analysis of multi-layer networks discovers graph patterns that are critical for revealing the mechanisms of systems. An essential prerequisite of successful multi-layer networks analysis is how to remove the redundancy of networks, i.e., structural reduction of multi-layer networks. The current algorithms quantify the distance among various layers by using either the topological structure or the representative vectors , which fail to fully characterize the relationship among various layers, leading to low accuracy. To overcome this problem, we propose a nonnegative matrix factorization algorithm for multi-layer networks reduction (NMF-MNR), which jointly factorizes the topological structure and graph representation of various layers. Specifically, we adopt the network embedding to obtain low-dimensional graph representation of each layer, and then jointly decompose the adjacency matrix and graph representation to extract the feature matrix for each layer, which provides a better way to characterize layers of networks. The experimental results on both artificial and real-world networks demonstrate that NMF-MNR is more accurate and robust than state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Zhihao Huang and Zengfa Dou and Xiaoke Ma},
  doi          = {10.1016/j.asoc.2021.107781},
  journal      = {Applied Soft Computing},
  pages        = {107781},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Embedding regularized nonnegative matrix factorization for structural reduction in multi-layer networks},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computer aided pain detection and intensity estimation using
compact CNN based fusion network. <em>ASOC</em>, <em>112</em>, 107780.
(<a href="https://doi.org/10.1016/j.asoc.2021.107780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pain is an indication of uneasiness, and its assessment is crucial for medical diagnosis and treatment of patients. Subjective methods like visual inspection and self-report of pain, are prone to human errors. Hence, objective methods i.e. behavioural, physiological are being focused on by researchers in the recent past. In this paper, a behavioural indicator i.e. facial expression-based fully automated pain assessment system is proposed. This system includes a novel fusion structure of Convolutional Networks for assessing various pain intensities from raw facial images . Here, we have proposed joint learning of robust pain-related facial expression features from fused RGB appearance and shape-based latent representation. We have suggested that the joint learning from both feature representations results in a more robust pain assessment model as compared to learning from either of the representation independently. Given this, the proposed system used two shallower networks, i.e., Spatial Appearance Network and Shape Descriptor Network. The proposed model has been evaluated extensively on the UNBC-McMaster dataset for both pain classification and intensity estimation. For pain level classification the proposed technique achieved 0.94 of F1-score. Moreover, for pain intensity estimation the proposed model has achieved a mean absolute error of 0.22 and accuracy of 0.92.},
  archive      = {J_ASOC},
  author       = {Ashish Semwal and Narendra D. Londhe},
  doi          = {10.1016/j.asoc.2021.107780},
  journal      = {Applied Soft Computing},
  pages        = {107780},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Computer aided pain detection and intensity estimation using compact CNN based fusion network},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarse–fine surrogate model driven multiobjective
evolutionary fuzzy clustering algorithm with dual memberships for noisy
image segmentation. <em>ASOC</em>, <em>112</em>, 107778. (<a
href="https://doi.org/10.1016/j.asoc.2021.107778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the segmentation performance and boost evolutionary efficiency of multiobjective evolutionary clustering algorithms on noisy images , this paper proposes a coarse–fine surrogate model driven multiobjective evolutionary fuzzy clustering algorithm with dual membership functions (CFS-MOEFC). The existing fuzzy clustering validity-based fitness functions generally consider one single fuzzy membership function , which cannot fully process the uncertainty in an image. Moreover, most existing fitness functions used in image segmentation only utilize one type of spatial information derived from the image, which cannot behave robust on images with uncertain types of noise. To deal with the above problems, dual fuzzy membership functions are first designed by utilizing the local and non-local image spatial information. Then, the designed dual membership functions and the two kinds of spatial information are both used to construct spatial information-motivated fitness functions for image segmentation . To promote the time efficiency, the coarse–fine surrogate model is employed to assist the evolutionary optimization process, such as approximating the fitness functions instead of real function evaluations and evolving satisfactory cluster centers for CFS-MOEFC. Besides, a dual memberships-driven cluster validity index combining the local and non-local spatial information is designed for selecting an optimal solution from the final non-dominated solution set of CFS-MOEFC. Experiments on Berkeley and Magnetic Resonance (MR) images indicate that CFS-MOEFC greatly improves the segmentation accuracy on images with multiple types of noise, preserves more significant detailed image information, and behaves well on time cost comparing with state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Feng Zhao and Feifan Liu and Chaoqi Li and Hanqiang Liu and Rong Lan and Jiulun Fan},
  doi          = {10.1016/j.asoc.2021.107778},
  journal      = {Applied Soft Computing},
  pages        = {107778},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Coarse–fine surrogate model driven multiobjective evolutionary fuzzy clustering algorithm with dual memberships for noisy image segmentation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A feature interaction learning approach for crowdfunding
project recommendation. <em>ASOC</em>, <em>112</em>, 107777. (<a
href="https://doi.org/10.1016/j.asoc.2021.107777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdfunding is an emerging internet platform that provides financial support for people in need. With the development of crowdfunding platforms , the number of projects released on these platforms is increasing, and thus it is challenging for lenders to find suitable crowdfunding projects quickly. A personalized recommender system is helpful for solving this problem. To this end, a crowdfunding project recommendation approach is proposed in this work for predicting how likely a lender is to fund a project. Specifically, given the fact that the lenders consider not only their interests but also the benefits they can obtain when funding, we first design a module that predicts the success rate of the projects. Then, a feature interaction learning model based on deep learning , called the crowdfunding feature interaction learning model, is proposed. It integrates all features, automatically recognizes the importance of features, and learns the feature interaction. This allows us to identify combination of useful features more accurately and provide effective predictions. Extensive experiments are conducted on a dataset collected from a real-world crowdfunding platform, and the results show that our approach has a 4.57\% improvement on AUC (area under the curve) compare with the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Yingyuan Xiao and Chichang Liu and Wenguang Zheng and Hongya Wang and Ching-Hsien Hsu},
  doi          = {10.1016/j.asoc.2021.107777},
  journal      = {Applied Soft Computing},
  pages        = {107777},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature interaction learning approach for crowdfunding project recommendation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian neural networks for virtual flow metering: An
empirical study. <em>ASOC</em>, <em>112</em>, 107776. (<a
href="https://doi.org/10.1016/j.asoc.2021.107776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have presented promising results from the application of machine learning (ML) to the modeling of flow rates in oil and gas wells. Encouraging results and advantageous properties of ML models, such as computationally cheap evaluation and ease of calibration to new data, have sparked optimism for the development of data-driven virtual flow meters (VFMs). Data-driven VFMs are developed in the small data regime, where it is important to question the uncertainty and robustness of models. The modeling of uncertainty may help to build trust in models, which is a prerequisite for industrial applications. The contribution of this paper is the introduction of a probabilistic VFM based on Bayesian neural networks. Uncertainty in the model and measurements is described, and the paper shows how to perform approximate Bayesian inference using variational inference. The method is studied by modeling on a large and heterogeneous dataset, consisting of 60 wells across five different oil and gas assets. The predictive performance is analyzed on historical and future test data, where an average error of 4\%–6\% and 8\%–13\% is achieved for the 50\% best performing models, respectively. Variational inference appears to provide more robust predictions than the reference approach on future data. Prediction performance and uncertainty calibration is explored in detail and discussed in light of four data challenges. The findings motivate the development of alternative strategies to improve the robustness of data-driven VFMs.},
  archive      = {J_ASOC},
  author       = {Bjarne Grimstad and Mathilde Hotvedt and Anders T. Sandnes and Odd Kolbjørnsen and Lars S. Imsland},
  doi          = {10.1016/j.asoc.2021.107776},
  journal      = {Applied Soft Computing},
  pages        = {107776},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian neural networks for virtual flow metering: An empirical study},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Re-evaluation of the healthcare service quality criteria for
the covid-19 pandemic: Z-number fuzzy cognitive map. <em>ASOC</em>,
<em>112</em>, 107775. (<a
href="https://doi.org/10.1016/j.asoc.2021.107775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hospitals as healthcare centers have faced many challenges with the Covid-19 spread, which results in a decline in the quality of health care. Because the number of patients referred to hospitals increases dramatically during the pandemic, providing high-quality services and satisfying them is more important than ever to maintain community health and create loyal customers in the future. However, health care quality standards are generally designed for normal circumstances. The SERVPERF standard, which measures customer perceptions of service quality, has also been adjusted for hospital service quality measurement. In this study, the SERVPERF standard criteria for health services are evaluated in the Covid-19 pandemic. For this purpose, by considering the causal relationships between the criteria and using Z-Number theory and Fuzzy Cognitive Maps (FCMs), the importance of these criteria in the prevalence of infectious diseases was analyzed. According to the results, hospital reliability, hospital hygiene, and completeness of the hospital with ratios 0.9559, 0.9305, and 0.9268 are respectively the most influential criteria in improving the quality of health services in the spread of infectious diseases circumstances such as the Covid-19 pandemic. A review of the literature shows that in previous studies, comprehensive research has not been done on prioritizing the criteria for measuring the quality of health services in the context of the spread of infectious diseases.},
  archive      = {J_ASOC},
  author       = {Naeira Elyas Pour Babroudi and Kamyar Sabri-Laghaie and Nazli Ghanbari Ghoushchi},
  doi          = {10.1016/j.asoc.2021.107775},
  journal      = {Applied Soft Computing},
  pages        = {107775},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Re-evaluation of the healthcare service quality criteria for the covid-19 pandemic: Z-number fuzzy cognitive map},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving multi-depot electric vehicle scheduling problem by
column generation and genetic algorithm. <em>ASOC</em>, <em>112</em>,
107774. (<a href="https://doi.org/10.1016/j.asoc.2021.107774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric vehicles are increasingly applied in bus enterprises due to their advantages of environmental protection and low cost. Although there exist many studies on fuel vehicle scheduling, there still lack studies on electric vehicle scheduling in public transit. In this paper, we study a multi-depot electric vehicle scheduling problem (MD-EVSP) in public transit and propose a genetic algorithm based column generation approach (GA-CG) for MD-EVSP. A column refers to the route of a vehicle. First, a heuristic is devised to create a set of initial columns. Then, starting from the initial columns, the column generation approach with a label correcting algorithm is used to generate a set of candidate columns. Finally, a genetic algorithm is devised to select a subset of columns from the column set to construct the final solution. A solution coding scheme is designed to represent a subset of columns, that is, a candidate solution to MD-EVSP. An elite strategy is integrated into the genetic algorithm to improve the global search capability. GA-CG is applied to a real-world problem with three bus lines in Qingdao, China. Experiments show that compared with the branch and price (BP) algorithm and the manual scheduling scheme, GA-CG can effectively solve the problem and its computational time is about 40 times shorter than the BP algorithm.},
  archive      = {J_ASOC},
  author       = {Chunlu Wang and Congcong Guo and Xingquan Zuo},
  doi          = {10.1016/j.asoc.2021.107774},
  journal      = {Applied Soft Computing},
  pages        = {107774},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving multi-depot electric vehicle scheduling problem by column generation and genetic algorithm},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective algorithm for crop pattern optimization in
agriculture. <em>ASOC</em>, <em>112</em>, 107772. (<a
href="https://doi.org/10.1016/j.asoc.2021.107772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crop growth and crop yield in agriculture depend upon many factors such as weather conditions, soil type, and application of fertilizers. The crop net return can be increased by deciding suitable crops for a particular land depending upon its weather conditions. The application of an appropriate amount of fertilizers also promotes crop growth and yield. On the other hand, the use of fertilizers needs to be minimized to reduce the capital cost as well as to prevent its harmful effect on soil and the environment. This paper models the problem of increasing net crop benefit and reducing the application of fertilizers as multi-objective optimization functions. A hybrid CSA-PSO optimization algorithm is proposed for solving multi-objective problems by combining crow search algorithm (CSA) and particle swarm optimization (PSO). The performance of the proposed algorithm is evaluated against CEC 2009 benchmark functions . This algorithm is implemented for crop pattern optimization in India’s Telangana state, which depicts the proposed algorithm’s feasibility and effectiveness.},
  archive      = {J_ASOC},
  author       = {Sonal Jain and Dharavath Ramesh and Diptendu Bhattacharya},
  doi          = {10.1016/j.asoc.2021.107772},
  journal      = {Applied Soft Computing},
  pages        = {107772},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective algorithm for crop pattern optimization in agriculture},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ELTHON: An escher-like tile design method using hierarchical
optimization. <em>ASOC</em>, <em>112</em>, 107771. (<a
href="https://doi.org/10.1016/j.asoc.2021.107771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Escher-like tiling attempts to design a tile whose copies cover a plane with no overlaps and no gaps. Deforming a given image shape into a tileable shape while maintaining the original shape as far as possible is a difficult problem. A tileable shape similar to a given image can be generated by an analytical optimization method (AOM) that requires no iterative calculations. However, the generated shape is often non-tileable due to edge self-intersections; moreover, as the method is sensitive to small changes of the input image shape, the input shape must be adjusted manually by much trial and error. To avoid this problem, this paper proposes an Escher-Like Tile design method using Hierarchical OptimizatioN (ELTHON), which divides the tile-design problem and resolves the subproblems by two different methods. The given problem (such as the goal figure shape) is modified by an upper-layer optimizer, and feasible solutions (such as the tileable shapes) are found by a lower-layer optimizer based on AOM. Because the upper layer employs metaheuristics such as a genetic algorithm , which support flexible objective functions and constraints, different types of figure similarity metrics and constraints can be selected to avoid the generation of non-tileable shapes. Experimental results showed that ELTHON designed tiles without violating the constraints, whereas 58.9\% of the solutions found by AOM violated the constraints. Additionary, in 16 of 32 figures tested, ELTHON produced tileable shapes with higher similarity to given images compared with AOM, whereas AOM outperformed ELTHON in only two figures.},
  archive      = {J_ASOC},
  author       = {Asuka Hisatomi and Hitomi Koba and Kazunori Mizuno and Satoshi Ono},
  doi          = {10.1016/j.asoc.2021.107771},
  journal      = {Applied Soft Computing},
  pages        = {107771},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ELTHON: An escher-like tile design method using hierarchical optimization},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated production and transportation scheduling
problem with order acceptance and resource allocation decisions.
<em>ASOC</em>, <em>112</em>, 107770. (<a
href="https://doi.org/10.1016/j.asoc.2021.107770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the global market competition, coordinating decisions and integrating different aspects of supply chains would help managers promote the overall performance of systems, and reduce the overall costs. The integration of production and transportation is one of the essential considerations of manufacturers to achieve more efficiency levels. In this paper, an integrated production and transportation scheduling problem is addressed. The objective is to maximize the revenue of accepted orders and to minimize the sum of delivery cost, weighted tardiness cost, and fixed cost of vehicles, by considering resource allocation and resource-dependent processing times. The problem is regarded as strong NP-Hard. A Mixed Integer Linear Programming (MILP) is proposed. Also, two meta-heuristic solution approaches, based on Adaptive Genetic Algorithm (AGA) and Tabu Search (TS) are proposed to solve this problem. The computational experiments based on small-scale and large-scale analyses are provided to assess the effects of different parameters of the problems on both optimality and solving time criteria. It is deduced that the proposed AGA has a better performance and higher efficiency, rather than the proposed TS, in terms of optimality . The results indicate the capability of the proposed algorithms in solving the real-life problems in an efficient manner.},
  archive      = {J_ASOC},
  author       = {Sajede Aminzadegan and Mohammad Tamannaei and Majid Fazeli},
  doi          = {10.1016/j.asoc.2021.107770},
  journal      = {Applied Soft Computing},
  pages        = {107770},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated production and transportation scheduling problem with order acceptance and resource allocation decisions},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A rough margin-based multi-task ν-twin support vector
machine for pattern classification. <em>ASOC</em>, <em>112</em>, 107769.
(<a href="https://doi.org/10.1016/j.asoc.2021.107769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twin support vector machine (TSVM) has attracted significant attention in recent years, but it is suitable for solving the single-task learning (STL) problems. It trains each task independently and neglects the relationships among all tasks. Conversely, multi-task learning (MTL) explores the shared information between multiple correlated tasks, which obtains a better classifier than STL. Nevertheless, the existing multi-task twin support vector machines give the same penalties to the misclassified samples . In fact, the misclassified samples play a different role in generating separating hyperplane . Motivated by above studies, we put forward a rough margin-based multi-task ν ν -twin support vector machine (rough MT- ν ν -TSVM) in this paper. The proposed rough MT- ν ν -TSVM gives different penalties to the misclassified samples depending on their positions. It not only takes full advantage of rough ν ν -TSVM, but also discovers the commonality among tasks and maintains the individuality of each task. Therefore, compared with the state-of-the-art algorithms, our method yields better classification performance. In addition, we apply it to Chinese wine dataset to verify the effectiveness. Finally, the related extensions are further discussed, especially a fast SMO-type decomposition method (SDM) is introduced to handle relatively large-scale problems for acceleration. Comprehensive experiments are conducted on eleven benchmark datasets and an image dataset. The results demonstrate that our proposed algorithm can avoid over-fitting and achieve better classification accuracy , meanwhile it does not increase computational time compared with DMTSVM and MT- ν ν -TSVM.},
  archive      = {J_ASOC},
  author       = {Ran An and Yitian Xu and Xuhua Liu},
  doi          = {10.1016/j.asoc.2021.107769},
  journal      = {Applied Soft Computing},
  pages        = {107769},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A rough margin-based multi-task ν-twin support vector machine for pattern classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global solar radiation prediction: Application of novel
hybrid data-driven model. <em>ASOC</em>, <em>112</em>, 107768. (<a
href="https://doi.org/10.1016/j.asoc.2021.107768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the significant prerequisites for harvesting solar energy is precise global solar radiation (GHI) forecasts. However, variability and uncertainty are inherent characteristics of solar radiation. It is challenging to show better generalization using current data analysis approaches. Thus, this research presents a new intelligence framework by hybridizing Support Vector Regression (SVR) with the Grasshopper Optimization Algorithm (GOA) and the Boruta-based feature selection algorithm (BA) for forecasting GHI values at different sites of Saudi Arabia. Interestingly, the most significant distinction that differentiates this proposed prediction model (SVR-GOA-BA K K ) from other models is that the GOA is automatically employed to search for optimal SVR’s hyperparameters. In contrast, these hyperparameters are chosen randomly and manually in conventional models. Consequently, the contribution helps save time, reduce cost, and avoid the possibility of models’ overfitting or underfitting caused by random and manual selection. A diversity of statistical measures has justified the proposed model’s effectiveness and superiority. In terms of mean absolute percentage error (MAPE), the proposed model outperformed the standalone SVR models by 32.15–39.69\% at different study sites. In tuning the SVR’s parameters, GOA outperforms popular optimization algorithms. All the simulation test results demonstrate the superiority of the proposed model. Hence, the proposed approach provides a foundation for precise solar radiation forecasting, which can aid in the growth of renewable-energy-based technologies.},
  archive      = {J_ASOC},
  author       = {Massoud Alrashidi and Musaed Alrashidi and Saifur Rahman},
  doi          = {10.1016/j.asoc.2021.107768},
  journal      = {Applied Soft Computing},
  pages        = {107768},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Global solar radiation prediction: Application of novel hybrid data-driven model},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-varying black–litterman portfolio optimization using a
bio-inspired approach and neuronets. <em>ASOC</em>, <em>112</em>,
107767. (<a href="https://doi.org/10.1016/j.asoc.2021.107767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Black–Litterman model is a very important analytical tool for active portfolio management because it allows investment analysts to incorporate investor’s views into market equilibrium returns. In this paper, we define and study the time-varying Black–Litterman portfolio optimization under nonlinear constraints (TV-BLPONC) problem as a nonlinear programming (NLP) problem. More precisely, the nonlinear constraints refer to transaction costs and cardinality constraints. Furthermore, a speedy weights-and-structure-determination (WASD) algorithm for the power-activation feed-forward neuronet (PFN) is presented to solve time-series modeling and forecasting problems. Inhere, the investor’s views in the TV-BLPONC problem are considered as a forecasting problem and, thus, they are produced by the WASD-based PFN. In addition, using the beetle antennae search (BAS) algorithm a computational method is introduced to solve the TV-BLPONC problem. For all we know, this is an innovative approach that integrates modern neural network and meta-heuristic optimization methods to provide a solution to the TV-BLPONC problem in large portfolios. Our approach is tested on portfolios of up to 90 stocks with real-world data, and the results show that it is more than 30 times faster than other methods. Our technique’s speed and precision are verified in this way, showing that it is an outstanding alternative to ordinary methods. In order to support and promote the findings of this work, we have constructed two complete MATLAB packages for the interested user, which are freely available through GitHub.},
  archive      = {J_ASOC},
  author       = {Theodore E. Simos and Spyridon D. Mourtas and Vasilios N. Katsikis},
  doi          = {10.1016/j.asoc.2021.107767},
  journal      = {Applied Soft Computing},
  pages        = {107767},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time-varying Black–Litterman portfolio optimization using a bio-inspired approach and neuronets},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Building fuzzy relationships between compressive strength
and 3D microstructural image features for cement hydration using
gaussian mixture model-based polynomial radial basis function neural
networks. <em>ASOC</em>, <em>112</em>, 107766. (<a
href="https://doi.org/10.1016/j.asoc.2021.107766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, chemical compositions and physical properties such as the contents of clinkers, surface area and particle size distribution are used to estimate cement compressive strength (CCS). However, the conventional approaches are limited by the high complexity of physical changes and chemical reactions during cement hydration, which perform poorly facing the complex and uncertain evolution of cement paste . Considering that the cement microstructure can directly reflect the state of cement hydration and the information related to CCS at microscale , microtomography, which can image three-dimensional microstructure of cement paste , offers scientists another way to study CCS. Moreover, it enables us to understand the formation mechanism of the physical properties of cement and helps design high-performance materials. This paper studies the relationship between cement microstructure and compressive strength using fuzzy neural networks . The fuzzy relationships between CCS and microstructural image features are built as the “if-then” format using the polynomial-based radial basis function networks with Gaussian mixture model (GMM-PRBFNNs) from microtomography images. The GMM-PRBFNNs are proposed to improve the performance by using GMM to generate membership functions for constructing the premise of the fuzzy rules. Moreover, four types of polynomials such as constant, linear, quadratic and modified quadratic are considered as the weights between the hidden layer and the output layer for the consequent of fuzzy rules. Experimental results manifest that the built fuzzy relationships not only perform well in approximating CCS but are also easy to comprehend.},
  archive      = {J_ASOC},
  author       = {Liangliang Zhang and Sung-Kwun Oh and Witold Pedrycz and Bo Yang and Yamin Han},
  doi          = {10.1016/j.asoc.2021.107766},
  journal      = {Applied Soft Computing},
  pages        = {107766},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Building fuzzy relationships between compressive strength and 3D microstructural image features for cement hydration using gaussian mixture model-based polynomial radial basis function neural networks},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-channel convolutional neural network approach to
automate the citation screening process. <em>ASOC</em>, <em>112</em>,
107765. (<a href="https://doi.org/10.1016/j.asoc.2021.107765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The systematic literature review (SLR) process is separated into several steps to increase rigor and reproducibility. The selection of primary studies (i.e., citation screening) is an important step in the SLR process. The citation screening process aims to identify the relevant primary studies fairly and with high rigor using selection criteria. Through the study selection criteria, reviewers determine whether an article should be included or excluded from the SLR. However, the screening process is highly time-consuming and error-prone as the researchers must read each title and possibly hundreds to thousands of abstracts and full-text documents. This study aims to automate the citation screening process using Deep Learning algorithms. With this, it is aimed to reduce the time and costs of the citation screening process and increase the precision and recall of the relevant primary studies. A Multi-Channel Convolutional Neural Network (CNN) is proposed, which can automatically classify a given set of citations. As the architecture uses the title and abstract as features, our end-to-end pipeline is domain-independent. We have performed six experiments to assess the performance of Multi-Channel CNNs across 20 publicly available systematic literature review datasets. It was shown that for 18 out of 20 review datasets, the proposed method achieved significant workload savings of at least 10\%, while in several cases, our model yielded a statistically significantly better performance over two benchmark review datasets. We conclude that Multi-Channel CNNs are effective for the citation screening process in SLRs. Multi-Channel CNNs perform best on large datasets of over 2500 samples with few abstracts missing.},
  archive      = {J_ASOC},
  author       = {Raymon van Dinter and Cagatay Catal and Bedir Tekinerdogan},
  doi          = {10.1016/j.asoc.2021.107765},
  journal      = {Applied Soft Computing},
  pages        = {107765},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-channel convolutional neural network approach to automate the citation screening process},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel rule-based evolving fuzzy system applied to the
thermal modeling of power transformers. <em>ASOC</em>, <em>112</em>,
107764. (<a href="https://doi.org/10.1016/j.asoc.2021.107764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big Data advancements motivate researchers to develop and improve intelligent models to deal efficiently and effectively with data. In this scenario, time series forecasting obtains even more attention. The literature demonstrated the better performance of such models in this subject. Forecasting is widely used in strategic planning to support decision-making, providing competitive differential to organizations. In this paper, a novel rule-based evolving Fuzzy System is proposed for time series forecasting. This is a robust model able to develop and update its structure in unknown environments, capture dynamics and changes of streams, and produce accurate results even when dealing with complex data. The introduced model implements the distance correlation to improve the rules’ quality by reducing their standard deviation. The model is evaluated using two synthetic datasets : the Mackey–Glass time-series and the nonlinear dynamic system identification. And finally, the introduced system is implemented to predict the hot spot temperature using three datasets from a real power transformer . Hot spot monitoring is necessary to maximize the load capacity and the lifespan of power transformers. The proposed method is evaluated in terms of root-mean-square error, non-dimensional index error, mean absolute error , runtime, and the number of final rules. The results are compared with traditional forecasting models and with some related state-of-the-art rule-based evolving Fuzzy Systems. The new evolving Fuzzy System outperformed the compared models for the Mackey–Glass time-series and the power transformers datasets concerning the errors. A statistical test comprised the superior performance of the introduced model. The algorithm also obtained a competitive execution time and number of final rules. The results demonstrate the high level of autonomy and adaptation of the model to predict accurately complex and non-stationary data. Seeing the importance of accurate models to deal with data to support decision-making, the results suggest the model’s implementation as a forecasting tool in strategic planning.},
  archive      = {J_ASOC},
  author       = {Kaike Sa Teles Rocha Alves and Eduardo Pestana de Aguiar},
  doi          = {10.1016/j.asoc.2021.107764},
  journal      = {Applied Soft Computing},
  pages        = {107764},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel rule-based evolving fuzzy system applied to the thermal modeling of power transformers},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The algorithmic composition for music copyright protection
under deep learning and blockchain. <em>ASOC</em>, <em>112</em>, 107763.
(<a href="https://doi.org/10.1016/j.asoc.2021.107763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To strengthen music copyright protection effectively, a new deep learning neural network music composition neural network (MCNN) is proposed. The probability distribution of LSTM generation is adjusted by constructing a reasonable reward function. Music theory rules are used to constrain the generated music style to realize the intelligent generation of specific music style. Then, the digital music copyright protection system based on blockchain is constructed from three perspectives of confirming right, using right, and protecting right. The validity of the model is further verified by relevant data. The results show that the composition algorithm based on deep learning can realize music creation, and the qualified rate reaches 95.11\%. Compared with the composition algorithm in the latest study, the model achieves 62.4 percent satisfaction with subjective samples and a recognition rate of 75.6 percent for musical sentiment classification. It is proved that the music copyright protection model based on block chain can ensure that the copyright owners of works obtain corresponding economic benefits from various distribution channels, which is helpful to build a harmonious music market environment. In short, the innovation of this study is reflected in that it fills in the gap of detailed comparative study of the differences in the application of different models, realizes the framework of music copyright protection system, and provides convenient conditions for composers.},
  archive      = {J_ASOC},
  author       = {Nana Wang and Hui Xu and Feng Xu and Lei Cheng},
  doi          = {10.1016/j.asoc.2021.107763},
  journal      = {Applied Soft Computing},
  pages        = {107763},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The algorithmic composition for music copyright protection under deep learning and blockchain},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). K-relevance vectors: Considering relevancy beside nearness.
<em>ASOC</em>, <em>112</em>, 107762. (<a
href="https://doi.org/10.1016/j.asoc.2021.107762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study combines two different learning paradigms, k-nearest neighbor (k-NN) rule, as memory-based learning paradigm and relevance vector machines (RVM), as statistical learning paradigm. The purpose is to improve the performance of k-NN rule through selection of important features with sparse Bayesian learning method. This combination is performed in kernel space and is called k-relevance vector (k-RV). The proposed model significantly prunes irrelevant features. Our combination of k-NN and RVM presents a new concept of similarity measurement for k-NN rule, we call it k-relevancy which aims to consider “relevancy” in the feature space beside “nearness” in the input space. We also introduce a new parameter, responsible for early stopping of iterations in RVM that is able to improve the classification accuracy . Intensive experiments are conducted on several classification datasets from University of California Irvine (UCI) repository and two real datasets from computer vision domain. The performance of k-RV is highly competitive compared to a few state-of-the-arts in terms of classification accuracy .},
  archive      = {J_ASOC},
  author       = {Sara Hosseinzadeh Kassani and Farhood Rismanchian and Peyman Hosseinzadeh Kassani},
  doi          = {10.1016/j.asoc.2021.107762},
  journal      = {Applied Soft Computing},
  pages        = {107762},
  shortjournal = {Appl. Soft. Comput.},
  title        = {K-relevance vectors: Considering relevancy beside nearness},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constructing a stock-price forecast CNN model with gold and
crude oil indicators. <em>ASOC</em>, <em>112</em>, 107760. (<a
href="https://doi.org/10.1016/j.asoc.2021.107760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose algorithms to predict future stock market trends based on 8 different input features, including financial technology indicators, gold prices, a gold price volatility index, crude oil price, a crude oil price volatility index, and other characteristic data using two different labeling methods with separate classification algorithms of two and three output categories, respectively including predicted stock price changes (up and down) and recommended trading actions (buy, sell, and hold), and analyze the validity of these characteristic data in terms of their ability to predict future trends. The S&amp;P 500 (GSPC) is the target of these forecasts. Sample data from 2010 to 2018 are divided 8:2, between training and verification data, while data from 2019 are used to test the proposed approach. CNN and LSTM models are used for comparison of classification accuracy and investment returns, respectively. Bayesian optimization (BO) hyperparameters are used to improve the accuracy of the model and increase the return on investment (ROI) of the output predictions. The purpose of this study is to verify whether using gold prices, a gold volatility index, crude oil price, and a crude oil price volatility indices as input features can enable a deep learning model accurately to predict future stock price trends, and to discuss separately the applicability of CNN and LSTM models to the abovementioned characteristics and financial indicators. We also present the results of experiments conducted to evaluate the proposed method in terms of classification accuracy and confusion matrix . In the case of three-category classification, the model takes feature data as input to outputs a predicted trading order on whether to buy, sell, or hold a given set of stocks tomorrow as well as the timing of entry and exit from each position, and also backtests the data outside the sample to find the combination of characteristics and indicators best maximizing ROI. Using this three-category method, we obtain a comprehensive ROI for a given set of individual stocks and assess whether each type of stock is suitable for the prediction model based on input features such as gold and crude oil or the fields that are suitable for the given feature. Experimental results show that the proposed approach as able to predict whether stock price will rise or fall in the next 10 days, and the model accuracy rate can reach 67\%. The results of experiments on the proposed combined CNN model with eight features, referred to as CNN8, achieved an ROI on 2019 data outside the sample period of up to 13.23\%, which was superior to the 12.08\% and 11.06\% obtained by the models designed CNN4 (CNN with four input features) and LSTM8(LSTM with eight input features), respectively. The F1 score increased from 0.75 0.79 as a result of applying BO. The results indicate that considering the price of gold, the gold volatility index, crude oil price, and crude oil price volatility index can help obtain better ROI for companies in certain fields, such as the semiconductor, petroleum, and automotive industries, rather than merely considering financial indicators. However, for companies related to apparel, fast food, and copy processing, the input characteristics of purely financial technical indicators were found to be suitable.},
  archive      = {J_ASOC},
  author       = {Yu-Chen Chen and Wen-Chen Huang},
  doi          = {10.1016/j.asoc.2021.107760},
  journal      = {Applied Soft Computing},
  pages        = {107760},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constructing a stock-price forecast CNN model with gold and crude oil indicators},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate multi-class image segmentation using weak
continuity constraints and neutrosophic set. <em>ASOC</em>,
<em>112</em>, 107759. (<a
href="https://doi.org/10.1016/j.asoc.2021.107759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a multi-class image segmentation method based on uncertainty management by weak continuity constraints and neutrosophic set (NS). To manage the uncertainties in the segmentation process , an image is mapped into the NS domain. In the NS domain, the image is represented as true, false, and indeterminate subsets. In the proposed method, accurate segmentation is achieved by minimizing an energy function in the NS domain. The theory of weak continuity constraints is integrated into the NS domain to generate the energy function. The weak continuity constraints take into account the spatial and boundary information of the segments to manage the uncertainties in the segmentation process . The proposed method can automatically segment an image iteratively without any prior knowledge about the number of classes. The performance of the proposed method is compared with state-of-the-art methods and it is found to be quite satisfactory. The proposed method’s performance under noise perturbations is statistically validated using a modified Cramer–Rao bound. The bound predicts the performance of image segmentation algorithms and serves as a benchmark for segmentation results.},
  archive      = {J_ASOC},
  author       = {Soumyadip Dhar and Malay K. Kundu},
  doi          = {10.1016/j.asoc.2021.107759},
  journal      = {Applied Soft Computing},
  pages        = {107759},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Accurate multi-class image segmentation using weak continuity constraints and neutrosophic set},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization of urban heat effect mitigation based on
multi-type ant colony algorithm. <em>ASOC</em>, <em>112</em>, 107758.
(<a href="https://doi.org/10.1016/j.asoc.2021.107758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The urban heat effect brings with a series of challenges for urban ecology and population health. In order to mitigate the adverse impacts of the urban heat effect quantitatively and effectively, a new land use search and exchange strategy is proposed to improve the multi-type ant colony algorithm in this paper and applied to a case study of Kunming, China. The results reveal the following: (1) the optimization algorithm can quickly locate the high-temperature area of the city and optimize the land use composition of this area; (2) the mean surface temperature of the core area can be reduced by 0.43–0.85 °C after land use optimization; and (3) Compared with Pareto optimization strategy and particle swarm optimization algorithm, the method proposed in this paper can achieve better optimization effect, higher computing efficiency and stability. This method can provide an effective scientific basis for sustainable urban planning and design, urban ecology and livability, and other urban retrofits .},
  archive      = {J_ASOC},
  author       = {Yaping Zhang and Xu Chen and Danjv Lv and Yan Zhang},
  doi          = {10.1016/j.asoc.2021.107758},
  journal      = {Applied Soft Computing},
  pages        = {107758},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of urban heat effect mitigation based on multi-type ant colony algorithm},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical forecast reconciliation with machine learning.
<em>ASOC</em>, <em>112</em>, 107756. (<a
href="https://doi.org/10.1016/j.asoc.2021.107756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last 15 years, studies on hierarchical forecasting have moved away from single-level approaches towards proposing linear combination approaches across multiple levels of the hierarchy. Such combinations offer coherent reconciled forecasts, improved forecasting performance and aligned decision-making. This paper proposes a novel hierarchical forecasting approach based on machine learning . The proposed method allows for non-linear combinations of the base forecasts, thus being more general than linear approaches. We structurally combine the objectives of improved post-sample empirical forecasting accuracy and coherence. Due to its non-linear nature, our approach selectively combines the base forecasts in a direct and automated way without requiring that the complete information must be used for producing reconciled forecasts for each series and level. The proposed method is evaluated both in terms of accuracy and bias using two different data sets coming from the tourism and retail industries. Our results suggest that the proposed method gives superior point forecasts than existing approaches, especially when the series comprising the hierarchy are not characterized by the same patterns.},
  archive      = {J_ASOC},
  author       = {Evangelos Spiliotis and Mahdi Abolghasemi and Rob J. Hyndman and Fotios Petropoulos and Vassilios Assimakopoulos},
  doi          = {10.1016/j.asoc.2021.107756},
  journal      = {Applied Soft Computing},
  pages        = {107756},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical forecast reconciliation with machine learning},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep subclass reconstruction network for fault diagnosis of
rotating machinery under various operating conditions. <em>ASOC</em>,
<em>112</em>, 107755. (<a
href="https://doi.org/10.1016/j.asoc.2021.107755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real-world industrial application, all measured vibration signals are usually suffered from substantial spurious factors and characterized as large intra-class and intra-subclass variations due to changes in the operating conditions (i.e., working loads, shaft speeds), which seriously deteriorates most of the machine learning methods’ ability to learn the discriminative feature representations. In this work, we propose a novel subclass reconstruction network (SCRN) to learn discriminative feature representations from raw vibration signals under different working conditions by suppressing the intra-class and intra-subclass variations in the feature space. Specifically, a novel and simple average strategy is developed to represent the cluster centroid of each subclass information as an effective supervised representation which is effectively embedded into the SCRN model. Furthermore, a new cost function of SCRN is formulated by jointly minimizing the basic and subclass-level reconstruction errors. To better exploit the discriminative information and improve classification performance, we further develop a deep subclass reconstruction network (DSCRN) model by stacking multiple SCRN models together through non-linear transformations to learn better deep feature representations. Extensive comparative evaluations on three benchmarks CWRU, 2009 PHM, and MFPT demonstrate that the proposed methods achieve consistently better diagnosis performance than existing state-of-the-art methods. Especially, when only 1\% training samples from CWRU are used, the proposed DSCRN has significant improvement with classification performance of 94.55\% over compared method with a gain of at least 9\%.},
  archive      = {J_ASOC},
  author       = {Hui Yu and Kai Wang and Yan Li and Mengfan He},
  doi          = {10.1016/j.asoc.2021.107755},
  journal      = {Applied Soft Computing},
  pages        = {107755},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep subclass reconstruction network for fault diagnosis of rotating machinery under various operating conditions},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). 2D multi-area coverage path planning using l-SHADE in
simulated ocean survey. <em>ASOC</em>, <em>112</em>, 107754. (<a
href="https://doi.org/10.1016/j.asoc.2021.107754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ocean environmental surveys typically involve multi-area coverage path planning tasks. The most important problem is improving the coverage efficiency of the task. A new path planning method based on Successful History-Based Adaptive Differential Evolution variants with Linear population size reduction(L-SHADE) is presented to solve this problem. The method comprises two parts: the part of sub area coverage path planning and the part of finding the optimized sequence of sub area start points. The key idea is establishing the relationship between the starting point of each sub area and the optimized multi-area path. We implement the method through numbering the possible starting point of sub area path and proposing a computing formula. In addition, the results of L-SHADE mutation process are optimized which make L-SHADE possible to apply in multi-area coverage path planning. This method avoids area discretization and exponential growth of computational quantities, and it is suitable for complex areas as well as multi-area. The simulation results with MATLAB showed the improvement of coverage path planning task execution efficiency. Compared with the method thinking of the sub area as the center of it, our method reduced the multi-area coverage path length by 4\%–7\%. From the simulations and analysis, we concluded that the method is able to improve the efficiency and stability of multi-area coverage path planning.},
  archive      = {J_ASOC},
  author       = {Guanzhong Chen and Yue Shen and Yixiao Zhang and Wenfeng Zhang and Dianrui Wang and Bo He},
  doi          = {10.1016/j.asoc.2021.107754},
  journal      = {Applied Soft Computing},
  pages        = {107754},
  shortjournal = {Appl. Soft. Comput.},
  title        = {2D multi-area coverage path planning using L-SHADE in simulated ocean survey},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal convolutional autoencoder for unsupervised anomaly
detection in time series. <em>ASOC</em>, <em>112</em>, 107751. (<a
href="https://doi.org/10.1016/j.asoc.2021.107751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning temporal patterns in time series remains a challenging task up until today. Particularly for anomaly detection in time series, it is essential to learn the underlying structure of a system’s normal behavior. Periodic or quasiperiodic signals with complex temporal patterns make the problem even more challenging: Anomalies may be a hard-to-detect deviation from the normal recurring pattern. In this paper, we present TCN-AE, a t emporal c onvolutional n etwork a uto e ncoder based on dilated convolutions. Contrary to many other anomaly detection algorithms, TCN-AE is trained in an unsupervised manner . The algorithm demonstrates its efficacy on a comprehensive real-world anomaly benchmark comprising electrocardiogram (ECG) recordings of patients with cardiac arrhythmia. TCN-AE significantly outperforms several other unsupervised state-of-the-art anomaly detection algorithms. Moreover, we investigate the contribution of the individual enhancements and show that each new ingredient improves the overall performance on the investigated benchmark.},
  archive      = {J_ASOC},
  author       = {Markus Thill and Wolfgang Konen and Hao Wang and Thomas Bäck},
  doi          = {10.1016/j.asoc.2021.107751},
  journal      = {Applied Soft Computing},
  pages        = {107751},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Temporal convolutional autoencoder for unsupervised anomaly detection in time series},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimal block knowledge driven backtracking search
algorithm for distributed assembly no-wait flow shop scheduling problem.
<em>ASOC</em>, <em>112</em>, 107750. (<a
href="https://doi.org/10.1016/j.asoc.2021.107750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed assembly flow shop scheduling problem (DAFSP) is an important scenario in manufacturing system . In this paper, an optimal block knowledge driven backtracking search algorithm (BKBSA) is proposed to solve the distributed assembly No-wait flow shop scheduling problem (DANWFSP) with the objective of minimizing the completion time of assembly process. In BKBSA, three constructive heuristics are proposed to generate a competitive initial solution. Block-shifting based on block knowledge is embedded in the mutation strategy of BKBSA. The proposed block-shifting ensures that the optimal subsequence of a candidate solution is not destroyed in the mutation operation. The similarity between candidate solutions is utilized as feedback indicator to control the utilization of block-shifting. In addition, the VND algorithm based on factory-to-factory is proposed to further improve the optimal solution. Finally, the BKBSA and the other three state-of-the-art algorithms for DANWFSP are tested on 810 large-scale instances and 900 small-scale instances. The statistical analysis results show that BKBSA is an effective algorithm to solve DANWFSP.},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Jinlong Zhao and Ling Wang and Jianxin Tang},
  doi          = {10.1016/j.asoc.2021.107750},
  journal      = {Applied Soft Computing},
  pages        = {107750},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimal block knowledge driven backtracking search algorithm for distributed assembly no-wait flow shop scheduling problem},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective coordination settings for directional overcurrent
relay using hybrid gradient-based optimizer. <em>ASOC</em>,
<em>112</em>, 107748. (<a
href="https://doi.org/10.1016/j.asoc.2021.107748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript announces a novel hybrid optimization model comprising the gradient-based optimizer (GBO) and Linear population size reduction technique of Success-History-based Adaptive Differential Evolution (LSHADE) algorithm, so-called GB-LSHADE, for optimal relay coordination problem (ORCP). The ORCP is framed as a nonlinear constrained engineering problem that aims to optimize the time-dial, pickup current, and relay characteristics so that the relay clearance times are minimized. The proposed GB-LSHADE operates in two phases meanwhile the first phase starts with GBO to enrich the exploitative tendencies while the second phase is adopted to boost the local searching competence and avoid the premature convergence. To verify the efficacy of the proposed optimization method, a comprehensive simulation has been conducted on three test-systems (8-bus, 15-bus, and IEEE 30-bus) with different complexities. Many scenarios including fixed and varied relay curves as per IEC 60255 are demonstrated. At a final stage of this work, the computational complexity of the proposed approach is carried out and analyzed. It can be confirmed that the adapted optimal settings produced by the GB-LSHADE achieve the full co-ordination of the protection without any violations for the systems under study. In addition, the numerical simulated results show that the GB-LSHADE supersedes the other viable optimizers reported in specialized literature thru further validations and comprehensive comparisons.},
  archive      = {J_ASOC},
  author       = {Rizk M. Rizk-Allah and Attia A. El-Fergany},
  doi          = {10.1016/j.asoc.2021.107748},
  journal      = {Applied Soft Computing},
  pages        = {107748},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effective coordination settings for directional overcurrent relay using hybrid gradient-based optimizer},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary algorithm for overlapping community detection
using a merged maximal cliques representation scheme. <em>ASOC</em>,
<em>112</em>, 107746. (<a
href="https://doi.org/10.1016/j.asoc.2021.107746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overlapping community detection in complex networks such as social, biological, economic, and other real-world networks has become an important area of research in the past two decades. Community detection problem can be modeled as a multiobjective optimization problem and it has been successfully solved using Evolutionary Algorithms (EA). The representation scheme of a solution in an EA affects the size of search space and thereby its convergence. A clique-based representation scheme is suitable to represent overlapping communities since overlapping cliques represent overlapping communities. The community to which a clique belongs becomes the community of its containing nodes. However, there are two issues viz., (1) cliques may heavily overlap since they might share many nodes with other cliques (2) one-node and two-node cliques increase the length of the representation of the EA. This paper proposes a merged-maximal-clique based representation scheme which reduces the chromosome length with fewer number of cliques. The proposed scheme overcomes the drawbacks of the existing clique-based scheme and naturally brings the initial solutions of the EA closer to the actual solutions. Moreover, operators of the EA are enhanced by renumbering the solutions to avoid duplicate solutions taking part in evolution. An EA based on decomposition equipped with the new representation scheme and enhanced operators is tested on both real-world and synthetic networks. The proposed EA finds better community partitions in fewer generations compared to the existing clique-based and non-clique-based algorithms. This study proves that clique-based schemes are efficient for overlapping community detection.},
  archive      = {J_ASOC},
  author       = {A.C. Ramesh and G. Srivatsun},
  doi          = {10.1016/j.asoc.2021.107746},
  journal      = {Applied Soft Computing},
  pages        = {107746},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary algorithm for overlapping community detection using a merged maximal cliques representation scheme},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using differential evolution and moth–flame optimization for
scientific workflow scheduling in fog computing. <em>ASOC</em>,
<em>112</em>, 107744. (<a
href="https://doi.org/10.1016/j.asoc.2021.107744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is an interesting technology aimed at providing various processing and storage resources at the IoT networks’ edge. Energy consumption is one of the essential factors that can directly impact the maintenance cost and CO 2 emissions of fog environments. Energy consumption can be mitigated by effective scheduling approaches, in which tasks are going to be mapped on the best possible resources regarding some conflicting objectives. To deal with these issues, we introduce an opposition-based hybrid discrete optimization algorithm , called DMFO-DE. For this purpose, first, a discrete and Opposition-Based Learning (OBL) version of the Moth–Flame Optimization (MFO) algorithm is provided, and it then is combined with the Differential Evolution (DE) algorithm to improve the convergence speed and prevent local optima problem. The DMFO-DE is then employed for scientific workflow scheduling in fog computing environments using the Dynamic Voltage and Frequency Scaling (DVFS) method. The Heterogeneous Earliest Finish Time (HEFT) algorithm is used to find the tasks execution order in the scientific workflows. Our workflow scheduling approach mainly tries to decrease the scheduling process’s energy consumption by minimizing the applied Virtual Machines (VMs), makespan, and communication between dependent tasks. For evaluating the performance of the proposed scheduling scheme, extensive simulations are conducted on the scientific workflows with four different sizes. The experimental results indicate that scheduling using the DMFO-DE algorithm can outperform other metrics such as the number of applied VMs, and energy consumption.},
  archive      = {J_ASOC},
  author       = {Omed Hassan Ahmed and Joan Lu and Qiang Xu and Aram Mahmood Ahmed and Amir Masoud Rahmani and Mehdi Hosseinzadeh},
  doi          = {10.1016/j.asoc.2021.107744},
  journal      = {Applied Soft Computing},
  pages        = {107744},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using differential evolution and Moth–Flame optimization for scientific workflow scheduling in fog computing},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting users’ preferences by fuzzy rough set
quarter-sphere support vector machine. <em>ASOC</em>, <em>112</em>,
107740. (<a href="https://doi.org/10.1016/j.asoc.2021.107740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems aim to support users in decision-making through the knowledge extracted from historical ratings. However, many of these ratings may be noisy and/or missing, causing degradation in the quality of the recommendations. Considering these issues, this paper presents a new one-class classifier to predict ratings in recommendation systems. The proposed method estimates the shared informative neighbors of each user by a probability fuzzy rough set method. Since the fuzzy rough set theory is sensitive to noisy samples, the quarter-sphere SVM classifier is designed to reduce the impact of noise on the results. The proposed classifier can satisfactorily determine a boundary around the target class while it reduces the acceptance probability of the outliers and non-target class(es). The theoretical interpretations are provided to prove the statistical stability of the proposed method. Also, noise analysis has been carried out. Through extensive experiments on several real-world data sets, it is confirmed that the proposed method outperforms the other six methods in terms of accuracy, recall, precision, and computational time.},
  archive      = {J_ASOC},
  author       = {Javad Hamidzadeh and Ebrahim Rezaeenik and Mona Moradi},
  doi          = {10.1016/j.asoc.2021.107740},
  journal      = {Applied Soft Computing},
  pages        = {107740},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting users’ preferences by fuzzy rough set quarter-sphere support vector machine},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting load capacity of shear walls using SVR–RSM model.
<em>ASOC</em>, <em>112</em>, 107739. (<a
href="https://doi.org/10.1016/j.asoc.2021.107739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of the shear capacity of reinforced concrete shear walls (RCSW) is essential for the wind and seismic design of buildings. However, due to the diverse structural configurations, multitude of load scenarios, and highly nonlinear relations between the design parameters and the shear load capacity, this prediction is very complex. Existing pertinent design code provisions such as the American Concrete Institute ACI-318 and the Eurocode rely on empirical expressions that have various limitations and attain low predictive accuracy . Hence, in this paper, we pioneer a novel hybrid intelligent model to predict the ultimate shear capacity of RCSW. The support vector regression (SVR) and response surface model (RSM) were coupled based on two calibrating strategies in a novel hybrid modeling approach called RSM–SVR. The accuracy, tendency and uncertainty of the proposed SVR–RSM model along with that of three existing empirical relations and two design code provisions were assessed using various statistical metrics based on a comprehensive experimental database retrieved from the open literature. The existing design codes and empirical models were found to be inflicted with high variability and did not capture the influence of the key design parameters on the shear capacity in a robust and rational manner. Conversely, it is shown that the proposed RSM–SVR modeling approach achieved superior accurate predictions for the shear strength of RCSW. The proposed RSM–SVRmodel enhanced RMSE for the training (testing) dataset by 510\% (150\%) compared to the Baghi et al. model, 550\% (190\%) compared to the ACI 318-14 design code, 530\% (155\%) compared to the Chandra et al. model, 320\% (145\%) compared to the RSM model, and 450\% (90\%) compared to the SVR model. The novel approach also better captured the influence of the key design parameters, demonstrating robust tendency and much lower uncertainty. Thus, the proposed novel model could be harvested in intelligent generative design and for the enhancement of pertinent provisions in design codes. The proposed method achieves outstanding performance, while maintaining superior computational efficiency and low run time.},
  archive      = {J_ASOC},
  author       = {Behrooz Keshtegar and Moncef L. Nehdi and Nguyen-Thoi Trung and Reza Kolahchi},
  doi          = {10.1016/j.asoc.2021.107739},
  journal      = {Applied Soft Computing},
  pages        = {107739},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting load capacity of shear walls using SVR–RSM model},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Many-objective cloud manufacturing service selection and
scheduling with an evolutionary algorithm based on adaptive environment
selection strategy. <em>ASOC</em>, <em>112</em>, 107737. (<a
href="https://doi.org/10.1016/j.asoc.2021.107737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud manufacturing service selection and scheduling (CMSSS) problem has obtained wide attentions in recent years. However, most existing methods describe this problem as single-, bi-, or tri-objective models. Little work deals with this problem in four or more objectives simultaneously. This paper investigated CMSSS problem in consideration of the interests of users, cloud platform and service providers. An eight-objective CMSSS optimization model is constructed for the problem. Meanwhile, a many-objective evolutionary algorithm with adaptive environment selection (MaOEA-AES) is designed to address the problem. Specifically, diversity-based population partition technology is used to divide the population into multiple subregions to maintain the population diversity, and an adaptive penalty boundary intersection (APBI) distance is designed to select elitist solutions in different stages of evolutionary process. The proposed algorithm is tested on 2 cases with 5 and 8 objectives in CMSSS problems and each of them has sixteen experimental groups with different problem scales. The experiment results show that MaOEA-AES is competitive to resolve the MaO-CMSSS model compared with eight state-of-the-art evolutionary algorithms in convergence and diversity.},
  archive      = {J_ASOC},
  author       = {Tianri Wang and Pengzhi Zhang and Juan Liu and Minmin Zhang},
  doi          = {10.1016/j.asoc.2021.107737},
  journal      = {Applied Soft Computing},
  pages        = {107737},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Many-objective cloud manufacturing service selection and scheduling with an evolutionary algorithm based on adaptive environment selection strategy},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HD-NSA: A real-valued negative selection algorithm based on
hierarchy division. <em>ASOC</em>, <em>112</em>, 107726. (<a
href="https://doi.org/10.1016/j.asoc.2021.107726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The negative selection algorithm (NSA) is an important algorithm for generating immune detectors in artificial immune systems . However, the original NSA randomly generates candidate detectors that produce a large number of redundant detectors, and it is difficult to cover the entire antibody space. Moreover, the randomly generated candidate detectors have to be compared with all the self-sets; therefore, the inefficient generation of the detector seriously influences the application of NSA. To overcome these defects, a real-valued NSA based on hierarchy division (HD-NSA) is proposed. First, the feature space is divided into self and non-self subgrids, and the center point of the non-self subgrid is specified as the candidate detector, and the specified candidate detector is compared with the self-antigens located in adjacent subgrids rather than with all the self-sets. Theoretical analysis demonstrated that the HD-NSA can effectively reduce the time complexity of the NSA algorithm. Furthermore, experiments on the Abalone data set show that the detector training time of HD-NSA decreased by 97.9\%, 71.2\%, 56.9\% and 90.1\%, respectively, compared with the classical RNSA, V-Detector, GF-RNSA and BIORV-NSA, whereas the detector detection rate increased by 50\%, 25.8\%, 13.8\% and 10.5\%, respectively.},
  archive      = {J_ASOC},
  author       = {Junjiang He and Wen Chen and Tao Li and Beibei Li and Yongbin Zhu and Meng Huang},
  doi          = {10.1016/j.asoc.2021.107726},
  journal      = {Applied Soft Computing},
  pages        = {107726},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HD-NSA: A real-valued negative selection algorithm based on hierarchy division},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A stochastic bi-objective simulation–optimization model for
plasma supply chain in case of COVID-19 outbreak. <em>ASOC</em>,
<em>112</em>, 107725. (<a
href="https://doi.org/10.1016/j.asoc.2021.107725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As of March 24, 2020, the Food and Drug Administration (FDA) authorized to bleed the newly recovered from Coronavirus Disease 2019 (COVID-19), i.e., the ones whose lives were at risk, separate Plasma from their blood and inject it to COVID-19 patients. In many cases, as observed the plasma antibodies have cured the disease. Therefore, a four-echelon supply chain has been designed in this study to locate the blood collection centers, to find out how the collection centers are allocated to the temporary or permanent plasma-processing facilities, how the temporary facilities are allocated to the permanent ones, along with determining the allocation of the temporary and permanent facilities to hospitals. A simulation approach has been employed to investigate the structure of COVID-19 outbreak and to simulate the quantity of plasma demand. The proposed bi-objective model has been solved in small and medium scales using ɛ ɛ ɛ -constraint method, Strength Pareto Evolutionary Algorithm II (SPEA-II), Non-dominated Sorting Genetic Algorithm II (NSGA-II), Multi-Objective Grey Wolf Optimizer (MOGWO) and Multi Objective Invasive Weed Optimization algorithm (MOIWO) approaches. One of the novelties of this research is to study the system dynamic structure of COVID-19’s prevalence so that to estimate the required plasma level by simulation. Besides, this paper has focused on blood substitutability which is becoming increasingly important for timely access to blood. Due to shorter computational time and higher solution quality, MOIWO is selected to solve the proposed model for a large-scale case study in Iran. The achieved results indicated that as the plasma demand increases, the amount of total system costs and flow time rise, too. The proposed simulation model has also been able to calculate the required plasma demand with 95\% confidence interval.},
  archive      = {J_ASOC},
  author       = {Hossein Shirazi and Reza Kia and Peiman Ghasemi},
  doi          = {10.1016/j.asoc.2021.107725},
  journal      = {Applied Soft Computing},
  pages        = {107725},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A stochastic bi-objective simulation–optimization model for plasma supply chain in case of COVID-19 outbreak},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level retrieval with semantic axiomatic fuzzy set
clustering for question answering. <em>ASOC</em>, <em>111</em>, 107858.
(<a href="https://doi.org/10.1016/j.asoc.2021.107858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers open-domain and multi-hop reading comprehension tasks that require complex multi-step reasoning processes. The study is particularly challenging because it requires a model to learn to explore “bridge” information to connect text snippets relevant to the answer. Unlike the usual neural-network-based retrieval models , which are difficult to interpret, this paper proposes a coarse-to-fine unsupervised evidence sentences retrieval model based on the Axiomatic Fuzzy Sets clustering with both reasoning ability and interpretability . According to the entities that appeared in a question, a chained inference retrieval was carried out to get the coarser candidate documents from knowledge bases. Then, sentence-level multi-feature scoring rules based on the part of speech and grammar are proposed. The Axiomatic Fuzzy Sets clustering algorithm based on the feature scores selects finer and sentence-level evidence by semantic descriptions . The retrieval process of the candidate sentences is unsupervised and straightforward, which does not require word embedding . Our model achieves state-of-the-art results in three open-domain QA datasets: HotpotQA, SQuAD Open and Natural Questions Open.},
  archive      = {J_ASOC},
  author       = {Qi Lang and Xiaodong Liu and Yingjie Deng},
  doi          = {10.1016/j.asoc.2021.107858},
  journal      = {Applied Soft Computing},
  pages        = {107858},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-level retrieval with semantic axiomatic fuzzy set clustering for question answering},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Similarity-evaluation-based evolving of flexible neural
trees for imbalanced classification. <em>ASOC</em>, <em>111</em>,
107852. (<a href="https://doi.org/10.1016/j.asoc.2021.107852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A flexible neural tree (FNT) is a special and effective kind of artificial neural network that can search optimal network structures using tree structure evolving algorithms, leading to its high performance in many real classification and prediction problems. However, the standard FNT model does not take into account tree structure similarities in its evolving process. This may result to a rapid and significant decrease of its tree structure population. Standard FNT also suffers from imbalanced data . In this study, we propose a new similarity evaluation method for FNT (SEFNT) to keep the population diversity and deal with the imbalanced data . The main idea of SEFNT is twofold. First, the difference between two nodes is introduced for similarity measurement. Second, the node position and height of trees are also taken into account for accurate tree structure distance measurement. SEFNT uses imbalanced fitness function to control its evolving procedure to deal with imbalanced problems. We compare SEFNT with 10 imbalanced methods on 46 KEEL datasets and 10 UCI datasets. The experimental results show that our approach can significantly improve the classification performance of FNT. In the comparisons with other algorithms, SEFNT shows significantly better performance. We also apply the proposed method to a practical imbalanced classification problem, that is, Internet video traffic identification. The results imply that our method is effective in dealing with practical problems.},
  archive      = {J_ASOC},
  author       = {Min Qiu and Lizhi Peng and Ying Pang and Bo Yang and Panpan Li},
  doi          = {10.1016/j.asoc.2021.107852},
  journal      = {Applied Soft Computing},
  pages        = {107852},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Similarity-evaluation-based evolving of flexible neural trees for imbalanced classification},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive information bottleneck for high-dimensional
co-occurrence data clustering. <em>ASOC</em>, <em>111</em>, 107837. (<a
href="https://doi.org/10.1016/j.asoc.2021.107837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering high-dimensional data is quite challenging due to lots of redundant and irrelevant information contained in features. Most existing methods sequentially or jointly perform the feature dimensionality reduction and data clustering on the low-dimensional representations. However, the relationships between the clustered data points and the dimension-reduced features, as well as the influence of the relationships on the low-dimensional feature subspace learning are neglected in these methods. In this paper, an embarrassingly simple yet effective interactive information bottleneck (IIB) method is proposed for high-dimensional co-occurrence data clustering by simultaneously performing data clustering and low-dimensional feature subspace learning. What is different from existing methods is that, we perform data clustering while maximally preserving the correlations between the data clusters and the learned dimension-reduced features, and simultaneously learn the low-dimensional feature subspace while maintaining the correlations with the data clustering results obtained in the previous iteration. Thus, the two stages are interactive and refined mutually. Finally, a new twin “draw-and-merge” method is designed for optimization. Experimental results on four high-dimensional datasets demonstrate the superiority and effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Shizhe Hu and Ruobin Wang and Yangdong Ye},
  doi          = {10.1016/j.asoc.2021.107837},
  journal      = {Applied Soft Computing},
  pages        = {107837},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interactive information bottleneck for high-dimensional co-occurrence data clustering},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SANE: A sequence combined attentive network embedding model
for COVID-19 drug repositioning. <em>ASOC</em>, <em>111</em>, 107831.
(<a href="https://doi.org/10.1016/j.asoc.2021.107831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 has now spread all over the world and causes a huge burden for public health and world economy. Drug repositioning has become a promising treatment strategy in COVID-19 crisis because it can shorten drug development process, reduce pharmaceutical costs and reposition approval drugs. Existing computational methods only focus on single information, such as drug and virus similarity or drug–virus network feature, which is not sufficient to predict potential drugs. In this paper, a sequence combined attentive network embedding model SANE is proposed for identifying drugs based on sequence features and network features. On the one hand, drug SMILES and virus sequence features are extracted by encoder–decoder in SANE as node initial embedding in drug–virus network. On the other hand, SANE obtains fields for each node by attention-based Depth-First-Search (DFS) to reduce noises and improve efficiency in representation learning and adopts a bottom-up aggregation strategy to learn node network representation from selected fields. Finally, a forward neural network is used for classifying. Experiment results show that SANE has achieved the performance with 81.98\% accuracy and 0.8961 AUC value and outperformed state-of-the-art baselines. Further case study on COVID-19 indicates that SANE has a strong predictive ability since 25 of the top 40 (62.5\%) drugs are verified by valuable dataset and literatures. Therefore, SANE is powerful to reposition drugs for COVID-19 and provides a new perspective for drug repositioning.},
  archive      = {J_ASOC},
  author       = {Xiaorui Su and Zhuhong You and Lei Wang and Lun Hu and Leon Wong and Boya Ji and Bowei Zhao},
  doi          = {10.1016/j.asoc.2021.107831},
  journal      = {Applied Soft Computing},
  pages        = {107831},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SANE: A sequence combined attentive network embedding model for COVID-19 drug repositioning},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble-learning regression to estimate sleep apnea
severity using at-home oximetry in adults. <em>ASOC</em>, <em>111</em>,
107827. (<a href="https://doi.org/10.1016/j.asoc.2021.107827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overnight pulse oximetry has shown usefulness to simplify obstructive sleep apnea (OSA) diagnosis when combined with machine-learning approaches. However, the development and evaluation of a single model with ability to reach high diagnostic performance in both community-based non-referral and clinical referral cohorts are still pending. Since ensemble-learning algorithms are known for their generalization ability , we propose a least-squares boosting (LSBoost) model aimed at estimating the apnea–hypopneaindex (AHI), as the correlate clinical measure of disease severity. A thorough characterization of 8, 762 nocturnal blood-oxygen saturation signals (SpO 2 ) obtained at home was conducted to extract the oximetric information subsequently used in the training, validation, and test stages. The estimated AHI derived from our model achieved high diagnostic ability in both referral and non-referral cohorts reaching intra-class correlation coefficients within 0.889–0.924, and Cohen’s κ κ within 0.478–0.663 when considering the four OSA severity categories. These resulted in accuracies ranging 87.2\%–96.6\%, 81.1\%–87.6\%, and 91.6\%–94.6\% when assessing the three typical AHI severity thresholds, 5 events/hour (e/h), 15 e/h, and 30 e/h, respectively. Our model also revealed the importance of the SpO 2 predictors, thereby minimizing the ‘black box’ perception traditionally attributed to the machine-learning approaches. Furthermore, a decision curve analysis emphasized the clinical usefulness of our proposal. Therefore, we conclude that the LSBoost-based model can foster development of clinically applicable and cost saving protocols for detection of patients attending primary care services, or to avoid full polysomnography in specialized sleep facilities, thus demonstrating the diagnostic usefulness of SpO 2 signals obtained at home.},
  archive      = {J_ASOC},
  author       = {Gonzalo C. Gutiérrez-Tobal and Daniel Álvarez and Fernando Vaquerizo-Villar and Andrea Crespo and Leila Kheirandish-Gozal and David Gozal and Félix del Campo and Roberto Hornero},
  doi          = {10.1016/j.asoc.2021.107827},
  journal      = {Applied Soft Computing},
  pages        = {107827},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble-learning regression to estimate sleep apnea severity using at-home oximetry in adults},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring syntactic and semantic features for authorship
attribution. <em>ASOC</em>, <em>111</em>, 107815. (<a
href="https://doi.org/10.1016/j.asoc.2021.107815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authorship attribution is to extract features to identify authors of anonymous documents. Many previous works on authorship attribution focus on statistical style features (e.g., sentence/word length), content features (e.g., frequent words, n-grams). Modeling these features by regression or some transparent machine learning methods gives a portrait of the authors’ writing style. But these methods do not capture the syntactic (e.g., dependency relationship) or semantic (e.g., topics) information. In recent years, some researchers model syntactic trees or latent semantic information by neural networks . However, few works take them together. In this paper, we propose a novel M ulti- C hannel S elf- A ttention N etwork (MCSAN) incorporating both the inter-channel and inter-positional interaction to extract n-grams of the characters, words, parts of speech (POS), phrase structures, dependency relationships, and topics from multiple dimensions (style, content, syntactic and semantic features) to distinguish different authors. And then we incorporate these extracted features with logistic regression (LR) to do experiments, and the experimental results manifest that our extracted features are effective. Our methods improve 2.1\% and 3.0\% on CCAT10 and CCAT50, respectively, comparing with state-of-the-art models.},
  archive      = {J_ASOC},
  author       = {Haiyan Wu and Zhiqiang Zhang and Qingfeng Wu},
  doi          = {10.1016/j.asoc.2021.107815},
  journal      = {Applied Soft Computing},
  pages        = {107815},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exploring syntactic and semantic features for authorship attribution},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Jamming detection at the edge of drone networks using
multi-layer perceptrons and decision trees. <em>ASOC</em>, <em>111</em>,
107806. (<a href="https://doi.org/10.1016/j.asoc.2021.107806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As wireless networks play an increasingly key role in everyday life, it is necessary to secure them from radio frequency attacks , such as jamming, which are hard to detect, especially because they may be easily mistaken for other network conditions. Within this challenging context, the paper proposes a framework for jamming detection in drone networks, relying on a distributed approach based on supervised machine learning techniques , namely, Multi-layer Perceptrons and Decision Trees . Given a reference data packet trace set, our framework computes the features of some predefined metrics, such as throughput, PDR and RSSI , which vary during a jamming attack, and that can therefore be used to detect it. We evaluate our framework using datasets from publicly available standardized jamming attack scenarios with IEEE 802.11p radio data, and via ns3-based simulation datasets from networks of drones using WiFi. We show that the performance of the classifiers improves as the sampling time of the packets decreases. We also show that the Multi-layer Perceptron can be effectively generalized to achieve jamming detection accuracy superior to that of Decision Trees even when applied to communication scenarios for which it has not been specifically trained. Our proposed framework reaches a satisfactory accuracy level of 96\%, while requiring low computational and hardware capabilities, thus proving to be suitable for resource-constrained drone networks.},
  archive      = {J_ASOC},
  author       = {Claudia Greco and Pasquale Pace and Stefano Basagni and Giancarlo Fortino},
  doi          = {10.1016/j.asoc.2021.107806},
  journal      = {Applied Soft Computing},
  pages        = {107806},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Jamming detection at the edge of drone networks using multi-layer perceptrons and decision trees},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A 3D template-based point generation network for 3D
reconstruction from single images. <em>ASOC</em>, <em>111</em>, 107749.
(<a href="https://doi.org/10.1016/j.asoc.2021.107749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based approaches in 3D reconstruction problem have attracted researchers, due to the excellent performance of this approach in image segmentation and image classification . The increasing attention to the learning- based approach for the 3D reconstruction application is also due to the availability of 3D datasets shared publicly, such as ShapeNet and ModelNet datasets. Several deep learning approaches use voxel representation-based approaches. However, voxel-based methods suffer from inefficiency and inability to create higher dimensional 3D results. Another representation is by using point cloud representation, an unstructured 3D points in the object’s surface. However, learning such irregular structures is a challenging task due to the unordered properties of such representations. This paper proposes a new framework for 3D reconstruction of 2D images that introduces a 3D template-based point generation network. The 3D template-based point generation network infers a 3D template and generates 3D point clouds representing the reconstructed 3D object, based on an input image. The proposed network introduces two inputs, the encoded 2D image and the encoded 3D point template produced by an image classification module and a 3D template generation module. Experiments on the ShapeNet dataset show better performance than existing methods in terms of the Chamfer distance between the 3D ground-truth data and the 3D reconstructed data.},
  archive      = {J_ASOC},
  author       = {Anny Yuniarti and Agus Zainal Arifin and Nanik Suciati},
  doi          = {10.1016/j.asoc.2021.107749},
  journal      = {Applied Soft Computing},
  pages        = {107749},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A 3D template-based point generation network for 3D reconstruction from single images},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy consumption prediction of appliances using machine
learning and multi-objective binary grey wolf optimization for feature
selection. <em>ASOC</em>, <em>111</em>, 107745. (<a
href="https://doi.org/10.1016/j.asoc.2021.107745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of the energy consumed by household appliances is a challenging research topic owing to a transition toward the Internet of Everything. Although classical machine learning algorithms have evolved significantly in recent years owing to the advancements in big data and the Internet of Things , several challenges remain to be addressed. Thus, obtaining highly accurate predictions using as few computational resources as possible is the primary research goal of many studies. In this study, the energy consumptions of appliances were predicted using a method based on multi-objective binary grey wolf optimization , wherein the random forest , extra trees, decision tree , and K-nearest neighbor regression algorithms were employed. The two objectives of the present study were the maximization of the prediction performance of the algorithms and minimization of the number of selected features. The results were ranked using multi-objective optimization on the basis of ratio analysis. The proposed method was tested on the appliances energy prediction dataset publicly available at the UCI Machine Learning Repository, and its results were compared with those obtained using other similar methods.},
  archive      = {J_ASOC},
  author       = {Dorin Moldovan and Adam Slowik},
  doi          = {10.1016/j.asoc.2021.107745},
  journal      = {Applied Soft Computing},
  pages        = {107745},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy consumption prediction of appliances using machine learning and multi-objective binary grey wolf optimization for feature selection},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grey forecasting models based on internal optimization for
novel corona virus (COVID-19). <em>ASOC</em>, <em>111</em>, 107735. (<a
href="https://doi.org/10.1016/j.asoc.2021.107735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pandemic forecasting has become an uphill task for the researchers on account of the paucity of sufficient data in the present times. The world is fighting with the Novel Coronavirus to save human life. In a bid to extend help to the concerned authorities, forecasting engines are invaluable assets. Considering this fact, the presented work is a proposal of two Internally Optimized Grey Prediction Models (IOGMs). These models are based on the modification of the conventional Grey Forecasting model (GM(1, 1)). The IOGMs are formed by stacking infected case data with diverse overlap periods for forecasting pandemic spread at different locations in India. First, IOGM is tested using time series data . Its two models are then employed for forecasting the pandemic spread in three large Indian states namely, Rajasthan , Gujarat, Maharashtra and union territory Delhi. Several test runs are carried out to evaluate the performance of proposed grey models and conventional grey models GM(1, 1) and NGM(1, 1, k). It is observed that the prediction accuracies of the proposed models are satisfactory and the forecasted results align with the mean infected cases. Investigations based on the evaluation of error indices indicate that the model with a higher overlap period provides better results.},
  archive      = {J_ASOC},
  author       = {Akash Saxena},
  doi          = {10.1016/j.asoc.2021.107735},
  journal      = {Applied Soft Computing},
  pages        = {107735},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grey forecasting models based on internal optimization for novel corona virus (COVID-19)},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evolutionary fuzzy system to support the replacement
policy in water supply networks: The ranking of pipes according to their
failure risk. <em>ASOC</em>, <em>111</em>, 107731. (<a
href="https://doi.org/10.1016/j.asoc.2021.107731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an evolutionary fuzzy system is proposed to predict unexpected pipe failures in water supply networks. The system seeks to underpin the decisions of management companies regarding the maintenance and replacement plans of pipes. On the one hand, fuzzy logic provides high degrees of interpretability over other black box models , which is requested in engineering application where decisions have social consequences. On the other hand, the genetic algorithm helps to optimize the parameters that govern the model, specifically, for two purposes: (i) the selection of variables; and (ii) the optimization of membership functions. Data from a real water supply network are used to evaluate the accuracy of the developed system. Several graphs that depict the ranking of pipes according to their risk of failure against the network length to be replaced support the choice of the most successful model. In fact, results demonstrate that the annual replacement of 6.75\% of the network length makes it possible to prevent 41.14\% of unexpected pipe failures.},
  archive      = {J_ASOC},
  author       = {Alicia Robles-Velasco and Jesús Muñuzuri and Luis Onieva and Pablo Cortés},
  doi          = {10.1016/j.asoc.2021.107731},
  journal      = {Applied Soft Computing},
  pages        = {107731},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary fuzzy system to support the replacement policy in water supply networks: The ranking of pipes according to their failure risk},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data driven day-ahead electrical load forecasting through
repeated wavelet transform assisted SVM model. <em>ASOC</em>,
<em>111</em>, 107730. (<a
href="https://doi.org/10.1016/j.asoc.2021.107730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrical load forecasting is an integral tool used by the grid operator to operate the smart power network. The information related to the electrical load is a prerequisite towards the effective and optimal operation of the power network with renewable and conventional generation resources. Economical bidding in the energy markets is directing research towards a superior forecasting model. Statistical and machine learning models have been used for electrical load forecasting while considering the electrical load as a time-series signal. This paper proposes a hybrid model that combines the Wavelet Transform (WT) and Support Vector Machine (SVM) features in estimating a regression model for electrical load forecasting utilizing the historical time-series information of electrical load. The WT decomposes the electrical load time-series data into various sub-series. The error contribution in forecasting due to the individual sub-series is estimated using Mean Absolute Error (MAE) in forecasting for each sub-series. The proposed Repeated WT-based SVM model (RWT-SVM) selects the sub-series with the highest MAE for further decomposition through WT. This results in a better forecasting model for the sub-series with the highest MAE, thereby improving the overall forecasting ability of the RWT-SVM model. The superiority of the proposed Repeated WT-based SVM model (RWT-SVM) for electrical load forecasting is justified using various data sets and comparing with some of the existing forecasting models.},
  archive      = {J_ASOC},
  author       = {Aasim and S.N. Singh and Abheejeet Mohapatra},
  doi          = {10.1016/j.asoc.2021.107730},
  journal      = {Applied Soft Computing},
  pages        = {107730},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data driven day-ahead electrical load forecasting through repeated wavelet transform assisted SVM model},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient feature selection framework based on
information theory for high dimensional data. <em>ASOC</em>,
<em>111</em>, 107729. (<a
href="https://doi.org/10.1016/j.asoc.2021.107729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays a vital role in many fields, particularly in pattern recognition and bioinformatics, for selecting informative and relevant features from high dimensional datasets. The increase in dimensionality of data along with the existence of redundant and irrelevant features leads to challenging performance issues when processing and analysing the data. In this paper, an effective feature selection technique called mutual information and Monte Carlo based feature selection (MIMCFS) is proposed. It comprises of two stages. The first stage aims to select predominant features from the high dimensional data . The second stage involves elimination of redundant features that were selected in the first stage. For the purpose of implementing the first stage, a new feature selection strategy based on the approximate Markov blanket and the concept of mutual information is proposed to find out irrelevant and redundant features. In second stage, to avoid misjudgement of redundant features as relevant features, a new strategy based on Monte Carlo tree search technique is proposed in order to completely eradicate redundant features and to improve feature interaction. For experimental evaluation, eight benchmark microarray datasets including imbalanced ones pertaining to cancer analysis are used. Further, in order to compare and justify the performance of the proposed feature selection method, seven state-of-art feature selection techniques namely CFS, Relief, DISR, JMI , CMIM and CMI are employed. The outputs from these feature selection techniques are provided to three standard classifiers namely Naive Bayes, SVM and C4.5 in order to assess the significance of the selected features in building classification models . 10-fold cross validation is adopted to evaluate the classifiers. Accuracy, precision, recall, f-measure, standard deviation, statistical significance metrics are measured to quantify the classifier performance. Experimental results demonstrate the outstanding performance of the proposed algorithm when compared to that of the standard existing methods.},
  archive      = {J_ASOC},
  author       = {G. Manikandan and S. Abirami},
  doi          = {10.1016/j.asoc.2021.107729},
  journal      = {Applied Soft Computing},
  pages        = {107729},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient feature selection framework based on information theory for high dimensional data},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DanHAR: Dual attention network for multimodal human activity
recognition using wearable sensors. <em>ASOC</em>, <em>111</em>, 107728.
(<a href="https://doi.org/10.1016/j.asoc.2021.107728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we present a new dual attention method called DanHAR, which blends channel and temporal attention on residual networks to improve feature representation ability for sensor-based HAR task. Specially, the channel attention plays a key role in deciding what to focus, i.e., sensor modalities, while the temporal attention can focus on the target activity from a long sensor sequence to tell where to focus. Extensive experiments are conducted on four public HAR datasets, as well as weakly labeled HAR dataset. The results show that dual attention mechanism is of central importance for many activity recognition tasks. We obtain 2.02\%, 4.20\%, 1.95\%, 5.22\% and 5.00\% relative improvement over regular ConvNets respectively on WISDM dataset, UNIMIB SHAR dataset, PAMAP2 dataset, OPPORTUNITY dataset, as well as weakly labeled HAR dataset. The DanHAR is able to surpass other state-of-the-art algorithms at negligible computational overhead. Visualizing analysis is conducted to show that the proposed attention can capture the spatial–temporal dependencies of multimodal sensing data, which amplifies the more important sensor modalities and timesteps during classification. The results are in good agreement with normal human intuition.},
  archive      = {J_ASOC},
  author       = {Wenbin Gao and Lei Zhang and Qi Teng and Jun He and Hao Wu},
  doi          = {10.1016/j.asoc.2021.107728},
  journal      = {Applied Soft Computing},
  pages        = {107728},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DanHAR: Dual attention network for multimodal human activity recognition using wearable sensors},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Genetic algorithms and satin bowerbird optimization for
optimal allocation of distributed generators in radial system.
<em>ASOC</em>, <em>111</em>, 107727. (<a
href="https://doi.org/10.1016/j.asoc.2021.107727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this document, the topic of discussion is the combination of two existing algorithms to generate a new hybrid technique. The two algorithms that are subjected to said amalgamation are Genetic Algorithms (GA) and Stain Bowerbird Optimization algorithms (SBO). These two methodologies have profound utility themselves and are used in a multitude of scenarios. The easy application and the constructive outcomes manifested by these two algorithms birthed the idea of their combined usage. Following up on this, the hybrid GASBO was created. GASBO was an optimization approach used to detect and categorize the allotted renewable energy assets in a specific energy generation complex. This was done to regulate the energy dispensing systems otherwise known as ‘distributing’ systems. These renewable resources are reflected by environmental factors and the energy they create is also dependent on their surroundings. Factors like sunlight, rain, waves, and tides etcetera play major roles in determining the outcome of the created energy. Contrary to what it may appear like, the position of the DG sources in the structure affects the outcome a lot. These sources contain fuel cells and photovoltaic cells: in short, devices that can harness energy from a seemingly infinite supply like sunlight. As mentioned before, the GASBO assisted in providing the best location for the system and it also categorized the sources according to their abilities. The potential and position of the sources in the grid are of vast importance. The main purpose of GASBO is to optimize the overall system by improving its efficiency and reducing collateral harm. This shows that GASBO is quite a fundamental tool. It has also been tested on several systems like IEEE 33-bus. The facts in this paper are based on published projects.},
  archive      = {J_ASOC},
  author       = {Ashraf Mohamed Hemeida and Omaima M. Bakry and Al-Attar A. Mohamed and Eman A. Mahmoud},
  doi          = {10.1016/j.asoc.2021.107727},
  journal      = {Applied Soft Computing},
  pages        = {107727},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Genetic algorithms and satin bowerbird optimization for optimal allocation of distributed generators in radial system},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Random fourier feature-based fuzzy clustering with
p-laplacian regularization. <em>ASOC</em>, <em>111</em>, 107724. (<a
href="https://doi.org/10.1016/j.asoc.2021.107724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random feature is one successful technique to approximate traditional kernel functions , and the random feature-based fuzzy clustering has been proved to be effective and efficient for handling non-linear data. However, the existing random feature-based fuzzy clustering methods fail to consider the locality information hidden in original input data. From some perspective, the membership obtained by fuzzy clustering can be seen as the encoding results of data. Thus, constraining the relationships between membership degrees to be consistent with that of data is beneficial to improve clustering performance. To this end, we propose a novel random Fourier feature-based fuzzy clustering method (pLRFCM) in this paper. The random Fourier feature is used to approximate Gaussian kernels in this method, and the fuzzy clustering is performed in the feature space. More importantly, the p p -Laplacian regularization is conducted on the membership matrix to preserve the local structures of original data into the clustering results , to guarantee good partition of data. The maximum-entropy technique is also utilized to fine-tune the weights of features automatically during the process of clustering, so as to further promote the performance of clustering. In the experiments on four synthetic non-linear datasets and eight real-world datasets, pLRFCM outperforms several classical and state-of-the-art fuzzy clustering methods .},
  archive      = {J_ASOC},
  author       = {Yingxu Wang and Tianjun Li and Long Chen and Guangmei Xu and Jin Zhou and C. L. Philip Chen},
  doi          = {10.1016/j.asoc.2021.107724},
  journal      = {Applied Soft Computing},
  pages        = {107724},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Random fourier feature-based fuzzy clustering with p-laplacian regularization},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smooth non-negative sparse representation for face and
handwritten recognition. <em>ASOC</em>, <em>111</em>, 107723. (<a
href="https://doi.org/10.1016/j.asoc.2021.107723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sparse representation problem, there is always interest to reduce the solution space by introducing additional constraints. This can lead to efficient application-specific algorithms. Despite known advantages of sparsity and non-negativity for image data representation, limited studies have addressed these characteristics simultaneously, due to the challenges involved. In this paper, we propose a novel inexpensive sparse non-negative reconstruction method. We utilise a non-negativity penalty term within a convex function while imposing sparsity at the same time. Our method, termed as SnSA (smooth non-negative sparse approximation) applies a novel thresholding strategy on the sparse coefficients during the minimisation of the proposed convex function . The main advantage of SnSA algorithm is that hard zeroing the negative samples which leads to unstable and non-optimal sparse solution is avoided. Instead, a differentiable smoothing function is proposed that allows gradual suppression of negative samples leading to a sparse non-negative solution. This way, the algorithm is driven towards a solution with a balance in maximising the sparsity and minimising the reconstruction error. Our numerical and experimental results on both synthetic signals and well-established face and handwritten image databases, indicate higher classification performance of the proposed method compared to the state-of-the-art techniques.},
  archive      = {J_ASOC},
  author       = {Aboozar Ghaffari and Mahdi Kafaee and Vahid Abolghasemi},
  doi          = {10.1016/j.asoc.2021.107723},
  journal      = {Applied Soft Computing},
  pages        = {107723},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Smooth non-negative sparse representation for face and handwritten recognition},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dilated adversarial u-net network for automatic gross tumor
volume segmentation of nasopharyngeal carcinoma. <em>ASOC</em>,
<em>111</em>, 107722. (<a
href="https://doi.org/10.1016/j.asoc.2021.107722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nasopharyngeal carcinoma (NPC) is a malignant tumor in the nasopharyngeal epithelium and is mainly treated by radiotherapy . The accurate delineation of the target tumor can greatly improve the radiotherapy effectiveness. However, due to the small size of the NPC imaging volume, the scarcity of labeled samples, the low signal-to-noise ratio in small target areas and the lack of detailed features, automatic gross tumor volume (GTV) delineation inspired by advances in domain adaption for high-resolution image processing has become a great challenge. In addition, since computed tomography (CT) images have the low resolution of soft tissues, it is difficult to identify small volume tumors, and segmentation accuracy of this kind of small GTV is very low. In this paper, we propose an automatic segmentation model based on adversarial network and U-Net for NPC delineation. Specifically, we embed adversarial classification learning into a segmentation network to balance the distribution differences between the small targets in the sample and the large target categories. To reduce the loss weight of large target categories with large samples, and simultaneously increase the weight of small target categories, we design a new U-Net based on focal loss as a GTV segmentation model for adjusting the effect of different categories on the final loss. This method can effectively solve the feature bias caused by the imbalance of the target volume distribution. Furthermore, we conduct a pre-processing of images using an algorithm based on distribution histograms to ensure that the same or approximate CT value represents the same organization. In order to evaluate our proposed method, we perform experiments on the open datasets from StructSeg2019 and the datasets provided by Sichuan Provincial Cancer Hospital. The results of the comparison with some typical up-to-date methods demonstrate that our model can significantly enhance detection accuracy and sensitivity for NPC segmentation.},
  archive      = {J_ASOC},
  author       = {Yanhua Liu and Xiaoguang Yuan and Xin Jiang and Pei Wang and Jinqiao Kou and Haofeng Wang and Mingzhe Liu},
  doi          = {10.1016/j.asoc.2021.107722},
  journal      = {Applied Soft Computing},
  pages        = {107722},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dilated adversarial U-net network for automatic gross tumor volume segmentation of nasopharyngeal carcinoma},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regional energy internet project investment decision making
framework through interval type-2 fuzzy number based choquet integral
fuzzy synthetic model. <em>ASOC</em>, <em>111</em>, 107718. (<a
href="https://doi.org/10.1016/j.asoc.2021.107718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of traditional energy system , the essential obstacles to the coordination between different energy sources is increasingly obvious, which greatly hinders the further improvement of energy efficiency. As a promising energy supply and consumption system , Regional Energy Internet (REI) has significant practical values in improving the coordination of traditional energy and promoting large-scale renewable energy penetration. Due to the large scale, high ambiguity and numerous key interactive criteria, the investment decision making of REI project is a key problem in application management, but it is challenging and has not been solved. Therefore, this paper explores a targeted and scientific REI investment decision making framework through the interval type-2 fuzzy number, Choquet integral and fuzzy synthetic evaluation model according to different preferences of investors to ensure the successful implementation and effective resources integration of potential REI projects. To verify the effectiveness of the framework, a case study with three typical REI projects simulated from a two-stage stochastic optimization model is studied, and corresponding investment countermeasures are proposed as a reference to allocate resources and prevent risk. Besides, since REI investment decision-making has not been deeply studied, this paper contributes to literature and expands knowledge.},
  archive      = {J_ASOC},
  author       = {Yunna Wu and Ting Zhang and Liqi Yi},
  doi          = {10.1016/j.asoc.2021.107718},
  journal      = {Applied Soft Computing},
  pages        = {107718},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Regional energy internet project investment decision making framework through interval type-2 fuzzy number based choquet integral fuzzy synthetic model},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bi-objective grey wolf optimization algorithm combined levy
flight mechanism for the FMC green scheduling problem. <em>ASOC</em>,
<em>111</em>, 107717. (<a
href="https://doi.org/10.1016/j.asoc.2021.107717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more and more enterprises have laid great emphasis on eco-friendly manufacturing processes while enhancing agility, this paper focuses on the green scheduling problem of the Flexible Manufacturing Cell with material handling robots (FMC-R). In the FMC, each job is characterized by several operations, processing machines, and machining time. Robots at one straight track transfer jobs to the related machines. Distinguished from most of the FMC scheduling problems, this paper considers not only the operation sequencing of jobs but also the robots’ transportation processes. A bi-objective mathematical model is established aiming to minimize the total makespan and the total energy consumption of the FMC system simultaneously. Due to the NP-hard nature of the problem, a Levy Flight and Weighted Distance-updated Multi-objective Grey Wolf Algorithm (LWMOGWO) is proposed. It combines the Levy flight with Multi-objective Grey Wolf Algorithm (MOGWO). A weighted distance updating mechanism is integrated to calculate the positions of individuals. A local neighbourhood search strategy is adopted, which makes it possible to take full advantage of the three leading wolves in population. The performance of the LWMOGWO algorithm is evaluated through groups of experiments by comparing with multiple hybrid meta-heuristic algorithms, hybrid particle swarm optimization (NGPSO) algorithm, an improved grey wolf optimization (IGWO) algorithm, MOGWO, NSGA-II (Non-dominated sorting genetic algorithm), and PLMEAPS. Results reveal the proposed LWMOGWO algorithm has a higher quality of solutions in solving the scheduling of the FMC-R problem.},
  archive      = {J_ASOC},
  author       = {Binghai Zhou and Yuanrui Lei},
  doi          = {10.1016/j.asoc.2021.107717},
  journal      = {Applied Soft Computing},
  pages        = {107717},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-objective grey wolf optimization algorithm combined levy flight mechanism for the FMC green scheduling problem},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multistage decision-making method for multi-source
information with shapley optimization based on normal cloud models.
<em>ASOC</em>, <em>111</em>, 107716. (<a
href="https://doi.org/10.1016/j.asoc.2021.107716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is difficult to make scientific decisions for multistage complex decision-making problems, which are heavily affected by multi-level attributes, stage changes, and multi-source heterogeneous information. The normal cloud model is an uncertainty transformation model for constructing mappings between numerical values and their linguistic representations , which can be used to describe multi-source evaluation information for a better reflection of the distribution characteristics and uncertainties. This study aims to develop an effective multi-level multi-attribute decision-making method based on normal cloud models to solve multistage evaluation problems with multi-source heterogeneous information. Firstly, an optimization model is established to capture stage weights and lower-level attribute weights based on cloud distance. Secondly, a bidirectional cloud projection measure is proposed to obtain the measurement values of upper-level attributes based on horizontal and vertical reference points. Thirdly, the comprehensive evaluation values of alternatives are obtained based on Shapley weights of the upper-level attributes. Finally, an illustrative example is presented to clarify the feasibility and superiority of the proposed method. The result indicates that our method is (1) a flexible evaluation framework based on target reference points in fuzzy environments, (2) a powerful measurement tool for aggregating multi-source information, and (3) an objective decision-making method considering the interrelationships between objects. It is of great significance for the enterprises to optimize their operation mechanisms and resource allocation based on these evaluation results.},
  archive      = {J_ASOC},
  author       = {Weiqiao Liu and Jianjun Zhu},
  doi          = {10.1016/j.asoc.2021.107716},
  journal      = {Applied Soft Computing},
  pages        = {107716},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multistage decision-making method for multi-source information with shapley optimization based on normal cloud models},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two approaches for the min-degree constrained minimum
spanning tree problem. <em>ASOC</em>, <em>111</em>, 107715. (<a
href="https://doi.org/10.1016/j.asoc.2021.107715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an undirected, connected and weighted graph, the min-degree constrained minimum spanning tree (md_MST) problem aims to find a spanning tree ( T T ) with minimum cost subject to each non-leaf vertex in T T must have at least d d degree, where d d is a constant positive integer. Being a NP NP -hard problem, it finds several practical applications in the design of networks. In this paper, we present two approaches for this problem in which the first approach is a hybrid metaheuristic technique (hABC) combining an artificial bee colony algorithm with local search, and the second approach is iterated local search (ILS). The proposed hABC distinguishes itself from the existing hybrid artificial bee colony algorithm particularly by its size of employed bee population, initial solution generation, neighborhood operators, the way scout bee phase is handled and the local search. The proposed hABC works with a population of only 3 solutions that effectively coordinates with other key components of hABC and helps in finding new high quality solutions particularly for larger size of available benchmark instances. Extensive experiments on four data-sets of 105 benchmark instances demonstrate that ILS and hABC dominate state-of-the-art approaches. Specifically, hABC finds new best values for 41 instances, whereas ILS finds new best values for 28 instances in comparison to state-of-the-art approaches. Experimental results also show that hABC without local search (hABC-LS) is overall superior to state-of-the-art approaches except one where hABC-LS is comparable, but computationally faster on available benchmark instances.},
  archive      = {J_ASOC},
  author       = {Sudishna Ghoshal and Shyam Sundar},
  doi          = {10.1016/j.asoc.2021.107715},
  journal      = {Applied Soft Computing},
  pages        = {107715},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two approaches for the min-degree constrained minimum spanning tree problem},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible control of discrete event systems using environment
simulation and reinforcement learning. <em>ASOC</em>, <em>111</em>,
107714. (<a href="https://doi.org/10.1016/j.asoc.2021.107714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete Event Systems ( DESs DESs ) are classically modeled as Finite State Machines ( FSMs FSMs ), and controlled in a maximally permissive, controllable, and nonblocking way using Supervisory Control Theory ( SCT SCT ). While SCT SCT is powerful to orchestrate events of DESs DESs , it fail to process events whose control is based on probabilistic assumptions. In this research, we show that some events can be approached as usual in SCT SCT , while others can be processed using Artificial Intelligence . We present a tool to convert SCT SCT controllers into Reinforcement Learning ( RL RL ) simulation environments, from where they become suitable for intelligent processing. Then, we propose a RL RL -based approach that recognizes the context under which the selected set of stochastic events occur, and treats them accordingly, aiming to find suitable decision making as complement to deterministic outcomes of the SCT SCT . The result is an efficient combination of safe and flexible control, which tends to maximize performance for a class of DES DES that evolves probabilistically. Two RL RL algorithms are tested, State–Action–Reward–State–Action (SARSA) and N-step SARSA, over a flexible automotive plant control. Results suggest a performance improvement 9 times higher when using the proposed combination in comparison with non-intelligent decisions.},
  archive      = {J_ASOC},
  author       = {Kallil M.C. Zielinski and Lucas V. Hendges and João B. Florindo and Yuri K. Lopes and Richardson Ribeiro and Marcelo Teixeira and Dalcimar Casanova},
  doi          = {10.1016/j.asoc.2021.107714},
  journal      = {Applied Soft Computing},
  pages        = {107714},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Flexible control of discrete event systems using environment simulation and reinforcement learning},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Multi-fidelity evolutionary multitasking optimization for
hyperspectral endmember extraction. <em>ASOC</em>, <em>111</em>, 107713.
(<a href="https://doi.org/10.1016/j.asoc.2021.107713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Endmember extraction plays an indispensable role in hyperspectral image processing , which is also an important step to decompose the mixed pixels in spectral unmixing . Most of the existing methods based on the principle of convex geometry and intelligent optimization algorithms have achieved an excellent result, but they also suffer from the influence of outliers and expensive computation respectively. Furthermore, it is difficult to determine the number of endmembers in a hyperspectral image without any prior conditions. In order to alleviate these problems, we propose a multi-fidelity evolutionary multitasking optimization framework for hyperspectral endmember extraction in this article. In the proposed framework, multiple tasks to extract different numbers of endmembers are uniformly coded into a single population, aiming to process these similar tasks simultaneously with the implicit genetic transfer. Accordingly, our proposed algorithm is capable of extracting the best endmember set within a certain range of the number of endmembers without any prior knowledge. In addition, an ensemble surrogate model is firstly built to approximate the high-fidelity fitness with the low-fidelity fitness in endmember extraction, which greatly alleviates the time-consuming problem of individual evaluation. The experimental results on the simulated and the real hyperspectral data demonstrate the effectiveness of the proposed framework. The accuracy and the efficiency of endmember extraction are greatly improved compared with other classic endmember extraction methods.},
  archive      = {J_ASOC},
  author       = {Jianzhao Li and Hao Li and Yiting Liu and Maoguo Gong},
  doi          = {10.1016/j.asoc.2021.107713},
  journal      = {Applied Soft Computing},
  pages        = {107713},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-fidelity evolutionary multitasking optimization for hyperspectral endmember extraction},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A scheduling and planning method for geological disasters.
<em>ASOC</em>, <em>111</em>, 107712. (<a
href="https://doi.org/10.1016/j.asoc.2021.107712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upon the occurrence of a disaster, emergency supplies should arrive at disaster areas with different severities in the shortest time. Therefore, it is of pivotal importance to plan and schedule appropriate processes. This paper first elaborates on the principle of ordered arrival of emergency supplies. Then, it establishes a multi-objective multi-constraint emergency material scheduling (EMS) model based on the principle of ordered arrival. Subsequently, a hybrid ant colony optimization (HACO) is proposed to solve EMS. The computational results for 15 multi-depot benchmark instances are reported and the average error between HACO and the best known solution (BKS) is only 0.16\% Then, real geological disasters from Hubei province in China are examined as a case study. Experimental results show that HACO performs better than non-dominated sorting genetic algorithm II (NSGA-II) and multi-objective ant colony algorithm (MOACO) in the case study and benchmark instances.},
  archive      = {J_ASOC},
  author       = {Fang Wan and Haixiang Guo and Jinling Li and Mingyun Gu and Wenwen Pan and Yangjian Ying},
  doi          = {10.1016/j.asoc.2021.107712},
  journal      = {Applied Soft Computing},
  pages        = {107712},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A scheduling and planning method for geological disasters},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The application of gradient evolution algorithm to an
intuitionistic fuzzy neural network for forecasting medical cost of
acute hepatitis treatment in taiwan. <em>ASOC</em>, <em>111</em>,
107711. (<a href="https://doi.org/10.1016/j.asoc.2021.107711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting is important in the decision-making process. Therefore, among many forecasting algorithms that have been developed, fuzzy neural network (FNN) has been widely applied. The FNN structure is commonly trained using a back-propagation algorithm. However, this algorithm is sensitive to the initial weights and high computation, especially for complex problems. Thus, this study intended to overcome these drawbacks by applying the gradient evolution (GE) algorithm to train the intuitionistic FNN (IFNN). The proposed algorithm, GEIFNN, was verified using ten benchmark datasets. The results were compared with some other metaheuristic-based IFNN algorithms, such as genetic algorithm , particle swarm optimization algorithm , and differential evolution algorithm . The computational results show that the proposed GEIFNN relatively outperformed other tested algorithms, especially in training error. Furthermore, the proposed algorithm was applied to forecast medical costs for the treatment of acute hepatitis in Taiwan. The result also shows that GEIFNN can obtain the smallest training error.},
  archive      = {J_ASOC},
  author       = {R.J. Kuo and Ferani E. Zulvia},
  doi          = {10.1016/j.asoc.2021.107711},
  journal      = {Applied Soft Computing},
  pages        = {107711},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The application of gradient evolution algorithm to an intuitionistic fuzzy neural network for forecasting medical cost of acute hepatitis treatment in taiwan},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing a new ensemble approach with multi-class SVMs for
manuka honey quality classification. <em>ASOC</em>, <em>111</em>,
107710. (<a href="https://doi.org/10.1016/j.asoc.2021.107710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops an approach to Manuka honey quality classification, using feature reduction and support vector machines (SVMs). We have developed a database of hyperspectral images of honey, including different quality Manuka honey. This paper focusses on machine learning techniques , specifically SVMs, to classify the quality of Manuka honey. We discuss the different types of multi-class SVMs and show the advantages of their different hierarchical structures. We look to evaluate the often overlooked tree-based multi-class SVM structures as they could have some advantages for our type of dataset. We also propose an ensemble technique for classifying the multi-class hyperspectral honey dataset, mitigating a key drawback of these tree-based methods, that they cannot correct for misclassifications . The results show that in our Manuka honey dataset, the SVMs with hierarchical structures can outperform the parallel based SVM structures and had the best performance overall when used in combination with the class embodiment autoencoder . The new ensemble technique is outperforming the benchmark SVMs for all the evaluated feature reduction techniques and is solving a drawback of the tree-based SVMs.},
  archive      = {J_ASOC},
  author       = {Tessa Phillips and Waleed Abdulla},
  doi          = {10.1016/j.asoc.2021.107710},
  journal      = {Applied Soft Computing},
  pages        = {107710},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing a new ensemble approach with multi-class SVMs for manuka honey quality classification},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A self-adaptive approach for white blood cell classification
towards point-of-care testing. <em>ASOC</em>, <em>111</em>, 107709. (<a
href="https://doi.org/10.1016/j.asoc.2021.107709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As important immune cells in the human body, white blood cells play a very significant role in the auxiliary diagnosis of many major diseases. Clinically, changes in the number and morphology of white blood cells and their subtypes are the prediction index for important, serious diseases, such as anaemia, malaria, infections, and tumours. The application of image recognition technology and cloud computing to assist in medical diagnosis is a hot topic in current research, which we believe have great potential to further improve real-time detection and improve medical diagnosis. This paper proposes a novel automatic classification framework for the recognition of five subtypes of white blood cells, in the hope of contributing to disease prediction. First, we present an adaptive threshold segmentation method to deal with blood smear images with nonuniform colour and uneven illumination. The method is designed based on colour space information and threshold segmentation. After successfully separating the white blood cell from the blood smear image, a large number of features, including geometrical, colour, and texture features are extracted. However, redundant features can affect the classification speed and efficiency, and in view of that, a feature selection algorithm based on classification and regression trees (CART) is designed to successfully remove irrelevant and redundant features from the initial features. The selected prominent features are fed into a particle swarm optimisation support vector machine (PSO-SVM) classifier to recognise the types of white blood cells. Finally, to evaluate the performance of the proposed white blood cell classification methodology, we build a white blood cell data set containing 500 blood smear images for experiments. The proposed methodology achieves 99.76\% classification accuracy , which well demonstrates its effectiveness.},
  archive      = {J_ASOC},
  author       = {Na Dong and Meng-die Zhai and Jian-fang Chang and Chun-ho Wu},
  doi          = {10.1016/j.asoc.2021.107709},
  journal      = {Applied Soft Computing},
  pages        = {107709},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-adaptive approach for white blood cell classification towards point-of-care testing},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting fully vaccinated people against COVID-19 and
examining future vaccination rate for herd immunity in the US, asia,
europe, africa, south america, and the world. <em>ASOC</em>,
<em>111</em>, 107708. (<a
href="https://doi.org/10.1016/j.asoc.2021.107708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-2019) has spread rapidly all over the world and it is known that the most effective way to eliminate the disease is vaccination. Although the traditional vaccine development process is quite long, more than ten COVID-19 vaccines have been approved for use in about a year. The COVID-19 vaccines that have been administered are highly effective enough, but achieving herd immunity is required to end the pandemic. The motivation of this study is to contribute to review the countries’ vaccine policies and adjusting the manufacturing plans of the vaccine companies. In this study, the total number of people fully vaccinated against COVID-19 was forecasted in the US, Asia, Europe, Africa, South America, and the World with the Autoregressive Integrated Moving Average (ARIMA) model, which is a new approach in vaccination studies. Additionally, for herd immunity, the percentage of fully vaccinated people in these regions at the beginning of 2021 summer was determined. ARIMA results show that in the US, Asia, Europe, Africa, South America, and the World will reach 139 million, 109 million, 127 million, 8 million, 38 million, and 441 million people will be fully vaccinated on 1 June 2021, respectively. According to these results, 41.8\% of the US, 2.3\% of Asia, 17\% of Europe, 0.6\% of Africa, 8.8\% of South America, and 5.6\% of the World population will be fully vaccinated people against the COVID-19. Results show that countries are far from the herd immunity threshold level desired to reach for safely slow or stop the COVID-19 epidemic.},
  archive      = {J_ASOC},
  author       = {Pınar Cihan},
  doi          = {10.1016/j.asoc.2021.107708},
  journal      = {Applied Soft Computing},
  pages        = {107708},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting fully vaccinated people against COVID-19 and examining future vaccination rate for herd immunity in the US, asia, europe, africa, south america, and the world},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel weight pruning strategy for light weight neural
networks with application to the diagnosis of skin disease.
<em>ASOC</em>, <em>111</em>, 107707. (<a
href="https://doi.org/10.1016/j.asoc.2021.107707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based models have achieved significant advances in manifold computer vision problems, but tedious parameter tuning has complicated their application to computer-aided diagnostic (CAD) systems. As such, this study introduces a novel pruning strategy to improve the accuracy of five lightweight deep convolutional neural network (DCNN) architectures applied to the classification of skin disease. Unlike conventional pruning methods (such as optimal brain surgeon), the proposed technique does not change the model size yet improves performance after fine tuning. This training approach, intended to improve accuracy without increasing model complexity, is experimentally verified using 1167 pathological images. The clinical data included 11 different skin disease types collected over the past ten years, with varying image quantities in each category. A novel hierarchical pruning method, based on standard deviation, is then developed and used to prune parameters in each convolution layer according to the different weight distributions. This training strategy achieves an 83.5\% Top-1 accuracy using a pruned MnasNet (12.5 MB), which is 1.8\% higher than that of unpruned InceptionV3 (256 MB). Comparative experiments using other networks (MobileNetV2, SqueezeNet, ShuffleNetV2, Xception, ResNet50, DenseNet121) and dataset (HAM10000) also demonstrate consistent improvements when adopting the proposed model training technique. This distinctive robustness across various network types and simple deployment demonstrates the potential of this methodology for generalization to other computer vision tasks.},
  archive      = {J_ASOC},
  author       = {Kun Xiang and Linlin Peng and Haiqiong Yang and Mingxin Li and Zhongfa Cao and Shancheng Jiang and Gang Qu},
  doi          = {10.1016/j.asoc.2021.107707},
  journal      = {Applied Soft Computing},
  pages        = {107707},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel weight pruning strategy for light weight neural networks with application to the diagnosis of skin disease},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Defect classification based on deep features for railway
tracks in sustainable transportation. <em>ASOC</em>, <em>111</em>,
107706. (<a href="https://doi.org/10.1016/j.asoc.2021.107706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rail tracks are the most important component of train movement in rail transportation. Therefore, real-time detection of defects on track surfaces is important but also difficult because of the noise, low contrast, and inhomogeneity of density. In recent years, tools have been developed for robust and highly accurate defect detection with advances in deep learning technologies. However, the existing deep learning algorithms require a large number of parameters to be set, which is computationally expensive. Therefore, those algorithms cannot fulfill the requirements for quick inspection. In this study, rail surface defects were detected by fusing the features of two deep learning models. SqueezeNet and MobileNetV2, the two models selected for this purpose, are both smaller in size and faster than other deep learning models. However, both of these models are less accurate than other models. Therefore, in this study, a fusion model with high accuracy is proposed by combining the features of the two models. First, a contrast adjustment is applied to the original image of the rail, and then the rail track location is determined. Then, most weighted features are selected from each network, and the defects are determined by giving the reduced features to Support Vector Machines (SVM). Experimental results show that the proposed method gives better results for multiple rail surface defects under low contrast than using a single deep learning model.},
  archive      = {J_ASOC},
  author       = {Ilhan Aydin and Erhan Akin and Mehmet Karakose},
  doi          = {10.1016/j.asoc.2021.107706},
  journal      = {Applied Soft Computing},
  pages        = {107706},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Defect classification based on deep features for railway tracks in sustainable transportation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New benchmark algorithms for no-wait flowshop group
scheduling problem with sequence-dependent setup times. <em>ASOC</em>,
<em>111</em>, 107705. (<a
href="https://doi.org/10.1016/j.asoc.2021.107705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Setup operations and waiting time between production procedures are prime examples of non-value-adding activities that can be alleviated through well-informed production decisions. Application of the No-wait Flowshop Group Scheduling Problems with Sequence-Dependent Setup Times (NWFGSP_SDST) as an optimization tool helps maintain the setup and waiting times at their minimum when optimizing advanced production systems. This research contributes to the understudied literature of Group Scheduling Problems (GSP), developing two metaheuristics , a Revised Multi-start Simulated Annealing (RMSA) and a local search-based variant (RMSA L S LS ), to solve the NWFGSP_SDST problem. Yielding the best-found solution in more than 99.7 percent of the benchmark instances, it is shown that RMSA is superior to the existing state-of-the-art algorithms developed to solve the NWFGSP_SDST problem. Besides, about 80 percent of the best-found solutions were further improved by RMSA L S LS with the statistical analysis confirming that RMSA L S LS reduced the total completion time obtained by RMSA at the expense of longer computational time. Overall, this research explored scheduling as an operational strategic tool facilitating lean production.},
  archive      = {J_ASOC},
  author       = {Chen-Yang Cheng and Pourya Pourhejazy and Kuo-Ching Ying and Yi-Hsiu Liao},
  doi          = {10.1016/j.asoc.2021.107705},
  journal      = {Applied Soft Computing},
  pages        = {107705},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New benchmark algorithms for no-wait flowshop group scheduling problem with sequence-dependent setup times},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A text mining-based framework to discover the important
factors in text reviews for predicting the views of live streaming.
<em>ASOC</em>, <em>111</em>, 107704. (<a
href="https://doi.org/10.1016/j.asoc.2021.107704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live streaming has become one of the leisure activities of most people due to the rich and various contents. For young generation, to watch other people playing games on the live streaming platform is becoming very popular. Related researches mainly focused on predicting the number of viewers, finding popular streamer, studying the gift giving behaviors, and so on. Relatively few studies focused on how viewers’ comments affect users’ viewing behaviors, since the power of text comments in social media have been confirmed. In addition, published studies usually employed questionnaire survey methods which are prone to experimental effects. And online text comments will be more objective and less sampling bias than data collected by questionnaires. Consequently, this study focuses live streaming of games and uses viewers’ text comments for experimental analysis. A text mining-based framework which includes Least Absolute Shrinkage and Selection Operator (LASSO), Support Vector Machine-Recursive Feature Elimination (SVM-RFE), Chi-square test will be proposed to determine the important keywords of predicting the number of views in live streaming. Support Vector Machine (SVM) will be utilized to evaluate the performances of candidate feature subsets. Then, K-means and Latent Semantic Analysis (LSA) using Singular Value Decomposition (SVD) have been used to organize the selected keywords into understandable concepts. Real cases of game live streaming cases will be collected from Twitch.tv for our experiments. Results can be used as a reference for live streaming platforms and live channels, and help them to increase the number of viewers for further income enhancement.},
  archive      = {J_ASOC},
  author       = {Wen-Kuo Chen and Long-Sheng Chen and Yi-Ting Pan},
  doi          = {10.1016/j.asoc.2021.107704},
  journal      = {Applied Soft Computing},
  pages        = {107704},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A text mining-based framework to discover the important factors in text reviews for predicting the views of live streaming},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A surrogate-assisted radial space division evolutionary
algorithm for expensive many-objective optimization problems.
<em>ASOC</em>, <em>111</em>, 107703. (<a
href="https://doi.org/10.1016/j.asoc.2021.107703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model management is the key for surrogate-assisted evolutionary algorithms to provide high-quality solutions for computationally expensive optimization problems . In this paper, a surrogate-assisted evolutionary algorithm based on radial space division is proposed for expensive many-objective optimization problems. The algorithm projects the individuals in the high-dimensional space to the radial space. Then, the location distribution of the individuals in the radial space and the uncertain information provided by the Kriging model are used for managing the surrogate model . In addition, two archives are used to manage the model, a fixed archive is used to hold the data that builds the Kriging model, and a variable archive is used to save the non-dominant solution. Finally, the performance of the proposed algorithm is tested on three sets of benchmark problems and two automobile structure design problem. The simulation results show that the algorithm is more competitive than the commonly used surrogate-assisted evolutionary algorithms.},
  archive      = {J_ASOC},
  author       = {Qinghua Gu and Yufeng Zhou and Xuexian Li and Shunling Ruan},
  doi          = {10.1016/j.asoc.2021.107703},
  journal      = {Applied Soft Computing},
  pages        = {107703},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A surrogate-assisted radial space division evolutionary algorithm for expensive many-objective optimization problems},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A competitive predator–prey approach to enhance surveillance
by UAV swarms. <em>ASOC</em>, <em>111</em>, 107701. (<a
href="https://doi.org/10.1016/j.asoc.2021.107701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present the competitive optimisation of a swarm of Unmanned Aerial Vehicles (UAV) protecting a restricted area from a number of intruders following a Predator–Prey approach. We propose a Competitive Coevolutionary Genetic Algorithm (CompCGA) which optimises the parameters of the UAVs (i.e. predators) to maximise the detection of intruders, while the parameters of the intruders (i.e. preys) are optimised to maximise their intrusion success rate. Having chosen the CACOC (Chaotic Ant Colony Optimisation for Coverage) as the base mobility model for the UAVs, we propose an improved new version, where its behaviour is modified by identifying and optimising new parameters to improve the overall success rate when detecting intruders. Six case studies have been optimised using simulations by performing 30 independent runs (180 in total) of our CompCGA. Finally, we conducted a series of master tournaments (1, 800, 000 evaluations) using the best specimens obtained from each run and case study to test the robustness of our proposed approach against unexpected intruders. Our surveillance system improved the average percentage of intruders detected with respect to CACOC by a maximum of 126\%. More than 90\% of intruders were detected on average when using a swarm of 16 UAVs while CACOC’s detection rates are always under 80\% in all cases.},
  archive      = {J_ASOC},
  author       = {Daniel H. Stolfi and Matthias R. Brust and Grégoire Danoy and Pascal Bouvry},
  doi          = {10.1016/j.asoc.2021.107701},
  journal      = {Applied Soft Computing},
  pages        = {107701},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A competitive Predator–Prey approach to enhance surveillance by UAV swarms},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Federated matrix factorization for privacy-preserving
recommender systems. <em>ASOC</em>, <em>111</em>, 107700. (<a
href="https://doi.org/10.1016/j.asoc.2021.107700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems recommend contents or services via collecting and analyzing numerous user data, which may raise serious privacy concerns when the recommender is untrusted. Inspired by federated learning , a user-level distributed matrix factorization framework has been proposed where the model can be learned via collecting gradient information from users (instead of the raw data). This approach focuses on protecting model-privacy and value-privacy from untrusted recommender but has limited consideration on existence-privacy. To address this issue, we enhance the aforementioned framework with Homomorphic Encryption and randomized response. Extensive experiments demonstrate that our method can provide more secure protection for users’ privacy with less performance degradation and smaller computational burden.},
  archive      = {J_ASOC},
  author       = {Yongjie Du and Deyun Zhou and Yu Xie and Jiao Shi and Maoguo Gong},
  doi          = {10.1016/j.asoc.2021.107700},
  journal      = {Applied Soft Computing},
  pages        = {107700},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated matrix factorization for privacy-preserving recommender systems},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A memory-trait-driven decomposition–reconstruction–ensemble​
learning paradigm for oil price forecasting. <em>ASOC</em>,
<em>111</em>, 107699. (<a
href="https://doi.org/10.1016/j.asoc.2021.107699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the prediction performance in oil price forecasting, a novel memory-trait-driven decomposition–reconstruction–ensemble learning paradigm is proposed for oil price forecasting. The proposed methodology consists of four steps, i.e., data decomposition for original complex time series, component reconstruction for decomposed components, individual prediction for the reconstructed components, and ensemble output based on the individual component prediction results, which are all driven by memory traits. For verification purpose, the West Texas Intermediate (WTI) crude oil spot prices are used as the sample data. The experimental results demonstrated that the proposed methodology can produce the better and more robust results relative to the benchmarking models listed in this study. This indicates that the proposed memory-trait-driven decomposition–reconstruction–ensemble​ methodology can be used as a promising solution to oil price prediction with the traits of memory.},
  archive      = {J_ASOC},
  author       = {Lean Yu and Mengyao Ma},
  doi          = {10.1016/j.asoc.2021.107699},
  journal      = {Applied Soft Computing},
  pages        = {107699},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A memory-trait-driven decomposition–reconstruction–ensemble​ learning paradigm for oil price forecasting},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Harris hawks optimisation with simulated annealing as a deep
feature selection method for screening of COVID-19 CT-scans.
<em>ASOC</em>, <em>111</em>, 107698. (<a
href="https://doi.org/10.1016/j.asoc.2021.107698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) is a contagious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It may cause severe ailments in infected individuals. The more severe cases may lead to death. Automated methods which can detect COVID-19 in radiological images can help in the screening of patients. In this work, a two-stage pipeline composed of feature extraction followed by feature selection (FS) for the detection of COVID-19 from CT scan images is proposed. For feature extraction, a state-of-the-art Convolutional Neural Network (CNN) model based on the DenseNet architecture is utilised. To eliminate the non-informative and redundant features, the meta-heuristic called Harris Hawks optimisation (HHO) algorithm combined with Simulated Annealing (SA) and Chaotic initialisation is employed. The proposed approach is evaluated on the SARS-COV-2 CT-Scan dataset which consists of 2482 CT-scans. Without the Chaotic initialisation and the SA, the method gives an accuracy of around 98.42\% which further increases to 98.85\% on the inclusion of the two and thus delivers better performance than many state-of-the-art methods and various meta-heuristic based FS algorithms . Also, comparison has been drawn with many hybrid variants of meta-heuristic algorithms. Although HHO falls behind a few of the hybrid variants, when Chaotic initialisation and SA are incorporated into it, the proposed algorithm performs better than any other algorithm with which comparison has been drawn. The proposed algorithm decreases the number of features selected by around 75\% , which is better than most of the other algorithms.},
  archive      = {J_ASOC},
  author       = {Rajarshi Bandyopadhyay and Arpan Basu and Erik Cuevas and Ram Sarkar},
  doi          = {10.1016/j.asoc.2021.107698},
  journal      = {Applied Soft Computing},
  pages        = {107698},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Harris hawks optimisation with simulated annealing as a deep feature selection method for screening of COVID-19 CT-scans},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial bee colony algorithm based on multiple
neighborhood topologies. <em>ASOC</em>, <em>111</em>, 107697. (<a
href="https://doi.org/10.1016/j.asoc.2021.107697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, as an effective global optimization technique, artificial bee colony (ABC) algorithm has attracted increasing attention for its good performance yet easy implementation. However, the solution search equation of ABC emphasizes exploration over exploitation, which greatly affects the convergence speed and accuracy of ABC. To solve this problem, many ABC variants have been developed to enhance exploitation by using the superior individuals. However, the concept of neighborhood topology has rarely been considered, which has significant effect on the dissemination of search information among individuals. Hence, in this work, we propose a new ABC variant based on multiple neighborhood topologies (named ABC-MNT). In ABC-MNT, three kinds of neighborhood topologies are used for different individuals to perform diverse abilities of disseminating search information, contributing to a better balance between exploration and exploitation. Considering the characteristics of different neighborhood topologies, three modified solution search equations are assigned to the three neighborhood topologies, respectively, to generate offspring. Furthermore, to preserve search experience of the scout bee phase, the global neighborhood search and opposition-based learning techniques are used. Extensive experiments are carried out on two widely used test suites, and eight well-established ABC variants and five other evolutionary algorithms are included in the performance comparison. The comparison results verify that ABC-MNT can obtain better or at least comparable performance on most of the benchmark functions .},
  archive      = {J_ASOC},
  author       = {Xinyu Zhou and Yanlin Wu and Maosheng Zhong and Mingwen Wang},
  doi          = {10.1016/j.asoc.2021.107697},
  journal      = {Applied Soft Computing},
  pages        = {107697},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Artificial bee colony algorithm based on multiple neighborhood topologies},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing a testing kit supply network for suspected
COVID-19 cases under mixed uncertainty approach. <em>ASOC</em>,
<em>111</em>, 107696. (<a
href="https://doi.org/10.1016/j.asoc.2021.107696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the COVID-19 (C-19) pandemic and the challenges it poses to global health and the medical communities, this research aims to investigate the factors affecting of reduction health inequalities related to the C-19 to tackle the increasing number of outbreaks and their social consequences in such a pandemic. Hence, we design a COVID-19 testing kit supply network (C-19TKSN) to allocate various C-19 test kits to the suspected C-19 cases depending on the time between the emergence of their first symptoms and the time they are tested. In particular, this model aims to minimize the total network cost and decrease false results C-19 test by considering the fundamental characteristics of a diagnostic C-19 test (i.e., specificity and sensitivity). In the sensitivity characteristic, a gamma formula is presented to estimate the error rate of false-negative results. The nature of the C-19TKSN problem is dynamic over time due to difficult predictions and changes in the number of C-19 patients. For this reason, we consider the potential demands relating to different regions of the suspected C-19 cases for various C-19 test kits and the rate of prevalence of C-19 as stochastic parameters . Accordingly, a multi-stage stochastic programming (MSSP) method with a combined scenario tree is proposed to deal with the stochastic data in a dynamic environment. Then, a fuzzy approach is employed based on M e Me measure to cope with the epistemic uncertainty of input data. Eventually, the practicality and capability of the proposed model are shown in a real-life case in Iran. The results demonstrate that the performance of the MSSP model is significantly better in comparison with the two-stage stochastic programming (TSSP) model regarding the false results and the total cost of the network.},
  archive      = {J_ASOC},
  author       = {Seyyed-Mahdi Hosseini-Motlagh and Mohammad Reza Ghatreh Samani and Parnian Farokhnejad},
  doi          = {10.1016/j.asoc.2021.107696},
  journal      = {Applied Soft Computing},
  pages        = {107696},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Designing a testing kit supply network for suspected COVID-19 cases under mixed uncertainty approach},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-carbon joint scheduling in flexible open-shop
environment with constrained automatic guided vehicle by multi-objective
particle swarm optimization. <em>ASOC</em>, <em>111</em>, 107695. (<a
href="https://doi.org/10.1016/j.asoc.2021.107695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reduction of carbon emissions is receiving growing attention in manufacturing industry. In the real-life flexible open-shop manufacturing environments , the workpieces generally need frequent transport between machines so that the limitation of the transport resource is non-negligible. Joint scheduling is a better way to achieve global optimum than sequential scheduling due to the interactive relationship between production and transport. However, owing to the extreme complexity, joint scheduling has received little attention over the past years. In this paper, the low-carbon joint scheduling in flexible open-shop environment with constrained automatic guided vehicle (LCJS-FOSCA) is investigated. A mixed-integer programming (MIP) model is formulated with the objectives to minimize the total carbon emission and makespan. Facing the extreme complexity of LCJS-FOSCA, an enhanced multi-objective particle swarm optimization (EMOPSO) is developed. In EMOPSO, several improvement strategies including the initialization method for high-quality solutions, chaotic position updating and mutation for global exploration, and problem-knowledge-based neighborhood search for local exploitation, are proposed. Finally, the following results are obtained through a comprehensive case study: (1) the proposed strategies make significant promotion on the convergence and comprehensive quality, and EMOPSO is verified to be effective for solving LCJS-FOSCA; (2) compared with the two sequential scheduling methods, the makespan obtained by joint scheduling is reduced by approximately 27.7\% and 25.5\%, and the total carbon emission is reduced by 3.3\% and 4.2\%; (3) the optimal numbers of AGV in various scales of transport time are provided based on the analysis of optimization results. This work presents a significance on promoting cleaner production of the modern manufacturing industry.},
  archive      = {J_ASOC},
  author       = {Weihua Tan and Xiaofang Yuan and Guoming Huang and Zhixian Liu},
  doi          = {10.1016/j.asoc.2021.107695},
  journal      = {Applied Soft Computing},
  pages        = {107695},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Low-carbon joint scheduling in flexible open-shop environment with constrained automatic guided vehicle by multi-objective particle swarm optimization},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting asthma control level using feature-based time
series classification. <em>ASOC</em>, <em>111</em>, 107694. (<a
href="https://doi.org/10.1016/j.asoc.2021.107694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asthma symptom control is the best method for asthma treatment. Early detection of asthma control status can provide the time required for presenting preventive treatment programs to reduce the future risk of asthma exacerbation and episodes of poor asthma control. Accordingly, the current study aims to outline a novel model for classifying asthma control status using a time series/time sequence-based classification approach . Few researchers have addressed the problem of classifying asthma control level in the context of the time series/time sequences dynamics-based approach. It is a significant defect in chronic disease management such as asthma that requires continuous monitoring of symptoms. As a result, it is needed to examine the effect of the time-series/time-sequences of the stimuli affecting asthma status in classifying the control level of this disease. We have designed a daily asthma self-monitoring form and collected a total of 2870 daily assessments of asthma control on 96 asthmatics patients older than five years for 9 months. Using a 7-day window of time, we created the asthma control dataset with clinical variables, the patient’s medical history, and environmental parameters, to detect asthma control level using a time series/time sequence-based classification model . Our best model yielded a recall of 87\%, a specificity of 94\%, a precision of 89\%, a negative predictive value of 93\%, an accuracy of 92\%, an F-measure of 88\%, and an AUC of 87\%. Our study indicated that time series classification models among data mining techniques have great potential in creating a prognostic model for chronic diseases such as asthma. Furthermore, the time-series/time sequences of daily asthma symptoms also play a significant role in classifying asthma control levels.},
  archive      = {J_ASOC},
  author       = {Roghaye Khasha and Mohammad Mehdi Sepehri and Nasrin Taherkhani},
  doi          = {10.1016/j.asoc.2021.107694},
  journal      = {Applied Soft Computing},
  pages        = {107694},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detecting asthma control level using feature-based time series classification},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multimodal approach for regional GDP prediction using
social media activity and historical information. <em>ASOC</em>,
<em>111</em>, 107693. (<a
href="https://doi.org/10.1016/j.asoc.2021.107693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a multimodal approach with which to predict the regional Gross Domestic Product (GDP) by combining historical GDP values with the embodied information in Twitter messages concerning the current economic condition. This proposal is of great interest, since it delivers forecasts at higher frequencies than both the official statistics (published only annually at the regional level in Spain) and the existing unofficial quarterly predictions (which rely on economic indicators that are available only after months of delay). The proposed method is based on a two-stage architecture. In the first stage, a multi-task autoencoder is initially used to obtain a GDP-related representation of tweets, which are then filtered to remove outliers and to obtain the GDP prediction from the consensus of opinions. In a second stage, this result is combined with the historical GDP values of the region using a multimodal network. The method is evaluated in four different regions of Spain using the tweets written by the most relevant economists, politicians, newspapers and institutions in each one. The results show that our approach successfully learns the evolution of the GDP using only historical information and tweets, thus making it possible to provide earlier forecasts about the regional GDP. This method also makes it possible to establish which the most or least influential opinions regarding this prediction are. As an additional exercise, we have assessed how well our method predicted the effect of the COVID-19 pandemic.},
  archive      = {J_ASOC},
  author       = {Javier Ortega-Bastida and Antonio Javier Gallego and Juan Ramón Rico-Juan and Pedro Albarrán},
  doi          = {10.1016/j.asoc.2021.107693},
  journal      = {Applied Soft Computing},
  pages        = {107693},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multimodal approach for regional GDP prediction using social media activity and historical information},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correcting data imbalance for semi-supervised COVID-19
detection using x-ray chest images. <em>ASOC</em>, <em>111</em>, 107692.
(<a href="https://doi.org/10.1016/j.asoc.2021.107692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key factor in the fight against viral diseases such as the coronavirus (COVID-19) is the identification of virus carriers as early and quickly as possible, in a cheap and efficient manner. The application of deep learning for image classification of chest X-ray images of COVID-19 patients could become a useful pre-diagnostic detection methodology. However, deep learning architectures require large labelled datasets. This is often a limitation when the subject of research is relatively new as in the case of the virus outbreak, where dealing with small labelled datasets is a challenge. Moreover, in such context, the datasets are also highly imbalanced, with few observations from positive cases of the new disease. In this work we evaluate the performance of the semi-supervised deep learning architecture known as MixMatch with a very limited number of labelled observations and highly imbalanced labelled datasets. We demonstrate the critical impact of data imbalance to the model’s accuracy. Therefore, we propose a simple approach for correcting data imbalance, by re-weighting each observation in the loss function, giving a higher weight to the observations corresponding to the under-represented class. For unlabelled observations, we use the pseudo and augmented labels calculated by MixMatch to choose the appropriate weight. The proposed method improved classification accuracy by up to 18\%, with respect to the non balanced MixMatch algorithm. We tested our proposed approach with several available datasets using 10, 15 and 20 labelled observations, for binary classification (COVID-19 positive and normal cases). For multi-class classification (COVID-19 positive, pneumonia and normal cases), we tested 30, 50, 70 and 90 labelled observations. Additionally, a new dataset is included among the tested datasets, composed of chest X-ray images of Costa Rican adult patients.},
  archive      = {J_ASOC},
  author       = {Saul Calderon-Ramirez and Shengxiang Yang and Armaghan Moemeni and David Elizondo and Simon Colreavy-Donnelly and Luis Fernando Chavarría-Estrada and Miguel A. Molina-Cabello},
  doi          = {10.1016/j.asoc.2021.107692},
  journal      = {Applied Soft Computing},
  pages        = {107692},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Correcting data imbalance for semi-supervised COVID-19 detection using X-ray chest images},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RBF neural network modeling approach using PCA based LM–GA
optimization for coke furnace system. <em>ASOC</em>, <em>111</em>,
107691. (<a href="https://doi.org/10.1016/j.asoc.2021.107691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network prediction and data processing have been widely used in chemical industry, however, there exist many disturbance variables that will affect the system output, and traditional neural network prediction model has poor accuracy. In this paper, the dimensionality reduction of normalized input variables that affect the outputs is first implemented by using principal component analysis (PCA). Then, radial basis function (RBF) neural network model is established. Levenberg–Marquardt (LM) algorithm is used to initialize the weights of RBF neural network, which overcomes the influence of initial weights during the training process. Genetic algorithm (GA) is further introduced to train the centers, widths and weights to improve the modeling accuracy. Finally, the root mean square error (RMSE) is used to evaluate the prediction performances. Compared with two RBF neural network modeling methods, the proposed method can improve the prediction accuracy greatly.},
  archive      = {J_ASOC},
  author       = {Jili Tao and Zheng Yu and Ridong Zhang and Furong Gao},
  doi          = {10.1016/j.asoc.2021.107691},
  journal      = {Applied Soft Computing},
  pages        = {107691},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RBF neural network modeling approach using PCA based LM–GA optimization for coke furnace system},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Economic power generation scheduling exploiting hill-climbed
sine–cosine​ algorithm. <em>ASOC</em>, <em>111</em>, 107690. (<a
href="https://doi.org/10.1016/j.asoc.2021.107690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper recommends a stochastic population-based optimization procedure named the hill-climbed Sine–Cosine algorithm (HcSCA) to solve the economic generation scheduling (EcGS) of thermal units. The sine–cosine algorithm creates a set of preliminary random individuals and needs them to change away from or near to the destination to create a new population-based on functions of sine and cosine. The sine–cosine algorithm finds the solution quickly while balancing exploration and exploitation but stagnates to a sub-optimal solution. To avoid stagnation, the sine–cosine optimization procedure coordinates local search and hill-climbing heuristics. The specific features i.e. valve-point loading effect, ramp-rate limits, generator limits for generation, prohibited operating zones, and power demand constraints with losses are assumed to resolve power economic thermal generation scheduling. The success of the recommended method is observed on small- and large-scale power test systems. The outcome of the recommended algorithm is matched with erstwhile algorithms. From the experimental results, it is experienced that HcSCA provides a competing solution to economic thermal generation scheduling problems.},
  archive      = {J_ASOC},
  author       = {Gurpreet Kaur and J.S. Dhillon},
  doi          = {10.1016/j.asoc.2021.107690},
  journal      = {Applied Soft Computing},
  pages        = {107690},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Economic power generation scheduling exploiting hill-climbed Sine–Cosine​ algorithm},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impact of the learners diversity and combination method on
the generation of heterogeneous classifier ensembles. <em>ASOC</em>,
<em>111</em>, 107689. (<a
href="https://doi.org/10.1016/j.asoc.2021.107689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensembles of classifiers is a proven approach in machine learning with a wide variety of research works. The main issue in ensembles of classifiers is not only the selection of the base classifiers , but also the combination of their outputs. According to the literature, it has been established that much is to be gained from combining classifiers if those classifiers are accurate and diverse. However, it is still an open issue how to define the relation between accuracy and diversity in order to define the best possible ensemble of classifiers. In this paper, we propose a novel approach to evaluate the impact of the diversity of the learners on the generation of heterogeneous ensembles. We present an exhaustive study of this approach using 27 different multiclass datasets and analysing their results in detail. In addition, to determine the performance of the different results, the presence of labelling noise is also considered.},
  archive      = {J_ASOC},
  author       = {M. Paz Sesmero and José Antonio Iglesias and Elena Magán and Agapito Ledezma and Araceli Sanchis},
  doi          = {10.1016/j.asoc.2021.107689},
  journal      = {Applied Soft Computing},
  pages        = {107689},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Impact of the learners diversity and combination method on the generation of heterogeneous classifier ensembles},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning directed locomotion in modular robots with
evolvable morphologies. <em>ASOC</em>, <em>111</em>, 107688. (<a
href="https://doi.org/10.1016/j.asoc.2021.107688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vision behind this paper looks ahead to evolutionary robot systems where morphologies and controllers are evolved together and ‘newborn’ robots undergo a learning process to optimize their inherited brain for the inherited body. The specific problem we address is learning controllers for the task of directed locomotion in evolvable modular robots. To this end, we present a test suite of robots with different shapes and sizes and compare two learning algorithms, Bayesian optimization and HyperNEAT. The experiments in simulation show that both methods obtain good controllers, but Bayesian optimization is more effective and sample efficient. We validate the best learned controllers by constructing three robots from the test suite in the real world and observe their fitness and actual trajectories. The obtained results indicate a reality gap, but overall the trajectories are adequate and follow the target directions successfully.},
  archive      = {J_ASOC},
  author       = {Gongjin Lan and Matteo De Carlo and Fuda van Diggelen and Jakub M. Tomczak and Diederik M. Roijers and A.E. Eiben},
  doi          = {10.1016/j.asoc.2021.107688},
  journal      = {Applied Soft Computing},
  pages        = {107688},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning directed locomotion in modular robots with evolvable morphologies},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-level classification and modified PSO clustering
based ensemble approach for credit scoring. <em>ASOC</em>, <em>111</em>,
107687. (<a href="https://doi.org/10.1016/j.asoc.2021.107687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit scoring is a statistical technique that guides financial institutions to make informed decisions regarding the extension of loans to customers based on cautious examination of their historical records with the intent of reducing the organization’s operational costs and eliminate potential risks. Irrelevant attributes often degrade the classification accuracy , thus feature selection can help in dealing efficaciously with large datasets. It has been well established based on numerous studies that heterogeneous ensemble-based models have unparalleled performance among several mathematical and Artificial Intelligence-based techniques devised for the issue. This paper proposes a novel approach namely Multi-Level Classification and Cluster based Ensemble (MLCCE) that incorporates the strengths of both feature selection and ensemble-based classification. MLCCE uses the attribute dependency-based feature selection scheme followed by multi-level classification. Finally the model utilizes Particle Swarm Optimization based clustering followed by a weighted combination that corresponds to the performance of the individual classifier in different spatial regions of data. During performance evaluation, MLCCE has shown remarkable results on both the benchmark credit scoring datasets—Australian and German dataset as compared to other ensemble-based methods.},
  archive      = {J_ASOC},
  author       = {Indu Singh and Narendra Kumar and Srinivasa K.G. and Shivam Maini and Umang Ahuja and Siddhant Jain},
  doi          = {10.1016/j.asoc.2021.107687},
  journal      = {Applied Soft Computing},
  pages        = {107687},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-level classification and modified PSO clustering based ensemble approach for credit scoring},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive neural architecture optimization model for
retinal disorder diagnosis on 3D medical images. <em>ASOC</em>,
<em>111</em>, 107686. (<a
href="https://doi.org/10.1016/j.asoc.2021.107686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture design is one of the critical tasks for deep neural models because of the high variety of structure options. This research proposes an adaptive neural architecture optimization (ANAO) model to optimize the convolutional neural network (CNN) structure based on neural blocks, which are collected from the existing state-of-the-art CNNs. An integer programming model is proposed for the optimization process, where the objective is to maximize the designed model accuracy and convergence speed. Constraints are constructed to restrict the CNN design requirements. To enhance the training efficiency of the designed CNN model, a novel objective function is proposed, which considers the accuracy and the training convergence trend. A recurrent neural network is applied to evaluate the performance of the candidate models to boost the efficiency of the optimization process. A heuristic process is proposed to conduct the optimization. The proposed ANAO model is applied for the retinal disorder diagnosis. Eight state-of-the-art CNNs are tested for comparison with the proposed ANAO model from both accuracy and convergence trend perspectives. Experimental results show that the proposed ANAO model can optimize the CNN architecture to adaptively fit a given dataset and achieve quite high-level performance.},
  archive      = {J_ASOC},
  author       = {Haifeng Wang and Daehan Won and Sang Won Yoon},
  doi          = {10.1016/j.asoc.2021.107686},
  journal      = {Applied Soft Computing},
  pages        = {107686},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive neural architecture optimization model for retinal disorder diagnosis on 3D medical images},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective two-echelon location-routing problem for
cash logistics: A metaheuristic approach. <em>ASOC</em>, <em>111</em>,
107685. (<a href="https://doi.org/10.1016/j.asoc.2021.107685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a two-echelon location routing framework for cash-in-transit. In order to mitigate the risk of robbery in cash transportation, a dynamic risk index is considered. The utilized risk function encompasses both the amount of cash carried by a vehicle and the travel time of a route, and it relaxes a pre-defined parameter of risk threshold in the literature. Multiple exact and metaheuristic methodologies are utilized and evaluated on several small to medium-sized instances and a case study. The effectiveness of the proposed methods is evaluated and shown by examining various multi-objective performance measures . The case study is researched in more depth to obtain managerial insights, and the results show that depending on the risk or cost efficiency of the solutions on a Pareto frontier , the risk of traversing longer routes or transporting larger amounts of cash can be determining in locating new bank vaults.},
  archive      = {J_ASOC},
  author       = {Alireza Fallahtafti and Ehsan Ardjmand and William A. Young II and Gary R. Weckman},
  doi          = {10.1016/j.asoc.2021.107685},
  journal      = {Applied Soft Computing},
  pages        = {107685},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective two-echelon location-routing problem for cash logistics: A metaheuristic approach},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective optimized driving strategy of dual-motor EVs
using NSGA-II as a case study and comparison of various intelligent
algorithms. <em>ASOC</em>, <em>111</em>, 107684. (<a
href="https://doi.org/10.1016/j.asoc.2021.107684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The driving strategy of the driver is one of the critical factors which affect the energy consumption of the vehicle. However, it is often overlooked in the researches of energy management . Moreover, there are few studies devoted to formulating specific driving strategies which consider the driving preference of the driver. This study proposes a multi-objective optimized driving strategy for dual-motor electric vehicles based on the intelligent algorithms, which mainly studies the flat road sections, the long uphill road sections, and the vehicle acceleration sections. The acceleration duration, energy consumption, and driving comfort are defined as the optimized objectives. For the economy performance, the optimized driving strategy designs the travel speed trajectory according to the given road information to guide and adjust the driving behavior, so that the vehicle can be driven economically. In the process of studying vehicle multi-objective acceleration strategy, NSGA-II, MOPSO , PESA-II and SPEA2 were adopted to solve the multi-objective optimization problem with three conflicting objectives. And the performance comparison and statistical analysis of the four algorithms were carried out. Finally, a spliced driving cycle was constructed to validate the effectiveness of the driving strategy. The results indicated that the proposed driving strategy has an excellent performance towards driving assistance.},
  archive      = {J_ASOC},
  author       = {Xinyou Lin and Zhili Lin and Shenshen Wei},
  doi          = {10.1016/j.asoc.2021.107684},
  journal      = {Applied Soft Computing},
  pages        = {107684},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimized driving strategy of dual-motor EVs using NSGA-II as a case study and comparison of various intelligent algorithms},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transmission trend of the COVID-19 pandemic predicted by
dendritic neural regression. <em>ASOC</em>, <em>111</em>, 107683. (<a
href="https://doi.org/10.1016/j.asoc.2021.107683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2020, a novel coronavirus disease became a global problem. The disease was called COVID-19, as the first patient was diagnosed in December 2019. The disease spread around the world quickly due to its powerful viral ability. To date, the spread of COVID-19 has been relatively mild in China due to timely control measures. However, in other countries, the pandemic remains severe, and COVID-19 protection and control policies are urgently needed, which has motivated this research. Since the outbreak of the pandemic, many researchers have hoped to identify the mechanism of COVID-19 transmission and predict its spread by using machine learning (ML) methods to supply meaningful reference information to decision-makers in various countries. Since the historical data of COVID-19 is time series data , most researchers have adopted recurrent neural networks (RNNs), which can capture time information, for this problem. However, even with a state-of-the-art RNN, it is still difficult to perfectly capture the temporal information and nonlinear characteristics from the historical data of COVID-19. Therefore, in this study, we develop a novel dendritic neural regression (DNR) method to improve prediction performance. In the DNR, the multiplication operator is used to capture the nonlinear relationships between input feature signals in the dendrite layer. Considering the complex and large landscape of DNR’s weight space, a new scale-free state-of-matter search (SFSMS) algorithm is proposed to optimize the DNR, which combines the state-of-matter search algorithm with a scale-free local search. The SFSMS achieves a better global search ability and thus can effectively reduce the possibility of falling into local minima. In addition, according to Takens’s theorem, phase space reconstruction techniques are used to discover the information hidden in the high-dimensional space of COVID-19 data, which further improves the precision of prediction. The experimental results suggest that the proposed method is more competitive in solving this problem than other prevailing methods.},
  archive      = {J_ASOC},
  author       = {Minhui Dong and Cheng Tang and Junkai Ji and Qiuzhen Lin and Ka-Chun Wong},
  doi          = {10.1016/j.asoc.2021.107683},
  journal      = {Applied Soft Computing},
  pages        = {107683},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transmission trend of the COVID-19 pandemic predicted by dendritic neural regression},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent energy prediction techniques for fog computing
networks. <em>ASOC</em>, <em>111</em>, 107682. (<a
href="https://doi.org/10.1016/j.asoc.2021.107682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy Efficiency is a key concern for future fog-enabled Internet of Things (IoT). Since Fog Nodes (FNs) are energy-constrained devices, task offloading techniques must consider the energy consumption of the FNs to maximize the performance of IoT applications. In this context, accurate energy prediction can enable the development of intelligent energy-aware task offloading techniques. In this paper, we present two energy prediction techniques, the first one is based on the Recursive Least Square (RLS) filter and the second one uses the Artificial Neural Network (ANN). Both techniques use inputs such as the number of tasks and size of the tasks to predict the energy consumption at different fog nodes. Simulation results show that both techniques have a root mean square error of less than 3\%. However, the ANN-based technique shows up to 20\% less root mean square error as compared to the RLS-based technique.},
  archive      = {J_ASOC},
  author       = {Umar Farooq and Muhammad Wasif Shabir and Muhammad Awais Javed and Muhammad Imran},
  doi          = {10.1016/j.asoc.2021.107682},
  journal      = {Applied Soft Computing},
  pages        = {107682},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent energy prediction techniques for fog computing networks},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of safe areas in flood as emergency evacuation
stations using modified particle swarm optimization with local search.
<em>ASOC</em>, <em>111</em>, 107681. (<a
href="https://doi.org/10.1016/j.asoc.2021.107681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency evacuation throughout and after the flood is a crucial task to mitigating more instantaneous impacts, whilst refining social resilience for longer-term recovery. To enhance the evacuation process, the determination and prediction of safe areas before a flood is necessary. Indeed, the safe area or shelter in place could play two roles during the flood; as temporary shelters and as meeting points (station) for gathering before evacuation. This paper aims to determine the safe area according to the spatial and environmental characteristics of the urban extent (accessibility, topography, congestion, and land use). The main contribution is finding safe areas using modified particle swarm optimization (MPSO) with local search (LMPSO). The proposed method recognizes the optimal location of temporary shelters as evacuation stations. It has been implemented in Districts 3, 6, and 7 of Tehran, the capital of Iran. The comparison between the achieved results of MPSO and LMPSO demonstrated that the LMPSO is more efficient than the modified version. Since LMPSO is less sensitive to local minima and converged to minimum cost faster than MPSO, and the distribution of optimum locations of safe areas has been balanced, so all the population could benefit from these stations. The comparison among the results of MPSO, LMPSO, GA , ACO and genetic simulated annealing algorithms justified the efficiency of LMPSO too.},
  archive      = {J_ASOC},
  author       = {Najmeh Neysani Samany and Mahdi Sheybani and Sisi Zlatanova},
  doi          = {10.1016/j.asoc.2021.107681},
  journal      = {Applied Soft Computing},
  pages        = {107681},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detection of safe areas in flood as emergency evacuation stations using modified particle swarm optimization with local search},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data augmentation by guided deep interpolation.
<em>ASOC</em>, <em>111</em>, 107680. (<a
href="https://doi.org/10.1016/j.asoc.2021.107680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art machine learning algorithms require large amount of high quality data. In practice, however, the sample size is commonly low and data is imbalanced along different class labels. Low sample size and imbalanced class distribution can significantly deteriorate the predictive performance of machine learning models. In order to overcome data quality issues, we propose a novel data augmentation method, Guided Deep Interpolation (GDI). It is based on a convolutional auto-encoder network, which is equipped with an auxiliary linear self-expressive layer. The network is trained by minimizing a composite objective function so that to extract the underlying clustered structure of semantic similarities of data points while high reconstruction quality is also preserved. The trained network is used to define a sampling strategy and a synthetic data generation procedure. Making use of the weights of the self-expressive layer, we introduce a measure of semantic variability to quantify how similar a data point to other data points on average. Based on the proposed measure of semantic variability, a joint distribution is defined. Using the distribution we can draw pairs of similar data points so that one point is semantically underrepresented (isolated) while its pair possesses relatively high semantic variability. A sampled pair is interpolated in the deep feature space of the network so that to increase semantic variability while preserve class label of the semantically underrepresented data point. The trained decoder is used to determine pixel space representations of latent space interpolations. The resulting data augmentation procedure generates synthetic samples by increasing the semantic variability of semantically underrepresented instances in a class label preserving way. Our experimental results show that the proposed method outperforms traditional and generative model-based data augmentation methods on low sample size and imbalanced data sets.},
  archive      = {J_ASOC},
  author       = {Gergely Szlobodnyik and Lóránt Farkas},
  doi          = {10.1016/j.asoc.2021.107680},
  journal      = {Applied Soft Computing},
  pages        = {107680},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data augmentation by guided deep interpolation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic fuzzy neighborhood rough set approach for
interval-valued information systems with fuzzy decision. <em>ASOC</em>,
<em>111</em>, 107679. (<a
href="https://doi.org/10.1016/j.asoc.2021.107679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many extended rough set models are proposed to acquire valuable knowledge from interval-valued information system. These existing models mainly focus on different forms of similarity relations. However, most of these similarity relations are qualitative rather than quantitative, which is not reasonable in some practical cases. In addition, with the arrival of new objects and the removal of obsolete objects, the interval-valued information system with fuzzy decision (IvIS_FD) is always changing with time. Therefore, how to efficiently mining knowledge from dynamic IvIS_FD is a meaningful topic. Motivated by these two issues, we study the dynamic fuzzy neighborhood rough set approach for IvIS_FD, aiming to effectively update the rough approximations when the IvIS_FD evolves over time. Firstly, δ δ -fuzzy neighborhood relation is defined to describe the similarity relation between objects quantitatively. Secondly, we introduce a novel fuzzy neighborhood rough set model and its matrix representation suitable for IvIS_FD. On this basis, we discuss the incremental mechanisms to update fuzzy neighborhood approximations when multiple objects are added to or deleted from an IvIS_FD, respectively. Meanwhile, corresponding dynamic algorithms are designed and explained. Finally, experiments are performed on nine public data sets to evaluate the performance of the dynamic fuzzy neighborhood rough set model. Experimental results verify that the proposed model is effective and efficient for updating rough approximations in dynamic IvIS_FD.},
  archive      = {J_ASOC},
  author       = {Lei Yang and Keyun Qin and Binbin Sang and Weihua Xu},
  doi          = {10.1016/j.asoc.2021.107679},
  journal      = {Applied Soft Computing},
  pages        = {107679},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic fuzzy neighborhood rough set approach for interval-valued information systems with fuzzy decision},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differential evolution with mixed mutation strategy based on
deep reinforcement learning. <em>ASOC</em>, <em>111</em>, 107678. (<a
href="https://doi.org/10.1016/j.asoc.2021.107678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of differential evolution (DE) algorithm significantly depends on mutation strategy. However, there are six commonly used mutation strategies in DE. It is difficult to select a reasonable mutation strategy in solving the different real-life optimization problems . In general, the selection of the most appropriate mutation strategy is based on personal experience. To address this problem, a mixed mutation strategy DE algorithm based on deep Q-network (DQN), named DEDQN is proposed in this paper, in which a deep reinforcement learning approach realizes the adaptive selection of mutation strategy in the evolution process. Two steps are needed for the application of DQN to DE. First, the DQN is trained offline through collecting the data about fitness landscape and the benefit (reward) of applying each mutation strategy during multiple runs of DEDQN tackling the training functions. Second, the mutation strategy is predicted by the trained DQN at each generation according to the fitness landscape of every test function. Besides, a historical memory parameter adaptation mechanism is also utilized to improve the DEDQN. The performance of the DEDQN algorithm is evaluated by the CEC2017 benchmark function set, and five state-of-the-art DE algorithms are compared with the DEDQN in the experiments. The experimental results indicate the competitive performance of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Zhiping Tan and Kangshun Li},
  doi          = {10.1016/j.asoc.2021.107678},
  journal      = {Applied Soft Computing},
  pages        = {107678},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution with mixed mutation strategy based on deep reinforcement learning},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPHC: A heuristic clustering method to customer
segmentation. <em>ASOC</em>, <em>111</em>, 107677. (<a
href="https://doi.org/10.1016/j.asoc.2021.107677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer segmentation refers to dividing customer groups into multiple different sub-communities according to customer characteristics. The accurate segmentation of customers is critical for decision-makers to fully understand the customer requirements (CRs) in the market and then design market activities to satisfy customers. In past studies, clustering algorithms have been widely used to solve customer segmentation. However, it is still difficult to divide customers clearly when facing real customer requirement data (CRD). To solve these difficulties, this paper develops a heuristic clustering method for customer segmentation, termed Gaussian Peak Heuristic Clustering (GPHC, for short). Specifically, this paper utilizes the entropy method and standardized Gaussian distribution to filter and model interval CRD. Then, the customer preference pattern hidden in CRD could be recognized by niching genetic algorithm and hierarchical clustering . Finally, the clustering result of CRD will be obtained by the k -means algorithm based on heuristics information from customer preference patterns. Furthermore, customer segmentation can be extracted from the clustering result . A practical case is used to illustrate the effectiveness of GPHC in solving the customer segmentation problem. Experiments show that the customer segmentation result output by our method is consistent with the customer segmentation result given by experts. Besides, the robustness of GPHC in the face of complex customer segmentation scenarios has been verified through numerical experiments.},
  archive      = {J_ASOC},
  author       = {Zhao-Hui Sun and Tian-Yu Zuo and Di Liang and Xinguo Ming and Zhihua Chen and Siqi Qiu},
  doi          = {10.1016/j.asoc.2021.107677},
  journal      = {Applied Soft Computing},
  pages        = {107677},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GPHC: A heuristic clustering method to customer segmentation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discrete spider monkey optimization for the vehicle
routing problem with stochastic demands. <em>ASOC</em>, <em>111</em>,
107676. (<a href="https://doi.org/10.1016/j.asoc.2021.107676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vehicle Routing Problem (VRP) is a classical NP-hard combinatorial optimization problem . In recent years, a lot of heuristic algorithms have been proposed for optimizing the problem, and many simulation and practical experiments are performed to evaluate and verify the effectiveness of different heuristic algorithms . In this paper, we focus on the Vehicle Routing Problem with stochastic demands (VRPSD) in which the customer demands follow known probability distributions. A hybrid algorithm called DSMO-GA which combines discrete spider monkey algorithm (SMO) with genetic algorithm (GA) is proposed for solving the VRPSD problem. In the proposed DSMO-GA for VRPSD, the individuals are coded as sets and sequences, and a conflict elimination method is designed to cancel the infeasible codes. Accordingly, a non adjacent 2-OPT is designed to enhance population diversity. The proposed DSMO-GA effectively integrates the advantages of DSMO and GA to further balance the global exploration and local exploitation capacity. Five groups of different experiments are conducted and the results show that DSMO-GA is valid for the VRPSD. Furthermore, the impact of various parameters on the performance of DSMO-GA are also investigated.},
  archive      = {J_ASOC},
  author       = {Xiaoyun Xia and Weizhi Liao and Yu Zhang and Xue Peng},
  doi          = {10.1016/j.asoc.2021.107676},
  journal      = {Applied Soft Computing},
  pages        = {107676},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete spider monkey optimization for the vehicle routing problem with stochastic demands},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An oppositional-cauchy based GSK evolutionary algorithm with
a novel deep ensemble reinforcement learning strategy for COVID-19
diagnosis. <em>ASOC</em>, <em>111</em>, 107675. (<a
href="https://doi.org/10.1016/j.asoc.2021.107675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel coronavirus (COVID-19) has globally attracted attention as a severe respiratory condition. The epidemic has been first tracked in Wuhan, China, and has progressively been expanded in the entire world. The growing expansion of COVID-19 around the globe has made X-ray images crucial for accelerated diagnostics. Therefore, an effective computerized system must be established as a matter of urgency, to facilitate health care professionals in recognizing X-ray images from COVID-19 patients. In this work, we design a novel artificial intelligent-based automated X-ray image analysis framework based on an ensemble of deep optimized convolutional neural networks (CNNs) in order to distinguish coronavirus patients from non-patients. By developing a modified version of gaining–sharing knowledge (GSK) optimization algorithm using the Opposition-based learning (OBL) and Cauchy mutation operators , the architectures of the deployed deep CNNs are optimized automatically without performing the general trial and error procedures. After obtaining the optimized CNNs, it is also very critical to identify how to decrease the number of ensemble deep CNN classifiers to ensure the classification effectiveness. To this end, a selective ensemble approach is proposed for COVID-19 X-ray based image classification using a deep Q network that combines reinforcement learning (RL) with the optimized CNNs. This approach increases the model performance in particular and therefore decreases the ensemble size of classifiers. The experimental results show that the proposed deep RL optimized ensemble approach has an excellent performance over two popular X-ray image based COVID-19 datasets. Our proposed advanced algorithm can accurately identify the COVID-19 patients from the normal individuals with a significant accuracy of 0.991441, precision of 0.993568, recall (sensitivity) of 0.981445, F-measure of 0.989666 and AUC of 0.990337 for Kaggle dataset as well as an excellent accuracy of 0.987742, precision of 0.984334, recall (sensitivity) of 0.989123, F-measure of 0.984939 and AUC of 0.988466 for Mendely dataset.},
  archive      = {J_ASOC},
  author       = {Seyed Mohammad Jafar Jalali and Milad Ahmadian and Sajad Ahmadian and Abbas Khosravi and Mamoun Alazab and Saeid Nahavandi},
  doi          = {10.1016/j.asoc.2021.107675},
  journal      = {Applied Soft Computing},
  pages        = {107675},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An oppositional-cauchy based GSK evolutionary algorithm with a novel deep ensemble reinforcement learning strategy for COVID-19 diagnosis},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Usage of intelligent medical aided diagnosis system under
the deep convolutional neural network in lumbar disc herniation.
<em>ASOC</em>, <em>111</em>, 107674. (<a
href="https://doi.org/10.1016/j.asoc.2021.107674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the diagnosis efficiency of lumbar disc herniation (LDH) and reduce the impact of manual intervention in the traditional computer-aided diagnosis (CAD) system, the magnetic resonance imaging (MRI) images of LDH patients were selected as the research objects in this study. Firstly, the conditional deep convolutional generative adversarial network (CDCGAN) was constructed, and the influence of the improved T-ReLu activation function on the classification effect of the model was analyzed comparatively. Secondly, the model was applied to classification of MRI images of LDH patients to analyzed comparatively the effects of different parameters on the classification accuracy . Then, an aided diagnosis system of LDH covering the MRI feature extraction, 3D modeling, and CDCGAN model classification was built, and the model was tested with the real data. Finally, the Spearman correlation was adopted to analyze the correlation between the MRI quantitative indexes based on the aided diagnosis system of LDH and the course of LDH. It was found that the classification accuracy of the CDCGAN model with improved activation function on the Cifar-100 data set was 96.07\%, and the classification accuracy of MRI images of LDH patients was 94.41\% when the parameter N was 85. After the cross-validation, it was found that the diagnostic accuracy of the aided diagnosis system of LDH constructed in this study was 94.15\% on LDH diseases. In addition, it was found that the Kyphotic angle of the herniated dise value and the relative signal intensity value in the MRI quantitative indicators showed a very significant negative correlation with the prevalence of LDH ( P &lt; 0.01), while the index of disc herniation, nucleus protrusion rate, ratio of horizontal deviation angle, and the ratio between the protruded part and the dural sac showed extremely positive correlations with the course of LDH ( P &lt; 0.01). It suggested that applying the CDCGAN model to the aided diagnosis system of LDH based on the MRI quantitative indicators could improve the accuracy of diagnosis of LDH.},
  archive      = {J_ASOC},
  author       = {Gang Chen and Zhengkuan Xu},
  doi          = {10.1016/j.asoc.2021.107674},
  journal      = {Applied Soft Computing},
  pages        = {107674},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Usage of intelligent medical aided diagnosis system under the deep convolutional neural network in lumbar disc herniation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparing seven methods for state-of-health time series
prediction for the lithium-ion battery packs of forklifts.
<em>ASOC</em>, <em>111</em>, 107670. (<a
href="https://doi.org/10.1016/j.asoc.2021.107670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key aspect for the forklifts is the state-of-health (SoH) assessment to ensure the safety and the reliability of uninterrupted power source. Forecasting the battery SoH well is imperative to enable preventive maintenance and hence to reduce the costs. This paper demonstrates the capabilities of gradient boosting regression for predicting the SoH timeseries under circumstances when there is little prior information available about the batteries. We compared the gradient boosting method with light gradient boosting, extra trees, extreme gradient boosting, random forests , long short-term memory networks and with combined convolutional neural network and long short-term memory networks methods. We used multiple predictors and lagged target signal decomposition results as additional predictors and compared the yielded prediction results with different sets of predictors for each method. For this work, we are in possession of a unique data set of 45 lithium-ion battery packs with large variation in the data. The best model that we derived was validated by a novel walk-forward algorithm that also calculates point-wise confidence intervals for the predictions; we yielded reasonable predictions and confidence intervals for the predictions. Furthermore, we verified this model against five other lithium-ion battery packs; the best model generalised to greater extent to this set of battery packs. The results about the final model suggest that we were able to enhance the results in respect to previously developed models. Moreover, we further validated the model for extracting cycle counts presented in our previous work with data from new forklifts; their battery packs completed around 3000 cycles in a 10-year service period, which corresponds to the cycle life for commercial Nickel–Cobalt–Manganese (NMC) cells.},
  archive      = {J_ASOC},
  author       = {Matti Huotari and Shashank Arora and Avleen Malhi and Kary Främling},
  doi          = {10.1016/j.asoc.2021.107670},
  journal      = {Applied Soft Computing},
  pages        = {107670},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comparing seven methods for state-of-health time series prediction for the lithium-ion battery packs of forklifts},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new composite approach for COVID-19 detection in x-ray
images using deep features. <em>ASOC</em>, <em>111</em>, 107669. (<a
href="https://doi.org/10.1016/j.asoc.2021.107669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new type of coronavirus, COVID 19, appeared in China at the end of 2019. It has become a pandemic that is spreading all over the world in a very short time. The detection of this disease, which has serious health and socio-economic damages, is of vital importance. COVID-19 detection is performed by applying PCR and serological tests. Additionally, COVID detection is possible using X-ray and computed tomography images. Disease detection has an important position in scientific researches that includes artificial intelligence methods. The combined models, which consist of different phases, are frequently used for classification problems. In this paper, a new combined approach is proposed to detect COVID-19 cases using deep features obtained from X-ray images. Two main variances of the approach can be presented as single layer-based (SLB) and feature fusion-based (FFB). SLB model consists of pre-processing, deep feature extraction, post-processing, and classification phases . On the other side, the FFB model consists of pre-processing, deep feature extraction, feature fusion , post-processing, and classification phases. Four different SLB and six different FFB models were developed according to the number and binary combination of layers used in the feature extraction phase. Each model is employed for binary and multi-class classification experiments. According to experimental results, the accuracy performance for COVID-19 and no-findings classification of the proposed FFB3 model is 99.52\%, which is better than the best performance accuracy (of 98.08\%) in the literature. Concurrently, for multi-class classification, the proposed FFB3 model has an accuracy performance of 87.64\% outperforming the best existing work (which reported an 87.02\% classification performance). Various metrics, including sensitivity, specificity, precision, and F1-score metrics are used for performance analysis. For all performance metrics, the FFB3 model recorded a higher success rate than existing work in the literature. To the best of our knowledge, these accuracy rates are the best in the literature for the dataset and data split type (five-fold cross-validation). Composite models (SLBs and FFBs), which are generated in this paper, are successful ways to detect COVID-19. Experimental results show that feature extraction, pre-processing, post-processing, and hyperparameter tuning are the steps are necessary to obtain a higher success. For prospective works, different types of pre-trained models and other hyperparameter tuning methods can be implemented.},
  archive      = {J_ASOC},
  author       = {Tayyip Ozcan},
  doi          = {10.1016/j.asoc.2021.107669},
  journal      = {Applied Soft Computing},
  pages        = {107669},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new composite approach for COVID-19 detection in X-ray images using deep features},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A predictive intelligence system of credit scoring based on
deep multiple kernel learning. <em>ASOC</em>, <em>111</em>, 107668. (<a
href="https://doi.org/10.1016/j.asoc.2021.107668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Banks face the task of improving the accuracy in predicting the behavior of individuals who utilize credit cards, as issuing cards to an appropriate applicant is considered an important matter. Credit card debt that is overdue by at least six months has seen a rapid increase in the past decade in the Chinese credit card industry. The problem of delinquency in credit cards not only affects the development of credit cards but it also influences the sustainability of banks. The use of credit risk assessment is critical in providing a solid foundation upon which the credit card issuer can appropriately approve an applicant for a credit card. In previous studies, a variety of machine learning methods have been proposed to assess credit risk. However, conventional methods are viewed as shallow models and are not good at representing compositional features. Thus, this study applies a deep multiple kernel classifier as a state-of-the-art technique, which is proficient in coping with deep structure and complex data in credit risk assessment. It will support decision-makers issuing credit cards in China appropriately. The results indicate that deep multiple kernel classifier outperforms conventional and ensemble models. Credit card departments with better risk management can avoid possible bad debt, hence benefiting banks’ operations. The applications of predictive intelligence enhance the prediction of human behavior in the credit card industry.},
  archive      = {J_ASOC},
  author       = {Cheng-Feng Wu and Shian-Chang Huang and Chei-Chang Chiou and Yu-Min Wang},
  doi          = {10.1016/j.asoc.2021.107668},
  journal      = {Applied Soft Computing},
  pages        = {107668},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A predictive intelligence system of credit scoring based on deep multiple kernel learning},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PERMS: An efficient rescue route planning system in
disasters. <em>ASOC</em>, <em>111</em>, 107667. (<a
href="https://doi.org/10.1016/j.asoc.2021.107667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The occurrence of natural and man-made disasters usually leads to significant social and economic disruption, as well as high numbers of casualties. Such occurrences are difficult to predict due to the huge number of parameters with mutual interdependencies which need to be investigated to provide reliable predictive capabilities . In this work, we present high-Performance Emergent Rescue Management e-System ( PERMS ), an efficient rescue route planning scheme operating within a high-performance emergent rescue management system for vehicles based on the mobile cloud computing paradigm. More specifically, an emergency rescue planning problem (ERRP) is investigated as a multiple travelling salesman problem (MTSP), as well as a novel phased heuristic rescue route planning scheme. This consists of an obstacle constraints and task of equal division-based K-means++ clustering algorithm (OT-K-means++), which is more suitable for clustering victims in disaster environments, and a glow-worm swarm optimisation algorithm based on chaotic initialisation (GSOCI), which provides the appropriate rescue route for each vehicle. A prototype is developed to evaluate the performance of this proposed approach, which demonstrates a better performance compared to other well-known and widely used algorithms. As demonstrated by the validation process, our approach enhances the accuracy and convergence speed for solving the emergency rescue planning problem. Furthermore, it shortens the length of the rescue route, as well as rescue time, and it leads to reasonable and balanced allocation of emergency rescue tasks, whilst achieving an overall efficient rescue process. More specifically, by considering scenarios with 200 victims, compared with K-means and K-means++, OT-K-means++ reduces the time cost of clustering by 9.52\% and 17.39\% respectively, and reduces the number of iterations by 11.11\% and 15.78\% respectively. Compared with ACO or GA , GSOCI reduces the length of rescue route by 9.81\% and 16.36\% respectively, and reduces the time of rescue by 4.35\% and 15.38\% respectively.},
  archive      = {J_ASOC},
  author       = {Xiaolong Xu and Lei Zhang and Marcello Trovati and Francesco Palmieri and Eleana Asimakopoulou and Olayinka Johnny and Nik Bessis},
  doi          = {10.1016/j.asoc.2021.107667},
  journal      = {Applied Soft Computing},
  pages        = {107667},
  shortjournal = {Appl. Soft. Comput.},
  title        = {PERMS: An efficient rescue route planning system in disasters},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end predictive intelligence diagnosis in brain tumor
using lightweight neural network. <em>ASOC</em>, <em>111</em>, 107666.
(<a href="https://doi.org/10.1016/j.asoc.2021.107666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel brain tumor end-to-end detection approach based on predictive intelligence using lightweight neural network is presented in practical application in medical data center in hospital and can be beneficial for smart healthcare. While medical intelligence diagnosis has received much attention from academia, little effort has been made in predictive brain tumor intelligence diagnosis and the issue of practical use have been largely overlooked. The technique we applied is referred to as deep learning and predictive intelligence. Firstly, a novel end-to-end brain tumor detection based on lightweight neural network instead of common neural network is utilized in our network to realize the trade-off between the accuracy and efficiency. Secondly, in addition to predictive intelligence, edge intelligence is also adopted in our architecture to make it more easily deployed and dealt with data processing in medical data center in hospital in practical application and balance a lot of resources. Several sets of experiments have carried out to test the validity of brain tumor intelligence diagnosis and it has demonstrated that it is promising and similar to the state-of-the art. The research has resulted in a solution of medical predictive intelligence diagnosis and it proves to be encouraging. It has contributed to our present understanding of practical application of intelligence diagnosis on the pioneer work.},
  archive      = {J_ASOC},
  author       = {Linjuan Ma and Fuquan Zhang},
  doi          = {10.1016/j.asoc.2021.107666},
  journal      = {Applied Soft Computing},
  pages        = {107666},
  shortjournal = {Appl. Soft. Comput.},
  title        = {End-to-end predictive intelligence diagnosis in brain tumor using lightweight neural network},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An interval type-2 fuzzy kano-prospect-TOPSIS based QFD
model: Application to chinese e-commerce service design. <em>ASOC</em>,
<em>111</em>, 107665. (<a
href="https://doi.org/10.1016/j.asoc.2021.107665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the rapidly changing customer preferences and demands, the e-commerce industry encounters various uncertainties and risks to enhance competitiveness. Quality function deployment (QFD) is a commonly used model, which can translate customer requirements (CRs) into products or service design requirements (DRs), to improve competitiveness by launching a new business. To measure the uncertainties and behavioral risk factors in e-commerce service design, we propose a new QFD model based on the Kano model and TOPSIS method by considering the behavior of experts with prospect theory under interval type-2 fuzzy linguistic environment. The categories of CRs are identified using the Kano model and the weights of CRs are determined dynamically according to the development stages of enterprises. The priorities of DRs are ranked using the extended TOPSIS method with prospect theory. A case study of China’s e-commerce service design is used to show the application of the proposed QFD model. The prioritizing results show the flexibility of the proposed model on determining the weights of CRs and the priorities of DRs for enterprises at different development stages.},
  archive      = {J_ASOC},
  author       = {Tong Wu and Xinwang Liu and Jindong Qin and Francisco Herrera},
  doi          = {10.1016/j.asoc.2021.107665},
  journal      = {Applied Soft Computing},
  pages        = {107665},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interval type-2 fuzzy kano-prospect-TOPSIS based QFD model: Application to chinese e-commerce service design},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential convolutional neural networks for classification
of cognitive tasks from EEG signals. <em>ASOC</em>, <em>111</em>,
107664. (<a href="https://doi.org/10.1016/j.asoc.2021.107664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive abilities encompass all aspects of mental functioning ranging from simple to complex tasks. These skills have a tremendous effect on our day-to-day routine. The electroencephalogram (EEG) is a powerful tool to analyze brain activities while performing different cognitive tasks. In this paper, we consider four cognitive tasks (Symbol digit modality test, Stroop test, Benton’s visual retention test, and Hopkins verbal learning test) along with a baseline task, carried out by healthy subjects and record their EEG. We perform phase–amplitude coupling to extract the features of classification, and segregate them into the tasks, through deep learning algorithms. The Sequential Convolutional Network (SCN) is designed to classify these features. Multi-branch Convolutional Network (MBCN) is also proposed, which is inspired by the ResNeXt architecture and the inception module. The performance of the proposed model is evaluated using the metrics such as accuracy, F1-score, precision, and specificity using the EEG signals collected from the PAC dataset and real-time recording. The performance evaluation reveals that MBCN outperforms SCN by achieving higher accuracy, F1-score, precision, and sensitivity of 88.33\%, 87.9\%, 89.18\%, and 88.23\% respectively. Also, the computational complexity of the MBCN architecture is found to be less than the SCN model. Evaluation results show that the proposed MBCN model outperforms the traditional methods.},
  archive      = {J_ASOC},
  author       = {Suchetha M. and Madhumitha R. and Sorna Meena M. and Sruthi R.},
  doi          = {10.1016/j.asoc.2021.107664},
  journal      = {Applied Soft Computing},
  pages        = {107664},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sequential convolutional neural networks for classification of cognitive tasks from EEG signals},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bus arrival time prediction and reliability analysis: An
experimental comparison of functional data analysis and bayesian support
vector regression. <em>ASOC</em>, <em>111</em>, 107663. (<a
href="https://doi.org/10.1016/j.asoc.2021.107663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To maintain the stability and punctuality of bus systems, an accurate forecast of arrival time is essential to devise control strategies to prevent bus bunching especially under congested traffic conditions. Transit agencies provide travelers with accurate and reliable bus arrival times to downstream stations to improve transit service quality so as to attract more transit riders. Varieties of approaches have been dedicated to providing high prediction accuracy while the measure of the associated uncertainty is ignored. Noting that the quantification of uncertainty is vital for robust performance, this paper proposes data-driven approaches based on the Functional Data Analysis (FDA) and Bayesian Support Vector Regression (BSVR) for short-term bus travel time prediction while anticipating various uncertainties. To capture spatial–temporal dynamic traffic conditions along the route so as to increase the accuracy of the journey time prediction and to capture the skewness in journey time distribution, a probabilistic nested delay operator is adopted. Journey time reliability analysis is then conducted using the skewness of dynamic journey time distribution. An empirical study is carried out by fusing the bus transit date of No. 261 bus route and Floating Car Data (FCD) in Guangzhou. The proposed FDA and BSVR methods applied in conjunction with the probabilistic nested delay operator turn out to be highly competitive when performing forecasts under various traffic conditions. Comparative studies indicate that FDA provides more accurate prediction results and tends to anticipate uncertainties in journey time distribution more effectively.},
  archive      = {J_ASOC},
  author       = {Y.P. Huang and C. Chen and Z.C. Su and T.S. Chen and A. Sumalee and T.L. Pan and R.X. Zhong},
  doi          = {10.1016/j.asoc.2021.107663},
  journal      = {Applied Soft Computing},
  pages        = {107663},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bus arrival time prediction and reliability analysis: An experimental comparison of functional data analysis and bayesian support vector regression},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-attribute group decision model based on unbalanced
and multi-granular linguistic information: An application to assess
entrepreneurial competencies in secondary schools. <em>ASOC</em>,
<em>111</em>, 107662. (<a
href="https://doi.org/10.1016/j.asoc.2021.107662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in multi-attribute group decision making require the development of structures flexible enough to deal with unbalanced and multi-granular linguistic information. New distances between linguistic terms are needed to aggregate opinions and measure consensus among decision makers with different profiles. In this paper, firstly, based on the lattice structure of hesitant fuzzy linguistic terms sets, a perceptual-based distance able to capture differences between unbalanced linguistic assessments is developed. Secondly, a projected algebraic structure is defined to deal with multi-perceptual group decision-making contexts where each decision maker has its own qualitative reasoning approach. To this end, a methodology to aggregate unbalanced linguistic information based on different perceptual maps is developed. This methodology can also deal with different multi-granularity linguistic environments. Finally, through an illustrative example based on real data provided by the Andorra Government in a pilot test, the proposed framework is applied to classify and rank a set of secondary students according to their degree of entrepreneurial competency.},
  archive      = {J_ASOC},
  author       = {Olga Porro and Núria Agell and Mónica Sánchez and Francisco Javier Ruiz},
  doi          = {10.1016/j.asoc.2021.107662},
  journal      = {Applied Soft Computing},
  pages        = {107662},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-attribute group decision model based on unbalanced and multi-granular linguistic information: An application to assess entrepreneurial competencies in secondary schools},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of TODIM with different types of fuzzy sets: A
state-of the-art survey. <em>ASOC</em>, <em>111</em>, 107661. (<a
href="https://doi.org/10.1016/j.asoc.2021.107661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-criteria decision making (MCDM) is a common method used to solve complex decision-making problems. One such method, TODIM (TOmada de Decisão Iterativa Multicritério), is derived from prospect theory, which considers the psychological behaviors of decision makers (DMs). The perceptions of DMs of the alternatives may be uncertain because, for example, of complex decision-making circumstances or their limited knowledge. Therefore, fuzzy sets (FSs) have been used to describe DMs’ vague perceptions. The combination of TODIM with different types of FSs has developed to deal with different uncertain decision-making problems. We systematically analyze TODIM with different types of FSs to show its state and possible future direction. A bibliometric analysis of existing research on this topic is given, followed by an analysis of the dominance function of TODIM, including how to represent a gain or loss, and how to obtain the original and relative weights. We then describe the combination of TODIM with other methods. Applications of current methods are summarized, and some challenges and possible directions of future research are provided. Fuzzy MCDM with the psychological factors of decision makers will surely receive increased attention from researchers and practitioners in the future.},
  archive      = {J_ASOC},
  author       = {Xiaoli Tian and Wanqing Li and Li Liu and Gang Kou},
  doi          = {10.1016/j.asoc.2021.107661},
  journal      = {Applied Soft Computing},
  pages        = {107661},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of TODIM with different types of fuzzy sets: A state-of the-art survey},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal foraging algorithm with direction prediction.
<em>ASOC</em>, <em>111</em>, 107660. (<a
href="https://doi.org/10.1016/j.asoc.2021.107660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal foraging algorithm (OFA) was presented as a stochastic search algorithm to solve global optimization problems in 2017. As an emerging algorithm, there are many excellent potentials to be explored. To enhance the performance of OFA, a novel optimal foraging algorithm with direction prediction is presented in this paper, named OFA/DP. During the iterations, the population information can be fully utilized by OFA/DP. Once a new optimal solution is found, the evolutionary direction prediction strategy is applied to generate more potential candidates. In addition, considering the situation that the population does not evolve for a long time, which means that the algorithm has achieved the global optima or trapped into local optima. In this case, the Gaussian oscillation strategy is adopted to attempt to find a better solution, and escaping from local optima. To validate the efficiency of the proposed algorithm, the numerical experiments on 20 benchmark functions , 30 CEC test functions, 6 large scale functions, 28 CEC2017 constrained problems, 3 engineering problems, 6 unconstrained multi-objective functions and 10 constrained multi-objective functions are executed. The simulation results and the statistical test demonstrate that OFA/DP has a superior performance in most of functions with faster convergence speed.},
  archive      = {J_ASOC},
  author       = {ZhongQuan Jian and GuangYu Zhu},
  doi          = {10.1016/j.asoc.2021.107660},
  journal      = {Applied Soft Computing},
  pages        = {107660},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal foraging algorithm with direction prediction},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving the regenerator location problem with an iterated
greedy approach. <em>ASOC</em>, <em>111</em>, 107659. (<a
href="https://doi.org/10.1016/j.asoc.2021.107659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of digital communications has resulted in new services that require from secure and robust connections. Nowadays, a signal must be transmitted to distant nodes, and the quality of the signal deteriorates as the distance between the endpoints increases. Regenerators are special components that are able to restore the signal, in order to increase the distance that the signal can travel without losing quality. These special components are very expensive to deploy and maintain and, for this reason, it is desirable to deploy the minimum number of regenerators in a network. We propose a metaheuristic algorithm based on the Iterated Greedy methodology to tackle the Regenerator Location Problem, whose objective is to minimize the number of regenerators required in a network. The extensive computational experiments show the performance of the proposed method compared with the best previous algorithm found in the state of the art.},
  archive      = {J_ASOC},
  author       = {Juan David Quintana and Raul Martin-Santamaria and Jesus Sanchez-Oro and Abraham Duarte},
  doi          = {10.1016/j.asoc.2021.107659},
  journal      = {Applied Soft Computing},
  pages        = {107659},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving the regenerator location problem with an iterated greedy approach},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using z-number to measure the reliability of new information
fusion method and its application in pattern recognition. <em>ASOC</em>,
<em>111</em>, 107658. (<a
href="https://doi.org/10.1016/j.asoc.2021.107658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information fusion has traditionally been a concern. In the fusion process, how to effectively take care of the ambiguity and uncertainty of data is a fascinating problem. Dempster–Shafer evidence theory shows powerful functions in dealing with uncertainty information, and Z-number can comprehensively model the ambiguity and reliability of information. Inspired by this, this paper proposed a new information fusion method based on Dempster–Shafer theory and K-means clustering and it established the reliability evaluation criterion based on Z-number. Comparison and discussion verify the rationality of the proposed method, which also illustrates the method has better robustness and sensitivity than existing methods, some critical issues in DST, e.g., conflict management, evidence stuck, are well investigated and overcome by the proposed method. Number examples and the application further shows the application potential of the proposed method in a data-driven intelligent system.},
  archive      = {J_ASOC},
  author       = {Ye Tian and Xiangjun Mi and Huizi Cui and Pengdan Zhang and Bingyi Kang},
  doi          = {10.1016/j.asoc.2021.107658},
  journal      = {Applied Soft Computing},
  pages        = {107658},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using Z-number to measure the reliability of new information fusion method and its application in pattern recognition},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-criteria food waste treatment method selection using
single-valued neutrosophic-CRITIC-MULTIMOORA framework. <em>ASOC</em>,
<em>111</em>, 107657. (<a
href="https://doi.org/10.1016/j.asoc.2021.107657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proper management and treatment of food waste have become a key concern due to its significant environmental, social, and economic ramifications. The selection of the most appropriate food waste treatment method among a set of alternative methods can be regarded as a multi-criteria decision-making problem because of the association of numerous qualitative and quantitative attributes. In this paper, we offer a novel integrated framework by combining criteria interaction through inter-criteria correlation (CRITIC) and multi-objective optimization based on ratio analysis with the full multiplicative form (MULTIMOORA) methods with single-valued neutrosophic sets (SVNSs) for assessing the multi-criteria food waste treatment methods selection. In this methodology, the CRITIC technique is applied for computing the attribute weights, and the MULTIMOORA model is employed for estimating the ranking of the options within SVNSs context. To examine the introduced methodology’s efficiency and achievability , a case study of food waste treatment method (FWTM) assessment is discussed in the SVNSs setting. Further, comparative study and sensitivity investigation are offered to certify the presented framework for prioritizing FWTMs. The final results indicate that the proposed approach achieves better solutions than the extant models.},
  archive      = {J_ASOC},
  author       = {Pratibha Rani and Arunodaya Raj Mishra and R. Krishankumar and K.S. Ravichandran and Samarjit Kar},
  doi          = {10.1016/j.asoc.2021.107657},
  journal      = {Applied Soft Computing},
  pages        = {107657},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria food waste treatment method selection using single-valued neutrosophic-CRITIC-MULTIMOORA framework},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimising the job-shop scheduling problem using a
multi-objective jaya algorithm. <em>ASOC</em>, <em>111</em>, 107654. (<a
href="https://doi.org/10.1016/j.asoc.2021.107654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an effective multi-objective Jaya (EMOJaya) algorithm to solve a multi-objective job-shop scheduling problem, aiming to simultaneously minimise the makespan, total flow time and mean tardiness. A strategy based on grey entropy parallel analysis (GEPA) is developed to assess and select solutions during the search process. To obtain a high-quality reference sequence for GEPA, an opposition-based learning (OBL) strategy is used in parallel. Additionally, the OBL strategy is incorporated into Jaya’s search operation and external archive to enhance the search ability and convergence rate of the algorithm. Computational experiments based on 30 benchmark instances with different scales confirm that GEPA and OBL can significantly improve the performance of our proposed EMOJaya. Experimental results also show that EMOJaya is able to outperform three state-of-the-art multi-objective algorithms in solving the problem at hand in terms of convergence, diversity and distribution. Further, EMOJaya can obtain more high-quality scheduling schemes, which provide more and better options for decision makers .},
  archive      = {J_ASOC},
  author       = {Lijun He and Wenfeng Li and Raymond Chiong and Mehdi Abedi and Yulian Cao and Yu Zhang},
  doi          = {10.1016/j.asoc.2021.107654},
  journal      = {Applied Soft Computing},
  pages        = {107654},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimising the job-shop scheduling problem using a multi-objective jaya algorithm},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fault diagnosis of rolling bearing based on laplacian
regularization. <em>ASOC</em>, <em>111</em>, 107651. (<a
href="https://doi.org/10.1016/j.asoc.2021.107651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to design a reasonable classification method to identify the states is a critical step in rolling bearing fault diagnosis. Along with labeled samples, the Laplacian regularization (LapR) classification method, a graph-based semi-supervised learning algorithm, can also exploit the wealth of numerous cheap unlabeled samples to obtain acceptable performance. Based on the LapR classification method, a novel fault diagnosis method of rolling bearing is put forward. First, the vibration datasets or feature datasets are constructed into an undirected and weighted K-nearest neighbor graph, which can fully reflect the similarity between dataset elements. Then, the labels of all the dataset elements are interpreted as graph signals which are indexed by the represented graph’s vertices. Finally, when the constraint given by the known dataset elements is satisfied, the labels of the unknown dataset elements can be determined by finding a graph signal with minimal total variation. Experimental results reveal that whether the vibration datasets or feature datasets are analyzed, the LapR classification method is obviously superior to the popularly known classification methods in identifying the rolling bearing states especially when there are very few known samples.},
  archive      = {J_ASOC},
  author       = {Yiyuan Gao and Dejie Yu},
  doi          = {10.1016/j.asoc.2021.107651},
  journal      = {Applied Soft Computing},
  pages        = {107651},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault diagnosis of rolling bearing based on laplacian regularization},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AComNN: Attention enhanced compound neural network for
financial time-series forecasting with cross-regional features.
<em>ASOC</em>, <em>111</em>, 107649. (<a
href="https://doi.org/10.1016/j.asoc.2021.107649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many works spring out to adopt the forecast-based approach to support the investment decision in the financial market. Nevertheless, most of them do not consider mining the hidden patterns in the cross-regional financial time-series. However, the fluctuation in financial markets has always been affected by the global economy, instead of a single market. To overcome this issue, this article proposes an Attention enhanced Compound Neural Network (AComNN) that can be applied on features of multiple-sources, including different financial markets and economic entities. The proposed novel approach compounds of Artificial Neural Network (ANN), Long Short-Term Memory (LSTM), and self-attention to progressively capture the time-zone-dependent context behind the financial time-series across regions with multiple filters. Thereby, it provides trading signals for supporting the financial investment decision. The proposed AComNN has been applied on the Hong Kong Hang Seng Index (HSI) trend prediction based on various initial features across regions. The experimental result demonstrates that the AComNN achieves the highest average accuracy for the one-day ahead trend prediction over 60\%. Besides, it reveals highly superior competitiveness on the forecasting capability improved by 13.36\% on average compared with the baselines. Therefore, we encourage to adopt the proposed method to the practitioners and provide a new thought, considering the analysis of cross-regional features, in the financial time-series forecasting.},
  archive      = {J_ASOC},
  author       = {Zhen Yang and Jacky Keung and Md Alamgir Kabir and Xiao Yu and Yutian Tang and Miao Zhang and Shuo Feng},
  doi          = {10.1016/j.asoc.2021.107649},
  journal      = {Applied Soft Computing},
  pages        = {107649},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AComNN: Attention enhanced compound neural network for financial time-series forecasting with cross-regional features},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification and prioritization of strategies to tackle
COVID-19 outbreak: A group-BWM based MCDM approach. <em>ASOC</em>,
<em>111</em>, 107642. (<a
href="https://doi.org/10.1016/j.asoc.2021.107642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world is reeling in the midst of the novel coronavirus pandemic with fear of rising toll due to the deadly virus. Decision making during a pandemic outbreak has numerous challenges. Covid19 has become a challenging problem for organizations, countries and the world at large. It is even more complicated when governments and medical care communities are changing their priorities based on the growing challenges and level of effectiveness of measures taken in other countries. In this study, a potential application of a well-known MCDM method called the Group Best–Worst Method is presented to overcome such challenges and draw the strategies to handle COVID19 outbreak. The methodology is applied to rank the 10 identified strategies based on their relative importance provided by multiple groups of stakeholder. These strategies focus on social distancing, medical care, essential commodities, financial support to poor people, public awareness, overall impact of COVID19, digital surveillance of infected or doubtful people, maintaining the economy of the country, and an effect on industries. Furthermore, the local and global weights along with ranking order of strategies are obtained. A sensitivity analysis has also been done to show the change in global weights and ranking order of strategies.},
  archive      = {J_ASOC},
  author       = {Naeem Ahmad and Md. Gulzarul Hasan and Rejaul Karim Barbhuiya},
  doi          = {10.1016/j.asoc.2021.107642},
  journal      = {Applied Soft Computing},
  pages        = {107642},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identification and prioritization of strategies to tackle COVID-19 outbreak: A group-BWM based MCDM approach},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Particle swarm optimization algorithm using complex-order
derivative concept: A comprehensive study. <em>ASOC</em>, <em>111</em>,
107641. (<a href="https://doi.org/10.1016/j.asoc.2021.107641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive study of the Particle Swarm Optimization (PSO) algorithm, called complex-order PSO (CPSO). In the core of new set of algorithms, we employ the complex-order derivative and the conjugate order differential concepts in the position and velocity adaption mechanisms. To determine the influence of the control parameters on the quality of the results, a sensitivity analysis is conducted. A number of value- and rank-based tests assesses the algorithms’ performance. For a suite of benchmark functions , the standard deviation and the mean best of the results are reported. Additionally, the Friedman test specifies the average ranking from the obtained results. The effect of the complex-order operation and the population size are analyzed using the Taguchi test. An application example illustrates the performance of the CPSO.},
  archive      = {J_ASOC},
  author       = {Seyed Mehdi Abedi Pahnehkolaei and Alireza Alfi and J.A. Tenreiro Machado},
  doi          = {10.1016/j.asoc.2021.107641},
  journal      = {Applied Soft Computing},
  pages        = {107641},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Particle swarm optimization algorithm using complex-order derivative concept: A comprehensive study},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A matheuristic for making order acceptance decisions in
multi-product, multi-stage manufacturing systems. <em>ASOC</em>,
<em>111</em>, 107640. (<a
href="https://doi.org/10.1016/j.asoc.2021.107640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss a planning model with load-dependent lead times for making order acceptance decisions in multi-product, multi-stage manufacturing systems . Semiconductor wafer fabrication facilities (wafer fabs) belong to this class of manufacturing systems. A profit-based objective function is considered. Clearing functions are used in the planning formulation to correctly represent the lead time behavior in the case of a congested system. Order acceptance decisions are made with respect to flexible due dates, i.e., it is possible to reject certain orders if there is not enough capacity. Such acceptance decisions are important, for instance, in short-term demand supply matching algorithms that are crucial for demand fulfillment and available to promise decisions in semiconductor manufacturing . The resulting planning problem is formulated as a mixed integer linear program . We first show that the resulting planning problem is NP-hard. Hence, computationally tractable approaches must be designed. Therefore, variable neighborhood search is hybridized with linear programming to solve large-sized problem instances in reasonable amount of computing time in the present paper. Results of computational experiments for problem instances that are derived from a scaled-down wafer fab simulation model are provided and analyzed. The computational results demonstrate that the proposed matheuristic outperforms time-based decomposition approaches from the literature.},
  archive      = {J_ASOC},
  author       = {Hung-Kai Wang and Lars Mönch},
  doi          = {10.1016/j.asoc.2021.107640},
  journal      = {Applied Soft Computing},
  pages        = {107640},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A matheuristic for making order acceptance decisions in multi-product, multi-stage manufacturing systems},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-stage simulation assisted differential evolution
algorithm for reliable chance constrained programming with minimum risk
level. <em>ASOC</em>, <em>111</em>, 107637. (<a
href="https://doi.org/10.1016/j.asoc.2021.107637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently the literature of Simulation Optimization for solving stochastic constrained optimization problems has been substantially increasing. From the optimization perspective, the population based meta-heuristics algorithms, such as Differential Evolutionary (DE), has shown tremendous success in solving continuous constrained optimization problems . While from the simulation and stochastic perspectives, Monte-Carlo Simulation alongside Chance Constrained Programming (CCP), is one of the most successful combined approaches to handle constraint uncertainty, also called chance constraints. Risk levels are used in CCP to bound the stochastic constraints infeasibility. However, the determination of the risk levels is not precise and is based on subjective personal judgments; due to the lack of a scientific approach to determine their values especially in critical problems that require the minimum possible risk levels. Furthermore, reliability in such problems is a vital requirement, especially for studies under worst-case scenarios. Therefore, we propose a novel approach, called Two-stage simulation assisted Differential Evolutionary for Chance constrained programming algorithm (Two-stage DE-CCP), with two major contributions. The first contribution aims to obtain minimum boundaries for the chance constrains’ risk levels, by utilizing the meta-heuristics DE implicit capabilities with a two stages modification. While the second contribution proposed as a Reliable Chance Constrained Programming (RCCP) with a novel scheme for chance constraints’ feasibility checking, that is called Extreme Scenario Feasibility Checking (ESFC). This name reveals that ESFC aims to obtain more reliable solutions, through comparing the extreme/worst-case scenarios of the chance constraints. The proposed algorithm is applied under normal CCP feasibility checking and ESFC for RCCP, while a comparison between them is performed and an empirical Reliability ratio (Rr) is proposed. The experimental results on benchmark problems and real application show that the proposed algorithm is able to provide decision makers with the minimum possible risk levels. While a substantially high Rr is obtained from the proposed RCCP, as compared to traditional CCP.},
  archive      = {J_ASOC},
  author       = {Amany M. Akl and Ruhul A. Sarker and Daryl L. Essam},
  doi          = {10.1016/j.asoc.2021.107637},
  journal      = {Applied Soft Computing},
  pages        = {107637},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage simulation assisted differential evolution algorithm for reliable chance constrained programming with minimum risk level},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ant colony optimization based light weight binary search for
efficient signature matching to filter ransomware. <em>ASOC</em>,
<em>111</em>, 107635. (<a
href="https://doi.org/10.1016/j.asoc.2021.107635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ransomware is a form of malicious software which when deployed encrypts or locks the files and demands a ransom to have the files decrypted and accessible. In today’s digital world, devices connected to the network are vulnerable to ransomwares. Signature based methods are used to detect known ransomwares at an early stage to minimize the damage. This type of detection involves a predefined repository of static signatures which block the ransomwares based on the signatures in the repository. Fuzzy hashing method and Clam Av method for signature based ransomware detection have been proposed in literature. However, these methods raised false negatives and false positives respectively. Also, the speed of comparison in fuzzy hashing method was a limitation. In this paper an Ant Colony Optimization (ACO) based approach for filtering ransomwares by matching the incoming signature hash value with the signature database is proposed. Termed Ant Colony Optimization based Light weight Binary Search for Signature based Ransomware detection (ACOLBSR) algorithm, the ant agent finds the search space in the signature database. If a search space does not exist in the signature database, the incoming file is logged. If the search space exists in the database, the ant agent employs a binary search to match the incoming signature hash value with the signatures in the search space. If a match is found, the file is accepted or blocked if it is a goodware or ransomware respectively. If the incoming signature does not match with the signatures in the search space, the file is logged. The advantage of ACOLBSR algorithm is that the ransomwares are strictly filtered based on the signatures in the database. The computational complexity of ACOLBSR algorithm is less compared to binary search and sequential search. Also, it is shown that ACOLBSR algorithm do not generate false positives when compared to ClamAV, SplitScreen, fuzzy hashing and classification based approaches. Experimental results comparing the performance of the ACOLBSR scheme with the binary search scheme, sequential search scheme and classification based approaches are presented.},
  archive      = {J_ASOC},
  author       = {Sreelaja N.K.},
  doi          = {10.1016/j.asoc.2021.107635},
  journal      = {Applied Soft Computing},
  pages        = {107635},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ant colony optimization based light weight binary search for efficient signature matching to filter ransomware},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face inpainting based on GAN by facial prediction and fusion
as guidance information. <em>ASOC</em>, <em>111</em>, 107626. (<a
href="https://doi.org/10.1016/j.asoc.2021.107626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face inpainting, a special case of image inpainting , aims to complete the occluded facial regions with unconstrained pose and orientation. However, existing methods generate unsatisfying results with easily detectable flaws. There are often fuzzy boundaries and details near the holes. Especially, for face inpainting, the face region semantic information (face structure, contour, and content information) has not been fully utilized, which leads to unnatural face images, such as asymmetry eyebrow and different sizes of eyes. This is unrealistic in many practical applications. To solve the problems, a new generative adversarial network by facial prediction and fusion as guidance information, is proposed for large missing regions of face inpainting. In the proposed method, two stages are adopted to complete coarse inpainting and refinement of the face. In Stage-I, we combine generator with a new encoder–decoder network with variational autoencoder-based backbone to predict the face region semantic information (including face structure, contour and content information) and do facial fusion for face inpainting. This could fully explore face region semantic information and generate coordinated coarse face images. Stage-II builds upon Stage-I results to refine face image. Both global and patch discriminators are used to synthesize high-quality photo-realistic inpainting. Experimental results on both CelebA and CelebA-HQ datasets demonstrate the effectiveness and efficiency of our method.},
  archive      = {J_ASOC},
  author       = {Xian Zhang and Canghong Shi and Xin Wang and Xi Wu and Xiaojie Li and Jiancheng Lv and Imran Mumtaz},
  doi          = {10.1016/j.asoc.2021.107626},
  journal      = {Applied Soft Computing},
  pages        = {107626},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Face inpainting based on GAN by facial prediction and fusion as guidance information},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporated vehicle lateral control strategy for stability
and enhanced energy saving in distributed drive hybrid bus.
<em>ASOC</em>, <em>111</em>, 107617. (<a
href="https://doi.org/10.1016/j.asoc.2021.107617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle stability and energy efficiency are important considerations in vehicle engineering. In this context, the current paper presents an energy saving strategy for hybrid electric vehicles that incorporates vehicle lateral dynamic control in conjunction with energy efficiency. To this end, we first model the nonlinear vehicle lateral dynamics of a hybrid electric bus via a Takagi–Sugeno approach and combine this model with an H_{\infty } H_{\infty } state-feedback controller via parallel distributed compensation. The controller matrices are obtained using linear matrix inequalities through an optimal energy-to-energy performance norm of the nonlinear vehicle model. Second, we propose a reference side-slip angle generating method and a set of tire force distribution rules, which under the premise of ensuring vehicle stability, minimize the overall energy consumption of the vehicle. Finally, we put forward a new speed prediction method based on vehicle lateral dynamics for hybrid electric vehicle energy saving . Human-in-the-loop simulated driving experiments are conducted where the bus performs lane-changing maneuvers with enhanced control properties under various driving conditions, demonstrating the reliability of the proposed energy-saving performance measures.},
  archive      = {J_ASOC},
  author       = {Lin Li and Serdar Coskun and Reza Langari and Junqiang Xi},
  doi          = {10.1016/j.asoc.2021.107617},
  journal      = {Applied Soft Computing},
  pages        = {107617},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incorporated vehicle lateral control strategy for stability and enhanced energy saving in distributed drive hybrid bus},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGA implementation of fuzzy sparse adaptive equalizer for
indoor wireless communication systems. <em>ASOC</em>, <em>111</em>,
107616. (<a href="https://doi.org/10.1016/j.asoc.2021.107616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel equalization is a basic requirement of wireless receiver to alleviate the effects of inter symbol interference (ISI) that helps in faithful reconstruction of original information. The accuracy of channel state information (CSI) also affects the equalizer performance designed for fading communication channels. The paper proposes a robust decision feedback equalizer (DFE) for indoor wireless channels with limited user mobility using sparse fuzzy modeling . The main contributions of the paper can be realized looking into three important aspects. First, the equalizer is designed for practical indoor channel conditions using adaptive model whose learning parameter is selected using fuzzy rule. Second, the computational complexity is substantially reduced by incorporating norm-based sparsity to the cost function. Third important point is validation of equalization model using Xilinx FPGA and Artix 7 board. QPSK modulated data is transmitted through the channel having non-ideal frequency response which is characterized by IEEE 802.11 model. Further bit error rate(BER), mean square error(MSE), eye diagram and power delay profile(PDP) are taken as the performance measures to test the equalizer in presence of fading effects and user mobility. Both MATLAB simulation and FPGA implementation results are presented to justify the usefulness of the proposed equalization model for indoor wireless communication .},
  archive      = {J_ASOC},
  author       = {Swetaleena Sahoo ( M.Tech. ) and Yash Keju Barapatre ( B.Tech. ) and Harish Kumar Sahoo ( Ph.D. ) and Sarita Nanda ( Ph.D. )},
  doi          = {10.1016/j.asoc.2021.107616},
  journal      = {Applied Soft Computing},
  pages        = {107616},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FPGA implementation of fuzzy sparse adaptive equalizer for indoor wireless communication systems},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-stage consensus model based on opinion dynamics and
evolution of social power in large-scale group decision making.
<em>ASOC</em>, <em>111</em>, 107615. (<a
href="https://doi.org/10.1016/j.asoc.2021.107615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main challenges in large scale group decision making (LSGDM) problem are how to tackle with the great number of participants and how to achieve a common solution accepted by most of participants. To address these challenges, in this paper, we propose a novel framework based on opinion evolution to study the consensus reaching process (CRP) in the LSGDM. In the proposed framework, we focus on the CRP in a dynamical social influence relationship context and the whole CRP is divided into two stages. In the first stage, we design a social power and opinion evolution iterative algorithm to estimate the final consensus opinions in each sub-group. In the second stage, the opinion leaders are selected as the representatives of each sub-group to participate in the consensus process. Subsequently, we develop a self-appraisal mechanism to evaluate the confidence degree of each opinion leader. Furthermore, an opinion leaders’ networked preference evolution mechanism is proposed to investigate the consensus formation of the second stage. Finally, we use this model on a case study and compare it with some existing models.},
  archive      = {J_ASOC},
  author       = {Shengli Li and Rosa M. Rodríguez and Cuiping Wei},
  doi          = {10.1016/j.asoc.2021.107615},
  journal      = {Applied Soft Computing},
  pages        = {107615},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage consensus model based on opinion dynamics and evolution of social power in large-scale group decision making},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Removal notice to: “Detecting anomalies within unmanned
aerial vehicle (UAV) video based on contextual saliency” [appl. Soft
comput. 96 (2020) 106715]. <em>ASOC</em>, <em>110</em>, 107833. (<a
href="https://doi.org/10.1016/j.asoc.2021.107833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Mostafa Al-Gabalawy},
  doi          = {10.1016/j.asoc.2021.107833},
  journal      = {Applied Soft Computing},
  pages        = {107833},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Removal notice to: “Detecting anomalies within unmanned aerial vehicle (UAV) video based on contextual saliency” [Appl. soft comput. 96 (2020) 106715]},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Applying machine learning for combating fake news
and internet/media content manipulation. <em>ASOC</em>, <em>110</em>,
107779. (<a href="https://doi.org/10.1016/j.asoc.2021.107779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Michał Choraś and Konstantinos Demestichas and Álvaro Herrero and Michał Woźniak},
  doi          = {10.1016/j.asoc.2021.107779},
  journal      = {Applied Soft Computing},
  pages        = {107779},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Editorial: Applying machine learning for combating fake news and Internet/Media content manipulation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptively reversed diffusion dual-drive evolutionary
algorithm in dynamic environments for intelligence prediction.
<em>ASOC</em>, <em>110</em>, 107761. (<a
href="https://doi.org/10.1016/j.asoc.2021.107761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction problems are difficult to be carried out in a dynamic environment, for two key questions: one is how to monitor environmental changes, the other is how to make respond timely after the environment has changed. Multi-population strategy is often adopted to address both two key problems. However, there are two stubborn questions that limit and affect the effectiveness of the strategy, i.e., (1) Search overlaps are easy to occur which lead to lose the ability of local exploit, (2) In the course of evolution, the search range of subpopulations tends to assimilate, so the subpopulation will gradually lose the ability to explore whole search space. Therefore, this paper adopts multi-population strategy, and presents a novel intelligence algorithm based on particle swarm optimization to solve the above problem, called adaptively reversed diffusion dual-drive evolutionary algorithm (ARDDEA). (1) Firstly, ARDDEA monitors the environmental changes by setting the global dynamic sentry in each subgroup. (2) Secondly, in order to avoid searching overlapping of the sub-population, a new exclusion strategy is proposed in this paper. A new distance determination method, i.e., between-swarms average Mahalanobis​ distance, is devised in the exclusion strategy to decide the inter-population distance. If the distance is too small between two sub-populations, furtherly, a Hill–valley decision function is used to determine whether they tracked the same peak or not. If so, the inferior subpopulations will be reinitialized by a reverse diffusion operation ( RD ) proposed in this paper. Besides, (3) a new dual-drive kinetic updating equation is proposed to enhance the search capability of the population. The new algorithm compared with several state-of-art dynamic optimization algorithms on the moving peak problem. The results show that the ARDDEA algorithm can track the optimal solution more effectively in the dynamic environment, and shows strong robustness and adaptability. It is a hope algorithm applied to prediction problems.},
  archive      = {J_ASOC},
  author       = {Lanlan Kang and Wenliang Cao and Ruey-Shun Chen and Yeh-Cheng Chen},
  doi          = {10.1016/j.asoc.2021.107761},
  journal      = {Applied Soft Computing},
  pages        = {107761},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptively reversed diffusion dual-drive evolutionary algorithm in dynamic environments for intelligence prediction},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bi-objective trapezoidal fuzzy mixed integer linear
program-based distribution center location decision for large-scale
emergencies. <em>ASOC</em>, <em>110</em>, 107757. (<a
href="https://doi.org/10.1016/j.asoc.2021.107757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at solving three important and challenging issues: (i) How to build a 0-1 mixed integer linear program-based distribution center location model for large-scale emergencies (we call emergency distribution center (EDC) location model for short). (ii) How to develop an effective method to convert the trapezoidal fuzzy EDC location model into a crisp one. (iii) How to demonstrate the validity, flexibility and superiorities of the constructed model and developed method. Considering the acceptance degree of fuzzy constraints to be violated, this paper defines a novel flexible ranking relation for trapezoidal fuzzy numbers (TrFNs) by using the α α -cut set and analyzes its desirable properties . A bi-objective trapezoidal fuzzy EDC location model (BTFELM) is built for reflecting the urgency and uncertainty of large-scale emergencies. Some essential definitions and theorems are proposed for equivalently converting the fuzzy constraints of the BTFELM into crisp ones. By employing the graded mean integration representation of TrFN, the BTFELM is ultimately transformed into a single objective crisp EDC location linear program . Wenchuan earthquake case is studied to illustrate the practicality and validity of the proposed method. Furthermore, the sensitivity and comparative analyses are provided to justify the flexibility and superiorities of the proposed method. Finally, the accuracy analyses are conducted to illustrate the reliability of the proposed method.},
  archive      = {J_ASOC},
  author       = {Shu-ping Wan and Ze-hui Chen and Jiu-ying Dong},
  doi          = {10.1016/j.asoc.2021.107757},
  journal      = {Applied Soft Computing},
  pages        = {107757},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-objective trapezoidal fuzzy mixed integer linear program-based distribution center location decision for large-scale emergencies},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep neural network model for fashion collocation
recommendation using side information in e-commerce. <em>ASOC</em>,
<em>110</em>, 107753. (<a
href="https://doi.org/10.1016/j.asoc.2021.107753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fashion collocation recommendation model is a useful tool to enhance customer experiences and increase cross-selling in e-commerce. Prior research efforts focus on using titles and images to build the model. However, accessible side information in e-commerce, such as textual descriptions, purchase data and category information of items, generally bear valuable information regarding this task. This study seeks to develop a fashion collocation recommendation model that leverages accessible side information in e-commerce. First, a knowledge graph is built to model the purchase data and category information of items and an approach is subsequently proposed to learn the knowledge embeddings in the knowledge graph. To cope with the challenge of Chinese titles, a model is developed to encode the Chinese titles into vectors. Finally, a fashion collocation recommendation model combines the knowledge embeddings of the knowledge graph and the encoded titles to compute the probability of fashion collocation between items. The experiments are conducted on two datasets. Compared with baseline models , our model obtains nearly 6\% and 1\% AUC improvement in TIANCHI and POG dataset, respectively. The experimental results demonstrate that our efforts to leverage side information to improve fashion collocation recommendations are feasible and effective.},
  archive      = {J_ASOC},
  author       = {Siyu Wang and Jiangtao Qiu},
  doi          = {10.1016/j.asoc.2021.107753},
  journal      = {Applied Soft Computing},
  pages        = {107753},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep neural network model for fashion collocation recommendation using side information in e-commerce},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emotion classification on eye-tracking and
electroencephalograph fused signals employing deep gradient neural
networks. <em>ASOC</em>, <em>110</em>, 107752. (<a
href="https://doi.org/10.1016/j.asoc.2021.107752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion produces complex neural processes and physiological changes under appropriate event stimulation. Physiological signals have the advantage of better reflecting a person’s actual emotional state than facial expressions or voice signals. An electroencephalogram (EEG) is a signal obtained by collecting, amplifying, and recording the human brain’s weak bioelectric signals on the scalp. The eye-tracking (E.T.) signal records the potential difference between the retina and the cornea and the potential generated by the eye movement muscle. Furthermore, the different modalities of physiological signals will contain various information representations of human emotions. Finding this different modal information is of great help to get higher recognition accuracy. The E.T. and EEG signals are synchronized and fused in this research, and an effective deep learning (DL) method was used to combine different modalities. This article proposes a technique based on a fusion model of the Gaussian mixed model (GMM) with the Butterworth and Chebyshev signal filter. Features extraction on EEG and E.T. are subsequently calculated. Secondly, the self-similarity (SSIM), energy (E), complexity (C), high order crossing (HOC), and power spectral density (PSD) for EGG, and electrooculography power density estimation ((EOG-PDE), center gravity frequency (CGF), frequency variance (F.V.), root mean square frequency (RMSF) for E.T. are selected hereafter; the max–min method is applied for vector normalization. Finally, a deep gradient neural network (DGNN) for EEG and E.T. multimodal signal classification is proposed. The proposed neural network predicted the emotions under the eight emotions event stimuli experiment with 88.10\% accuracy. For the evaluation indices of accuracy (Ac), precision (Pr), recall (Re), F-measurement (Fm), precision–recall (P.R.) curve, true-positive rate (TPR) of receiver operating characteristic curve (ROC), the area under the curve (AUC), true-accept rate (TAR), and interaction on union (IoU), the proposed method also performs with high efficiency compared with several typical neural networks including the artificial neural network (ANN), SqueezeNet, GoogleNet, ResNet-50, DarkNet-53, ResNet-18, Inception-ResNet, Inception-v3, and ResNet-101.},
  archive      = {J_ASOC},
  author       = {Qun Wu and Nilanjan Dey and Fuqian Shi and Rubén González Crespo and R. Simon Sherratt},
  doi          = {10.1016/j.asoc.2021.107752},
  journal      = {Applied Soft Computing},
  pages        = {107752},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Emotion classification on eye-tracking and electroencephalograph fused signals employing deep gradient neural networks},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensor and actuator fault-tolerant control based on fuzzy
unknown input observer: A polynomial fuzzy approach. <em>ASOC</em>,
<em>110</em>, 107747. (<a
href="https://doi.org/10.1016/j.asoc.2021.107747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies sensor and actuator Fault-Tolerant Control (FTC) of nonlinear systems . The fault tolerance is necessary to have desired performance and prevent the system from instability. A Polynomial Fuzzy Unknown Input Observer (PFUIO) with immeasurable premise variable is proposed to estimate the states of the system, and sensor and actuator faults; and to use them in the controller to compensate the faults. The essence of the sensor fault necessitates considering the third class of Polynomial Fuzzy Model (PFM) with immeasurable premise variables. This makes the design procedure more complicated. Application of PFM reduces the conservatism and increases modeling accuracy. Moreover, the disturbance and model uncertainties are considered to realize more practical conditions. The polynomiality leads to Sum Of Squares (SOS), which can be solved using existing software. The proposed approach is evaluated by applying it to the inverted pendulum system. Simulating results show better performance of the proposed method as compared with the Linear Matrix Inequality (LMI) approach},
  archive      = {J_ASOC},
  author       = {Farzaneh Sabbaghian-Bidgoli and Mohammad Farrokhi},
  doi          = {10.1016/j.asoc.2021.107747},
  journal      = {Applied Soft Computing},
  pages        = {107747},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sensor and actuator fault-tolerant control based on fuzzy unknown input observer: A polynomial fuzzy approach},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification utility aware data stream anonymization.
<em>ASOC</em>, <em>110</em>, 107743. (<a
href="https://doi.org/10.1016/j.asoc.2021.107743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data streams are continuous, infinite and ordered sequences of data. In comparison to static dataset anonymization , data stream anonymization confront with a number of constraints and difficulties due to the dynamic nature of data flow. The literature already addressed the k-anonymization of data streams which contain quasi-identifier attributes. However, today most data streams contain sensitive and classification target attributes as well. This work’s main motivation is to develop a k-anonymization method for data streams which additionally protects the sensitivity and enables effective classification models . The k-anonymization, as a result, is formulated as a weighted multi-objective optimization problem. There are three objectives with respective weights as user parameters. A clustering based k-anonymization algorithm is developed as the solution. An extensive experimental evaluation on three real datasets shows the effectiveness of our proposal in various configurations. Moreover, the experimental results also confirm that our proposal attains better classification accuracies in comparison to popular data stream anonymization techniques.},
  archive      = {J_ASOC},
  author       = {Ugur Sopaoglu and Osman Abul},
  doi          = {10.1016/j.asoc.2021.107743},
  journal      = {Applied Soft Computing},
  pages        = {107743},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification utility aware data stream anonymization},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A coordinated strategy for multi-machine power system
stability. <em>ASOC</em>, <em>110</em>, 107742. (<a
href="https://doi.org/10.1016/j.asoc.2021.107742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the fast growth of the energy demand, the current power system may reach the marginal operating design resulting in unfavourable frequency/voltages conditions, the timeframe of stability and unexpected faults. Therefore, it is necessary to use all capacity without any additional development costs and having a secure and safe system. Hereby, this study addresses a novel coordination strategy for improving low-frequency oscillations in multi-machine models under various operating conditions. To achieve this goal, the proposed solution is divided into three main parts, the first part presents eigenvalues and time-domain analyzes to extract the unstable modes for a nonlinear model of multi-machine to capture a real-world problem. The second part proposes two controllers based on fuzzy theory and thyristor controlled series compensator (TCSC) with power oscillation damper (POD) structure to reach considerable damping of low-frequency oscillations. Since an uncoordinated design between two controllers in the power system aggravated instability, therefore, the last part proposes a modified virus colony search (VCS) to optimally adjust the decision variables with two conflicting objectives based on time and frequency domains. To demonstrate the proposed coordinated strategy, the well-known two-area and four-machine test system have been selected and the obtained results are compared in several loading conditions such as ± ± 20 load changing, with other available controllers through several analysing indices. According to numerical results, the proposed design can improve the statical indices such as integral of time multiplied absolute value of the error (ITAE) and figure of demerit (FD) about 12 and 11\%, respectively. Also, the eigenvalues can be collected between the damping ratio 0.2 and real part -1.0.},
  archive      = {J_ASOC},
  author       = {Ali Ghasemi-Marzbali and Roya Ahmadiahangar},
  doi          = {10.1016/j.asoc.2021.107742},
  journal      = {Applied Soft Computing},
  pages        = {107742},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A coordinated strategy for multi-machine power system stability},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting impact of hitchhike on coexisted heterogeneous
IoT networks. <em>ASOC</em>, <em>110</em>, 107741. (<a
href="https://doi.org/10.1016/j.asoc.2021.107741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Internet of Things (IoT), plenty of heterogeneous nodes coexist and contend for wireless channel for data transmission. Therefore, improving the channel utilization is of great importance. Hitchhike is a promising mechanism of improving the channel utilization. With Hitchhike, a node can carry control signals on the preamble field of another transmitting packet, thereby eliminating the extra time of transmitting the control signals and improving the channel utilization. However, this control signals may seriously interfere with the packet transmission of coexisted heterogeneous networks . In this paper, considering that an 802.15.4 network with Hitchhike enabled and an 802.11 network coexist, we develop theoretical models to predict the impact of Hitchhike on the time and frequency synchronizations, the packet error rate , as well as the system throughput of the 802.11 network. Extensive experiments verify that the proposed prediction models are very accurate and effective. Experimental results show that when control signals are superposed on the 802.11 Preamble, they can hurt the 802.11 performance (specifically, frame synchronization, coarse frequency estimation, and system throughput). However, they have little effect on symbol timing synchronization. This study is very helpful for providing parameter optimization and protocol-design guidance for the coexistence of heterogeneous IoT networks. When multiple heterogeneous networks coexist, we may apply machine learning algorithms to help design better Hitchhike protocols. The insights from our theoretical model (say, the frequency spectrum of the Hitchhike control message is a good design parameter for minimizing the interference with coexisted heterogeneous networks) can be used to choose appropriate machine learning algorithms and construct appropriate algorithm modules, for improving the performance of Hitchhike protocols in complex IoT scenarios.},
  archive      = {J_ASOC},
  author       = {Li Feng and Yi Liu and Jianlan Guo and Yuqiang Chen},
  doi          = {10.1016/j.asoc.2021.107741},
  journal      = {Applied Soft Computing},
  pages        = {107741},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting impact of hitchhike on coexisted heterogeneous IoT networks},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of bitcoin price based on manipulating
distribution strategy. <em>ASOC</em>, <em>110</em>, 107738. (<a
href="https://doi.org/10.1016/j.asoc.2021.107738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since Bitcoin has been the most popular digital currency in the global financial market, the prediction of its price has been an important area in finance. Recently, numerous researches on prediction of financial indices including Bitcoin based on machine learning techniques have been developed. However, little studies pay attention to the strategy of manipulating original distribution to obtain better performances. Because the return data of Bitcoin prices are highly concentrated near zero, small changes of values in the components can cause different results. Based on this characteristic, we propose flattening distribution strategy (FDS) based on the copula theory as a strategy of the manipulating distribution of components artificially to improve the prediction of Bitcoin price return. We consider multilayer perceptron (MLP), recurrent neural networks (RNN), and long short-term memory (LSTM) to assess the performances of FDS. Finally, we find that the proposed algorithms based on FDS improve significantly the prediction accuracy of the return of Bitcoin price for each of the three architectures.},
  archive      = {J_ASOC},
  author       = {Eunho Koo and Geonwoo Kim},
  doi          = {10.1016/j.asoc.2021.107738},
  journal      = {Applied Soft Computing},
  pages        = {107738},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of bitcoin price based on manipulating distribution strategy},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale covering rough sets with applications to data
classification. <em>ASOC</em>, <em>110</em>, 107736. (<a
href="https://doi.org/10.1016/j.asoc.2021.107736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When facing with a complex problem, one often needs to consider dealing with it at what level of granularity . Multi-scale knowledge representation provides us an opportunity to analyze problems from different granularity . However, as well as traditional rough sets model, most of existing multi-scale rough set models are based on partitions generated from equivalence relations, which limits their application in real data. In this paper, we set forth a new data analysis model with multi-scale coverings by extending partitions to coverings. To this end, a new type of decision tables, i.e., multi-scale covering decision tables are formalized to deal with knowledge representation under multi-scale framework. Optimal scale selection for consistent and inconsistent covering decision tables are then proposed to obtain acceptable decisions under coarser scales. Furthermore, the acquisition of optimal rules with higher accuracy and covering rate are discussed. Extensive experiments on some real-world data sets are set up to examine the effectiveness and feasibility of the proposed model. Experimental results show that the multi-scale covering theory gives a new way to enhance the generalization ability of the classification model .},
  archive      = {J_ASOC},
  author       = {Zhehuang Huang and Jinjin Li},
  doi          = {10.1016/j.asoc.2021.107736},
  journal      = {Applied Soft Computing},
  pages        = {107736},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale covering rough sets with applications to data classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel hybrid method for direction forecasting and trading
of apple futures. <em>ASOC</em>, <em>110</em>, 107734. (<a
href="https://doi.org/10.1016/j.asoc.2021.107734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, a novel hybrid method MCXGBoost–Bagging–RegPSO​ is proposed for direction forecasting of the high-frequency Apple Futures’ price and simulation trading. First, a multi-classification method based on the eXtreme Gradient Boosting (XGBoost) is established for Apple Futures price direction classification, while the Regrouping Particle Swarm Optimization (RegPSO) is adopted to optimize the parameters of the movement magnitude levels, XGBoost , and the pre-designed trading rules. Next, a Bagging method is incorporated into the proposed approach to solve the overfitting problem. Then, the proposed method predicts the price movement direction and magnitude level, and a one-year high-frequency trading simulation is executed based on the price direction forecasting results. Finally, several evaluation indicators are used to assess the direction prediction and profitability performances of the proposed method. Experimental results demonstrate that the proposed approach successfully achieved outstanding performance in terms of hit ratio, accumulated return, maximum drawdown, and return–risk ratio. As far as it is concerned, the proposed method could be considered as a useful reference for both intraday investors engaged in high-frequency trading and regulators of the Apple Futures market.},
  archive      = {J_ASOC},
  author       = {Shangkun Deng and Xiaoru Huang and Zhaohui Qin and Zhe Fu and Tianxiang Yang},
  doi          = {10.1016/j.asoc.2021.107734},
  journal      = {Applied Soft Computing},
  pages        = {107734},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel hybrid method for direction forecasting and trading of apple futures},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MSMANet: A multi-scale mesh aggregation network for brain
tumor segmentation. <em>ASOC</em>, <em>110</em>, 107733. (<a
href="https://doi.org/10.1016/j.asoc.2021.107733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fine segmentation of brain tumor, which is instrumental in brain tumor diagnosis, treatment planning and prognosis, is becoming a research hotspot in medical images processing . However, manual segmentation is labor-intensive, time-consuming and easily affected by subjective and objective factors. Automatic segmentation methods based on convolutional neural networks have attracted significant interest in recent years. In this paper, a novel multi-scale mesh aggregation network (MSMANet) for brain tumor segmentation is proposed. Firstly, an improved Inception module is introduced to replace the standard convolution in the encoder to extract and aggregate effective information from different receptive fields. Secondly, a novel mesh aggregation strategy is proposed to gradually refine the shallow features and further alleviate the semantic gap . This strategy maximizes the aggregation of multi-level features at different scales and realizes the complementary advantages among features. Finally, the ability of network identification and convergence is improved by employing attention mechanism and deep supervision. Experiments were implemented on BraTS2018 to evaluate the proposed network. The dice similarity coefficient (DSC) of MSMANet in enhancing tumor, whole tumor and tumor core are 0.758, 0.890 and 0.811, respectively. Experimental results indicate that satisfactory performance is achieved compared with the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Yan Zhang and Yao Lu and Wankun Chen and Yankang Chang and Haiming Gu and Bin Yu},
  doi          = {10.1016/j.asoc.2021.107733},
  journal      = {Applied Soft Computing},
  pages        = {107733},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MSMANet: A multi-scale mesh aggregation network for brain tumor segmentation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). K-means clustering and kNN classification based on negative
databases. <em>ASOC</em>, <em>110</em>, 107732. (<a
href="https://doi.org/10.1016/j.asoc.2021.107732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, privacy protection has become an important issue in data mining. k -means clustering and kNN classification are two popular data mining algorithms , which have been widely studied in the past decade. In this paper, we mainly study the problem of privacy protection during k k -means clustering and kNN classification. Negative database ( NDB ) is a new type of data representation which can protect privacy as well as support distance estimation, so it is promising to apply NDB s to privacy-preserving k -means clustering and kNN classification. Existing privacy-preserving k -means clustering and kNN classification algorithms based on NDB s could effectively protect data privacy, but their clustering or classification performance has a non-negligible degradation. To alleviate this problem, we propose a new NDB generation algorithm (named QK -hidden algorithm). Compared to existing NDB generation algorithms, the QK -hidden algorithm employs a new set of parameters to control the selection of different bits when generating NDB records, and this enables a fine-grained control of the accuracy of distance estimation. By the QK -hidden algorithm, private data are converted into NDB s before inputting to k k -means and kNN algorithms . Then, we propose an approach specialized for estimating Euclidean distance from the NDB s generated by the Q K QK -hidden algorithm, and this approach is used instead of calculating Euclidean distance on plain text. Moreover, we design a model for estimating the cluster centers from NDB s for the k k -means algorithm. The remaining steps of k k -means and kNN algorithms keep the same to original versions. Experimental results demonstrate that the proposed algorithms could achieve more than 30\% improvement on the clustering performance (in terms of average Davies–Bouldin Index) and 9\% improvement on the classification performance (in terms of macro precision, recall and F1 score) in comparison with some existing works.},
  archive      = {J_ASOC},
  author       = {Dongdong Zhao and Xiaoyi Hu and Shengwu Xiong and Jing Tian and Jianwen Xiang and Jing Zhou and Huanhuan Li},
  doi          = {10.1016/j.asoc.2021.107732},
  journal      = {Applied Soft Computing},
  pages        = {107732},
  shortjournal = {Appl. Soft. Comput.},
  title        = {K-means clustering and kNN classification based on negative databases},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conflict resolution in the multi-stakeholder stepped
spillway design under uncertainty by machine learning techniques.
<em>ASOC</em>, <em>110</em>, 107721. (<a
href="https://doi.org/10.1016/j.asoc.2021.107721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimal spillway design is of great significance since these structures can reduce erosion downstream of the dams. This study proposes a risk-based optimization framework for a stepped spillway to achieve an economical design scenario with the minimum loss in hydraulic performance. Accordingly, the stepped spillway was simulated in the FLOW-3D® model, and the validated model was repeatedly performed for various geometric states. The results were used to form a Multilayer Perceptron artificial neural network (MLP-ANN) surrogate model . Then, a risk-based optimization model was formed by coupling the MLP-ANN and NSGA-II. The concept of conditional value at risk (CVaR) was utilized to reduce the risk of the designed spillway malfunctions in high flood flow rates, while minimizing the construction cost and the loss in hydraulic performance. Lastly, given the conflicting objectives of stakeholders, the non-cooperative graph model for conflict resolution (GMCR) was applied to achieve a compromise on the Pareto optimal solutions . Applicability of the suggested approach in the Jarreh Dam, Iran, resulted in a practical design scenario, which simultaneously minimizes the loss in hydraulic performance and the project cost and satisfies the priorities of decision-makers.},
  archive      = {J_ASOC},
  author       = {Mehrdad Ghorbani Mooselu and Mohammad Reza Nikoo and Parnian Hashempour Bakhtiari and Nooshin Bakhtiari Rayani and Azizallah Izady},
  doi          = {10.1016/j.asoc.2021.107721},
  journal      = {Applied Soft Computing},
  pages        = {107721},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Conflict resolution in the multi-stakeholder stepped spillway design under uncertainty by machine learning techniques},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TDMatcher: A topic-based approach to task-developer matching
with predictive intelligence for recommendation. <em>ASOC</em>,
<em>110</em>, 107720. (<a
href="https://doi.org/10.1016/j.asoc.2021.107720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence is currently gripping the business world, which is the next step on the journey from Big Data to full automation. As crowdsourcing has been widely adopted by more enterprises and developers, the software crowdsourcing platform is able to collect enough data. Therefore, we introduce predictive intelligence to solve complex problems. This provides a bridge between software developers and enterprises: developers look for suitable tasks, whose aim is to gain revenues with respect to their interests and abilities; enterprises look for developers that are able to complete crowdsourcing tasks and/or solve hard problems. One main problem is the prediction challenge, i.e. , how to perfectly predict the developers for the software crowdsourcing tasks and make appropriate recommendations. To solve the problem, this paper introduces predictive intelligence and proposes TDMatcher , which can effectively perform task-developer pairs prediction and recommendations for software crowdsourcing. First, we builds a unified model for tasks and developers such that they can be matched in the same domain space. Second, we quantitatively measures the matching degree between tasks and developers. Third, we randomly generates potential matchings between developers and crowdsourcing tasks and then employs an MCMC sampling approach to optimize the whole process. Highly matched task-developer pairs can be achieved in the sampling process. In order to solve the cold-start problem, we constructs a social network for each new developer, which indicates that the developer’s interests/abilities to be modeled We implemented TDMatcher and evaluated it against the state-of-the-art approaches on the real-world dataset. The experimental results clearly demonstrate the superiority of TDMatcher . We measured our proposed TDMatcher through the accuracy , diversity and Harmonic Mean of TDMatcher , and found that: (1) TDMatcher outperforms the state-of-the-arts by 15+\% in the prediction accuracy and 30\% in diversity; and (2) TDMatcher achieves a balance between accuracy and diversity. We believe that TDMatcher provides crowdsourcing platforms with much more capabilities in finding appropriate developers to complete crowdsourcing tasks or vice versa.},
  archive      = {J_ASOC},
  author       = {Yiyang Fu and Benjun Shen and Yuting Chen and Linpeng Huang},
  doi          = {10.1016/j.asoc.2021.107720},
  journal      = {Applied Soft Computing},
  pages        = {107720},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TDMatcher: A topic-based approach to task-developer matching with predictive intelligence for recommendation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extension of safety and critical effect analysis to
neutrosophic sets for the evaluation of occupational risks.
<em>ASOC</em>, <em>110</em>, 107719. (<a
href="https://doi.org/10.1016/j.asoc.2021.107719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occupational health and safety applications require a comprehensive risk analysis to protect employees from accidents caused by various hazard sources in the workplace. Therefore, in the literature, there are various risk assessment methods utilizing various parameters. One of them is safety and critical effect analysis (SCEA) which is the method that calculates risk magnitude based on the highest number of parameters that are probability, severity, frequency, and detectability . However, the risk assessment phase includes uncertainties and inconsistencies caused by both evaluations of experts and the complex structure of the handled process. Therefore, we proposed a new risk assessment approach to cope with these uncertainties and inconsistencies by extending SCEA with the Neutrosophic sets . In this study, each parameter, probability, severity, frequency, and detectability , is defined by using truth membership function, falsity membership function, and indeterminacy membership function in Neutrosophic sets (NSs) to deal with incomplete, indeterminate, and inconsistent information existing in the expert preferences and complex structure of the handled process. It is the first time, the indeterminacy of NSs is defined based on truth and falsity definitions by using a rule-based system based on probability, severity, frequency, and detectability parameters. Furthermore, Mamdani fuzzy inference system used in the SCEA method is adapted to NSs. The proposed approach is applied to the metalworking and woodworking workshop of a university. The obtained results are compared with the results of the SCEA method. It is concluded that the proposed approach is better than SCEA method to consider inconsistencies in the risk evaluation.},
  archive      = {J_ASOC},
  author       = {Merve Karamustafa and Selcuk Cebi},
  doi          = {10.1016/j.asoc.2021.107719},
  journal      = {Applied Soft Computing},
  pages        = {107719},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extension of safety and critical effect analysis to neutrosophic sets for the evaluation of occupational risks},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OrbitNet: A new CNN model for automatic fault diagnostics of
turbomachines. <em>ASOC</em>, <em>110</em>, 107702. (<a
href="https://doi.org/10.1016/j.asoc.2021.107702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unplanned outage due to faults in a high-fidelity turbomachine such as steam turbine and centrifugal compressor often results in the reduced reliability and productivity of a factory while increasing its maintenance costs. Shaft orbit images generated from turbomachine vibration signals have been used to diagnose component faults. However, the existing methods were developed mostly by either using features extracted from orbits or utilizing simulation data which may produce inaccurate results in practical applications due to system complexity and data uncertainties. This paper presents a novel deep learning convolution neural network methodology for accurately automatic diagnostics of multiple faults in general rotating machines by adeptly integrating advanced signal processing with orbit images augmentation, considering the high non-linearity and uncertainty of sensed vibration signals. Environmental noise in vibration signals are filtered through the integration of multiresolution discrete wavelet packet transform and Bayesian hypothesis testing-based automatic thresholding. Shaft orbit images generated from the cleansed vibration data are augmented to increase their representativity and generalization. A novel multi-layer convolutional neural network model, OrbitNet, is specially designed to improve its generality and robustness while avoid possible overfitting in fault identification of various turbomachines . The proposed model retains the pattern information in the axis trajectory to the greatest extent, with the ability of accurately capturing features of various faults in different turbomachines . A generic implementation procedure is proposed for automatic fault diagnosis of rotating machinery based on the presented methodology. A comparison study is conducted to demonstrate the effectiveness and feasibility of the proposed methodology by using the sensed vibration signals collected from three real-world centrifugal compressors, two steam turbines and one generator with four different fault modes including imbalance, friction, misalignment and oil whirl.},
  archive      = {J_ASOC},
  author       = {Xiaomo Jiang and Shuhua Yang and Fumin Wang and Shengli Xu and Xiaofang Wang and Xueyu Cheng},
  doi          = {10.1016/j.asoc.2021.107702},
  journal      = {Applied Soft Computing},
  pages        = {107702},
  shortjournal = {Appl. Soft. Comput.},
  title        = {OrbitNet: A new CNN model for automatic fault diagnostics of turbomachines},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective proportional–integral–derivative
optimization algorithm for parameters optimization of double-fed
induction generator-based wind turbines. <em>ASOC</em>, <em>110</em>,
107673. (<a href="https://doi.org/10.1016/j.asoc.2021.107673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The meta-heuristic algorithm inspired by natural may reduce the optimization performance due to excessive imitation. This paper proposes a novel multi-objective proportional–integral–derivative optimization algorithm inspired by mathematical thought to provide a better non-dominated solution for multi-objective problems. The idea of the proportional–integral–derivative control algorithm is introduced to cooperate with multi-objective optimization problems for the first time. The proposed algorithm is employed to store and maintain non-dominated solutions. Two groups of controllers of the proposed algorithm are designed for the multi-objective optimization problems , i.e., exploitative controllers aim to obtain the local optimal solution ; explorative controllers aim to obtain the global optimal solution . To verify the effectiveness of the multi-objective proportional–integral–derivative optimization algorithm , eight comparison algorithms are compared with eight benchmark functions ; five comparison algorithms are compared under the multi-objective parameters optimization problem of double-fed induction generator-based wind turbines . The results of benchmark functions show that the multi-objective proportional–integral–derivative optimization algorithm has superior convergence performances and outperforms other comparison algorithms. The proposed algorithm has excellent optimization performance to obtain the minimum deviation of rotor speed and reactive power for the wind power system controller.},
  archive      = {J_ASOC},
  author       = {Linfei Yin and Qi Gao},
  doi          = {10.1016/j.asoc.2021.107673},
  journal      = {Applied Soft Computing},
  pages        = {107673},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective proportional–integral–derivative optimization algorithm for parameters optimization of double-fed induction generator-based wind turbines},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Credit risk modeling on data with two timestamps in
peer-to-peer lending by gradient boosting. <em>ASOC</em>, <em>110</em>,
107672. (<a href="https://doi.org/10.1016/j.asoc.2021.107672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is extremely important for a Peer-to-Peer (P2P) lending platform to provide their lenders with a reliable model to predict borrowers’ credit risk. This study introduces four different gradient boosting methods to model the credit risk of borrowers in P2P platforms and estimates the performance of models in a more practical scenario. The timeliness of observations and features is fully investigated in modeling. Each observation is associated with two timestamps that guide the splitting of data set for training and testing models, while no timestamp or only one timestamp was used in most previous research. The hyperparameters of gradient boosting models are optimized by the Bayesian approach , and the models are evaluated in different scenarios. The importance of features implied by gradient boosting models is examined as well. The empirical result shows that the models’ performance from k − k− fold cross validation is overestimated in comparison to that from rolling windows guided by two timestamps of observations, which indicates that the timestamps plays a significant impact on evaluating the performance of credit risk models in P2P lending.},
  archive      = {J_ASOC},
  author       = {Ligang Zhou and Hamido Fujita and Hao Ding and Rui Ma},
  doi          = {10.1016/j.asoc.2021.107672},
  journal      = {Applied Soft Computing},
  pages        = {107672},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Credit risk modeling on data with two timestamps in peer-to-peer lending by gradient boosting},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention induced multi-head convolutional neural network
for human activity recognition. <em>ASOC</em>, <em>110</em>, 107671. (<a
href="https://doi.org/10.1016/j.asoc.2021.107671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks , including convolutional neural networks (CNNs), have been widely adopted for human activity recognition in recent years. They have attained significant performance improvement over traditional techniques due to their strong feature representation capabilities. Some of the challenges faced by the HAR community is the non-availability of a substantial amount of labeled training samples, and the higher computational cost and system resources requirements of deep learning architectures as opposed to shallow learning algorithms. To address these challenges, we propose an attention-based multi-head model for human activity recognition (HAR). This framework contains three lightweight convolutional heads, with each head designed using one-dimensional CNN to extract features from sensory data. The lightweight multi-head model is induced with attention to strengthen the representation ability of CNN, allowing for automatic selection of salient features and suppress unimportant ones. We conducted ablation studies and experiments on two publicly available benchmark datasets: WISDM and UCI HAR, to evaluate our model. The experimental outcome demonstrates the effectiveness of the proposed framework in activity recognition and achieves better accuracy while ensuring computational efficiency.},
  archive      = {J_ASOC},
  author       = {Zanobya N. Khan and Jamil Ahmad},
  doi          = {10.1016/j.asoc.2021.107671},
  journal      = {Applied Soft Computing},
  pages        = {107671},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention induced multi-head convolutional neural network for human activity recognition},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AFLN-DGCL: Adaptive feature learning network with
difficulty-guided curriculum learning for skin lesion segmentation.
<em>ASOC</em>, <em>110</em>, 107656. (<a
href="https://doi.org/10.1016/j.asoc.2021.107656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated skin lesion segmentation is a crucial step in the whole computer-aided (CAD) skin disease process. Recently, the fully convolutional network (FCN) has achieved outstanding performance on this task. However, it remains challenging because of three problems: (1) the difficult cases on dermoscopy images, including low contrast lesion, bubble and hair occlusion cases; (2) the overfitting problem of FCN-based methods that is caused by the imbalanced training of difficult samples and easy samples; (3) the over-segmentation problem of FCN-based methods. This work proposes a new skin lesion segmentation framework. Specifically, feature representations from dermoscopy images are learned by the Adaptive Feature Learning Network (AFLN). An ensemble learning method is introduced to build a fusion model, enabling the AFLN model to capture the multi-scale information. We propose a Difficulty-Guided Curriculum Learning (DGCL) with step-wise training strategy to handle the overfitting problem caused by the imbalanced training. Finally, a Selecting-The-Biggest-Connected-Region (STBCR) is proposed to alleviate the over-segmentation problem of the fusion model. The method performance is compared using the same defined metrics (DICE, JAC, and ACC) with other state-of-the-art works on publicly available ISIC 2016, ISIC 2017, and ISIC 2018 databases, and results (0.931, 0.875, and 0.966), (0.881, 0.807, and 0.948), and (0.920, 0.856, and 0.966) illustrate its advantages. The excellent and robust performances on three public databases proved that our method has the potential to be applied to CAD skin diseases diagnosis.},
  archive      = {J_ASOC},
  author       = {Peng Tang and Xintong Yan and Qiaokang Liang and Dan Zhang},
  doi          = {10.1016/j.asoc.2021.107656},
  journal      = {Applied Soft Computing},
  pages        = {107656},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AFLN-DGCL: Adaptive feature learning network with difficulty-guided curriculum learning for skin lesion segmentation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid particle swarm optimization algorithm for solving the
clustered vehicle routing problem. <em>ASOC</em>, <em>110</em>, 107655.
(<a href="https://doi.org/10.1016/j.asoc.2021.107655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a variant of the classical capacitated vehicle routing problem called clustered vehicle routing problem (CluVRP). In CluVRP, customers are grouped into different clusters. A vehicle visiting a cluster cannot leave the cluster until all customers in the same cluster have been served. Each cluster and customer have to be served only once. A new hybrid metaheuristic , combining the particle swarm optimization (PSO) and variable neighborhood search (VNS) for the specific problem, is proposed to solve the CluVRP. In the hybrid PSO, the basic PSO principle ensures the solution diversity and VNS ensures solution intensity to bring the solution to the local optima. Extensive computational experiments have been performed on numerous benchmark instances with various sizes obtained from the CluVRP literature to evaluate the performance of the proposed hybrid PSO. The obtained results of the proposed algorithm are compared with the results found in the literature to validate the effectiveness of the proposed hybrid PSO. The proposed algorithm is proven to be superior to the state-of-the-art algorithms on the CluVRP. The proposed algorithm obtains 138 new best-known solutions among the 293 benchmark instances.},
  archive      = {J_ASOC},
  author       = {Md. Anisul Islam and Yuvraj Gajpal and Tarek Y. ElMekkawy},
  doi          = {10.1016/j.asoc.2021.107655},
  journal      = {Applied Soft Computing},
  pages        = {107655},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid particle swarm optimization algorithm for solving the clustered vehicle routing problem},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of government strategies against COVID-19
pandemic using q-rung orthopair fuzzy TOPSIS method. <em>ASOC</em>,
<em>110</em>, 107653. (<a
href="https://doi.org/10.1016/j.asoc.2021.107653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 outbreak, which emerged in China and continues to spread rapidly all over the world, has brought with it increasing numbers of cases and deaths. Governments have suffered serious damage and losses not only in the field of health but also in many other fields. This has directed governments to adopt and implement various strategies in their communities. However, only a few countries succeed partially from the strategies implemented while other countries have failed. In this context, it is necessary to identify the most important strategy that should be implemented by governments. A decision problem based on the decisions of many experts, with some contradictory and multiple criteria, should be taken into account in order to evaluate the multiple strategies implemented by various governments. In this study, this decision process is considered as a multi-criteria decision making (MCDM) problem that also takes into account uncertainty. For this purpose, q-rung orthopair fuzzy sets (q-ROFSs) are used to allow decision-makers to their assessments in a wider space and to better deal with ambiguous information. Accordingly, two different Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) approaches are recommended under the q-ROFS environment and applied to determine the most appropriate strategy. The results of the proposed approaches determine the A1 — Mandatory quarantine and strict isolation strategy as the best strategy. Comparisons with other q-rung orthopair fuzzy MCDM methods and intuitionistic fuzzy TOPSIS method are also presented for the validation of the proposed methods. Besides, sensitivity analyses are conducted to check the robustness of the proposed approaches and to observe the effect of the change in the q parameter.},
  archive      = {J_ASOC},
  author       = {Nurşah Alkan and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2021.107653},
  journal      = {Applied Soft Computing},
  pages        = {107653},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of government strategies against COVID-19 pandemic using q-rung orthopair fuzzy TOPSIS method},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based dynamic user preference modeling and
nonlinear feature interaction learning for collaborative filtering
recommendation. <em>ASOC</em>, <em>110</em>, 107652. (<a
href="https://doi.org/10.1016/j.asoc.2021.107652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional collaborative filtering (CF) method based on static user preference modeling and linear matching function learning severely limits the recommendation performance. To solve the above problem, in this article, we adopt dynamic user preference modeling and nonlinear matching function learning in the CF recommendation. For dynamic user preference modeling, a two-layer neural attention network is used, which fully considers the predicted item, the recent historical interacted items and their interaction time to estimate the contribution weight of each interacted item in user preferences modeling. For nonlinear matching function learning, we add a single hidden layer neural network on top of the traditional matrix factorization (MF) model, which can significantly improve the feature interaction learning capabilities of the model with only a few additional parameters. Extensive experiments show that our method significantly outperforms the state-of-the-art CF methods and the key technologies we proposed in this research have a positive effect on improving the recommendation performance.},
  archive      = {J_ASOC},
  author       = {Ruiqin Wang and Yunliang Jiang and Jungang Lou},
  doi          = {10.1016/j.asoc.2021.107652},
  journal      = {Applied Soft Computing},
  pages        = {107652},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-based dynamic user preference modeling and nonlinear feature interaction learning for collaborative filtering recommendation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A many-objective whale optimization algorithm to perform
robust distributed clustering in wireless sensor network. <em>ASOC</em>,
<em>110</em>, 107650. (<a
href="https://doi.org/10.1016/j.asoc.2021.107650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The substantial increase in the usage of wireless sensor networks (WSNs) encourages to develop data clustering in event monitoring applications. Many centralized algorithms with single objective optimization are employed to solve this problem. However privacy, security and technical constraints are key issues in traditional centralized approach. Moreover, many WSN applications like condition monitoring and target tracking require more than three objectives for effective partitioning of dataset. This paper proposes many-objective whale optimization algorithm to handle robust distributed clustering in WSN. Initially, a swarm based many-objective whale optimization (MaOWOA) is discussed where reference point based leader selection method is utilized in updating the solutions instead of grid based leader selection as in multi-objective approach. This method gives better convergence and diversity. The simulation result of proposed approach is evaluated on many-objective DTLZ test problems against existing many-objective methods which is faster in terms of simulation time and gives competitive results in terms of generational distance (GD), inverse generational distance (IGD), spacing (SP) and hyper volume difference (HVD). Further, the encouraging results of the proposed MaOWOA are applied to perform robust distributed clustering in WSNs which is termed as distributed many-objective clustering using whale optimization algorithm (DMaOWOA). In this approach, a weight based method is incorporated to detect and remove the outliers and diffusion method of cooperation is used for distributed clustering. The proposed DMaOWOA is tested on one synthetic and three practical WSN datasets. It is observed that DMaOWOA based clustering performs up to 6\% and 8\% improvement in terms of Silhouette index as compared to particle swarm optimization based many-objective distributed clustering (DMaOPSO) and distributed K-Means (DK-Means) clustering algorithm, respectively.},
  archive      = {J_ASOC},
  author       = {Dinesh Kumar Kotary and Satyasai Jagannath Nanda and Rachana Gupta},
  doi          = {10.1016/j.asoc.2021.107650},
  journal      = {Applied Soft Computing},
  pages        = {107650},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A many-objective whale optimization algorithm to perform robust distributed clustering in wireless sensor network},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Offline robust tuning of the motion control for
omnidirectional mobile robots. <em>ASOC</em>, <em>110</em>, 107648. (<a
href="https://doi.org/10.1016/j.asoc.2021.107648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, mobile robots have been helpful systems to perform a wide variety of complex tasks in daily life applications from industry, academy, and home. These robots carry out mobility on flat terrains, mainly in narrow spaces that are difficult to access or dangerous for humans. Therefore, increasing the efficiency of their movements through control technologies has become a topic of great interest for researchers. Among controllers, the linear ones are widely used to improve the efficiency of mobile robots because of their simplicity, reliability, and practicality, notwithstanding advanced control strategies. A well-tuned linear controller can show outstanding performances in controlled environments where the modeled and simulated conditions used for its adjustment are not too far from reality. However, actual operating environments are subject to uncertainties and disturbances that can hardly be accounted for during the controller tuning process. The above compromises the performance of the mobile robot in practice, and finding the appropriate controller parameters that enhance robustness becomes a crucial task. Therefore, this work presents a robust tuning approach for the controller of an omnidirectional mobile robot based on the solution of a nonlinear dynamic optimization problem through meta-heuristics. Robustness is incorporated in the optimization problem by minimizing the sensitivity to the control performance indexes. Simultaneously, this is included through dynamic and stochastic variations in the meta-heuristic optimizer hyperparameters. A comparative statistical analysis is performed using robust and non-robust tuning approaches. Based on simulated and experimental tests, the proposed robust approach shows notable performance improvements regarding the non-robust one while minimizing operation errors in the presence of different uncertainty magnitudes.},
  archive      = {J_ASOC},
  author       = {Omar Serrano-Pérez and Miguel G. Villarreal-Cervantes and Alejandro Rodríguez-Molina and Javier Serrano-Pérez},
  doi          = {10.1016/j.asoc.2021.107648},
  journal      = {Applied Soft Computing},
  pages        = {107648},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Offline robust tuning of the motion control for omnidirectional mobile robots},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimization for ternary fixed polarity reed–muller
expressions based on ternary quantum shuffled frog leaping algorithm.
<em>ASOC</em>, <em>110</em>, 107647. (<a
href="https://doi.org/10.1016/j.asoc.2021.107647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logic minimization is one of the most crucial steps in combinational logic synthesis . The minimization for ternary fixed polarity Reed–Muller (FPRM) expressions aims to find a polarity that produces a ternary FPRM expression with as few operation terms as possible. However, the size of the ternary FPRM optimization space is much larger than that of binary FPRM optimization space , and the minimization for ternary FPRM expressions is a computationally hard problem. In this paper, we first propose a ternary quantum shuffled frog leaping algorithm (TQSFL) to solve the three-valued combinatorial optimization problem . The TQSFL divides frog individuals into three subpopulations: a subpopulation with a global updating strategy, a subpopulation with a local updating strategy, and a subpopulation with a random updating strategy, and performs local depth search on the three subpopulations based on the proposed ternary quantum rotation gate, ternary quantum correction mechanism, and ternary quantum crossover operator . Moreover, based on the TQSFL, we propose a minimization algorithm (MA) for ternary FPRM expressions, which searches for a polarity that produces a ternary FPRM expression with as few operation terms as possible by using the TQSFL. Experimental results demonstrated the effectiveness of the MA in minimizing ternary FPRM expressions.},
  archive      = {J_ASOC},
  author       = {Zhenxue He and Limin Xiao and Xiang Wang},
  doi          = {10.1016/j.asoc.2021.107647},
  journal      = {Applied Soft Computing},
  pages        = {107647},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Minimization for ternary fixed polarity Reed–Muller expressions based on ternary quantum shuffled frog leaping algorithm},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimum geometrical pattern and design of real-size diagrid
structures using accelerated fuzzy-genetic algorithm with bilinear
membership function. <em>ASOC</em>, <em>110</em>, 107646. (<a
href="https://doi.org/10.1016/j.asoc.2021.107646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagrids are the efficient systems of tube structures for tall buildings . One of the design considerations for these structures is the geometrical pattern of the system. In this paper, a new method of fuzzy-genetic algorithm based on bilinear membership functions is proposed with an improved crossover operator and penalty function. The method is applied on tall buildings with a diagrid system to find the optimum geometrical patterns and the overall structural weight. Various three-dimensional diagrid structures with 24, 36, 42, 56, and 60 stories and different slenderness ratios are analyzed under gravity and wind load. Then the effects of variation in the number of bays (4, 6, and 8) are investigated and compared with each other. The results show that by increasing the dimension of the structure, the structural weight is reduced up to 33\% in some cases. However, the obtained angle of the diagrid members (range of 63 to 79 degrees) is increased by increasing the number of stories and the height of the structure. The optimum weight and geometrical pattern of the models is obtained and a formulation is extracted from the results regarding the optimum angle of a diagrid system. Considering GA, results show the merit of the accelerated fuzzy-genetic algorithm regarding the convergence and the avoidance of being trapped in local minimum.},
  archive      = {J_ASOC},
  author       = {Payam Ashtari and Roghaye Karami and Salar Farahmand-Tabar},
  doi          = {10.1016/j.asoc.2021.107646},
  journal      = {Applied Soft Computing},
  pages        = {107646},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimum geometrical pattern and design of real-size diagrid structures using accelerated fuzzy-genetic algorithm with bilinear membership function},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coronavirus disease (COVID-19) detection using x-ray images
and enhanced DenseNet. <em>ASOC</em>, <em>110</em>, 107645. (<a
href="https://doi.org/10.1016/j.asoc.2021.107645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2019 novel coronavirus (COVID-19) originating from China, has spread rapidly among people living in other countries. According to the World Health Organization (WHO), by the end of January, more than 104 million people have been affected by COVID-19, including more than 2 million deaths. The number of COVID-19 test kits available in hospitals is reduced due to the increase in regular cases. Therefore, an automatic detection system should be introduced as a fast, alternative diagnostic to prevent COVID-19 from spreading among humans. For this purpose, three different BiT models: DenseNet, InceptionV3, and Inception-ResNetV4 have been proposed in this analysis for the diagnosis of patients infected with coronavirus pneumonia using X-ray radiographs in the chest. These three models give and examine Receiver Operating Characteristic (ROC) analyses and uncertainty matrices, using 5-fold cross-validation. We have performed the simulations which have visualized that the pre-trained DenseNet model has the best classification efficiency with 92\% among two other models proposed (83.47\% accuracy for inception V3 and 85.57\% accuracy for Inception-ResNetV4).},
  archive      = {J_ASOC},
  author       = {Saleh Albahli and Nasir Ayub and Muhammad Shiraz},
  doi          = {10.1016/j.asoc.2021.107645},
  journal      = {Applied Soft Computing},
  pages        = {107645},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Coronavirus disease (COVID-19) detection using X-ray images and enhanced DenseNet},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Production scheduling in industrial mining complexes with
incoming new information using tree search and deep reinforcement
learning. <em>ASOC</em>, <em>110</em>, 107644. (<a
href="https://doi.org/10.1016/j.asoc.2021.107644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial mining complexes have implemented digital technologies and advanced sensors to monitor and gather real-time data about their different operational aspects , starting from the supply of materials from the mineral deposits involved to the products provided to customers. However, technologies are not available to respond in real-time to the incoming new information to adapt the short-term production schedule of a mining complex. A short-term production schedule determines the daily/weekly/monthly sequence of extraction, the destination of materials and utilization of processing streams. This paper presents a novel self-learning artificial intelligence algorithm for mining complexes that learns, from its own experience, to adapt the short-term production scheduling decisions by responding to incoming new information. The algorithm plays the game of short-term production scheduling on its own using a Monte Carlo tree search to train a deep neural network agent that adapts the short-term production schedule with incoming new information. The deep neural network agent evaluates the short-term production scheduling decisions and, in parallel, performs searches using the Monte Carlo tree search to generate experiences. The experiences are then used to train the agent. The agent improves the strength of the tree search, which results in an even stronger self-play to generate better experiences. An application of the proposed algorithm at a real-world copper mining complex shows its exceptional performance to adapt the 13-week short-term production schedule almost in real-time. The adapted production schedule successfully meets the different production requirements and makes better use of the processing capabilities, while also increasing copper concentrate production by 7\% and cash flows by 12\% compared to the initial production schedule. A video of the proposed algorithm can be found at https://youtu.be/_gSbzxMc_W8 .},
  archive      = {J_ASOC},
  author       = {Ashish Kumar and Roussos Dimitrakopoulos},
  doi          = {10.1016/j.asoc.2021.107644},
  journal      = {Applied Soft Computing},
  pages        = {107644},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Production scheduling in industrial mining complexes with incoming new information using tree search and deep reinforcement learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time prediction of COVID-19 patients health situations
using artificial neural networks and fuzzy interval mathematical
modeling. <em>ASOC</em>, <em>110</em>, 107643. (<a
href="https://doi.org/10.1016/j.asoc.2021.107643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the end of 2019, the SARS-CoV-2 virus caused an outbreak of COVID-19 disease. The spread of this once-in-a-century pathogen increases demand for appropriate medical care, which strains the capacity and resources of hospitals in a critical way. Given the limited time available to prepare for the required demand, health care administrators fear they will not be ready to face patient’s influx. To aid health managers with the Prioritization and Scheduling COVID-19 Patients problem, a tool based on Artificial Intelligence (AI) through the Artificial Neural Networks (ANN) method, and Operations Research (OR) through a Fuzzy Interval Mathematical model was developed. The results indicated that combining both models provides an effective assessment under scarce initial information to select a suitable list of patients for a set of hospitals. The proposed approach allows to achieve a key goal: minimizing death rates under each hospital constraints of available resources. Furthermore, there is a serious concern regarding the resurgence of the COVID-19 virus which could cause a more severe pandemic. Thus, the main outcome of this study is the application of the above-mentioned approaches, especially when combining them, as efficient tools serving health establishments to manage critical resources.},
  archive      = {J_ASOC},
  author       = {Mohamed Ali Elleuch and Amal Ben Hassena and Mohamed Abdelhedi and Francisco Silva Pinto},
  doi          = {10.1016/j.asoc.2021.107643},
  journal      = {Applied Soft Computing},
  pages        = {107643},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time prediction of COVID-19 patients health situations using artificial neural networks and fuzzy interval mathematical modeling},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGA implementation of epileptic seizure detection using
semisupervised reduced deep convolutional neural network. <em>ASOC</em>,
<em>110</em>, 107639. (<a
href="https://doi.org/10.1016/j.asoc.2021.107639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an optimized variational mode decomposition (OVMD), reduced deep convolutional neural network (RDCNN), and multi-kernel random vector functional link network (MKRVFLN) are combined to recognize the epileptic seizure using electroencephalogram (EEG) signals. An improved particle swarm optimization based on the maximum value of log energy entropy is introduced to compute the optimized values of the number of band-limited intrinsic mode functions (BLIMFs) and the data-fidelity factor. The Kurtosis and correlation coefficient are used to extract the most efficient BLIMF from highly nonlinear and non-stationary multi-class epilepsy EEG signals. The RDCNN structure is designed to extract the most discriminative unsupervised features and feed into the novel supervised MKRVFLN classifier to train efficiently by reducing the cross-entropy loss for recognizing the seizure epochs efficaciously. The Bonn University, Germany , Neurology and Sleep centre, New Delhi single-channel EEG, and Boston Children’s Hospital multichannel scalp EEG (sEEG) datasets are considered to evaluate the overall efficiency of the proposed method. The proposed method has produced 100\% classification accuracy (CA) for classification problem (CP) 1 to 4 present in database-A, as well as for all CP incorporated with database-B and also it has 99.88\% CA for CP-5 of the Bonn University database using a ten-fold cross-validation strategy. Furthermore, the proposed method has also the sensitivity of 100\%, specificity of 99.34\%, and CA of 99.37\% with a negligible false positive per hour (FPR/h) of 0.663\% by adopting 50\% training, 30\% testing and 20\% validation of total data present in Boston database and its overall performance outperforms as compared to other state-of-the-art methods. The less computational complexity , higher learning speed, accurate epileptic seizure recognition, remarkable classification accuracy , short event recognition time, and negligible FPR/h are the main advantages of the proposed OVMD-RDCNN-MKRVFLN method over RDCNN, OVMD-RDCNN, and OVMD-RDCNN-KRVFLN methods. The novel RDCNN-MKRVFLN digital architecture is designed and implemented on a high-speed field-programmable gate array (FPGA) hardware environment to design a computer-aided-diagnosis (CAD) system for online epileptic seizure diagnosis. The simplicity, feasibility, and practicability of the proposed method validate the effectiveness of automatic seizure recognition.},
  archive      = {J_ASOC},
  author       = {Mrutyunjaya Sahani and Susanta Kumar Rout and Pradipta Kishore Dash},
  doi          = {10.1016/j.asoc.2021.107639},
  journal      = {Applied Soft Computing},
  pages        = {107639},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FPGA implementation of epileptic seizure detection using semisupervised reduced deep convolutional neural network},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A data-driven approach to simultaneous fault detection and
diagnosis in data centers. <em>ASOC</em>, <em>110</em>, 107638. (<a
href="https://doi.org/10.1016/j.asoc.2021.107638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The failure of cooling systems in data centers (DCs) leads to higher indoor temperatures, causing crucial electronic devices to fail, and produces a significant economic loss. To circumvent this issue, fault detection and diagnosis (FDD) algorithms and associated control strategies can be applied to detect, diagnose, and isolate faults. Existing methods that apply FDD to DC cooling systems are designed to successfully overcome individually occurring faults but have difficulty in handling simultaneous faults. These methods either require expensive measurements or those made over a wide range of conditions to develop training models, which can be time-consuming and costly. We develop a rapid and accurate, single and multiple FDD strategy for a DC with a row-based cooling system using data-driven fault classifiers informed by a gray-box temperature prediction model. The gray-box model provides thermal maps of the DC airspace for single as well as a few simultaneous failure conditions, which are used as inputs for two different data-driven classifiers, CNN and RNN , to rapidly predict multiple simultaneous failures. The model is validated with testing data from an experimental DC. Also, the effect of adding Gaussian white noise to training data is discussed and observed that even with low noisy environment , the FDD strategy can diagnose multiple faults with accuracy as high as 100\% while requiring relatively few simultaneous fault training data samples. Finally, the different classifiers are compared in terms of accuracy, confusion matrix , precision, recall and F1-score.},
  archive      = {J_ASOC},
  author       = {Sahar Asgari and Rohit Gupta and Ishwar K. Puri and Rong Zheng},
  doi          = {10.1016/j.asoc.2021.107638},
  journal      = {Applied Soft Computing},
  pages        = {107638},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A data-driven approach to simultaneous fault detection and diagnosis in data centers},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Channel pruning guided by spatial and channel attention for
DNNs in intelligent edge computing. <em>ASOC</em>, <em>110</em>, 107636.
(<a href="https://doi.org/10.1016/j.asoc.2021.107636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have achieved remarkable success in many computer vision tasks recently, but the huge number of parameters and the high computation overhead hinder their deployments on resource-constrained edge devices. It is worth noting that channel pruning is an effective approach for compressing DNN models. A critical challenge is to determine which channels are to be removed, so that the model accuracy will not be negatively affected. In this paper, we first propose Spatial and Channel Attention (SCA), a new attention module combining both spatial and channel attention that respectively focuses on “where” and “what” are the most informative parts. Guided by the scale values generated by SCA for measuring channel importance, we further propose a new channel pruning approach called Channel Pruning guided by Spatial and Channel Attention (CPSCA). Experimental results indicate that SCA achieves the best inference accuracy, while incurring negligibly extra resource consumption, compared to other state-of-the-art attention modules. Our evaluation on two benchmark datasets shows that, with the guidance of SCA, our CPSCA approach achieves higher inference accuracy than other state-of-the-art pruning methods under the same pruning ratios.},
  archive      = {J_ASOC},
  author       = {Mengran Liu and Weiwei Fang and Xiaodong Ma and Wenyuan Xu and Naixue Xiong and Yi Ding},
  doi          = {10.1016/j.asoc.2021.107636},
  journal      = {Applied Soft Computing},
  pages        = {107636},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Channel pruning guided by spatial and channel attention for DNNs in intelligent edge computing},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memetic algorithms for mapping p-body interacting systems in
effective quantum 2-body hamiltonians. <em>ASOC</em>, <em>110</em>,
107634. (<a href="https://doi.org/10.1016/j.asoc.2021.107634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing is an emerging research area which promises to offer a revolution in the computing performance. The world’s first commercially available quantum computer has been the D-Wave machine which aims at solving complex problems by representing them in terms of Ising Hamiltonians . This formulation allows addressing several combinatorial optimization problems since generally it is possible to map any problem to the Hamiltonian of the Ising model . However, D-Wave’s architecture restricts the Ising Hamiltonian to the case with only 2-body interactions. Therefore, in order to face problems mapped on systems with p p -body interactions ( p ≥ 2 p≥2 ), it is necessary to implement a procedure to compute 2-body effective Hamiltonians of p p -body interacting systems. Due to the complexity of this task, recently, meta-heuristic methods have been applied with promising results. The aim of this paper is to implement a procedure to convert from p p -body to 2-body Hamiltonians by means of memetic algorithms . As shown in the experimental session involving the ferromagnetic p p -spin model as a case study, the proposed approach improves by 60\% on average over the state-of-the-art meta-heuristic approaches.},
  archive      = {J_ASOC},
  author       = {Giovanni Acampora and Vittorio Cataudella and Pratibha Raghupati Hegde and Procolo Lucignano and Gianluca Passarelli and Autilia Vitiello},
  doi          = {10.1016/j.asoc.2021.107634},
  journal      = {Applied Soft Computing},
  pages        = {107634},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Memetic algorithms for mapping p-body interacting systems in effective quantum 2-body hamiltonians},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive intelligence powered attentional stacking matrix
factorization algorithm for the computational drug repositioning.
<em>ASOC</em>, <em>110</em>, 107633. (<a
href="https://doi.org/10.1016/j.asoc.2021.107633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) technologies are widely used to study the computational drug repositioning to find potential new uses for marketed drugs, which have a huge impact on drug development. However, due to the sparsity of datasets and the linear limitations of traditional AI or machine learning models lead to the inability of most methods to effectively obtain hidden feature of drugs and diseases To end this, Predictive Intelligence (PI) has been proposed in recent years. Hence, in this work, we propose a PI powered Attentional Stacking Matrix Factorization (PI-ASMF). Firstly, in order to extract the effective latent factor of the drugs or diseases the ASMF model superimposes the drug–disease similarity information and the drug–disease association information into the PI model, which ensures that it can learn the effective latent factor and alleviates the cold start problem to a certain extent. Subsequently, the latent factors of drugs and diseases are element-wise multiplied with their respective attentional vectors to learn the adaptive weights of each feature. Finally, the latent factors of drugs and diseases are fed into the prediction module to generate predictive values of potential drug–disease associations. The AUC and AUPR values of PI-ASMF model on two real datasets are 0.892 and 0.168, 0.913 and 0.25, respectively, which verified its superiority and validity.},
  archive      = {J_ASOC},
  author       = {Shaohong Yan and Aimin Yang and Shanshan Kong and Bin Bai and Xiaoyu Li},
  doi          = {10.1016/j.asoc.2021.107633},
  journal      = {Applied Soft Computing},
  pages        = {107633},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predictive intelligence powered attentional stacking matrix factorization algorithm for the computational drug repositioning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive neighborhood simulated annealing for
sustainability-oriented single machine scheduling with deterioration
effect. <em>ASOC</em>, <em>110</em>, 107632. (<a
href="https://doi.org/10.1016/j.asoc.2021.107632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research considers the realistic phenomenon of cutting tool deterioration in the scheduling of jobs and tool replacement activities to ensure sustainable machining operations. Typically, each job has a basic processing time (independent of tool deterioration) and a tool age-dependent prolongation function. Nevertheless, such a strategy may replace the cutting tools before its useful lifespan to avoid longer processing time as it leads to higher energy consumption and likelihood of tardy jobs. Given the reduced tool operating duration, the prospect of exploiting its capability to limit the processing time prolongation is not considered by prior works. In this research, a single-machine scheduling approach that determines the job processing time based on tool age and operating duration (utilization) is studied. The problem is formulated as a mixed-integer linear program (MILP) with the objective of minimizing the weighted costs of energy consumption, tooling, and job tardiness. Due to its computational complexity , a new variant of simulated annealing (SA) algorithm is proposed to solve large instances, where an adaptive strategy is utilized to determine the neighborhood size for enhanced exploration and exploitation. Computational results for an exhaustive set of instances indicate the proposed scheduling strategy to always outperform the existing method, yielding up to a 43\% reduction in total cost. Besides, the proposed variant of SA effectively solves the scheduling problem and consistently dominates the traditional SA method.},
  archive      = {J_ASOC},
  author       = {Mohamed Salama and Sharan Srinivas},
  doi          = {10.1016/j.asoc.2021.107632},
  journal      = {Applied Soft Computing},
  pages        = {107632},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive neighborhood simulated annealing for sustainability-oriented single machine scheduling with deterioration effect},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group article recommendation based on ER rule in scientific
social networks. <em>ASOC</em>, <em>110</em>, 107631. (<a
href="https://doi.org/10.1016/j.asoc.2021.107631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group Recommendation Systems (GRS) is an emerging area in both research and practice and has been successfully developed in many domains as a type of information filter to overcome the information overload problem. With the growth of Scientific Social Networks (SSNs), the need for article recommendation is emerging. Considering that researchers can be grouped according to their research interests, and article recommendation to a group of users has not been addressed in the literature, this paper aims to develop and test an inferential model to accurately recommend articles for group researchers in SSNs. In this paper, a novel approach for group article recommendation, referred to as GPRAH_ER, is proposed to improve the processes of both individual prediction and group aggregation. In the stage of individual prediction, the Probabilistic Matrix Factorization method is adopted and is further unified by using articles’ contents and group information. In the stage of group aggregation, the ER rule is introduced in the aggregation process, since it possesses the advantages of identifying group members’ impacts based on the group member’s weight and reliability. To verify the performance of the proposed method, experiments are conducted on a real dataset CiteULike. The experimental results show that the proposed GPRAH_ER method outperforms other benchmark methods, and provides a more effective recommendation of articles to researchers in SSNs.},
  archive      = {J_ASOC},
  author       = {Gang Wang and Han-Ru Wang and Ying Yang and Dong-Ling Xu and Jian-Bo Yang and Feng Yue},
  doi          = {10.1016/j.asoc.2021.107631},
  journal      = {Applied Soft Computing},
  pages        = {107631},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Group article recommendation based on ER rule in scientific social networks},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention augmented convolutional neural network for
acoustics based machine state estimation. <em>ASOC</em>, <em>110</em>,
107630. (<a href="https://doi.org/10.1016/j.asoc.2021.107630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of technology is leading to the emergence of smart factories where the Artificial Intelligence paradigm of deep learning plays a significant role in processing data streams from machines. This paper presents the application of Augmented Attention Blocks embedded in a deep convolutional neural network for the purposes of estimating the state of remote machines using remotely collected acoustic data. An Android application was developed for the purposes of transferring audio data from a remote machine to a base station. At the base station, we propose and developed a deep convolutional neural network called MAABL ( M obileNetv2 with A ugmented A ttention Bl ock). The structure of the neural network is constructed by combining an inverted residual block of MobileNetv2 with an augmented attention mechanism block. Attention Mechanism is an attempt to selectively concentrate on a few relevant things, while ignoring others in deep neural networks. Due to the presence of audio frames containing silent features not relevant to the task at hand, an Attention Mechanism is particularly important when processing audio data. The MAABL network proposed in this paper obtains the state of the art results on the accuracy and parameters of three different acoustic data sets. On a relatively large-scale acoustic dataset regarding machine faults, the method proposed in this paper achieves 98\% accuracy on the test set. Moreover, after using transfer learning, the model achieved the state of the art accuracy with less training time and fewer training samples.},
  archive      = {J_ASOC},
  author       = {Jiannan Tan and John Oyekan},
  doi          = {10.1016/j.asoc.2021.107630},
  journal      = {Applied Soft Computing},
  pages        = {107630},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention augmented convolutional neural network for acoustics based machine state estimation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NMFCDA: Combining randomization-based neural network with
non-negative matrix factorization for predicting CircRNA-disease
association. <em>ASOC</em>, <em>110</em>, 107629. (<a
href="https://doi.org/10.1016/j.asoc.2021.107629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies suggest that circRNA is closely related to the occurrence and development of human diseases, and it has great application prospects in the field of disease diagnostic markers. However, restricted by the environment and conditions, it is usually time-consuming and labor-intensive to use biological experimental methods to identify the association between circRNA and disease. In this study, we propose a novel computational framework NMFCDA that combines randomization-based neural network Pseudoinverse Learning (PIL) with Non-Negative Matrix Factorization (NMF) to predict circRNA-disease associations. The model first fuses circRNA natural language sequence information, disease semantic information , and circRNA and disease Gaussian interaction profile (GIP) kernel similarity information into a unified matrix, then uses NMF algorithm to obtain its key features, and finally uses randomization-based PIL to search for the global optimal solution to accurately predict the association between circRNA and disease. In the benchmark data set circR2Disease, NMFCDA achieved a prediction accuracy of 92.56\% and an AUC of 0.9278, significantly higher than other classifier models and previous existing methods. Furthermore, 26 of the top 30 disease-associated circRNAs with the highest predictive scores were confirmed by the relevant literature. These results indicate that NMFCDA can be used as a useful prediction tool to provide theoretical basis and reliable circRNA candidates for biological experiments.},
  archive      = {J_ASOC},
  author       = {Lei Wang and Zhu-Hong You and Xi Zhou and Xin Yan and Hao-Yuan Li and Yu-An Huang},
  doi          = {10.1016/j.asoc.2021.107629},
  journal      = {Applied Soft Computing},
  pages        = {107629},
  shortjournal = {Appl. Soft. Comput.},
  title        = {NMFCDA: Combining randomization-based neural network with non-negative matrix factorization for predicting CircRNA-disease association},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decision boundary clustering for efficient local SVM.
<em>ASOC</em>, <em>110</em>, 107628. (<a
href="https://doi.org/10.1016/j.asoc.2021.107628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scaling up support vector machine (SVM) for large data sets remains one of its main challenges. One way to achieve this is to break down the problem into smaller ones using clustering techniques where local SVM models are constructed. Although this approach is considerably fast compared to the standard SVM, its performance is sometimes inferior even when a local kernel SVM is used as in k k -local SVM (KSVM). This often occurs due to the overfitting of some local models when the corresponding clusters are unbalanced, i.e., most of their patterns belong to one class. To alleviate this problem for KSVM, a new supervised clustering technique is proposed to partition the data around the decision boundary into nearly balanced clusters. For a binary classification problem, this is accomplished as follows. First, one of the class regions (e.g. the more dense) is clustered into k k clusters. Then, the clusters that are closest to the decision boundary are determined. Finally, these clusters are expanded to include the closest patterns from all classes. In this way, each cluster includes a reasonable number of patterns of each class that helps to mitigate the overfitting problem. A comparison of the proposed approach with KSVM, clustered SVM (CSVM), the standard SVM and two of the top ensemble classification trees, namely Random Forest and AdaBoost , for several benchmark large data sets was accomplished. The experimental results showed that the proposed approach outperforms KSVM and CSVM and competes the best model, especially when the radial basis kernel is used, for most data sets.},
  archive      = {J_ASOC},
  author       = {Hatem A. Fayed and Amir F. Atiya},
  doi          = {10.1016/j.asoc.2021.107628},
  journal      = {Applied Soft Computing},
  pages        = {107628},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decision boundary clustering for efficient local SVM},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Elastic train scheduling model. <em>ASOC</em>, <em>110</em>,
107627. (<a href="https://doi.org/10.1016/j.asoc.2021.107627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Train scheduling plays a significant role in the optimal use of the railway resources and satisfaction of customers. This paper presents a novel innovation, which is the elasticity of train length, or the possibility of compression and stretching of the train length in accordance with rail route conditions. The proposed sustainable model provides optimal train scheduling with minimum travel time as well as optimal revenue and cost by developing a balance between length, speed, traction power, and energy consumption of trains while there is the possibility of carrying homogeneous and heterogeneous commodities. Thus, the new model is referred to as the sustainable elastic train scheduling model. Implementing the model in a real case using an introduced genetic algorithm proves its success and increases railway revenues by at least 48 percent with the saving of 25 percent in time. Sensitivity analysis of the model reveals the model is more sensitive to changes in the objective function of travel time. In addition to 50\% improvement in using railroad capacity, it is possible to achieve the minimum increase of 71\% in revenue.},
  archive      = {J_ASOC},
  author       = {Ahmad Reza Jafarian-Moghaddam},
  doi          = {10.1016/j.asoc.2021.107627},
  journal      = {Applied Soft Computing},
  pages        = {107627},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Elastic train scheduling model},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-surrogate-assisted dual-layer ensemble feature
selection algorithm. <em>ASOC</em>, <em>110</em>, 107625. (<a
href="https://doi.org/10.1016/j.asoc.2021.107625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble feature selection has attracted much attention of scholars because of its good robustness. However, existing methods on dealing with high-dimensional or large-scale data still have some shortcomings, such as high computational cost or high feature redundancy. In view of it, this paper proposes a new ensemble feature selection algorithm for large-scale data, called Multi-surrogate-assisted Dual-layer Ensemble Feature Selection (MDEFS). In MDEFS, a filter ensemble feature selection method with fast search ability is firstly developed to remove irrelevant or weakly-relevant features in the first layer. In the second layer, a particle swarm-based ensemble method with global search ability is proposed to select optimal feature subset from those remaining features. Furthermore, a multi-surrogate-assisted search mechanism of swarm is developed to reduce the cost of MDEFS on processing large-scale data, where the whole original dataset is replaced by multiple types of representative samples. Finally, the proposed algorithm is applied to 13 datasets and compared with 7 feature selection algorithms. Experimental results show that the multi-surrogate-assisted search mechanism can obviously reduce the running time of MDEFS, and the proposed ensemble approach can make MDEFS obtain better feature subsets, whose average accuracy is 0.72\% higher than the best comparison algorithm on each data set. All results indicate that MDEFS is a robust and competitive feature selection algorithm .},
  archive      = {J_ASOC},
  author       = {Zhi Jiang and Yong Zhang and Jun Wang},
  doi          = {10.1016/j.asoc.2021.107625},
  journal      = {Applied Soft Computing},
  pages        = {107625},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-surrogate-assisted dual-layer ensemble feature selection algorithm},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Competition-guided multi-neighborhood local search algorithm
for the university course timetabling problem. <em>ASOC</em>,
<em>110</em>, 107624. (<a
href="https://doi.org/10.1016/j.asoc.2021.107624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel competition-guided multi-neighborhood local search (CMLS) algorithm for solving the curriculum-based course timetabling problem. In comparison with the classical metaheuristic methods in the literature, the proposed algorithm includes three main contributions. Firstly, a new way of combining multiple neighborhoods is presented, according to which, only one neighborhood is selected at each iteration to make a trade-off between large search space and high computational efficiency. Secondly, two heuristic rules are proposed to determine the probabilities of selecting the neighborhood. Thirdly, a competition-based restart strategy is proposed, i.e., two SA-based multi-neighborhood local search procedures are implemented during the search process, and the elite one is selected from the two results obtained by the two procedures as the initial solution for the next round of search. The proposed algorithm is evaluated on the well-known benchmark instances, and the computational results show that the proposed algorithm is highly competitive compared with 6 state-of-the-art algorithms in the literature. Analysis of the essential ingredients of the proposed algorithm is also presented.},
  archive      = {J_ASOC},
  author       = {Ting Song and Mao Chen and Yulong Xu and Dong Wang and Xuekun Song and Xiangyang Tang},
  doi          = {10.1016/j.asoc.2021.107624},
  journal      = {Applied Soft Computing},
  pages        = {107624},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Competition-guided multi-neighborhood local search algorithm for the university course timetabling problem},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WOA-TLBO: Whale optimization algorithm with
teaching-learning-based optimization for global optimization and facial
emotion recognition. <em>ASOC</em>, <em>110</em>, 107623. (<a
href="https://doi.org/10.1016/j.asoc.2021.107623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Whale Optimization Algorithm (WOA) is a recently developed algorithm that is based on the chasing mechanism of humpback whales. Benefiting from the unique structure, WOA has virtuous global search capability. One of the drawbacks of this algorithm is the slow convergence rate that limits its real-world application. In resolving complicated global optimization problems , without any exertion for adequate fine-tuning preliminary constraints, Teaching-learning-based optimization (TLBO) is smooth to plunge into local optimal, but it has a fast convergence speed. By given the features of WOA and TLBO , an active hybrid WOA-TLBO algorithm is proposed for resolving optimization difficulties. To explore the enactment of the proposed WOA-TLBO algorithm, several experimentations are accompanied by regular benchmark test functions and compared with six other algorithms. The investigational outcomes indicate the more magnificent concert of the proposed WOA-TLBO algorithm for the benchmark function results. The proposed method has also been applied to the Facial Emotion Recognition (FER) functional problem. FER is the thought-provoking investigation zone that empowers us to classify the expression of the human face in everyday life. Centered on the portions’ actions in the human face, the maximum of the standard approaches fail to distinguish the expressions precisely as the expressions. In this paper, we have proposed FER’s productive process using WOA-TLBO based MultiSVNN (Multi-Support Vector Neural Network). Investigational outcomes deliver an indication of the virtuous enactment of the proposed technique resolutions in terms of accurateness.},
  archive      = {J_ASOC},
  author       = {A. Vijaya Lakshmi and P. Mohanaiah},
  doi          = {10.1016/j.asoc.2021.107623},
  journal      = {Applied Soft Computing},
  pages        = {107623},
  shortjournal = {Appl. Soft. Comput.},
  title        = {WOA-TLBO: Whale optimization algorithm with teaching-learning-based optimization for global optimization and facial emotion recognition},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven decision making based on evidential reasoning
approach and machine learning algorithms. <em>ASOC</em>, <em>110</em>,
107622. (<a href="https://doi.org/10.1016/j.asoc.2021.107622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large volumes of data have been accumulated in identical or very similar contexts of decision making. To generate accurate and explanatory decision recommendations by using these data, this paper proposes a data-driven multi-criteria decision making (MCDM) method based on machine learning (ML) algorithms and the evidential reasoning (ER) approach. In the method, based on the assessments of all historical alternatives, a comparison framework is designed to determine the most appropriate ML algorithm with the highest predictive accuracy . An optimization model is then constructed to connect the appropriate ML algorithm with the ER approach. Through the optimization model, the difference between overall assessments derived from the ER approach and the predicted results derived from the appropriate ML algorithm is minimized to learn criterion weights. The learned criterion weights are used to generate accurate and explanatory decisions. Such a combination takes advantage of high predictability of ML algorithms and favorable interpretability of the ER approach simultaneously. To demonstrate the validity and applicability of the proposed method, it is used to aid the diagnosis of thyroid nodules for a tertiary hospital located in Hefei, Anhui, China. Its merits are further highlighted by its comparison with two traditional ER approaches and the appropriate ML algorithm.},
  archive      = {J_ASOC},
  author       = {Chao Fu and Che Xu and Min Xue and Weiyong Liu and Shanlin Yang},
  doi          = {10.1016/j.asoc.2021.107622},
  journal      = {Applied Soft Computing},
  pages        = {107622},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven decision making based on evidential reasoning approach and machine learning algorithms},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of decision support tool for clustering urban
regional risk based on r-ArcGIS bridge. <em>ASOC</em>, <em>110</em>,
107621. (<a href="https://doi.org/10.1016/j.asoc.2021.107621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cities with dense population are susceptible and vulnerable to natural disasters and man-made accidents, making regional risk analysis thus significant for urban public safety. Although risk maps are able to present the broad characteristics of regional risk spatial distribution, quantitative identification of risk clusters in urban areas is still a challenge. In this context, we design and develop a user-friendly customized ArcGIS add-in in this study. In particular, a novel technical routing is proposed for customizing ArcGIS add-in tool based on R-ArcGIS Bridge. The design, architecture and implementation of the tool as well as its core functional modules are introduced. Moreover, R scripts are developed to implement K-means algorithm and Gap statistics validity index for clustering regional risk, respectively. Based on a case study of a typical urban district in China, we introduce the add-in’s functionalities as well as its related decision-making procedures. Results successfully obtained provide evidence that the tool is able to partition regional risk clusters and estimate the optimal number of clusters in urban areas. The simple loosely-coupled architecture also suggests a promising future for embedding some novel geospatial clustering algorithms to extend its capabilities in the next step. This work offers new insights on promoting future urban regional risk management with the use of GIS and clustering algorithms , and it provides a valuable demonstration on extending and enhancing the analysis capacity of GIS by harnessing its power with the statistical analysis capability of R packages through R-ArcGIS Bridge.},
  archive      = {J_ASOC},
  author       = {Ming Zhao and Xiang Liu and Zengfeng Sun},
  doi          = {10.1016/j.asoc.2021.107621},
  journal      = {Applied Soft Computing},
  pages        = {107621},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of decision support tool for clustering urban regional risk based on R-ArcGIS bridge},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WDO optimized detection for mammographic masses and its
diagnosis: A unified CAD system. <em>ASOC</em>, <em>110</em>, 107620.
(<a href="https://doi.org/10.1016/j.asoc.2021.107620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, breast cancer is the leading cause of cancer-related deaths among women. Early detection and diagnosis of breast cancer can effectively reduce the mortality rate. Masses are one of the manifestations of imperceptible breast cancer visible in mammograms whose detection and diagnosis is a challenging task due to its subtle nature. In this paper, a unified computer-aided detection/diagnosis scheme is proposed for the automatic detection and diagnosis of mammographic masses. Being simple and effective, multilevel image thresholding based on Otsu’s method is considered for localization of suspicious mass lesions, but its performance is compromised while defining the precise threshold value especially when the intensity profile in and around the anomalies does not vary much. To mitigate the issue, a nature-inspired optimization algorithm , wind driven optimization, is proposed which in combination with Otsu’s multilevel thresholding identifies the potential candidates of mass lesions. To reduce the false positives and diagnose the malignant masses, a texture-based multi-gradient local quinary pattern (M-GQP) feature is introduced which renders higher consistency in computing gradient information from the uniform and near-uniform areas. Due to eight distinct Sobel masks, it offers a better resolution in edge regions in addition to the micro information at different orientations. Four popular classifiers (support vector machine, Fisher’s linear discriminant , K-nearest neighbor, and an ensemble classifier) are also investigated to discriminate malignant tumors from benign lesions and normal tissues. The proposed integrated approach for the detection and characterization of mammographic masses is evaluated on two benchmark databases — mini-MIAS and DDSM comprising of 68 and 500 mammograms, respectively and achieved a sensitivity of 96.9\% and 96.2\% with 0.09 and 0.17 false positives per image after false positive reduction for the respective datasets. The diagnosis of malignant masses prior to and following false-positive reduction observes an A z Az value of 0.98 and 0.94 with an accuracy of 99.04\% and 92.65\%, respectively for the mini-MIAS dataset while the same for the DDSM are 0.99 and 0.92 with an accuracy of 98.33\% and 78.50\%, respectively. In a three-class classification (normal, benign and malignant), an accuracy of 99.04\% and 97.76\% with an F 1 F1 score of 0.98 and 0.97 for malignancy is obtained for the mini-MIAS and DDSM images, respectively which are promising in nature when compared to other competing schemes in the state-of-the-art.},
  archive      = {J_ASOC},
  author       = {Romesh Laishram and Rinku Rabidas},
  doi          = {10.1016/j.asoc.2021.107620},
  journal      = {Applied Soft Computing},
  pages        = {107620},
  shortjournal = {Appl. Soft. Comput.},
  title        = {WDO optimized detection for mammographic masses and its diagnosis: A unified CAD system},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extension of particle swarm optimization algorithm for
solving transportation problem in fuzzy environment. <em>ASOC</em>,
<em>110</em>, 107619. (<a
href="https://doi.org/10.1016/j.asoc.2021.107619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation problem is one of the most common real life optimization problems seeking to minimize the total transportation cost while delivering a product from a number of sources to a number of destinations. In this paper, the transportation costs have been considered as generalized trapezoidal fuzzy numbers representing the uncertainty therein whereas the supply and the demand levels are crisp numbers. Particle Swarm Optimization has been extended to solve this fuzzy transportation problem. To illustrate the proposed extension of PSO, two numerical examples from existing literature have been solved and the results have been compared with those of the existing approaches. A few randomly generated problems of different dimensions have also been solved and the convergence rate of the proposed PSO has also been studied with respect to the variants of inertia weight, population size and the number of iterations. It has been observed that the proposed PSO works efficiently to obtain the optimal solutions and also removes the barricades of the traditional solution techniques.},
  archive      = {J_ASOC},
  author       = {Gurwinder Singh and Amarinder Singh},
  doi          = {10.1016/j.asoc.2021.107619},
  journal      = {Applied Soft Computing},
  pages        = {107619},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extension of particle swarm optimization algorithm for solving transportation problem in fuzzy environment},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image steganography based on style transfer and quaternion
exponent moments. <em>ASOC</em>, <em>110</em>, 107618. (<a
href="https://doi.org/10.1016/j.asoc.2021.107618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the existing steganography methods can successfully embed secret information into the carrier image without introducing distortion into the appearance of the carrier image, the difference in distribution between the carrier image and stego image still cannot resist the detection by statistics-based steganalysis algorithms. To improve the capability of resisting steganalysis algorithms, an image steganography scheme based on style transfer and quaternion exponent moments is proposed in this paper. First, the geometric invariance of quaternion exponent moments is combined to accomplish the task of embedding secret information. Next, the style transfer is performed on a stego image embedded with the secret information, and the stylized image is used to transmit through the common channel. Then, the receiver attempts to remove the style from the stylized image and to restore the stylized image back to its original appearance. For this purpose, a de-stylized network is designed to reconstruct the stego image from the stylized image. Finally, an extracting algorithm is used to extract the transmitted secret image from the reconstructed stego image. In the steganography process, the main goal is to achieve that even when the appearance and distribution of the carrier image have been changed, that should also look like an independent and normal behavior to an eavesdropper. Extensive experiments are conducted to verify the feasibility of the proposed scheme. Experimental and analysis results indicate that the proposed scheme can generate an independent and meaningful image and successfully transmit a secret image and has the ability to extract a secret image at a low bit error rate. In addition, the proposed scheme provides high security.},
  archive      = {J_ASOC},
  author       = {Qi Li and Xingyuan Wang and Bin Ma and Xiaoyu Wang and Chunpeng Wang and Zhiqiu Xia and Yunqing Shi},
  doi          = {10.1016/j.asoc.2021.107618},
  journal      = {Applied Soft Computing},
  pages        = {107618},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image steganography based on style transfer and quaternion exponent moments},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BerConvoNet: A deep learning framework for fake news
classification. <em>ASOC</em>, <em>110</em>, 107614. (<a
href="https://doi.org/10.1016/j.asoc.2021.107614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news has become a major concern over the Internet. It influences people directly and should be identified. In the recent years, various Machine Learning (ML) and Deep Learning (DL) based data-driven approaches have been suggested for fake news classification. Most of the ML based approaches use hand-crafted features extracted from input textual content. Moreover, in DL based approaches, an efficient word embedding representation of input data is also a major concern. This paper presents a deep learning framework, BerConvoNet , to classify the given news text into fake or real with minimal error. The presented framework has two main building blocks : a news embedding block (NEB) and a multi-scale feature block (MSFB). NEB uses Bidirectional Encoder Representations from Transformers (BERT) for extracting word embeddings from a news article. Next, these embeddings are fed as an input to MSFB. The MSFB consists of multiple kernels (filters) of varying sizes. It extracts various features from news word embedding . The output of MSFB is fed as an input to a fully connected layer for classification. To validate the performance of BerConvoNet , several experiments have been performed on four benchmark datasets and various performance measures are used to evaluate the results. Furthermore, the ablative experiments with respect to news article embedding, kernel size, and batch size have been carried out to ensure the quality of prediction. Comparative analysis of the presented model is done with other state of the art models. It shows that BerConvoNet outplays other models on various performance metrics.},
  archive      = {J_ASOC},
  author       = {Monika Choudhary and Satyendra Singh Chouhan and Emmanuel S. Pilli and Santosh Kumar Vipparthi},
  doi          = {10.1016/j.asoc.2021.107614},
  journal      = {Applied Soft Computing},
  pages        = {107614},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BerConvoNet: A deep learning framework for fake news classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the diversity and robustness of parameterised
multi-objective test suites. <em>ASOC</em>, <em>110</em>, 107613. (<a
href="https://doi.org/10.1016/j.asoc.2021.107613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of optimisation algorithms heavily relies on comparing performance across benchmark problem suites. In continuous unconstrained multi-objective optimisation, the most popular suites are ZDT, DTLZ and WFG. For each problem in these suites, there are construction parameters that control characteristics such as the degree of multimodality or deceptiveness. Despite encouragement from the suites’ authors to do otherwise, experiments are largely performed using only the original values of these parameters. It is important to understand the robustness of these test problems, and their potential to create a diversity of challenging problem landscapes to guide future algorithm testing and development. In this paper we propose a methodology for evaluating robustness of the benchmark test problems by strategically varying construction parameters and exploring how problem difficulty and landscape characteristics are affected. Our methodology adopts both Latin Hyper-cube Sampling and a design and analysis of experiments model to construct more diverse problem instances within the benchmark problem classes. These problem variants are evaluated for eight diverse multi-objective optimisation algorithms to contribute to our understanding of problem robustness. We measure robustness of problems indirectly in terms of impacts on algorithm performance and rankings, and directly in terms of Exploratory landscape Analysis (ELA) metrics that are used to establish problem robustness from a landscape characteristics perspective. Our results show that only eleven of the 21 benchmark problems are robust for algorithms in absolute terms, nine in relative terms, and seven which provide evidence of both types of algorithm robustness. There are also nine problems which satisfy requirements for landscape robustness. Of these, only four of the 21 benchmark problem classes are robust across all measures. These results highlight the importance of diversity in selecting benchmark problems, as the majority of the test suite problems, if only default construction parameters are considered, do not support robust conclusions to be drawn in general about how algorithms perform in the presence of various constructed characteristics intended to challenge algorithms. The existing benchmark test problems are currently insufficient for understanding algorithm performance, certainly with the popularly used default parameters , and more efforts in generating diverse problem instances would serve the research community well.},
  archive      = {J_ASOC},
  author       = {Estefania Yap and Mario Andrés Muñoz and Kate Smith-Miles},
  doi          = {10.1016/j.asoc.2021.107613},
  journal      = {Applied Soft Computing},
  pages        = {107613},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the diversity and robustness of parameterised multi-objective test suites},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust supervised rough granular description model with the
principle of justifiable granularity. <em>ASOC</em>, <em>110</em>,
107612. (<a href="https://doi.org/10.1016/j.asoc.2021.107612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, granular computing has been developed as a unified data description paradigm. As a popular soft computing supervised learning model, rough sets theory-based data description approach has been intensively investigated in data mining research . Feasible information granulation and approximation approaches have been recognized as two key features of data descriptors in rough sets. In this study, we propose a Dempster–Shafer theory-based rough granular description model based on a principle of justifiable granularity . First, we apply evidence information to show the performance of information granules generated from various data density regions, and definitions of lower and upper approximation sets are discussed considering characteristics of data credibility and plausibility, respectively. Furthermore, we propose a robust rough description model to identify some extreme instances, such as outlier and noise instances. Moreover, a set of pseudo labels is provided to enhance the robustness of the proposed model. Finally, to search for an optimum granularity , justifiable granularity is quantified from the perspectives of legitimacy and interpretability , and then optimized by a particle swarm optimization algorithm . Extensive comparative experiments with several representative rough granular description models illustrate that the proposed model achieves almost all the best approximation quality, number difference, and neighborhood credibility values. These experimental results demonstrate that the proposed approach is reasonable, effective, and robust, and is a promising rough granular description model for complex data in real-world applications.},
  archive      = {J_ASOC},
  author       = {Hengrong Ju and Weiping Ding and Xibei Yang and Hamido Fujita and Suping Xu},
  doi          = {10.1016/j.asoc.2021.107612},
  journal      = {Applied Soft Computing},
  pages        = {107612},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust supervised rough granular description model with the principle of justifiable granularity},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Particle swarm optimization of partitions and fuzzy order
for fuzzy time series forecasting of COVID-19. <em>ASOC</em>,
<em>110</em>, 107611. (<a
href="https://doi.org/10.1016/j.asoc.2021.107611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major hyperparameters which affect fuzzy time series (FTS) forecasting are the number of partitions, length of partition intervals in the universe of discourse, and the fuzzy order. There are very few studies which have considered an integrated solution to optimize all the hyperparameters. In this paper, we strive to achieve optimum values of all three hyperparameters for fuzzy time series forecasting of the COVID-19 pandemic using the Particle Swarm Optimization (PSO) algorithm. We specifically propose two techniques, namely nested FTS-PSO and exhaustive search FTS-PSO for determining the optimal interval length, as an augmentation to the FTS-PSO model that optimizes the interval length and the fuzzy order. Nested PSO has two PSO loops: (i) the inner PSO optimizes the combination of fuzzy order and boundaries of intervals for a given number of partitions defined by the outer loop, and the resultant cost is fed back to the outer PSO; (ii) the outer PSO optimizes the number of partitions to reduce the cost while meeting the defined constraint. Exhaustive search FTS-PSO also has two loops where the inner loop is similar to nested FTS-PSO while the outer loop iterates over a pre-defined search space of number of partitions. We analyze the effectiveness of the two approaches by comparing with ARIMA, FbProphet, and the state-of-the-art FTS and FTS-PSO models. We adopt COVID-19 highly affected 10 countries worldwide to perform forecasting of coronavirus confirmed cases. We consider two phases of COVID-19 spread, one from the year 2020 and another from 2021. Our study provides an analytical aspect of the COVID-19 pandemic, and aims to achieve optimal number and length of intervals along with fuzzy order for FTS forecasting of COVID-19. The results prove that the exhaustive search FTS-PSO outperformed all the methods whereas nested FTS-PSO performed moderately well.},
  archive      = {J_ASOC},
  author       = {Naresh Kumar and Seba Susan},
  doi          = {10.1016/j.asoc.2021.107611},
  journal      = {Applied Soft Computing},
  pages        = {107611},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Particle swarm optimization of partitions and fuzzy order for fuzzy time series forecasting of COVID-19},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of smart camera systems based on artificial
intelligence network for social distance detection to fight against
COVID-19. <em>ASOC</em>, <em>110</em>, 107610. (<a
href="https://doi.org/10.1016/j.asoc.2021.107610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, an artificial intelligence network-based smart camera system prototype, which tracks social distance using a bird’s-eye perspective, has been developed. “MobileNet SSD-v3”, “Faster-R-CNN Inception-v2”, “Faster-R-CNN ResNet-50” models have been utilized to identify people in video sequences. The final prototype based on the Faster R-CNN model is an integrated embedded system that detects social distance with the camera. The software developed using the “Nvidia Jetson Nano” development kit and Raspberry Pi camera module calculates all necessary actions in itself, detects social distance violations, makes audible and light warnings, and reports the results to the server. It is predicted that the developed smart camera prototype can be integrated into public spaces within the “sustainable smart cities, ” the scope that the world is on the verge of a change.},
  archive      = {J_ASOC},
  author       = {Onur Karaman and Adi Alhudhaif and Kemal Polat},
  doi          = {10.1016/j.asoc.2021.107610},
  journal      = {Applied Soft Computing},
  pages        = {107610},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of smart camera systems based on artificial intelligence network for social distance detection to fight against COVID-19},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Glucose forecasting using genetic programming and latent
glucose variability features. <em>ASOC</em>, <em>110</em>, 107609. (<a
href="https://doi.org/10.1016/j.asoc.2021.107609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a set of genetic programming methods to obtain accurate predictions of subcutaneous glucose values from diabetic patients. We explore the usefulness of different features that identify the latent glucose variability. New features, including average glucose, glucose variability and glycemic risk, are generated as input variables of the genetic programming algorithm in order to improve the accuracy of the models in the prediction phase. The performance of traditional genetic programming, and models created with classified glucose values, are compared to those using latent glucose variability features. We experimented with a set of 18 different features and we also performed a study of the importance of the variables in the models. The Bayesian statistical analysis shows that the use of glucose variability as latent variables improved the predictions of the models, not only in terms of RMSE, but also in the reduction of dangerous predictions, i.e., those predictions that could lead to wrong decisions in the clinical practice.},
  archive      = {J_ASOC},
  author       = {Sergio Contador and J. Manuel Velasco and Oscar Garnica and J. Ignacio Hidalgo},
  doi          = {10.1016/j.asoc.2021.107609},
  journal      = {Applied Soft Computing},
  pages        = {107609},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Glucose forecasting using genetic programming and latent glucose variability features},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph convolutional networks for enhanced resolution 3D
electrical capacitance tomography image reconstruction. <em>ASOC</em>,
<em>110</em>, 107608. (<a
href="https://doi.org/10.1016/j.asoc.2021.107608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three dimensional Electrical Capacitance Tomography (3D ECT) is an inexpensive tool for diagnosing non-conductive components of industrial processes. Although relatively mature, it still requires much work to improve its inverse nature of imaging capability. In particular, high resolution 3D ECT image reconstruction is very time-consuming and computationally heavy, and the best-known 3D ECT image reconstruction techniques have already reached their limits. Thus, there is a strong need to change a direction towards modern computational intelligence solutions. Therefore, this work proposes using graph convolutional networks (GCN) to raise the 3D ECT image quality. Mainly, it takes advantage of GCN’s ability to effectively use specific geometrical relationships hidden in the finite modeling unstructured grids commonly used to build 3D ECT images. These relationships are first encoded by a graph representing an ECT volumetric finite element grid. A GCN is next trained in a graph-to-graph framework with pairs of graphs representing high-quality nonlinear image reconstruction results as input and a simulated phantom as output. As a result, a trained GCN model fed with lower resolution 3D ECT image enhances its quality and spatial resolution. Tomographic image quality and resolution enhancement was evaluated using normalized mean square error and Pearson correlation coefficient, which improved by 35.5\% and 3.74\%, respectively.},
  archive      = {J_ASOC},
  author       = {Anna Fabijańska and Robert Banasiak},
  doi          = {10.1016/j.asoc.2021.107608},
  journal      = {Applied Soft Computing},
  pages        = {107608},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph convolutional networks for enhanced resolution 3D electrical capacitance tomography image reconstruction},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep reinforcement learning and parameter transfer based
approach for the multi-objective agile earth observation satellite
scheduling problem. <em>ASOC</em>, <em>110</em>, 107607. (<a
href="https://doi.org/10.1016/j.asoc.2021.107607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The agile earth observation satellite scheduling problem (AEOSSP) consists of selecting and scheduling a number of tasks from a set of user requests in order to optimize one or multiple criteria. In this paper, we consider a multi-objective version of AEOSSP (called MO-AEOSSP) where the failure rate and the timeliness of scheduled requests are optimized simultaneously. Due to its NP-hardness, traditional iterative problem-tailored heuristic methods are sensitive to problem instances and require massive computational overhead. We thus propose a deep reinforcement learning and parameter transfer based approach (RLPT) to tackle the MO-AEOSSP in a non-iterative manner. RLPT first decomposes the MO-AEOSSP into a number of scalarized sub-problems by a weight sum approach where each sub-problem can be formulated as a Markov Decision Process (MDP). RLPT then applies an encoder–decoder structure neural network (NN) trained by a deep reinforcement learning procedure to producing a high-quality schedule for each sub-problem. The resulting schedules of all scalarized sub-problems form an approximate pareto front for the MO-AEOSSP. Once a NN of a subproblem is trained, RLPT applies a parameter transfer strategy to reducing the training expenses for its neighboring sub-problems. Experimental results on a large set of randomly generated instances show that RLPT outperforms three classical multi-objective evolutionary algorithms (MOEAs) in terms of solution quality, solution distribution and computational efficiency. Results on various-size instances also show that RLPT is highly general and scalable. To the best of our knowledge, this study is the first attempt that applies deep reinforcement learning to a satellite scheduling problem considering multiple objectives.},
  archive      = {J_ASOC},
  author       = {Luona Wei and Yuning Chen and Ming Chen and Yingwu Chen},
  doi          = {10.1016/j.asoc.2021.107607},
  journal      = {Applied Soft Computing},
  pages        = {107607},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning and parameter transfer based approach for the multi-objective agile earth observation satellite scheduling problem},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensembles of priority rules for resource constrained project
scheduling problem. <em>ASOC</em>, <em>110</em>, 107606. (<a
href="https://doi.org/10.1016/j.asoc.2021.107606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource constrained project scheduling problem is an NP-hard problem that attracts many researchers because of its complexity and daily use. In literature there are a lot of various solving methods for this problem. The priority rules are one of the prominent methods used in practice. Because of their simplicity, speed, and possibility to react to changes in the system, they can be used in a dynamic environment. In this paper, ensembles of priority rules were created to improve the performance of priority rules created with genetic programming . For ensemble creation, four different methods will be considered: simple ensemble combination, BagGP, BoostGP, and cooperative coevolution. The priority rules that are part of the ensemble will be combined with the sum and vote methods in reaching the final decision. Additionally, the ensemble subset search method will be applied to the created ensembles to find the optimal subset of priority rules. The results achieved in this paper show that ensembles of priority rules can achieve significantly better results than those achieved when using only a single priority rule.},
  archive      = {J_ASOC},
  author       = {Mateja Đumić and Domagoj Jakobović},
  doi          = {10.1016/j.asoc.2021.107606},
  journal      = {Applied Soft Computing},
  pages        = {107606},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensembles of priority rules for resource constrained project scheduling problem},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-robot path-planning algorithm for autonomous
navigation using meta-reinforcement learning based on transfer learning.
<em>ASOC</em>, <em>110</em>, 107605. (<a
href="https://doi.org/10.1016/j.asoc.2021.107605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptability of multi-robot systems in complex environments is a hot topic. Aiming at static and dynamic obstacles in complex environments, this paper presents dynamic proximal meta policy optimization with covariance matrix adaptation evolutionary strategies (dynamic-PMPO-CMA) to avoid obstacles and realize autonomous navigation . Firstly, we propose dynamic proximal policy optimization with covariance matrix adaptation evolutionary strategies (dynamic-PPO-CMA) based on original proximal policy optimization (PPO) to obtain a valid policy of obstacles avoidance. The simulation results show that the proposed dynamic-PPO-CMA can avoid obstacles and reach the designated target position successfully. Secondly, in order to improve the adaptability of multi-robot systems in different environments, we integrate meta-learning with dynamic-PPO-CMA to form the dynamic-PMPO-CMA algorithm. In training process, we use the proposed dynamic-PMPO-CMA to train robots to learn multi-task policy. Finally, in testing process, transfer learning is introduced to the proposed dynamic-PMPO-CMA algorithm. The trained parameters of meta policy are transferred to new environments and regarded as the initial parameters. The simulation results show that the proposed algorithm can have faster convergence rate and arrive the destination more quickly than PPO, PMPO and dynamic-PPO-CMA.},
  archive      = {J_ASOC},
  author       = {Shuhuan Wen and Zeteng Wen and Di Zhang and Hong Zhang and Tao Wang},
  doi          = {10.1016/j.asoc.2021.107605},
  journal      = {Applied Soft Computing},
  pages        = {107605},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-robot path-planning algorithm for autonomous navigation using meta-reinforcement learning based on transfer learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ANN-based ensemble model for change point estimation in
control charts. <em>ASOC</em>, <em>110</em>, 107604. (<a
href="https://doi.org/10.1016/j.asoc.2021.107604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signaling in the control charts is usually followed by a substantial amount of delay, in which precise identification of the time when a change has occurred in a process simplifies the removal of change causes. This problem is referred to as change point (CP) estimation in the literature. This paper proposes a novel ensemble model to estimate CP under different processes’ changes in the phase II applications, known as ANNCP. It uses an evolutionary artificial neural network (ANN) as an underlying reasoning scheme to combine the predictions of multiple techniques and make the final decision. Specifically, a hybrid model of genetic algorithm (GA) and simulated annealing (SAN) with a new loss function is used to optimize the weights of ANN. The experimental results indicate that ANNCP can produce promising results under different conditions as compared with other state-of-the-art methods reported in the literature.},
  archive      = {J_ASOC},
  author       = {Ali Yeganeh and Farhad Pourpanah and Alireza Shadman},
  doi          = {10.1016/j.asoc.2021.107604},
  journal      = {Applied Soft Computing},
  pages        = {107604},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ANN-based ensemble model for change point estimation in control charts},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparative empirical study on constraint handling in
offline data-driven evolutionary optimization. <em>ASOC</em>,
<em>110</em>, 107603. (<a
href="https://doi.org/10.1016/j.asoc.2021.107603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are some practical optimization problems that can be only optimized using historical data, which is known as offline data-driven optimization problems . Since the real function evaluations are not available in the optimization process, surrogate models must replace the real fitness evaluations to guide the search. The key issue in offline data-driven optimization is how to make full use of offline data for building robust and accurate surrogate models . However, the existing offline algorithms do not consider optimization problems with constraints. Since the predicted values of the surrogate models for constraints are inaccurate, it becomes difficult to balance the objective function and the constraints during the optimization process. In this paper, we discuss a number of commonly used constraint handling techniques and combine them with an offline data-driven evolutionary algorithm . Also, four different test functions are designed with various constraints and difficulties. The results on the test function show that the multi-objective-based constraint handling technique is more likely to obtain feasible solutions, while stochastic ranking has better quality of feasible solutions.},
  archive      = {J_ASOC},
  author       = {Pengfei Huang and Handing Wang},
  doi          = {10.1016/j.asoc.2021.107603},
  journal      = {Applied Soft Computing},
  pages        = {107603},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comparative empirical study on constraint handling in offline data-driven evolutionary optimization},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D robotic navigation using a vision-based deep
reinforcement learning model. <em>ASOC</em>, <em>110</em>, 107602. (<a
href="https://doi.org/10.1016/j.asoc.2021.107602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address a problem of vision-based 3D robotic navigation using deep reinforcement learning for an Autonomous Underwater Vehicle (AUV). Our research offers conclusions from the experimental study based on one of the RoboSub 2018 competition tasks. However, it can be generalized to any navigation task consisting of movement from a starting point to the front of the next station. The presented reinforcement learning-based model predicts the robot’s steering settings using the data acquired from the robot’s sensors. Its Vision Module may be based on a built-in convolutional network or a pre-trained TinyYOLO network so that a comparison of various levels of features’ complexity is possible. To enable evaluation of the proposed solution, we prepared a test environment imitating the real conditions. It provides the ability to steer the agent simulating the AUV and calculate values of rewards, used for training the model by evaluating its decisions. We study the solution in terms of the reward function form, the model’s hyperparameters and the exploited camera images processing method , and provide an analysis of the correctness and speed of the model’s functioning. As a result, we obtain a valid model able to steer the robot from the starting point to the destination based on visual cues and inputs from other sensors.},
  archive      = {J_ASOC},
  author       = {P. Zieliński and U. Markowska-Kaczmar},
  doi          = {10.1016/j.asoc.2021.107602},
  journal      = {Applied Soft Computing},
  pages        = {107602},
  shortjournal = {Appl. Soft. Comput.},
  title        = {3D robotic navigation using a vision-based deep reinforcement learning model},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical deep reinforcement learning to drag heavy
objects by adult-sized humanoid robot. <em>ASOC</em>, <em>110</em>,
107601. (<a href="https://doi.org/10.1016/j.asoc.2021.107601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most research on robot manipulation focuses on objects that are light enough for the robot to pick them up. However, in our daily life, some objects are too big or too heavy to be picked up or carried, so that dragging them is necessary. Although bipedal humanoid robots have nowadays good mobility on level ground, dragging unfamiliar objects including large and heavy objects on various surfaces is an interesting research area with many applications, which will provide insights into human manipulation and will encourage the development of novel algorithms for robot motion planning and control. This is a challenging problem, not only because of the unknown and potentially variable friction of the foot, but also because the feet of the robot may slip during unbalanced poses. In this paper, we propose a novel hierarchical deep learning algorithm that learns how to drag heavy objects with an adult-sized humanoid robot for the first time. First, we present a Three-layered Convolution Volumetric Network (TCVN) for 3D object classification with point clouds volumetric occupancy grid integration. Second, we propose a lightweight real-time instance segmentation method named Tiny-YOLACT for the detection and classification of the floor surface. Third, we propose a deep Q-learning algorithm to learn the policy control of the Center of Mass of the robot (DQL-COM). The DQL-COM algorithm learning is bootstrapped using the ROS Gazebo simulator. After initial training, we complete training on the THORMANG-Wolf, a 1.4 m tall adult-sized humanoid robot with 27 degrees of freedom and weighing 48 kg, on three distinct types of surfaces. We evaluate the performance of our approach by dragging eight different types of objects (e.g., a small suitcase, a large suitcase, a chair). The extensive experiments (480 times on the real robot) included dragging a heavy object with a mass of 84.6 kg (two times greater than the robot’s weight) and showed remarkable success rates of 92.92\% when combined with the force–torque sensors, and 83.75\% without force–torque sensors.},
  archive      = {J_ASOC},
  author       = {Saeed Saeedvand and Hanjaya Mandala and Jacky Baltes},
  doi          = {10.1016/j.asoc.2021.107601},
  journal      = {Applied Soft Computing},
  pages        = {107601},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical deep reinforcement learning to drag heavy objects by adult-sized humanoid robot},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based c-BiLSTM for fake news detection.
<em>ASOC</em>, <em>110</em>, 107600. (<a
href="https://doi.org/10.1016/j.asoc.2021.107600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms have radically transformed the creation and dissemination of news. Users can easily access this news in a fast and efficient manner. However, some users might post negative and fraudulent content in the form of comments or posts. Such content can constitute a threat to an individual or an organization. Therefore, the identification of fake news has become a major research field in natural language processing (NLP). The main challenge is to determine whether the news is real or fake. In this paper, we propose an attention-based convolutional bidirectional long short-term memory (AC-BiLSTM) approach for detecting fake news and classifying them into six categories. The evaluation of our proposed approach on a benchmarked dataset shows a significant improvement in accuracy rate in comparison with other existing classification models . In particular, this work contributes to the progress in the field of detecting fake news and confirms the feasibility of our proposed approach in classifying fake news on social media.},
  archive      = {J_ASOC},
  author       = {Tina Esther Trueman and Ashok Kumar J. and Narayanasamy P. and Vidya J.},
  doi          = {10.1016/j.asoc.2021.107600},
  journal      = {Applied Soft Computing},
  pages        = {107600},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-based C-BiLSTM for fake news detection},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A support vector regression (SVR)-based method for dynamic
load identification using heterogeneous responses under interval
uncertainties. <em>ASOC</em>, <em>110</em>, 107599. (<a
href="https://doi.org/10.1016/j.asoc.2021.107599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A support vector regression (SVR)-based method is presented in this paper, which aims to reconstruct the uncertain dynamic load using heterogeneous responses. Essentially, the technology of load identification can be regarded as an issue of nonlinear regression . Considering the incomplete limitation of the single response signal, the complicated relationship between the dynamic load and heterogeneous responses for a specific structure may be determined through some support vectors in simulated training samples based on the SVR algorithm. In view of multi-source uncertainties, an interval dimension-by-dimension (D–D) method is investigated, which can approximate the load changing concerning the each-dimensional uncertain parameter by Legendre orthogonal polynomials. Then, the dynamic load boundaries can be calculated through the maxima/minima of the approximation polynomial in the time history. The validity and specificity of the proposed methodology are clarified by 3 numerical examples. In addition, the results indicate that the proposed method can be utilized to identify the interval of dynamic load with outstanding accuracy and efficiency.},
  archive      = {J_ASOC},
  author       = {Yaru Liu and Lei Wang and Kaixuan Gu},
  doi          = {10.1016/j.asoc.2021.107599},
  journal      = {Applied Soft Computing},
  pages        = {107599},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A support vector regression (SVR)-based method for dynamic load identification using heterogeneous responses under interval uncertainties},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum marine predators algorithm for addressing multilevel
image segmentation. <em>ASOC</em>, <em>110</em>, 107598. (<a
href="https://doi.org/10.1016/j.asoc.2021.107598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a modified marine predators algorithm based on quantum theory to handle the multilevel image segmentation problem. The main aims of using quantum theory is to enhance the ability of marine predators algorithm to find the optimal threshold levels to enhance the segmentation process . The proposed quantum marine predators algorithm gets the idea of finding a particle in the space based on a possible function borrowed from the Schrodinger wave function that determines the position of each particle at any time. This rectification in the search mechanism of the marine predators algorithm contributes to strengthening of exploration and exploitation of the algorithm. To analyze the performance of the proposed algorithm, we conduct a set of experiments. In the first experiment, the results of the developed quantum marine predators algorithm are compared with eight well-known meta-heuristics based on benchmark test functions. The second experiment demonstrates the applicability of the algorithm, in addressing multilevel threshold image segmentation . A set of ten gray-scale images assess the quality of the quantum marine predators algorithm and its performance is compared with other meta-heuristic algorithms. The experimental results show that the proposed algorithm performs well compared with other algorithms in terms of convergence and the quality of segmentation.},
  archive      = {J_ASOC},
  author       = {Mohamed Abd Elaziz and Davood Mohammadi and Diego Oliva and Khodakaram Salimifard},
  doi          = {10.1016/j.asoc.2021.107598},
  journal      = {Applied Soft Computing},
  pages        = {107598},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum marine predators algorithm for addressing multilevel image segmentation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the performance of deep learning for numerical
optimization: An application to protein structure prediction.
<em>ASOC</em>, <em>110</em>, 107596. (<a
href="https://doi.org/10.1016/j.asoc.2021.107596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have recently drawn considerable attention to build and evaluate artificial learning models for perceptual tasks. On the other hand, optimization is the problem of selecting a set of element to find an optimal/near optimal criterion. Here, we present a study on the performance of the deep learning models to deal with global optimization problems . More precisely, we would like to learn how to optimize the problems by using the machine learning techniques . Different from proposing very large networks with GPU computational burden and long training time, we focus on searching for lightweight implementations to find the best architecture. The performance of NAS is first analyzed through empirical experiments on CEC 2017 benchmark suite. Thereafter, it is applied to a set of protein structure prediction (PSP) problems. The experiments reveal that the generated learning models can achieve competitive results when compared to hand-designed algorithms; given enough computational budget.},
  archive      = {J_ASOC},
  author       = {Hojjat Rakhshani and Lhassane Idoumghar and Soheila Ghambari and Julien Lepagnot and Mathieu Brévilliers},
  doi          = {10.1016/j.asoc.2021.107596},
  journal      = {Applied Soft Computing},
  pages        = {107596},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the performance of deep learning for numerical optimization: An application to protein structure prediction},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ELM-based adaptive neuro swarm intelligence techniques for
predicting the california bearing ratio of soils in soaked conditions.
<em>ASOC</em>, <em>110</em>, 107595. (<a
href="https://doi.org/10.1016/j.asoc.2021.107595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes novel integration of extreme learning machine (ELM) and adaptive neuro swarm intelligence (ANSI) techniques for the determination of California bearing ratio (CBR) of soils for the subgrade layers of railway tracks, a critical real-time problem of geotechnical engineering . Particle swarm optimization (PSO) with adaptive and time-varying acceleration coefficients (TAC) was employed to optimize the learning parameters of ELM. Three novel ELM-based ANSI models, namely ELM coupled-modified PSO (ELM-MPSO), ELM coupled-TAC PSO (ELM-TPSO), and ELM coupled-improved PSO (ELM-IPSO) were developed for predicting the CBR of soils in soaked conditions. Compared to standard PSO (SPSO), the modified and improved version of PSO are capable of converging to a high-quality solution at early iterations. A detailed comparison was made between the proposed models and other conventional soft computing techniques , such as conventional ELM, artificial neural network , genetic programming , support vector machine , group method of data handling , and three ELM-based swarm intelligence optimized models (ELM-based grey wolf optimization , ELM-based slime mould algorithm, and ELM-based Harris hawks optimization). Experimental results reveal that the proposed ELM-based ANSI models can attain the most accurate prediction and confirm the dominance of MPSO over SPSO. Considering the consequences and robustness of the proposed models, it can be concluded that the newly constructed ELM-based ANSI models, especially ELM-MPSO, can solve the difficulties in tuning the acceleration coefficients of SPSO by the trial-and-error method for predicting the CBR of soils and be further applied to other real-time problems of geotechnical engineering .},
  archive      = {J_ASOC},
  author       = {Abidhan Bardhan and Pijush Samui and Kuntal Ghosh and Amir H. Gandomi and Siddhartha Bhattacharyya},
  doi          = {10.1016/j.asoc.2021.107595},
  journal      = {Applied Soft Computing},
  pages        = {107595},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ELM-based adaptive neuro swarm intelligence techniques for predicting the california bearing ratio of soils in soaked conditions},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning and boosted trees for injuries prediction in
power infrastructure projects. <em>ASOC</em>, <em>110</em>, 107587. (<a
href="https://doi.org/10.1016/j.asoc.2021.107587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrical injury impacts are substantial and massive. Investments in electricity will continue to increase, leading to construction project complexities, which undoubtedly contribute to injuries and associated effects. Machine learning (ML) algorithms are used to quantify and model causes of injuries; however, conventional ML techniques do not produce optimal results since they require careful engineering to transform data into suitable forms. In this study, we develop and compare state-of-the-art ML algorithms (deep learning and boosted trees) with other conventional methods to overcome this problem by analyzing incident cases obtained from a leading UK power infrastructure company. The predictive performance of the developed models was benchmarked using a statistical comparison between observations and predicted values. Results showed that the implementation of deep feedforward neural networks achieved the best predictions (accuracy = = 0.967 and Cohen Kappa measure = = 0.964). Furthermore, we conduct a sensitivity analysis to determine the effect of input parameters and data sizes on the modes’ behavior. The sensitivity analysis results showed strong generalization abilities of the deep learning and boosted tree models and their effectiveness for safety risks management .},
  archive      = {J_ASOC},
  author       = {Ahmed Oyedele and Anuoluwapo Ajayi and Lukumon O. Oyedele and Juan Manuel Davila Delgado and Lukman Akanbi and Olugbenga Akinade and Hakeem Owolabi and Muhammad Bilal},
  doi          = {10.1016/j.asoc.2021.107587},
  journal      = {Applied Soft Computing},
  pages        = {107587},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning and boosted trees for injuries prediction in power infrastructure projects},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic optimal control parameters with incomplete
information on design variables using a heuristic algorithm.
<em>ASOC</em>, <em>110</em>, 107586. (<a
href="https://doi.org/10.1016/j.asoc.2021.107586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertain structural performance and cost ineffectiveness are two major barriers in applying active control. Fuzzy theory is often adopted to handle the difficulty of insufficient statistical information. To fully capture the uncertainty, a probabilistic based optimization framework, 3P3MSOS (3-Parameter 3-Moment Symbiotic Optimization Search), is proposed to find the optimal controller parameters satisfying the predefined reliability target. To overcome the issue of lacking probability distribution function (PDF), only the first three moments from sample data is used. To lessen the computational burden, dimension reduction technique is adopted. To obtain a fast and an accurate reliability index, the limit state is converted to the 3-Parameter Lognormal distribution of an equivalent single variable function. The first three moments of the corresponding function are obtained using Gaussian–Hermite integration. SOS metaheuristic algorithm is adopted to minimize the required control force considering probabilistic constraints . To demonstrate the proposed framework in the absence of PDF, common state feedback such as the pole placement method and linear quadratic regulator (LQR) for a 6-story building with 14 random variables is provided. Accuracy of the calculated reliability is verified with Monte Carlo Simulation . Compared to Monte-Carlo Simulation (MCS), error induced by 3P3M is around 10\% if COV is less than 0.15. Meanwhile, the time taken by 3P3M is only 8E-4 times the time taken by MCS. 3P3MSOS takes the least time to find a converged solution that fulfills the constraint requirement (i.e., β = 3 β=3 .0) among three investigated algorithms. Results indicate that the proposed framework is able to deliver a promising controller design , in which the reliability calculation is accurate and affordable.},
  archive      = {J_ASOC},
  author       = {Kuo-Wei Liao and John Thedy},
  doi          = {10.1016/j.asoc.2021.107586},
  journal      = {Applied Soft Computing},
  pages        = {107586},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Probabilistic optimal control parameters with incomplete information on design variables using a heuristic algorithm},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Duo-LDL method for label distribution learning based on
pairwise class dependencies. <em>ASOC</em>, <em>110</em>, 107585. (<a
href="https://doi.org/10.1016/j.asoc.2021.107585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label Distribution Learning (LDL) is a new learning paradigm with numerous applications in various domains. It is a generalization of both standard multiclass classification and multilabel classification. Instead of a binary value, in LDL, each label is assigned a real number which corresponds to a degree of membership of the object being classified to a given class. In this paper a new neural network approach to Label Distribution Learning (Duo-LDL), which considers pairwise class dependencies, is introduced. The method is extensively tested on 15 well-established benchmark sets, against 6 evaluation measures, proving its competitiveness to state-of-the-art non-neural LDL approaches. Additional experimental results on artificially generated data demonstrate that Duo-LDL is especially effective in the case of most challenging benchmarks, with extensive input feature representations and numerous output classes.},
  archive      = {J_ASOC},
  author       = {Adam Żychowski and Jacek Mańdziuk},
  doi          = {10.1016/j.asoc.2021.107585},
  journal      = {Applied Soft Computing},
  pages        = {107585},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Duo-LDL method for label distribution learning based on pairwise class dependencies},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discrete cuckoo search algorithms for test case
prioritization. <em>ASOC</em>, <em>110</em>, 107584. (<a
href="https://doi.org/10.1016/j.asoc.2021.107584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression testing is an essential aspect of the software development lifecycle . As the software evolves, the test suite grows, hence the cost and effort to retest the software. Test case prioritization is one of the mitigation techniques for regression testing. It ranks the test cases to maximize the desired properties, e.g., detecting faults early. The efficiency and effectiveness of test case prioritization techniques can be enhanced using optimization algorithms . Nature-inspired algorithms are gaining more attention due to their easy implementation and quality of the solutions. This paper proposes the discrete cuckoo search algorithm for test case prioritization. The prioritization problem deals with ordering the test cases. Therefore, a new adaptation strategy using asexual genetic reproduction is introduced to convert real numbers into permutation sequences. Furthermore, the cuckoo search algorithm’s effectiveness is extended with the genetic algorithm’s mutation operator to balance the trade-off between exploration and exploitation. An in-depth comparative study on four case studies is conducted between the proposed algorithms, existing state-of-the-art algorithms and baseline approach. Statistical investigation confirms that the proposed hybrid cuckoo search algorithm outperforms the genetic algorithm , particle swarm optimization , ant colony optimization , tree seed algorithm and random search by 4.29\%, 5.52\%, 8.38\%, 2.74\% and 10.80\%, respectively.},
  archive      = {J_ASOC},
  author       = {Anu Bajaj and Om Prakash Sangwan},
  doi          = {10.1016/j.asoc.2021.107584},
  journal      = {Applied Soft Computing},
  pages        = {107584},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete cuckoo search algorithms for test case prioritization},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable data-driven optimization for complex systems
with non-preferential multiple outputs using belief rule base.
<em>ASOC</em>, <em>110</em>, 107581. (<a
href="https://doi.org/10.1016/j.asoc.2021.107581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To better handle problems with non-preferential multi-outputs (NPMO), a new approach is proposed in this study by employing the belief rule base (BRB) to provide a superior nonlinearity modeling ability as well as good explainability. The new approach is thus called NPMO–BRB. First, a new optimization model is constructed where the optimization objective is the integration of multi-outputs and respective constraints are designed. Then, a new optimization algorithm with a new customized gene makeup is designed where the NPMO–BRB inferencing process is embedded in the fitness calculation procedure. A practical case study on Changsha Metro Line 4 is studied to use multiple geological parameters to infer multiple operational parameters. Case study results show that NPMO–BRB has shown superior performance in comparison with the random forest (RF), the backpropagation neural network (BPNN), the Gradient Gaussian Process (GPR), as well as multiple separate BRBs. Owing to the explainability provided by the NPMO–BRB approach, further investigations into the belief distribution comparison reveal more information that can be used as practical work guidelines.},
  archive      = {J_ASOC},
  author       = {Leilei Chang and Limao Zhang},
  doi          = {10.1016/j.asoc.2021.107581},
  journal      = {Applied Soft Computing},
  pages        = {107581},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable data-driven optimization for complex systems with non-preferential multiple outputs using belief rule base},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ensemble deep learning method as data fusion system for
remote sensing multisensor classification. <em>ASOC</em>, <em>110</em>,
107563. (<a href="https://doi.org/10.1016/j.asoc.2021.107563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the great achievements in designing remote sensing sensors, the extraction of useful information from multisource remote sensing data remains a challenging problem. Most of the recent research projects have applied single deep learning systems for data fusion and classification. The idea of using ensemble deep learning algorithms through a multisensor fusion system can improve the performance of data fusion tasks. In this research, however, a multi-sensor classification strategy, which is based on deep learning ensemble procedure and decision fusion framework, is investigated for the fusion of Light Detection and Ranging (LiDAR), Hyperspectral Images (HS), and very high-resolution Visible (Vis) images. This research proposes a basic classifier based on deep Convolutional Neural Network (CNN) in which the softmax layer is replaced by a Support Vector Machine (CNN-SVM). Then, a random feature selection is applied to generate two separate CNN-SVM ensemble systems, one for LiDAR and Vis and the other one for HS data. To overcome the similarity and overfitting between the deep features and the classifiers provided by two ensemble systems and to select the best subsets of the classifiers, two diversity measures select the most diverse combinations of the classifiers. Finally, a decision fusion method combines the obtained diverse classifiers from CNN ensembles. Results demonstrate that the proposed method achieves higher accuracy, and its performance outperforms some of the existing methods. The proposed ensemble CNN method improved single deep CNN, random forest , and Adaboost between 2\% to 10\% in terms of classification accuracy .},
  archive      = {J_ASOC},
  author       = {Behnaz Bigdeli and Parham Pahlavani and Hamed Amini Amirkolaee},
  doi          = {10.1016/j.asoc.2021.107563},
  journal      = {Applied Soft Computing},
  pages        = {107563},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble deep learning method as data fusion system for remote sensing multisensor classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph relation network for person counting in construction
site using UAV. <em>ASOC</em>, <em>110</em>, 107562. (<a
href="https://doi.org/10.1016/j.asoc.2021.107562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its portability and maneuverability , Unmanned Aerial Vehicles (UAVs) are increasingly used in industrial fields. Our work aims to develop a framework of person counting executed by an UAV, which can count the total number of persons accurately. To solve the problem of multiple counts for the same person, this work proposes a novel Graph Similarity-based Person Counting Network (GSPCN) which consists of three modules: person detection network, person re-identification module, and person counting module. To begin with, we detect the bounding boxes and the corresponding images for each object in image sequence. Secondly, we calculate the visual similarity between persons. And then, each person is taken as the root node to construct a graph, then we calculate the similarity between different graphs as the graph similarity. After that, we comprehensively consider the visual similarity and graph similarity to determine whether the person is repeated. Finally, by removing all duplicate persons, we can get the total number of persons. The proposed framework is tested in a real-world scenario and it empirically outperforms the existing state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Zun Liu and Xiaonan Hu and Jianqiang Li and Jie Chen and Wenlian Huang and Xiaoyu Zhao and Victor C.M. Leung},
  doi          = {10.1016/j.asoc.2021.107562},
  journal      = {Applied Soft Computing},
  pages        = {107562},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph relation network for person counting in construction site using UAV},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online support vector quantile regression for the dynamic
time series with heavy-tailed noise. <em>ASOC</em>, <em>110</em>,
107560. (<a href="https://doi.org/10.1016/j.asoc.2021.107560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an online support vector quantile regression approach with an ɛ ɛ ɛ -insensitive pinball loss function, called Online-SVQR, for dynamic time series with heavy-tailed noise. Online-SVQR is robust to heavy-tailed noise, as it can control the negative influence of heavy-tailed noise by using a quantile parameter. By using an incremental learning algorithm to update the new samples, the coefficients of Online-SVQR reflect the dynamic information in the examined time series. During each incremental training process, the nonsupport vector is ignored while the support vector continues training with new updated samples. Online-SVQR can select useful training samples and discard irrelevant samples. As a result, the training speed of Online-SVQR is accelerated. Experimental results on one artificial dataset and three real-world datasets indicate that Online-SVQR outperforms ɛ ɛ ɛ -support vector quantile regression in terms of both sample selection ability and training speed.},
  archive      = {J_ASOC},
  author       = {Yafen Ye and Yuanhai Shao and Chunna Li and Xiangyu Hua and Yanru Guo},
  doi          = {10.1016/j.asoc.2021.107560},
  journal      = {Applied Soft Computing},
  pages        = {107560},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online support vector quantile regression for the dynamic time series with heavy-tailed noise},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptual similarity measurement based on generative
adversarial neural networks in graphics design. <em>ASOC</em>,
<em>110</em>, 107548. (<a
href="https://doi.org/10.1016/j.asoc.2021.107548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the similarity between images is of paramount importance in computer vision . However, the commonly used pixelwise similarity metrics do not match well with perceptual similarity. The purpose of this paper is to propose a visual similarity measurement method, which can be effectively used for plagiarism detection in graphic design . Plagiarism detection of designs refers to the identification and determination of major similarities. It is difficult to carry out the similarity learning process in traditional deep neural network due to the insufficient of training samples. To overcome this problem, a novel scheme is proposed for measuring perceptual similarity of graphics by using a constraint Generative Adversarial Network (GAN) model. The generator of GAN is used to create similar graphics following the common plagiarism features of logo design. Unlike the traditional discriminator which judges the authenticity of the generated image and the original image, the modified discriminator is used to calculate the perceptual similarity of the graphics pair. In graphics design, plagiarism mainly focuses on the changes of shape, color and style, which has certain cognitive subjectivity. Therefore, design experts were invited to participate in a group of cognitive analysis experiments. A perceptual constraint model is established to limit the generation of plagiarized graphics according to “design and visual rationality”. Promising results demonstrate that the proposed method can be used for plagiarism detection of logo design. Given its effectiveness and conceptual simplicity, I hope it can serve as a baseline and contribute to the future research on plagiarism detection of artworks.},
  archive      = {J_ASOC},
  author       = {Bin Yang},
  doi          = {10.1016/j.asoc.2021.107548},
  journal      = {Applied Soft Computing},
  pages        = {107548},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Perceptual similarity measurement based on generative adversarial neural networks in graphics design},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The minimum latency in location routing fuzzy inventory
problem for perishable multi-product materials. <em>ASOC</em>,
<em>110</em>, 107543. (<a
href="https://doi.org/10.1016/j.asoc.2021.107543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper analyzes the integrated fuzzy model with minimal latency in the location routing inventory of perishable multi-product materials, taking into account environmental constraints. Due to the corrupt nature of some items, minimizing the latency is very important in designing the supply chain systems of perishable items. On the other hand, considering environmental constraints provides a model for adapting as much as possible to real-world conditions. For this purpose, according to the above, a multi-period model was designed with three main objective functions, including minimizing the total supply chain costs, minimizing networks time, and minimizing the amount of pollution caused by chain activities. Due to the complex nature of the problem, the need to use metaheuristic approaches is inevitable. Therefore, two genetic algorithms NSGA II and PESA II were used in large, medium and small scales and the Epsilon constraint approach was used for small-scale problems. The results of the performance of algorithms based on standard indicators show better performance of NSGA II in small sizes and higher efficiency of PESA II in medium and large dimensions in terms of solution time and proximity to the Pareto boundary. Also, according to the results obtained through sensitivity analysis on the two main parameters of the problem, it was found that increasing the duration of corruption does not change costs and pollution, but increases the time of arrival of goods. On the other hand, by reducing the cost of transporting goods, we see a significant reduction in costs, but there is no change in the second and third objective functions, i.e. the time of arrival of goods and pollution.},
  archive      = {J_ASOC},
  author       = {Samaneh Daroudi and Hamed Kazemipoor and Esmaeel Najafi and Mohammad Fallah},
  doi          = {10.1016/j.asoc.2021.107543},
  journal      = {Applied Soft Computing},
  pages        = {107543},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The minimum latency in location routing fuzzy inventory problem for perishable multi-product materials},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fixed set search application for minimizing the makespan on
unrelated parallel machines with sequence-dependent setup times.
<em>ASOC</em>, <em>110</em>, 107521. (<a
href="https://doi.org/10.1016/j.asoc.2021.107521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of minimizing the makespan on unrelated parallel machines with sequence-dependent setup times. The term unrelated machines is used in the sense that there is no correlation between the processing times for jobs between different machines. Due to the NP-hardness of this problem, a wide range of metaheuristics has been developed to find near-optimal solutions. Out of such methods, the ones based on constructive greedy algorithms like the Greedy Randomized Adaptive Search Procedure (GRASP), Ant Colony Optimization (ACO) and Worm Optimization (WO) proved to be most efficient. The Fixed Set Search (FSS) is a novel population-based metaheuristic of this type that adds a learning mechanism to the GRASP. The basic concept of FSS is to avoid focusing on specific high quality solutions but on parts or elements that such solutions have. This is done through fixing a set of elements that exist in such solutions and dedicating computational effort to finding near-optimal solutions for the underlying subproblem . In this work, the FSS is applied to the problem of interest. Computational experiments show that the FSS manages to significantly outperform the GRASP, ACO and WO on the standard benchmark instances when the quality of found solutions is considered without an increase in computational cost. This application of the FSS is significant as it shows that it can also be applied to scheduling type problems, in addition to covering and routing ones.},
  archive      = {J_ASOC},
  author       = {Raka Jovanovic and Stefan Voß},
  doi          = {10.1016/j.asoc.2021.107521},
  journal      = {Applied Soft Computing},
  pages        = {107521},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fixed set search application for minimizing the makespan on unrelated parallel machines with sequence-dependent setup times},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learned fuzzy control for inertial sensing: Micro
electro mechanical systems. <em>ASOC</em>, <em>109</em>, 107597. (<a
href="https://doi.org/10.1016/j.asoc.2021.107597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a new fuzzy logic controller (FLC) for micro-electro-mechanical-system gyroscopes (MEMS-Gs). The nonlinearities and unknown dynamics are modeled by the designed non-singleton type-3 fuzzy system (NT3FS) and an adaptive control scheme is presented. To improve the control accuracy, the tracking error dynamics are identified by a deep learned Boltzmann machine (RBM) and the nonlinear model predictive controller (NMPC) is designed by the optimized RBM model. Finally, the approximation errors are tackled by an adaptive compensator. The parameters of RBM are learned by contrastive divergence (CD) method and rules of NT3FS are optimized by the tuning laws that are obtained through the stability investigation. In various simulations and comparisons with some other conventional FLCs the superiority of the designed controller is shown. A good tracking accuracy of chaotic references and well robustness performance are obtained by the suggested control scheme.},
  archive      = {J_ASOC},
  author       = {Ardashir Mohammadzadeh and Reza Hadjiaghaie Vafaie},
  doi          = {10.1016/j.asoc.2021.107597},
  journal      = {Applied Soft Computing},
  pages        = {107597},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep learned fuzzy control for inertial sensing: Micro electro mechanical systems},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal allocation of photovoltaic/wind energy system in
distribution network using meta-heuristic algorithm. <em>ASOC</em>,
<em>109</em>, 107594. (<a
href="https://doi.org/10.1016/j.asoc.2021.107594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, allocation of hybrid photovoltaic panels , wind turbines and battery storage (PV/WT/BA) system in distribution network is presented aimed active losses cost minimization , voltage profile enhancement and minimizing power purchased from the hybrid system by the network. Meta-heuristic improved whale optimizer algorithm (IWOA) is used to determine the optimal location and size of the PV/WT/BA system components as decision variables. The conventional WOA is inspired by social behavior and the hunting of humpback whales and in this study its performance is improved by using crossover and mutation operators of differential evolution (DE) method to avoid getting caught in local optimal and reinforcement to achieve global optimal. The methodology is implemented on IEEE 33 bus network considering seasonal variations. The results indicated that optimal determination of the decision variables in the network minimizes the active losses cost, voltage deviations and cost of power purchased from the hybrid system by the network using IWOA. The superiority of the IWOA is confirmed in benchmark test functions solution with very competitive results and also better results in statistic analysis and higher convergence speed and accuracy in comparison with the WOA , DE and particle swarm optimization (PSO). Also the results showed that the highest losses and better voltage are related to the summer and the lowest values are obtained in autumn season. The solar panels only participate in energy generation in spring and summer seasons, cost of power purchased by the network is highest in summer and lowest in autumn season. Moreover, the network is placed in stable current and voltage condition and the network power losses and voltage deviations are minimized.},
  archive      = {J_ASOC},
  author       = {Armin Arasteh and Payam Alemi and M. Beiraghi},
  doi          = {10.1016/j.asoc.2021.107594},
  journal      = {Applied Soft Computing},
  pages        = {107594},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal allocation of photovoltaic/wind energy system in distribution network using meta-heuristic algorithm},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fuzzy α-similarity relation-based attribute reduction
approach in incomplete interval-valued information systems.
<em>ASOC</em>, <em>109</em>, 107593. (<a
href="https://doi.org/10.1016/j.asoc.2021.107593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As generalizations of single-valued information systems, interval-valued information systems (IVISs) can better express the real data with uncertainty in some applications. Attribute reduction methods for complete IVISs or complete interval-valued decision systems (IVDSs) have been developed. However, there are few researches on attribute reduction for incomplete interval-valued information systems (IIVISs). The paper aims to investigate the attribute reduction issue in IIVISs. Firstly, the maximal and minimal distances, which characterize the difference between two interval values, are defined, and the maximal and minimal similarity degrees are given. Secondly, the fuzzy α α -similarity relation is defined based on similarity between interval values, and the concept of α α -equivalence relation is raised. Thirdly, entropy measures are investigated for IIVISs in view of α α -equivalence relations. Fourthly, a new attribute reduction approach for IIVISs is proposed by using conditional entropy , and its corresponding algorithm is given. Finally, experiments to verify the effectiveness and feasibility of the newly proposed approach for attribute reduction in IIVISs are presented. These results will be helpful to perfect the uncertainty measurement model, and provide an approach for attribute reduction in IIVISs.},
  archive      = {J_ASOC},
  author       = {Xiaofeng Liu and Jianhua Dai and Jiaolong Chen and Chucai Zhang},
  doi          = {10.1016/j.asoc.2021.107593},
  journal      = {Applied Soft Computing},
  pages        = {107593},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy α-similarity relation-based attribute reduction approach in incomplete interval-valued information systems},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Short-term prediction of COVID-19 spread using grey rolling
model optimized by particle swarm optimization. <em>ASOC</em>,
<em>109</em>, 107592. (<a
href="https://doi.org/10.1016/j.asoc.2021.107592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of the spread of coronavirus disease 2019 (COVID-19) is vital in taking preventive and control measures to reduce human health damage. The Grey Modelling (1, 1) is a popular approach used to construct a predictive model with a small-sized dataset.​ In this study, a hybrid model based on grey prediction and rolling mechanism optimized by particle swarm optimization algorithm (PSO) was applied to create short-term estimates of the total number of confirmed COVID-19 cases for three countries, Germany , Turkey, and the USA. A rolling mechanism that updates data in equal dimensions was applied to improve the forecasting accuracy of the models. The PSO algorithm was used to optimize the Grey Modelling parameters (1, 1) to provide more robust and efficient solutions with minimum errors. To compare the accuracy of the predictive models, a nonlinear autoregressive neural network (NARNN) was also developed. According to the analysis results, Grey Rolling Modelling (1, 1) optimized by PSO algorithm performs better than the classical Grey Modelling (1, 1), Grey Rolling Modelling (1, 1), and NARNN models for predicting the total number of confirmed COVID-19 cases. The present study can provide an important basis for countries to allocate health resources and formulate epidemic prevention policies effectively.},
  archive      = {J_ASOC},
  author       = {Zeynep Ceylan},
  doi          = {10.1016/j.asoc.2021.107592},
  journal      = {Applied Soft Computing},
  pages        = {107592},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term prediction of COVID-19 spread using grey rolling model optimized by particle swarm optimization},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating multi-source transfer learning, active learning
and metric learning paradigms for time series prediction. <em>ASOC</em>,
<em>109</em>, 107583. (<a
href="https://doi.org/10.1016/j.asoc.2021.107583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Time Series Prediction (TSP) algorithms assume that the training and testing data follow the same distribution and a large amount of data can be obtained. However, the time-varying nature of time series data is a very difficult problem, which will lead to the inconsistent distribution between new data and old data. In TSP problem scenario, how to transfer knowledge in a relatively long-time span effectively is a serious challenge. To address this issue, in this work, a novel Multi-Source Active Metric Transfer Learning (MS-AMTL) algorithm, integrating Multi-Source Transfer Learning , Active Learning, and Metric Learning paradigms, is designed to make the most of, instead of throw away directly, the long-ago data, with effect, along with its precise mathematic derivation. The setting of multi-source domains can make the dependability of domains and the similarity of sources be fully exerted, reducing the impact of negative transfer. The Active Learning (AL) module in MS-AMTL is designed to select the most representative instances, so that dependability of the domains can be ensured and information redundancy can be reduced. The Metric Learning (ML) module in MS-AMTL is developed to measure the similarity between instances and then the similarity of sources, more effectively. Extensive experiments on one artificial dataset, three real-world datasets and four financial datasets proved the superiority of our proposed algorithm in transfer learning and prediction performance in comparison with several other state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Qitao Gu and Qun Dai and Huihui Yu and Rui Ye},
  doi          = {10.1016/j.asoc.2021.107583},
  journal      = {Applied Soft Computing},
  pages        = {107583},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating multi-source transfer learning, active learning and metric learning paradigms for time series prediction},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MODE-CNN: A fast converging multi-objective optimization
algorithm for CNN-based models. <em>ASOC</em>, <em>109</em>, 107582. (<a
href="https://doi.org/10.1016/j.asoc.2021.107582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been used to solve many problems in computer science with a high level of success, and have been applied in many fields in recent years. However, most of the designs of these models are still tuned manually; obtaining the highest performing CNN model is therefore very time-consuming, and is sometimes not achievable. Recently, researchers have started using optimization algorithms for the automatic adjustment of the hyper-parameters of CNNs. In particular, single-objective optimization algorithms have been used to achieve the highest network accuracy for the design of a CNN. When these studies are examined, it can be seen that the most significant problem in the optimization of the parameters of a CNN is that a great deal of time is required for tuning. Hence, optimization algorithms with high convergence rates are needed for the parameter optimization of deep networks. In this study, we first develop an algorithm called MODE-CNN, based on the multi-objective differential evolution (MODE) algorithm for parameter optimization of CNN or CNN-based methods. MODE-CNN is then compared with four different multi-objective optimization algorithms. This comparison is carried out using 16 benchmark functions and four different metrics, with 100 independent runs. It is observed that the algorithm is robust and competitive compared to alternative approaches, in terms of its accuracy and convergence. Secondly, the MODE-CNN algorithm is used in the parameter optimization of a CNN-based method, developed previously by the authors, for the segmentation and classification of medical images. In this method, there are three parameters that influence the test time and accuracy: the general stride (GS), neighbour distance (ND), and patch accuracy (PA). These parameters need to be optimized to give the highest possible accuracy and lowest possible test time. With the MODE-CNN algorithm, the most appropriate GS, ND, and PA values are obtained for the test time and accuracy. As a result, it is observed that the MODE-CNN algorithm is successful, both in comparison with multi-objective algorithms and in the parameter optimization of a CNN-based method.},
  archive      = {J_ASOC},
  author       = {Özkan İni̇k and Mustafa Altıok and Erkan Ülker and Barış Koçer},
  doi          = {10.1016/j.asoc.2021.107582},
  journal      = {Applied Soft Computing},
  pages        = {107582},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MODE-CNN: A fast converging multi-objective optimization algorithm for CNN-based models},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlinear-based chaotic harris hawks optimizer: Algorithm
and internet of vehicles application. <em>ASOC</em>, <em>109</em>,
107574. (<a href="https://doi.org/10.1016/j.asoc.2021.107574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harris Hawks Optimizer (HHO) is one of the many recent algorithms in the field of metaheuristics . The HHO algorithm mimics the cooperative behavior of Harris Hawks and their foraging behavior in nature called surprise pounce. HHO benefits from a small number of controlling parameters setting, simplicity of implementation, and a high level of exploration and exploitation. To alleviate the drawbacks of this algorithm, a modified version called Nonlinear based Chaotic Harris Hawks Optimization (NCHHO) is proposed in this paper. NCHHO uses chaotic and nonlinear control parameters to improve HHO’s optimization performance . The main goal of using the chaotic maps in the proposed method is to improve the exploratory behavior of HHO. In addition, this paper introduces a nonlinear control parameter to adjust HHO’s exploratory and exploitative behaviors. The proposed NCHHO algorithm shows an improved performance using a variety of chaotic maps that were implemented to identify the most effective one, and tested on several well-known benchmark functions . The paper also considers solving an Internet of Vehicles (IoV) optimization problem that showcases the applicability of NCHHO in solving large-scale, real-world problems. The results demonstrate that the NCHHO algorithm is very competitive, and often superior, compared to the other algorithms. In particular, NCHHO provides 92\% better results in average to solve the uni-modal and multi-modal functions with problem dimension sizes of D = 30 and 50, whereas, with respect to the higher dimension problem, our proposed algorithm shows 100\% consistent improvement with D = 100 and 1000 compared to other algorithms. In solving the IoV problem, the success rate was 62.5\%, which is substantially better in comparison with the state-of-the-art algorithms. To this end, the proposed NCHHO algorithm in this paper demonstrates a promising method to be widely used by different applications, which brings benefits to industries and businesses in solving their optimization problems experienced daily , such as resource allocation, information retrieval, finding the optimal path for sending data over networks, path planning , and so many other applications.},
  archive      = {J_ASOC},
  author       = {Amin Abdollahi Dehkordi and Ali Safaa Sadiq and Seyedali Mirjalili and Kayhan Zrar Ghafoor},
  doi          = {10.1016/j.asoc.2021.107574},
  journal      = {Applied Soft Computing},
  pages        = {107574},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Nonlinear-based chaotic harris hawks optimizer: Algorithm and internet of vehicles application},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fuzzy granular sparse learning model for identifying
antigenic variants of influenza viruses. <em>ASOC</em>, <em>109</em>,
107573. (<a href="https://doi.org/10.1016/j.asoc.2021.107573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse learning has significant applications in statistics, big data, bioinformatics and machine learning . In big data systems , a large amount of redundant, missing and noisy data cause sparsity , and the rapid changes of information result in uncertainty. Since the traditional sparse learning model is difficult to deal with uncertain data, we propose a Fuzzy Granular Sparse Learning (FGSL) model for identifying antigenic variants of influenza viruses. Firstly, a fuzzy set theory is introduced to measure and granulate the influenza viruses. Some fuzzy granules are induced by a single feature fuzzy granulation. Then, a fuzzy granular vector is constructed from these fuzzy granules, and the fuzzy granular regression is presented. Some constraint norms for granules and granular vectors are proposed, which are two granule norms and four granular vector norms. Therefore, the FGSL model is constructed based on granular regression and constraint norms. The FGSL model includes granular ridge and lasso regressions under different constraint norms. Furthermore, we prove the derivative forms of two granular regression functions , guaranteeing the convergence of the FGSL model. The optimization problem of the FGSL model is discussed and two gradient descent algorithms of the FGSL model are designed. Finally, we employ the FGSL model to serologic data and hemagglutinin sequences for learning antigenicity-associated mutations and inferring antigenic variants. The experimental results confirm some advantages of the FGSL model with fast convergence, low RMSE and strong feature selection ability. We successfully identify antigenic variants of influenza viruses by the FGSL model.},
  archive      = {J_ASOC},
  author       = {Yumin Chen and Zhiwen Cai and Lei Shi and Wei Li},
  doi          = {10.1016/j.asoc.2021.107573},
  journal      = {Applied Soft Computing},
  pages        = {107573},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy granular sparse learning model for identifying antigenic variants of influenza viruses},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-precision linearized interpretation for fully connected
neural network. <em>ASOC</em>, <em>109</em>, 107572. (<a
href="https://doi.org/10.1016/j.asoc.2021.107572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the widespread application of deep neural networks in finance, medical treatment, and autonomous driving , these networks face multiple security threats, such as maliciously constructed adversarial samples that can easily mislead deep neural network model classification, causing errors. Therefore, creating an interpretable model or designing an interpretation method is necessary to improve its security. This paper presents an interpretation scheme, named Convergent Interpretation for Deep Neural Networks (CIDNN), to obtain a provably convergent and consistent interpretation for deep neural networks . The main idea of CIDNN is to first convert the deep neural networks into a set of mathematically convergent Piecewise Linear Neural Networks (PLNN), then convert the PLNN into a set of equivalent linear classifiers . In this way, each linear classifier can be interpreted by its decision features. By analyzing the convergence of the local approximation interpretation scheme, we prove that this interpretable model can be sufficiently close to the deep neural network with certain conditions. Experiments show the convergence of CIDNN’s interpretation, and the interpretation conforms with similar samples in the synthetic dataset . Besides, we demonstrate the semantical meaning of CIDNN in the Fashion-MNIST dataset.},
  archive      = {J_ASOC},
  author       = {Xia Lei and Yongkai Fan and Kuan-Ching Li and Arcangelo Castiglione and Qian Hu},
  doi          = {10.1016/j.asoc.2021.107572},
  journal      = {Applied Soft Computing},
  pages        = {107572},
  shortjournal = {Appl. Soft. Comput.},
  title        = {High-precision linearized interpretation for fully connected neural network},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A cerebral microbleed diagnosis method via FeatureNet and
ensembled randomized neural networks. <em>ASOC</em>, <em>109</em>,
107567. (<a href="https://doi.org/10.1016/j.asoc.2021.107567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cerebral microbleed (CMB) is a type of biomarker, which is related to cerebrovascular diseases. In this paper, a novel computer aided diagnosis method for CMB detection was presented. Firstly, sliding neighborhood algorithm was used to generate CMB and non-CMB samples from brain susceptibility weighted images. Then, a 15-layer proposed FeatureNet was trained for extracting features from the input samples. Afterwards, structure after the first fully connected layer in FeatureNet was replaced by three randomized neural networks for classification: Schmidt neural network, random vector functional-link net, and extreme learning machine , and the weights and biases in early layers of FeatureNet were frozen during the training of those three classifiers. Finally, the output of the three classifiers was ensemble by majority voting mechanism to get better classification performance. In our experiment, five-fold cross validation was employed for evaluation. Results revealed that our FeatureNet-SNN, FeatureNet-RVFL and FeatureNet-ELM yielded accuracy of 98.22\%, 98.23\%, and 97.54\%, respectively, and the ensembled FeatureNet-EN improved the accuracy to 98.60\%, which outperformed several existing state-of-the-art approaches. The proposed FeatureNet-EN model could provide accurate CMB detection, and thus reduce death tolls. Impact Statement — We propose a 15-layer FeatureNet to detect cerebral microbleed (CMB). We propose three FeatureNet variants: FeatureNet-SNN, FeatureNet-RVFL and FeatureNet-ELM. We use ensemble learning to combine three FeatureNet variants, and generate a FeatureNet-EN. The proposed FeatureNet-SNN, FeatureNet-RVFL and FeatureNet-ELM yielded accuracy of 98.22\%, 98.23\%, and 97.54\%, respectively, and the ensembled FeatureNet-EN improved the accuracy to 98.60\%, better than state-of-the-art methods. This method could provide accurate CMB detection, and thus reduce death tolls.},
  archive      = {J_ASOC},
  author       = {Si-Yuan Lu and Deepak Ranjan Nayak and Shui-Hua Wang and Yu-Dong Zhang},
  doi          = {10.1016/j.asoc.2021.107567},
  journal      = {Applied Soft Computing},
  pages        = {107567},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cerebral microbleed diagnosis method via FeatureNet and ensembled randomized neural networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Yellow swarm: LED panels to advise optimal alternative tours
to drivers in the city of malaga. <em>ASOC</em>, <em>109</em>, 107566.
(<a href="https://doi.org/10.1016/j.asoc.2021.107566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic congestion is one of the main concerns of citizens living in big cities. In this article we propose a novel system, called Yellow Swarm, with the aim of improving road traffic at a low cost and easy implementation by using a set of LED panels placed throughout the city in order to spread road traffic over its streets. The system is optimally configured by a new epigenetic algorithm which calculates the time slots for each suggested detour shown in the panels. Yellow Swarm was tested on three scenarios of the city of Malaga, Spain, using the real city layout and actual traffic flows calculated by our flow generator algorithm with data published by the city’s authorities. Our results show shorter average travel times (up to 19 s) and a reduction of greenhouse gas emissions (up to 4.5\%) and fuel consumption (up to 1.6\%), which have been achieved without increasing the maximum travel times (reduced by 205 s). Yellow Swarm, as a distributed traffic management system in the city, represents a major improvement that benefits drivers on long trips by reducing travel times and preventing traffic jams.},
  archive      = {J_ASOC},
  author       = {Daniel H. Stolfi and Enrique Alba},
  doi          = {10.1016/j.asoc.2021.107566},
  journal      = {Applied Soft Computing},
  pages        = {107566},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Yellow swarm: LED panels to advise optimal alternative tours to drivers in the city of malaga},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Eye state detection based on weight binarization convolution
neural network and transfer learning. <em>ASOC</em>, <em>109</em>,
107565. (<a href="https://doi.org/10.1016/j.asoc.2021.107565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of eye state can assist the related work in the field of computer vision such as face recognition, expression recognition, pose estimation and human–computer interaction. This paper proposes an Weight Binarization Convolution Neural Network and Transfer Learning (WBCNNTL) based eye state detection method, in which the WBCNNTL is composed of deep convolution neural network and the weight is binarized. The human eye state features can be extracted by Convolutional Neural Network (CNN) effectively, and binary network not only speeds up the computation, but also helps to reduce the storage space and fewer parameters of the model. Transfer learning applies the knowledge or patterns learned from the source domain to different but related fields or problems, which improves the training efficiency of the new model. Experiments on eye state detection are performed using Closed Eyes in the wild (CEW), FER2013 and Zhejiang University Eyeblink (ZJU) Databases, from which the experiment results show the average accuracy obtained by our method are 97.41\% on CEW and are 97.15\% on ZJU, the computing speed of binary network is faster than non-binary network. Moreover, our method requires less storage space due to lightweight binary model, which maintains better detection capability on CEW compared with some state-of-the-art works.},
  archive      = {J_ASOC},
  author       = {Zhen-Tao Liu and Cheng-Shan Jiang and Si-Han Li and Min Wu and Wei-Hua Cao and Man Hao},
  doi          = {10.1016/j.asoc.2021.107565},
  journal      = {Applied Soft Computing},
  pages        = {107565},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Eye state detection based on weight binarization convolution neural network and transfer learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Load frequency control of a microgrid employing a 2D sine
logistic map based chaotic sine cosine algorithm. <em>ASOC</em>,
<em>109</em>, 107564. (<a
href="https://doi.org/10.1016/j.asoc.2021.107564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a maiden application of a two dimensional Sine Logistic map based chaotic sine cosine algorithm (2D-SLCSCA) optimized classical PID controller for load frequency control (LFC) of an islanded microgrid (MG). In comparison to random variables and 1D chaotic sequences , the 2D chaotic sequences are more ergodic and possess a wider chaotic range, thereby enhancing the global convergence speed and search capability of an algorithm. Initially, the proposed 2D-SLCSCA is tested on eight classical benchmark test functions and its performance is compared with 1D Logistic map based chaotic SCA (1D-LCSCA), 1D Sine map based chaotic SCA (1D-SCSCA), and the SCA incorporating random variables . Test results reveal that the proposed algorithm exhibits better convergence characteristics, statistics, and execution time. Finally, the proposed 2D-SLCSCA is implemented for the LFC analysis of the islanded MG . To establish the competence of the proposed algorithm in this regard, its performance is compared with 2D Hénon map based chaotic SCA, 2D Lozi map based chaotic SCA, improved salp swarm algorithm (ISSA), SCA, grey wolf optimizer (GWO), and particle swarm optimization (PSO) algorithm considering diverse load disturbance patterns in the MG . Simulation results affirm that the proposed control scheme augments the frequency response of the MG exhibiting a maximum percentage improvement of 78.89\%, 78.86\%, and 96.51\% in peak overshoot ( O S p e a k OSpeak ), peak undershoot ( U S p e a k USpeak ) and objective function ( O F I T S E OFITSE ) value, respectively as compared to the other algorithms. Furthermore, sensitivity of the proposed 2D-SLCSCA is validated considering ± ± 30\% variation in the MG parameters.},
  archive      = {J_ASOC},
  author       = {Bhuvnesh Khokhar and Surender Dahiya and K.P. Singh Parmar},
  doi          = {10.1016/j.asoc.2021.107564},
  journal      = {Applied Soft Computing},
  pages        = {107564},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Load frequency control of a microgrid employing a 2D sine logistic map based chaotic sine cosine algorithm},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning-based forecasting of firemen ambulances’
turnaround time in hospitals, considering the COVID-19 impact.
<em>ASOC</em>, <em>109</em>, 107561. (<a
href="https://doi.org/10.1016/j.asoc.2021.107561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When ambulances’ turnaround time (TT) in emergency departments is prolonged, it not only affects the victim severely but also causes unavailability of resources in emergency medical services (EMSs) and, consequently, leaves a locality unprotected. This problem may worsen with abnormal situations, e.g., the current coronavirus disease 2019 (COVID-19) pandemic. Taking this into consideration, this paper presents a first study on the COVID-19 impact on ambulances’ TT by analyzing historical data from the Departmental Fire and Rescue Service of the Doubs (SDIS 25), in France, for three hospitals. Because the TTs of SDIS 25 ambulances increased, this paper also calculated and analyzed the number of breakdowns in services, which augmented due to shortage of ambulances that return on service in time. It is, therefore, vital to have a decision-support tool to better reallocate resources by knowing the time EMSs ambulances and personnel will be in use. Thus, this paper proposes a novel two-stage methodology based on machine learning (ML) models to forecast the TT of each ambulance in a given time and hospital. The first stage uses a multivariate model of regularly spaced time series to predict the average TT (AvTT) per hour, which considers temporal variables and external ones (e.g., COVID-19 statistics, weather data). The second stage utilizes a multivariate irregularly spaced time series model, which considers temporal variables of each ambulance departure, type of intervention, external variables, and the previously predicted AvTT as inputs. Four state-of-the-art ML models were considered in this paper, namely, Light Gradient Boosted Machine, Multilayer Perceptron, Long Short-Term Memory, and Prophet. As shown in the results, the proposed methodology provided remarkable results for practical purposes. The AvTT accuracies obtained for the three hospitals were 90.16\%, 97.02\%, and 93.09\%. And the TT accuracies were 74.42\%, 86.63\%, and 76.67\%, all with an error margin of ± ± 10 min.},
  archive      = {J_ASOC},
  author       = {Selene Cerna and Héber H. Arcolezi and Christophe Guyeux and Guillaume Royer-Fey and Céline Chevallier},
  doi          = {10.1016/j.asoc.2021.107561},
  journal      = {Applied Soft Computing},
  pages        = {107561},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine learning-based forecasting of firemen ambulances’ turnaround time in hospitals, considering the COVID-19 impact},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective fake news detection method using WOA-xgbTree
algorithm and content-based features. <em>ASOC</em>, <em>109</em>,
107559. (<a href="https://doi.org/10.1016/j.asoc.2021.107559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the fast development of the internet and online platforms such as social media feeds, news blogs, and online newspapers, deceptive reports have been universally spread online. This manipulated news is a matter of concern due to its potential role in shaping public opinion. Therefore, the fast spread of fake news creates an urgent need for automatic systems to detect deceitful articles. This motivates many researchers to introduce solutions for the automatic classification of news items. This paper proposed a novel system to detect fake news articles based on content-based features and the WOA-Xgbtree algorithm. The proposed system can be applied in different scenarios to classify news articles. The proposed approach consists of two main stages: first, the useful features are extracted and analyzed, and then an Extreme Gradient Boosting Tree (xgbTree) algorithm optimized by the Whale Optimization Algorithm (WOA) to classify news articles using extracted features. In our experiments, we considered the bases of the investigation on classification accuracy and the F1-measure. Then, we compared the optimized model with several benchmark classification algorithms based on a dataset that compiled over 40, 000 various news articles recently. The results indicate that the proposed approach achieved good classification accuracy and F1 measure rate and successfully classified over 91 percent of articles.},
  archive      = {J_ASOC},
  author       = {Saeid Sheikhi},
  doi          = {10.1016/j.asoc.2021.107559},
  journal      = {Applied Soft Computing},
  pages        = {107559},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective fake news detection method using WOA-xgbTree algorithm and content-based features},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contextual recommender system for e-commerce applications.
<em>ASOC</em>, <em>109</em>, 107552. (<a
href="https://doi.org/10.1016/j.asoc.2021.107552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s arena of global village organizations, social applications, and commercial websites provides huge information about products, individuals, and activities. This is leading to a plethora of content that requires effective handling to obtain the desired information. A recommendation system (RS) suggests relevant items to the user according to his/her desired preference. It processes various information related to users and items. However, RSs suffer from data sparsity . Generally, deep learning techniques are used in RSs for deep analysis of item contents to create precise recommendations. However, the effective handling of user reviews in parallel with item reviews is still an open research domain that can be further explored. In this paper, a hybrid model that handles both user and item metadata concurrently is proposed with the aim of solving the sparsity problem. To demonstrate the viability of the proposed methodology, a series of experiments was performed on three real-world datasets. The results show that the proposed model outperforms other state-of-the-art approaches to the best of our knowledge.},
  archive      = {J_ASOC},
  author       = {Zafran Khan and Muhammad Ishfaq Hussain and Naima Iltaf and Joonmo Kim and Moongu Jeon},
  doi          = {10.1016/j.asoc.2021.107552},
  journal      = {Applied Soft Computing},
  pages        = {107552},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contextual recommender system for E-commerce applications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A data-driven approach towards the full anthropometric
measurements prediction via generalized regression neural networks.
<em>ASOC</em>, <em>109</em>, 107551. (<a
href="https://doi.org/10.1016/j.asoc.2021.107551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anthropometry is a science concerning human body dimension measurements and proportions that is widely applied in multiple disciplines, particularly in apparel and “human-centered” product design. Designers and engineers generally use percentile anthropometric data that leads to significant errors in practice. Although adopting non-statistic body dimensions could resolve this issue, gaining detailed body measurements is a challenging task as neither manual measurement nor 3D scanning is efficient in a cost-effective manner. With the rapid development of artificial neural networks , predicting body sizes and shapes, instead of measuring them, has become a new trend. This work presents a unique Generalized Regression Neural Network architecture that is capable of accurately predicting 76 detailed body measurements from seven easily measured body features with high tolerance to input measurement errors. The proposed model outperforms the existing regression models and can be easily implemented in the design process.},
  archive      = {J_ASOC},
  author       = {Lining Wang and Tien Ju Lee and Jan Bavendiek and Lutz Eckstein},
  doi          = {10.1016/j.asoc.2021.107551},
  journal      = {Applied Soft Computing},
  pages        = {107551},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A data-driven approach towards the full anthropometric measurements prediction via generalized regression neural networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A social participatory allocation network method with
partial relations of alternatives and its application in sustainable
food supply chain selection. <em>ASOC</em>, <em>109</em>, 107550. (<a
href="https://doi.org/10.1016/j.asoc.2021.107550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing a good food supply chain is helpful for realizing the coordination of economic benefits and environmental optimization, so as to promote the sustainable development of the society. To solve such a problem, we need to ensure the accuracy of the original information and the validity of the aggregated information. However, experts may not be able to give professional evaluations in some aspects, and a simple ranking of alternatives cannot accurately reflect the relations between alternatives. To solve these problems, this paper proposes a comprehensive method integrating the social participatory allocation network (SPAN) and ORESTE methods under the q-rung orthopair fuzzy environment. To better deal with the imprecise and uncertain information, the q-rung orthopair fuzzy set is adopted in this study to represent decision-makers’ opinions. The delegate mechanism of the SPAN method is introduced to calculate the weights of experts, which make the assessments of alternatives credible. The ORESTE method is used to explore the relations between alternatives in detail. The preference function is used to deal with the case where the data is not ordinal. Finally, a case study concerning the selection of sustainable food supply chains is presented to verify the applicability of the proposed method.},
  archive      = {J_ASOC},
  author       = {Yilu Long and Huchang Liao},
  doi          = {10.1016/j.asoc.2021.107550},
  journal      = {Applied Soft Computing},
  pages        = {107550},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A social participatory allocation network method with partial relations of alternatives and its application in sustainable food supply chain selection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient clustering with robust outlier mitigation for
wi-fi fingerprint based indoor positioning. <em>ASOC</em>, <em>109</em>,
107549. (<a href="https://doi.org/10.1016/j.asoc.2021.107549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi fingerprint systems provide cost-effective and reliable solution for indoor positioning. However, such systems incur high calibration cost in the training phase and high searching overhead in the positioning phase. Moreover, huge storage requirement for the radio map of a large-scale fingerprint system is another major issue. Several solutions based on crowd-sourcing or machine learning technique have been proposed in literature to reduce the calibration overhead. On the other hand, various clustering methods have been proposed over the past decade to reduce the searching overhead. However, none of the existing systems has addressed the issue of high storage requirement for the fingerprint database constructed in the training phase. Moreover, presence of outlier in the received signal strength ( RSS ) measurements severely impacts the positioning accuracy of such systems. Thus, this paper proposes an efficient clustering strategy for fingerprint based positioning systems to reduce the storage overhead and searching overhead incurred by such systems and also proposes a robust outlier mitigation technique to improve their positioning accuracy. The performances of our proposed positioning system are evaluated and compared with five existing fingerprint techniques in both the simulation test bed as well as real indoor environment via extensive experimentation. The experimental results demonstrate that our proposed system can not only reduce the storage overhead and searching overhead but also improve the positioning accuracy compared to the other existing techniques.},
  archive      = {J_ASOC},
  author       = {Pampa Sadhukhan and Supriya Gain and Keshav Dahal and Samiran Chattopadhyay and Nilkantha Garain and Xinheng Wang},
  doi          = {10.1016/j.asoc.2021.107549},
  journal      = {Applied Soft Computing},
  pages        = {107549},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient clustering with robust outlier mitigation for wi-fi fingerprint based indoor positioning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mutual information-based recommender system using
autoencoder. <em>ASOC</em>, <em>109</em>, 107547. (<a
href="https://doi.org/10.1016/j.asoc.2021.107547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, most of the websites like Amazon, YouTube and Netflix use collaborative filtering methods to recommend various types of items to users. There are two principal categories of collaborative filtering; memory-based and model-based. The memory-based methods use the users’ similarity measures and have several advantages over the model-based techniques, including being easily explained and easy modeling updates with new ratings and items. However, the memory-based methods’ performance reduces when the data is sparse, and unlike the model-based methods, memory-based methods are not scalable. In this paper, we propose a method that exploit the benefits of both similarity-based and model-based approaches. We address both the reliability and the online updating problems based on a novel user-similarity based method. To calculate the new similarity metric we use the predicted user rating vectors in the autoencoder’s output and apply mutual information to the predicted vectors in order to find similar users. We depict a similarity graph according to the mutual information rate, which is calculated for each pair of users. We implement the proposed method on the Netflix movie recommendation dataset. According to our experiments, the proposed approach has a significant advantage over the other methods, such as the standard autoencoder , the matrix factorization , and the similarity-based methods.},
  archive      = {J_ASOC},
  author       = {Zahra Noshad and Asgarali Bouyer and Mohammad Noshad},
  doi          = {10.1016/j.asoc.2021.107547},
  journal      = {Applied Soft Computing},
  pages        = {107547},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mutual information-based recommender system using autoencoder},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A metaheuristic multi-objective optimization method for
dynamical network biomarker identification as pre-disease stage signal.
<em>ASOC</em>, <em>109</em>, 107544. (<a
href="https://doi.org/10.1016/j.asoc.2021.107544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deciphering the signals that are attached to the transition from normal to disease stage is crucial in preventive medicine to understand the progression of complex diseases. Between normal and disease stages there exists the pre-disease stage, in which the disease is yet reversible towards the normal stage. Traditionally, molecular biomarkers have been used to identify the pre-disease stage. However, they have limitations because they have an individual and static nature. In complex diseases, the dynamics and interplays of certain genes have to be taken into account in order to identify the pre-disease stage. Therefore, in complex diseases, it is necessary to use dynamical network biomarkers (DNBs). The development of time-course omics data has been crucial to the use of DNBs as biomarker. In this article, a new two-step method is proposed for the identification of DNBs as pre-disease stage signal. In the first step, the relevant genes in the dataset are pre-filtered using a differential gene expression analysis. In the second step, the DNBs are identified, from a multi-objective optimization viewpoint, by using an Artificial Bee Colony based on Dominance (ABCD) algorithm. Specifically, identified DNBs optimize three objectives: they are the smallest gene network that shows the strongest signal in the earliest time-point of the disease progression and best correlates with the disease phenotype. The proposed method has been evaluated with five time-course microarray datasets and the results have been compared with five methods from other authors, surpassing their results. The effectiveness of the proposed method has been also proved with a leave-one-out cross-validation and a Gene Ontology term enrichment. In fact, the proposed method obtains values around 90\% for accuracy, precision, recall, and F1 scores.},
  archive      = {J_ASOC},
  author       = {Veredas Coleto-Alcudia and Miguel A. Vega-Rodríguez},
  doi          = {10.1016/j.asoc.2021.107544},
  journal      = {Applied Soft Computing},
  pages        = {107544},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A metaheuristic multi-objective optimization method for dynamical network biomarker identification as pre-disease stage signal},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An artificial bee colony algorithm for scheduling call
centres with weekend-off fairness. <em>ASOC</em>, <em>109</em>, 107542.
(<a href="https://doi.org/10.1016/j.asoc.2021.107542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The workforce scheduling problem in call centres is defined as assigning a number of employees to various overlapping shifts during a specified planning horizon, considering some regulations and preferences, so as to minimize the labour cost. This is often difficult to solve to optimality due to its combinatorial structure. In this paper, we propose an enhanced artificial bee colony (EABC) algorithm to solve the problem. After introducing a good-quality solution generated by a heuristic algorithm to the initial population, the solutions are further exploited by four elaborate neighbourhood structures, which are designed based on the decomposed structure of the problem and deeply embedded in the solution evolution mode. Furthermore, a modified abandoning mode is employed in the onlooker stage. Using real instances from Chengxi Call Centre in Suqian, China, the experimental results show that the proposed algorithm can achieve (sub-)optimal solutions for large-scale problems. Furthermore, a comparative evaluation of EABC is carried out against a hybrid artificial bee colony (HABC) and simulated annealing (SA). The results show that EABC outperforms the other two algorithms for all problem instances. In addition, we analyse the influence of the weekend-off fairness on the labour cost.},
  archive      = {J_ASOC},
  author       = {Yue Xu and Xiuli Wang},
  doi          = {10.1016/j.asoc.2021.107542},
  journal      = {Applied Soft Computing},
  pages        = {107542},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An artificial bee colony algorithm for scheduling call centres with weekend-off fairness},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Railway dangerous goods transportation system risk
identification: Comparisons among SVM, PSO-SVM, GA-SVM and GS-SVM.
<em>ASOC</em>, <em>109</em>, 107541. (<a
href="https://doi.org/10.1016/j.asoc.2021.107541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, three algorithms are applied to obtain the parameters of Radial Basis Function (RBF) kernels of Support Vector Machines (SVM), which include: PSO (Particle Swarm Optimization), GA (Genetic Algorithm) and GS (Grid Search). The three improved SVM approaches are applied to identify the risk of railway dangerous goods transportation system (RDGTS). The statistical occurrence frequency of each sub-risk indicator of the happened RDGTS accidents is used as the basis of experts’ scores, the experts’ scores are presented in interval numbers form, which are used as the inputs of the four approaches. The accuracy rate, optimization time consuming, Mean Square Error (MSE), Receiver Operating Characteristic Curve (ROC) and Area Under Curve (AUC) are used as the evaluation indexes of the identification results. The comparison studies are conducted by using SVM with linear kernel (SVM-L) and SVM with polynomial kernel (SVM-P), respectively. By using such new methodology, the risk identification problem (evaluation problem) is transferred into a classification problem with faster identification speed, higher identification efficiency and higher accuracy. The identification results show that: GS-SVM is the optimal approach to identify the risk factors of Human; SVM is the optimal approach to identify the risk factors of Machine, Materials, Environment and Management. SVM has the shortest optimization time consuming, GA-SVM has the highest accuracy, hence, SVM and GA-SVM are better to applied to identify the risk of RDGTS. The optimization time consuming of all models is no more than 5 s, which means the RDGTS risk identification results could be obtained fast and high-efficiently and the mental strength for researchers can be reduced by using the SVM and improved SVM models. For the risk identification results considering MSE and AUC, GS-SVM is the most accurate and best classification algorithm , and the results based on SVM-P is better than the results based on SVM-L. The results based on PSO-SVM, GA-SVM and GS-SVM have better accuracy and reliability than that based on SVM-L or SVM-P, which means the PSO-SVM, GA-SVM and GS-SVM based risk identification approaches are practicable.},
  archive      = {J_ASOC},
  author       = {Wencheng Huang and Hongyi Liu and Yue Zhang and Rongwei Mi and Chuangui Tong and Wei Xiao and Bin Shuai},
  doi          = {10.1016/j.asoc.2021.107541},
  journal      = {Applied Soft Computing},
  pages        = {107541},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Railway dangerous goods transportation system risk identification: Comparisons among SVM, PSO-SVM, GA-SVM and GS-SVM},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Status evaluation of provinces affected by COVID-19: A
qualitative assessment using fuzzy system. <em>ASOC</em>, <em>109</em>,
107540. (<a href="https://doi.org/10.1016/j.asoc.2021.107540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of COVID-19 had already shown its harmful impact on mankind, especially on health sectors, global economy, education systems, cultures, politics, and other important fields. Like most of the affected countries in the globe, India is now facing serious crisis due to COVID-19 in the recent times. The evaluation of the present status of the provinces affected by COVID-19 is very much essential to the government authorities to impose preventive strategies in controlling the spread of COVID-19 and to take necessary measures. In this article, a computational methodology is developed to estimate the present status of states and provinces which are affected due to COVID-19 using a fuzzy inference system . The factors such as population density, number of COVID-19 tests, confirmed cases of COVID-19, recovery rate, and mortality rate are considered as the input parameters of the proposed methodology. Considering positive and negative factors of the input parameters, the rule base is developed using triangular fuzzy numbers to capture uncertainties associated with the model. The application potentiality is validated by evaluating Pearson’s correlation coefficient. A sensitivity analysis is also performed to observe the changes of final output by varying the tolerance ranges of the inputs. The results of the proposed method show that some of the provinces have very poor performance in controlling the spread of COVID-19 in India. So, the government needs to take serious attention to deal with the pandemic situation of COVID-19 in those provinces.},
  archive      = {J_ASOC},
  author       = {Bappaditya Ghosh and Animesh Biswas},
  doi          = {10.1016/j.asoc.2021.107540},
  journal      = {Applied Soft Computing},
  pages        = {107540},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Status evaluation of provinces affected by COVID-19: A qualitative assessment using fuzzy system},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified framework for knowledge measure with application:
From fuzzy sets through interval-valued intuitionistic fuzzy sets.
<em>ASOC</em>, <em>109</em>, 107539. (<a
href="https://doi.org/10.1016/j.asoc.2021.107539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to establish a new mathematical framework for knowledge measure (KM) from fuzzy sets (FSs) through interval-valued intuitionistic fuzzy sets (IVIFSs). A novel KM is developed first in the context of intuitionistic fuzzy sets (IFSs) based on the normalized Hamming distance combined with the technique for order preference by similarity to ideal solution (TOPSIS), complying with the latest axiomatic definition . More efforts are made to investigate this new kind of axiomatic system and measuring model in the contexts of IVIFSs and FSs, respectively. This allows us to further recognize the nature of knowledge and present a unified framework for KM from FSs through IVIFSs. The developed technique is then used for image thresholding as a brand-new KM application, in which a new classification rule of pixels and a more efficient method for (interval-valued) intuitionistic fuzzification of images are proposed, thus leading to a knowledge-driven thresholding methodology. Experimental results show the outperformance of the developed technique with application. This is the first attempt to apply the latest KM theory to image processing , with which we undoubtedly create a new instance for the potential application areas of this theory.},
  archive      = {J_ASOC},
  author       = {Kaihong Guo and Hao Xu},
  doi          = {10.1016/j.asoc.2021.107539},
  journal      = {Applied Soft Computing},
  pages        = {107539},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A unified framework for knowledge measure with application: From fuzzy sets through interval-valued intuitionistic fuzzy sets},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction and analysis of train arrival delay based on
XGBoost and bayesian optimization. <em>ASOC</em>, <em>109</em>, 107538.
(<a href="https://doi.org/10.1016/j.asoc.2021.107538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate train arrival delay prediction is critical for real-time train dispatching and for the improvement of the transportation service. This study proposes a data-driven method that combines eXtreme Gradient Boosting (XGBoost) and a Bayesian optimization (BO) algorithm to predict train arrival delays. First, eleven characteristics that may affect the train arrival time at the next scheduled station are identified as independent variables. Second, an XGBoost prediction model that captures the relation between the train arrival delays and various railway system characteristics is established. Third, the BO algorithm is applied to the hyperparameter optimization of the XGBoost model to improve the prediction accuracy. Subsequently, case studies using data from two high-speed railway (HSR) lines in China are performed to analyze the prediction efficiency and accuracy of the proposed model for different delay bins and at different stations. The results on two HSR lines demonstrate that the proposed method outperforms other benchmark models regarding the performance metrics of the determination coefficient (0.9889/0.9905), root-mean-squared error (2.686/1.887), and mean absolute error (0.896/ 0.802). In addition, the statistical test is carried out using Friedman Test (FT) and Wilcoxon Signed Rank Test (WSRT) to validate the efficacy of the proposed method. Furthermore, the train arrival delays at different abnormal events can also be accurately forecasted using the proposed method; the results indicate that the proposed method outperforms other benchmark methods, especially in the prediction of long delays caused by specific abnormal events.},
  archive      = {J_ASOC},
  author       = {Rui Shi and Xinyue Xu and Jianmin Li and Yanqiu Li},
  doi          = {10.1016/j.asoc.2021.107538},
  journal      = {Applied Soft Computing},
  pages        = {107538},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction and analysis of train arrival delay based on XGBoost and bayesian optimization},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolving convolutional neural networks by symbiotic
organisms search algorithm for image classification. <em>ASOC</em>,
<em>109</em>, 107537. (<a
href="https://doi.org/10.1016/j.asoc.2021.107537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are important neural networks in the deep learning field. In general, CNN design requires manual iterations and adjustments based on the characteristics of the data and a priori knowledge of the application area, and such processes are time-consuming, labor-intensive, and weaken the generalizability of the CNN architecture. Due to the complexity of the CNN architecture, its automatic construction is difficult. Automating the design of a CNN architecture using evolutionary algorithms is viable, but the field is still in its infancy. We propose an architectural search algorithm for CNNs that employs a symbiotic organisms search (SOS) algorithm called sosCNN. First, a SOS algorithm with strong global optimization abilities was introduced for CNN architecture search, and this algorithm facilitates the automatic building of good CNN architectures. Then, a novel integrated coding update method is proposed that reduces the loss of convolutional layers , which results in a searched architecture with stronger feature extraction capability. Finally, three new non-numeric computational strategies, namely, binary segmentation, slack gain, and dissimilar mutation, were combined with the SOS algorithm. We tested the sosCNN algorithm against 24 algorithms, including state-of-the-art algorithms, on nine widely used image classification datasets. The experimental results show that the average classification error of sosCNN is reduced by 0.04\% to 7.37\% on the nine benchmark test sets compared to the state of art algorithms, which is a very promising result.},
  archive      = {J_ASOC},
  author       = {Fahui Miao and Li Yao and Xiaojie Zhao},
  doi          = {10.1016/j.asoc.2021.107537},
  journal      = {Applied Soft Computing},
  pages        = {107537},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving convolutional neural networks by symbiotic organisms search algorithm for image classification},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disentangling copy-moved source and target areas.
<em>ASOC</em>, <em>109</em>, 107536. (<a
href="https://doi.org/10.1016/j.asoc.2021.107536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy-move is a very popular image falsification where a semantically coherent part of the image, the source area, is copied and pasted at another position within the same image as the so-called target area. The majority of existing copy-move detectors search for matching areas and thus identify the source and target zones indifferently, while only the target really represents a tampered area. To the best of our knowledge, at the moment of preparing this paper there has been only one published method called BusterNet that is capable of performing source and target disambiguation by using a specifically designed deep neural network . Different from the deep-learning-based BusterNet method, we propose in this paper a source and target disentangling approach based on local statistical model of image patches. Our proposed method acts as a second-stage detector after a first stage of copy-move detection of duplicated areas. We had the following intuition: even if no manipulation ( e.g., scaling and rotation) is added on target area, its boundaries should expose a statistical deviation from the pristine area and the source area; further, if the target area is manipulated, the deviation should appear not only on the boundaries but on the full zone. Our method relies on machine learning tool with Gaussian Mixture Model to describe likelihood of image patches. Likelihoods are then compared between the pristine region and the candidate source/target areas as identified by the first-stage detector. Experiments and comparisons demonstrate the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Ludovic Darmet and Kai Wang and François Cayre},
  doi          = {10.1016/j.asoc.2021.107536},
  journal      = {Applied Soft Computing},
  pages        = {107536},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Disentangling copy-moved source and target areas},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A calibrated piecewise-linear FGM approach for travel
destination recommendation during the COVID-19 pandemic. <em>ASOC</em>,
<em>109</em>, 107535. (<a
href="https://doi.org/10.1016/j.asoc.2021.107535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After months of lockdown due to the COVID-19 pandemic, more people are planning regional trips because overseas travel is still not feasible. However, choosing a suitable travel destination during the COVID-19 pandemic is challenging because the factors critical to the selection process are very different from those usually considered. Furthermore, without sufficient literature or data for reference, existing methods based on psychological analyses or mining past experiences may not be applicable. Consequently, a fuzzy multi-criteria decision-making method – the calibrated piecewise-linear fuzzy geometric mean (FGM) approach – is proposed in this study for travel destination recommendation during the COVID-19 pandemic. The contribution of this research is twofold. First, the critical factors that affect the selection of a suitable travel destination during the COVID-19 pandemic are discussed. Second, the accuracy and efficiency using existing fuzzy analytic hierarchy process (FAHP) methods have been enhanced. The calibrated piecewise-linear FGM approach has been successfully applied to recommend suitable travel destinations to fifteen travelers for regional trips in Taiwan during the COVID-19 pandemic.},
  archive      = {J_ASOC},
  author       = {Toly Chen and Yu-Cheng Wang},
  doi          = {10.1016/j.asoc.2021.107535},
  journal      = {Applied Soft Computing},
  pages        = {107535},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A calibrated piecewise-linear FGM approach for travel destination recommendation during the COVID-19 pandemic},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of an MCDM model with data mining techniques for
green supplier evaluation and selection. <em>ASOC</em>, <em>109</em>,
107534. (<a href="https://doi.org/10.1016/j.asoc.2021.107534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation and selection of green suppliers are some of the most important tasks in green supply chain management (GSCM). The purpose of this study is to develop an effective green supplier evaluation model. Currently the evaluation criteria are determined through a literature review combined with decision-maker opinions. A few studies have used data mining techniques to screen the core criteria. This study proposes a novel hybrid MCDM model, that integrates the support vector machine (SVM), the fuzzy best worst method (FBWM) and, the fuzzy technique for order preference by similarity to an ideal solution (FTOPSIS) approaches to select the most suitable green suppliers. A case study, using data from a multinational electronics manufacturer, is carried out for illustration. First, the SVM is used to extract the core criteria from the historical data. The original 25 criteria are reduced to 13 criteria. Then, the FBWM is used to obtain the weights of the core criteria. Finally, the FTOPSIS is used to integrate the performance and prioritize the green suppliers. Finally, practical management implications and suggestions for improvement for decision-makers and green suppliers are provided},
  archive      = {J_ASOC},
  author       = {James J.H. Liou and Mu-Hsin Chang and Huai-Wei Lo and Min-Hsi Hsu},
  doi          = {10.1016/j.asoc.2021.107534},
  journal      = {Applied Soft Computing},
  pages        = {107534},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of an MCDM model with data mining techniques for green supplier evaluation and selection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Breast cancer diagnosis using thermal image analysis: A
data-driven approach based on swarm intelligence and supervised learning
for optimized feature selection. <em>ASOC</em>, <em>109</em>, 107533.
(<a href="https://doi.org/10.1016/j.asoc.2021.107533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the deadliest forms of cancer in women but the disease has a good prognosis when diagnosed early. The gold standard for the diagnosis of breast cancer is mammography imaging analysis but the acquisition of mammograms is a painful and embarrassing procedure for women involving breast compression. Alternative methods have been investigated in the last years, including breast thermography, which does not involve ionizing radiation , pain or contact with the breast. However, the accuracy of these modern techniques still needs to be improved to allow the widespread use in practical applications but machine learning techniques have brought in an increased accuracy and reduction in false positives and false negatives to the analysis of breast thermograms. We propose a methodology for detecting and classifying breast lesions using a database of real images of Brazilian patients. We divide our methodology into three steps. In the first step, the shape and texture characteristics of breast thermograms are extracted using Zernike and Haralick moments. The second step is the feature selection process using multi-objective binary optimization algorithms based on swarm intelligence . The third step is to analyze the best vectors’ classification using eleven algorithms such as Convolutional Neural Networks , Extreme Learning Machines , and Support Vector Machines . Finally, we discuss the computational time and performance of various techniques based on swarm intelligence , artificial neural networks , and statistical models to improve the computational time and accuracy of breast cancer diagnoses. Indeed, we observe that the feature selection process has helped us decrease computational time with a high potential to improve diagnostic accuracy . We also demonstrate that the extracted features considering the shape of breast lesions are highly important to a high diagnostic accuracy .},
  archive      = {J_ASOC},
  author       = {Mariana Macedo and Maira Santana and Wellington P. dos Santos and Ronaldo Menezes and Carmelo Bastos-Filho},
  doi          = {10.1016/j.asoc.2021.107533},
  journal      = {Applied Soft Computing},
  pages        = {107533},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Breast cancer diagnosis using thermal image analysis: A data-driven approach based on swarm intelligence and supervised learning for optimized feature selection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Offshore wind farm site selection using interval rough
numbers based best-worst method and MARCOS. <em>ASOC</em>, <em>109</em>,
107532. (<a href="https://doi.org/10.1016/j.asoc.2021.107532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past 20 years, the development of offshore wind farms has become increasingly important across the world. One of the most crucial reasons for that is offshore wind turbines have higher average speeds than those onshore, producing more electricity. In this study, a new hybrid approach integrating Interval Rough Numbers (IRNs) into Best-Worst Method (BWM) and Measurement of Alternatives and Ranking according to Compromise Solution (MARCOS) is introduced for multi-criteria intelligent decision support to choose the best offshore wind farm site in a Turkey’s coastal area. Four alternatives in the Aegean Sea are considered based on a range of criteria. The results show the viability of the proposed approach which yields Bozcaada as the appropriate site, when compared to and validated using the other multi-criteria decision-making techniques from the literature, including IRN based MABAC, WASPAS, and MAIRCA.},
  archive      = {J_ASOC},
  author       = {Muhammet Deveci and Ender Özcan and Robert John and Dragan Pamucar and Himmet Karaman},
  doi          = {10.1016/j.asoc.2021.107532},
  journal      = {Applied Soft Computing},
  pages        = {107532},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Offshore wind farm site selection using interval rough numbers based best-worst method and MARCOS},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary-based prediction interval estimation by
blending solar radiation forecasting models using meteorological weather
types. <em>ASOC</em>, <em>109</em>, 107531. (<a
href="https://doi.org/10.1016/j.asoc.2021.107531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has shown that the integration or blending of different forecasting models is able to improve the predictions of solar radiation. However, most works perform model blending to improve point forecasts, but the integration of forecasting models to improve probabilistic forecasting has not received much attention. In this work the estimation of prediction intervals for the integration of four Global Horizontal Irradiance (GHI) forecasting models (Smart Persistence, WRF-solar, CIADcast, and Satellite) is addressed. Several short-term forecasting horizons, up to one hour ahead, have been analyzed. Within this context, one of the aims of the article is to study whether knowledge about the synoptic weather conditions, which are related to the stability of weather, might help to reduce the uncertainty represented by prediction intervals. In order to deal with this issue, information about which weather type is present at the time of prediction, has been used by the blending model. Four weather types have been considered. A multi-objective variant of the Lower Upper Bound Estimation approach has been used in this work for prediction interval estimation and compared with two baseline methods: Quantile Regression (QR) and Gradient Boosting (GBR). An exhaustive experimental validation has been carried out, using data registered at Seville in the Southern Iberian Peninsula. Results show that, in general, using weather type information reduces uncertainty of prediction intervals, according to all performance metrics used. More specifically, and with respect to one of the metrics (the ratio between interval coverage and width), for high-coverage (0.90, 0.95) prediction intervals, using weather type enhances the ratio of the multi-objective approach by 2\%–3\%. Also, comparing the multi-objective approach versus the two baselines for high-coverage intervals, the improvement is 11\%–17\% over QR and 10\%–44\% over GBR. Improvements for low-coverage intervals (0.85) are smaller.},
  archive      = {J_ASOC},
  author       = {Inés M. Galván and Javier Huertas-Tato and Francisco J. Rodríguez-Benítez and Clara Arbizu-Barrena and David Pozo-Vázquez and Ricardo Aler},
  doi          = {10.1016/j.asoc.2021.107531},
  journal      = {Applied Soft Computing},
  pages        = {107531},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary-based prediction interval estimation by blending solar radiation forecasting models using meteorological weather types},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel multi-objective mutation flower pollination
algorithm for the optimization of industrial enterprise r&amp;d
investment allocation. <em>ASOC</em>, <em>109</em>, 107530. (<a
href="https://doi.org/10.1016/j.asoc.2021.107530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial enterprises are the main body of national scientific and technological innovation activities, and the improvement of their research and development (R&amp;D) output capacity plays a pivotal role in the enhancement of national innovation capabilities. The R&amp;D process is an input–output process, and its results have various forms. In this study, we construct a multi-objective R&amp;D investment allocation optimization model from three dimensions representing China’s innovation capability, and propose a novel multi-objective mutation flower pollination algorithm (MOMFPA) to solve the model. We employ a two paired-sample T-tests to test the difference hypothesis of the model, and the test results show that the model is effective and reasonable. The MOMFPA and the weighting algorithm are respectively used for empirical analysis, and the optimized results obtained by the MOMFPA are found to be better than those obtained by the weighting algorithm, thereby demonstrating the validity and application value of the MOMFPA. Moreover, the multi-objective model is used to predictively optimize China’s future R&amp;D investment. The forecasting results indicate that China should focus on increasing the proportions of R&amp;D investment in high-tech equipment manufacturing and electronic information technology, while reducing the proportions of R&amp;D investment in traditional industries, such as mining, food, paper, and metallurgy.},
  archive      = {J_ASOC},
  author       = {Yan Song and Kangkang Zhang and Xianpei Hong and Xinyun Li},
  doi          = {10.1016/j.asoc.2021.107530},
  journal      = {Applied Soft Computing},
  pages        = {107530},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel multi-objective mutation flower pollination algorithm for the optimization of industrial enterprise R&amp;D investment allocation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and modeling of adaptive IIR filtering systems using
a weighted sum - variable length particle swarm optimization.
<em>ASOC</em>, <em>109</em>, 107529. (<a
href="https://doi.org/10.1016/j.asoc.2021.107529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of optimization algorithms for designing Infinite Impulse Response (IIR) filters has been considered in many studies. The concern in this area is the multimodal error surface of such filters and their fitting with filter coefficients . The order of the modeled system has a direct effect on the number of coefficients, complexities of the error surface, and the filter’s stability. This paper proposes an efficient approach based on a variable length particle swarm optimization algorithm with a weighted sum fitness function (WS-VLPSO). The proposed WS-VLPSO is utilized as an effective adaptive algorithm for designing optimal IIR filters. The approach is based on the inclusion of the order as a discrete variable in the particle vector, which is done with the goal of intelligent minimizing of order and thereby reducing the design complexity of IIR filters. To ensure the optimality of the systems, the objective function is considered as a weighted sum. Also, a new criterion called Optimum Modeling Indicator (OMI) is introduced, a measure to determine the percentage reduction of order and the success rate of the proposed approach. The proposed algorithm is also applied for solving the sensor coverage problem as another real-world variable length engineering optimization application. Evaluation of simulation results, with Monte-Carlo simulation, indicates the acceptable improvement of identified structures and the significant performance of the proposed approaches. Note that the source codes of the paper will be publicly available at https://github.com/ali-ece .},
  archive      = {J_ASOC},
  author       = {Ali Mohammadi and Seyed Hamid Zahiri and Seyyed Mohammad Razavi and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2021.107529},
  journal      = {Applied Soft Computing},
  pages        = {107529},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design and modeling of adaptive IIR filtering systems using a weighted sum - variable length particle swarm optimization},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Cooperation and profit allocation for two-echelon logistics
pickup and delivery problems with state–space–time networks.
<em>ASOC</em>, <em>109</em>, 107528. (<a
href="https://doi.org/10.1016/j.asoc.2021.107528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation is a powerful strategy to reduce the costs of pickup and delivery services in the conventional two-echelon pickup and delivery problems (2E-PDPs). This study develops a cooperative 2E-PDP with state–space–time network (C2E-PDPSST) and incorporates vehicle routing problem and profit allocation optimization. Transportation resource sharing (trucks/vehicles) is presented as a major cooperative strategy in C2E-PDPSST to reduce the number of vehicles and improve resource utilization. A bi-objective mixed integer model is proposed to minimize total operating costs and number of vehicles and is formulated based on state–space–time networks. A novel methodology, which combines the time-dependent forward dynamic programming algorithm and a hybrid heuristic algorithm comprising modified k -means clustering algorithm and improved multi-objective particle swarm optimization (IMOPSO) algorithm, is designed for solving C2E-PDPSST. The clustering process speeds up the solution by reducing the computational complexity . The IMOPSO algorithm combines inter- and intra-route operations to find the Pareto optimal solutions with predefined iteration and termination rules. The inter-route operations will improve the population diversity, while the intra-route operations will generate an optimal solution for each vehicle route. Thus, an effective combination of local and global solution search can be achieved. Profit allocation schemes are investigated using the criteria of minimum cost remaining savings. Our results on benchmark instances show that the proposed IMOPSO has better computational performance than the conventional MOPSO and multi-objective genetic algorithm. A case study based on the realistic logistics network in Chengdu City, China is conducted for validation. The proposed methodology considering state–space–time network performs better in terms of reducing traveling costs, and improving the flexibility and efficiency of the entire network.},
  archive      = {J_ASOC},
  author       = {Yong Wang and Shuanglu Zhang and Xiangyang Guan and Jianxin Fan and Haizhong Wang and Yong Liu},
  doi          = {10.1016/j.asoc.2021.107528},
  journal      = {Applied Soft Computing},
  pages        = {107528},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperation and profit allocation for two-echelon logistics pickup and delivery problems with state–space–time networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation and prediction methods for launch safety of
propellant charge based on support vector regression. <em>ASOC</em>,
<em>109</em>, 107527. (<a
href="https://doi.org/10.1016/j.asoc.2021.107527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the consensus that the oversized initial combustion surface area of fractured propellant charge is the main cause of breech blow, the evaluation method for launch safety of propellant charge with initial dynamic vivacity ratio as the characterization parameter has drawn extensive attentions and inspired many variants. However, the characterization of propellant charge stacking configuration and the prediction of initial dynamic vivacity ratio are still two unsolved problems . To fill the gaps, this study provides a novel evaluation method for the launch safety of propellant charge through applying the machine learning and the error analysis methods. A new description with three parameters is proposed for dynamic compression and fracture process, and verified more accurate than the previous method with maximum compression stress as the single parameter. In addition, it identifies that the support vector regression is more suitable than back propagation neural network and least squares support vector machine in small sample training. And the corresponding model has been demonstrated by experiments to be of convinced accuracy and superior generalization capability. Through determining the model error distribution, this study makes it feasible to predict initial dynamic vivacity ratio and give an upper limit value with high confidence.},
  archive      = {J_ASOC},
  author       = {Xin Zhao and Xiaoting Rui and Chao Li and Zhenzheng Ma and Yunfei Miao},
  doi          = {10.1016/j.asoc.2021.107527},
  journal      = {Applied Soft Computing},
  pages        = {107527},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation and prediction methods for launch safety of propellant charge based on support vector regression},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A green scheduling algorithm for the distributed flowshop
problem. <em>ASOC</em>, <em>109</em>, 107526. (<a
href="https://doi.org/10.1016/j.asoc.2021.107526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, sustainable development and green manufacturing have attracted widespread attention to environmental problems becoming increasingly serious. Meanwhile, affected by the intensification of market competition and economic globalization, distributed manufacturing systems have become increasingly common. This paper addresses the energy-efficient scheduling of the distributed permutation flowshop (EEDPFSP) with the criteria of minimizing both total flow time and total energy consumption . Considering the distributed and multi-objective optimization complexity, an improved NSGAII algorithm (INSGAII) is proposed. First, we analyze the problem-specific characteristics and designed new operators based on the knowledge of the problem. Second, four constructive heuristic algorithms are proposed to produce high-quality initial solutions. Third, inspired by the artificial bee colony algorithm , we propose a new colony generation method using the operators designed. Fourth, a local intensification is designed for exploiting better non-dominated solutions. The influence of parameter settings is investigated by experiments to determine the optimal parameter configuration of the INSGAII. Finally, a large number of computational tests and comparisons have been carried out to verify the effectiveness of the proposed INSGAII in solving EEDPFSP.},
  archive      = {J_ASOC},
  author       = {Yuan-Zhen Li and Quan-Ke Pan and Kai-Zhou Gao and M. Fatih Tasgetiren and Biao Zhang and Jun-Qing Li},
  doi          = {10.1016/j.asoc.2021.107526},
  journal      = {Applied Soft Computing},
  pages        = {107526},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A green scheduling algorithm for the distributed flowshop problem},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Genetically optimized fuzzy c-means data clustering of
IoMT-based biomarkers for fast affective state recognition in
intelligent edge analytics. <em>ASOC</em>, <em>109</em>, 107525. (<a
href="https://doi.org/10.1016/j.asoc.2021.107525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoMT sensors such as wearables, moodables, ingestible sensors and trackers have the potential to provide a proactive approach to healthcare. But grouping, traversing and selectively tapping the IoMT data traffic and its immediacy makes data management &amp; decision analysis a pressing issue. Evidently, the selection process for real-world, time-constrained health problems involves looking at multivariate time-series data generated simultaneously from various wearables resulting in data overload and accuracy issues. Computational intelligence of edge analytics can extend predictive capability by quickly turning digital biomarker data into actions for remote monitoring and trigger alarm during emergency incidents without relying on backend servers. But the pervasive generation of data streams from IoMT levies significant issues in data visualization and exploratory data analysis. This paper presents a genetically optimized Fuzzy C-means data clustering technique for affective state recognition on the edge. Clustering segregates the biomarker data in chunks and generates a summarized data for each subject which is then genetically optimized to avoid stagnation in local optima. A multi-level convolution neural network is finally used to classify the affective states into the baseline, stress and amusement categories. The model is evaluated on the publicly available WESAD dataset and compares favorably to state-of-the-art with less time complexity. It demonstrates the use of data clustering technique for numerosity reduction of real-time data streams in intelligent edge analytics which facilitates fast analysis of affective state of the user.},
  archive      = {J_ASOC},
  author       = {Akshi Kumar and Kapil Sharma and Aditi Sharma},
  doi          = {10.1016/j.asoc.2021.107525},
  journal      = {Applied Soft Computing},
  pages        = {107525},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Genetically optimized fuzzy C-means data clustering of IoMT-based biomarkers for fast affective state recognition in intelligent edge analytics},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Copula-based reliability and sensitivity analysis of aging
dams: Adaptive kriging and polynomial chaos kriging methods.
<em>ASOC</em>, <em>109</em>, 107524. (<a
href="https://doi.org/10.1016/j.asoc.2021.107524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-dependent reliability and sensitivity analysis, in which the nature of demand, capacity and the limit state function varies over the life cycle of the structural system, is a challenging task. Meta-models are a suitable tool to construct inexpensive-to-evaluate and accurate surrogates in analysis of modern engineering problems. The paper’s contribution is threefold: First, the superiority of polynomial chaos Kriging (PCK) meta-model in terms of efficiency and accuracy is depicted in comparison to two other state-of-the-art methods in an explicit representation of a pilot gravity dam. Second, the problem of aging dams is analytically studied. Adaptive reliability approaches which benefit from Kriging and PCK meta-models are also investigated in a comparative analysis with classical reliability methods. Third, the importance of considering nonlinear dependency between random variables by copula theory is investigated under the reliability and sensitivity concepts. The results show that PCK meta-model can be used as an effective technique in uncertainty quantification (UQ) of dams. Furthermore, Kriging and PCK-assisted reliability methods can establish fairly accurate meta-models to perform reliability analysis in structural UQ with low computational efforts. Finally, the concept of UQ is propagated to ”dam class” in which the dam shape and its age are assumed to be variable. A generalized program is developed to assist dam owners and decision-makers in approximate failure probability estimation of gravity dams. This paper paves the road for global risk assessment of dams.},
  archive      = {J_ASOC},
  author       = {A. Amini and A. Abdollahi and M.A. Hariri-Ardebili and U. Lall},
  doi          = {10.1016/j.asoc.2021.107524},
  journal      = {Applied Soft Computing},
  pages        = {107524},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Copula-based reliability and sensitivity analysis of aging dams: Adaptive kriging and polynomial chaos kriging methods},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep-learning based multimodal system for covid-19
diagnosis using breathing sounds and chest x-ray images. <em>ASOC</em>,
<em>109</em>, 107522. (<a
href="https://doi.org/10.1016/j.asoc.2021.107522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covid-19 has become a deadly pandemic claiming more than three million lives worldwide. SARS-CoV-2 causes distinct pathomorphological alterations in the respiratory system, thereby acting as a biomarker to aid its diagnosis. A multimodal framework (Ai-CovScan) for Covid-19 detection using breathing sounds, chest X-ray (CXR) images, and rapid antigen test (RAnT) is proposed. Transfer Learning approach using existing deep-learning Convolutional Neural Network (CNN) based on Inception-v3 is combined with Multi-Layered Perceptron (MLP) to develop the CovScanNet model for reducing false-negatives. This model reports a preliminary accuracy of 80\% for the breathing sound analysis, and 99.66\% Covid-19 detection accuracy for the curated CXR image dataset. Based on Ai-CovScan, a smartphone app is conceptualised as a mass-deployable screening tool, which could alter the course of this pandemic. This app ’s deployment could minimise the number of people accessing the limited and expensive confirmatory tests, thereby reducing the burden on the severely stressed healthcare infrastructure.},
  archive      = {J_ASOC},
  author       = {Unais Sait and Gokul Lal K.V. and Sanjana Shivakumar and Tarun Kumar and Rahul Bhaumik and Sunny Prajapati and Kriti Bhalla and Anaghaa Chakrapani},
  doi          = {10.1016/j.asoc.2021.107522},
  journal      = {Applied Soft Computing},
  pages        = {107522},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep-learning based multimodal system for covid-19 diagnosis using breathing sounds and chest X-ray images},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformative computing for products sales forecast based
on SCIM. <em>ASOC</em>, <em>109</em>, 107520. (<a
href="https://doi.org/10.1016/j.asoc.2021.107520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online agricultural product trading has the characteristics of rapid and diversified transaction data; there is a fuzzy correspondence between sales volume influencing factors and sales volume levels. Based on this, this paper combines the data preprocessing technology of fuzzy membership and optimized deep learning algorithm, adding a self-encoding method with sparseness restriction, and proposes a deep learning sales forecasting model based on transformative computing with fuzzy membership-the super crown model (Super Imperial Crown Model, referred to as SICM). The model uses fuzzy membership to process the weighted relationship between sales influencing factors and sales rank, and uses a sparse autoencoder network to adaptively extract sample features; sales rank classification prediction uses Softmax classifier; BP fine-tuning is used to Achieve parameter optimization. Finally, use the collected transaction data to apply R software to simulate the optimized model and compare and analyze the comprehensive prediction performance. The results show that the super crown model can realize real-time and accurate dynamic sales classification prediction according to the characteristics of current online agricultural product transaction data, effectively overcome the imbalance of supply and demand caused by information imbalance, and promote the study of deep learning in the field of e-commerce transactions effect. Presented algorithm based on transformative computing techniques can be used in optimization of sales processes, management and analysis of sales markets.},
  archive      = {J_ASOC},
  author       = {Shengdong Mu and Yuanyuan Wang and Fengyu Wang and Lidia Ogiela},
  doi          = {10.1016/j.asoc.2021.107520},
  journal      = {Applied Soft Computing},
  pages        = {107520},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transformative computing for products sales forecast based on SCIM},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-period uncertain portfolio optimization model with
minimum transaction lots and dynamic risk preference. <em>ASOC</em>,
<em>109</em>, 107519. (<a
href="https://doi.org/10.1016/j.asoc.2021.107519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate variable description of security returns is necessary to establish a valid portfolio optimization model. It is usually assumed to be a random variable when the historical data of security returns are sufficient. However, when historical data are too limited to estimate its probability distribution, uncertain variables are employed to characterize security returns to be effective. This paper focuses on a multi-period portfolio optimization problem in uncertain environment with the consideration of minimum transaction lots. Different from the previous multi-period work assuming the total available wealth in the end of the investment horizon is consistently in an exponential format, our study provides a simplified additive format of the total wealth, which may make the process of experimental calculation concise, since it is a linear function of decision variables. Besides, we consider the investor’s dynamic risk preference along the whole investment horizon. With these realistic constraints derived from the complex financial markets, we build a multi-period mean-VaR (value-at-risk) model with the objective of maximizing the terminal wealth under the risk control over the whole investment. Genetic algorithm is used to solve the proposed model, and two numerical examples are given to illustrate the effectiveness of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Yuanzhen Dai and Zhongfeng Qin},
  doi          = {10.1016/j.asoc.2021.107519},
  journal      = {Applied Soft Computing},
  pages        = {107519},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-period uncertain portfolio optimization model with minimum transaction lots and dynamic risk preference},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy adaptive charged system search for global
optimization. <em>ASOC</em>, <em>109</em>, 107518. (<a
href="https://doi.org/10.1016/j.asoc.2021.107518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a new fuzzy adaptive Charged System Search (CSS) for global optimization. The suggested algorithm includes a parameter tuning process based on fuzzy logic with the aim of improving its performance. In this regard, four linguistic variables are defined which configures a fuzzy system for parameter identification of the standard CSS algorithm. This process provides a focus for the algorithm on higher levels of global searching in the initial iterations while the local search is considered in the last iterations. Twenty mathematical benchmark functions , the Competitions on Evolutionary Computation (CEC) regarding CEC 2020 benchmark, three well-known constrained, and two engineering problems are utilized to validate the new algorithm. Moreover, the performance of the new algorithm is compared and contrasted with other metaheuristic algorithms . The obtained results reveal the superiority of the proposed approach in dealing with different unconstraint, constrained, and engineering design problems .},
  archive      = {J_ASOC},
  author       = {Siamak Talatahari and Mahdi Azizi and Mehdi Toloo},
  doi          = {10.1016/j.asoc.2021.107518},
  journal      = {Applied Soft Computing},
  pages        = {107518},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy adaptive charged system search for global optimization},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stepping ahead firefly algorithm and hybridization with
evolution strategy for global optimization problems. <em>ASOC</em>,
<em>109</em>, 107517. (<a
href="https://doi.org/10.1016/j.asoc.2021.107517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent optimization algorithms based on swarm principles have been widely researched in recent times. The Firefly Algorithm (FA) is an intelligent swarm algorithm for global optimization problems . In literature, FA has been seen as an efficient and robust optimization algorithm. FA is an algorithm that has obtained good to best results in complex problems. Therefore, there are many instances of modification, and hybridization of FA with other optimizing algorithms, but further improvements are still possible. This research first proposes a new modification of FA by introducing a novel and unique stepping ahead parameter. The concept is based on being proactive rather than reactive, which is the normal behavior of a standard FA. The notion behind stepping ahead is to send a firefly ahead then the best known position to look for even better solution. Second, a new design of a hybrid of the newly modified FA with Covariance Matrix Adaptation Evolution Strategy (CMAES) to improve the exploitation further while maintaining good exploration in the fireflies is presented. The main use of CMAES in this hybrid algorithm is to provide diversity to fireflies, as a result it improves exploitation. Traditionally, hybridization has combined two or more algorithms in terms of structure only, and consideration was not given to the increase in time complexity or diversity. In this paper, the two algorithms are not run in separate cycles rather CMAES is placed inside the FA. Through this novelty, CMAES is initiated inside FA loop and an extra loop is avoided and at the same time CMAES diversifies FA solutions. The structure of algorithm together with the strength of individual solution are used. The newly established modified FA and hybrid are used to solve selected sixty five global optimization benchmark problems together with eight real-world problems from CEC 2011. The proposed algorithms have outperformed FA algorithm in both benchmark and real-world problems. The optimal solutions found by FA in benchmark problems was 69.2\% while FA-Step algorithm achieved 73.9\% and FA-CMAES algorithm obtained 92.3\%. In real-world problem, FA obtained 37.5\%, FA-Step was 50\% while FA-CMAES was 75\%. The results invariably show that the proposed algorithms perform significantly better than the standalone methods as well as the algorithms from the literature.},
  archive      = {J_ASOC},
  author       = {Ravneil Nand and Bibhya Nand Sharma and Kaylash Chaudhary},
  doi          = {10.1016/j.asoc.2021.107517},
  journal      = {Applied Soft Computing},
  pages        = {107517},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stepping ahead firefly algorithm and hybridization with evolution strategy for global optimization problems},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative filtering via heterogeneous neural networks.
<em>ASOC</em>, <em>109</em>, 107516. (<a
href="https://doi.org/10.1016/j.asoc.2021.107516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last few years, the deep neural network is utilized to solve the collaborative filtering problem, a method of which has achieved immense success on computer vision , speech recognition as well as natural language processing . On one hand, the deep neural network can be used to capture the side information of users and items. On the other hand, it is also capable of modeling interactions between users and items. Most of existing approaches exploit the neural network with solo structure to model user–item interactions such that the learning representation may be insufficient over the extremely sparse rating data. Recently, a large number of neural networks with mixed structures are devised for learning better representations. A carefully designed hybrid network is able to achieve considerable accuracy but only requires a small amount of extra computation. In order to model user–item interactions, we elaborate a hybrid neural network consisting of the global neural network and several local neural blocks. The multi-layer perceptron is adopted to build the global neural network and the residual network is used to form the local neural block which is inserted into two adjacent global layers. The hybrid network is further combined with the generalized matrix factorization to capture both the linear and nonlinear relationships between users and items. It is verified by experimental results on benchmark datasets that our method is superior to certain state-of-the-art approaches in terms of top-n item recommendation.},
  archive      = {J_ASOC},
  author       = {Wei Zeng and Ge Fan and Shan Sun and Biao Geng and Weiyi Wang and Jiacheng Li and Weibo Liu},
  doi          = {10.1016/j.asoc.2021.107516},
  journal      = {Applied Soft Computing},
  pages        = {107516},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Collaborative filtering via heterogeneous neural networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cascaded panoptic segmentation method for high resolution
remote sensing image. <em>ASOC</em>, <em>109</em>, 107515. (<a
href="https://doi.org/10.1016/j.asoc.2021.107515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Great progress has been made for remote sensing image segmentation with the development of Deep Convolutional Neural Networks . However, Multiple convolutions significantly reduce the resolution and lead to the loss of many key information, the prediction accuracy of pixel categories is reduced. And DCNN accumulate context information on a large receptive field, which leads to blurred boundary segmentation of objects. This paper proposes a cascaded panoptic segmentation network to target the aforementioned problems. Firstly, a shared feature pyramid network backbone and a new hybrid task cascade framework are designed, which share the features and integrate the complementary features of different tasks in different stages, which can extract rich context information. Then, a functional module is designed to learn the mask quality of predicted instances in Mask R-CNN to calibrate the inconsistency between mask quality and mask score, thus to deal with the scale change of the object. Finally, a new Visual-saliency ranking module is designed to overcome the mutual occlusion problem between the prediction results, and strengthen robustness to illumination. The experimental results prove that our method still has significant advantages even compared with the most advanced methods, and ablation experiments also verify the effectiveness of our designed strategies.},
  archive      = {J_ASOC},
  author       = {Xia Hua and Xinqing Wang and Ting Rui and Faming Shao and Dong Wang},
  doi          = {10.1016/j.asoc.2021.107515},
  journal      = {Applied Soft Computing},
  pages        = {107515},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cascaded panoptic segmentation method for high resolution remote sensing image},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Noise-tolerant gradient-oriented neurodynamic model for
solving the sylvester equation. <em>ASOC</em>, <em>109</em>, 107514. (<a
href="https://doi.org/10.1016/j.asoc.2021.107514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recursive neural networks are generally divided into dynamic neural networks and static neural networks to refer to the neural networks with one or more feedback links in the network structure. Inevitably, there exist some problems such as poor approximation performance and poor stable convergence performance due to complex network structure. The noise-tolerant gradient-oriented neurodynamic (NTGON) model proposed in this study is an improved model based on the traditional idea of a gradient neural network (GNN) model. The proposed NTGON model can obtain accurate and efficient results under the condition of various noises when computing the Sylvester equation , which is effectively used to solve various problems with noise pollution that are frequently encountered in practical engineering. Compared with the original GNN model for the Sylvester equation , the NTGON model exponentially converges to the theoretical solution starting from any initial state. It is demonstrated that the noise-polluted NTGON model converges to the theoretical solution globally no matter how large the unknown matrix-form noise is. Furthermore, simulation results show that the proposed NTGON model achieves a performance that is superior to that of the original GNN model for solving the Sylvester equation in the presence of noise.},
  archive      = {J_ASOC},
  author       = {Bei Liu and Dongyang Fu and Yimeng Qi and Haoen Huang and Long Jin},
  doi          = {10.1016/j.asoc.2021.107514},
  journal      = {Applied Soft Computing},
  pages        = {107514},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Noise-tolerant gradient-oriented neurodynamic model for solving the sylvester equation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust MILP and gene expression programming based on
heuristic rules for mixed-model multi-manned assembly line balancing.
<em>ASOC</em>, <em>109</em>, 107513. (<a
href="https://doi.org/10.1016/j.asoc.2021.107513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current dynamic markets require manufacturing industries to organize a robust plan to cope with uncertain demand planning. This work addresses the mixed-model multi-manned assembly line balancing under uncertain demand and aims to optimize the assembly line configuration by a robust mixed-integer linear programming (MILP) model and a robust solution generation mechanism embedded with dispatching rules . The proposed model relaxes the cycle time constraint and designs robust sequencing constraints and objective functions to ensure the line configuration can meet all the demand plans. Furthermore, two solution generation mechanisms, including a task-operator-sequence and an operator-task-sequence, are designed. To quickly find a suitable line configuration, a gene expression programming (GEP) approach with multi-attribute representation is proposed to obtain efficient dispatching rules which are ultimately embedded into the solution generation mechanisms. Experimental results show that solving the proposed MILP model mathematically is effective when tackling small and medium-scale instances. However, for large instances, the dispatching rules generated by the GEP have significant superiority over traditional heuristic rules and those rules mined by a genetic programming algorithm.},
  archive      = {J_ASOC},
  author       = {Zikai Zhang and Qiuhua Tang and Manuel Chica},
  doi          = {10.1016/j.asoc.2021.107513},
  journal      = {Applied Soft Computing},
  pages        = {107513},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A robust MILP and gene expression programming based on heuristic rules for mixed-model multi-manned assembly line balancing},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic recognition of glaucoma in fundus images using
deep learning and random forest classifier. <em>ASOC</em>, <em>109</em>,
107512. (<a href="https://doi.org/10.1016/j.asoc.2021.107512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is perpetual damage of optic nerves which causes fractional or complete visual misfortune. The fundamental reason for this illness is the increment of the intra-ocular pressure inside the eye which harms the optic nerve. Retinal images give indispensable data about an eye’s wellbeing. Based on progressions in retinal images technology, it is conceivable to create frameworks that can investigate these retinal imageries for better determination. This work presents a glaucoma recognition technique by estimating CDR (Cup to Disc Ratio) from fundus images. The size of the optic disc and optic cup is utilized to distinguish the presence of glaucoma. Therefore, the segmentation of the optic disc and optic cup is the primary step in glaucoma recognition. Decreasing the number of features and reducing the error are the two clashing destinations. The proposed glaucoma recognition technique comprises image acquisition, feature extraction, and glaucoma evaluation stages. The contrast enhancement operation is performed in image acquisition. While boundaries of OD (Optic Disc) and OC (optic cup) are segmented in the feature extraction stage and it is performed by utilizing Au-Net. Then CDR ratio of an abused image is computed to assess glaucoma in the images. Thereafter, a random forest classifier has been used to classify the glaucomatous images based on the CDR values. The performance of the proposed method has been evaluated with different techniques such as Deformable U-Net, Full-Deformable U-Net, and Original U-Net. From the outcomes, it can be noticed that the proposed method gives better performance in terms of classification accuracy of 99\% and 14\% of segmentation accuracy has been compared when compared with Original U-Net.},
  archive      = {J_ASOC},
  author       = {Shanmugam P. and Raja J. and Pitchai R.},
  doi          = {10.1016/j.asoc.2021.107512},
  journal      = {Applied Soft Computing},
  pages        = {107512},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An automatic recognition of glaucoma in fundus images using deep learning and random forest classifier},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic boundary enhancement and position attention network
with long-range dependency for semantic segmentation. <em>ASOC</em>,
<em>109</em>, 107511. (<a
href="https://doi.org/10.1016/j.asoc.2021.107511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some factors hinder advancements in semantic segmentation techniques, including intraclass inconsistency and interclass distinction. In addition, existing methods with self-attention mechanisms disregard semantic boundaries in highest-level feature maps, which hinders further performance improvement. To improve semantic segmentation performance, we propose a semantic boundary enhancement and position network (SBEPNet) that enhances feature maps with semantic boundaries and captures useful contextual information along these boundaries. Specifically, edge enhancement is employed to extract semantic boundaries, which are used to enhance the high-level feature maps. The enhanced feature maps are input into the boundary enhancement attention module to guide the learning of the discriminative long-range dependencies along object boundaries. The resultant feature maps are further refined by fusing the feature maps from the position attention module and the original feature maps. The experimental results verify the effectiveness of SBEPNet, which demonstrates the high potential for improving the generalizability of semantic segmentation.},
  archive      = {J_ASOC},
  author       = {Xi Chen and Zhen Han and Xiaoping Liu and Zhiqiang Li and Tao Fang and Hong Huo and Qingli Li and Min Zhu and Min Liu and Haolei Yuan},
  doi          = {10.1016/j.asoc.2021.107511},
  journal      = {Applied Soft Computing},
  pages        = {107511},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semantic boundary enhancement and position attention network with long-range dependency for semantic segmentation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Increasing energy efficiency of rule-based fuzzy clustering
algorithms using CLONALG-m for wireless sensor networks. <em>ASOC</em>,
<em>109</em>, 107510. (<a
href="https://doi.org/10.1016/j.asoc.2021.107510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of its efficiency, clustering is used for effective communication in Wireless Sensor Networks (WSNs). In the WSN clustering area, fuzzy approaches are found to be superior to crisp cluster counterparts when the boundaries between clusters are unclear. As a result, many studies have proposed some fuzzy-based solutions to the cluster problem in WSNs. Most rule-based fuzzy clustering systems employ field experts in trial and error processes, identifying and defining fuzzy rules as well as the forms of membership functions at the output; thus, considerable time has been allocated to realize and define these functions. Therefore, it is almost impossible or impractical to achieve a fuzzy system optimally. In this study, we propose a modified clonal selection algorithm (CLONALG-M) to improve the energy efficiency of rule-based fuzzy clustering algorithms . Although some studies in the literature focus on fuzzy optimization in general, to the best of our knowledge, performance improvement of rule-based fuzzy clustering algorithms is not taken into account. The CLONALG-M algorithm based on the Clonal Selection Principle is used to elucidate the basic principles of an adaptive immune system. In this study, we apply this principle to determine the approximate deployment of output-based membership functions that increase the performance of rule-based fuzzy clustering algorithms , whose rule base and shape of membership functions are previously known. Experimental analysis and evaluations of the proposed approach have been performed on selected fuzzy clustering approaches, and obtained results show that our approach performs and adapts well for improving performance of fuzzy output functions.},
  archive      = {J_ASOC},
  author       = {Seyyit Alper Sert and Adnan Yazici},
  doi          = {10.1016/j.asoc.2021.107510},
  journal      = {Applied Soft Computing},
  pages        = {107510},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Increasing energy efficiency of rule-based fuzzy clustering algorithms using CLONALG-M for wireless sensor networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hidden multi-distance loss-based full-convolution hashing.
<em>ASOC</em>, <em>109</em>, 107508. (<a
href="https://doi.org/10.1016/j.asoc.2021.107508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve retrieval efficiency and quality, learning to hash has been widely used in approximate nearest neighbor queries . Deep learning is characterized by high precision in extracting data features ; therefore, deep-learning-based hashing has attracted more attention. Existing methods have some weaknesses, such as complex training and losing spatial information. We design a new deep hashing algorithm named HLFH, which is very simple technique but achieves amazingly good performance. HLFH is optimized and improved in two aspects: network structure and hashing loss. Concerning network structure, a new full convolutional hashing network is proposed to preserve spatial information of features. A smooth activation function is used in the hashing layer to reduce the quantization error . Concerning hashing loss, the semantic information of data is then used to generate binary codes by hidden multi-distance loss, i.e., combination of triplet loss and quadruplet loss. With these two new techniques, our method is more accurate than many other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Mingwen Yuan and Binbin Qin and Jianhao Li and Jiangbo Qian and Yu Xin},
  doi          = {10.1016/j.asoc.2021.107508},
  journal      = {Applied Soft Computing},
  pages        = {107508},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hidden multi-distance loss-based full-convolution hashing},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AWAP: Adaptive weighted attribute propagation enhanced
community detection model for bitcoin de-anonymization. <em>ASOC</em>,
<em>109</em>, 107507. (<a
href="https://doi.org/10.1016/j.asoc.2021.107507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin is a kind of decentralized cryptocurrency and widely used in online payment partially for its anonymity mechanism. The anonymity, however, also attracts the usage of cryptocurrency by criminals in ransomware and money laundering, and limits its further application and development. In this paper, we aim to improve Bitcoin’s auditability with de-anonymization. Many previous studies have used heuristic clustering or supervised machine learning to analyze the historical transactions for identifying user behaviors. However, heuristic clustering only considers the topological structure of the transaction graph and ignores the transaction attributes . While supervised learning is usually limited by the size of labeled datasets, resulting in an unsatisfactory accuracy . To resolve the above problems, we propose an A daptive W eighted A ttribute P ropagation enhanced community detection model, named AWAP, which considers both the transaction’s topological structure and the transaction attributes . We first parse the transaction data from the public ledger and construct a bipartite graph to describe correlations between addresses and transactions. Then, we use a five-step feature engineering pipeline to extract Bitcoin address attributes and build an attribute graph. Finally, we design an adaptive weighted attribute propagation algorithm running on the attribute graph to classify the Bitcoin addresses and identify user behaviors. Extensive experiments highlight that AWAP model achieves 12\% higher accuracy and 25\% higher F-score on average, compared to the state-of-the-art Bitcoin address classifiers and other community detection algorithms . To evaluate the effectiveness of AWAP, we also present two case studies on Bitcoin address classification and Bitcoin trace-ability in ransomware .},
  archive      = {J_ASOC},
  author       = {Xie Xueshuo and Wang Jiming and Ye Junyi and Fang Yaozheng and Lu Ye and Li Tao and Wang Guiling},
  doi          = {10.1016/j.asoc.2021.107507},
  journal      = {Applied Soft Computing},
  pages        = {107507},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AWAP: Adaptive weighted attribute propagation enhanced community detection model for bitcoin de-anonymization},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantics aware adversarial malware examples generation for
black-box attacks. <em>ASOC</em>, <em>109</em>, 107506. (<a
href="https://doi.org/10.1016/j.asoc.2021.107506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial pseudo-benign examples can be generated to evade malware detection algorithms based on deep learning . Current works on adversarial examples generation mainly focus on the gradient-based attacks due to their easy-to-implement features. Although the Generative Adversarial Network (GAN) has shown a superior performance on adversarial attacks , there is not much work on applying GAN to malware composition due to its complexity and weakness in processing discrete data. API call sequence is considered as the very representative feature to analyze malware behavioral characteristics. However, it is troublesome to insert API calls into the original sequence to cover the malicious purpose with implementation on GAN. In this paper, we propose an adversarial sequence generating algorithm, which highlights the contextual relationship between API calls by using word embedding . We train a recurrent neural network based substitute detection model to fit the black-box malware detection model. We demonstrate the attack against API call sequence-based malware classifiers, and experimental results show that the proposed scheme is efficient and effective, almost all of the generated pseudo-benign malware examples can fool the detection algorithms. It outruns other GAN based schemes in performance and has a lower overhead of API call inserting.},
  archive      = {J_ASOC},
  author       = {Xiaowei Peng and Hequn Xian and Qian Lu and Xiuqing Lu},
  doi          = {10.1016/j.asoc.2021.107506},
  journal      = {Applied Soft Computing},
  pages        = {107506},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semantics aware adversarial malware examples generation for black-box attacks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A3CMal: Generating adversarial samples to force targeted
misclassification by reinforcement learning. <em>ASOC</em>,
<em>109</em>, 107505. (<a
href="https://doi.org/10.1016/j.asoc.2021.107505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms have been proved to be vulnerable to adversarial attacks . The potential adversary is able to force the model to produce deliberate errors by elaborately modifying the training samples. For malware analysis, most of the existing research on evasion attacks focuses on a detection scenario, while less attention is paid to the classification scenario which is vital to decide a suitable system response in time. To fulfill this gap, this paper tries to address the misclassification problem in malware analysis. A reinforcement learning model named A3CMal is proposed. This adversarial model aims to generate adversarial samples which can fool the target classifier. As a core component of A3CMal, the self-learning agent constantly takes optimal actions to confuse the classification by slightly modifying samples on the basis of the observed states. Extensive experiments are performed to test the validity of A3CMal. The results show that the proposed A3CMal can force the target classifier to make wrong predictions while preserving the malicious functionality of the malware. Remarkably, not only can it cause the system to indicate an incorrect classification, but also can mislead the target model to classify malware into a specific category. Furthermore, our experiments demonstrate that the PE-based classifier is vulnerable to the adversarial samples generated by A3CMal.},
  archive      = {J_ASOC},
  author       = {Zhiyang Fang and Junfeng Wang and Jiaxuan Geng and Yingjie Zhou and Xuan Kan},
  doi          = {10.1016/j.asoc.2021.107505},
  journal      = {Applied Soft Computing},
  pages        = {107505},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A3CMal: Generating adversarial samples to force targeted misclassification by reinforcement learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multilabel CNN for forensic footwear impression
descriptor identification. <em>ASOC</em>, <em>109</em>, 107496. (<a
href="https://doi.org/10.1016/j.asoc.2021.107496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years deep neural networks have become the workhorse of computer vision . In this paper, we employ a deep learning approach to classify footwear impression’s features known as descriptors for forensic use cases. Within this process, we develop and evaluate an effective technique for feeding downsampled greyscale impressions to a neural network pre-trained on data from a different domain. Our approach relies on learnable preprocessing layer paired with multiple interpolation methods used in parallel. We empirically show that this technique outperforms using a single type of interpolated image without learnable preprocessing, and can help to avoid the computational penalty related to using high resolution inputs, by making more efficient use of the low resolution inputs. We also investigate the effect of preserving the aspect ratio of the inputs, which leads to considerable boost in accuracy without increasing the computational budget with respect to squished rectangular images. Finally, we formulate a set of best practices for transfer learning with greyscale inputs, potentially widely applicable in computer vision tasks ranging from footwear impression classification to medical imaging .},
  archive      = {J_ASOC},
  author       = {Marcin Budka and Akanda Wahid Ul Ashraf and Matthew Bennett and Scott Neville and Alun Mackrill},
  doi          = {10.1016/j.asoc.2021.107496},
  journal      = {Applied Soft Computing},
  pages        = {107496},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep multilabel CNN for forensic footwear impression descriptor identification},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-root solver for discontinuous and non-differentiable
equations by integrating genetic algorithm and derivative-free iterative
methods. <em>ASOC</em>, <em>109</em>, 107493. (<a
href="https://doi.org/10.1016/j.asoc.2021.107493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic Algorithm (GA) has a strong global searching ability but limited convergence efficiency at later stage, while derivative-free iterative methods have high local convergence efficiency but strict requirements on the initial approximation. Combining GA and derivative-free iterative methods, a multi-root solver is proposed for a class of complex nonlinear functions with discontinuity, non-differentiability and multi-root in this work. Firstly, an improved GA (IGA) is presented by integrating an adaptive crossover operator and three kinds of manual intervention measures into the standard GA to further improve the global searching ability. Then, a global search in the domain is implemented by using the proposed IGA, and the qualified individuals are selected as initial approximations. Finally, based on this, local accurate convergence is achieved by combining the derivative-free iterative method. To demonstrate the effectiveness of the proposed method, a series of numerical experiments are conducted, and the results show that the proposed multi-root solver has better performance in efficiency, stability as well as applicability.},
  archive      = {J_ASOC},
  author       = {Tian Qiu and Haigen Hu and Ruixin Chen and Qianwei Zhou and Qiu Guan and Xiaoxin Li},
  doi          = {10.1016/j.asoc.2021.107493},
  journal      = {Applied Soft Computing},
  pages        = {107493},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-root solver for discontinuous and non-differentiable equations by integrating genetic algorithm and derivative-free iterative methods},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Search trajectory networks: A tool for analysing and
visualising the behaviour of metaheuristics. <em>ASOC</em>,
<em>109</em>, 107492. (<a
href="https://doi.org/10.1016/j.asoc.2021.107492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of metaheuristics inspired by natural and social phenomena have been proposed in the last few decades, each trying to be more powerful and innovative than others. However, there is a lack of accessible tools to analyse, contrast and visualise the behaviour of metaheuristics when solving optimisation problems . When the metaphors are stripped away, are these algorithms different in their behaviour? To help to answer this question, we propose a data-driven, graph-based model, search trajectory networks (STNs) in order to analyse, visualise and directly contrast the behaviour of different types of metaheuristics. One strength of our approach is that it does not require any additional sampling or algorithmic methods. Instead, the models are constructed from data gathered while the metaheuristics are solving the optimisation problems . We present our methodology, and consider in detail two case studies covering both continuous and combinatorial optimisation. In terms of metaheuristics, our case studies cover the main current paradigms: evolutionary, swarm, and stochastic local search approaches.},
  archive      = {J_ASOC},
  author       = {Gabriela Ochoa and Katherine M. Malan and Christian Blum},
  doi          = {10.1016/j.asoc.2021.107492},
  journal      = {Applied Soft Computing},
  pages        = {107492},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Search trajectory networks: A tool for analysing and visualising the behaviour of metaheuristics},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level transfer learning for improving the performance
of deep neural networks: Theory and practice from the tasks of facial
emotion recognition and named entity recognition. <em>ASOC</em>,
<em>109</em>, 107491. (<a
href="https://doi.org/10.1016/j.asoc.2021.107491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning has become a promising field in machine learning owing to its wide application prospects. Its effectiveness has spawned various methodologies and practices. Transfer learning refers to improving the performance of target learners in the target domain by transferring the knowledge contained in different yet related source domains. In other words, we can use data from additional domains or tasks to train a model with superior generalization. Using transfer learning, the dependence on considerable target-domain data can be reduced, thereby constructing target learners. Recently, the fields of computer vision (CV) and natural language processing (NLP) have witnessed the emergence of transfer learning, which has significantly improved the most advanced technology on a wide range of CV and NLP tasks. A typical approach of applying transfer learning to deep neural networks is to fine-tune a pretrained model of the source domain with data obtained from the target domain. This paper proposes a novel framework, based on the fine-tuning approach, called multilevel transfer learning (mLTL). Under this framework, we concluded the crucial findings and principles regarding the training sequence of related domain datasets and demonstrated its effectiveness by performing facial emotion and named entity recognition tasks. According to the experimental results, the deep neural network models using mLTL outperformed the original models on the target tasks.},
  archive      = {J_ASOC},
  author       = {Jason C. Hung and Jia-Wei Chang},
  doi          = {10.1016/j.asoc.2021.107491},
  journal      = {Applied Soft Computing},
  pages        = {107491},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-level transfer learning for improving the performance of deep neural networks: Theory and practice from the tasks of facial emotion recognition and named entity recognition},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative filtering via factorized neural networks.
<em>ASOC</em>, <em>109</em>, 107484. (<a
href="https://doi.org/10.1016/j.asoc.2021.107484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plenty of deep learning models have been extensively studied over recent years to meet the needs of personalized services . In order to improve the recommendation quality, neural networks are devised more and more complex. As a result, a large number of parameters emerge in the network which significantly increases the burden of computation and storage. When these deep learning models run on the resource-limited devices, their applications are immensely restrained. In consequence, to reduce parameters in the network is of great importance in the aspect of application. In this paper, the factorized neural network model (FNN) is proposed to solve such a problem. The conventional matrix factorization method is used to decompose the weight matrix of each layer in the fully-connected network and the regularization is applied to place constraints on parameters. By our method, more than 95\% of parameters in the weighted matrices can be reduced and there is a less than 1\% accuracy drop.},
  archive      = {J_ASOC},
  author       = {Xinke Zhao and Wei Zeng and Yixin He},
  doi          = {10.1016/j.asoc.2021.107484},
  journal      = {Applied Soft Computing},
  pages        = {107484},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Collaborative filtering via factorized neural networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Negative correlation hidden layer for the extreme learning
machine. <em>ASOC</em>, <em>109</em>, 107482. (<a
href="https://doi.org/10.1016/j.asoc.2021.107482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme Learning Machine (ELM) algorithms have achieved unprecedented performance in supervised machine learning tasks. However, the preconfiguration of the nodes in the hidden layer in ELM models through randomness does not always lead to a suitable transformation of the original features. Consequently, the performance of these models relies on broad exploration of these feature mappings, generally using a large number of nodes in the hidden layer. In this paper, a novel ELM architecture is presented, called Negative Correlation Hidden Layer ELM (NCHL-ELM), based on the Negative Correlation Learning (NCL) framework. This model incorporates a parameter into each node in the original ELM hidden layer, and these parameters are optimized by reducing the error in the training set and promoting the diversity among them in order to improve the generalization results. Mathematically, the ELM minimization problem is perturbed by a penalty term, which represents a measure of diversity among the parameters. A variety of regression and classification benchmark datasets have been selected in order to compare NCHL-ELM with other state-of-the-art ELM models. Statistical tests indicate the superiority of our method in both regression and classification problems.},
  archive      = {J_ASOC},
  author       = {Carlos Perales-González and Francisco Fernández-Navarro and Javier Pérez-Rodríguez and Mariano Carbonero-Ruz},
  doi          = {10.1016/j.asoc.2021.107482},
  journal      = {Applied Soft Computing},
  pages        = {107482},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Negative correlation hidden layer for the extreme learning machine},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information guiding and sharing enhanced simultaneous heat
transfer search and its application to k-means optimization.
<em>ASOC</em>, <em>109</em>, 107476. (<a
href="https://doi.org/10.1016/j.asoc.2021.107476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Heat Transfer Search (SHTS) is a novel meta-heuristic algorithm proposed recently and it can solve some optimization problems . However, it still has some deficiencies, such as weak search ability owing to poor information exploitation so that it cannot solve complicated problems well. So an Information Guiding and Sharing enhanced SHTS (IGS-SHTS) is proposed in this paper. Firstly, Grey wolf optimizer is embedded into SHTS to enhance information sharing through the two different search methods. Secondly, an information sharing way through different agents is added. Thirdly, a sinusoidal crossover is utilized to fulfill information guiding through the historical individual-best population guiding the current one. Finally, an information guiding way, the global best agent guiding the worst one, is embedded to strengthen the worst agent. A large number of experimental results on the complex functions from CEC2017 test set and clustering optimization show that IGS-SHTS has stronger search ability compared with SHTS and quite a few state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Xinming Zhang and Fangyuan Yang},
  doi          = {10.1016/j.asoc.2021.107476},
  journal      = {Applied Soft Computing},
  pages        = {107476},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Information guiding and sharing enhanced simultaneous heat transfer search and its application to k-means optimization},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting nickel futures price based on the empirical
wavelet transform and gradient boosting decision trees. <em>ASOC</em>,
<em>109</em>, 107472. (<a
href="https://doi.org/10.1016/j.asoc.2021.107472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the prediction accuracy of futures price, this paper proposes a hybrid approach based on gradient boosting decision tree (GBDT), correlation analysis, and empirical wavelet transform (EWT) and applies it to predict the settlement price of the London Metal Exchange (LME) Nickel. Firstly, the Spearman correlation coefficient is adopted to analyze the relationship between each input variable and the settlement price. Then, the EWT likewise used to decompose each variable time series into an independent component. And those components are paralleled as new input variables. Next, the GBDT is carried for forecasting the settlement price of LME Nickel. To validate the effectiveness of the EWT, comparison experiments between EMD (empirical mode decomposition) and EWT are carried out. The decomposition results show that the EWT can correctly decompose the settlement price and avoid redundant components. To better measure the viability and efficiency of the proposed method, experiments are on three financial data sets from Tushare before predicting Nickel price. The various implementations are illustrated that the EWT–GBDT can achieve the best results no matter on which data set. Therefore, we have reason to believe that the EWT–GBDT is a useful algorithm to predict LME Nickel settlement prices.},
  archive      = {J_ASOC},
  author       = {Qinghua Gu and Yinxin Chang and Naixue Xiong and Lu Chen},
  doi          = {10.1016/j.asoc.2021.107472},
  journal      = {Applied Soft Computing},
  pages        = {107472},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting nickel futures price based on the empirical wavelet transform and gradient boosting decision trees},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning where to look for COVID-19 growth: Multivariate
analysis of COVID-19 cases over time using explainable convolution–LSTM.
<em>ASOC</em>, <em>109</em>, 107469. (<a
href="https://doi.org/10.1016/j.asoc.2021.107469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determinant factors which contribute to the prediction should take into account multivariate analysis for capturing coarse-to-fine contextual information. From the preliminary descriptive analysis, it shows that environmental factor such as UV (ultraviolet) is one of the essential factors that should be considered to observe the COVID-19 epidemic drivers. Moreover, there are education, government, morphological, health, economic, and behavioral factors contributing to the growth of COVID-19. Besides descriptive analysis, in this research, multivariate analysis is considered to provide comprehensive explanations about factors contributing to pandemic dynamics. To achieve rich explanations, visual attribution of explainable Convolution-LSTM is utilized to see high contributing factors responsible for the growth of daily COVID-19 cases. Our model consists of 1 D CNN in the first layer to capture local relationships among variables followed by LSTM layers to capture local dependencies over time. It produces the lowest prediction errors compared to the other existing models. This permits us to employ gradient-based visual attribution for generating saliency maps for each time dimension and variable. These are then used for explaining which variables throughout which period of the interval is contributing for a given time-series prediction, likewise as explaining that during that time intervals were the joint contribution of most vital variables for that prediction. The explanations are useful for stakeholders to make decisions during and post pandemics. The explainable Convolution–LSTMcode is available here: https://github.com/cbasemaster/time-series-attribution .},
  archive      = {J_ASOC},
  author       = {Novanto Yudistira and Sutiman Bambang Sumitro and Alberth Nahas and Nelly Florida Riama},
  doi          = {10.1016/j.asoc.2021.107469},
  journal      = {Applied Soft Computing},
  pages        = {107469},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning where to look for COVID-19 growth: Multivariate analysis of COVID-19 cases over time using explainable convolution–LSTM},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification-enhancement deep hashing for large-scale
video retrieval. <em>ASOC</em>, <em>109</em>, 107467. (<a
href="https://doi.org/10.1016/j.asoc.2021.107467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of video data on the Internet, retrieving and detecting similar video contents effectively has become a challenging problem. Whereas hashing is a mature technique for dealing with this problem, especially in image retrieval , when hashing techniques are applied to videos, a large gap in performance is always encountered compared with that of image hashing methods because of the complicated structures of videos. Generally, existing video hashing methods just directly apply image hashing approaches into video frames without considering temporal structure , leading to low performance in video retrieval. In this study, we proposed a video hashing method, called classification-enhancement deep hashing (CEDH), for large-scale video searches. The proposed CEDH first fuses the spatial–temporal information of videos in a deep end-to-end hashing network, and then leverages both neighborhood structure of semantics and triple similarity information to learn video hash codes. Subsequently, to enhance the precision of the hash codes during hash learning, a classification module is added after the fully connected layer of the deep network. We also use an additional code constraint to make the hash codes more suitable for containing sufficient information. Extensive experiments on three real-world large-scale video datasets show that our proposed method significantly outperforms state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Xiushan Nie and Xin Zhou and Yang Shi and Jiande Sun and Yilong Yin},
  doi          = {10.1016/j.asoc.2021.107467},
  journal      = {Applied Soft Computing},
  pages        = {107467},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification-enhancement deep hashing for large-scale video retrieval},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SVM-based anomaly detection in remote working: Intelligent
software SmartRadar. <em>ASOC</em>, <em>109</em>, 107457. (<a
href="https://doi.org/10.1016/j.asoc.2021.107457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing the productivity of flexible working during working hours is a common concern for employers. Employees working in different sectors use various tools to carry out their tasks and work in different working environments. With the Covid-19 pandemic that began in the beginning of 2020, remote working has become an essential part of life. However, one of the biggest problems faced by managers and employers is how to control remote workers. In this study, SmartRadar software was developed to track employee computer use behavior and detect anomalous behavior. Anomalous behavior is defined as computer-based activities or processes carried out during work time which are not related to the tasks for which the employee is responsible. Clicking, mouse wheel scrolling, copying and other similar actions by the user are processed, a summary of the data is generated, and a multi-dimensional dataset is created. Anomalous behavior can then be detected using support vector machines . The proposed software has been shown to detect anomalous computer use behavior by employees with a high degree of accuracy. The favorable results of the study show that the proposed method and the software could be used for tracking and reporting purposes both in workplaces and in flexible working conditions.},
  archive      = {J_ASOC},
  author       = {Mustafa Akpinar and M. Fatih Adak and Goker Guvenc},
  doi          = {10.1016/j.asoc.2021.107457},
  journal      = {Applied Soft Computing},
  pages        = {107457},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SVM-based anomaly detection in remote working: Intelligent software SmartRadar},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of risk-based fuzzy decision support systems in
new product development: An r-VIKOR approach. <em>ASOC</em>,
<em>109</em>, 107456. (<a
href="https://doi.org/10.1016/j.asoc.2021.107456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Innovative manufacturing firms strive to sustain and enhance their competitive advantages by running a range of new product development (NPD) projects in a consistent manner. The capital and time required to execute the NPD projects have substantially increased over the past years. This magnified the risk-aversion behavior of R&amp;D managers and has increased their sensitivity towards the underlying risk of NPD projects. In particular, the R&amp;D departments have recently started to proactively assess the accuracy of ambiguous information that is extensively used in preliminary market study and customer requirements analysis. Thanks to its high performance in dynamic environments, the R-numbers method can be employed to capture and analyze the risk of fuzzy numbers in a variety of decision making models. To tackle the complexity of such analysis, this paper proposes a novel risk-based fuzzy VIKOR (R-VIKOR) methodology. Using the interpretive structural modeling, the risk factors are first classified to identify and rank the existing critical risk factors of NPD projects. The ultimate goal of this study is to develop a practical yet simple decision support system tool that enables the R&amp;D managers to effectively examine the riskiness of fuzzy information and assess the relevant risk factors. A real-world case study is presented to test and examine the accuracy and effectiveness of the proposed risk management method .},
  archive      = {J_ASOC},
  author       = {Seyedeh Anahita Mousavi and Hamidreza Seiti and Ashkan Hafezalkotob and Sobhan Asian and Rouhollah Mobarra},
  doi          = {10.1016/j.asoc.2021.107456},
  journal      = {Applied Soft Computing},
  pages        = {107456},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of risk-based fuzzy decision support systems in new product development: An R-VIKOR approach},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multi-scale attentional features for medical image
segmentation. <em>ASOC</em>, <em>109</em>, 107445. (<a
href="https://doi.org/10.1016/j.asoc.2021.107445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of medical images is a difficult task in the field of computer vision owing to the various backgrounds, shapes, size, and colors of polyps or tumors. Despite the success of deep learning (DL)-based encoder–decoder architectures in medical image segmentation , these models have several disadvantages. First, an architecture such as U-Net cannot encode multi-scale semantic information at a different level on the decoder side . Second, it fails to reimpose the feature maps adeptly due to its limited capability on capturing long-range feature dependencies. In this study, we solve this problem by capturing multi-scale global feature maps, which forces the network to learn different semantic information at each scale. Further, we utilize the attention mechanism to suppress noise and the undesirable features, leading to a thorough restoration of contextual feature dependencies. Finally, we propose a novel method which leverages the compound scaled EfficientNet as a encoder backbone for efficient feature extraction and the UNet decoder to reconstruct the fine-grained details. We evaluated the proposed method using three different medical datasets: Kvasir-SEG, nuclei segmentation, and skin-lesion segmentation. The experimental results demonstrate that the proposed method takes an unassailable lead in terms of segmentation accuracy over the baseline models across different datasets and backbone architectures. Further, the proposed method strengthens the segmentation quality of varying shapes, object shapes, suppresses the noise, and leads to a better performance.},
  archive      = {J_ASOC},
  author       = {Sahadev Poudel and Sang-Woong Lee},
  doi          = {10.1016/j.asoc.2021.107445},
  journal      = {Applied Soft Computing},
  pages        = {107445},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep multi-scale attentional features for medical image segmentation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Retraction notice to “video associated cross-modal
recommendation algorithm based on deep learning” [appl. Soft comput. 82
(2019) 105597]. <em>ASOC</em>, <em>109</em>, 107270. (<a
href="https://doi.org/10.1016/j.asoc.2021.107270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Fan Yang and Hao Xie and Huxiong Li},
  doi          = {10.1016/j.asoc.2021.107270},
  journal      = {Applied Soft Computing},
  pages        = {107270},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Retraction notice to “Video associated cross-modal recommendation algorithm based on deep learning” [Appl. soft comput. 82 (2019) 105597]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive neighbors pursuit for radar images
classification. <em>ASOC</em>, <em>109</em>, 107194. (<a
href="https://doi.org/10.1016/j.asoc.2021.107194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding appropriate class-separating metric and labeling rules is crucial in the construction of image classifiers. In this paper a Divergence-Chebyshev Neighbors Pursuit (DCNP) algorithm is proposed for rapid Polarimetric Synthetic Aperture Radar (PolSAR) image classification. First, an information-theoretic divergence is defined to measure the similarity of polarimetric features between pixels. Then a divergence-Chebyshev distance is defined to reveal the affinity of pixels in both the polarization and spatial domains. Moreover, inspired by human’s learning characteristic that the knowledge is learned little by little, the DCNP algorithm is designed to progressively determine the labels of unknown pixels. Some experiments are conducted on several real PolSAR image datasets and the results show that our method can achieve accurate classification with a small number of labeled data, and outperforms its counterparts in terms of several guidelines.},
  archive      = {J_ASOC},
  author       = {Shuyuan Yang and Guangying Xu and Huixiao Meng and Min Wang},
  doi          = {10.1016/j.asoc.2021.107194},
  journal      = {Applied Soft Computing},
  pages        = {107194},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Progressive neighbors pursuit for radar images classification},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-level k-nearest neighbors approach for invasive plants
detection and classification. <em>ASOC</em>, <em>108</em>, 107523. (<a
href="https://doi.org/10.1016/j.asoc.2021.107523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invasive plants (IPs) are considered major threat to biodiversity and ecosystems due to their potential to compete for resources with other plants or crops. Timely and effective detection is important to prevent and control their growth. Recent advancements in unmanned aerial systems (UAS) have brought an effective, inexpensive, and non-invasive approach to monitoring plants. The use of UAS is promising in the accurate and rapid identification of IPs in the field. However, challenges exist when using UAS for IP detection as they generate a huge number of images that must be interpreted by researchers manually. This time-consuming and error-prone process significantly limits the applicability of UAS in a variety of environmental and agriculture monitoring applications. To reduce processing time and increase detection accuracy, we develop an automatic and intelligent detection approach to identify IPs with high levels of efficiency and accuracy from images of the monitoring areas captured by UAS. First, the original field images are segmented into patch images using the simple linear iterative clustering (SLIC) superpixels algorithm. Then a novel two-level K-nearest neighbors (TLKNN) algorithm is proposed to classify the patch images into different IP categories. In it, a deep convolutional neural network (CNN) is used to extract the features from the patch image and a kernel map function is utilized to obtain the kernel features. Two feature sets are fed to train two K-nearest neighbor (KNN) models. Finally, the final classification is determined by the selection results by KNN models. In the experiments, two public UAV image sets are employed to train and verify the proposed method. Several metrics such as accuracy, recall, precision, and F1-score, are used to evaluate the performance. The proposed TLKNN is compared with two newly proposed convolutional neural network (CNN) methods on the same datasets. Experimental results indicate the proposed TLKNN algorithm achieves better classification performance and reaches the highest accuracy, recall rate, precision rate, and F1-score. The improved performance not only means that IPs can be detected more accurately and automatically, but also that there are environmental benefits in developing an intelligent strategy for IP prevention and control.},
  archive      = {J_ASOC},
  author       = {Yanhui Guo and Chunlai Du and Yun Zhao and Tih-Fen Ting and Thomas A. Rothfus},
  doi          = {10.1016/j.asoc.2021.107523},
  journal      = {Applied Soft Computing},
  pages        = {107523},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-level K-nearest neighbors approach for invasive plants detection and classification},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the suitability of stacking-based ensembles in smart
agriculture for evapotranspiration prediction. <em>ASOC</em>,
<em>108</em>, 107509. (<a
href="https://doi.org/10.1016/j.asoc.2021.107509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart agriculture aims at generating high harvest yields with an efficient resource management, such as the estimation of crop irrigation. One of the factors on which a productive crop irrigation depends on is evapotranspiration, defined as the water loss process from the soil. This is mainly measured by empirical equations, even though they are conditioned by the specific climatological variables they require. In recent years, data mining techniques are proposed as a powerful alternative to predict evapotranspiration. Among them, ensembles are notable in that they provide accurate estimators in different scenarios. Stacking is an ensemble-building technique aimed at strengthening the prediction capabilities of the system by the combined learning from the original features in the data and synthetic features created from the predictions of multiple models. This research proposes the usage of stacking for evapotranspiration prediction, which has been overlooked in the specialized literature, with the aim of a more sustainable management of water resources. The proposal is compared to other state-of-the-art empirical equations and data mining methods over several real-world climatological datasets of different agricultural areas in Spain. This comparison is performed considering separate datasets with features based on temperature, mass transfer, radiation and, finally, using the main meteorological variables together. The results obtained show that stacking is the best approach in all datasets and each group of features evaluated, running as good alternative to predict evapotranspiration when using data of a different nature and under different conditions.},
  archive      = {J_ASOC},
  author       = {Juan Martín and José A. Sáez and Emilio Corchado},
  doi          = {10.1016/j.asoc.2021.107509},
  journal      = {Applied Soft Computing},
  pages        = {107509},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the suitability of stacking-based ensembles in smart agriculture for evapotranspiration prediction},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved tunicate swarm algorithm: Solving the dynamic
economic emission dispatch problems. <em>ASOC</em>, <em>108</em>,
107504. (<a href="https://doi.org/10.1016/j.asoc.2021.107504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes improved tunicate swarm algorithm (ITSA) for solving and optimizing the dynamic economic emission dispatch (DEED) problem. The DEED optimization target is to reduce the fuel cost and pollutant emission of the power system . In addition, DEED is a complex optimization problem and contains multiple optimization goals. To strengthen the ability of the ITSA algorithm for solving DEED, the tent mapping is employed to generate initial population for improving the directionality in the optimization process. Meanwhile, the gray wolf optimizer is used to generate the global search vector for improving global exploration ability, and the Levy flight is introduced to expand the search range. Three test systems containing 5, 10 and 15 generator units are employed to verify the solving performance of ITSA. The test results show that the ITSA algorithm can provide a competitive scheduling plan for test systems containing different units. ITSA proposed algorithm gives the optimal economic and environmental dynamic dispatch scheme for achieving more precise dispatch strategy.},
  archive      = {J_ASOC},
  author       = {Ling-Ling Li and Zhi-Feng Liu and Ming-Lang Tseng and Sheng-Jie Zheng and Ming K. Lim},
  doi          = {10.1016/j.asoc.2021.107504},
  journal      = {Applied Soft Computing},
  pages        = {107504},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved tunicate swarm algorithm: Solving the dynamic economic emission dispatch problems},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light-weighted ensemble network with multilevel activation
visualization for robust diagnosis of COVID19 pneumonia from large-scale
chest radiographic database. <em>ASOC</em>, <em>108</em>, 107490. (<a
href="https://doi.org/10.1016/j.asoc.2021.107490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the coronavirus disease 2019 (COVID19) pandemic has killed more than one million people worldwide. In the present outbreak, radiological imaging modalities such as computed tomography (CT) and X-rays are being used to diagnose this disease, particularly in the early stage. However, the assessment of radiographic images includes a subjective evaluation that is time-consuming and requires substantial clinical skills. Nevertheless, the recent evolution in artificial intelligence (AI) has further strengthened the ability of computer-aided diagnosis tools and supported medical professionals in making effective diagnostic decisions. Therefore, in this study, the strength of various AI algorithms was analyzed to diagnose COVID19 infection from large-scale radiographic datasets. Based on this analysis, a light-weighted deep network is proposed, which is the first ensemble design (based on MobileNet, ShuffleNet, and FCNet) in medical domain (particularly for COVID19 diagnosis) that encompasses the reduced number of trainable parameters (a total of 3.16 million parameters) and outperforms the various existing models. Moreover, the addition of a multilevel activation visualization layer in the proposed network further visualizes the lesion patterns as multilevel class activation maps (ML-CAMs) along with the diagnostic result (either COVID19 positive or negative). Such additional output as ML-CAMs provides a visual insight of the computer decision and may assist radiologists in validating it, particularly in uncertain situations Additionally, a novel hierarchical training procedure was adopted to perform the training of the proposed network. It proceeds the network training by the adaptive number of epochs based on the validation dataset rather than using the fixed number of epochs. The quantitative results show the better performance of the proposed training method over the conventional end-to-end training procedure. A large collection of CT-scan and X-ray datasets (based on six publicly available datasets) was used to evaluate the performance of the proposed model and other baseline methods . The experimental results of the proposed network exhibit a promising performance in terms of diagnostic decision. An average F1 score (F1) of 94.60\% and 95.94\% and area under the curve (AUC) of 97.50\% and 97.99\% are achieved for the CT-scan and X-ray datasets, respectively. Finally, the detailed comparative analysis reveals that the proposed model outperforms the various state-of-the-art methods in terms of both quantitative and computational performance.},
  archive      = {J_ASOC},
  author       = {Muhammad Owais and Hyo Sik Yoon and Tahir Mahmood and Adnan Haider and Haseeb Sultan and Kang Ryoung Park},
  doi          = {10.1016/j.asoc.2021.107490},
  journal      = {Applied Soft Computing},
  pages        = {107490},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Light-weighted ensemble network with multilevel activation visualization for robust diagnosis of COVID19 pneumonia from large-scale chest radiographic database},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A seasonal-trend decomposition-based dendritic neuron model
for financial time series prediction. <em>ASOC</em>, <em>108</em>,
107488. (<a href="https://doi.org/10.1016/j.asoc.2021.107488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial time series prediction is a hot topic in machine learning field, but existing works barely catch the point of such data. In this study, we employ the most suitable preprocessing technology, machine learning model, and training algorithm to construct a novel seasonal-trend decomposition-based dendritic neuron model (STLDNM) to tackle this issue. The model’s unique part is to use the seasonal-trend decomposition based on loess (STL) as preprocessing technology. Particularly, the STL can extract seasonal and trend features from the original data, so that a simple polynomial fitting method can be used to handle these sub-series. Next, the remained complex residual component is predicted by an anti-overfitting dendritic neuron model (DNM) trained by an efficient back-propagation algorithm. Finally, the processed components are added up to obtain the predicting result. sixteen real-world stock market indices are used to test STLDNM. The experimental results show that it can perform significantly better than other previous convinced models under different assessment criteria. This model successfully reveals the internal feature of financial data and certainly improves the predicting accuracy due to the rightful methodology selection. Therefore, the newly designed STLDNM not only has high potentials for practical applications in the financial aspect but also provides novel inspirations for complex time series prediction problem researchers.},
  archive      = {J_ASOC},
  author       = {Houtian He and Shangce Gao and Ting Jin and Syuhei Sato and Xingyi Zhang},
  doi          = {10.1016/j.asoc.2021.107488},
  journal      = {Applied Soft Computing},
  pages        = {107488},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A seasonal-trend decomposition-based dendritic neuron model for financial time series prediction},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A financial statement fraud model based on synthesized
attribute selection and a dataset with missing values and imbalanced
classes. <em>ASOC</em>, <em>108</em>, 107487. (<a
href="https://doi.org/10.1016/j.asoc.2021.107487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many financial fraud events have occurred over the past few decades. These events have led to massive losses for investors. Hence, government officials have begun to focus on the problem and have issued several decrees (acts) on financial fraud. Many scholars have explored the factors of financial fraud, and although their results performed well, most studies have not generated a set of useful rules to support auditors. Furthermore, data on financial statement fraud usually constitute an imbalanced class problem, and previous work minimally addresses this problem. Therefore, this study, based on the handling of missing values and imbalanced classes, builds a detecting model of financial statement fraud. First, it utilizes listwise and pairwise deletion to remove missing values. Second, it proposes three merged attribute selection methods and applies a nonlinear distance correlation to select important attributes. Third, it applies undersampling and oversampling to address the imbalanced classes. Finally, it uses rule-based classifiers to generate a set of useful rules. In practice, this study employs a list of fraudulent companies to collect data on financial statement fraud. We summarize the results as follows: (1) the pairwise deletion removes fewer records than does listwise removal in handling missing values; (2) the merged attribute selection (Com_I4) has the best performance on the four evaluation criteria; (3) the oversampling can enhance accuracy, and has the lowest type 1 and type 2 errors; (4) the random forest of Com_I4 can build the optimal model of financial statement fraud in the pairwise deletion and random oversampling; and (5) the results show that the ensemble learning (random forest) is a robust model in this study. Finally, these results in this study can be provided to practitioners, investors, and auditing personnel as references.},
  archive      = {J_ASOC},
  author       = {Ching-Hsue Cheng and Yung-Fu Kao and Hsien-Ping Lin},
  doi          = {10.1016/j.asoc.2021.107487},
  journal      = {Applied Soft Computing},
  pages        = {107487},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A financial statement fraud model based on synthesized attribute selection and a dataset with missing values and imbalanced classes},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective dyna-q based routing in wireless mesh
network. <em>ASOC</em>, <em>108</em>, 107486. (<a
href="https://doi.org/10.1016/j.asoc.2021.107486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routing is an essential part for network deployment to maintain and improve the network performance. With the rapid demands of various wireless applications, delay and energy efficiency are two fundamentally important aspects in the next-generation communication. A multi-objective Dyna-Q based routing (MODQR) approach to improve both delay and energy performances is proposed in this paper. For delay, the interference, asymmetrical link condition and probability of transmission failure (PTF) are considered. When deriving the PTF, the gray physical interference model is used. For energy, the ratio of consumed energy to energy capacity is used to monitor energy condition. Reinforcement learning is an effective way to solve the routing problem in a network with uncertain conditions. A path can be chosen by iterative exploration and exploitation. Dyna-Q is an effective reinforcement learning algorithm which can increase the convergence speed. To the best of our knowledge, Dyna-Q is used to solve the multi-objective routing problem for the first time. The path with least end-to-end delay and largest energy efficiency will be selected. Simulation results show that MODQR can obtain up to 80.97\%, 83.48\% and 86.15\% better network performance than three other state-of-the-art routing methods.},
  archive      = {J_ASOC},
  author       = {Yuan Chai and Xiao-Jun Zeng},
  doi          = {10.1016/j.asoc.2021.107486},
  journal      = {Applied Soft Computing},
  pages        = {107486},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective dyna-Q based routing in wireless mesh network},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural network-based multi-task learning for inpatient flow
classification and length of stay prediction. <em>ASOC</em>,
<em>108</em>, 107483. (<a
href="https://doi.org/10.1016/j.asoc.2021.107483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inpatient unit resources are among the most expensive and valuable resources for healthcare organizations. Inpatient resources such as room, bed, and medical devices can be more efficiently managed if we can predict inpatient flow and length of stay (LOS) before admission and inpatient bed assignment [1] . Patient LOS prediction has been researched individually using classical machine learning methods, such as linear regression, regression trees, random forest , and neural networks for a long time. Inpatient LOS and flow share many common features in training predictive models because both are closely related to relevant features such as recovery status and surgery types. Besides, these two tasks are closely related. For example, a patient with a more complex inpatient flow tends to have a longer LOS. This paper is the first comprehensive study that links them together as multi-tasks and develops an artificial neural network-based multi-task learning model (ANNML) for mixed types of task prediction in inpatient LOS and flow identification. The constructed multi-task learning model was tested on a real-life dataset collected from a large hospital in New York City and compared with four single-task learning models. The results show that ANNML can use the most relevant features to achieve a better prediction accuracy for both task types and has less overfitting and testing variance than single-task learning models.},
  archive      = {J_ASOC},
  author       = {Lu He and Sreenath Chalil Madathil and Greg Servis and Mohammad T. Khasawneh},
  doi          = {10.1016/j.asoc.2021.107483},
  journal      = {Applied Soft Computing},
  pages        = {107483},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neural network-based multi-task learning for inpatient flow classification and length of stay prediction},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distribution-based imaging for multiple sclerosis lesion
segmentation using specialized fuzzy 2-means powered by nakagami
transmutations. <em>ASOC</em>, <em>108</em>, 107481. (<a
href="https://doi.org/10.1016/j.asoc.2021.107481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distribution-based imaging is a promising methodology mainly to differentiate suspicious regions from surrounding tissues by applying a distribution to the images vertically or horizontally, ideally in both directions. The methodology is very useful for contouring and highlighting desired regions even under near-zero contrast conditions ; it also leads to flexible segmentation of the lesions by parametric kernels and provides robust results when supported by solid post-segmentation protocols. Given these benefits, what we propose in this research is a specialized fuzzy 2-means algorithm enhanced by parametric distribution-based imaging framework to offer novel solutions for multiple-sclerosis (MS) identification and segmentation from flair MRI images. The interchangeable distributions employed in this research are Rayleigh, Weibull, Gamma, Exponential and Chi-square, which all are mathematically transmuted from Nakagami distribution. The Nakagami m-parameter is defining the shape of the distributions unless a special parameter exists; while the highlighted areas are segmented by fuzzy 2-means. All parameters are optimized using a set of MICCAI 2016 MS lesion segmentation challenge taken by Siemens Verio 3T scanner and 0.8245 dice score is achieved by Nakagami-Gamma. However, when the optimized framework is tested by other 4 sets with same resolution and size properties, the highest average dice score 0.7113 is obtained by Nakagami–Rayleigh; while Nakagami-Gamma transmutation is resulted in 0.7112 dice score with significantly better sensitivity.},
  archive      = {J_ASOC},
  author       = {Orcan Alpar and Ondrej Krejcar and Rafael Dolezal},
  doi          = {10.1016/j.asoc.2021.107481},
  journal      = {Applied Soft Computing},
  pages        = {107481},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distribution-based imaging for multiple sclerosis lesion segmentation using specialized fuzzy 2-means powered by nakagami transmutations},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hesitant pythagorean fuzzy ELECTRE-II method for
multi-criteria decision-making problems. <em>ASOC</em>, <em>108</em>,
107479. (<a href="https://doi.org/10.1016/j.asoc.2021.107479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hesitant Pythagorean fuzzy sets have remarkable theoretical and practical features, which derive from their role as a generalization of Pythagorean fuzzy sets that embeds the benefits of hesitation. Their advantages motivate us to extend existing decision-making methods to the case where data and information are in the form of several values. We propose an ELimination and Choice Translating REality-II (ELECTRE-II) technique under hesitant Pythagorean fuzzy (HPF) information to handle diverse opinions of decision experts. The main contribution of this work is the formulation of the basic structure of an HPF ELECTRE-II method, including three kinds of outranking sets (concordance, indifferent, and discordance), two types of outranking matrices (concordance and discordance), two kinds of outranking relations (weak and strong), and two types of outranking graphs (strong and weak graphs). Further, we discuss some quantitative applications to guarantee the applicability and flexibility of the presented framework. To endorse the advantages and accuracy of HPF ELECTRE-II technique, we provide a comprehensive comparative analysis with existing techniques, such as ELECTRE-II approach under hesitant fuzzy data, ELECTRE group decision-making method under fuzzy knowledge, PF ELECTRE-1, and HPF ELECTRE-II methods. Moreover, we state some important insights and discuss the limitations of the model here proposed.},
  archive      = {J_ASOC},
  author       = {Muhammad Akram and Anam Luqman and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2021.107479},
  journal      = {Applied Soft Computing},
  pages        = {107479},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hesitant pythagorean fuzzy ELECTRE-II method for multi-criteria decision-making problems},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scene ancient chinese text recognition with deep
coupled alignments. <em>ASOC</em>, <em>108</em>, 107475. (<a
href="https://doi.org/10.1016/j.asoc.2021.107475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of multi-scene ancient Chinese text recognition (MACR) is challenging due to large-scale categories, high intra-class variance and inter-class similarity and complicated backgrounds. Little effort has been devoted to MACR research due to insufficient datasets and language barrier. Because the sub-dataset generation process of sub-dataset is mutually blind, there are discrepancies in the class category number, deep feature representation and class center distribution after the dataset statistics and character analysis are performed. The general deep learning method that assumes that data are independent and identically distributed is inappropriate. The deep coupled alignments (CA) module based on domain adaptation is presented to alleviate domain and class center shifts. In addition, a cross-domain fusion (CF) module is proposed to mitigate negative transfer in partial domain adaptation by updating the target domain with the full-class and augmenting the source domain with pseudo labeled samples. Extensive experiments of the proposed method are conducted, and the results illustrate the superiority of CA–CF to previous methods in terms of the model size and recognition accuracy.},
  archive      = {J_ASOC},
  author       = {Kaili Wang and Yaohua Yi and Ziwei Tang and Jibing Peng},
  doi          = {10.1016/j.asoc.2021.107475},
  journal      = {Applied Soft Computing},
  pages        = {107475},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scene ancient chinese text recognition with deep coupled alignments},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary neural architecture search for remaining useful
life prediction. <em>ASOC</em>, <em>108</em>, 107474. (<a
href="https://doi.org/10.1016/j.asoc.2021.107474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of Industry 4.0 , making accurate predictions of the remaining useful life (RUL) of industrial components has become a crucial aspect in predictive maintenance (PdM). To this aim, various Deep Neural Network (DNN) models have been proposed in the recent literature. However, while the architectures of these models have a large impact on their performance, they are usually determined empirically. To exclude the time-consuming process and the unnecessary computational cost of manually engineering these models, we present a Neural Architecture Search (NAS) technique based on an Evolutionary Algorithm (EA) applied to optimize the architecture of a DNN used to predict the RUL. The EA explores the combinatorial parameter space of a multi-head Convolutional Neural Network with Long Short Term Memory (CNN-LSTM) to search for the best architecture. In particular, our method requires minimum computational resources by making use of an early stopping policy and a history of the evaluated architectures. We dub the proposed method ENAS-PdM. To our knowledge, this is the first work where an EA-based NAS is used to optimize a CNN-LSTM architecture in the field of PdM. In our experiments, we use the well-established Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dataset from NASA. Compared to the current state-of-the-art, our method obtains better results in terms of two different metrics, RMSE and Score, when aggregating across all the C-MAPSS sub-datasets. Without aggregation, we achieve lower RMSE in 3 out of 4 sub-datasets. Our experimental results verify that the proposed method is a reliable tool for obtaining state-of-the-art RUL predictions and as such it can have a strong impact in several industrial applications, especially those with limited available computing power.},
  archive      = {J_ASOC},
  author       = {Hyunho Mo and Leonardo Lucio Custode and Giovanni Iacca},
  doi          = {10.1016/j.asoc.2021.107474},
  journal      = {Applied Soft Computing},
  pages        = {107474},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary neural architecture search for remaining useful life prediction},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An energy aware grouping memetic algorithm to schedule the
sensing activity in WSNs-based IoT for smart cities. <em>ASOC</em>,
<em>108</em>, 107473. (<a
href="https://doi.org/10.1016/j.asoc.2021.107473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Networks (WSNs) are the main component in the Internet of Things (IoT) and smart cities to sense our environment, gather essential and meaningful data, and forward it to the base station (BS). Nowadays, IoT utilizes WSN as a necessary platform for sensing and communication of the data. One of the main strategies to minimize consumption of energy in a WSN with highly dense sensors is to maximize sleeping sensors at each time. This way implies to schedule the activity of sensors, i.e. determining when a sensor node is kept idle (sleep mode) and when a sensor node is activated to sense environment (active mode). Due to heterogeneity of the sensor nodes in WSNs-based IoT for smart cities, one approach for scheduling the sensing activity is clustering the sensors into K mutually different subsets, so that every subset of sensors alone can cover all targets of the network. In this case, we can solve finding the maximum number of sensor subsets, or equivalently sensor covers problem, by conversion it to SET K-COVER problem. In this paper, an energy aware Grouping Memetic Algorithm (GMA) is proposed for solving the SET K-COVER problem. The proposed GMA varies in four general ways from any of the other evolutionary algorithms for solving the SET K-COVER problem. First, to transform solution structures linked to the SET K-COVER problem into chromosome genes, a new encoding scheme was being used. Second, specific genetic operators appropriate for the chromosomes are used, based on the encoding used. Third, to direct the search process in the solution space of the SET K-COVER problem, a novel proper fitness function is proposed. Fourth, a local improvement algorithm which uses a sensor dominance rule is proposed. This study, carried out detailed experiments of different numbers of targets and different numbers of sensors in WSNs to assess the proposed algorithm. In several instances of the problem, the experiments demonstrate that the algorithm works considerably better in terms of solution quality than many other heuristics and evolutionary algorithms and significantly outperformed the evolutionary algorithms in terms of runtime, suggesting the usefulness of the proposed algorithm to increase the lifetime of WSN in IoT and smart cities.},
  archive      = {J_ASOC},
  author       = {Mohammad Bagher Dowlatshahi and Marjan Kuchaki Rafsanjani and Brij B. Gupta},
  doi          = {10.1016/j.asoc.2021.107473},
  journal      = {Applied Soft Computing},
  pages        = {107473},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An energy aware grouping memetic algorithm to schedule the sensing activity in WSNs-based IoT for smart cities},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An intelligent quantitative trading system based on
intuitionistic-GRU fuzzy neural networks. <em>ASOC</em>, <em>108</em>,
107471. (<a href="https://doi.org/10.1016/j.asoc.2021.107471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantitative trading based on intelligent algorithms has been a hot topic in the financial fields. However, the high noises and outliers increase the uncertainty and non-stationary of financial data such that intelligent algorithms often cease to be effective in the application scenario. Therefore, how to reduce the influence of uncertainty and extract the main trends of financial data is meaningful for trading. Motivated by it, in this article, a novel trading system based on intuitionistic fuzzy neural networks with gated recurrent unit (GRU) is proposed. Firstly, empirical mode decomposition (EMD) is applied to preprocess the original data and obtain the main trends of financial time series on the basis of smoothing data. Secondly, in order to tackle with uncertainty of financial data, a novel interval type-2 intuitionistic fuzzy system (IT2IFS) is proposed for the reasoning process, where hesitancydegree is involved for the learning processes In view of the strong dependence on the time dimension of series data, the gated recurrent unit is integrated into the reasoning model to strengthen the temporal connection. Thirdly, by using the above IT2IFS-GRU model, the quantitative trading system with a concise trading strategy is constructed. By using commodity futures and foreign exchange data, the superior trading results including net profits and precisions, etc. can be obtained, which verifies the effectiveness and availability of the proposed model.},
  archive      = {J_ASOC},
  author       = {Yuan Wang and Chao Luo},
  doi          = {10.1016/j.asoc.2021.107471},
  journal      = {Applied Soft Computing},
  pages        = {107471},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intelligent quantitative trading system based on intuitionistic-GRU fuzzy neural networks},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-operator communication based differential evolution
with sequential tabu search approach for job shop scheduling problems.
<em>ASOC</em>, <em>108</em>, 107470. (<a
href="https://doi.org/10.1016/j.asoc.2021.107470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a multi-operator based differential evolution with a communication strategy (MCDE) being integrated with a sequential Tabu Search (MCDE/TS) to solve the job shop scheduling problem (JSSP) with the objective of minimizing makespan. The three variants of DE which are implemented in the proposed algorithm evolve as independent sub-populations, which relate to a communication strategy that maintains the diversity and quality of each sub-population by employing a proposed mixed selection strategy to avoid premature convergence. The best solution order obtained from MCDE is then passed to Tabu Search (TS) and the evolution process is continued, creating neighbour solutions with N7 neighbourhood structure. This algorithm ensures the population diversity with curving the premature convergence but experiences faster convergence. The design of experiment for parameter tuning is employed for the best combination of the proposed algorithm’s parameter. The performance of the proposed MCDE/TS algorithm is evaluated against a number of state-of-the-art algorithms to show its competence in solving 122 standard benchmark instances.},
  archive      = {J_ASOC},
  author       = {Shahed Mahmud and Alireza Abbasi and Ripon K. Chakrabortty and Michael J. Ryan},
  doi          = {10.1016/j.asoc.2021.107470},
  journal      = {Applied Soft Computing},
  pages        = {107470},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-operator communication based differential evolution with sequential tabu search approach for job shop scheduling problems},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new efficient two-stage method for damage localization and
quantification in shell structures. <em>ASOC</em>, <em>108</em>, 107468.
(<a href="https://doi.org/10.1016/j.asoc.2021.107468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article proposes a new efficient two-stage approach for damage localization and quantification in shell structures using a modal flexibility sensitivity-based damage indicator abbreviated as MFBDI and a recently developed parameter-free optimization algorithm named golden ratio optimization method (GROM). In the first stage, the damage indicator MFBDI is employed to localize possible damage elements in the monitored shell structure. These possible damage elements also help define the search space of optimization problem in the next step. In the second stage, the GROM as a robust optimization solver is implemented to update the finite element (FE) model of the shell structure for refined localization of damage and quantification of its severity. The accuracy and efficiency of the proposed two-stage approach are demonstrated by two numerical simulation examples including a hypar shell and a spherical shell. The simultaneous influences of spatially-incomplete and inaccurate vibration data on damage prediction results are also taken into consideration. The obtained results reveal that the proposed approach can provide an efficient and accurate damage localization and quantification procedure for the studied shell structures .},
  archive      = {J_ASOC},
  author       = {D. Dinh-Cong and T. Nguyen-Thoi},
  doi          = {10.1016/j.asoc.2021.107468},
  journal      = {Applied Soft Computing},
  pages        = {107468},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new efficient two-stage method for damage localization and quantification in shell structures},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcing learning in deep belief networks through
nature-inspired optimization. <em>ASOC</em>, <em>108</em>, 107466. (<a
href="https://doi.org/10.1016/j.asoc.2021.107466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques usually face drawbacks related to the vanishing gradient problem, i.e., the gradient becomes gradually weaker when propagating from one layer to another until it finally vanishes away and no longer helps in the learning process. Works have addressed this problem by introducing residual connections, thus assisting gradient propagation. However, such a subject of study has been poorly considered for Deep Belief Networks . In this paper, we propose a weighted layer-wise information reinforcement approach concerning Deep Belief Networks . Moreover, we also introduce metaheuristic optimization to select proper weight connections that improve the network’s learning capabilities. Experiments conducted over public datasets corroborate the effectiveness of the proposed approach in image classification tasks.},
  archive      = {J_ASOC},
  author       = {Mateus Roder and Leandro Aparecido Passos and Gustavo H. de Rosa and Victor Hugo C. de Albuquerque and João Paulo Papa},
  doi          = {10.1016/j.asoc.2021.107466},
  journal      = {Applied Soft Computing},
  pages        = {107466},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reinforcing learning in deep belief networks through nature-inspired optimization},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Click-event sound detection in automotive industry using
machine/deep learning. <em>ASOC</em>, <em>108</em>, 107465. (<a
href="https://doi.org/10.1016/j.asoc.2021.107465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the automotive industry, despite the robotic systems on the production lines, factories continue employing workers in several custom tasks getting for semi-automatic assembly operations. Specifically, the assembly of electrical harnesses of engines comprises a set of connections between electrical components. Despite the task is easy to perform, employees tend not to notice that a few components are not being connected properly due to physical fatigue provoked by repetitive tasks. This yields a low quality of the assembly production line and possible hazards. In this work, we propose a sound detection system based on machine/deep learning (ML/DL) approaches to identify click sounds produced when electrical harnesses are connected. The purpose of this system is to count the number of connections properly made and to feedback to the employees. We collect and release a public dataset of 25, 000 click sounds of 25 ms length at 22 kHz during three months of assembly operations in an automotive production line located in Mexico. Then, we design an ML/DL-based methodology for click sound detection of assembled harnesses under real conditions of a noisy environment (noise level ranging from − 16 . 67 −16.67 dB to − 12 . 87 −12.87 dB) including other machinery sounds. Our best ML/DL model (i.e., a combination between five acoustic features and an optimized convolutional neural network) is able to detect click sounds in a real assembly production line with an accuracy of 94 . 55 ± 0 . 83 94.55±0.83\%. To the best of our knowledge, this is the first time a click sounds detection system in assembling electrical harnesses of engines for giving feedback to the workers is proposed and implemented in a real-world automotive production line. We consider this work valuable for the automotive industry on how to apply ML/DL approaches for improving the quality of semi-automatic assembly operations.},
  archive      = {J_ASOC},
  author       = {Ricardo Espinosa and Hiram Ponce and Sebastián Gutiérrez},
  doi          = {10.1016/j.asoc.2021.107465},
  journal      = {Applied Soft Computing},
  pages        = {107465},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Click-event sound detection in automotive industry using machine/deep learning},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic and non-invasive parkinson’s disease diagnosis and
severity rating using LSTM network. <em>ASOC</em>, <em>108</em>, 107463.
(<a href="https://doi.org/10.1016/j.asoc.2021.107463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has a huge potential in healthcare for uncovering the hidden patterns from large volume of clinical data to diagnose different diseases. This paper presents a novel deep learning architecture based long short term memory (LSTM) network for severity rating of Parkinson’s disease (PD) using gait pattern . Unlike machine learning (ML) algorithms, the LSTM network avoids the need for hand crafted features and learns the long-term temporal dependencies in the gait cycle for robust diagnosis of PD. The primary advantage of the LSTM network is that it solves the vanishing gradient problem by introducing the memory blocks in place of self-connected hidden units, thereby deciding when to learn new information. Three distinct gait datasets containing vertical ground reaction force (VGRF) recordings for different walking scenarios are used for training the LSTM network. To avoid data overfitting, the proposed approach utilizes dropout and L2 regularization techniques. For solving the cost function, Adam, a stochastic gradient-based optimizer, is employed and the severity of PD is categorized based on unified Parkinson’s disease rating scale (UPDRS) and Hoehn and Yahr (H&amp;Y) scale. The experimental results reveal that Adam optimized LSTM network can effectively learn the gait kinematic features and offer an average accuracy of 98.6\% for binary classification and 96.6\% for multi-class classification, with an accuracy improvement of 3.4\% in comparison with the related techniques.},
  archive      = {J_ASOC},
  author       = {Balaji E. and Brindha D. and Vinodh Kumar Elumalai and Vikrama R.},
  doi          = {10.1016/j.asoc.2021.107463},
  journal      = {Applied Soft Computing},
  pages        = {107463},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic and non-invasive parkinson’s disease diagnosis and severity rating using LSTM network},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effects of memory and genetic operators on artificial bee
colony algorithm for single container loading problem. <em>ASOC</em>,
<em>108</em>, 107462. (<a
href="https://doi.org/10.1016/j.asoc.2021.107462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial Bee Colony (ABC) algorithm is widely used to achieve optimum solution in a short time in integer-based optimization problems . However, the complexity of integer-based problems such as Knapsack Problems (KP) requires robust algorithms to avoid excessive solution search time. ABC algorithm that provides both the exploitation and the exploration approach is used as an alternative approach for various KP problems in the literature. However, it is rarely used for the Single Container Loading problem (SCLP) which is an important part of the transportation systems. In this study, the exploitation and exploration aspects of the ABC algorithm are improved by using memory mechanisms and genetic operators to develop three different hybrid ABC algorithms. The developed algorithms and the basic ABC algorithm are applied to a SCLP dataset from the literature to observe the effects of the memory mechanism and the genetic operators separately. Besides, a joint hybrid ABC algorithm using both reinforcement approaches is proposed to solve the SCLP. The results show that the joint hybrid ABC algorithm has emerged as a promising approach to solving SCLP with an average performance, and the genetic operators are more effective than the memory mechanism to develop a hybrid ABC algorithm.},
  archive      = {J_ASOC},
  author       = {Tuğrul Bayraktar and Filiz Ersöz and Cemalettin Kubat},
  doi          = {10.1016/j.asoc.2021.107462},
  journal      = {Applied Soft Computing},
  pages        = {107462},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effects of memory and genetic operators on artificial bee colony algorithm for single container loading problem},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data selection to avoid overfitting for foreign exchange
intraday trading with machine learning. <em>ASOC</em>, <em>108</em>,
107461. (<a href="https://doi.org/10.1016/j.asoc.2021.107461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic trading requires tuning hyperparameters to fit the time series data ; however, it often suffers from overfitting of data that can lead to loss of money in action. Further, only a few studies discuss how to select trading exchange pairs and frequencies in response to the fitness of machine learning models. To cope with these problems, we developed a log-distance path loss model (to measure and reduce the overfitting in data modeling and determine exchange pairs and frequencies effectively. We conducted several experiments for different metrics using several influential factors such as machine learning models, learning objectives, trading strategies, and hyperparameter turning cases to validate the proposed approach. The obtained results indicate that the proposed metric is significantly superior to other methods in terms of accuracy, in-sample return (i.e., return of training data), and F1-score. Thus, using our path loss metric to guide data modeling , we provide a method to deal with the overfitting problem and yield positive trading returns.},
  archive      = {J_ASOC},
  author       = {Yuan-Long Peng and Wei-Po Lee},
  doi          = {10.1016/j.asoc.2021.107461},
  journal      = {Applied Soft Computing},
  pages        = {107461},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data selection to avoid overfitting for foreign exchange intraday trading with machine learning},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image processing meets time series analysis: Predicting
forex profitable technical pattern positions. <em>ASOC</em>,
<em>108</em>, 107460. (<a
href="https://doi.org/10.1016/j.asoc.2021.107460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using technical price patterns is one of the well-known techniques for predicting future trends in financial markets. Some of these patterns are profitable under certain conditions and some might be non-profitable based on the target market situation and spread. This paper aims to propose a model that works along with the moving average crossover technical pattern. The outputs of the technical price pattern, which are long or short signals, are given as input to the proposed model to predict its profitability. We use a joint model that benefits from two different types of intelligent processing techniques, namely image processing which is applied to candlesticks extracted from price history, and time series analysis which is applied to the numerical features. For the former process, Convolutional Neural Network (CNN) is used and for the latter process, CNN with Long Short-Term Memory (LSTM) is used for the prediction. The proposed model is applied to the data from EUR/USD pairs. The tests were performed for spread values of 0.5, 1, 1.5, and 2. We show that the hybrid model achieves superior results compared to the individual ones, Relative Strength Index (RSI) and Bollinger Bands (BB) technical analysis patterns, as well as two state-of-the-art price prediction models based on CNN-Bidirectional LSTM (BiLSTM) and Phase-State Reconstruction (PSR) with LSTM.},
  archive      = {J_ASOC},
  author       = {Arya Hadizadeh Moghaddam and Saeedeh Momtazi},
  doi          = {10.1016/j.asoc.2021.107460},
  journal      = {Applied Soft Computing},
  pages        = {107460},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image processing meets time series analysis: Predicting forex profitable technical pattern positions},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty-driven active view planning in feature-based
monocular vSLAM. <em>ASOC</em>, <em>108</em>, 107459. (<a
href="https://doi.org/10.1016/j.asoc.2021.107459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional feature-based monocular visual simultaneous localization and mapping (vSLAM) methods suffer from frequent tracking failure in low-texture scenes. Although tracking stability can be guaranteed by actively adjusting the camera’s orientation to track known landmarks, it can easily lead to the problem of over-exploitation. This means that instead of discovering new landmarks, the camera focuses on an area that has already been observed, which is not conducive to fully exploring unknown environments. To address this problem, an uncertainty-driven active view planning framework is proposed to actively adjust the orientation of the monocular camera equipped on a three degree of freedom (3-DoF) pan–tilt. As a result, a trade-off between exploitation, i.e., making full use of known information, and exploration, i.e., obtaining more information of unknown environments can be achieved. First, a novel landmark uncertainty model is established to represent the uncertainty of environmental information. Second, the trade-off problem is formulated as an inequality-constrained optimization mathematical model, whose objective function is related to landmark uncertainty. Last, Karush–Kuhn–Tucker (KKT) conditions are utilized to solve the optimization problem . Experimental results on a publicly available monocular dataset and in a real-world environment show that this framework reduces the rate of tracking failure by 50\% on average. The localization error is also reduced by 0.07 m for translation and 0.004 rad/m for rotation on average. Meanwhile, the number of reconstructed landmarks increases by 17.86\% on average, which indicates an appropriate trade-off between exploitation and exploration.},
  archive      = {J_ASOC},
  author       = {Xu-Yang Dai and Qing-Hao Meng and Sheng Jin},
  doi          = {10.1016/j.asoc.2021.107459},
  journal      = {Applied Soft Computing},
  pages        = {107459},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty-driven active view planning in feature-based monocular vSLAM},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of renewable energy sources in china using an
interval type-2 fuzzy large-scale group risk evaluation method.
<em>ASOC</em>, <em>108</em>, 107458. (<a
href="https://doi.org/10.1016/j.asoc.2021.107458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most effective ways to alleviate energy crisis and environmental pollution, the renewable energy sources (RESs) have received increasing attention. Different RESs enjoy different characteristics and are suitable for different scenarios, thus it is essential to evaluate them before installation. Due to the increasing complexity of reality, the RESs evaluation usually involves various risks and large-scale group decision makers . To manage these risks and decision makers, this paper proposes an interval type-2 fuzzy large-scale group risk evaluation method. First, the interval type-2 fuzzy sets (IT2FSs) are employed to encode the qualitative information provided by the decision makers. Then, a new clustering approach integrating consensus reaching model and risk measurement model is developed to manage the decision makers and enhance the evaluation efficiency. After the clustering process , the selection procedure is activated and an interval type-2 fuzzy centroid-based ranking method is presented to rank the candidate RESs. Finally, a case study in China is provided to illustrate the effectiveness of the proposed method and comparisons are also made to verify the advantages.},
  archive      = {J_ASOC},
  author       = {Xiaohong Pan and Yingming Wang},
  doi          = {10.1016/j.asoc.2021.107458},
  journal      = {Applied Soft Computing},
  pages        = {107458},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of renewable energy sources in china using an interval type-2 fuzzy large-scale group risk evaluation method},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From WASD to BLS with application to pattern classification.
<em>ASOC</em>, <em>108</em>, 107455. (<a
href="https://doi.org/10.1016/j.asoc.2021.107455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single-hidden-layer feedforward neural network (SLFN) exhibits an excellent approximation ability, despite its simple structure. This study examines two neural network models developed based on the general structure of the SLFN, i.e., the weights-and-structure-determination (WASD) neural network and broad learning system (BLS). The BLS and WASD neural networks exhibit many similarities in terms of their structure and algorithm, while BLS incorporates certain distinct ideas that render it an improved version of the WASD neural network. The research described in this paper is the first approach to establish a connection between the BLS and WASD neural networks. Moreover, pattern classification experiments on a foot dataset and several benchmark datasets are conducted using these two kinds of neural network models , and the performance of different models is compared and analyzed. Experimental results show that dividing the feature nodes into one group is the best choice for our foot dataset, and that the cross-layer connectivity and sparse coding of BLS can effectively improve the classification accuracy . In addition, it is shown that the dynamic update algorithm of BLS can improve the accuracy and save a lot of time when updating the model structure, but the final accuracy depends on the initial structure and parameters. Comparative experiments show that BLS achieves 89.12\% classification accuracy for the foot dataset, which is the highest among all the demonstrated WASD neural networks and two other SLFN models. Finally, extended experiments on five benchmark datasets show that BLS can achieve satisfactory accuracy on multiple types of datasets, while WASD neural networks are possibly inadequate in processing image datasets in contrast.},
  archive      = {J_ASOC},
  author       = {Mei Liu and Hongwei Li and Yan Li and Long Jin and Zhiguan Huang},
  doi          = {10.1016/j.asoc.2021.107455},
  journal      = {Applied Soft Computing},
  pages        = {107455},
  shortjournal = {Appl. Soft. Comput.},
  title        = {From WASD to BLS with application to pattern classification},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient nonlinear interval uncertain optimization
method using legendre polynomial chaos expansion. <em>ASOC</em>,
<em>108</em>, 107454. (<a
href="https://doi.org/10.1016/j.asoc.2021.107454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many specialized numerical techniques have been developed on interval uncertainty optimization. However, the conventional nested optimization strategy is time-consuming. The improved Taylor-based interval optimization avoids the laborious inner optimization, nonetheless, it is intrusive and presents significant computational challenges in the large-level uncertain optimization problems . To end these, this paper proposes a Legendre polynomial expansion method combined with the subinterval technique to evaluate the range enclosure of an interval function , where the expansion coefficients are computed through the collocation method. The detailed comparisons demonstrate that such Legendre-based polynomial expansion provides much more confidence in interval bounds evaluation than the Taylor expansion , and is capable of maintaining very sharp solution enclosures even when the uncertainty level is increased. More exciting, it is highly efficient. Subsequent application of the Legendre polynomial expansion in the interval bound estimation of the uncertain objective function and constraints, and combining an appropriate outer optimizer, result in a novel nonlinear interval uncertain optimization method for complicated engineering systems, whereby the time-consuming optimization nesting is avoided. A beam design example and an electromagnetic buffer optimization example illustrate that the proposed interval optimization method has fine computational precision and high computational efficiency, especially is capable of attacking the increase of uncertainty level. Moreover, the new method does not require the derivative information of the uncertain objective and constraints, such that applies to any engineering uncertain optimization problems , including the complex engineering problems which are usually referred to as “black-box”.},
  archive      = {J_ASOC},
  author       = {Liqun Wang and Guolai Yang and Zixuan Li and Fengjie Xu},
  doi          = {10.1016/j.asoc.2021.107454},
  journal      = {Applied Soft Computing},
  pages        = {107454},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient nonlinear interval uncertain optimization method using legendre polynomial chaos expansion},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A predictive intelligence approach to classify
brain–computer interface based eye state for smart living.
<em>ASOC</em>, <em>108</em>, 107453. (<a
href="https://doi.org/10.1016/j.asoc.2021.107453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, brain–computer interface (BCI) based systems have become an emerging technology facilitating smart living. Accurate identification of eye states (open or closed) via an EEG-based BCI interface has many applications in a smart living environment, such as controlling devices and monitoring health status. Artificial neural networks (ANNs), including deep neural networks , are currently quite popular in many applications. In this study, a robust and unique ANN-based ensemble method is developed in which multiple ANNs are trained individually using different parts of the training data. The outcomes of each ANN are then combined using another ANN to enhance the predictive intelligence. The outcome of this ANN is considered the ultimate prediction of the user’s eye state. The proposed ensemble method requires minimal training time and yields highly accurate eye state classification. An extensive analysis of bias and variance was used to assess the generalization ability of the proposed model while applying it to a real BCI environment and dataset. The proposed model outperforms traditional ANNs and other machine learning tools for eye state classification.},
  archive      = {J_ASOC},
  author       = {Mohammad Mehedi Hassan and Md Rafiul Hassan and Shamsul Huda and Md. Zia Uddin and Abdu Gumaei and Ahmed Alsanad},
  doi          = {10.1016/j.asoc.2021.107453},
  journal      = {Applied Soft Computing},
  pages        = {107453},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A predictive intelligence approach to classify brain–computer interface based eye state for smart living},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovery of arbitrarily shaped significant clusters in
spatial point data with noise. <em>ASOC</em>, <em>108</em>, 107452. (<a
href="https://doi.org/10.1016/j.asoc.2021.107452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial point data is an important data source that represents the locations of spatial events (such as crime, disease cases, and earthquakes). Detecting clusters I n spatial point data plays a key role in exploratory spatial data analysis . Although much attention has been paid to the clustering of spatial points, how to automatically and efficiently discover the statistically significant clusters with irregular shapes is still a challenging work. On that account, an automatic method to detect the statistically significant high-density clusters in spatial point data with noise is proposed in this paper. First, the Voronoi diagram of the spatial points is constructed, and the densities of spatial points are defined by the areas of the Voronoi cells. Then, high-density points are automatically detected using spatial hotspot statistics analysis, and a density-based clustering strategy is further adapted to combine the neighboring high-density points into candidate clusters. Finally, a statistical significance test is proposed to evaluate the significance of the candidate clusters under the spatially homogeneous or heterogeneous distribution assumption. We tested the proposed method with the simulated data sets and the real-world taxi trajectory data for detecting the pick-up hotspot regions in Wuhan, China. Results show that the proposed method can successfully find arbitrarily shaped significant clusters that existing state-of-the-art clustering algorithms may fail to find in spatial point data with noise.},
  archive      = {J_ASOC},
  author       = {Jincai Huang and Jianbo Tang},
  doi          = {10.1016/j.asoc.2021.107452},
  journal      = {Applied Soft Computing},
  pages        = {107452},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discovery of arbitrarily shaped significant clusters in spatial point data with noise},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parameter identification of DC arc models using chaotic
quantum cuckoo search. <em>ASOC</em>, <em>108</em>, 107451. (<a
href="https://doi.org/10.1016/j.asoc.2021.107451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DC arc fault is difficult to be detected, which brings great challenge to the safety of DC distribution system. The investigation of arc fault detection based on simulation method is able to reduce the cost and contributes to the research in the system where it is not suitable to carry out experiments. Accurate arc model is the foundation of simulation research. To address the gaps existing in arc models and corresponding parameter identification methods, two novel arc models and a new optimization algorithm are presented in this paper. First of all, hook static model was proposed to describe the nonlinearity near the transition point in static curve of arc. Then, segmented noise model was developed to flexibly fit the diversiform shape of arc noise’s frequency spectrum (FS). Finally, in order to improve the accuracy of parameter identification of arc models, this paper introduced chaos mechanism into quantum cuckoo search (CQCS): using chaotic map to initialize the population and generate the random parameter p ; using lifespan based chaotic local search to further update the position of best solution. Based on the data obtained from established experimental platform, the effectiveness of the proposed models and optimization algorithm is verified. Meanwhile, compared with the existing methodologies, the proposed models and optimization algorithm are of better performance.},
  archive      = {J_ASOC},
  author       = {Zhendong Yin and Li Wang and Yaojia Zhang and Yang Gao},
  doi          = {10.1016/j.asoc.2021.107451},
  journal      = {Applied Soft Computing},
  pages        = {107451},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter identification of DC arc models using chaotic quantum cuckoo search},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Walk-forward empirical wavelet random vector functional link
for time series forecasting. <em>ASOC</em>, <em>108</em>, 107450. (<a
href="https://doi.org/10.1016/j.asoc.2021.107450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of accurately forecasting a time series covers numerous disciplines, from economics to engineering. Among the thousands of machine learning models, random vector functional link (RVFL) is a robust and efficient model which has demonstrated its success in various challenging forecasting problems. RVFL is an efficient universal function appropriator that randomly generates the weights between the input and hidden layers. However, RVFL still lacks the strong ability to extract meaningful multi-scale features from input data because of the single-layer random mapping of enhancement nodes. Therefore, we propose to combine the empirical wavelet transformation (EWT) with RVFL to strengthen the multi-scale feature extraction ability. The EWT can decompose the original time series into several sub-series which carry the information of different frequencies. Besides, we propose a walk-forward decomposition mechanism to implement the EWT. By introducing such a walk-forward mechanism and the combination of EWT and RVFL, the hybrid model achieves high accuracy and averts the data leakage problem during forecasting. A detailed and comprehensive empirical study on twenty-six public time series validates the proposed model’s superiority compared with ten popular baseline models from the literature.},
  archive      = {J_ASOC},
  author       = {Ruobin Gao and Liang Du and Kum Fai Yuen and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2021.107450},
  journal      = {Applied Soft Computing},
  pages        = {107450},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Walk-forward empirical wavelet random vector functional link for time series forecasting},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HSSAGA: Designation and scheduling of nurses for taking care
of COVID-19 patients using novel method of hybrid salp swarm algorithm
and genetic algorithm. <em>ASOC</em>, <em>108</em>, 107449. (<a
href="https://doi.org/10.1016/j.asoc.2021.107449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic is viewed as the most basic worldwide disaster that humankind has observed since the second World War. There is no report of any clinically endorsed antiviral medications or antibodies that are successful against COVID-19. It has quickly spread everywhere, presenting tremendous well-being, financial, ecological, and social difficulties to the whole human populace. The COVID flare-up is seriously disturbing the worldwide economy. Practically all the countries are battling to hinder the transmission of the malady by testing and treating patients, isolating speculated people through contact following, confining huge social affairs, keeping up total or incomplete lockdown, etc. Proper scheduling of nursing workers and optimal designation of nurses may significantly affect the quality of clinical facilities. It is delivered by eliminating unbalanced workloads or undue stress, which could lead to decreased nurse performance and potential human errors., Nurses are frequently asked to leave while caring for all sick patients. However, regular scheduling formulas are not thought to consider this possibility because they are out of scheduling control in typical scenarios. In this paper, a novel model of the Hybrid Salp Swarm Algorithm and Genetic Algorithm (HSSAGA) is proposed to solve nurses’ scheduling and designation. The findings of the suggested test function algorithm demonstrate that this algorithm has outperformed state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Moein Qaisari Hasan Abadi and Sara Rahmati and Abbas Sharifi and Mohsen Ahmadi},
  doi          = {10.1016/j.asoc.2021.107449},
  journal      = {Applied Soft Computing},
  pages        = {107449},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HSSAGA: Designation and scheduling of nurses for taking care of COVID-19 patients using novel method of hybrid salp swarm algorithm and genetic algorithm},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple global optima location using differential
evolution, clustering, and local search. <em>ASOC</em>, <em>108</em>,
107448. (<a href="https://doi.org/10.1016/j.asoc.2021.107448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal optimization can be divided into two main categories. The first focuses on finding only one global optimum, e.g., one peak, and the second focuses on finding multiple global optima, e.g., multiple peaks, and it is the focus of this work. The first category can be approached by single-solution and population-based metaheuristics , while in the second category, the use of population-based metaheuristics is best suited. Finding multiple global optima is more common in real-world optimization problems where there is no previous information about the number of globally optimal solutions in the search space landscape. Thus, this work proposes a self-adaptive Differential Evolution with DBSCAN algorithm and a two-step exploitation routine, named NCjDE-2LS a r ar , applied to multiple global optima multimodal optimization. The jDE algorithm with Michalewicz mutation strategy is employed as a global optimizer. Candidate solutions are grouped in an external archive using the DBSCAN algorithm. External archive solutions represent possible peaks that will feed the Nelder–Mead and Hooke–Jeeves exploitation algorithms. The peak ratio is used as a performance metric. Six state-of-the-art algorithms are used to compare the results obtained by the proposed approach. Results show that the proposed algorithm outperforms the state-of-the-art algorithms concerning the mean absolute value of the peak ratio evaluation metric . The algorithm also achieved the perfect peak ratio (100\%) in 10 out of 20 functions.},
  archive      = {J_ASOC},
  author       = {Gabriel Dominico and Rafael Stubs Parpinelli},
  doi          = {10.1016/j.asoc.2021.107448},
  journal      = {Applied Soft Computing},
  pages        = {107448},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple global optima location using differential evolution, clustering, and local search},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Experimental evaluation of ensemble classifiers for
imbalance in big data. <em>ASOC</em>, <em>108</em>, 107447. (<a
href="https://doi.org/10.1016/j.asoc.2021.107447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets are growing in size and complexity at a pace never seen before, forming ever larger datasets known as Big Data. A common problem for classification, especially in Big Data, is that the numerous examples of the different classes might not be balanced. Some decades ago, imbalanced classification was therefore introduced, to correct the tendency of classifiers that show bias in favor of the majority class and that ignore the minority one. To date, although the number of imbalanced classification methods have increased, they continue to focus on normal-sized datasets and not on the new reality of Big Data. In this paper, in-depth experimentation with ensemble classifiers is conducted in the context of imbalanced Big Data classification, using two popular ensemble families (Bagging and Boosting) and different resampling methods. All the experimentation was launched in Spark clusters, comparing ensemble performance and execution times with statistical test results, including the newest ones based on the Bayesian approach. One very interesting conclusion from the study was that simpler methods applied to unbalanced datasets in the context of Big Data provided better results than complex methods. The additional complexity of some of the sophisticated methods, which appear necessary to process and to reduce imbalance in normal-sized datasets were not effective for imbalanced Big Data.},
  archive      = {J_ASOC},
  author       = {Mario Juez-Gil and Álvar Arnaiz-González and Juan J. Rodríguez and César García-Osorio},
  doi          = {10.1016/j.asoc.2021.107447},
  journal      = {Applied Soft Computing},
  pages        = {107447},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Experimental evaluation of ensemble classifiers for imbalance in big data},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep convolutional neural network for musical genre
classification via new self adaptive sea lion optimization.
<em>ASOC</em>, <em>108</em>, 107446. (<a
href="https://doi.org/10.1016/j.asoc.2021.107446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Music Genre Classification (MGC) is said to be a basic element for retrieving the music information. In fact, music genre labels are very useful to organize albums, songs, and artists in border groups that share similar characteristics. Henceforth, a precise and effective MGC system is required to enhance the retrieved music genres. This paper tactics to propose a new music genre classification model that includes two major processes: Feature extraction and Classification. In the feature extraction phase, features like “non-negative matrix factorization (NMF) features, Short-Time Fourier Transform (STFT) features and pitch features” are extracted. The extracted features are then subjected to a classification process via Deep Convolutional Neural Network (DCNN) model. In order to improve the classification accuracy , the DCNN model is trained using a new Self Adaptive SA-SLnO (SA-SLnO) model through optimizing the weight. Finally, the performance of adopted work is evaluated over other existing approaches with respect to error analysis and statistical measures, respectively.},
  archive      = {J_ASOC},
  author       = {Balachandra Kumaraswamy and P.G. Poonacha},
  doi          = {10.1016/j.asoc.2021.107446},
  journal      = {Applied Soft Computing},
  pages        = {107446},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep convolutional neural network for musical genre classification via new self adaptive sea lion optimization},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid binary grey wolf optimizer for selection and
reduction of reference points with extreme learning machine approach on
local GNSS/leveling geoid determination. <em>ASOC</em>, <em>108</em>,
107444. (<a href="https://doi.org/10.1016/j.asoc.2021.107444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and optimization from natural phenomena and observations of the physical earth is an extremely important issue. In the light of the developments in computer and artificial intelligence technologies, the applications of learning-based modeling and optimization techniques in all kinds of study fields are increasing. In this research, the applicability of four different state-of-the-art metaheuristic algorithms which are Particle swarm optimization (PSO), Tree-Seed Algorithm (TSA), Artificial Bee Colony (ABC) algorithm, and Grey Wolf Optimizer (GWO), in local GNSS/leveling geoid studies have been examined. The most suitable geoid model has been tried to be obtained by using different reference points via the well-known machine learning algorithms, Artificial Neural Network (ANN) and Extreme Learning Machine (ELM), at the existing GNSS/leveling points in Burdur city of Turkey. In this study, eight different hybrid approaches are proposed by using each metaheuristic algorithm together with machine learning methods. By using these hybrid approaches, the model closest to the minimum number of reference points has been tried to be obtained. Furthermore, the performance of the hybrid approaches has been compared. According to the comparisons, the hybrid approach performed with GWO and ELM has achieved better results than other proposed hybrid approaches. As a result of the research, it has been seen that the most suitable local GNSS/Leveling geoid can be determined with a lower number of reference points in an appropriate distribution.},
  archive      = {J_ASOC},
  author       = {Kemal Tütüncü and Mehmet Akif Şahman and Ekrem Tuşat},
  doi          = {10.1016/j.asoc.2021.107444},
  journal      = {Applied Soft Computing},
  pages        = {107444},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid binary grey wolf optimizer for selection and reduction of reference points with extreme learning machine approach on local GNSS/leveling geoid determination},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised anomaly detection with LSTM autoencoders using
statistical data-filtering. <em>ASOC</em>, <em>108</em>, 107443. (<a
href="https://doi.org/10.1016/j.asoc.2021.107443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address one of the most challenging industry problems, we develop an enhanced training algorithm for anomaly detection in unlabelled sequential data such as time-series. We propose the outputs of a well-designed system are drawn from an unknown probability distribution, U U , in normal conditions. We introduce a probability criterion based on the classical central limit theorem that allows evaluation of the likelihood that a data-point is drawn from U U . This enables the labelling of the data on the fly. Non-anomalous data is passed to train a deep Long Short-Term Memory (LSTM) autoencoder that distinguishes anomalies when the reconstruction error exceeds a threshold. To illustrate our algorithm’s efficacy, we consider two real industrial case studies where gradually-developing and abrupt anomalies occur. Moreover, we compare our algorithm’s performance with four of the recent and widely used algorithms in the domain. We show that our algorithm achieves considerably better results in that it timely detects anomalies while others either miss or lag in doing so.},
  archive      = {J_ASOC},
  author       = {Sepehr Maleki and Sasan Maleki and Nicholas R. Jennings},
  doi          = {10.1016/j.asoc.2021.107443},
  journal      = {Applied Soft Computing},
  pages        = {107443},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised anomaly detection with LSTM autoencoders using statistical data-filtering},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constraint-handling techniques within differential evolution
for solving process engineering problems. <em>ASOC</em>, <em>108</em>,
107442. (<a href="https://doi.org/10.1016/j.asoc.2021.107442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide range of process systems engineering problems involve an optimisation formulation that is difficult to solve due to sources of discontinuity and non-convexity and a high number of constraints to satisfy. Differential Evolution algorithm (DE) has proven to be robust for the solution of highly non-convex and mixed-integer problems; nevertheless, its performance greatly depends on the constraint-handling technique used. In this study, numerical comparisons of some state-of-the-art constraint-handling techniques are performed: static penalty function, stochastic ranking, feasibility rules, ε ε constrained method and gradient-based repair. The obtained results show that the gradient-based repair technique deserves a special attention when solving highly constrained problems. This technique enables to efficiently satisfy both inequality and equality constraints, which makes it particularly adapted for the solution of process engineering optimization problems .},
  archive      = {J_ASOC},
  author       = {Victor H. Cantú and Catherine Azzaro-Pantel and Antonin Ponsich},
  doi          = {10.1016/j.asoc.2021.107442},
  journal      = {Applied Soft Computing},
  pages        = {107442},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constraint-handling techniques within differential evolution for solving process engineering problems},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Learning large-scale fuzzy cognitive maps using an
evolutionary many-task algorithm. <em>ASOC</em>, <em>108</em>, 107441.
(<a href="https://doi.org/10.1016/j.asoc.2021.107441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy cognitive maps (FCMs) are a powerful tool for simulating and analyzing complex systems. Many efficient methods based on evolutionary algorithms have been proposed to learn small-scale FCMs. However, large number of function evaluations of those methods make them difficult to cope with large-scale FCM learning problems. To overcome this issue, we propose a random inactivation-based batch many-task evolutionary algorithm, termed as IBMTEA-FCM. Inspired by the probability of knowledge sharing in different tasks, the problem of FCM learning is first modeled as a many-task optimization problem , in which each task represents learning local connections of a node in a single FCM. To ensure the effectiveness of knowledge transfer, all tasks are randomly divided into multiple batches to optimize separately. In this method, an evolutionary many-task framework is employed to overcome the proposed many-task FCM learning problem and we randomly deactivate weighted edges to ensure the sparsity of FCM in the evolutionary process. The performance of IBMTEA-FCM is validated on both synthetic datasets and a practical study of gene regulatory network reconstruction. Compared with existing classical methods, the experimental results show that IBMTEA-FCM can learn large-scale FCMs with higher accuracy and less computational cost.},
  archive      = {J_ASOC},
  author       = {Chao Wang and Jing Liu and Kai Wu and Chaolong Ying},
  doi          = {10.1016/j.asoc.2021.107441},
  journal      = {Applied Soft Computing},
  pages        = {107441},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning large-scale fuzzy cognitive maps using an evolutionary many-task algorithm},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topic-level sentiment analysis of social media data using
deep learning. <em>ASOC</em>, <em>108</em>, 107440. (<a
href="https://doi.org/10.1016/j.asoc.2021.107440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inception of Web 2.0 and freedom to facilitate the dissemination of information, sharing views, expressing opinions with regards to current world level events, services, products, etc. social media platforms have been mainly contributing to user-generated content. Such social media data consist of various themes discussed online and are associated with sentiments of the users. To catch up with the speed of streaming data at which it generates on social media platforms , it is crucial to detect the topics being discussed on social media platforms and analyze the sentiments of users towards those topics in an online manner to make timely decisions. Motivated by the same, this paper proposes a deep learning based topic-level sentiment analysis model. The novelty of the proposed approach is that it works at the sentence level to extract the topic using online latent semantic indexing with regularization constraint and then applies topic-level attention mechanism in long short-term memory network to perform sentiment analysis . The proposed model is unique in the sense that it supports scalable and dynamic topic modeling over streaming short text data and performs sentiment analysis at topic-level. For SemEval-2017 Task 4 Subtask B dataset as a case of in-domain topic-level sentiment analysis, average recall of 0.879 has been achieved, whereas, for out-of-domain data, average recall of 0.846, 0.824 and 0.794 has been achieved for newly developed datasets collected under the hashtags #ethereum, #bitcoin and #facebook from Twitter. To assess the performance of the model for scalability, we analyzed the model in terms of average time in milliseconds for creation of feature vectors, throughput in terms of topics detected per second and average response time in seconds to handle the sentiment analysis queries. The experimental results are significant enough to enable large scale topic modeling over streaming data and perform topic-level sentiment analysis.},
  archive      = {J_ASOC},
  author       = {Ajeet Ram Pathak and Manjusha Pandey and Siddharth Rautaray},
  doi          = {10.1016/j.asoc.2021.107440},
  journal      = {Applied Soft Computing},
  pages        = {107440},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Topic-level sentiment analysis of social media data using deep learning},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privileged information-driven random network based
non-iterative integration model for building energy consumption
prediction. <em>ASOC</em>, <em>108</em>, 107438. (<a
href="https://doi.org/10.1016/j.asoc.2021.107438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate building energy consumption (BEC) prediction plays an increasingly significant role in energy control and conservation. However, owing to the high level of randomness of BEC data , acquiring accurate prediction results is difficult. To further enhance the predicted precision and robustness, we herein present a non-iterative decomposition–integration model to realise BEC prediction. Our proposed method merges the ensemble empirical mode decomposition (EEMD), learning using a privileged information (LUPI) paradigm-based random vector functional link network (RVFL+), and support vector regression (SVR) to generate satisfactory results. In this model, EEMD is first adopted for the decomposition of historical energy consumption data. Subsequently, the decomposed subsignals are input into the RVFL+ network, and the features of high correlation with BEC are selected as privileged information to constrain the weight of the output layer of RVFL+ network in order to obtain their corresponding output RVFL+ models of each subsignal. Finally, the resulting RVFL+ output models are aggregated and input into the SVR model to acquire the prediction results, thereby breaking through the limitation of the single-prediction model and improving prediction accuracy. To verify our proposed model, five actual BEC datasets were employed for experiments: Jinan in China, Fairbanks and California in the USA, Vancouver in Canada, and Sydney in Australia. Experiment results indicated that our proposed EEMD-RVFL+-SVR method had better accuracy and anti-noise performance. Specially, the mean absolute percentage error of the proposed method, compared with the EMD-SVR, EEMD-SVR, EEMD-PSO-GA-SVR, EEMD-RVFL, ARIMA-SVM, FLS-SVM, EMD-RVFL+-SVR, CEEMD-RVFL+-SVR, SVR, RVFL, RVFL+, BPNN, and Wavelet neural network , was reduced by 65.07\%, 37.20\%, 40.61\%, 52.27\%, 44.66\%, 34.76\%, 50.01\%, 39.06\%, 43.04\%, 53.77\%, 46.38\%, 41.82\% and 56.14\% respectively.},
  archive      = {J_ASOC},
  author       = {Hongchang Sun and Wenwen Zhai and Yugang Wang and Lei Yin and Fengyu Zhou},
  doi          = {10.1016/j.asoc.2021.107438},
  journal      = {Applied Soft Computing},
  pages        = {107438},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Privileged information-driven random network based non-iterative integration model for building energy consumption prediction},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-start iterated local search, exact and matheuristic
approaches for minimum capacitated dominating set problem.
<em>ASOC</em>, <em>108</em>, 107437. (<a
href="https://doi.org/10.1016/j.asoc.2021.107437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a multi-start iterated local search (MS-ILS), an exact approach based on an improved integer linear programming (ILP) model, and a matheuristic combining the ILP approach with MS-ILS through solution merging for the minimum capacitated dominating set (CAPMDS) problem for undirected graphs . This problem is an extension of the well-known minimum dominating set problem, where there is a cap on the number of nodes that each node can dominate, and the goal is to find a dominating set of minimum cardinality where the number of dominated nodes for each dominating node is within this cap. This NP NP -hard problem has several real world applications in resource constrained environments such as clustering of wireless sensor networks along with selection of cluster heads , document summarization in information retrieval. Computational results on the standard benchmark instances of this problem show the superiority of our approaches over state-of-the-art approaches available in the literature in their respective categories.},
  archive      = {J_ASOC},
  author       = {Mallikarjun Rao Nakkala and Alok Singh and André Rossi},
  doi          = {10.1016/j.asoc.2021.107437},
  journal      = {Applied Soft Computing},
  pages        = {107437},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-start iterated local search, exact and matheuristic approaches for minimum capacitated dominating set problem},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safety risk assessment of metro construction under epistemic
uncertainty: An integrated framework using credal networks and the EDAS
method. <em>ASOC</em>, <em>108</em>, 107436. (<a
href="https://doi.org/10.1016/j.asoc.2021.107436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safety risk assessment of metro construction is necessary to prevent catastrophic accidents that may cause heavy financial losses and casualties. In this paper, we introduce a comprehensive risk assessment framework that incorporates credal networks (CNs) and an improved evaluation based on distance from average solution (EDAS) method. First, the CN model can intuitively demonstrate dependencies between risk factors, capture the epistemic uncertainty of expert judgement in the form of imprecise probability, and propagate the uncertainty quickly and accurately with advanced inference algorithms. In addition, the improved EDAS method can be used to comprehensively identify critical risk factors with multiple indicators, including the nature of the risk itself and company capabilities, being taken into account. Moreover, to effectively aggregate expert opinions, a weighting method that integrates subjective weights and similarity-based objective weights is provided. A case study on the safety risk analysis of metro construction in China verified the feasibility of the framework. The proposed approach can be extended to other similar projects to help engineers systematically assess risks during the life cycle of the project to ensure the safety of metro construction.},
  archive      = {J_ASOC},
  author       = {Wen-hui Hou and Xiao-kang Wang and Hong-yu Zhang and Jian-qiang Wang and Lin Li},
  doi          = {10.1016/j.asoc.2021.107436},
  journal      = {Applied Soft Computing},
  pages        = {107436},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Safety risk assessment of metro construction under epistemic uncertainty: An integrated framework using credal networks and the EDAS method},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing instance-level constrained clustering through
differential evolution. <em>ASOC</em>, <em>108</em>, 107435. (<a
href="https://doi.org/10.1016/j.asoc.2021.107435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering has always been a powerful tool in knowledge discovery. Traditionally unsupervised, it received renewed attention when it was shown to produce better results when provided with new types of information, thus leading to a new kind of semi-supervised learning: constrained clustering. This technique is a generalization of traditional clustering that considers additional information encoded by constraints. Constraints can be given in the form of instance-level must-link and cannot-link constraints, which this paper focuses on. We propose the first application of Differential Evolution to the constrained clustering problem , which has proven to produce a better exploration–exploitation trade-off when comparing with previous approaches. We will compare the results obtained by this proposal to those obtained by previous nature-inspired techniques and by some of the state-of-the-art algorithms on 25 datasets with incremental levels of constraint-based information, supporting our conclusions with the aid of Bayesian statistical tests.},
  archive      = {J_ASOC},
  author       = {Germán González-Almagro and Julián Luengo and José-Ramón Cano and Salvador García},
  doi          = {10.1016/j.asoc.2021.107435},
  journal      = {Applied Soft Computing},
  pages        = {107435},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing instance-level constrained clustering through differential evolution},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A chaos recurrent ANFIS optimized by PSO to predict ground
vibration generated in rock blasting. <em>ASOC</em>, <em>108</em>,
107434. (<a href="https://doi.org/10.1016/j.asoc.2021.107434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of explosives is a common and economical method to fragment and/or displace hard rocks in tunnels and surface and underground mines. Ground vibration , as a side environmental effect induced by blast events, has detrimental impacts on nearby structures like dams and buildings. Therefore, an accurate and reliable estimation of ground vibration is imperative. The goal of this paper is to present a new hybrid model by combining chaos recurrent adaptive neuro-fuzzy inference system (CRANFIS) and particle swarm optimization (PSO) to predict ground vibration. To the best of our knowledge, this is the first research that predicts the ground vibration through a model integrating CRANFIS and PSO. To evaluate the efficiency of the proposed model, the results of CRANFIS-PSO were compared with those of the CRANFIS, RANFIS, ANFIS, artificial neural network (ANN), and several empirical methods. In other words, first, the empirical methods were developed; then, due to their unacceptable performance, the artificial intelligence methods were developed. The results clearly indicated the superiority of CRANFIS-PSO over the above-mentioned methods in terms of predicting ground vibration. The values of coefficient of determination ( R 2 R2 ) obtained from CRANFIS-PSO, CRANFIS, RANFIS, ANFIS, and ANN models were 0.997, 0.967, 0.958, 0.822, and 0.775, respectively. Accordingly, the CRANFIS-PSO model could be employed as a reliable and accurate data intelligent model to solve highly-nonlinear problems such as the prediction of blast-induced flyrock and air-overpressure.},
  archive      = {J_ASOC},
  author       = {Wei Zhu and Hima Nikafshan Rad and Mahdi Hasanipanah},
  doi          = {10.1016/j.asoc.2021.107434},
  journal      = {Applied Soft Computing},
  pages        = {107434},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A chaos recurrent ANFIS optimized by PSO to predict ground vibration generated in rock blasting},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video salient object detection using dual-stream
spatiotemporal attention. <em>ASOC</em>, <em>108</em>, 107433. (<a
href="https://doi.org/10.1016/j.asoc.2021.107433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video salient object detection plays an important role in many exciting applications in different areas. However, the existing deep learning-based video salient object detection methods still struggle in scenes of large salient object variabilities and great background scene diversity between and within frames. In this paper, we propose a dual-stream spatiotemporal attention network (DSSANet) for saliency detection in videos. It creatively introduces a multiplex attention mechanism to effectively extract and fuse spatiotemporal features of video salient object over frames in the video, thereby improving saliency detection performance. The DSSANet consists of: (1) A context feature path leverages a novel attention-augmented convolutional LSTM to effectively model the long-range dependency of the great temporal variation in the salient object over frames. (2) A content feature path creatively leverages an attention-based 1D dilated convolution to effectively model the local pixel correlation structure of each pixel in the salient object and the surrounding objects. (3) A refinement fusion module fuses these two features from their paths and further refines the fused feature by an attention-based feature selection. By integrating these three parts, DSSANet accurately detects the salient object from the video. The extensive experiments are performed on four public datasets and demonstrate the effectiveness of DSSANet and the superiority to five state-of-the-art video salient object detection methods.},
  archive      = {J_ASOC},
  author       = {Chenchu Xu and Zhifan Gao and Heye Zhang and Shuo Li and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.asoc.2021.107433},
  journal      = {Applied Soft Computing},
  pages        = {107433},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Video salient object detection using dual-stream spatiotemporal attention},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An joint end-to-end framework for learning with noisy
labels. <em>ASOC</em>, <em>108</em>, 107426. (<a
href="https://doi.org/10.1016/j.asoc.2021.107426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved excellent performance in image classification research, part of which is due to the large-scale training data with accurate annotations. However, it is expensive and time-consuming to collect such clean data. In contrast, when collecting a dataset by crawling from websites, noisy labels are ubiquitous, which makes it easy for deep neural networks (DNNs) to overfit noisy labels and cause performance degradation . Most recent efforts have been focused on defending noisy labels by roughly ignoring some samples with high losses, which are treated as noise or reweight the training data in the loss function. Both strategies inevitably have priori conditions, such as a clean validation set or a ground-truth noise transition matrix , which are impractical in real-world datasets. In this paper, we propose a novel end-to-end framework for noise correction, called End-to-end Correction with Mixup and Balance terms (ECMB). ECMB can completely correct noisy labels to true labels and keep the number of each class more balanced. This framework uses a backbone network that is pre-trained by using an improved Mixup entropy instead of the traditional cross entropy, and does not need any extra conditions. In addition, we introduce a new balance term that can update noisy labels more accurately. Compared with other state-of-the-art methods, the experimental results on publicly available CIFAR-10, CIFAR-100 and Clothing1M datasets demonstrate that our method has superior performance.},
  archive      = {J_ASOC},
  author       = {Qian Zhang and Feifei Lee and Ya-gang Wang and Damin Ding and Wei Yao and Lu Chen and Qiu Chen},
  doi          = {10.1016/j.asoc.2021.107426},
  journal      = {Applied Soft Computing},
  pages        = {107426},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An joint end-to-end framework for learning with noisy labels},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evolutionary many-objective approach to multiview
clustering using feature and relational data. <em>ASOC</em>,
<em>108</em>, 107425. (<a
href="https://doi.org/10.1016/j.asoc.2021.107425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many application domains involve the consideration of multiple data sources. Typically, each of these data views provides a different perspective of a given set of entities. Inspired by early work on multiview (supervised) learning, multiview algorithms for data clustering offer the opportunity to consider and integrate all this information in an unsupervised setting. In practice, some complex real-world problems may give rise to a handful or more data views, each with different reliability levels. However, existing algorithms are often limited to the consideration of two views only, or they assume that all the views have the same level of importance. Here, we describe the design of an evolutionary algorithm for the problem of multiview cluster analysis, exploiting recent advances in the field of evolutionary optimization to address settings with a larger number of views. The method is capable of considering views that are represented in the form of distinct feature sets, or distinct dissimilarity matrices , or a combination of the two. Our experimental results on standard (including real-world) benchmark datasets confirm that the adoption of a many-objective evolutionary algorithm addresses limitations of previous work, and can easily scale to settings with four or more data views. The final highlight of our paper is an illustration of the potential of the approach in an application to breast lesion classification.},
  archive      = {J_ASOC},
  author       = {Adán José-García and Julia Handl and Wilfrido Gómez-Flores and Mario Garza-Fabre},
  doi          = {10.1016/j.asoc.2021.107425},
  journal      = {Applied Soft Computing},
  pages        = {107425},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary many-objective approach to multiview clustering using feature and relational data},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated multi-node hadoop framework to predict
high-risk factors of diabetes mellitus using a multilevel MapReduce
based fuzzy classifier (MMR-FC) and modified DBSCAN algorithm.
<em>ASOC</em>, <em>108</em>, 107423. (<a
href="https://doi.org/10.1016/j.asoc.2021.107423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people’s food habits, physical activity are extracted from social media using the API’s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55\% of people are in a high-risk group with positive sentiments on the data extracted. More than 70\% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70\% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55\% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.},
  archive      = {J_ASOC},
  author       = {J. Ramsingh and V. Bhuvaneswari},
  doi          = {10.1016/j.asoc.2021.107423},
  journal      = {Applied Soft Computing},
  pages        = {107423},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated multi-node hadoop framework to predict high-risk factors of diabetes mellitus using a multilevel MapReduce based fuzzy classifier (MMR-FC) and modified DBSCAN algorithm},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A predictive GA-based model for closed high-utility itemset
mining. <em>ASOC</em>, <em>108</em>, 107422. (<a
href="https://doi.org/10.1016/j.asoc.2021.107422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining patterns with high utilization (or called high-utility itemset mining, HUIM) is considered a major issue in recent decades especially in the market (e.g., supermarket) engineering since it reveals the profitable information/products for decision-making. Many existing works focused on mining high-utility itemsets from databases that revealed a very large amount of patterns. This process cannot make the precise decision in a short time, e.g., real-time and online decision-making system, since it is not a trivial task to find the appropriate and useful information from a huge amount of the discovered knowledge in a limited time. Mining closed patterns with high utilization (or called closed high-utility pattern mining) is an alternative way to reveal fewer and concise patterns with high utilization in market engineering. However, many past works considered the complete mining progress of all HUIs and they do not consider the correlation between transactions thus when the transactions are not highly relevant, the trained model could not be perfectly used for the predication, which shows inappropriate results in machine learning tasks. In this paper, we consider the clustering models that can divide the transactions into the proper groups based on their correlation, which can be used to make a better accuracy model for prediction. To reduce the mining progress of the CHUIs, a compact GA model is also used in the mining progress, thus in the large-scale situation, a sufficient number of satisfied CHUIs can be discovered in a limited time. Based on the designed framework, the designed model can, not only mine the good enough CHUIs in the limited time for the large-scale environment but also the adopted clustering concept can make a better predictive model in the machine learning process, thus higher accuracy results can be obtained. Experimental results are then evaluated to compare the state-of-the-art CHUI-Miner and CLS-Miner with the developed DcGA model, and the designed DcGA achieves the best runtime performance and satisfied mining results.},
  archive      = {J_ASOC},
  author       = {Jerry Chun-Wei Lin and Youcef Djenouri and Gautam Srivastava and Unil Yun and Philippe Fournier-Viger},
  doi          = {10.1016/j.asoc.2021.107422},
  journal      = {Applied Soft Computing},
  pages        = {107422},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A predictive GA-based model for closed high-utility itemset mining},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fitness–distance balance based adaptive guided differential
evolution algorithm for security-constrained optimal power flow problem
incorporating renewable energy sources. <em>ASOC</em>, <em>108</em>,
107421. (<a href="https://doi.org/10.1016/j.asoc.2021.107421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most difficult types of problems computationally is the security-constrained optimal power flow (SCOPF), a non-convex, nonlinear, large-scale, nondeterministic polynomial time optimization problem . With the use of renewable energy sources in the SCOPF process, the uncertainties of operating conditions and stress on power systems have increased even more. Thus, finding a feasible solution for the problem has become a still greater challenge. Even modern powerful optimization algorithms have been unable to find realistic solutions for the problem. In order to solve this kind of difficult problem, an optimization algorithm needs to have an unusual exploration ability as well as exploitation–exploration balance. In this study, we have presented an optimization model of the SCOPF problem involving wind and solar energy systems . This model has one problem space and innumerable local solution traps, plus a high level of complexity and discrete and continuous variables. To enable the optimization model to find the solution effectively, the adaptive guided differential evolution (AGDE) algorithm was improved by using the Fitness–Distance Balance (FDB) method with its balanced searching and high-powered diversity abilities. By using the FDB method, solution candidates guiding the search process in the AGDE algorithm could be selected more effectively as in nature. In this way, AGDE’s exploration and balanced search capabilities were improved. To solve the SCOPF problem involving wind and solar energy systems , the developed algorithm was tested on an IEEE 30-bus test system under different operational conditionals. The simulation results obtained from the proposed algorithm were effective in finding the optimal solution compared to the results of the metaheuristics algorithms and reported in the literature.},
  archive      = {J_ASOC},
  author       = {Ugur Guvenc and Serhat Duman and Hamdi Tolga Kahraman and Sefa Aras and Mehmet Katı},
  doi          = {10.1016/j.asoc.2021.107421},
  journal      = {Applied Soft Computing},
  pages        = {107421},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fitness–Distance balance based adaptive guided differential evolution algorithm for security-constrained optimal power flow problem incorporating renewable energy sources},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective differential evolution algorithm and a
constraint handling mechanism based on variables proportion for dynamic
economic emission dispatch problems. <em>ASOC</em>, <em>108</em>,
107419. (<a href="https://doi.org/10.1016/j.asoc.2021.107419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic economic emission dispatch (DEED) problems have become an important research issue in power system operations . It is a multi-objective optimization problem (MOP) with high dimensional, nonlinear, nonsmooth, and nonconvex, considering the power balance constraints, valve point effects, prohibited operating zones, and ramp rate limits. Therefore, an efficient multi-objective optimization technology is needed to solve conflicting objectives in the DEED problems. Besides, an efficient constraint handling mechanism is the key step in solving real-world problems. In this paper, a proportional dynamic adjustment decision (PDAD) variables method is proposed to deal with the constraints in the DEED problems by considering the difference in the power generation range of the unit. While, the constraint handling mechanism of the slack variable method is improved, and a dynamic slack variable (DSL) method is proposed. Besides, a non-dominant sorting differential evolution algorithm with a self-adaptive parameter operator and a local search operator (NSDESa_LS) is developed to solve the DEED problems. Finally, the performance of the proposed method is compared with state-of-the-art methods on 5-, 10-, and 40-unit systems. The results show that the proposed NSDESa_LS-PDAD method has a superior performance.},
  archive      = {J_ASOC},
  author       = {Baihao Qiao and Jing Liu and Xingxing Hao},
  doi          = {10.1016/j.asoc.2021.107419},
  journal      = {Applied Soft Computing},
  pages        = {107419},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective differential evolution algorithm and a constraint handling mechanism based on variables proportion for dynamic economic emission dispatch problems},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development and experimental realization of an adaptive
neural-based discrete model predictive direct torque and flux controller
for induction motor drive. <em>ASOC</em>, <em>108</em>, 107418. (<a
href="https://doi.org/10.1016/j.asoc.2021.107418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a neural network-based discrete predictive direct torque and flux control (NNPDTFC) for induction motor drive with the space vector modulation (SVM) technique. Moreover, this SVM technique with NNPDTFC is implemented to activate the inverter in the two-level operation and the performance is compared with the conventional PI direct torque and flux control (PIDTFC) technique. The PSO based model predictive control incorporated with the neural network is developed here in the NNPDTFC and is analyzed using MATLAB software. Disturbance reduction, simple control, and real-time implementation are the major features of NNPDTFC and it also enhances the transient performance of the motor drive by reducing settling time and peak overshoot . In addition, the flux and the torque ripples are significantly improved using the proposed NNPDTFC technique which is extensively used for the fast dynamic response of the induction motor drives as compared to PIDTFC. In order to show the potentiality of the proposed controller, a prototype controller is developed and validated with the laboratory setup and the control signals are generated for both NNPDTFC and PIDTFC using a low-cost Digital signal processor (DSP) controller which is fed to the induction motor of 3.7 kW capacity in the real-time platform. It is observed that the results with NNPDTFC are not only found to be extremely satisfactory even with the system and parameter uncertainties and external load perturbations but also, it produces enhanced dynamic as well as steady-state performance along with the reduced ripples in the signal flux, torque, and current compared to that of PIDTFC.},
  archive      = {J_ASOC},
  author       = {Abhimanyu Sahu and Kanungo Barada Mohanty and Rabi Narayan Mishra},
  doi          = {10.1016/j.asoc.2021.107418},
  journal      = {Applied Soft Computing},
  pages        = {107418},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development and experimental realization of an adaptive neural-based discrete model predictive direct torque and flux controller for induction motor drive},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis, modeling, and multi-objective optimization of
machining inconel 718 with nano-additives based minimum quantity
coolant. <em>ASOC</em>, <em>108</em>, 107416. (<a
href="https://doi.org/10.1016/j.asoc.2021.107416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current study, analysis, modeling, and optimization of machining with nano-additives based minimum quantity lubrication (MQL) during turning Inconel 718 are presented and discussed. Multi-walled carbon nanotubes (MWCNTs) and aluminum oxide (Al 2 O 3 ) gamma nanoparticles were utilized as used nano-additives. The studied design variables include cutting speed, feed rate, and nano-additives percentage (wt.\%). Three machining outputs were considered namely: flank wear , surface roughness, and energy consumption. The novelty here focuses on improving the MQL heat capacity by employing two different nano-fluids. The analysis of variance (ANOVA) technique was employed to investigate the influence of the design variables on the studied machining outputs. The results demonstrated that the usage of MQL-nanofluids improved the cutting process performance compared to the classical approach of MQL. It was found that 4 wt.\% of added MWCNTs decreased the flank wear by 45.6\% compared to the pure MQL. Similarly, it was found that 4 wt.\% of added Al 2 O 3 nanoparticles improved the tool wear by 37.2\%. Besides, the nanotubes additives showed more improvements than Al 2 O 3 nanoparticles in terms of tool wear, surface quality, and energy consumption. Regarding the modeling stage, artificial neural network (ANN), adaptive neuro-fuzzy inference system (ANFIS), and genetic programming (GP) are employed to model the measured outputs in terms of the studied parameters. These soft computing approaches provide various advantages through their self-learning capabilities, fuzzy principles, and evolutionary computational concept. In addition, a comparison among the developed models has been conducted to select the most accurate approach to present the machining characteristics. Finally, the non-dominated sorting genetic algorithm (NSGA-II) was utilized to optimize the studied cutting processes. Moreover, a comparison between the optimized results from different approaches is presented. The proposed methodology presented in this work can be further implemented in other machining cases to model, analyze as well as optimize the machining performance, especially for the hard-to-cut materials which are commonly used in different industries.},
  archive      = {J_ASOC},
  author       = {H. Hegab and A. Salem and S. Rahnamayan and H.A. Kishawy},
  doi          = {10.1016/j.asoc.2021.107416},
  journal      = {Applied Soft Computing},
  pages        = {107416},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis, modeling, and multi-objective optimization of machining inconel 718 with nano-additives based minimum quantity coolant},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformer-based identification of stochastic information
cascades in social networks using text and image similarity.
<em>ASOC</em>, <em>108</em>, 107413. (<a
href="https://doi.org/10.1016/j.asoc.2021.107413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the origin of information posted on social media and how this may have changed over time can be very helpful to users in determining whether they trust it or not. This currently requires disproportionate effort for the average social media user , who instead has to rely on fact-checkers or other intermediaries to identify information provenance for them. We show that it is possible to disintermediate this process by providing an automated mechanism for determining the information cascade where a post belongs. We employ a transformer-based language model as well as pretrained ResNet50 model for image similarity, to decide whether two posts are sufficiently similar to belong to the same cascade. By using semantic similarity , as well as image in addition to text, we increase accuracy where there is no explicit diffusion of reshares. In a new dataset of 1, 200 news items on Twitter, our approach is able to increase clustering performance above 7\% and 4.5\% for the validation and test sets respectively over the previous state of the art. Moreover, we employ a probabilistic subsampling mechanism, reducing significantly cascade creation time without affecting the performance of large-scale semantic text analysis and the quality of information cascade generation. We have implemented a prototype that offers this new functionality to the user and have deployed it in our own instance of social media platform Mastodon.},
  archive      = {J_ASOC},
  author       = {Panagiotis Kasnesis and Ryan Heartfield and Xing Liang and Lazaros Toumanidis and Georgia Sakellari and Charalampos Patrikakis and George Loukas},
  doi          = {10.1016/j.asoc.2021.107413},
  journal      = {Applied Soft Computing},
  pages        = {107413},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transformer-based identification of stochastic information cascades in social networks using text and image similarity},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal 3D object detection by 2D-guided precision
anchor proposal and multi-layer fusion. <em>ASOC</em>, <em>108</em>,
107405. (<a href="https://doi.org/10.1016/j.asoc.2021.107405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection , of which the goal is to obtain the 3D spatial structure information of the object, is a challenging topic in many visual perception systems, e.g., autonomous driving , augmented reality , and robot navigation . Most existing region proposal network (RPN) based 3D object detection methods generate anchors in the whole 3D searching space without using semantic information , which leads to the problem of inappropriate anchor size generation . To tackle the issue, we propose a 2D-guided precision anchor generation network (PAG-Net). Specifically speaking, we utilize a mature 2D detector to get 2D bounding boxes and category labels of objects as prior information. Then the 2D bounding boxes are projected into 3D frustum space for more precise and category-adaptive 3D anchors. Furthermore, current feature combination methods are early fusion, late fusion, and deep fusion, which only fuse features from high convolutional layers and ignore the data missing problem of point clouds. To obtain more efficient fusion of RGB images and point clouds features, we propose a multi-layer fusion model, which conducts nonlinear and iterative combinations of features from multiple convolutional layers and merges the global and local features effectively. We encode point cloud with the bird’s eye view (BEV) representation to solve the irregularity of point cloud. Experimental results show that our proposed approach improves the baseline by a large margin and outperforms most of the state-of-the-art methods on the KITTI object detection benchmark.},
  archive      = {J_ASOC},
  author       = {Yi Wu and Xiaoyan Jiang and Zhijun Fang and Yongbin Gao and Hamido Fujita},
  doi          = {10.1016/j.asoc.2021.107405},
  journal      = {Applied Soft Computing},
  pages        = {107405},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-modal 3D object detection by 2D-guided precision anchor proposal and multi-layer fusion},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving capacitated vehicle routing problem using
cooperative firefly algorithm. <em>ASOC</em>, <em>108</em>, 107403. (<a
href="https://doi.org/10.1016/j.asoc.2021.107403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacitated vehicle routing problem (CVRP) is a classical combinatorial optimization problem , which has received much attention due to its main challenges as distribution, logistics, and transportation. This proposed attempts to find the vehicle routes with minimizing traveling distance, in which the excellent solution delivers a set of customers in one visit by capacitated vehicle. For solving the CVRP problem, a cooperative hybrid firefly algorithm (CVRP-CHFA) is proposed in this paper with multiple firefly algorithm (FA) populations. Each FA is hybridized with two types of local search (i.e., Improved 2-opt as a local search and 2-h-opt as a mutation operator) and genetic operators. The proposed algorithms (FAs) communicate from time to time for exchanging some solutions (fireflies). The main aim of the hybridization and communication strategies is to maintain the diversity of populations to prevent the proposed algorithm from falling into local optima and overcome the drawbacks of a single swarm FA. The experiments are conducted on 108 instances from eight standard benchmarks. The results revealed that the proposed CVRP-CHFA got promising results compared to other well-known methods. Moreover, the proposed CVRP-CHFA significantly outperformed the recent three hybrid firefly algorithms.},
  archive      = {J_ASOC},
  author       = {Asma M. Altabeeb and Abdulqader M. Mohsen and Laith Abualigah and Abdullatif Ghallab},
  doi          = {10.1016/j.asoc.2021.107403},
  journal      = {Applied Soft Computing},
  pages        = {107403},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving capacitated vehicle routing problem using cooperative firefly algorithm},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid AI system based on ART neural network and mixture of
gaussians modules with application to intelligent monitoring of the wind
turbine. <em>ASOC</em>, <em>108</em>, 107400. (<a
href="https://doi.org/10.1016/j.asoc.2021.107400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lack of effective intelligent monitoring systems of big wind turbines is a crucial problem in their exploitation. The used systems are based on direct monitoring of the system parameters by using programmable electronics or on the simple artificial intelligent systems, such as a single neural network. The efficiency of such systems is very limited. In the paper an innovative system for intelligent monitoring of wind turbines is proposed. The system is fully automated. It is based on Adaptive Resonance neural network (ART network) combined with two distinct modules based on Mixture of Gaussians. The proposed system is applied to detection of fault and pre-fault states of wind turbines. The turbine operational state data and data from vibration channels are the system input. First, the system specifies clusters that correspond to the healthy states of the turbine. Then, failure states are detected on-line as soon as they occur because the creation of a new cluster is signalized immediately by the ART module. The new cluster, that represents a failure or a pre-failure state is encoded in the Gaussian module which plays the role of the long-term memory. The system effectiveness was verified by using real data from a wind turbine farm.},
  archive      = {J_ASOC},
  author       = {Andrzej Bielecki and Mateusz Wójcik},
  doi          = {10.1016/j.asoc.2021.107400},
  journal      = {Applied Soft Computing},
  pages        = {107400},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid AI system based on ART neural network and mixture of gaussians modules with application to intelligent monitoring of the wind turbine},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fuzzy recommendation system for predicting the customers
interests using sentiment analysis and ontology in e-commerce.
<em>ASOC</em>, <em>108</em>, 107396. (<a
href="https://doi.org/10.1016/j.asoc.2021.107396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Electronic commerce , customer reviews play a significant role in purchase making decision. Most of the existing recommendation systems consider the customer reviews, user purchase history and product rating for predicting the recommended product. Since the users interest are varying over time, the existing recommendation systems lack in finding the current relevant items to the customers. To overcome this problem, this article proposes a new fuzzy logic-based product recommendation system which dynamically predicts the most relevant products to the customers in online shopping according to the users’ current interests. A novel algorithm has been proposed in this paper for computing the sentimental score of the product with associated end user target category. Finally, the proposed fuzzy rules and ontology-based recommendation system uses ontology alignment for making decisions that are more accurate and predict dynamically based on the search context. The experimental results of the proposed recommendation system show better performance than the existing product recommendation systems in terms of prediction accuracy of the relevant products for target users and in the time taken to provide such recommendations.},
  archive      = {J_ASOC},
  author       = {R.V. Karthik and Sannasi Ganapathy},
  doi          = {10.1016/j.asoc.2021.107396},
  journal      = {Applied Soft Computing},
  pages        = {107396},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy recommendation system for predicting the customers interests using sentiment analysis and ontology in e-commerce},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building a fuzzy sentiment dimension for multidimensional
analysis in social networks. <em>ASOC</em>, <em>108</em>, 107390. (<a
href="https://doi.org/10.1016/j.asoc.2021.107390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous growth of social networks has made them one of the main information sources for researchers and companies, but at the same time, their pre-processing and analysis represent a great challenge. In this work, we create a Fuzzy Sentiment Dimension from textual data from social networks to allow sentiment analysis jointly with standard dimensions in a multidimensional model, and make easier and more flexible sentiment analysis in social networks. In particular, we use technologies such as fuzzy logic and multidimensional analysis, together with unsupervised tools for sentiment analysis. The use of unsupervised tools for sentiment analysis also allows both previously labeled and unlabeled documents to be analyzed. The performance results of the experimentation demonstrate the feasibility of the proposal.},
  archive      = {J_ASOC},
  author       = {Karel Gutiérrez-Batista and Maria-Amparo Vila and Maria J. Martin-Bautista},
  doi          = {10.1016/j.asoc.2021.107390},
  journal      = {Applied Soft Computing},
  pages        = {107390},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Building a fuzzy sentiment dimension for multidimensional analysis in social networks},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive parametric yield enhancement via collinear
multivariate analytics for semiconductor intelligent manufacturing.
<em>ASOC</em>, <em>108</em>, 107385. (<a
href="https://doi.org/10.1016/j.asoc.2021.107385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensionality and multicollinearity are increasingly critical for parameter estimation in multivariate analysis to empower intelligent manufacturing, in which severe over-inflated coefficients may degrade the interpretability for modeling and prediction. Partial least squares (PLS) has been employed to handle collinearity for supervised data, in which the latent variables or principal components include all the collinear variables into the derived model. Although the PLS coefficients are decoupled for each collinear variable, the coefficients may be difficult to appropriately reflect their real relationships with the outcome variables. For semiconductor manufacturing , since some variables with collinearity are essential to explain the outcome variables and thus should be kept for further analysis. To fill the gaps, this study aims to develop an integrated approach with domain knowledge for reconstructing the PLS loadings and reinforce the parameter estimation via an innovative approach based on collinear multivariate model (CMVM) by utilizing the ramp function for the loadings of latent variables to optimize the essential coefficients, especially for those coefficients that should be nonnegative in practice. An empirical study was conducted for validation that has shown practical viability of the developed approach to suggest the optimal magnitude of coefficients for each of collinear variables and significantly improve the coefficient interpretation with managerial implications. Indeed, the developed solution is implemented in real settings.},
  archive      = {J_ASOC},
  author       = {Chen-Fu Chien and Chia-Cheng Chen},
  doi          = {10.1016/j.asoc.2021.107385},
  journal      = {Applied Soft Computing},
  pages        = {107385},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive parametric yield enhancement via collinear multivariate analytics for semiconductor intelligent manufacturing},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep unLSTM network: Features with memory information
extracted from unlabeled data and their application on industrial
unsupervised industrial fault detection. <em>ASOC</em>, <em>108</em>,
107382. (<a href="https://doi.org/10.1016/j.asoc.2021.107382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world data, especially in industrial processes in which a property only fully retains normal historical data, are usually unlabeled or incorrectly labeled. Moreover, data from industrial processes are often time-dependent and associated with event sequences. These characteristics make process monitoring based on a current gauged sample unreasonable, thus requiring performance improvement. To overcome this bottleneck, this study proposes a deep unLSTM network to extract features with memory information from unlabeled data . The network comprises three parts, namely, encoder, code layer, and decoder. A normal historical training dataset is converted into a 3D input-form, and each layer outputs the 2D information, which contains all the states of an event sequence for connection to the next LSTM layer to the reconstruction layer. Additionally, a slicing operation is developed to provide accurate features rich in current memory information when constructing statistics. What is more, except traditional process monitoring statistics, a similarity statistic based on global hidden layer features is proposed to indicate the working state of the current process more comprehensively. Compared with other deep models used on the Tennessee Eastman and real steel plate processes, the proposed deep unLSTM network, which can extract the features with memory information, is more suitable for industrial fault detection.},
  archive      = {J_ASOC},
  author       = {Jianbo Yu and Xuefeng Yan},
  doi          = {10.1016/j.asoc.2021.107382},
  journal      = {Applied Soft Computing},
  pages        = {107382},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep unLSTM network: Features with memory information extracted from unlabeled data and their application on industrial unsupervised industrial fault detection},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual savant as a generic learning approach applied to the
basic independent next release problem. <em>ASOC</em>, <em>108</em>,
107374. (<a href="https://doi.org/10.1016/j.asoc.2021.107374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents how Virtual Savant (VS) can be used to automatically learn, from an exact algorithm, how to solve the basic independent Next Release Problem in a quick and accurate way. This variant of the Next Release Problem (NRP) is in essence a 0/1 Knapsack Problem and VS is applied to solve the underlying optimization problem . VS is a generic problem-solving approach based on machine learning and heuristics, that works by mimicking how a reference program produces solutions to problem instances. Essentially, VS learns how to generate solutions to a given problem from a reference algorithm and a training set of instances, and it is able to efficiently solve new problem instances by using the acquired knowledge. In this paper, an exact optimizer is used as a reference algorithm . Hence, we are using VS to learn from optimal solutions, which helps to reduce the approximation error inherent to the learning process. We compare five versions of VS (differing in the heuristics they implement) on a large benchmark, composed of problems with different sizes and difficulties. For the best VS configuration, which also has the lowest computational complexity , computed solutions differ less than 1\% from the optima in the worst case. Therefore, VS succeeds in learning how to solve the problem under study, and it does so in a highly efficient way, exempting the programmer from having a deep knowledge of the problem domain or highly specialized parallel programming skills.},
  archive      = {J_ASOC},
  author       = {Renzo Massobrio and Sergio Nesmachnow and Francisco Palomo-Lozano and Bernabé Dorronsoro},
  doi          = {10.1016/j.asoc.2021.107374},
  journal      = {Applied Soft Computing},
  pages        = {107374},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Virtual savant as a generic learning approach applied to the basic independent next release problem},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multivariate influence through neural networks ensemble:
Study of saharan dust intrusion in the canary islands. <em>ASOC</em>,
<em>107</em>, 107497. (<a
href="https://doi.org/10.1016/j.asoc.2021.107497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing and predicting the concentration of airborne dust is vital to the economic activity and to the health of the population. In this study, we use a set of artificial neural networks that we structure through ensemble learning to yield a complex variable, such as the concentration of dust, based on actual data such as air temperature, relative humidity, atmospheric pressure and wind speed. The statistical performance indices obtained, show the effectiveness of the proposed approach through the application of a cross-validation committee. It is thus vital to have a reliable calculation method for determining relative importances that can be applied to this type of ensemble architecture by way of artificial neural networks. Unlike other relative importance methods, where calculations are done based directly on the weights in the artificial neural network and whose results in ensemble sets exhibit high dispersion, we propose our own procedure, which selectively chooses the variation in the inputs to readjust the architecture of the neural network. This allows us to measure those variables with the greatest effect on the target variable, thus obtaining the multivariate influence on the surface dust concentration through a computational model . This method thus provides a real alternative for calculating and estimating relative importance that can be generalized to any type of problem for multivariate systems modeled using artificial neural networks for both, a simple configuration, and an ensemble architecture.},
  archive      = {J_ASOC},
  author       = {D. Gonzalez-Calvo and R.M. Aguilar and C. Criado-Hernandez and L.A. Gonzalez-Mendoza},
  doi          = {10.1016/j.asoc.2021.107497},
  journal      = {Applied Soft Computing},
  pages        = {107497},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multivariate influence through neural networks ensemble: Study of saharan dust intrusion in the canary islands},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 outbreak: An ensemble pre-trained deep learning
model for detecting informative tweets. <em>ASOC</em>, <em>107</em>,
107495. (<a href="https://doi.org/10.1016/j.asoc.2021.107495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On 11 March 2020, the (WHO) World Health Organization declared COVID-19 (CoronaVirus Disease 2019) as a pandemic. A further crisis has manifested mass fear and panic, driven by lack of information, or sometimes outright misinformation , alongside the coronavirus pandemic. Twitter is one of the prominent and trusted social media in this current outbreak. Over time, boundless COVID-19 headlines and vast awareness have been spreading, with tweets, updates, videos, and explosive posts. Few studies have been performed on the pandemic to detect and interrelate various disease types, including current coronavirus. However, it is pretty tricky to discriminate and detect a specific category. This work is motivated by the need to inform society about limiting irrelevant information and avoiding spreading negative emotions. In this context, the current work focuses on informative tweet detection in the pandemic to provide relevant information to the government, medical organizations, victims services, etc. This paper used a Majority Voting technique-based Ensemble Deep Learning (MVEDL) model. This MVEDL model is used to identify COVID-19 related (INFORMATIVE) tweets. The state-of-art deep learning models RoBERTa, BERTweet, and CT-BERT are used for best performance with the MVEDL model. The “COVID-19 English labeled tweets” dataset is used for training and testing the MVEDL model. The MVEDL model has shown 91.75 percent accuracy, 91.14 percent F1-score and outperforms the traditional machine learning and deep learning models. We also investigate how to use the MVEDL model for sentiment analysis on 226668 unlabeled COVID-19 tweets and their informative tweets. The application section discussed a comprehensive analysis of both actual and informative tweets. According to our knowledge, this is the first work on COVID-19 sentiment analysis using a deep learning ensemble model.},
  archive      = {J_ASOC},
  author       = {SreeJagadeesh Malla and Alphonse P.J.A.},
  doi          = {10.1016/j.asoc.2021.107495},
  journal      = {Applied Soft Computing},
  pages        = {107495},
  shortjournal = {Appl. Soft. Comput.},
  title        = {COVID-19 outbreak: An ensemble pre-trained deep learning model for detecting informative tweets},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive strategy in differential evolution via explicit
exploitation and exploration controls. <em>ASOC</em>, <em>107</em>,
107494. (<a href="https://doi.org/10.1016/j.asoc.2021.107494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-strategy adaptive differential evolution (DE) commonly involves trials of multiple strategies and then rewards better-performing ones with more resources. However, the trials of an exploitative or explorative strategy may result in over-exploitation or over-exploration. To improve the performance, this paper proposes a new strategy adaptation method, named explicit adaptation scheme (Ea scheme), which separates multiple strategies and employs them on-demand. It is done by dividing the evolution process into several Selective-candidate with Similarity Selection (SCSS) generations and adaptive generations. In the SCSS generations, the exploitation and exploration needs are learnt by utilizing a balanced strategy. To meet these needs, in adaptive generations, two other strategies, exploitative or explorative is adaptively used. Experimental studies on benchmark functions demonstrate the effectiveness of Ea scheme when compared with its variants and other adaptation methods. Furthermore, performance comparisons with state-of-the-art evolutionary algorithms and swarm intelligence-based algorithms show that EaDE is very competitive.},
  archive      = {J_ASOC},
  author       = {Sheng Xin Zhang and Wing Shing Chan and Kit Sang Tang and Shao Yong Zheng},
  doi          = {10.1016/j.asoc.2021.107494},
  journal      = {Applied Soft Computing},
  pages        = {107494},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive strategy in differential evolution via explicit exploitation and exploration controls},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Edge computing-based person detection system for top view
surveillance: Using CenterNet with transfer learning. <em>ASOC</em>,
<em>107</em>, 107489. (<a
href="https://doi.org/10.1016/j.asoc.2021.107489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing significantly expands the range of information technology in smart video surveillance applications in the era of intelligent and connected cities. Edge devices, including Internet of Things-based cameras and sensors, produce a large amount of data and have frequently become prominent components for various public surveillance and monitoring applications. The data generated by these smart devices are in the form of videos and images that need to be processed and analyzed in real-time with substantial computation resources. These developed techniques still require large computation resources for real-time surveillance applications. In this regard, Edge computing plays a promising role in order to provide high computation and low-latency requirements. With these motivations, in this work, a real-time top view-based person detection system is presented. We utilize a one-stage deep learning-based object detection algorithm , i.e., CenterNet, for person detection. The model detects the human as a single point, also referred to as its bounding box’s center point. The model does a key-point calculation to obtain the center point and regresses all other information regarding the target object’s features, size, location, and orientation. Training and testing of the model are performed on a top view data set. The detection results are also compared with conventional detection methods using the same data set. The overall detection accuracy of the model is 95\%.},
  archive      = {J_ASOC},
  author       = {Imran Ahmed and Misbah Ahmad and Joel J.P.C. Rodrigues and Gwanggil Jeon},
  doi          = {10.1016/j.asoc.2021.107489},
  journal      = {Applied Soft Computing},
  pages        = {107489},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Edge computing-based person detection system for top view surveillance: Using CenterNet with transfer learning},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semi-parametric ensemble model for profit evaluation and
investment decisions in online consumer loans with prepayments.
<em>ASOC</em>, <em>107</em>, 107485. (<a
href="https://doi.org/10.1016/j.asoc.2021.107485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, online consumer loans have advanced rapidly. Prepayment, which is the practice of repaying the loan before maturity, is increasingly emerging as a feature of such loans. Unlike for traditional loans, the effect of prepayment on online loans has not been well addressed by either industry or academia. In this study, we took Peer-to-Peer (P2P) online lending as an example in order to provide a new perspective for evaluating the profitability of online loans where there are also prepayments present. Firstly, we defined a future-value-based return rate to measure the profits of a loan. Then, we proposed an ensemble model based on a semi-parametric mixture distribution to predict the expected return rate. In addition, we formulated a kernel method by borrowing information from similarly profitable loans to assess the return risk. Finally, we established a method to optimise the portfolio selection while taking prepayments into account.},
  archive      = {J_ASOC},
  author       = {Ke Li and Fanyin Zhou and Zhiyong Li and Wanqing Li and Feng Shen},
  doi          = {10.1016/j.asoc.2021.107485},
  journal      = {Applied Soft Computing},
  pages        = {107485},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semi-parametric ensemble model for profit evaluation and investment decisions in online consumer loans with prepayments},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A variable neighborhood search approach for the
resource-constrained multi-project collaborative scheduling problem.
<em>ASOC</em>, <em>107</em>, 107480. (<a
href="https://doi.org/10.1016/j.asoc.2021.107480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-mode and multi-skill project scheduling problems widely exist in the real-world industry. Many efforts have been devoted to the multi-mode project scheduling problem and the multi-skill project scheduling problem. However, the integrated multi-mode and multi-skill project scheduling problem was seldom considered. This paper studies the resource-constrained multi-project scheduling problem in the development of high-end equipment and develops an integrated multi-mode and multi-skill scheduling model with different employee abilities. Based on the characteristics of high-end equipment development, we propose several optimal properties and design an effective heuristic algorithm . Since the resource-constrained project scheduling problem is NP-hard, three neighborhoods for the problem are constructed and a Variable Neighborhood Search Algorithm (VNS) is developed to solve the problem in a reasonable time. Finally, computational experiments are carried out to validate the performance of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Longqing Cui and Xinbao Liu and Shaojun Lu and Zhaoli Jia},
  doi          = {10.1016/j.asoc.2021.107480},
  journal      = {Applied Soft Computing},
  pages        = {107480},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A variable neighborhood search approach for the resource-constrained multi-project collaborative scheduling problem},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyper-parameter tuned light gradient boosting machine using
memetic firefly algorithm for hand gesture recognition. <em>ASOC</em>,
<em>107</em>, 107478. (<a
href="https://doi.org/10.1016/j.asoc.2021.107478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture is considered as one of the natural ways to interact with computers. The utility of hand gesture-based application is a recent trend and is an effective method for human–computer interaction. Though many static and other intelligent approaches using Machine learning (ML) are developed, still there is a marginal tradeoff between the computational cost and accuracy. In this paper, a Lightboost based Gradient boosting machine (LightGBM) is proposed for efficient hand gesture recognition . The hyper-parameters of the LightGBM are optimized with an improved memetic firefly algorithm . We have introduced a perturbation operator and incorporated it in the proposed memetic firefly algorithm for avoiding the local optimal solution in the traditional firefly algorithm. With comparative analysis among the proposed method and other competitive ML methods, the performance of the proposed method is found to be better in terms of various performance metrics such as accuracy, precision, recall, F1 score, and ROC–AUC. The proposed memetic firefly-based boosting approach is dominant over all the other considered methods with an accuracy of 99.36\% and is robust for accurate hand gesture recognition .},
  archive      = {J_ASOC},
  author       = {Janmenjoy Nayak and Bighnaraj Naik and Pandit Byomakesha Dash and Alireza Souri and Vimal Shanmuganathan},
  doi          = {10.1016/j.asoc.2021.107478},
  journal      = {Applied Soft Computing},
  pages        = {107478},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyper-parameter tuned light gradient boosting machine using memetic firefly algorithm for hand gesture recognition},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimized video content delivery for mobile networks using
the transformative computing model. <em>ASOC</em>, <em>107</em>, 107477.
(<a href="https://doi.org/10.1016/j.asoc.2021.107477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenge for future mobile networks to cope with enormous video traffic due to bottlenecks of the backhaul capacity and spectrum shortages. To address this situation, we study a content delivery solution for video-on-demand (VoD) applications in orthogonal frequency division multiplexing (OFDM)-based mobile networks in this paper. As done in some transformative computing models, the proposed solution firstly collects the user and environment information, and then computes a global-optimized video delivery scheme based on the collected information. The video delivery scheme is orchestrated using a novel video delivery model that implements deep cooperation among video users and device-to-device (D2D) communications. Several orchestration algorithms are proposed to improve the performance of video delivery.},
  archive      = {J_ASOC},
  author       = {Jong-Hyouk Lee and Zhiwei Yan and Xinchang Zhang},
  doi          = {10.1016/j.asoc.2021.107477},
  journal      = {Applied Soft Computing},
  pages        = {107477},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized video content delivery for mobile networks using the transformative computing model},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Q-learning-based parameter control in differential evolution
for structural optimization. <em>ASOC</em>, <em>107</em>, 107464. (<a
href="https://doi.org/10.1016/j.asoc.2021.107464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The operations of metaheuristic optimization algorithms depend heavily on the setting of control parameters. Therefore the addition of adaptive control parameter has been widely studied and shown to enhance the problem flexibility and overall performance of the algorithms. This paper proposes Q-learning Differential Evolution (qlDE) algorithm, an adaptive control parameter Differential Evolution(DE) algorithm, for structural optimization. The reinforcement learning Q-learning model is integrated into DE as an adaptive parameter controller , adaptively adjusting control parameters of the algorithm at each search iteration in order to optimally regulate its behavior for different search domains. Moreover, by automatically controlling the balance of exploration and exploitation at different stages of the process, the performance of the optimizer will also be enhanced. To verify the effectiveness and robustness of the proposed qlDE algorithm in comparison with the classical DE and several other algorithms in the literature, five benchmark examples of truss structural weight minimizations will be performed in this study.},
  archive      = {J_ASOC},
  author       = {Thanh N. Huynh and Dieu T.T. Do and Jaehong Lee},
  doi          = {10.1016/j.asoc.2021.107464},
  journal      = {Applied Soft Computing},
  pages        = {107464},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Q-learning-based parameter control in differential evolution for structural optimization},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ant colony optimization for traveling salesman problem based
on parameters optimization. <em>ASOC</em>, <em>107</em>, 107439. (<a
href="https://doi.org/10.1016/j.asoc.2021.107439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traveling salesman problem (TSP) is one typical combinatorial optimization problem . Ant colony optimization (ACO) is useful for solving discrete optimization problems whereas the performance of ACO depends on the values of parameters. The hybrid symbiotic organisms search (SOS) and ACO algorithm (SOS–ACO) is proposed for TSP. After certain parameters of ACO are assigned, the remaining parameters can be adaptively optimized by SOS. Using the optimized parameters, ACO finds the optimal or near-optimal solution and the complexity for assigning ACO parameters is greatly reduced. In addition, one simple local optimization strategy is incorporated into SOS–ACO for improving the convergence rate and solution quality. SOS–ACO is tested with different TSP instances in TSPLIB. The best solutions are within 2.33\% of the known optimal solution. Compared with some of the previous algorithms, SOS–ACO finds the better solutions under the same preconditions. Finally, the performance of SOS–ACO is analyzed according to the changes of some ACO parameters. The experimental results illustrate that SOS–ACO has good adaptive ability to various values of these parameters for finding the competitive solutions.},
  archive      = {J_ASOC},
  author       = {Yong Wang and Zunpu Han},
  doi          = {10.1016/j.asoc.2021.107439},
  journal      = {Applied Soft Computing},
  pages        = {107439},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ant colony optimization for traveling salesman problem based on parameters optimization},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustness of unified power quality conditioner by neural
network based on admittance estimation. <em>ASOC</em>, <em>107</em>,
107420. (<a href="https://doi.org/10.1016/j.asoc.2021.107420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper attempts to improve the dynamic performance of unified power quality conditioner (UPQC) by using of neural network based on admittance estimation strategy (NN-ADES). For improve the efficiency, tracking capability and robustness of UPQC, a control strategy is used for estimation of fundamental admittance components (conductance and susceptance) of distorted source voltage . In this method, the admittance components weight values of estimated admittance components input conductance clustered value which is very near to weight values of actual admittance components. The NN-ADES strategy is suitable for where load periodicity not constant. The neural network admittance estimation strategy for UPQC is implemented using MATLAB and FPGA board for mitigating balanced or unbalanced load conditions, voltage sag or voltage swell and harmonics of source voltage . Test results of proposed UPQC have produced acceptable results under balanced/unbalanced load conditions.},
  archive      = {J_ASOC},
  author       = {Majid Aryanezhad and Elahe Ostadaghaee},
  doi          = {10.1016/j.asoc.2021.107420},
  journal      = {Applied Soft Computing},
  pages        = {107420},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robustness of unified power quality conditioner by neural network based on admittance estimation},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A self-adaptive strategy based firefly algorithm for
constrained engineering design problems. <em>ASOC</em>, <em>107</em>,
107417. (<a href="https://doi.org/10.1016/j.asoc.2021.107417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firefly algorithm (FA) is an effective meta-heuristic method, which has drawn extensive attentions and inspired many variants. However, it is difficult to balance the relation of the exploration and exploitation of FA. To this end, FA with self-adaptive strategy (SAFA) is proposed to improve the performance of the algorithm for constrained engineering design problems . Firstly, the characteristics of the standard FA are analyzed, and the essence of FA easily trapped into the local optima is revealed by theory analysis and case study. Secondly, a self-adaptive strategy for attraction model is proposed to improve the exploitation ability, and another self-adaptive strategy for stochastic model is developed to ensure a better balance between the exploration and exploitation. Thirdly, a self-adaptive penalty function is presented to treat constraint conditions. Then, the initial parameter setting is investigated, and the proper initial parameter value corresponding to the optimal performance of SAFA is obtained. And then, SAFA and other FA variants are used to optimize a set of classical and Congress on Evolutionary Computation (CEC) 2015 benchmark functions . Meanwhile, the contributions of self-adaptive strategies for attraction model and stochastic model are investigated with experimental analysis, respectively Finally, SAFA and other meta-heuristic methods are employed to solve constrained engineering design problems . The effects of SAFA with standard and self-adaptive penalty function are compared. Numerical experimental results show that self-adaptive strategies for attraction model and stochastic model improve the performance of FA, and SAFA obtains the best solutions on most of the benchmark functions . In addition, SAFA has the advantage in solving problems with different kind of dimensions, and maintains reasonable population diversity throughout the iteration compared with the other FA variants. Meanwhile, SAFA can reduce the computational complexity and ensure the solution accuracy in solving constrained optimization problems .},
  archive      = {J_ASOC},
  author       = {Ran Tao and Zeng Meng and Huanlin Zhou},
  doi          = {10.1016/j.asoc.2021.107417},
  journal      = {Applied Soft Computing},
  pages        = {107417},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-adaptive strategy based firefly algorithm for constrained engineering design problems},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modified rough VIKOR based design concept evaluation method
compatible with objective design and subjective preference factors.
<em>ASOC</em>, <em>107</em>, 107414. (<a
href="https://doi.org/10.1016/j.asoc.2021.107414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design Concept Evaluation (DCE) is a crucial step in new product development. In complex DCE task, the designer as a decision-maker has to make a comprehensive choice by considering the design concept’s inherent objective design factor as well as external subjective preference factor from evaluators (designer, expert or customer). However, most of DCE methods only limited to one of two factors, which unilaterally evaluate the alternatives and miss the optimal one. To find more reasonable design concept, this study attempts to better compatible with objective design and subjective preference factors in evaluation process, and proposes a new DCE method using new integrated ideal solution definition (I-ISD) approach in modified VIKOR model based on rough number, named as R-VIKOR(I). To be specific, this study puts forward four definition rules to select the positive and negative ideal solution elements respectively for benefit-like quantitative attribute, cost-like quantitative attribute, important qualitative attribute and less important qualitative attribute by utilizing the information originated from design and preference data, and calculates the deviation between alternative and redefined ideal solution through rough VIKOR to obtain the best one. Three comparative experiments have been carried out to validate the performance of R-VIKOR(I) by analysing its robustness (experiment I), comparing it with other classical DCE methods based on rough TOPSIS , rough WASPAS and rough COPRAS (experiment II) and exploring the applicable of the proposed I-ISD approach (experiment III). Experimental results verify that R-VIKOR(I) could better balance the objective design attribute values and the subjective evaluator preference values to provide more reasonable evaluation result, especially this method has obvious advantage when evaluators have different preferences for design attribute values, a common case in modern personalized product development.},
  archive      = {J_ASOC},
  author       = {Jin Qi and Jie Hu and Yinghong Peng},
  doi          = {10.1016/j.asoc.2021.107414},
  journal      = {Applied Soft Computing},
  pages        = {107414},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified rough VIKOR based design concept evaluation method compatible with objective design and subjective preference factors},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating data visibility decision in a multi-objective
procurement transport planning under risk: A modified NSGA-II.
<em>ASOC</em>, <em>107</em>, 107406. (<a
href="https://doi.org/10.1016/j.asoc.2021.107406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is an in-depth investigation of the interplay between suppliers’ data visibility and procurement risks in the supplier selection and procurement planning problem. The role of suppliers’ ability to share high-quality data is incorporated into a decision-making environment. We consider supply chain visibility (as an exogenous decision) to increase the buyer’s awareness of the order status and manage supply chain risks . In doing so, a multi-attribute information visibility index is proposed to measure suppliers’ capability to share the required visible data. A novel multi-objective mixed-integer programming model is then developed to determine the cross-functional policy of purchasing and transportation under risk. A Non-dominated Sorting Genetic Algorithm (NSGA-II) is also tailored to attain Pareto-optimal solutions. Our result indicates the superiority of the proposed NSGA-II algorithm.},
  archive      = {J_ASOC},
  author       = {Fereshte Shabani-Naeeni and R. Ghasemy Yaghin},
  doi          = {10.1016/j.asoc.2021.107406},
  journal      = {Applied Soft Computing},
  pages        = {107406},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating data visibility decision in a multi-objective procurement transport planning under risk: A modified NSGA-II},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A genetic simulated annealing algorithm for parallel
partial disassembly line balancing problem. <em>ASOC</em>, <em>107</em>,
107404. (<a href="https://doi.org/10.1016/j.asoc.2021.107404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The timely recovery and disassembly of waste electrical and electronic equipment (WEEE) can not only obtain a higher economic benefit but also can reduce the impact of hazardous substances on the environment. The parallel disassembly line can disassemble different kinds of WEEE synchronously and improve disassembly efficiency. Therefore, a parallel partial disassembly line balancing model with stochastic disassembly time is established in this paper. The evaluation indexes of the disassembly line include the number of workstations, workload smoothness, and disassembly profits. A new genetic simulated annealing algorithm is proposed to optimize the model. The encoding and decoding strategies are constructed according to the characteristics of partial disassembly and parallel layout. Two-point mapping crossover and single-point insertion mutation operations are designed to ensure that the disassembly sequence meets the precedence constraints and disassembly constraints. The simulated annealing operation is applied to the results of the genetic operation. The proposed algorithm obtains better solutions than the tabu search algorithm in stochastic parallel assembly line balancing problems, and the proposed algorithm has better performance than the CPLEX solver, genetic algorithm , and simulated annealing in parallel disassembly line balancing problems. Finally, a parallel partial disassembly line for waste televisions and refrigerators is constructed, and the performance of the proposed multi-objective algorithm is superior to those of five classical multi-objective algorithms. The results show that the proposed model has a better practical application ability and that the proposed algorithm can improve the performance of disassembly lines.},
  archive      = {J_ASOC},
  author       = {Kaipu Wang and Xinyu Li and Liang Gao and Peigen Li and Surendra M. Gupta},
  doi          = {10.1016/j.asoc.2021.107404},
  journal      = {Applied Soft Computing},
  pages        = {107404},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic simulated annealing algorithm for parallel partial disassembly line balancing problem},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A combined multi-agent system for distributed multi-project
scheduling problems. <em>ASOC</em>, <em>107</em>, 107402. (<a
href="https://doi.org/10.1016/j.asoc.2021.107402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A distributed multi-project scheduling problem is considered, in which several projects share scarce resources, and a planning department (planner) is responsible for allocating the resources among the projects. Information asymmetry and heterogeneous resources are assumed to be due to the geographical distribution of the planner and the projects. The projects compete for the limited global resources to maximize their local benefit, such that they may lie or overstate resource importance to the planner. In this paper, a multi-agent system is developed to address this problem due to the concerns of private information and highly autonomous nature of project agents, which makes a central coordination approach unsuitable. Different from previous work, a project agent may employ the lying strategy to increase its possibility of winning the desired resource, while the planner can adopt an integrity policy to penalize this behaviour. Another main contribution is that a heuristic procedure is designed and combined with an argumentation-based approach for this multi-agent system that can improve computation efficiency. Finally, the proposed combined multi-agent system is compared with a central coordination algorithm to demonstrate its efficacy. Numerical experiments show that the combined multi-agent system is more effective in exploration. It outperforms the central coordination algorithm for problems of a larger scale, especially those with a tighter global resource constraint. Experimental results also reveal that the proper integrity policy could considerably reduce the negative effect of dishonesty of the project agents on the global objective by eliminating the potential to benefit from lying.},
  archive      = {J_ASOC},
  author       = {Fang Fu and Hong Zhou},
  doi          = {10.1016/j.asoc.2021.107402},
  journal      = {Applied Soft Computing},
  pages        = {107402},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combined multi-agent system for distributed multi-project scheduling problems},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FH-ACO: Fuzzy heuristic-based ant colony optimization for
joint virtual network function placement and routing. <em>ASOC</em>,
<em>107</em>, 107401. (<a
href="https://doi.org/10.1016/j.asoc.2021.107401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV) is a new networking paradigm, which replaces the specific-purpose hardware appliances with software virtualization to perform network functions on general-purpose servers. Due to resource limitation at network servers/links, virtual network function (VNF) placement/routing is one of the main challenges in the NFV architecture. The VNF placement/routing problem has been reported to be NP-hard, and consequently, most of studies have focused on designing heuristics or metaheuristics . In this paper, fuzzy heuristic-based ant colony optimization (named FH-ACO) is introduced as a fuzzy knowledge-based version of ACO to efficiently solve the VNF placement/routing problem. Our motivation is to simultaneously gain with fast speed of heuristics and high solution quality of metaheuristics . In the FH-ACO, two multi-criteria fuzzy inference systems are used as heuristic information to guide artificial ants in appropriate server/link selection during the solution construction phase. To solve the problem, a multi-objective function comprising power consumption , latency, reliability, and load balancing, is used. The proposed FH-ACO algorithm is an application-specific protocol, which can be adaptively tuned based on the application requirements. Simulation results on US and Pan-European network topologies demonstrate the superiority of the proposed FH-ACO algorithm against the existing techniques.},
  archive      = {J_ASOC},
  author       = {Mohammad Shokouhifar},
  doi          = {10.1016/j.asoc.2021.107401},
  journal      = {Applied Soft Computing},
  pages        = {107401},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FH-ACO: Fuzzy heuristic-based ant colony optimization for joint virtual network function placement and routing},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiobjective multifactorial immune algorithm for
multiobjective multitask optimization problems. <em>ASOC</em>,
<em>107</em>, 107399. (<a
href="https://doi.org/10.1016/j.asoc.2021.107399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by human brains’ ability to solve multiple tasks simultaneously, evolutionary multitasking is proposed to improve the overall efficiency of optimizing multiple tasks simultaneously by reusing the learned knowledge. The immune algorithm is inspired by the biological immune system that has been proven to be effective in many practical multiobjective optimization problems , with efficient convergence and search efficiency. In this paper, a novel multiobjective multifactorial immune algorithm is proposed with a novel information transfer method to solve multiobjective multitask optimization problems . For each task, information advantageous for this task will be transferred from the others to accelerate convergence through the proposed information transfer method. Finally, the proposed algorithm is compared with the state-of-the-art multiobjective evolutionary multitasking algorithms and the classic multiobjective evolutionary algorithms . The experimental results on the classical multiobjective multitask and the multiobjective many-task test suites demonstrate that the proposed algorithm provides very promising performances.},
  archive      = {J_ASOC},
  author       = {Zhiwei Xu and Kai Zhang},
  doi          = {10.1016/j.asoc.2021.107399},
  journal      = {Applied Soft Computing},
  pages        = {107399},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective multifactorial immune algorithm for multiobjective multitask optimization problems},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring superposition state in multi-scale quantum
harmonic oscillator algorithm. <em>ASOC</em>, <em>107</em>, 107398. (<a
href="https://doi.org/10.1016/j.asoc.2021.107398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on the merging of quantum mechanics and intelligence algorithms has received extensive attention in recent years. When applying quantum mechanics to intelligent algorithms, the first noteworthy problem is the representation of solutions in the form of a probability superposition. In the recently proposed multi-scale quantum harmonic oscillator algorithm (MQHOA), the representation of the solutions is not a superposition of the probability distributions, but a single distribution. This incorrect representation of the superposition state reduces the diversity of the solutions and limits the performance of the algorithm. To alleviate this problem, in this paper, a multiple-population-based mechanism that uses a quantum superposition is presented for MQHOA, which increases the search capacity of the algorithm by enhancing the diversity of the solutions. With the proposed method, the eigenstate is represented by the different size and distribution subpopulations of the particles. The optimization process is the process of continually changing the size and the distribution of the subpopulations to make the superposition state collapse to the ground state. The multiple-population-based superposition state greatly increases the diversity of the optimization system and enhances the movement efficiency of the particles. The effectiveness of the method based on the performance of the algorithm was thoroughly investigated by comparing the method with the original MQHOA and several well-known intelligent algorithms on unimodal and multimodal benchmark functions and on a real-world charge management problem for plug-in hybrid electric vehicles. The experimental results reveal the effectiveness of the multiple-population-based superposition state used by the proposed method.},
  archive      = {J_ASOC},
  author       = {Gang Xin and Peng Wang},
  doi          = {10.1016/j.asoc.2021.107398},
  journal      = {Applied Soft Computing},
  pages        = {107398},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exploring superposition state in multi-scale quantum harmonic oscillator algorithm},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective parallel integrated neural network system for
industrial data prediction. <em>ASOC</em>, <em>107</em>, 107397. (<a
href="https://doi.org/10.1016/j.asoc.2021.107397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, General Regression Neural Network (GRNN) have been more and more used for data prediction in industry, however, because its smoothing factor is difficult to determine, it is easy to obtain poor prediction accuracy when using it to predict complex problems in reality. To tackle these problems, an effective Parallel Integrated Neural Network System (PINN) is proposed in this paper. The model is a combination of GRNN and Adaptive Dynamic Grey Wolf Optimizer (ADGWO), in this model, the smoothing factor and calculation result of GRNN are taken as the individual position information and individual fitness of ADGWO, respectively, and the training of the model is completed through the optimization of ADGWO. Different from Grey Wolf Optimizer (GWO), ADGWO introduces the nonlinear cosine decreasing convergence factor, the weighted position update method and the central disturbance criterion, aiming to balance the exploitation and exploration. Applying PINN to the soil heavy metal datasets from Yinchuan of Ningxia and Wuhan, China for data prediction, the experimental results show that PINN has higher average prediction accuracy than several comparative models, especially an increase of 8.05\% compared with Wavelet Neural Network, which proves that PINN can be effectively applied to industrial data prediction.},
  archive      = {J_ASOC},
  author       = {Wenqi Cao and Cong Zhang},
  doi          = {10.1016/j.asoc.2021.107397},
  journal      = {Applied Soft Computing},
  pages        = {107397},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective parallel integrated neural network system for industrial data prediction},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A risk-averse decision based on IGDT/stochastic approach for
smart distribution network operation under extreme uncertainties.
<em>ASOC</em>, <em>107</em>, 107395. (<a
href="https://doi.org/10.1016/j.asoc.2021.107395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Renewable generation as well as electrical demand uncertainties cause significant technical challenges in addition to associated financial consequences in smart distribution networks (SDNs), particularly in the electricity markets, which are restructured and are featured by smart grids. In this paper, a risk-averse-strategy-based decision making tool is proposed to help the smart distribution network operator (SDNO) in day-ahead operational practices including optimal unit commitment (UC) and optimal distribution feeder reconfiguration (DFR). The proposed tool aims to reduce the consumers’ electricity prices as well as to optimize the financial transactions with the energy market, reliability of distributed generation (DG), electricity storage system (ESS) dispatch, and planning interruptible electrical demands in order to secure the specified revenue targets for SDNO by means of the risk-averse strategy. A bi-level stochastic optimization problem based on information gap decision theory (IGDT) is considered to preserve the SDNO from the risks of information gap between the predicted and actual uncertainty variables. The bi-level stochastic optimization problem is applied to a single-level problem obtained by Karush–Kuhn–Tucker method. As uncertainty variables compete to expand their enveloped-bounds, the enhanced ε ε -constraint method is employed to address the multifaceted IGDT-based stochastic optimization problem proposed in the study. Finally, the efficiency and efficacy of the proposed model are evaluated on an IEEE 33-bus SDN.},
  archive      = {J_ASOC},
  author       = {Masoud Khajehvand and Ahmad Fakharian and Mostafa Sedighizadeh},
  doi          = {10.1016/j.asoc.2021.107395},
  journal      = {Applied Soft Computing},
  pages        = {107395},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A risk-averse decision based on IGDT/stochastic approach for smart distribution network operation under extreme uncertainties},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective feature selection method using newton’s
law based PSO with GWO. <em>ASOC</em>, <em>107</em>, 107394. (<a
href="https://doi.org/10.1016/j.asoc.2021.107394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high dimensional data , the Feature Selection (FS) approach plays an important role in overcoming accuracy, time complexity, and space complexity. This paper proposes a binary version of the hybrid two-phase multi-objective FS approach, based on Particle Swarm Optimization (PSO) and Gray Wolf Optimization (GWO). The first objective is to minimize the classification error rate, and the second objective is to reduce the number of selected features. In the first phase, the proposed approach performs the global search , and in the second phase, it goes for the local search . In the case of global search , i.e., for the exploration phase, the property of PSO is used. In the case of local search , i.e., for the exploitation phase, the proposed method uses the modified version of PSO and GWO. The particles are learning not only from the positions but also from the mass and acceleration, based upon Newton’s second law of motion. In some cases, while in the iterations for updation of particles, a stall arises. Due to which the particles are not updating their positions. To avoid the stall in the iterations, this paper introduces a new term, i.e., population factor, which can omit the stall in the iterations if it arises. The prominent features which are selected from the proposed approach are tested on five well-known classification algorithms . The performance of the proposed approach is investigated on eight high-dimensional gene expression data . Experimental results comparison shows that the proposed approach for FS performs efficiently and effectively than other metaheuristics , statistical, and multi-objective FS methods.},
  archive      = {J_ASOC},
  author       = {Pradip Dhal and Chandrashekhar Azad},
  doi          = {10.1016/j.asoc.2021.107394},
  journal      = {Applied Soft Computing},
  pages        = {107394},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective feature selection method using newton’s law based PSO with GWO},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-SEAN: A cross-stitch semi-supervised neural attention
model for COVID-19 fake news detection. <em>ASOC</em>, <em>107</em>,
107393. (<a href="https://doi.org/10.1016/j.asoc.2021.107393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the COVID-19 pandemic sweeps across the world, it has been accompanied by a tsunami of fake news and misinformation on social media. At the time when reliable information is vital for public health and safety, COVID-19 related fake news has been spreading even faster than the facts. During times such as the COVID-19 pandemic, fake news can not only cause intellectual confusion but can also place people’s lives at risk. This calls for an immediate need to contain the spread of such misinformation on social media. We introduce CTF , a large-scale COVID-19 Twitter dataset with labelled genuine and fake tweets. Additionally, we propose Cross-SEAN , a cross-stitch based semi-supervised end-to-end neural attention model which leverages the large amount of unlabelled data . Cross-SEAN partially generalises to emerging fake news as it learns from relevant external knowledge. We compare Cross-SEAN with seven state-of-the-art fake news detection methods. We observe that it achieves 0.95 F1 Score on CTF , outperforming the best baseline by 9\%. We also develop Chrome-SEAN , a Cross-SEAN based chrome extension for real-time detection of fake tweets.},
  archive      = {J_ASOC},
  author       = {William Scott Paka and Rachit Bansal and Abhay Kaushik and Shubhashis Sengupta and Tanmoy Chakraborty},
  doi          = {10.1016/j.asoc.2021.107393},
  journal      = {Applied Soft Computing},
  pages        = {107393},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-SEAN: A cross-stitch semi-supervised neural attention model for COVID-19 fake news detection},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective drilling trajectory optimization using
decomposition method with minimum fuzzy entropy-based comprehensive
evaluation. <em>ASOC</em>, <em>107</em>, 107392. (<a
href="https://doi.org/10.1016/j.asoc.2021.107392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the optimization problem of drilling trajectory design, which plays a vital role in ensuring the safety and enhancing the efficiency of an industrial drilling process. Due to complex geological environment and limited capacity of equipment, the problem to be addressed features multi-objective and multi-constraint, and consists of two challenges: (1) how to formulate a proper optimization scheme and efficiently solve it for a group of solutions; and (2) how to pick out a desired result from the obtained Pareto solutions according to certain requirements. In this paper, to meet drilling practice, three objective functions are introduced regarding trajectory length , well-profile energy, and target error, respectively. Constraints are the range of decision parameters, non-negative constraints and bound of the target area. As a result, a comprehensive optimization model for the design of drilling trajectory is established. A novel optimization algorithm is devised to deal with the contradictory objectives and multiple nonlinear constraints, which combines an adaptive penalty function with multi-objective evolutionary algorithm based on decomposition. A fuzzy-entropy-based evaluation approach is further employed to determine a satisfactory solution from the group of obtained ones. A case study illustrates that (1) our optimization solution is indeed beneficial to the optimization of drilling trajectory; and (2) the optimization algorithm and the decision method therein outperform some existing ones, which shows both practical and theoretical significance.},
  archive      = {J_ASOC},
  author       = {Wendi Huang and Min Wu and Luefeng Chen and Xin Chen and Weihua Cao},
  doi          = {10.1016/j.asoc.2021.107392},
  journal      = {Applied Soft Computing},
  pages        = {107392},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective drilling trajectory optimization using decomposition method with minimum fuzzy entropy-based comprehensive evaluation},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A machine learning approach combining expert knowledge with
genetic algorithms in feature selection for credit risk assessment.
<em>ASOC</em>, <em>107</em>, 107391. (<a
href="https://doi.org/10.1016/j.asoc.2021.107391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most credit scoring algorithms are designed with the assumption to be executed in an environment characterized by an automatic processing of credit applications, without considering the input of expert opinions. Since in credit scoring applications expert opinions have been proved very helpful, in this work, we propose a combination strategy of integrating soft computing methods with expert knowledge. The ability of interpretation of the predictive power of each feature in the credit dataset is strengthened by the engagement of experts in the credit scoring process, and the proposed wrapper-based feature selection approach which explores how the features contributing most towards the classification of borrowers. In particular, an unsupervised machine learning algorithm allows experts to create one or more clustering scenarios regarding the features by defining a desired number of clusters per scenario. For each clustering scenario, the Analytic Hierarchy Process is applied for helping experts to make preferences for features of each cluster, set pair-wise constraints for features of equal importance, and evaluate their subjective judgments in terms of consistency. Then, expert opinions are taken into consideration for solving a credit scoring problem in the form of an optimization problem subject to constraints via soft computing methods based on supervised machine learning and evolutionary optimization algorithms . Testing instances on standard credit dataset are established to verify the effectiveness of the proposed methodology.},
  archive      = {J_ASOC},
  author       = {Pantelis Z. Lappas and Athanasios N. Yannacopoulos},
  doi          = {10.1016/j.asoc.2021.107391},
  journal      = {Applied Soft Computing},
  pages        = {107391},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A machine learning approach combining expert knowledge with genetic algorithms in feature selection for credit risk assessment},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Population management in metaheuristic algorithms: Could
less be more? <em>ASOC</em>, <em>107</em>, 107389. (<a
href="https://doi.org/10.1016/j.asoc.2021.107389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new parameter for population management is implemented in the Differential Evolution algorithm . The new scheme integrates a set of operators that analyze the exploration and exploitation effects during its operation. With these operators, the new method obtains important knowledge about its population diversity during its evolution. Under such conditions, the proposed method can reduce its population when the diversity is too low in order to reduce its computational cost and improve its search capacities simultaneously. To test the new additions, the proposed algorithm has been tested in a set of 29 complex functions and two interplanetary trajectory design problems. The outcome of the tests demonstrates a greatly improved performance when compared to the original Differential Evolution algorithm , a few of its most successful variants, and other algorithms.},
  archive      = {J_ASOC},
  author       = {Bernardo Morales-Castañeda and Daniel Zaldívar and Erik Cuevas and Alma Rodríguez and Mario A. Navarro},
  doi          = {10.1016/j.asoc.2021.107389},
  journal      = {Applied Soft Computing},
  pages        = {107389},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Population management in metaheuristic algorithms: Could less be more?},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization of a deteriorated two-warehouse inventory
problem with all-unit discount and shortages via tournament differential
evolution. <em>ASOC</em>, <em>107</em>, 107388. (<a
href="https://doi.org/10.1016/j.asoc.2021.107388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tournamenting process plays an important role in the selection of best alternative in a knockout system. This concept is applied in the development of a hybrid algorithm based on differential evolution known as tournament differential evolution. In this work, a hybrid tournament differential evolution algorithm is applied on a two-warehouse inventory control problem of deteriorating item with the objective for determining the lot-size, maximum shortage level and cycle length of the concerned system. The corresponding inventory model is formulated with two separate warehouse facilities, partially backlogged shortages and all-unit discount. Here the demand of the product increases with time. The corresponding optimization problem of the developed inventory model is highly non-linear constrained optimization problem which cannot be solved analytically. In this context, to solve the optimization problem, the said hybrid algorithm is developed with the help of six different options of tournamenting procedure and differential evolution. Then, to examine the validity of the proposed model and also to test the performance of the hybrid algorithm, numerical experiments are performed by solving a numerical example and nonparametric statistical test. Finally, to investigate the effects of changes of different parameters, sensitivity analyses are carried out on the best found policy and a fruitful conclusion is drawn along with the future scope of research.},
  archive      = {J_ASOC},
  author       = {Amalesh Kumar Manna and Md Akhtar and Ali Akbar Shaikh and Asoke Kumar Bhunia},
  doi          = {10.1016/j.asoc.2021.107388},
  journal      = {Applied Soft Computing},
  pages        = {107388},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of a deteriorated two-warehouse inventory problem with all-unit discount and shortages via tournament differential evolution},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A synergy of binary differential evolution and binary local
search optimizer to solve multi-objective profit based unit commitment
problem. <em>ASOC</em>, <em>107</em>, 107387. (<a
href="https://doi.org/10.1016/j.asoc.2021.107387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmentally constrained profit based unit commitment problem (PBUCP) is a combinatorial multi-objective optimization problem experienced in deregulated power systems , which optimizes ON/OFF status of generating units to maximize economic benefits with less environmental impacts. The PBUCP has gained less attention in the multi-objective framework. The Kyoto Protocol and various incentive policies have forced generation companies to consider emission minimization as an objective function while solving the PBUCP. In most of the recent studies in the literature, emissions are considered as a constraint, not as an objective function. A few studies have considered the emissions as an objective and suggested the compromised solutions of the multi-objective PBUCP. A synergy of the binary differential evolution (BDE) algorithm and a binary local search optimizer (BLSO) is proposed in the present research to solve the multi-objective PBUCP. A novel BLSO is proposed which deals with binary variables while exploiting the local search and is implemented on the commitment part of the multi-objective PBUCP. The proposed BLSO makes perturbations in the unit status based on the priority of units to refine the optimal solution searched by the BDE algorithm. The efficacy of proposed algorithms has been investigated on the small, medium and large power systems . A comparison with previously known best solutions is performed for the validation of obtained results. Computed results of proposed algorithms are dominant than the already published algorithms applied to solve the multi-objective PBUCP.},
  archive      = {J_ASOC},
  author       = {Jatinder Singh Dhaliwal and J.S. Dhillon},
  doi          = {10.1016/j.asoc.2021.107387},
  journal      = {Applied Soft Computing},
  pages        = {107387},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A synergy of binary differential evolution and binary local search optimizer to solve multi-objective profit based unit commitment problem},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale wavelet network algorithm for pediatric
echocardiographic segmentation via hierarchical feature guided fusion.
<em>ASOC</em>, <em>107</em>, 107386. (<a
href="https://doi.org/10.1016/j.asoc.2021.107386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic segmentation of critical anatomical structures in pediatric echocardiography is the essential steps for early diagnosis and treatment of congenital heart disease. However, current segmentation algorithms rarely extract the information based on effective feature enhancement algorithms. Simultaneously, the algorithms are susceptible to image quality and the lack of detail information. To solve this, we propose a multi-scale wavelet network (MS-Net) combined with a bidirectional feature fusion (BFF-Net) and a wavelet-Unet (W-Unet) for end-to-end pediatric echocardiographic segmentation. In MS-Net, the entire network uses the discrete wavelet transform (DWT) instead of the sampling operation to reduce the impact of image noise while avoiding information loss. The algorithm enhances the edge information by designing the edge attention module (EAM) in the BFF-Net branch and fuses the context and detail information via the bidirectional feature fusion. Secondly, this algorithm uses W-Unet to obtain the detail features of high-resolution images by network depth reduction and propagation method, which supplements the features extracted by the BFF-Net branch. Finally, the hierarchical features of BFF-Net and W-Unet are fused and updated by guided filtering (GF) to obtain the final segmentation prediction. Using 127 pediatric echocardiographic cases of self-selection as the experimental dataset , the left atrium and left ventricle of the echocardiogram were segmented. The Dice coefficient values of 0.9532 and 0.9155, the pixel accuracy of 0.9914, and the specificity values of 0.9975 and 0.9984 were obtained. It was thus verifying its potential and effectiveness as a clinical auxiliary tool.},
  archive      = {J_ASOC},
  author       = {Cheng Zhao and Bei Xia and Weiling Chen and Libao Guo and Jie Du and Tianfu Wang and Baiying Lei},
  doi          = {10.1016/j.asoc.2021.107386},
  journal      = {Applied Soft Computing},
  pages        = {107386},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale wavelet network algorithm for pediatric echocardiographic segmentation via hierarchical feature guided fusion},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization of hybrid renewable energy system in radial
distribution networks considering uncertainty using meta-heuristic crow
search algorithm. <em>ASOC</em>, <em>107</em>, 107384. (<a
href="https://doi.org/10.1016/j.asoc.2021.107384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, probabilistic design and placement of hybrid wind–photovoltaic​ system based on battery storage (PV/WT/Batt) in distribution networks with the aim of minimizing active losses and voltage deviations considering renewable units generation uncertainty and network load demand. The uncertainties are modeled with Monte Carlo simulation (MCS) using probability distribution function (PDF). It is assumed that the total load of hybrid system is supplied via the PV/WT/Batt system without the use of the power grid and the hybrid system extra power is transferred to the distribution network. The decision variables include the optimal installation location and hybrid system components size, i.e. the number of PVs, WTs and batteries, which is determined using an optimization method named improved crow search algorithm (ICSA) using behavior of crows searching for food hidden by them and the pursuit of other crows. In the ICSA, performance of traditional CSA is improved based on decreasing inertia weight method to increase the exploration capability. The proposed method is simulated on IEEE 33 and 69 bus distribution networks. The net present cost of the hybrid system is also assessed for its load supply as well as the improvement of the distribution network characteristics. The deterministic results indicated that design and placement of PV/WT/Batt system in the networks, optimally causes reduction of active losses and voltage deviations significantly. The superiority of the ICSA is compared with traditional CSA, well-known method of particle swarm optimization (PSO) and manta ray foraging optimization (MRFO) that the proposed method is resulted less power losses and less voltage deviations and more net saving the other methods. The results cleared that the designing cost of hybrid system is increased, reduced losses and voltage deviations considering uncertainty. So the probabilistic results indicated that considering uncertainty determines the worst possible events of the network for the operator and creates the possibility of logical decision making to improve the network characteristics.},
  archive      = {J_ASOC},
  author       = {Mohammad javad Aliabadi and Masoud Radmehr},
  doi          = {10.1016/j.asoc.2021.107384},
  journal      = {Applied Soft Computing},
  pages        = {107384},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of hybrid renewable energy system in radial distribution networks considering uncertainty using meta-heuristic crow search algorithm},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive multi-criteria group decision-making with
probabilistic linguistic information for emergency assistance of
COVID-19. <em>ASOC</em>, <em>107</em>, 107383. (<a
href="https://doi.org/10.1016/j.asoc.2021.107383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new method for interactive multi-criteria group decision-making (MCGDM) with probabilistic linguistic information and applies to the emergency assistance area selection of COVID-19 for Wuhan. First, a new possibility degree for PLTSs is defined and a new possibility degree algorithm is devised to rank a series of probabilistic linguistic term sets (PLTSs). Second, some new operational laws of PLTSs based on the Archimedean copulas and co-copulas are defined. A generalized probabilistic linguistic Choquet (GPLC) operator and a generalized probabilistic linguistic hybrid Choquet (GPLHC) operator are developed and their desirable properties are discussed in details. Third, a tri-objective nonlinear programming model is constructed to determine the weights of DMs. This model is transformed into a linear programming model to solve. The fuzzy measures of criterion subsets are derived objectively by establishing a goal programming model. Fourth, using the probabilistic linguistic Gumbel weighted average (PLGWA) operator, the collective normalized decision matrix is obtained by aggregating all individual normalized decision matrices. The overall evaluation values of alternatives are derived by the probabilistic linguistic Gumbel hybrid Choquet (PLGHC) operator. The ranking order of alternatives is generated. Finally, an emergency assistance example is illustrated to validate the proposed method of this paper.},
  archive      = {J_ASOC},
  author       = {Shu-Ping Wan and Wen-Bo Huang Cheng and Jiu-Ying Dong},
  doi          = {10.1016/j.asoc.2021.107383},
  journal      = {Applied Soft Computing},
  pages        = {107383},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interactive multi-criteria group decision-making with probabilistic linguistic information for emergency assistance of COVID-19},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy convolutional deep-learning model to estimate the
operational risk capital using multi-source risk events. <em>ASOC</em>,
<em>107</em>, 107381. (<a
href="https://doi.org/10.1016/j.asoc.2021.107381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operational Risk (OR) is usually caused by losses due to human errors, inadequate or defective internal processes, system failures or external events that affect an organization. According to the Basel II agreement, OR is defined by seven risk events: internal fraud , external fraud , labor relations , clients , damage to fixed assets , technological failures and failures in the execution &amp; administration of processes . However, due to the large amount of qualitative information, the uncertainty and the low frequency at which these risk events are generated in an organization, their modeling is still a technological challenge. This paper takes up this challenge and presents a fuzzy convolutional deep-learning model to estimate, based on the Basel III recommendations, the OR Loss Component (OR-LC) in an organization. The proposed model integrates qualitative information as linguistic random variables , as well as risk events data from different sources using multi-dimensional fuzzy credibility concepts. The results show the stability of the proposed model with respect to the OR-LC estimation from both structural and dimensional point of views, making it an ideal tool for modeling OR from the perspective of: (a) the regulators (Basel Committee on Banking Supervision) by allowing the integration of experts’ criteria into the OR-LC; (b) the insurers by allowing the integration of risk events from different sources; and (c) organizations and financial entities by allowing the a priori evaluation of the OR-LC of new financial products based on technological platforms and electronic channels.},
  archive      = {J_ASOC},
  author       = {Alejandro Pena and Alejandro Patino and Francisco Chiclana and Fabio Caraffini and Mario Gongora and Juan David Gonzalez-Ruiz and Eduardo Duque-Grisales},
  doi          = {10.1016/j.asoc.2021.107381},
  journal      = {Applied Soft Computing},
  pages        = {107381},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy convolutional deep-learning model to estimate the operational risk capital using multi-source risk events},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Health condition monitoring of machines based on long
short-term memory convolutional autoencoder. <em>ASOC</em>,
<em>107</em>, 107379. (<a
href="https://doi.org/10.1016/j.asoc.2021.107379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine health assessment is crucial to prognostics and health management (PHM), which can increase reliability of machines while reducing operation cost. However, the data collected from machines in real-world cases are noised and high-dimensional, so that it is very difficult to detect the early defects of machines. Moreover, it is still a challenging problem to capture sequential information from multi-sensor signals for machine health assessment. This paper proposes a novel autoencoder (AE), called long short-term memory convolutional autoencoder (LSTMCAE), where LSTM and convolutional units are embedded in this specific network for feature learning from sensor signals based on unsupervised-learning. A long short-term memory (LSTM) unit in LSTMCAE is utilized to capture sequential information from multi-sensor time series data . A convolutional and deconvolutional unit is further embedded after the LSTM unit to filter noise and extract features corresponding to health state of machines. Residual learning is employed to reduce the training difficulty and improve the feature learning performance of LSTMCAE. Multivariate Gaussian distribution (MGD) is adopted to generate health index (HI) based on reconstruction errors of LSTMCAE for quantifying machines health state. A contribution analysis-based feature selection method is proposed to select effective features for machinery health assessment. Experimental results on turbofan engines demonstrate the effectiveness of LSTMCAE for machine health assessment. Compared with other unsupervised learning methods, the HIs generated by LSTMCAE show better tendency. LSTMCAE can detect earlier slight degradation than LSTM-AE. Moreover, contribution analysis results indicate that the sensitive variables can be selected for machine health monitoring. Residual learning significantly improves the feature learning performance of LSTMCAE. Thus, LSTMCAE can well quantify the degradation trend of machines. The experimental results on engines show that the proposed method will be an effective tool for machine health assessment.},
  archive      = {J_ASOC},
  author       = {Zhuang Ye and Jianbo Yu},
  doi          = {10.1016/j.asoc.2021.107379},
  journal      = {Applied Soft Computing},
  pages        = {107379},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Health condition monitoring of machines based on long short-term memory convolutional autoencoder},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online ensemble learning algorithm for imbalanced data
stream. <em>ASOC</em>, <em>107</em>, 107378. (<a
href="https://doi.org/10.1016/j.asoc.2021.107378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical applications, due to the inability to collect complete training data sets at one time, the adaptability of the classifier is poor. Online ensemble learning can better solve this problem. However, most of the data streams are imbalanced. Imbalanced data stream will greatly affect the performance of online ensemble learning algorithm . To reduce the impact of imbalanced data stream, this paper proposes a cost sensitive online ensemble learning algorithm for imbalanced data stream. The algorithm uses a variety of equalization methods, mainly including the construction of initial base-classifier, dynamic calculation of misclassification cost, sampling method of samples in data stream and calculation of weight of base-classifier. Those methods can reduce the influence of imbalanced data stream and improve the classification performance under imbalanced data stream. The experimental results show that the performance of the proposed algorithm has the better classification performance for imbalanced data stream. Finally, the algorithm is applied to the network intrusion detection , and the simulation experiment on NSL-KDD data set can reduce the missing alarm rate and the false alarm rate . The experimental results show that the algorithm can improve the detection accuracy, especially the recognition rate of unknown intrusion behavior.},
  archive      = {J_ASOC},
  author       = {Hongle Du and Yan Zhang and Ke Gang and Lin Zhang and Yeh-Cheng Chen},
  doi          = {10.1016/j.asoc.2021.107378},
  journal      = {Applied Soft Computing},
  pages        = {107378},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online ensemble learning algorithm for imbalanced data stream},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safety-enhanced UAV path planning with spherical
vector-based particle swarm optimization. <em>ASOC</em>, <em>107</em>,
107376. (<a href="https://doi.org/10.1016/j.asoc.2021.107376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new algorithm named spherical vector-based particle swarm optimization (SPSO) to deal with the problem of path planning for unmanned aerial vehicles (UAVs) in complicated environments subjected to multiple threats. A cost function is first formulated to convert the path planning into an optimization problem that incorporates requirements and constraints for the feasible and safe operation of the UAV. SPSO is then used to find the optimal path that minimizes the cost function by efficiently searching the configuration space of the UAV via the correspondence between the particle position and the speed, turn angle and climb/dive angle of the UAV. To evaluate the performance of SPSO, eight benchmarking scenarios have been generated from real digital elevation model maps. The results show that the proposed SPSO outperforms not only other particle swarm optimization (PSO) variants including the classic PSO, phase angle-encoded PSO and quantum-behave PSO but also other state-of-the-art metaheuristic optimization algorithms including the genetic algorithm (GA), artificial bee colony (ABC), and differential evolution (DE) in most scenarios. In addition, experiments have been conducted to demonstrate the validity of the generated paths for real UAV operations. Source code of the algorithm can be found at https://github.com/duongpm/SPSO .},
  archive      = {J_ASOC},
  author       = {Manh Duong Phung and Quang Phuc Ha},
  doi          = {10.1016/j.asoc.2021.107376},
  journal      = {Applied Soft Computing},
  pages        = {107376},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Safety-enhanced UAV path planning with spherical vector-based particle swarm optimization},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-atlas classification of autism spectrum disorder with
hinge loss trained deep architectures: ABIDE i results. <em>ASOC</em>,
<em>107</em>, 107375. (<a
href="https://doi.org/10.1016/j.asoc.2021.107375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Autism Brain Imaging Data Exchange (ABIDE) database has emerged as a benchmark cohort for training and testing machine learning models for classifying autism. Considering recent successes using deep learning , increasing the overall classification accuracy on this dataset is a challenging endeavor. In this paper, we propose a multi-input deep neural network model for classifying autism. Our model’s architecture is designed to incorporate neuroimaging data preprocessed using three different reference atlases. For each training example, the proposed deep neural network simultaneously receives connectome data originating from three different parcellation strategies and automatically learns discriminative features from the three input sets. This process results in learned features that are more general and less dependent on a single brain parcellation method. Moreover, the proposed neural network is trained using the hinge loss function , which strongly penalizes misclassifications . We validate our model under cross-validation frameworks, using a set of 1, 038 real participants and an augmented set of 10, 038 samples. We achieve a classification accuracy of 78.07\% on real data and 79.13\% on augmented data, which is about 9\% higher than previously reported results. Our results reinforce the idea that discordance in resting-state network connectivity constitutes a major discriminatory feature between patients with autism and healthy individuals. Our proposed multi-atlas deep neural network classification framework is potentially applicable to additional brain disorders.},
  archive      = {J_ASOC},
  author       = {Thomas Martial Epalle and Yuqing Song and Zhe Liu and Hu Lu},
  doi          = {10.1016/j.asoc.2021.107375},
  journal      = {Applied Soft Computing},
  pages        = {107375},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-atlas classification of autism spectrum disorder with hinge loss trained deep architectures: ABIDE i results},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning and multilingual sentiment analysis on social
media data: An overview. <em>ASOC</em>, <em>107</em>, 107373. (<a
href="https://doi.org/10.1016/j.asoc.2021.107373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twenty-four studies on twenty-three distinct languages and eleven social media illustrate the steady interest in deep learning approaches for multilingual sentiment analysis of social media. We improve over previous reviews with wider coverage from 2017 to 2020 as well as a study focused on the underlying ideas and commonalities behind the different solutions to achieve multilingual sentiment analysis . Interesting findings of our research are (i) the shift of research interest to cross-lingual and code-switching approaches, (ii) the apparent stagnation of the less complex architectures derived from a backbone featuring an embedding layer, a feature extractor based on a single CNN or LSTM and a classifier, (iii) the lack of approaches tackling multilingual aspect-based sentiment analysis through deep learning , and, surprisingly, (iv) the lack of more complex architectures such as the transformers-based, despite results suggest the more difficult tasks requires more elaborated architectures.},
  archive      = {J_ASOC},
  author       = {Marvin M. Agüero-Torales and José I. Abreu Salas and Antonio G. López-Herrera},
  doi          = {10.1016/j.asoc.2021.107373},
  journal      = {Applied Soft Computing},
  pages        = {107373},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning and multilingual sentiment analysis on social media data: An overview},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implementing modified swarm intelligence algorithm based on
slime moulds for path planning and obstacle avoidance problem in mobile
robots. <em>ASOC</em>, <em>107</em>, 107372. (<a
href="https://doi.org/10.1016/j.asoc.2021.107372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning a collision-free path in the least processing time and cost within constraints is a central issue in designing an autonomous mobile robot (AMR). Nature-inspired swarm intelligence (NISI) metaheuristic algorithms are gaining popularity in path planning and obstacle avoidance (PPOA) problem in AMRs. An efficient PPOA algorithm’s objective encompasses the ability to read a workspace map and consequently create the shortest collision-free path for the robot to manoeuvre from start to goal in the least processing time and effort. The authors have implemented a modified NISI metaheuristic approach known as a Slime Mould Optimization Algorithm (SMOA) in this research. SMOA takes inspiration from the oscillatory nature of slime mould when it encounters prey. Its mathematical model utilizes adaptive weights to simulate an optimal path for capturing prey or food. The slime moulds produce positive and negative feedback while propagating towards food with excellent exploratory competency and exploitation propensity. For this, simulation has been carried out on MATLAB 2020a. Additionally, the performance of SMOA has been compared with other NISI metaheuristic approaches such as PSO , FA, SFLA and ABC . The results demonstrate that modified SMOA takes less time and effort to generate an optimal collision-free path as compared to other mentioned approaches.},
  archive      = {J_ASOC},
  author       = {Divya Agarwal and Pushpendra S. Bharti},
  doi          = {10.1016/j.asoc.2021.107372},
  journal      = {Applied Soft Computing},
  pages        = {107372},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Implementing modified swarm intelligence algorithm based on slime moulds for path planning and obstacle avoidance problem in mobile robots},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decision rules-based method for dynamic adjustment of
min–max ordering levels. <em>ASOC</em>, <em>107</em>, 107370. (<a
href="https://doi.org/10.1016/j.asoc.2021.107370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proper inventory control is challenging for supply chain management, especially under fluctuating and unpredictable demand. Complex distribution networks make the problem even more difficult. Despite a lot of inventory control methods have been developed, the Min–Max policy is still one of the most often implemented method in enterprise resource planning systems . The standard approach is however fairly static and ineffective for controlling stocks with high variability in demand. In this paper we propose a decision rules based approach to improve the efficiency of the Min–Max method through dynamic adjustment of the Min–Max ordering levels. The implementation of the improved Min–Max method in a company from the automotive industry resulted in a significant increase of company’s sales volume. Thanks to the use of decision rules the proposed approach is flexible enough to complement other existing methods for inventory management, making it possible to automate inventory control processes, even in a complex and dynamic environment.},
  archive      = {J_ASOC},
  author       = {Radosław Puka and Iwona Skalna and Adam Stawowy and Jerzy Duda and Marek Karkula},
  doi          = {10.1016/j.asoc.2021.107370},
  journal      = {Applied Soft Computing},
  pages        = {107370},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decision rules-based method for dynamic adjustment of Min–Max ordering levels},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The mixed integer robust maximum expert consensus models for
large-scale GDM under uncertainty circumstances. <em>ASOC</em>,
<em>107</em>, 107369. (<a
href="https://doi.org/10.1016/j.asoc.2021.107369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus commonly improves the efficiency of subsequent implementation of the solution in large-scale group decision making (LSGDM). The maximum expert consensus model (MECM) is a significance decision method for consensus reaching processes. In the MECM, the unit adjustment cost of each decision maker and a consensus budget are assumed to be deterministic. Within real-world decision environment, however, it is scarcely possible to obtain an exact numerical value of them. To solve this problem, novel MECMs are constructed on the basis of robust optimization under uncertainty circumstances. Considering the uncertainty of costs, we propose three mixed integer robust maximum expert consensus models (MIR-MECMs) by introducing uncertain box, ellipsoid and polyhedron sets, respectively. Further, we take the aggregation operators into these proposed models. Additionally, to comprehensively analyze the impact of the uncertain parameters in MECM, we develop two models under indeterminacy of the costs and a consensus budget. Finally, an improved genetic algorithm is provided to solve the proposed models in large-scale GDM. Numerical example is used to demonstrate that the solution of the deterministic MECM is too optimistic and the solution of the proposed models are more robust under the uncertainty circumstances. A simulation analysis shows that the ordered weighed averaging (OWA) operator is more stable performance with parameters perturbation .},
  archive      = {J_ASOC},
  author       = {Shaojian Qu and Yuanming Li and Ying Ji},
  doi          = {10.1016/j.asoc.2021.107369},
  journal      = {Applied Soft Computing},
  pages        = {107369},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The mixed integer robust maximum expert consensus models for large-scale GDM under uncertainty circumstances},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pareto-optimal solution for fixed-charge solid
transportation problem under intuitionistic fuzzy environment.
<em>ASOC</em>, <em>107</em>, 107368. (<a
href="https://doi.org/10.1016/j.asoc.2021.107368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intuitionistic fuzzy transportation problem considers both membership as well as non-membership functions. It may be linear or non-linear. In the literature, a lot of work is done in the case of linear membership and non-membership functions, but not in non-linear functions. The presented paper defines the non-membership functions of hyperbolic and exponential functions. The novelty lies in suggesting a unique approach of obtaining pareto-optimal solution of multi-objective fixed-charge solid transportation problem by using intuitionistic fuzzy programming approach with linear, hyperbolic, and exponential membership as well as non-membership functions. A real-life numerical illustration is solved, which exhibits the suitability of the proposed methodology as well as the functionality of all the membership and non-membership functions considered here.},
  archive      = {J_ASOC},
  author       = {Divya Chhibber and Dinesh C.S. Bisht and Pankaj Kumar Srivastava},
  doi          = {10.1016/j.asoc.2021.107368},
  journal      = {Applied Soft Computing},
  pages        = {107368},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pareto-optimal solution for fixed-charge solid transportation problem under intuitionistic fuzzy environment},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis and improvement of GSA’s optimization process.
<em>ASOC</em>, <em>107</em>, 107367. (<a
href="https://doi.org/10.1016/j.asoc.2021.107367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gravitational search algorithm (GSA) is one of the heuristic algorithms proposed in recent years, which is inspired by the law of universal gravitation between masses. However, many practical applications and researches show that when the region affected by global optimum occupies less search space, GSA is prone to fall into local optimum, especially when the optimal value is close to the boundary of the search space and there are sub-optimal solutions in the center of the space. By microscopic analysis of the particle motion process of GSA in the above optimization situation, we find that the Kbest mechanism and the characteristic of central convergence are the two main factors affecting the GSA optimization performance . In this paper, an improved algorithm called Balanced Gravitational Search Algorithm is proposed, in which the balance operator is designed to solve two inherent problems in GSA. Then the proposed method is firstly tested on 10 benchmark functions provided by CEC 2020 compared with the state-of-the-art variant algorithms of the GSA and other typical meta-heuristics. Further, the algorithms are tested and compared on the real-world optimization problems including CEC 2011 real-world optimization problems and the Multi-Layer Neural Network (MLNN) training problem based on wine dataset. The simulation results show that BGSA can significantly improve the optimization performance of GSA and it can be a good choice for solving real-world optimization problems.},
  archive      = {J_ASOC},
  author       = {Fang Su and Chengrui Duan and Ruopeng Wang},
  doi          = {10.1016/j.asoc.2021.107367},
  journal      = {Applied Soft Computing},
  pages        = {107367},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis and improvement of GSA’s optimization process},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A decomposition-based multi-objective optimization approach
for balancing the energy consumption of wireless sensor networks.
<em>ASOC</em>, <em>107</em>, 107365. (<a
href="https://doi.org/10.1016/j.asoc.2021.107365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor networks consist of many sensor nodes with limited resources and computing capability. Thus, managing energy consumption to prolong network lifetime is a critical issue. Several approaches have been proposed to extend the network lifetime, one of which involves deploying relay nodes to transfer data from sensors to the base station . However, the limited number of relay nodes is a challenge that often goes overlooked. This paper examines the problem of optimizing the network lifetime and the number of relay nodes in three-dimensional terrains. A novel algorithm called MOEA/D-LS is proposed with the aim of obtaining a better tradeoff between two objectives. The algorithm is a hybridization between multiobjective evolutionary algorithm based on decomposition, and a special local search to optimize the former’s subproblems . Simulation results on 3D datasets show that the proposed algorithm has a significantly better performance compared with existing algorithms on all measured metrics.},
  archive      = {J_ASOC},
  author       = {Nguyen Thi Tam and Tran Huy Hung and Huynh Thi Thanh Binh and Le Trong Vinh},
  doi          = {10.1016/j.asoc.2021.107365},
  journal      = {Applied Soft Computing},
  pages        = {107365},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decomposition-based multi-objective optimization approach for balancing the energy consumption of wireless sensor networks},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Direct least squares fitting of ellipses segmentation and
prioritized rules classification for curve-shaped chart patterns.
<em>ASOC</em>, <em>107</em>, 107363. (<a
href="https://doi.org/10.1016/j.asoc.2021.107363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In financial markets, appearances of chart patterns in time series are commonly considered as potential signals for imminent change in the direction of price movement. To identify chart patterns, time series data is usually segmented before it can be processed by different classification methods. However, existing segmentation methods are less effective in classifying 16 curve-shaped chart patterns from financial time series. In this paper, we propose three novel segmentation methods for classification of curve-shaped chart patterns based on direct least squares fitting of ellipses. These methods are implemented based on the principles of sliding windows, turning points, and bottom-up piece wise linear approximation. To further enhance the efficiency of classifying chart patterns from real-time streaming data, we propose a novel algorithm called Accelerating Classification with Prioritized Rules (ACPR). Experiments based on real datasets from financial markets reveal that the proposed approaches are effective in classifying curve-shaped patterns from time series. Experiment results reveal that the proposed segmentation methods with ACPR can significantly reduce the total execution time .},
  archive      = {J_ASOC},
  author       = {Iat-Long Lei and Phoey Lee Teh and Yain-Whar Si},
  doi          = {10.1016/j.asoc.2021.107363},
  journal      = {Applied Soft Computing},
  pages        = {107363},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Direct least squares fitting of ellipses segmentation and prioritized rules classification for curve-shaped chart patterns},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applying text similarity algorithm to analyze the triangular
citation behavior of scientists. <em>ASOC</em>, <em>107</em>, 107362.
(<a href="https://doi.org/10.1016/j.asoc.2021.107362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangular citation is a particular citation structure with research value and application significance, and the fundamental mechanism of its formation lies in the indirect citation between Literature A and Literature C. It is necessary to dig deeply into the citation context and citation motivation of indirect citation behavior to understand the internal mechanism of triangular citation. In this paper, the text-similarity algorithm was used to calculate the similarity of citation contents between B → → A and C → → A in 22, 536 triangular citation data, and the followers C with high citation similarity were identified. Secondly, from the perspectives of language, literature type, interdisciplinary citation, and author self-citation, combined literature characteristics and followers’ identification results, the context and motivation of indirect citation behaviors imposed by the followers C were analyzed. The results show that followers C appear more frequently in triangular citations. At the same time, due to the influence of language differences, literature types, discipline differences or author self-citation, follower C will have lazy citation motivation when citing Literature A, partially or out-and-out requote Literature A from the citation content of Literature B, resulting in a high similarity between the citation content of C → → A and B → → A.},
  archive      = {J_ASOC},
  author       = {Yunmei Liu and Min Chen},
  doi          = {10.1016/j.asoc.2021.107362},
  journal      = {Applied Soft Computing},
  pages        = {107362},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Applying text similarity algorithm to analyze the triangular citation behavior of scientists},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep residual computation model for heterogeneous data
learning in smart internet of things. <em>ASOC</em>, <em>107</em>,
107361. (<a href="https://doi.org/10.1016/j.asoc.2021.107361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Internet of Things (smart IoT) have emerged as a transformative computing paradigm recently. This new approach has made great contributions in the area of cyber–physical–social systems by employing various computational intelligence techniques like deep learning , for analyzing data, especially heterogeneous data from sensing and wireless communication . As a representative example of deep learning, deep residual networks have achieved excellent performance for big data feature learning since they can avoid gradient vanishing issues in deep learning models effectively. Unfortunately, they could not learn features for heterogeneous data, especially multi-modal data, in smart IoT. This paper proposes a deep residual computation model by generalizing the deep residual network in the tensor space. Especially, each multi-modal data object is represented as a tensor, while all hidden layers are also represented as tensors. Furthermore, we propose a tensor back-propagation algorithm to train the parameters of the deep residual computation model. Finally, we conduct extensive experiments to evaluate the presented deep residual model by comparing with the existing models such as multi-modal deep learning models, 3D deep residual models, deep computation models, and deep convolutional computation models. Results show that the proposed model produces more accurate classification results than other models for heterogeneous data feature learning in cyber–physical–social systems.},
  archive      = {J_ASOC},
  author       = {Hang Yu and Laurence T. Yang and Xiangchao Fan and Qingchen Zhang},
  doi          = {10.1016/j.asoc.2021.107361},
  journal      = {Applied Soft Computing},
  pages        = {107361},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep residual computation model for heterogeneous data learning in smart internet of things},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting malicious activity in twitter using deep learning
techniques. <em>ASOC</em>, <em>107</em>, 107360. (<a
href="https://doi.org/10.1016/j.asoc.2021.107360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Undoubtedly, social media, such as Facebook and Twitter, constitute a major part of our everyday life due to the incredible possibilities they offer to their users. However, Twitter and generally online social networks (OSNs) are increasingly used by automated accounts, widely known as bots, due to their immense popularity across a wide range of user categories. Their main purpose is the dissemination of fake news, the promotion of specific ideas and products, the manipulation of the stock market and even the diffusion of sexually explicit material. Therefore, the early detection of bots in social media is quite essential. In this paper, two methods are introduced targeting this that are mainly based on Natural Language Processing (NLP) to distinguish legitimate users from bots. In the first method, a feature extraction approach is proposed for identifying accounts posting automated messages. After applying feature selection techniques and dealing with imbalanced datasets, the subset of features selected is fed in machine learning algorithms . In the second method, a deep learning architecture is proposed to identify whether tweets have been posted by real users or generated by bots. To the best of the authors’ knowledge, there is no prior work on using an attention mechanism for identifying bots. The introduced approaches have been evaluated over a series of experiments using two large real Twitter datasets and demonstrate valuable advantages over other existing techniques targeting the identification of malicious users in social media.},
  archive      = {J_ASOC},
  author       = {Loukas Ilias and Ioanna Roussaki},
  doi          = {10.1016/j.asoc.2021.107360},
  journal      = {Applied Soft Computing},
  pages        = {107360},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detecting malicious activity in twitter using deep learning techniques},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attribute reduction methods in fuzzy rough set theory: An
overview, comparative experiments, and new directions. <em>ASOC</em>,
<em>107</em>, 107353. (<a
href="https://doi.org/10.1016/j.asoc.2021.107353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy rough set theory is a powerful tool to deal with uncertainty information, which has been successfully applied to the fields of attribute reduction , rule extraction , classification tree induction, etc. In order to comprehensively investigate attribute reduction methods in fuzzy rough set theory , this paper first briefly reviews the related concepts of fuzzy rough set theory. Then, all methods are summarized through six different aspects including data sources, preprocessing methods, fuzzy similarity metrics, fuzzy operations, reduction rules, and evaluation methods. Among them, reduction rules are reviewed in three categories, i.e., fuzzy dependency-based, fuzzy uncertainty measure-based, and fuzzy discernibility matrix-based. These three types of reduction rules are compared and analyzed through experiments. The experimental results clarify that these three reduction rules can retain fewer attributes and improve or maintain the classification accuracy of a classifier. Moreover, the statistical hypothesis test is conducted to evaluate the statistical difference of these methods. The results show that these algorithms are statistically significantly different. Finally, some new research directions are discussed.},
  archive      = {J_ASOC},
  author       = {Zhong Yuan and Hongmei Chen and Peng Xie and Pengfei Zhang and Jia Liu and Tianrui Li},
  doi          = {10.1016/j.asoc.2021.107353},
  journal      = {Applied Soft Computing},
  pages        = {107353},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attribute reduction methods in fuzzy rough set theory: An overview, comparative experiments, and new directions},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual VAEGAN: A generative model for generalized zero-shot
learning. <em>ASOC</em>, <em>107</em>, 107352. (<a
href="https://doi.org/10.1016/j.asoc.2021.107352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot learning (GZSL) aims to recognize samples from all classes based on training samples of seen classes by bridging the gap between the seen and unseen classes through the semantic descriptions (attributes). Recently, generative-based methods have been used to convert the GZSL task into a supervised learning problem by generating visual features for unseen classes. In this paper, we propose a dual framework based on variational auto-encoder (VAE) and generative adversarial network (GAN), known as dual VAEGAN, to produce more clear visual features than VAE and alleviate the model collapse problem of GAN. To avoid generating unconstraint visual features, the generated visual features are forced to map back into their respective common space using a cycle consistency loss. Meanwhile, the cycle-consistency loss promotes the diversity and preserves the semantic consistency of the generated visual features. The experimental results on the six standard datasets indicate that dual VAEGAN can produce promising results as compared with other methods reported in the literature.},
  archive      = {J_ASOC},
  author       = {Yuxuan Luo and Xizhao Wang and Farhad Pourpanah},
  doi          = {10.1016/j.asoc.2021.107352},
  journal      = {Applied Soft Computing},
  pages        = {107352},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual VAEGAN: A generative model for generalized zero-shot learning},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward more efficient heuristic construction of boolean
functions. <em>ASOC</em>, <em>107</em>, 107327. (<a
href="https://doi.org/10.1016/j.asoc.2021.107327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boolean functions have numerous applications in domains as diverse as coding theory, cryptography, and telecommunications. Heuristics play an important role in the construction of Boolean functions with the desired properties for a specific purpose. However, there are only sparse results trying to understand the problem’s difficulty. With this work, we aim to address this issue. We conduct a fitness landscape analysis based on Local Optima Networks (LONs) and investigate the influence of different optimization criteria and variation operators. We observe that the naive fitness formulation results in the largest networks of local optima with disconnected components. Also, the combination of variation operators can both increase or decrease the network size. Most importantly, we observe correlations of local optima’s fitness, their degrees of interconnection, and the sizes of the respective basins of attraction . This can be exploited to restart algorithms dynamically and influence the degree of perturbation of the current best solution when restarting.},
  archive      = {J_ASOC},
  author       = {Domagoj Jakobovic and Stjepan Picek and Marcella S.R. Martins and Markus Wagner},
  doi          = {10.1016/j.asoc.2021.107327},
  journal      = {Applied Soft Computing},
  pages        = {107327},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Toward more efficient heuristic construction of boolean functions},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble of classification models with weighted functional
link network. <em>ASOC</em>, <em>107</em>, 107322. (<a
href="https://doi.org/10.1016/j.asoc.2021.107322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble classifiers with random vector functional link network have shown improved performance in classification problems. In this paper, we propose two approaches to solve the classification problems. In the first approach, the original input space’s data points are mapped explicitly into a randomized feature space via neural network wherein the weights of the hidden layer are generated randomly. After feature projection, classification models twin bounded support vector machines (SVM), least squares twin SVM, twin k k -class SVM, least squares twin k k -class SVM and robust energy based least squares twin SVM are trained on the extended features (original features and randomized features). In the second approach, twin bounded support vector machines (SVM), least squares twin SVM, twin k k -class SVM, least squares twin k k -class SVM and robust energy based least squares twin SVM models are used to generate the weights of the hidden layer architecture and the weights of output layer are optimized via closed form solution . The performance of both the proposed architectures is evaluated on 33 datasets — including datasets from the UCI repository and fisheries data (not in UCI). Both the experimental results and statistical tests conducted demonstrate that the proposed approaches perform significantly better than the other baseline models . We also analyze the effect of the number of enhanced features on the performance of the given models.},
  archive      = {J_ASOC},
  author       = {M. Tanveer and M.A. Ganaie and P.N. Suganthan},
  doi          = {10.1016/j.asoc.2021.107322},
  journal      = {Applied Soft Computing},
  pages        = {107322},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble of classification models with weighted functional link network},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stock trading rule discovery with double deep q-network.
<em>ASOC</em>, <em>107</em>, 107320. (<a
href="https://doi.org/10.1016/j.asoc.2021.107320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock market serves as an important indicator of today’s economy. Predicting the price fluctuation of stocks and acquiring the maximum gains has been the main concern of investors. In recent years, deep learning models are widely applied to stock market prediction and have achieved good performances. However, the majority of these deep learning based models belong to supervised learning methods and are not capable of dealing with long-term targets. Therefore, in this paper we proposed a deep reinforcement learning based stock market trading model, which is suitable for predicting stock price fluctuation and stock transactions. We carefully devise the reward function and deep learning based policy network, which enables the model to capture the hidden dependencies and latent dynamics in the stock data. In order to evaluate the superiority of the proposed model, stock price trend forecasting and transaction is conducted on randomly selected stocks and stock market indices. Experiment results demonstrate that our model outperforms baseline methods on several indicators.},
  archive      = {J_ASOC},
  author       = {Yong Shi and Wei Li and Luyao Zhu and Kun Guo and Erik Cambria},
  doi          = {10.1016/j.asoc.2021.107320},
  journal      = {Applied Soft Computing},
  pages        = {107320},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stock trading rule discovery with double deep Q-network},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient cluster head election based on optimized
genetic algorithm for movable sinks in IoT enabled HWSNs. <em>ASOC</em>,
<em>107</em>, 107318. (<a
href="https://doi.org/10.1016/j.asoc.2021.107318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most crucial design constraint on the Internet of Things (IoT) enabled wireless sensor networks (WSNs) is energy dissipation . The inefficient data collection by the resource constraint sensors becomes a major roadblock in energy preservation to achieve network longevity. The energy of nodes has to be utilized in an efficient manner which helps in increasing the longevity of WSNs. Clustering is a technique that can utilize the energy of the sensors efficiently by maintaining load balancing among the sensors for increasing the lifetime and scalability of the networks. In this paper, the energy consumption of the network is improved by considering the Genetic Algorithm evolutionary computing technique. The proposed OptiGACHS protocol describes the improved cluster head (CH) selection procedure by incorporating criteria of distance, density, energy, and heterogeneous node’s capability for developing fitness function. The OptiGACHS protocol operates with single, multiple static, and multiple movable sinks to have an impartial comparative examination. Multiple movable sinks are proposed to shorten transmission distance between the sink and CH and also pact with the hot-spot problem. A deployment strategy for nodes is also discussed for energy and distance optimization during network operation. It is observed from simulations that the proposed OptiGACHS protocol outperforms existing protocols.},
  archive      = {J_ASOC},
  author       = {Aridaman Singh Nandan and Samayveer Singh and Lalit K. Awasthi},
  doi          = {10.1016/j.asoc.2021.107318},
  journal      = {Applied Soft Computing},
  pages        = {107318},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient cluster head election based on optimized genetic algorithm for movable sinks in IoT enabled HWSNs},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Class-specific early exit design methodology for
convolutional neural networks. <em>ASOC</em>, <em>107</em>, 107316. (<a
href="https://doi.org/10.1016/j.asoc.2021.107316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Network-based (CNN) inference is a demanding computational task where a long sequence of operations is applied to an input as dictated by the network topology . Optimisations by data quantisation, data reuse, network pruning, and dedicated hardware architectures have a strong impact on reducing both energy consumption and hardware resource requirements, and on improving inference latency. Implementing new applications from established models available from both academic and industrial worlds is common nowadays. Further optimisations by preserving model architecture have been proposed via early exiting approaches, where additional exit points are included in order to evaluate classifications of samples that produce feature maps with sufficient evidence to be classified before reaching the final model exit. This paper proposes a methodology for designing early-exit networks from a given baseline model aiming to improve the average latency for a targeted subset class constrained by the original accuracy for all classes. Results demonstrate average time saving in the order of 2 . 09 × 2.09× to 8 . 79 × 8.79× for dataset CIFAR10 and 15 . 00 × 15.00× to 20 . 71 × 20.71× for CIFAR100 for baseline models ResNet-21, ResNet-110, Inceptionv3-159, and DenseNet-121.},
  archive      = {J_ASOC},
  author       = {Vanderlei Bonato and Christos-Savvas Bouganis},
  doi          = {10.1016/j.asoc.2021.107316},
  journal      = {Applied Soft Computing},
  pages        = {107316},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Class-specific early exit design methodology for convolutional neural networks},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-strategy gravitational search algorithm for
constrained global optimization in coordinative operation of multiple
hydropower reservoirs and solar photovoltaic power plants.
<em>ASOC</em>, <em>107</em>, 107315. (<a
href="https://doi.org/10.1016/j.asoc.2021.107315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the solar photovoltaic power, a promising renewable energy, is witnessing a rapid development period. However, it is often difficult to perfectly capture the generation of solar photovoltaic plants because of various factors (like weather condition, solar radiation and human activities), increasing the operational risk and cost of power system . Hybrid energy system proves to be an effective measure to address this problem. Motivated by this practical necessity, this paper develops a novel hybrid gravitational search algorithm to solve the coordinative operation model of multiple hydropower reservoirs and solar photovoltaic power plants. In the proposed method, the gravitational search algorithm is set as the unified framework; the neighborhood search strategy is used to improve the convergence rate by considering the social information and individual experience; the adaptive mutation strategy is used to improve the population diversity by elite conservation and mutation operator ; the modified elastic-ball strategy and constraint handling technique are used to enhance the solution feasibility. The simulation results of numerical functions demonstrate the superiority of the developed method in convergence rate and global search ability. The hydro–solar operation results in different cases show that compared with the traditional methods, the proposed method can yield high-quality scheduling schemes to alleviate the peak shaving pressure of power system . Thus, the novelty of this paper is to provide an effective HGSA method for solving the complex engineering optimization problem .},
  archive      = {J_ASOC},
  author       = {Wen-jing Niu and Zhong-kai Feng and Shuai Liu},
  doi          = {10.1016/j.asoc.2021.107315},
  journal      = {Applied Soft Computing},
  pages        = {107315},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-strategy gravitational search algorithm for constrained global optimization in coordinative operation of multiple hydropower reservoirs and solar photovoltaic power plants},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantile fuzzy varying coefficient regression based on
kernel function. <em>ASOC</em>, <em>107</em>, 107313. (<a
href="https://doi.org/10.1016/j.asoc.2021.107313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy varying coefficient regression model is a generalized version of fuzzy linear regression model. This kind of model is flexible and adaptable than fuzzy linear regression model. In this paper, we introduce a fuzzy varying coefficient regression model based on the quantile loss function and under the kernel function . Based on the presented goodness of fit indices, we show that the proposed approach is robust under the outlier data . Some applications of this approach are studied on some data sets and a simulated data set.},
  archive      = {J_ASOC},
  author       = {Amir Hamzeh Khammar and Mohsen Arefi and Mohammad Ghasem Akbari},
  doi          = {10.1016/j.asoc.2021.107313},
  journal      = {Applied Soft Computing},
  pages        = {107313},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantile fuzzy varying coefficient regression based on kernel function},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustness analysis of classical and fuzzy decision trees
under adversarial evasion attack. <em>ASOC</em>, <em>107</em>, 107311.
(<a href="https://doi.org/10.1016/j.asoc.2021.107311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although decision trees have been widely applied to different security related applications, their security has not been investigated extensively in an adversarial environment. This work aims to study the robustness of classical decision tree (DT) and Fuzzy decision tree (FDT) under evasion attack that manipulate the features in order to mislead the decision of a classifier. To the best of our knowledge, existing attack methods cannot be applied to DT due to non-differentiation of its decision function. This is the first attack model designed for both DT and FDT. Our model quantifies the influence of changing a feature on the decision. The effectiveness of our method is compared with Papernot (PPNT) and Robustness Verification of Tree-based Models (RVTM), which are state-of-the-art attack methods for DT, and the attack methods employing surrogate and Generative Adversarial Network (GAN) methods. The experimental results suggest that the fuzzifying process increases the robustness of DT. Moreover, FDT with more membership functions is more vulnerable since a smaller number of features is usually used. This study fills the gap of examining the security issue of fuzzy systems in an adversarial environment.},
  archive      = {J_ASOC},
  author       = {Patrick P.K. Chan and Juan Zheng and Han Liu and E.C.C. Tsang and Daniel S. Yeung},
  doi          = {10.1016/j.asoc.2021.107311},
  journal      = {Applied Soft Computing},
  pages        = {107311},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robustness analysis of classical and fuzzy decision trees under adversarial evasion attack},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic multi-objective evolutionary algorithm with
objective space prediction strategy. <em>ASOC</em>, <em>107</em>,
107258. (<a href="https://doi.org/10.1016/j.asoc.2021.107258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve dynamic multi-objective optimization problems, a dynamic multi-objective evolutionary algorithm (DMOEA) must be able to deal with the dynamics of the environment, and such modifications can lead to new optimal solutions over time. Various algorithms have been proposed that modify the way a change is handled. Among them, prediction-based methods are promising for solving this kind of problem. They provide guided direction for population evolution through a prediction mechanism that assists the DMOEA to respond quickly to new changes. Based on these strategies, we propose a dynamic non-dominated sorting differential evolution improvement with prediction in the objective space (DOSP-NSDE). The proposal uses the objective space prediction (OSP) strategy for both the static evolutionary process (between changes) and the change reaction mechanism to predict the new optimal front location. Experiments were performed on a real-world problem and four sets of test problems: FDA, dMOP, UDF, and DF. Comparison of DOSP-NSDE with several algorithms in the literature, considering three metrics, is presented, showing that the proposal is competitive with most problems.},
  archive      = {J_ASOC},
  author       = {Elaine Guerrero-Peña and Aluizio F.R. Araújo},
  doi          = {10.1016/j.asoc.2021.107258},
  journal      = {Applied Soft Computing},
  pages        = {107258},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic multi-objective evolutionary algorithm with objective space prediction strategy},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Amniotic fluid segmentation based on pixel classification
using local window information and distance angle pixel. <em>ASOC</em>,
<em>107</em>, 107196. (<a
href="https://doi.org/10.1016/j.asoc.2021.107196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amniotic fluid surrounds and protects the fetus from colliding with one another during the uterus development process. It also protects the umbilical cord from the uterine wall pressure, helps fetus movement, and develops muscles and bones. Selection of the most profound areas of improper and withdrawal points are not straight caliper is very likely that affect the outcome screening. Furthermore, there are similarities in texture and gray level between objects, especially in the boundary area between amniotic fluid and other objects, such as the placenta and uterus, which causes the border area to be less clear. Therefore, this research proposes a novel pixel classification model to separate amniotic fluid from other objects with a limit on the specified window size to solve this issue. In contrast to the most existing semantic segmentation methods or pixel-wise classification, we use the sampling window technique to construct train sets of data to produce pixel-level information more specifically in certain areas. Furthermore, each window extracts pixel information on gray level features and local variance (GLLV), using a novel Distance Angle Pixel (DAP). To evaluate the proposed model performance, we perform an extensive comparison with state-of-art methods by testing it on amniotic fluid ultrasound images. The results showed that the proposed model with a 3 × × 3 window and random forest classifier could achieve the best value using an average Dice similarity coefficient (DSC) of 0.876, Jaccard/IoU of 0.768, and Pixel accuracy of 85.7\%. The proposed model has 0.324 DSC improved from U-Net, 0.046 from gray-level pixel classification, 0.092 from thresholding, 0, 252 from active contour, and 0.19 from rectangle window sampling.},
  archive      = {J_ASOC},
  author       = {Putu Desiana Wulaning Ayu and Sri Hartati and Aina Musdholifah and Detty S. Nurdiati},
  doi          = {10.1016/j.asoc.2021.107196},
  journal      = {Applied Soft Computing},
  pages        = {107196},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Amniotic fluid segmentation based on pixel classification using local window information and distance angle pixel},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to extract robust handcrafted features with a
single observation via evolutionary neurogenesis. <em>ASOC</em>,
<em>106</em>, 107424. (<a
href="https://doi.org/10.1016/j.asoc.2021.107424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in neuroscience demonstrate that neurogenesis in the human brain results in the born of new neurons, which evolve and replace mature neurons over time. This procedure causes a gradual reduction in the number of neurons, resulting in the human brain’s fast learning and thinking abilities. This paper models brain’s neurogenesis procedure by combining evolutionary algorithms with the Convolutional Neural Network (CNN) framework. This paper shows the promising effect of evolutionary neurogenesis by analyzing its performance for solving the challenging problem of handcrafted feature extraction, which is the primary requirement of all intelligent machines. The proposed approach benefits from the knowledge of a pre-trained CNN that contains mature neurons to evolve a newborn convolutional neuron, via Particle Swarm Optimization (PSO), to detect corners robustly. The proposed approach requires only a single training data to train a robust interest point detection model, and can be trained in about 20 min on CPU, which is significantly faster than other learning-based approaches. Besides, the results demonstrate that the proposed corner detection module outperforms existing techniques, in terms of robustness in various conditions, for approximately 20 percent. The proposed learning strategy can be generalized to solve other problems as well.},
  archive      = {J_ASOC},
  author       = {Mahdi Abolfazli Esfahani and Han Wang and Benyamin Bashari and Keyu Wu and Shenghai Yuan},
  doi          = {10.1016/j.asoc.2021.107424},
  journal      = {Applied Soft Computing},
  pages        = {107424},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning to extract robust handcrafted features with a single observation via evolutionary neurogenesis},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy cognitive networks with functional weights for time
series and pattern recognition applications. <em>ASOC</em>,
<em>106</em>, 107415. (<a
href="https://doi.org/10.1016/j.asoc.2021.107415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decades, a number of remarkable pattern recognition algorithms have been proposed, as a result of the continuous raising of pattern classification in one of the major application areas of artificial intelligence . Many real world problems require the use of efficient classification models pointing out the need for continuous research and study of new techniques. Such a proposal is to use Fuzzy Cognitive Maps (FCMs) and their extensions to solve distinct classification tasks . The extension named Fuzzy Cognitive Network (FCN) has the clear advantage of guaranteed convergence to equilibrium points, which in turn makes it more suitable for pattern recognition applications. However, in order to store the broad range of associations using FCN, large fuzzy rule databases have to be built. In this work, the FCNs with functional weights are introduced and their use in pattern recognition and time series prediction is proposed. The new scheme keeps the nice convergence properties of FCN but is alleviated from the memory and computational requirements of using large fuzzy rule databases, as well as from the inevitable human intervention. The training of the classifier is performed by using a combination of a gradient descent like procedure which uses either a linear or a bilinear parametric model of the network and a least squares method for the estimation of the functional weights. The efficiency and reliability of the proposed classifier is supported by its high overall performance on a set of publicly available time series and pattern recognition datasets outperforming other well-known machine learning models, as well as the most efficient FCM based classifiers.},
  archive      = {J_ASOC},
  author       = {Georgios D. Karatzinis and Yiannis S. Boutalis},
  doi          = {10.1016/j.asoc.2021.107415},
  journal      = {Applied Soft Computing},
  pages        = {107415},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy cognitive networks with functional weights for time series and pattern recognition applications},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SGOP: Surrogate-assisted global optimization using a
pareto-based sampling strategy. <em>ASOC</em>, <em>106</em>, 107380. (<a
href="https://doi.org/10.1016/j.asoc.2021.107380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new global optimization algorithm SGOP for computationally intensive black-box problems. Considering that multiple surrogates concurrently used in an optimization process can have more robust performance in most cases, a Pareto-based multi-point sampling strategy is presented to improve iterative efficiency. Ideally, a group of samples having best predictive values on all the surrogates and meanwhile keeping better space-filling feature are most appropriate to be selected in each cycle. Therefore, a four-objective optimization formula is presented, where Kriging, radial basis function , quadratic response surface and a sampling density function are defined as objective functions, respectively. The non-dominated sorting strategy is used to capture the Pareto solutions of the multi-objective problem and the new promising samples are adaptively chosen from their Pareto solutions set to drive the optimization cycle. Moreover, a dynamic monitor is presented to check the premature convergence. Once the trigger is activated, the search will focus on unexplored area. SGOP can not only build a reasonable balance between global exploration and local exploitation, but also has remarkable advantages in sampling efficiency. Finally, the new algorithm is tested on 17 benchmark cases and compared with several existing algorithms. The results show SGOP’s superior performance and strong robustness. Besides, SGOP is used for the shape optimization of a blended-wing-body underwater glider (BWBUG), and the lift–drag-ratio gets remarkable improvement.},
  archive      = {J_ASOC},
  author       = {Huachao Dong and Peng Wang and Weixi Chen and Baowei Song},
  doi          = {10.1016/j.asoc.2021.107380},
  journal      = {Applied Soft Computing},
  pages        = {107380},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SGOP: Surrogate-assisted global optimization using a pareto-based sampling strategy},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlational graph attention-based long short-term memory
network for multivariate time series prediction. <em>ASOC</em>,
<em>106</em>, 107377. (<a
href="https://doi.org/10.1016/j.asoc.2021.107377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-variate time series prediction models use the historical information of multiple exogenous series to predict the future values of the target series. At present, attention-based deep networks can obtain the spatial correlations between target series and multiple exogenous series, but it is difficult to capture temporal correlations across multiple time steps, which play a role in improving prediction accuracy. To that end, we propose a correlational graph attention-based Long Short-Term Memory network (CGA-LSTM), a nested network that nests the correlational attention mechanism in the graph attention mechanism to strengthen the spatio-temporal correlations. We construct the time series as a graph structure, where nodes represent time steps in exogenous series. To obtain sufficient expressive power , we propose a nonlinear transformation , correlational attention-based LSTM, instead of the original linear transformation to transform the exogenous series into higher-level features. The original linear transformations cannot obtain spatial correlations . The correlational attention mechanism can adaptively select the relevant exogenous series to obtain the spatial correlations. Then calculating the weight coefficients between the node and its neighbors to capture the temporal correlations. The performance of the proposed algorithm was tested on 4 datasets and compared with state-of-the-art methods. The experimental results show that our model is effective, can provide higher prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Shuang Han and Hongbin Dong and Xuyang Teng and Xiaohui Li and Xiaowei Wang},
  doi          = {10.1016/j.asoc.2021.107377},
  journal      = {Applied Soft Computing},
  pages        = {107377},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Correlational graph attention-based long short-term memory network for multivariate time series prediction},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FAF-DRVFL: Fuzzy activation function based deep random
vector functional links network for early diagnosis of alzheimer
disease. <em>ASOC</em>, <em>106</em>, 107371. (<a
href="https://doi.org/10.1016/j.asoc.2021.107371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a degenerative neural condition marked by gradual memory loss and cognitive impairment. It is irreversible in nature and leads to progressive cerebral cortex atrophy. Therefore, structural Magnetic Resonance Imaging (sMRI) is an important tool that can be used for early-stage prediction of AD. Currently, deep learning networks are used for the diagnosis of AD, but it suffers from the limitations of gradient descent training of deep networks like local minima, slow learning speed, and overfitting. Also, there is a need to select hyperparameters like learning rate, momentum, number of epochs, and regularization coefficient. This paper proposes a deep non-iterative random vector functional link (RVFL) neural network . First, the MRI images’ features are extracted using transfer learning , and the classification of the extracted features is done using a non-iterative random vector initialized RVFL network. At the hidden layer of the RVFL classifier, the fuzzy activation function (FAF), is used to calculate the hidden layer’s output. The proposed algorithm has been evaluated and compared with the state-of-the-art methods on the ADNI dataset consisting of Cognitive Normal (CN), AD, converter Mild Cognitive Impairment (cMCI) and non-converter Mild Cognitive Impairment (ncMCI) MRI images. The performance achieved for CN vs AD diagnosis includes accuracy (86.67\%), sensitivity (83.33\%), specificity (88.89\%), precision (83.33\%), recall (83.33\%) and F-score(86.07\%) as well as Receiver Operating Characteristics shows that proposed method outperforms over several compared methods.},
  archive      = {J_ASOC},
  author       = {Rahul Sharma and Tripti Goel and M. Tanveer and Shubham Dwivedi and R. Murugan},
  doi          = {10.1016/j.asoc.2021.107371},
  journal      = {Applied Soft Computing},
  pages        = {107371},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FAF-DRVFL: Fuzzy activation function based deep random vector functional links network for early diagnosis of alzheimer disease},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data set quality in machine learning: Consistency measure
based on group decision making. <em>ASOC</em>, <em>106</em>, 107366. (<a
href="https://doi.org/10.1016/j.asoc.2021.107366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance of Machine Learning models heavily depends on the quality of the training dataset. Among others, the quality of training data relies on the consistency of the labels assigned to similar items. Indeed, the labels should be coherently assigned (or collected) by avoiding inconsistencies for increasing the performance of the machine learning model. This study focuses on evaluating training data consistency for machine learning algorithms dealing with ranking problems, i.e., the Learning to Rank methods (LTR). This work defines a training data consistency measure based on the consensus value introduced in Group Decision Making. It investigates the statistical relationship between the proposed consistency measure and the performance of a deep neural network implementing an LTR method. This measure could drive data filtering at the training stage and guide model update decisions. Experimentation reveals a strong correlation between the proposed consistency measure and the performance of the model.},
  archive      = {J_ASOC},
  author       = {Giuseppe Fenza and Mariacristina Gallo and Vincenzo Loia and Francesco Orciuoli and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2021.107366},
  journal      = {Applied Soft Computing},
  pages        = {107366},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data set quality in machine learning: Consistency measure based on group decision making},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blood supply chain operation considering lifetime and
transshipment under uncertain environment. <em>ASOC</em>, <em>106</em>,
107364. (<a href="https://doi.org/10.1016/j.asoc.2021.107364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the characteristics of blood inventory control problem under the condition of blood shortage, the dynamic decision-making problem of blood supply chain is investigated in this paper. Firstly, based on the recursive equation , the state transition equations of two categories of blood demand under two inventory issue strategies (FIFO and LIFO) are given. The mathematical expressions of key indexes such as blood outdating and blood shortage are obtained. A blood collection decision-making method based on EWA (Estimated Withdrawal &amp; Aging) strategy is proposed. Then, an optimal model of blood transshipment problem is established with the goal of the shortest transshipment time and the maximum freshness of the transported blood. In addition, an allocation planning model with multiple priority requirements and fairness concerns is established to achieve the best fairness and the minimum shortage. Besides, a discrete event system simulation (DESS) framework is designed according to the characteristics of the model. Finally, the effectiveness of the decision-making method and EWA inventory strategy are verified by numerical simulation. The results show that safety stock, target stock level and fluctuation range of demand have significant impacts on the control effect of blood inventory.},
  archive      = {J_ASOC},
  author       = {Yufeng Zhou and Tiange Zou and Changshi Liu and Hongxia Yu and Liangyong Chen and Jiafu Su},
  doi          = {10.1016/j.asoc.2021.107364},
  journal      = {Applied Soft Computing},
  pages        = {107364},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Blood supply chain operation considering lifetime and transshipment under uncertain environment},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Dynamic assessment of internet public opinions based on the
probabilistic linguistic bayesian network and prospect theory.
<em>ASOC</em>, <em>106</em>, 107359. (<a
href="https://doi.org/10.1016/j.asoc.2021.107359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolution of an emergency, Internet public opinions usually catalyze escalation and spread of the emergency, and even affect the evolution of public opinions. Therefore, how to effectively manage Internet public opinions has become urgent. As an essential part of the Internet public opinion management, assessing the heat degree of Internet public opinions is necessary. Being problem-oriented, this paper analyzes the development and evolution of Internet public opinions, and identifies the characteristics of the heat degree assessment of Internet public opinions. Taking into account the continuously changing exterior environment, the dynamic nature of Internet public opinions, and the inadequacy and uncertainty of decision-making information, assessing the heat degree of Internet public opinions is regarded as a dynamic multi-attribute decision making problem under the probabilistic linguistic environment. Thus, this paper aims to develop a dynamic decision-making framework to assess the heat degrees of Internet public opinions under the probabilistic linguistic environment. First, the probabilistic linguistic Bayesian network (PLBN) is constructed, in which the nodes denote attributes and related factors, and the hierarchical network structure shows the relationship among attributes. Then, the probability information obtained by PLBN in the form of PLTS is converted into attribute weight information. Moreover, this paper starts from the nature of PLTS , discusses PLTSs in terms of the probability distribution, and then gives the concept of the dominance degree of PLTS, based on which, a dynamic decision-making model based on the idea of prospect theory that considers DMs’ bounded rationality is developed. Finally, five emergency events happened in China are selected as a case to illustrate the proposed approach and some analyses and discussions have been carried out to verify the validity of our approach.},
  archive      = {J_ASOC},
  author       = {Yixin Zhang and Zeshui Xu and Zhinan Hao and Huchang Liao},
  doi          = {10.1016/j.asoc.2021.107359},
  journal      = {Applied Soft Computing},
  pages        = {107359},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic assessment of internet public opinions based on the probabilistic linguistic bayesian network and prospect theory},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Adversarial transfer network with bilinear attention for
the detection of adverse drug reactions from social media.
<em>ASOC</em>, <em>106</em>, 107358. (<a
href="https://doi.org/10.1016/j.asoc.2021.107358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The drug safety issues related to adverse drug reactions (ADRs) are gradually becoming more important to the public. With social media booming, increasingly more patients would like to share their reactions to get support from others. These published posts comprise a valuable resource for ADR identification because of their timeliness. However, available social media datasets are rare. Moreover, the informality of the social media text is also a challenge for ADR identification. PubMed and social media differ greatly in expression and syntax. Introducing the PubMed datasets, which are usually normative and numerous, may be helpful to ADR identification in social media. To this end, we propose an adversarial transfer framework for ADR identification that transfers the auxiliary features from PubMed to social media datasets to improve the generalization of the model and mitigate the noise caused by colloquial expression in social media. Additionally, we add dynamic weight to the loss function to offset the training slants caused by imbalanced training data. We experimentally evaluate the method we proposed on two social media datasets and two PubMed datasets. The results show that our proposed method can improve the performance of ADR identification from social media.},
  archive      = {J_ASOC},
  author       = {Tongxuan Zhang and Hongfei Lin and Yuqi Ren and Zhihao Yang and Jian Wang and Shaowu Zhang and Bo Xu and Xiaodong Duan},
  doi          = {10.1016/j.asoc.2021.107358},
  journal      = {Applied Soft Computing},
  pages        = {107358},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial transfer network with bilinear attention for the detection of adverse drug reactions from social media},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Type-2 intuitionistic fuzzy matrix games based on a new
distance measure: Application to biogas-plant implementation problem.
<em>ASOC</em>, <em>106</em>, 107357. (<a
href="https://doi.org/10.1016/j.asoc.2021.107357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reality, to deal with uncertain information, researchers have introduced several extensions of fuzzy sets. But, in most cases, two-dimensional fuzzy sets (considering elements with their membership degree) are considered. However, it fails to capture all types of incomplete information. Occasionally, the inadequacy of the data depends upon the large span of source or time or both. In that case, two-dimensional fuzzy sets or type-1 fuzzy sets (T1FSs) are not sufficient to portray the situation. Type-2 fuzzy set (T2FS), a three-dimensional extent of the fuzzy set, appears here to bridge this gap. Type-2 intuitionistic fuzzy set (T2IFS) is a novel extension of T2FS. It can express the fuzzy preference of the decision-makers toward their decisions under different parameters. It also considers both the acceptance and non-acceptance of decision-makers. In the present study, we aim to develop matrix games in T2IF environment. To do this, we first define Hamacher aggregation operators in T2IF environment with their properties. We also propose the Minkowski distance of T2IFSs based on the Hausdorff metric. Then a similarity measure of T2IFS is formed. Next, we solve the matrix games by utilizing the proposed distance measure. The proposed methodology has been delineated with the biogas-plant implementation problem to check the applicability and validity.},
  archive      = {J_ASOC},
  author       = {Shuvasree Karmakar and Mijanur Rahaman Seikh and Oscar Castillo},
  doi          = {10.1016/j.asoc.2021.107357},
  journal      = {Applied Soft Computing},
  pages        = {107357},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Type-2 intuitionistic fuzzy matrix games based on a new distance measure: Application to biogas-plant implementation problem},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian neural architecture search using a training-free
performance metric. <em>ASOC</em>, <em>106</em>, 107356. (<a
href="https://doi.org/10.1016/j.asoc.2021.107356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are a powerful approach for time series prediction. However, their performance is strongly affected by their architecture and hyperparameter settings. The architecture optimization of RNNs is a time-consuming task, where the search space is typically a mixture of real, integer and categorical values. To allow for shrinking and expanding the size of the network, the representation of architectures often has a variable length. In this paper, we propose to tackle the architecture optimization problem with a variant of the Bayesian Optimization (BO) algorithm. To reduce the evaluation time of candidate architectures the Mean Absolute Error Random Sampling (MRS), a training-free method to estimate the network performance, is adopted as the objective function for BO. Also, we propose three fixed-length encoding schemes to cope with the variable-length architecture representation. The result is a new perspective on accurate and efficient design of RNNs, that we validate on three problems. Our findings show that (1) the BO algorithm can explore different network architectures using the proposed encoding schemes and successfully designs well-performing architectures, and (2) the optimization time is significantly reduced by using MRS, without compromising the performance as compared to the architectures obtained from the actual training procedure.},
  archive      = {J_ASOC},
  author       = {Andrés Camero and Hao Wang and Enrique Alba and Thomas Bäck},
  doi          = {10.1016/j.asoc.2021.107356},
  journal      = {Applied Soft Computing},
  pages        = {107356},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian neural architecture search using a training-free performance metric},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning classification framework for early
prediction of team-based academic performance. <em>ASOC</em>,
<em>106</em>, 107355. (<a
href="https://doi.org/10.1016/j.asoc.2021.107355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of the performance of teams during the collaboration of their members is considered a difficult task for various reasons. However, an accurate prediction would assist educators and learning experts in the extraction of useful knowledge for designing learning interventions for improving teams’ outcomes. The main objective of the current paper is to explore and propose a Deep Neural Network (DNN) framework for binary classification with two hidden layers, for the early prediction of teams’ performance in the domain of software engineering . The framework was evaluated by using different activation functions (Sigmoid, ReLu, and Tanh) and optimizers (Adagrad and Adadelta). A dataset created from over 30000 entries grouped in 74 teams was used for training and evaluating the framework. Additionally, the SHapley Additive exPlanations (SHAP) approach was used to interpret the framework and extract the most important features having a positive or negative impact on its final prediction. Among other conclusions, it was shown that the prediction accuracy of the framework, when the Adadelta and Adagrad optimizers were used, was found to be 76.73\% and 82.39\%, respectively, while its overall learning performance was 80.76\% and 86.57\%, meaning that it is capable of predicting teams’ performance adequately and accurately.},
  archive      = {J_ASOC},
  author       = {F. Giannakas and C. Troussas and I. Voyiatzis and C. Sgouropoulou},
  doi          = {10.1016/j.asoc.2021.107355},
  journal      = {Applied Soft Computing},
  pages        = {107355},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep learning classification framework for early prediction of team-based academic performance},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy c-means-based isolation forest. <em>ASOC</em>,
<em>106</em>, 107354. (<a
href="https://doi.org/10.1016/j.asoc.2021.107354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding anomalies (outliers) in databases is one of the most important issues in modern data analysis. One of the reasons is the occurrence of this issue in almost every type of database, including numerical, categorical, time, mixed, or graphic data. There are currently many methods often dedicated to specific data analysis. Finally, this topic is extremely interesting per se, as a research problem that intrigues researchers. One of the classic methods of data analysis dedicated to finding the anomalies in the data is Isolation Forest. However, this method, with a few exceptions, has not been modified from the time of its first publication, and, in particular, it has not yet appeared in combination with the typical fuzzy methods used for grouping such as Fuzzy C-Means (FCM) clustering. In this study, we thoroughly analyze this approach, as well as several related ones. We examine the possibilities of this technique and analyze it in detail for characteristics of data (database size, number of attributes, records, their type, etc.). It is worth noting that FCM allows to obtain membership grades of elements forming Isolation Forest nodes to clusters on the basis of which these nodes are built. Hence, at the stage of calculating the anomaly scores, this information is effectively used, in particular to express how much a given element may belong to a group of similar elements, which can be inferred from the characteristics of the cluster in which it lies. In this study, we propose a set of methods enhancing the Isolation Forest on a basis of Fuzzy C-Means. The results of numerical experiments carried using 27 various datasets and reported in this paper lead us to the conclusion that FCM can play a pivotal role in an enhancement of Isolation Forest approach and raises up the values of particular measures of effectiveness of the anomaly detection methods.},
  archive      = {J_ASOC},
  author       = {Paweł Karczmarek and Adam Kiersztyn and Witold Pedrycz and Dariusz Czerwiński},
  doi          = {10.1016/j.asoc.2021.107354},
  journal      = {Applied Soft Computing},
  pages        = {107354},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy C-means-based isolation forest},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on the artificial bee colony algorithm variants for
binary, integer and mixed integer programming problems. <em>ASOC</em>,
<em>106</em>, 107351. (<a
href="https://doi.org/10.1016/j.asoc.2021.107351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the optimization problems encountered in the real world are discrete type which involves decision variables defined in the discrete search space. Binary optimization problems, integer and mixed integer programming problems are of this category, and they require suitable solution representation and search operators to be solved by nature-inspired algorithms. One of the widely-used and well-known nature-inspired algorithms is Artificial Bee Colony (ABC) that has been originally proposed to solve the problems in the continuous domain, and hence, its standard version employs the search operators to exploit the information of the solution vectors encoded in the continuous domain. To be able to cope with the discrete problems, particularly binary, integer and mixed integer programming problems, which are also a group of numeric optimization problems, various encoding types, search operators and selection operators have been integrated into ABC. In this paper, we review the studies proposing new ABC variants to solve discrete numeric optimization problems. To the best of our knowledge, this will be the first comprehensive survey study on this topic. Therefore, we hope that this study would be beneficial to the readers interested in the use of ABC for the binary, integer and mixed integer discrete optimization problems.},
  archive      = {J_ASOC},
  author       = {Bahriye Akay and Dervis Karaboga and Beyza Gorkemli and Ebubekir Kaya},
  doi          = {10.1016/j.asoc.2021.107351},
  journal      = {Applied Soft Computing},
  pages        = {107351},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey on the artificial bee colony algorithm variants for binary, integer and mixed integer programming problems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantitative-integration-based TSK fuzzy classification
through improving the consistency of multi-hierarchical structure.
<em>ASOC</em>, <em>106</em>, 107350. (<a
href="https://doi.org/10.1016/j.asoc.2021.107350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional hierarchical fuzzy classifiers, there exist some main problems. These issues include the output of the previous training layer influencing with the input of the next layers, the inconsistency between the intra-layer and inter-layer, and the improvement of consequent parameters. Based on the above troubles, a novel quantitative-integration-based hierarchical Takagi–Sugeno–Kang (TSK) is proposed to handle the challenges in existing hierarchical fuzzy classifiers. As a novel hierarchical structure, the proposed classifier is built in a stacked manner. Each base building unit consists of an optimized zero-order TSK fuzzy classifier. For good interpretability of each base building unit, the antecedent parameters are solved by random selections of input features, random combinations of fuzzy rules, divisions of fuzzy partitions and generations of cluster centers. In addition, an improved method based on classical ridge regression is proposed to solve the problem of the consistency between intra-layer and inter-layer. In order to enhance the classification performance of the fuzzy classifier, the input of each layer is optimized to effectively resolve the problem that the output of the previous layer affects the input of the next base building unit. Moreover, a method that randomly selects a part of the sample from the original training set is adopted to open the manifold structure of input set. Furthermore, the experimental results indicate that this method can indeed make the QI-TSK-FC fuzzy classifier obtain good or comparative classification performance. To improve the generalization ability of the fuzzy classifier, a method that quantitatively solves the integrated output is considered so that the QI-TSK-FC model can quickly obtain the satisfactory optimal solution. Finally, the classification performance and interpretability of the proposed fuzzy classifier are verified through the MIT-BIH sleep datasets.},
  archive      = {J_ASOC},
  author       = {Ta Zhou and Yi Zhou and Shang Gao},
  doi          = {10.1016/j.asoc.2021.107350},
  journal      = {Applied Soft Computing},
  pages        = {107350},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantitative-integration-based TSK fuzzy classification through improving the consistency of multi-hierarchical structure},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discrete spotted hyena optimizer for solving distributed
job shop scheduling problems. <em>ASOC</em>, <em>106</em>, 107349. (<a
href="https://doi.org/10.1016/j.asoc.2021.107349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling is an important decision-making process for manufacturing companies. Shop scheduling is a subsection of scheduling related to manufacturing shops. The job shop scheduling problem (JSP) aims to minimize the makespan of a product value of the production process. This issue is related to optimizing the sequence of the jobs on appropriate machines. With globalization, manufacturing shops are scattered around the world. The distributed job shop scheduling problem (DJSP) tries to solve the optimizing sequence on these scattered facilities. DJSP is more complex than JSP. DJSP is solved by exact and heuristic solvers. Exact solvers more time-consuming processes for huge problems. In this work, a discrete version of the spotted hyena optimizer (DSHO) is proposed for solving DJSP. A workload-based facility order mechanism and a greedy heuristic approach are combined with the DSHO algorithm. 80 distributed job shop scheduling problems (DJSP) are solved by DSHO. For evaluating the performance of the DSHO, numerical results of the 480 (2 facilities to 7 facilities) large instances that are derived from well-known JSP benchmarks are compared with four different discrete meta-heuristic algorithms. The experimental results are shown that DSHO is a pioneer solver for DJSP.},
  archive      = {J_ASOC},
  author       = {Mehmet Akif Şahman},
  doi          = {10.1016/j.asoc.2021.107349},
  journal      = {Applied Soft Computing},
  pages        = {107349},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete spotted hyena optimizer for solving distributed job shop scheduling problems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid attention-based long short-term memory network for
sarcasm identification. <em>ASOC</em>, <em>106</em>, 107348. (<a
href="https://doi.org/10.1016/j.asoc.2021.107348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis of people’s opinion is used in a lot of business and decision-making scenarios. Although social media is an informal medium in which to express one’s opinions, it is being used in many business and decision-making scenarios now. Social media posts contain a lot of sarcastic statements that affect the automatic extraction of the correct sentiment of the post, as sarcasm can flip the overall polarity of the sentence. Sarcasm is a bitterly cutting form of irony to be unpleasant to somebody or to make fun of them. Therefore, identifying sarcastic statements from the users’ posts has become an important task to extract the actual sentiments from informal statements regarding an event or a person. In this work, we propose a hybrid attention-based Long Short Term Memory (HA-LSTM) network to identify sarcastic statements. This HA-LSTM network is different than the existing LSTM model, as the proposed HA-LSTM network combines 16 different linguistic features in their hidden layers. The proposed HA-LSTM network is validated with three benchmark datasets. The combination of 16 different linguistic features shows an improvement in the performance of the model in comparison with other state-of-the-art models with an improvement of up to 2\% in terms of F 1 F1 -score with three different gold standard datasets.},
  archive      = {J_ASOC},
  author       = {Rajnish Pandey and Abhinav Kumar and Jyoti Prakash Singh and Sudhakar Tripathi},
  doi          = {10.1016/j.asoc.2021.107348},
  journal      = {Applied Soft Computing},
  pages        = {107348},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid attention-based long short-term memory network for sarcasm identification},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble summarization of bio-medical articles integrating
clustering and multi-objective evolutionary algorithms. <em>ASOC</em>,
<em>106</em>, 107347. (<a
href="https://doi.org/10.1016/j.asoc.2021.107347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid exponential growth of scientific literature of bio-medicine, genomics, and other bio-sciences increases difficulties to access useful information timely and efficiently. The medical researchers, physicians and clinicians need to extract the gist of the novel articles effectively to gather knowledge and apply it properly for better treatment of the patients. In the paper, the bio-medical articles available in an open access source, PubMed MEDLINE, are considered for generating their extractive summaries using clustering and multi-objective evolutionary algorithm based ensemble summarization technique. Initially, we have extracted only the medical terms of an article and determined the concepts of all medical terms using Unified Medical Language System (UMLS) to represent the article in terms of a set of concepts. Next, extractive summaries of the article have been generated using various popularly used clustering algorithms and different centrality measures . The generated summaries are referred to as the base summaries of the article. Finally, multi-objective evolutionary algorithm is applied on these base summaries for generating an ensemble summary of the given article. The evolutionary algorithm uses two competent objective functions to measure the fitness value of each chromosome in the population. One objective function is the linear combination of jaccard similarity and Word2Vec similarity between the original article and the ensemble summary generated by the chromosome. The other objective function is defined as the information gain of the chromosome about the article by generating the ensemble summary. The best pareto optimal solution of the final population of the evolutionary algorithm provides the ensemble summary of the given article. The proposed ensemble method is compared with some state-of-art methods to demonstrate that it is both effective and statistically significant.},
  archive      = {J_ASOC},
  author       = {Chirantana Mallick and Asit Kumar Das and Weiping Ding and Janmenjoy Nayak},
  doi          = {10.1016/j.asoc.2021.107347},
  journal      = {Applied Soft Computing},
  pages        = {107347},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble summarization of bio-medical articles integrating clustering and multi-objective evolutionary algorithms},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UTF: Upgrade transfer function for binary meta-heuristic
algorithms. <em>ASOC</em>, <em>106</em>, 107346. (<a
href="https://doi.org/10.1016/j.asoc.2021.107346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, many optimization problems are discrete and very complex to solve. Some of them are in the class of NP-hard problems and their search spaces grow exponentially with the problem size. As a result, an exhaustive search will be impractical using exact algorithms. In the last decades, meta-heuristic algorithms as approximate algorithms have shown superior performance in solving these problems. The majority of these algorithms have been designed for continuous search spaces and are not able to solve binary optimization problems . Therefore, a transfer function is applied to convert the continuous search space to the binary one. The performance of such binary algorithms depends on their ability of exploration, exploitation and transfer function. Several transfer functions have been introduced so far but they have shown poor exploration and exploitation in solving some problems. In this study, a novel adaptive transfer function, based on two linear functions , is proposed to overcome the shortcomings of existing transfer functions. The proposed method called upgrade transfer function (UTF) adapts itself during running the algorithm to switch from exploration to exploitation. This capability also covers disadvantages of metaheuristic algorithms in terms of poor exploration and exploitation. The performance of UTF has been evaluated by three discrete optimization problems: function optimization, feature selection and the 0–1 multi-knapsack problem (MKP). The results of binary particle swarm optimization (BPSO), binary artificial bee colony (BABC) and several improved BPSO and BABC have been compared with those of UTF–BPSO and UTF–BABC using function optimization problems. Also, the efficiency of UTF–BPSO and UTF–BABC and some binary meta-heuristic algorithms such as binary salp swarm algorithm (BSSA) and binary gray wolf optimization (bGWO), binary dragon algorithm (BDA), binary multi-neighborhood artificial bee colony (BMNABC), binary hybrid topology particle swarm optimization quadratic interpolation (BHTPSO-QI), binary ant lion optimizer (bALO) and binary gravitational search algorithm (BGSA) have been evaluated by feature selection problems. Moreover, UTF and seventeen transfer functions have been applied in original PSO, ABC, SSA and GWO algorithms to solve low and high dimensions 0–1 MKP benchmark instances. The results showed that the new transfer function significantly enhances the performance of algorithms to achieve the best solution in the binary search space.},
  archive      = {J_ASOC},
  author       = {Zahra Beheshti},
  doi          = {10.1016/j.asoc.2021.107346},
  journal      = {Applied Soft Computing},
  pages        = {107346},
  shortjournal = {Appl. Soft. Comput.},
  title        = {UTF: Upgrade transfer function for binary meta-heuristic algorithms},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Unifying paragraph embeddings and neural collaborative
filtering for hybrid recommendation. <em>ASOC</em>, <em>106</em>,
107345. (<a href="https://doi.org/10.1016/j.asoc.2021.107345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering is one of widely used recommendation techniques. Despite the effectiveness of matrix factorization for collaborative filtering; however, the inner product operator, combining the multiplication of latent features linearly, may not be sufficient to capture the complex structure of user interaction ratings. On the other hand, we argue that there is a great deviation between user ratings and their real interest preference. In this paper, we propose a novel hybrid recommendation algorithm. It adopts neural networks to exploit user–item ratings for collaborative filtering, which is endowed a high level of non-linearity for capturing the complex structure of user interaction ratings. At the same time, it exploits item embeddings to capture the content feature for auxiliary information, which solves the cold start problem to some extent. In particular, we introduce paragraph embeddings to represent user reviews and item descriptions, and design two neural networks to capture the sentiment of user reviews and the content feature of items, respectively. And then, we treat these embeddings as attention weights of users and items, and unify them with user–item ratings to model the hybrid recommendation system. Extensive experiments on Amazon product dataset demonstrates that our algorithm performs better on rating prediction than other state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Yihao Zhang and Zhi Liu and Chunyan Sang},
  doi          = {10.1016/j.asoc.2021.107345},
  journal      = {Applied Soft Computing},
  pages        = {107345},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unifying paragraph embeddings and neural collaborative filtering for hybrid recommendation},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive management of multimodal biometrics—a deep learning
and metaheuristic approach. <em>ASOC</em>, <em>106</em>, 107344. (<a
href="https://doi.org/10.1016/j.asoc.2021.107344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the framework for adaptive rank-level biometric fusion: a new approach towards personal authentication . In this work, a novel attempt has been made to identify the optimal design parameters and framework of a multibiometric system, where the chosen biometric traits are subjected to rank-level fusion. Optimal fusion parameters depend upon the security level demanded by a particular biometric application. The proposed framework makes use of a metaheuristic approach towards adaptive fusion in the pursuit of achieving optimal fusion results at varying levels of security. Rank-level fusion rules have been employed to provide optimum performance by making use of Ant Colony Optimization technique. The novelty of the reported work also lies in the fact that the proposed design engages three biometric traits simultaneously for the first time in the domain of adaptive fusion, so as to test the efficacy of the system in selecting the optimal set of biometric traits from a given set. Literature reveals the unique biometric characteristics of the fingernail plate, which have been exploited in this work for the rigorous experimentation conducted. Index, middle and ring fingernail plates have been taken into consideration, and deep learning feature-sets of the three nail plates have been extracted using three customized pre-trained models, AlexNet, ResNet-18 and DenseNet-201. The adaptive multimodal performance of the three nail plates has also been checked using the already existing methods of adaptive fusion designed for addressing fusion at the score-level and decision-level. Exhaustive experiments have been conducted on the MATLAB R2019a platform using the Deep Learning Toolbox. When the cost of false acceptance is 1.9, experimental results obtained from the proposed framework give values of the average of the minimum weighted error rate as low as 0.0115, 0.0097 and 0.0101 for the AlexNet, ResNet-18 and DenseNet-201 based experiments respectively. Results demonstrate that the proposed system is capable of computing the optimal parameters for rank-level fusion for varying security levels, thus contributing towards optimal performance accuracy.},
  archive      = {J_ASOC},
  author       = {Surabhi Hom Choudhury and Amioy Kumar and Shahedul Haque Laskar},
  doi          = {10.1016/j.asoc.2021.107344},
  journal      = {Applied Soft Computing},
  pages        = {107344},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive management of multimodal Biometrics—A deep learning and metaheuristic approach},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matching large-scale biomedical ontologies with central
concept based partitioning algorithm and adaptive compact evolutionary
algorithm. <em>ASOC</em>, <em>106</em>, 107343. (<a
href="https://doi.org/10.1016/j.asoc.2021.107343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a unified model for describing biomedical knowledge, a biomedical ontology is of help to solve the issues of data heterogeneity in different biomedical databases. However, these ontologies might model same biomedical knowledge differently, yielding the heterogeneity problem. To address the biomedical ontology heterogeneity problem, it is necessary to match the heterogeneous concept pairs between two ontologies. How to reduce the computational complexity is a challenging problem when matching large-scale biomedical ontologies, which directly affects the matching efficiency and the alignment’s quality. To face this challenge, this work proposes a large-scale biomedical ontology partitioning and matching framework. In our proposal, a central concepts based ontology partitioning algorithm is first used to divide the ontology into several disjoint segments, which borrows the idea from the social network and Firefly Algorithm (FA). The proposed algorithm is able to partition the ontologies with low computation complexity, and at the same time, ensure the semantic completeness and the decent scale of each segment. Then, an Adaptive Compact Evolutionary Algorithm (ACEA) based matching technique is utilized to determine the ontology segment alignments, which can efficiently match the similar ontology segments. The experiment utilizes the biomedical testing cases provided by Ontology Alignment Evaluation Initiative (OAEI) to test our approach’s effectiveness, and the experimental results show that the alignments obtained by our method significantly outperforms the state-of-the-art biomedical ontology matching techniques.},
  archive      = {J_ASOC},
  author       = {Xingsi Xue and Jie Zhang},
  doi          = {10.1016/j.asoc.2021.107343},
  journal      = {Applied Soft Computing},
  pages        = {107343},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Matching large-scale biomedical ontologies with central concept based partitioning algorithm and adaptive compact evolutionary algorithm},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trustworthiness assessment for industrial IoT as multilayer
networks with von neumann entropy. <em>ASOC</em>, <em>106</em>, 107342.
(<a href="https://doi.org/10.1016/j.asoc.2021.107342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industrial Internet of Things (IIoT) has expanded worldwide rapidly, which brings key devices and applications of IIoT under a trustworthy umbrella that reinforces secure and safe IIoT services have never been more important. However, there are few effective methods for assessing the trustworthiness of IIoT networks and services, which may lead to a compromised system and massive decreases in productivity, or even catastrophic consequences . Complex networks have emerged to be a promising method to assess the trustworthiness of IIoT because they can reveal the latent features of networks and services. Enlightened by the potential of complex networks, a cloud-fog-edge computing paradigm for IIoT is presented and mapped to multilayer networks. Furthermore, we propose a Trustworthiness Assessment with Entropy (TAE) method, which quantitatively analyzes the topological characteristics of the IIoT networks and services. Experimental results on synthetic and real-world datasets present a comprehensive assessment of IIoT trustworthiness with the qualitative and quantitative analysis of von Neumann entropy, which proves the feasibility and robustness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Xing Wu and Jianjia Wang and Peng Wang and Zhaoxiang Bian and Tao Huang and Yike Guo and Hamido Fujita},
  doi          = {10.1016/j.asoc.2021.107342},
  journal      = {Applied Soft Computing},
  pages        = {107342},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Trustworthiness assessment for industrial IoT as multilayer networks with von neumann entropy},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy hypothesis testing: Systematic review and
bibliography. <em>ASOC</em>, <em>106</em>, 107331. (<a
href="https://doi.org/10.1016/j.asoc.2021.107331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In statistical inference hypotheses related to different kinds of phenomena are formulated, and then data are collected and analyzed, which either confirm or falsify these hypotheses. Considering traditional statistics, in the underlying models hypotheses and sample data should be well defined. However, these models are often inadequate with regard to real-life problems, as theoretical specifications and observed information are frequently imprecise, vague, incomplete, qualitative, linguistic or noisy. To relax this rigidity, numerous researchers have proposed modifications and extensions of statistical inference approaches with the help of concepts of fuzzy statistics. In the meantime there are many papers on the topic of hypothesis testing in fuzzy environments, especially based on fuzzy hypotheses and/or by using fuzzy data. In order to structure this variety of contributions, proposals and applications, we give a comprehensive systematic review in this paper and offer a bibliography on fuzzy hypothesis testing. The paper seeks to consolidate the topic of fuzzy hypothesis testing with the purpose of supporting new researchers in this field and highlighting potential directions for future research.},
  archive      = {J_ASOC},
  author       = {Nataliya Chukhrova and Arne Johannssen},
  doi          = {10.1016/j.asoc.2021.107331},
  journal      = {Applied Soft Computing},
  pages        = {107331},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy hypothesis testing: Systematic review and bibliography},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Federated learning for COVID-19 screening from chest x-ray
images. <em>ASOC</em>, <em>106</em>, 107330. (<a
href="https://doi.org/10.1016/j.asoc.2021.107330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the whole world is facing a great medical disaster that affects the health and lives of the people: the COVID-19 disease, colloquially known as the Corona virus. Deep learning is an effective means to assist radiologists to analyze the vast amount of chest X-ray images, which can potentially have a substantial role in streamlining and accelerating the diagnosis of COVID-19. Such techniques involve large datasets for training and all such data must be centralized in order to be processed. Due to medical data privacy regulations, it is often not possible to collect and share patient data in a centralized data server. In this work, we present a collaborative federated learning framework allowing multiple medical institutions screening COVID-19 from Chest X-ray images using deep learning without sharing patient data. We investigate several key properties and specificities of federated learning setting including the not independent and identically distributed (non-IID) and unbalanced data distributions that naturally arise. We experimentally demonstrate that the proposed federated learning framework provides competitive results to that of models trained by sharing data, considering two different model architectures. These findings would encourage medical institutions to adopt collaborative process and reap benefits of the rich private data in order to rapidly build a powerful model for COVID-19 screening.},
  archive      = {J_ASOC},
  author       = {Ines Feki and Sourour Ammar and Yousri Kessentini and Khan Muhammad},
  doi          = {10.1016/j.asoc.2021.107330},
  journal      = {Applied Soft Computing},
  pages        = {107330},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated learning for COVID-19 screening from chest X-ray images},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel classifier architecture based on deep neural network
for COVID-19 detection using laboratory findings. <em>ASOC</em>,
<em>106</em>, 107329. (<a
href="https://doi.org/10.1016/j.asoc.2021.107329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unfortunately, Coronavirus disease 2019 (COVID-19) is spreading rapidly all over the world. Along with causing many deaths, it has substantially affected the social life, economics, and infrastructure worldwide in a negative manner. Therefore, it is very important to be able to diagnose the COVID-19 quickly and correctly. In this study, a new feature group based on laboratory findings was obtained considering ethnical and genetic differences for interpretation of blood data. Then, using this feature group, a new hybrid classifier architecture based on deep learning was designed and COVID-19 detection was made. Classification performance indicators were obtained as accuracy of 94.95\%, F1-score of 94.98\%, precision of 94.98\%, recall of 94.98\% and AUC of 100\%. Achieved results were compared with those of the deep learning classifiers suggested in literature. According to these results, proposed method shows superior performance and can provide more convenience and precision to experts for diagnosis of COVID-19 disease.},
  archive      = {J_ASOC},
  author       = {Volkan Göreke and Vekil Sarı and Serdar Kockanat},
  doi          = {10.1016/j.asoc.2021.107329},
  journal      = {Applied Soft Computing},
  pages        = {107329},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel classifier architecture based on deep neural network for COVID-19 detection using laboratory findings},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). R-GWO: Representative-based grey wolf optimizer for solving
engineering problems. <em>ASOC</em>, <em>106</em>, 107328. (<a
href="https://doi.org/10.1016/j.asoc.2021.107328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The grey wolf optimizer (GWO) is a well-known nature-inspired algorithm, which shows a sufficient performance for solving various optimization problems . However, it suffers from low exploration and population diversity because its optimization process is only based on the best three wolves greedily, and the information of other wolves does not consider. In this paper, a representative-based grey wolf optimizer (R-GWO) is proposed to tackle with these weaknesses of the GWO. The R-GWO introduces a search strategy named representative-based hunting (RH) a combination of three effective trial vectors inspired by alpha wolves’ behaviors to improve the exploration and diversity of the population. The RH search strategy utilizes a representative archive to reduces the greediness and enhance the diversity of solutions, and it can also strike balance between the exploration and exploitation using a non-linear control parameter. The performance and applicability of the proposed R-GWO were evaluated on CEC 2018 benchmark functions and six engineering design problems . The results were compared by eight state-of-the-art metaheuristic algorithms : PSO , KH, GWO, WOA , EEGWO , BOA , HHO, and HGSO. Moreover, the results were statistically analyzed by three test Wilcoxon rank-sum, Friedman and mean absolute error (MAE). The performance results show that on all 29 functions with dimensions 30, 50, and 100, the R-GWO is superior to the competitor algorithms except on function 27 on all dimensions and function 22 on dimension 30. The proposed R-GWO is the most effective algorithm compared with competitor algorithms, with an overall effectiveness of 95.4\%. The experimental and statistical results show that the R-GWO is competitive and superior to compared algorithms and can solve engineering design problems better than competitor algorithms.},
  archive      = {J_ASOC},
  author       = {Mahdis Banaie-Dezfouli and Mohammad H. Nadimi-Shahraki and Zahra Beheshti},
  doi          = {10.1016/j.asoc.2021.107328},
  journal      = {Applied Soft Computing},
  pages        = {107328},
  shortjournal = {Appl. Soft. Comput.},
  title        = {R-GWO: Representative-based grey wolf optimizer for solving engineering problems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral acceleration prediction using genetic programming
based approaches. <em>ASOC</em>, <em>106</em>, 107326. (<a
href="https://doi.org/10.1016/j.asoc.2021.107326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary computation (EC) is a widely used computational intelligence that facilitates the formulation of a range of complex engineering problems. This study tackled two hybrid EC techniques based on genetic programming (GP) for ground motion prediction equations (GMPEs). The first method coupled regression analysis with multi-objective genetic programming . In this way, the strategy was maximizing the accuracy and minimizing the models’ complexity simultaneously. The second approach incorporated mesh adaptive direct search (MADS) into gene expression programming to optimize the obtained coefficients. A big data set provided by the Pacific Earthquake Engineering Research Centre (PEER) was used for the model development. Two explicit formulations were developed during this effort. In those formulae, we correlated spectral acceleration to a set of seismological parameters, including the period of vibration, magnitude, the closest distance to the fault ruptured area, shear wave velocity averaged over the top 30 meters, and style of faulting. The GP-based models are verified by a comprehensive comparison with the most well-known methods for GMPEs. The results show that the proposed models are quite simple and straightforward. The high degrees of accuracy of the predictions are competitive with the NGA complex models. Correlations of the predicted data using GEP-MADs and MOGP-R models with the real observations seem to be better than those available in the literature. Three statistical measures for GMPEs, such as E (\%), LLH, and EDR index, confirmed those observations.},
  archive      = {J_ASOC},
  author       = {Mostafa Gandomi and Ali R. Kashani and Ali Farhadi and Mohsen Akhani and Amir H. Gandomi},
  doi          = {10.1016/j.asoc.2021.107326},
  journal      = {Applied Soft Computing},
  pages        = {107326},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spectral acceleration prediction using genetic programming based approaches},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization of hydropower reservoir operation based on
hedging policy using jaya algorithm. <em>ASOC</em>, <em>106</em>,
107325. (<a href="https://doi.org/10.1016/j.asoc.2021.107325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The production and use of energy from hydropower generation play a vital role in the economy. Besides, the presence of uncertainty further increases the complexity in optimizing the reservoir operation. A synthetic streamflow generation based on historical inflow records was employed using the Thomas–Fiering model for handling the uncertainty and variability of reservoir inflows. However, under the circumstances of water deficiency, the hydropower output is significantly reduced. In this study, an investigation of a parameter free Jaya algorithm as an optimization method for reservoir operation was carried out. When deriving the optimal operational rule a hedging strategy is introduced to attenuate the impact of reduced water supply. This strategy can effectively counterbalance the lack of water supply with reservoir storage requirements. The higher amount of hydropower generated by the proposed algorithm than the other algorithms used in this study, such as genetic algorithm (GA), the ant colony algorithm (ACO), the bat algorithm (BA), the particle swarm optimization (PSO) algorithm, chicken swarm optimization (CSO) algorithm, grasshopper optimization algorithm (GOA), equilibrium optimizer (EO) and firefly algorithm (FA), has shown its efficiency in the reservoir system. Several reservoir performance indices, such as total hydropower generation, reliability, and resilience, were used to access the proposed algorithm and other algorithms efficiency},
  archive      = {J_ASOC},
  author       = {Kai Lun Chong and Sai Hin Lai and Ali Najah Ahmed and Wan Zurina Wan Jaafar and Ahmed El-Shafie},
  doi          = {10.1016/j.asoc.2021.107325},
  journal      = {Applied Soft Computing},
  pages        = {107325},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of hydropower reservoir operation based on hedging policy using jaya algorithm},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GAR: Graph adversarial representation for adverse drug event
detection on twitter. <em>ASOC</em>, <em>106</em>, 107324. (<a
href="https://doi.org/10.1016/j.asoc.2021.107324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adverse drug reaction events have become one of the main causes of patient death. Since traditional post-marketing surveillance systems based on spontaneous reports have a serious underreporting issue, in recent years research on the detection of adverse reaction events using social media such as Twitter as a data source has attracted increasing attention in recent year. Deep learning models usually rely on a large number of training samples. However, due to the characteristics of user-generated content and the time-consuming data annotation process, related research is faced with the problems caused by small-scale annotated datasets, which restricts deep learning models in achieving satisfactory results. Accordingly, we introduce two regularization methods are introduced at the representation level, i.e., graph embedding-based data augmentation and adversarial training , to improve the performance of detecting adverse events under such conditions. Besides, the applicable scope of these two methods is analyzed and discussed through experiments. Combined with the convolutional neural network , this paper proposes an adverse drug event detection framework that can make full use of the methods.},
  archive      = {J_ASOC},
  author       = {Chen Shen and Zhiheng Li and Yonghe Chu and Zhongying Zhao},
  doi          = {10.1016/j.asoc.2021.107324},
  journal      = {Applied Soft Computing},
  pages        = {107324},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GAR: Graph adversarial representation for adverse drug event detection on twitter},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simplified swarm optimization for bi-objection active
reliability redundancy allocation problems. <em>ASOC</em>, <em>106</em>,
107321. (<a href="https://doi.org/10.1016/j.asoc.2021.107321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reliability redundancy allocation problem (RRAP) is a well-known tool in system design, development, and management. The RRAP is always modeled as a nonlinear mixed-integer non-deterministic polynomial-time hardness (NP-hard) problem. To maximize the system reliability, the integer (component active redundancy level) and real variables (component reliability) must be determined to ensure that the cost limit and some nonlinear constraints are satisfied. In this study, a bi-objective RRAP is formulated by changing the cost constraint as a new goal, because it is necessary to balance the reliability and cost impact for the entire system in practical applications. To solve the proposed problem, a new simplified swarm optimization (SSO) with a penalty function, a real one-type solution structure, a number-based self-adaptive new update mechanism, a constrained nondominated-solution selection, and a new pBest replacement policy is developed in terms of these structures selected from full-factorial design to find the Pareto solutions efficiently and effectively. The proposed SSO outperforms several metaheuristic state-of-the-art algorithms, e.g., nondominated sorting genetic algorithm II (NSGA-II) and multi-objective particle swarm optimization (MOPSO), according to experimental results for four benchmark problems involving the bi-objective active RRAP.},
  archive      = {J_ASOC},
  author       = {Wei-Chang Yeh and Yi-Zhu Su and Xiao-Zhi Gao and Cheng-Feng Hu and Jing Wang and Chia-Ling Huang},
  doi          = {10.1016/j.asoc.2021.107321},
  journal      = {Applied Soft Computing},
  pages        = {107321},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Simplified swarm optimization for bi-objection active reliability redundancy allocation problems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fault detection based on auto-regressive extreme learning
machine for nonlinear dynamic processes. <em>ASOC</em>, <em>106</em>,
107319. (<a href="https://doi.org/10.1016/j.asoc.2021.107319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through utilizing the extreme learning machines (ELM) in modeling the nonlinear dynamic relationship of the time-series data, a novel fault detection approach based on auto-regressive ELM (ARELM) is proposed for nonlinear dynamic processes. The ARELM model attempts to predict each data vector as a nonlinear mapping of its previous measurements, the nonlinear dynamic relations in the time-series samples can thus be interpreted. The residual between the actual data vector and its prediction could be monitored instead for detecting the abnormalities in the defined nonlinear dynamic mechanism. Inherited from the advantages of the ELM, the calculation burden of offline training and online monitoring procedures is much reduced in contrast to kernel based approaches. The efficiency and superiority of the ARELM-based method over other state-of-the-art fault detection techniques for dynamic processes have been demonstrated.},
  archive      = {J_ASOC},
  author       = {Yang Chen and Chudong Tong and Yinghui Ge and Ting Lan},
  doi          = {10.1016/j.asoc.2021.107319},
  journal      = {Applied Soft Computing},
  pages        = {107319},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault detection based on auto-regressive extreme learning machine for nonlinear dynamic processes},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A self-adaptive multi-objective dynamic differential
evolution algorithm and its application in chemical engineering.
<em>ASOC</em>, <em>106</em>, 107317. (<a
href="https://doi.org/10.1016/j.asoc.2021.107317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new multi-objective dynamic differential evolution algorithm with parameter self-adaptive strategies, named SA-MODDE. All components of the algorithm are synergically designed to reach its full potential, containing parental selection, mutation strategy, parameter setting, survival selection, constraint handling, and termination criteria. The improvement measures emphasize exploiting Pareto dominance information more efficiently. Particularly, parameter adaptation schemes are introduced based on both prior knowledges of current individual and feedback information on previous promising solutions, and their effectiveness is validated by comparison with three fixed-parameter combinations. Extensive numerical tests are conducted on multiple test suites with five state-of-the-art peer competitors. The statistical results demonstrated that the SA-MODDE exhibits good proximity and diversity in dealing with benchmark functions with various characteristics. Three industrial (bio)chemical processes, including two optimal control and one reformulated constrained tri-objective, are investigated to show the feasibility and robustness of the SA-MODDE.},
  archive      = {J_ASOC},
  author       = {Xiaodong Zhang and Lu Jin and Chengtian Cui and Jinsheng Sun},
  doi          = {10.1016/j.asoc.2021.107317},
  journal      = {Applied Soft Computing},
  pages        = {107317},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-adaptive multi-objective dynamic differential evolution algorithm and its application in chemical engineering},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Functional deep echo state network improved by a bi-level
optimization approach for multivariate time series classification.
<em>ASOC</em>, <em>106</em>, 107314. (<a
href="https://doi.org/10.1016/j.asoc.2021.107314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multivariate time series (MTS) classification is one of the major tasks of time series data mining. Many methods have been proposed to investigate the MTS classification. Among them, the method based on feature representation is the most popular and widely used one. However, there exist some shortcomings for this method, such as unsatisfactory accuracy, being sensitive to noise and not able to fully make use of time series data attributes. In order to overcome these disadvantages, we propose a new method called functional deep echo state network (FDESN) for MTS classification that utilizes two special operators: temporal aggregation and spatial aggregation. In general, the parameters of the FDESN are determined by random selection, human experience or trial and error. This may increase the complexity of the FDESN or reduce the accuracy of the FDESN. In this study, a novel bi-level optimization approach is proposed to optimize the parameters of the FDESN. The parameter selection problem in the FDESN is transformed into the bi-level optimization problem . The state transition algorithm (STA) is used to solve the bi-level optimization problem . Finally, the experimental results show that the proposed method is superior to other methods. In addition, the proposed method is successfully applied to anode condition identification in aluminum electrolysis . For the aluminum electrolysis datasets, the proposed method improved the average classification accuracy by about 3.5\% compared with the other methods. For a specific aluminum electrolysis dataset ACS2504, the classification accuracy significantly increased from 77.92\% to 82.69\% by using the proposed method.},
  archive      = {J_ASOC},
  author       = {Zhaoke Huang and Chunhua Yang and Xiaofang Chen and Xiaojun Zhou and Guo Chen and Tingwen Huang and Weihua Gui},
  doi          = {10.1016/j.asoc.2021.107314},
  journal      = {Applied Soft Computing},
  pages        = {107314},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Functional deep echo state network improved by a bi-level optimization approach for multivariate time series classification},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal production and maintenance scheduling for a
degrading multi-failure modes single-machine production environment.
<em>ASOC</em>, <em>106</em>, 107312. (<a
href="https://doi.org/10.1016/j.asoc.2021.107312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two important aspects of manufacturing systems are production scheduling and maintenance planning. These aspects are interdependent, but in most research work, this dependency is ignored. This paper proposed an integrated mathematical model for joint production scheduling and maintenance planning for a degrading multi-failure single-machine manufacturing system, in which the machine has discrete deterioration states . The machine is subject to different failure modes. The first failure mode is the machine’s full deterioration, which is detected at the end of a job’s processing, and the other failure mode is random breakdowns, which are detected at the time of failure. Two machine deterioration state-based thresholds are considered, and five different maintenance actions may carry out a replacement, preventive, and corrective perfect or imperfect maintenance. Since the machine’s states’ transitions follow an exponential distribution , a closed-form matrix-based mathematical model with probabilistic input parameters is presented. This paper aims to optimize the total system’s cost, including the maintenance cost, machine energy consumption cost as well as the makespan penalty for exceeding a pre-determined threshold. The proposed model determines the optimal jobs’ sequence as well as the machine’s deterioration state-based thresholds. Due to the complexity of the developed model, a genetic algorithm (GA), simulated annealing (SA) algorithm, and a teaching–learning-based optimization (TLBO) algorithm have been used to solve the presented model. The algorithms are validated using a full enumeration technique, and the model is validated by applying different maintenance strategies. Our results demonstrate the superiority of the GA compared to the other algorithms.},
  archive      = {J_ASOC},
  author       = {Mani Sharifi and Sharareh Taghipour},
  doi          = {10.1016/j.asoc.2021.107312},
  journal      = {Applied Soft Computing},
  pages        = {107312},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal production and maintenance scheduling for a degrading multi-failure modes single-machine production environment},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Voice pathology detection by using the deep network
architecture. <em>ASOC</em>, <em>106</em>, 107310. (<a
href="https://doi.org/10.1016/j.asoc.2021.107310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathological voice disorders are among the conditions affecting negatively our daily life. The aim of this study is to introduce the new feature vector in the hybrid axis and multi-model in order to diagnose these disorders with more conventional methods. Two different databases are used, and the results are compared with the previous studies. Here, two types of fusion models (feature and decision level fusion) are used to increase the classification accuracy of the multi-model. The experimental results show that the proposed multi-model gives the highest classification accuracies with decision level fusion (DLF). Inspecting the results obtained from two databases, the highest accuracy rate (99.58\%) is obtained with DLF. It is also seen from the experiments that the proposed feature vector helps to classify pathological data successfully, depending on their pathological conditions . Together with the proposed multi-model, both LSTM and CNN are found to be similarly successful in the classification of data in multi-model architecture.},
  archive      = {J_ASOC},
  author       = {Haydar Ankışhan and Sıtkı Çağdaş İnam},
  doi          = {10.1016/j.asoc.2021.107310},
  journal      = {Applied Soft Computing},
  pages        = {107310},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Voice pathology detection by using the deep network architecture},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary learning based simulation optimization for
stochastic job shop scheduling problems. <em>ASOC</em>, <em>106</em>,
107309. (<a href="https://doi.org/10.1016/j.asoc.2021.107309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation Optimization (SO) techniques refer to a set of methods that have been applied to stochastic optimization problems , structured so that the optimizer(s) are integrated with simulation experiments. Although SO techniques provide promising solutions for large and complex stochastic problems , the simulation model execution is potentially expensive in terms of computation time. Thus, the overall purpose of this research is to advance the evolutionary SO methods literature by researching the use of metamodeling within these techniques. Accordingly, we present a new Evolutionary Learning Based Simulation Optimization (ELBSO) method embedded within Ordinal Optimization. In ELBSO a Machine Learning (ML) based simulation metamodel is created using Genetic Programming (GP) to replace simulation experiments aimed at reducing computation. ELBSO is evaluated on a Stochastic Job Shop Scheduling Problem (SJSSP), which is a well known complex production planning problem in most industries such as semiconductor manufacturing . To build the metamodel from SJSSP instances that replace simulation replications, we employ a novel training vector to train GP. This then is integrated into an evolutionary two-phased Ordinal Optimization approach to optimize an SJSSP which forms the ELBSO method. Using a variety of experimental SJSSP instances, ELBSO is compared with evolutionary optimization methods from the literature and typical dispatching rules . Our findings include the superiority of ELBSO over all other algorithms in terms of the quality of solutions and computation time. Furthermore, the integrated procedures and results provided within this article establish a basis for future SO applications to large and complex stochastic problems .},
  archive      = {J_ASOC},
  author       = {Amir Ghasemi and Amir Ashoori and Cathal Heavey},
  doi          = {10.1016/j.asoc.2021.107309},
  journal      = {Applied Soft Computing},
  pages        = {107309},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary learning based simulation optimization for stochastic job shop scheduling problems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fair, preference-based posted price resale e-market model
and clearing heuristics for circular economy. <em>ASOC</em>,
<em>106</em>, 107308. (<a
href="https://doi.org/10.1016/j.asoc.2021.107308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resale markets in which secondhand new and used goods are traded play an important role in circular economy with significant economic and environmental benefits. This study proposes a preference-based posted price electronic market model for resale markets which features several mechanisms to improve the market outcome. The proposed model allows market participants to post sales and purchase orders simultaneously inside a trading round which also enables participants to use the revenue obtained from the items to be sold to purchase other items in the market. Besides, each participant is allowed to declare a budget constraint which restricts the amount that the participant will spend in the market to prevent a possible budget deficiency. Furthermore, the model also allows participants to declare their preferences of substitutabilities in their orders. In this study, the proposed model is formally defined, the corresponding market clearing problem is formulated as a hierarchical multi-objective linear integer program to provide fair allocation between the participants. Four different objective functions are proposed, and their outcomes are compared to the current market system. Since the clearing problem is NP-Hard, several heuristic methods including ant colony optimization , artificial bee colony and genetic algorithms along with problem-specific operators are proposed. The performance of the model is statistically analyzed based on several experiments. The genetic algorithm using the proposed problem-specific operators provides solutions within 3\% of the optimal objective values and within 1\% of the optimal fairness on average. The results also indicate that the model provides improved market outcomes and fair allocation of items among the participants, and thus it has a potential to contribute to the growth of circular economy.},
  archive      = {J_ASOC},
  author       = {Ali Haydar Özer},
  doi          = {10.1016/j.asoc.2021.107308},
  journal      = {Applied Soft Computing},
  pages        = {107308},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fair, preference-based posted price resale e-market model and clearing heuristics for circular economy},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-focus image fusion using neutrosophic based wavelet
transform. <em>ASOC</em>, <em>106</em>, 107307. (<a
href="https://doi.org/10.1016/j.asoc.2021.107307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key aim of Multi-focus Image Fusion (MFIF) is to gather all the necessary and useful information as well as features from the source images and then merge that information to develop a fused image. This fused image is having more information and better image quality than the source images. There exist a number of MFIF techniques in spatial as well as transform domain. However, since the source images have low resolution, high level of noise and are blurred, it becomes quite difficult for the traditional MFIF techniques to provide a fused image with high image quality. Thus, to fuse the source images efficiently, a hybrid MFIF method is proposed which consists of Neutrosophic Set and Stationary Wavelet Transform . Various state-of-the-art MFIF techniques have been used to compare the performance of the Neutrosophic Stationary Wavelet Transform (NSWT) technique. To evaluate the proposed technique, quantitative as well as qualitative evaluation has been done on two different datasets. To quantitatively evaluate the proposed method based on the NSWT, two different types of evaluation metrics are used namely ”reference-based” and ”reference-less”. From the experimental results and comparison with the existing state-of-the-art techniques it has been found that the NSWT has achieved better quantitative and qualitative results that in turn helpful in extending the Depth-of-Field of the imaging system . The achieved fusion results has shown the effectiveness of the proposed MFIF technique.},
  archive      = {J_ASOC},
  author       = {Shiveta Bhat and Deepika Koundal},
  doi          = {10.1016/j.asoc.2021.107307},
  journal      = {Applied Soft Computing},
  pages        = {107307},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-focus image fusion using neutrosophic based wavelet transform},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparative analysis of fuzzy classifier and ANN with
histogram features for defect detection and classification in planetary
gearbox. <em>ASOC</em>, <em>106</em>, 107306. (<a
href="https://doi.org/10.1016/j.asoc.2021.107306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The planetary gearbox plays a vital role in many heavy-duty power transmission systems. It is essential to monitor such systems for smooth and continuous operations to anticipate machine downtime , production loss and to schedule maintenance. Vibration-based condition monitoring of mechanical systems has been gaining momentum among other methods due to its robustness. Generally, condition monitoring of planetary gearbox forms a classification problem. This study aims to make a comparative analysis between Fuzzy classifier and Artificial Neural Network (ANN) in terms of classification ability with histogram features. Here defect detection in planetary gearbox in different conditions such as gearbox in healthy condition (Healthy), gearbox with sun gear defect (SGD), gearbox with planet gear defect (PGD), gearbox with ring gear defect (RGD) and gearbox with sun and planet gear defect (SPGD) are considered. Analysis is performed using vibration signals acquired for different conditions of the planetary gearbox introducing one-fault-at-a-time. From the acquired vibration signals , histogram features are extracted and using decision tree algorithm the predominant features can be selected. Afterwards, a set of rules are formed from the selected features and given as input to the fuzzy classifier as well as ANN in order to evaluate the classification capability of each method. By running the experiment on 500 samples, the output show that both algorithms are found to have high accuracy in detecting defect and classifying the defect condition in planetary gearbox.},
  archive      = {J_ASOC},
  author       = {Syed Shaul Hameed and V. Muralidharan and Bernadetta Kwintiana Ane},
  doi          = {10.1016/j.asoc.2021.107306},
  journal      = {Applied Soft Computing},
  pages        = {107306},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comparative analysis of fuzzy classifier and ANN with histogram features for defect detection and classification in planetary gearbox},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A collaborative variable neighborhood descent algorithm for
the hybrid flowshop scheduling problem with consistent sublots.
<em>ASOC</em>, <em>106</em>, 107305. (<a
href="https://doi.org/10.1016/j.asoc.2021.107305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lot streaming is the most widely used technique of supporting the overlap of consecutive operations. Inspired by the real-world scenarios, this paper introduces this issue into the hybrid flowshop scheduling problem with consistent sublots (HFSP_CS). The innovations of this paper lie in developing a mixed integer linear programming (MILP) model, and in developing a collaborative variable neighborhood descent algorithm (CVND). The CVND evolves with a primary solution and an archive set comprising of promising solutions found in the progress, and contains four processes including the VND process, collaborative process, archive set and primary solution restart processes. Accordingly, the primary solution conducts the VND process with the defined neighborhood structures to implement the local exploitation. The archive set executes the collaborative process to learn from the historical information to implement the global search. The archive set restart is triggered when it is stuck into the local optima. The primary solution restart aims to conduct a large perturbation on the primary solution for the following loop of VND process. Regarding the problem-specific characteristics, the solution encoding and decoding are designed and an improvement strategy is developed to further improve the solution quality. To validate the CVND, two sets of instances are collected. Through comparing with the CPLEX solver, a heuristic and five state-of-the-art metaheuristics on small instances, the CVND shows the most suitable performance in terms of the objective values and algorithm efficiency. Through comparing with the heuristic and metaheuristics on medium-large instances, the CVND performs statistically better in terms of the relative percentage deviation values.},
  archive      = {J_ASOC},
  author       = {Biao Zhang and Quan-Ke Pan and Lei-Lei Meng and Xin-Li Zhang and Ya-Ping Ren and Jun-Qing Li and Xu-Chu Jiang},
  doi          = {10.1016/j.asoc.2021.107305},
  journal      = {Applied Soft Computing},
  pages        = {107305},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A collaborative variable neighborhood descent algorithm for the hybrid flowshop scheduling problem with consistent sublots},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved binary particle swarm optimization for feature
selection with new initialization and search space reduction strategies.
<em>ASOC</em>, <em>106</em>, 107302. (<a
href="https://doi.org/10.1016/j.asoc.2021.107302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is an important preprocessing technique for dimensionality reduction in classification problems. Particle swarm optimization (PSO) algorithms have been widely used as the optimizers for FS problems. However, with the increase of data dimensionality, the search space expands dramatically, which proposes significant challenges for optimization methods, including PSO . In this paper, we propose an improved sticky binary PSO (ISBPSO) algorithm for FS. ISBPSO adopts three new mechanisms based on a recently proposed binary PSO variant, sticky binary particle swarm optimization (SBPSO), to improve the evolutionary performance. First, a new initialization strategy using the feature weighting information based on mutual information is proposed. Second, a dynamic bits masking strategy for gradually reducing the search space during the evolutionary process is proposed. Third, based on the framework of memetic algorithms , a refinement procedure conducting genetic operations on the personal best positions of ISBPSO is used to alleviate the premature convergence problem . The results on 12 UCI datasets show that ISBPSO outperforms six benchmark PSO-based FS methods and two conventional FS methods (sequential forward selection and sequential backward selection) — ISBPSO obtains either higher or similar accuracies with fewer features in most cases. Moreover, ISBPSO substantially reduces the computation time compared with benchmark PSO-based FS methods. Further analysis shows that all the three proposed mechanisms are effective for improving the search performance of ISBPSO.},
  archive      = {J_ASOC},
  author       = {An-Da Li and Bing Xue and Mengjie Zhang},
  doi          = {10.1016/j.asoc.2021.107302},
  journal      = {Applied Soft Computing},
  pages        = {107302},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved binary particle swarm optimization for feature selection with new initialization and search space reduction strategies},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A vector angles-based many-objective particle swarm
optimization algorithm using archive. <em>ASOC</em>, <em>106</em>,
107299. (<a href="https://doi.org/10.1016/j.asoc.2021.107299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the easy implement and fast convergence speed in single objective optimization problems , the particle swarm optimization algorithm has been extended to many-objective optimization problems . When designing a many-objective particle swarm optimization algorithm, archive maintenance strategy and selection of leaders are two crucial issues. To addressing these two problems, this paper proposed a vector angles-based many-objective particle swarm optimization algorithm using archive. In this algorithm, a new method was proposed to make the external archive getting a balance between convergence and diversity, based on the values of vector angles of each solution. Also, we designed a novel strategy based on vector angle and decomposition to select g b e s t gbest and p b e s t pbest from the archive to promote evolution of the population. Besides, the shift-based density estimation was used as the standard to clone elite solutions to strengthen the quality of the external archive . The experiment results indicate that our algorithm has competitive performance comparing with the six state-of-the-art many-objective optimization algorithms on three widely used benchmarks DTLZ, WFG, and MaF.},
  archive      = {J_ASOC},
  author       = {Lei Yang and Xin Hu and Ke Li},
  doi          = {10.1016/j.asoc.2021.107299},
  journal      = {Applied Soft Computing},
  pages        = {107299},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A vector angles-based many-objective particle swarm optimization algorithm using archive},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty-based rainfall network design using a fuzzy
spatial interpolation method. <em>ASOC</em>, <em>106</em>, 107296. (<a
href="https://doi.org/10.1016/j.asoc.2021.107296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of optimization of rain gauge stations locations is critical given that rainfall data is central to various water-related studies. As rainfall data has vagueness in nature, fuzzy set theory can describe uncertainties existing in rainfall data. In this paper, we develop a framework to rainfall network design that combines fuzzy concepts and a deterministic spatial interpolation method known as Fuzzy Inverse Distance Weighted (FIDW). It addresses two important issues: (1) the assessment of two types of fuzzy mathematical approaches known as Fuzzy Standard IDW (FS-IDW) and Fuzzy Modified IDW (FM-IDW); (2) the comparison of the FIDW with spatial and spatiotemporal network designs using Ordinary Kriging (OK), known as OK-S and OK-ST, respectively. We consider four objective functions (OF): interval-based Estimation Error Variance Types 1 and 2 (EEVT1, EEVT2), Mean Square Error (MSE) and Coefficient of Determination (R 2 ). Four scenarios of number of removed stations including 5, 10, 15 and 20 are also analysed via statistical indicators. Firstly, the FIDW parameters (power and radius) are optimized for each OF. Then, we resort to a Genetic Algorithm (GA) to solve these OFs. Percentage of similarity between optimal removed station in both FIDW methods (FS-IDW and FM-IDW) is higher than OK-ST method. Between FIDW methods, FM-IDW yields better results. Statistical results of four removed stations (5, 10, 15 and 20 rain gauge stations) show that the highest variation in estimation accuracy is from 5 to 20 removed stations which belongs to EEVT1 OF and is around 30\%.},
  archive      = {J_ASOC},
  author       = {Bardia Bayat and Mohsen Nasseri and Eric Delmelle},
  doi          = {10.1016/j.asoc.2021.107296},
  journal      = {Applied Soft Computing},
  pages        = {107296},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty-based rainfall network design using a fuzzy spatial interpolation method},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ensemble approach of classification model for detection
and classification of power quality disturbances in PV integrated
microgrid network. <em>ASOC</em>, <em>106</em>, 107294. (<a
href="https://doi.org/10.1016/j.asoc.2021.107294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, different Power Quality Disturbances (PQDs) in Photovoltaic (PV) integrated Microgrid (MG) network have been detected and classified using a voting method of ensemble classification model along with Discrete Wavelet Transform (DWT) analysis. The proposed ensemble classification model is useful to classify the most common PQDs (voltage sag, voltage swell, and harmonics) in islanded MG network, and different PQ transients in both grid-connected and islanded MG network. For this study, a PV integrated MG model has been developed in the Matlab/Simulink software environment with introduction of different PQDs. The result obtained reveals that the performance of proposed ensemble classification model-2 (combination of Bayesian net, Multi-layer perceptron (MLP) and J48 decision tree (JDT) classifiers) attains higher classification accuracy (100\%) as compared to other ensemble classification model-1 (combination of Bayes net and MLP classifiers) and base classifiers such as Bayesian net, MLP and JDT . Further, the effectiveness of classifiers has been assessed using performance indices (PI) such as Kappa statistics, Mean absolute error (MAE), Root mean square error (RMSE), Precision, Recall, F-measure, and Receiver operating characteristics (ROC). From the results of PI, it can be concluded that the proposed ensemble model-2 outperforms ensemble model-1 and other base classifiers .},
  archive      = {J_ASOC},
  author       = {Arangarajan Vinayagam and Veerapandiyan Veerasamy and Padmavathi Radhakrishnan and Maheswari Sepperumal and Kalaivani Ramaiyan},
  doi          = {10.1016/j.asoc.2021.107294},
  journal      = {Applied Soft Computing},
  pages        = {107294},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble approach of classification model for detection and classification of power quality disturbances in PV integrated microgrid network},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local search for weighted sum coloring problem.
<em>ASOC</em>, <em>106</em>, 107290. (<a
href="https://doi.org/10.1016/j.asoc.2021.107290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weighted sum coloring problem (WSCP) is a weighted vertex coloring problem in which a weight is associated with each color. Its aim is to find a coloring of the vertices of a graph with the minimum sum of the costs of the used colors. The WSCP has important applications in the batch scheduling of distributed systems. In this paper, we obtain an integer linear programming (ILP) model for the WSCP, and design an efficient local search algorithm named LS-WSC to solve WSCP. To this end, we propose two ideas to make LS-WSC effective. Firstly, we design an adaptive strategy to change the number of color classes in current solution, which allows LS-WSC to search more feasible solution regions. Secondly, we design a greedy function and a tabu strategy, which make LS-WSC search in right direction. We also design a genetic algorithm GA-WSC to solve WSCP, which is used to experimentally evaluate the result of local search algorithms. The preliminary experimental results show that LS-WSC outperforms the famous CPLEX on almost all instances, and is competitive with the approximate algorithm GREEDY on most benchmark instances with vertex number less than 200. However, the performance of LS-WSC is worse than GREEDY algorithm ′ ′ s on most benchmark instances with vertex number greater than 200. Overall, the performance of LS-WSC is only marginally better than GA-WSC ′ ′ s. In order to improve the performance of LS-WSC, we use the configuration checking strategy to update the searching process of LS-WSC, and the new algorithm for solving WSCP is named as CCLS-WSC. The further experimental results show that CCLS-WSC algorithm outperforms the famous CPLEX, the approximate algorithm GREEDY, the genetic algorithm GA-WSC and LS-WSC on all benchmark instances.},
  archive      = {J_ASOC},
  author       = {Dangdang Niu and Bin Liu and Minghao Yin},
  doi          = {10.1016/j.asoc.2021.107290},
  journal      = {Applied Soft Computing},
  pages        = {107290},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local search for weighted sum coloring problem},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-stage ranking method to minimize ordinal violation for
pairwise comparisons. <em>ASOC</em>, <em>106</em>, 107287. (<a
href="https://doi.org/10.1016/j.asoc.2021.107287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise comparison is a powerful tool in intelligent decision making systems. Items are compared using numerical judgments that estimate the item weight ratios, which are provided by decision makers or transformed by objective data. The reliable assignment of numerical judgments is important, because judgment variety leads to significantly different results. However, it is difficult to provide exact ratios for decision makers owing to the limitations of knowledge. Although objective data provides an estimation of the ratios, a series of artificially defined rules is required to transform data into numerical judgments; however, these rules are subjective and arbitrary. Conversely, the dominance relationships between the items are obvious and reliable. Therefore, this study proposes a two-stage ranking method to minimize the ordinal violation that indicates the degree of conflict between the ranking result and the dominance. First, a 0–1 integer programming is designed and solved. Then, the second stage focuses on the topological sorting of nodes in a graph constructed using the optimal solution. To validate the effectiveness of the proposed method, we perform two experiments: a numerical example provided by participants and a real-world application involving ranking top tennis players. The results show that the proposed method not only avoids subjectivity in judgments, but also obtains the ranking that has the minimum ordinal violation among the compared methods.},
  archive      = {J_ASOC},
  author       = {Haomin Wang and Yi Peng and Gang Kou},
  doi          = {10.1016/j.asoc.2021.107287},
  journal      = {Applied Soft Computing},
  pages        = {107287},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage ranking method to minimize ordinal violation for pairwise comparisons},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time hierarchical risk assessment for UAVs based on
recurrent fusion autoencoder and dynamic FCE: A hybrid framework.
<em>ASOC</em>, <em>106</em>, 107286. (<a
href="https://doi.org/10.1016/j.asoc.2021.107286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective risk assessment is critical for unmanned aerial vehicles (UAVs) to ensure their safety and reliability. Up to now, the researchers have proposed quite a few methods for the above target. However, these methods are mainly based on path planning and collision theory, the risk caused by the abnormal status of UAVs themselves is generally ignored, which limits the further improvement on their performance. In practice, due to factors such as complicated compositions, variable condition monitoring (CM) data, and scarce failure records, etc., it is always a great challenge to implement the complete information fusion and accurate risk assessment for UAVs based on their real-time status. In this regard, a novel hybrid framework is proposed in this paper, which integrates the qualitative knowledge and the quantitative CM data, to evaluate the real-time hierarchical risk of UAVs. Specifically, the complicated UAV is firstly abstracted as a multi-level evaluating index system considering its qualitative logic compositions. Then, for each low-level index, given its multivariate CM data of several time instants, recurrent fusion autoencoder (RFA), a novel unsupervised neural network architecture , is proposed to extract their robust and complete feature embeddings automatically, where not only the information of variate dimension but also the information of time dimension can be fully fused. Furthermore, the risk of each low-level index is quantified by the adaptive Gaussian mixture model in a probabilistic way, which is truly data-driven with the help of the Bayesian hyperparameter optimization. Finally, the dynamic fuzzy comprehensive evaluation is utilized to evaluate the hierarchical risk of UAVs level by level, it should be noticed that our method can dynamically adjust the weights of each index employing the variable weight coefficients, which can capture the preliminary risk of UAVs more timely compared with the traditional methods. The proposed framework is validated on two typical datasets: the turbofan engine datasets (simulation) and the UAV flight datasets (real). The experimental results demonstrate the effectiveness and superiority of the hybrid framework on robust information fusion and accurate hierarchical risk assessment.},
  archive      = {J_ASOC},
  author       = {Xuanyuan Su and Laifa Tao and Hongmei Liu and Lizhi Wang and Mingliang Suo},
  doi          = {10.1016/j.asoc.2021.107286},
  journal      = {Applied Soft Computing},
  pages        = {107286},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time hierarchical risk assessment for UAVs based on recurrent fusion autoencoder and dynamic FCE: A hybrid framework},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum-inspired ensemble approach to multi-attributed and
multi-agent decision-making. <em>ASOC</em>, <em>106</em>, 107283. (<a
href="https://doi.org/10.1016/j.asoc.2021.107283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making is the process of selecting a belief among several other candidate beliefs. The selected belief is usually regarded as a decision while the agent that performs the decision-making process is called as a decision-maker. The decision-making process that makes use of multiple attribute cue pattern as well as multiple decision makers to select a belief is called as multi-attribute and multi-agent decision making. This research proposes quantum inspired approach to multi-attribute and multi-agent decision making process considering the ability of quantum theories to model several complex human cognitive processes as reported in the literature. The main contributions of this work are: a) quantum object based encoding of multi-attribute cue pattern b) ensemble based representation of multi-agent decision making. Further, an analogy between proposed work and double slit experiment to illustrate the interference effect in decision making is also presented. The novelty of this work lies in: a) proposing a quantum inspired ensemble approach to model multi-attribute and multi-agent decision making b) the proposed work outperforms the quantum approach as well as ensemble approach to the decision making with Pima Indiana Diabetes (PID) dataset. The proposed work yields a best accuracy of 90.5\% with PID dataset.},
  archive      = {J_ASOC},
  author       = {Ishwarya M.S. and Aswani Kumar Cherukuri},
  doi          = {10.1016/j.asoc.2021.107283},
  journal      = {Applied Soft Computing},
  pages        = {107283},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum-inspired ensemble approach to multi-attributed and multi-agent decision-making},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-criteria procedure in new product development using
different qualitative scales. <em>ASOC</em>, <em>106</em>, 107279. (<a
href="https://doi.org/10.1016/j.asoc.2021.107279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new multi-criteria procedure is devised for new product development decision-making made from survey data. Groups of panelists evaluate several product categories regarding different criteria, each one through a specific qualitative scale, which ultimately will guide decision-makers to develop a new product in a specific category. These qualitative scales are equipped with ordinal proximity measures that collect the perceptions about the proximities between the terms of the scales by means of ordinal degrees of proximity. The linguistic assessments provided by panelists are compared with the highest terms of the corresponding qualitative scales. In order to aggregate the obtained ordinal degrees of proximity, a homogenization process is provided. It avoids any cardinalization procedure in the ordinal proximity measures associated with the ordered qualitative scales used for assessing the alternatives regarding different criteria. Products categories are ranked taking into account the medians of the homogenized ordinal degrees of proximity.},
  archive      = {J_ASOC},
  author       = {José Luis García-Lapresta and Pablo Moreno-Albadalejo and David Pérez-Román and Víctor Temprano-García},
  doi          = {10.1016/j.asoc.2021.107279},
  journal      = {Applied Soft Computing},
  pages        = {107279},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-criteria procedure in new product development using different qualitative scales},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parallel constrained lower confidence bounding approach
for computationally expensive constrained optimization problems.
<em>ASOC</em>, <em>106</em>, 107276. (<a
href="https://doi.org/10.1016/j.asoc.2021.107276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computationally expensive constrained optimization problems (COPs) are very common in real-world engineering designs. This work proposes a parallel constrained lower confidence bounding (PCLCB) approach to solve challenging optimization problems with computationally expensive objective and constraints. In the PCLCB approach, a two-phase constrained lower confidence bounding (CLCB) criterion is proposed to adaptively allocate infill sample points. In the first phase where no feasible solution exists, a lower confidence bounding (LCB) function of constraints is developed to search for feasible points and improve the accuracies of the Kriging models . After a feasible point is found, the CLCB criterion focuses on local search in the second phase to improve the obtained optimum. In this phase, the search is implemented in a limited region where points may be feasible. Furthermore, an enhanced Kriging believer strategy is presented to parallelize the CLCB criterion to generate a batch of candidate points at each iteration. The PCLCB approach is tested on sixteen numerical benchmark cases and compared with the state-of-the-art alternative methods. Results indicate that the performance of PCLCB is highly competitive to the recently published algorithms in terms of efficiency and effectiveness. To verify the capability of PCLCB in solving real-world engineering problems, the lightweight design of micro-aerial vehicle (MAV) fuselage is presented. The weight of the MAV fuselage obtained by the proposed approach is on average 3.5\% less than that of the other alternative methods.},
  archive      = {J_ASOC},
  author       = {Ji Cheng and Ping Jiang and Qi Zhou and Jiexiang Hu and Leshi Shu},
  doi          = {10.1016/j.asoc.2021.107276},
  journal      = {Applied Soft Computing},
  pages        = {107276},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A parallel constrained lower confidence bounding approach for computationally expensive constrained optimization problems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smart water conservation through a machine learning and
blockchain-enabled decentralized edge computing network. <em>ASOC</em>,
<em>106</em>, 107274. (<a
href="https://doi.org/10.1016/j.asoc.2021.107274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting precise water usage corresponding to various beneficial usages is important for optimal and sustainable planning and management of water resources. Due to rapid population growth, there is an urgent need for devising water saving solutions. In this paper, we propose a blockchain based incentivized edge computing framework for water saving using soft computing methodologies . The framework facilitates decision makers in creating awareness among people about water savings in a easily understandable scientific way. Our incentivized blockchain based model uses edge computing at the house nodes of the network to predict the actual usage of a particular household in the locality based on several factors such as number of people, average income of family, profession of the members and previous water demands. By using Feed Forward Networks and Mixture Density Networks, we predict the water usage in terms of input factors and historical usage respectively, thus incorporating machine computing into the framework. With the two values from these methods, a comparison is made with the actual amount of water used by the householders. This research proposes deployment of the smart contract on the blockchain network for efficient and accurate reward distribution. Incentives and rewards are given in the blockchain network to houses with lesser consumption and penalties are imposed when usage crosses predicted and historic usage. The model ensures that accurate incentives are provided to the people in order to motivate them to avoid wastage of water. Results show that the methods used in our work perform better than other relevant networks on a self-synthesized dataset. The proposed methods converge well and show higher spatio-temporal accuracy.},
  archive      = {J_ASOC},
  author       = {Tanvi Thakur and Aryan Mehra and Vikas Hassija and Vinay Chamola and Rallapalli Srinivas and Karunesh K. Gupta and Ajit Pratap Singh},
  doi          = {10.1016/j.asoc.2021.107274},
  journal      = {Applied Soft Computing},
  pages        = {107274},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Smart water conservation through a machine learning and blockchain-enabled decentralized edge computing network},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimized recommendations by user profiling using apriori
algorithm. <em>ASOC</em>, <em>106</em>, 107272. (<a
href="https://doi.org/10.1016/j.asoc.2021.107272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering has been the most straightforward and most preferable approach in the recommender systems . This technique recommends an item to a target user from the preferences of top-k similar neighbors. In a sparse data scenario, the recommendation accuracy of the collaborative filtering degrades significantly due to the limitations of existing various similarity measures. Such constraints offer an open scope for enhancing the accuracy of optimized user-specific recommendations. Many techniques have been utilized for this, like Particle Swarm Optimization and other evolutionary collaborative filtering algorithms . The proposed approach utilizes the Apriori algorithm to form users’ profiles from the items’ ratings and categorical attributes. The user profile creation is performed using the apriori algorithm. The profile of each user involves the likes and disliking of categorical characteristics of objects by users. In the collected MovieLens dataset, the efficiency of the proposed recommendation approach is tested. The comparative results show proof that the proposed novel algorithm outperforms other prominent collaborative filtering algorithms on the MovieLens datasets based on rating prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Pradeep Kumar Singh and Esam Othman and Rafeeq Ahmed and Awais Mahmood and Habib Dhahri and Prasenjit Choudhury},
  doi          = {10.1016/j.asoc.2021.107272},
  journal      = {Applied Soft Computing},
  pages        = {107272},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized recommendations by user profiling using apriori algorithm},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive cuckoo algorithm with multiple search strategies.
<em>ASOC</em>, <em>106</em>, 107181. (<a
href="https://doi.org/10.1016/j.asoc.2021.107181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms are important methods to solve optimization problems and maintaining a balance between the global exploration and local exploitation is crucial to the performance of such algorithms. We propose a self-adaptive multi strategy cuckoo search algorithm (MSACS) based on the cuckoo search algorithm (CS). First, five different search strategies were proposed to calculate the use probability and control parameters by using adaptive strategies to ensure that the algorithm can autonomously adjust according to the change in the functions and iteration times . Second, the performance of the MSACS was tested on 28 common benchmark functions and compared with the performance of several CS algorithms, particle swarm optimization (PSO) algorithms and difference evolution algorithms (DE). MSACS achieved the best results on 17 of these functions and performed well on the remaining 11 functions. Finally, the improved algorithm was applied to the optimization of a ball screw driving system model. By adjusting the dimensionless input velocity function, the peak acceleration of screw is reduced and the peak acceleration of crank angle is reasonable.},
  archive      = {J_ASOC},
  author       = {Shuzhi Gao and Yue Gao and Yimin Zhang and Tianchi Li},
  doi          = {10.1016/j.asoc.2021.107181},
  journal      = {Applied Soft Computing},
  pages        = {107181},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive cuckoo algorithm with multiple search strategies},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new approach for computer-aided detection of coronavirus
(COVID-19) from CT and x-ray images using machine learning methods.
<em>ASOC</em>, <em>105</em>, 107323. (<a
href="https://doi.org/10.1016/j.asoc.2021.107323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 outbreak has been causing a global health crisis since December 2019. Due to this virus declared by the World Health Organization as a pandemic, the health authorities of the countries are constantly trying to reduce the spread rate of the virus by emphasizing the rules of masks, social distance, and hygiene. COVID-19 is highly contagious and spreads rapidly globally and early detection is of paramount importance. Any technological tool that can provide rapid detection of COVID-19 infection with high accuracy can be very useful to medical professionals. The disease findings on COVID-19 images, such as computed tomography (CT) and X-rays, are similar to other lung infections, making it difficult for medical professionals to distinguish COVID-19. Therefore, computer-aided diagnostic solutions are being developed to facilitate the identification of positive COVID-19 cases. The method currently used as a gold standard in detecting the virus is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Due to the high false-negative rate of this test and the delays in the test results, alternative solutions are sought. This study was conducted to investigate the contribution of machine learning and image processing to the rapid and accurate detection of COVID-19 from two of the most widely used different medical imaging modes, chest X-ray and CT images. The main purpose of this study is to support early diagnosis and treatment to end the coronavirus epidemic as soon as possible. One of the primary aims of the study is to provide support to medical professionals who are most worn out and working under intense stress during COVID-19 through smart learning methods and image classification models. The proposed approach was applied to three different public COVID-19 data sets and consists of five basic steps: data set acquisition, pre-processing, feature extraction, dimension reduction, and classification stages. Each stage has its sub-operations. The proposed model performs in considerable levels of COVID-19 detection for dataset-1 (CT), dataset-2 (X-ray) and dataset-3 (CT) with the accuracy of 89.41\%, 99.02\%, 98.11\%, respectively. On the other hand, in the X-ray data set, an accuracy of 85.96\% was obtained for COVID-19 (+), COVID-19 (-), and those with Pneumonia but not COVID-19 classes. As a result of the study, it has been shown that COVID-19 can be detected with a high success rate in about less than one minute with image processing and classical learning methods. In the light of the findings, it is possible to say that the proposed system will help radiologists in their decisions, will be useful in the early diagnosis of the virus, and can distinguish pneumonia caused by the COVID-19 virus from the pneumonia of other diseases.},
  archive      = {J_ASOC},
  author       = {Ahmet Saygılı},
  doi          = {10.1016/j.asoc.2021.107323},
  journal      = {Applied Soft Computing},
  pages        = {107323},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new approach for computer-aided detection of coronavirus (COVID-19) from CT and X-ray images using machine learning methods},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modes decomposition forecasting approach for
ultra-short-term wind speed. <em>ASOC</em>, <em>105</em>, 107303. (<a
href="https://doi.org/10.1016/j.asoc.2021.107303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate forecasting of ultra-short-term wind speed is of great significance in theory and practice. This paper proposes a modes decomposition forecasting approach based on adaptive variational mode decomposition and weighted combination models for ultra-short-term wind speed. First, an adaptive variational mode decomposition algorithm is used for decomposing the original ultra-short-term wind speed time series into several modal components. Second, auto regressive integrated moving average, support vector machine and the improved long short-term memory are determined as forecasting models of different components by Hurst exponent analysis. Then, an improved particle swarm optimization algorithm is proposed to optimize the weight coefficient of each forecasting model. Finally, the final forecasted value is obtained by multiplying the forecasted value of each sub-forecasting model by their respective weight coefficient. Four groups of measured ultra-short-term wind speed data with 5-minute, 10-minute, 20-minute and 30-minute sampling periods are taken as the research object. Compared with other single or combination forecasting models, the proposed forecasting approach has higher prediction accuracy and more promising prediction performance for ultra-short-term wind speed.},
  archive      = {J_ASOC},
  author       = {Zhongda Tian},
  doi          = {10.1016/j.asoc.2021.107303},
  journal      = {Applied Soft Computing},
  pages        = {107303},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modes decomposition forecasting approach for ultra-short-term wind speed},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Chi-MFlexDT: Chi-square-based multi flexible fuzzy decision
tree for data stream classification. <em>ASOC</em>, <em>105</em>,
107301. (<a href="https://doi.org/10.1016/j.asoc.2021.107301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a number of classification algorithms using fuzzy decision trees have been proposed. A key issue encountered in all of these algorithms is how to select the division feature. In traditional algorithms, it is common to use the information gain criterion. There is a claim that the use of information-based criteria leads to an unfair selection of features that have large number of values while not being suitable for division. This problem causes to decrease accuracy of classification in stream data. However, other algorithms for classifying stream data and extracting fuzzy rules have been proposed but the problem of unfairly selecting features with large amount of values still persists. This paper, present a new algorithm to solve problems of selecting splitting feature in fuzzy decision tree for classification of stream data. In proposed algorithm, we extend multi flexible fuzzy decision tree (MFlexDT) with chi-square based fuzzy partitioning of values. In this paper, we also extend traditional chi-square statistical to fuzzy chi-square statistical data distribution. In evaluating the proposed algorithm, tree depth and accuracy are two factors that affect performance. Experimental results show that the new algorithm has been able to improve the performance of other algorithms.},
  archive      = {J_ASOC},
  author       = {Farnaz Mahan and Maryam Mohammadzad and Seyyed Meysam Rozekhani and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2021.107301},
  journal      = {Applied Soft Computing},
  pages        = {107301},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chi-MFlexDT: Chi-square-based multi flexible fuzzy decision tree for data stream classification},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iteration-based parameter identification and its
applications about distributed parameter systems. <em>ASOC</em>,
<em>105</em>, 107300. (<a
href="https://doi.org/10.1016/j.asoc.2021.107300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a general problem to parameter identification in distributed parameter systems (DPSs), as well as handles a secure communication issue by the extended identification method. First, an iterative approach is put forward based on the approximator and historical information. Meanwhile, an execution scheme for the iterative approach is provided by an optimization policy . Then, combined with the reference model and measurement data, an extended identification method is applied in secure communication. Subsequently, a quantum-leading-following-based optimization with mutation strategy (MQLFBO) algorithm is proposed as the optimization policy . Next, the global convergence and computational complexity are respectively discussed for MQLFBO algorithm in theory. Finally, simulation experiments are performed on parameter identification for DPSs and its application in secure communication, which verify the fast convergence and high precision of the developed method.},
  archive      = {J_ASOC},
  author       = {Rui-Guo Li and Huai-Ning Wu},
  doi          = {10.1016/j.asoc.2021.107300},
  journal      = {Applied Soft Computing},
  pages        = {107300},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Iteration-based parameter identification and its applications about distributed parameter systems},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discrete grey wolf optimizer for symmetric travelling
salesman problem. <em>ASOC</em>, <em>105</em>, 107298. (<a
href="https://doi.org/10.1016/j.asoc.2021.107298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grey Wolf Optimizer (GWO) is a recently developed population-based metaheuristic algorithm which imitates the behaviour of grey wolves for survival. Initially, GWO was proposed to solve continuous optimization problems where it performed well. In recent years, numerous versions of GWO are available in the literature and GWO has been widely used to solve engineering problems. This paper presents a novel discrete GWO algorithm (D-GWO) to solve complex discrete travelling salesman problem (TSP). The 2-opt algorithm is incorporated into it to improve the performance of the proposed algorithm. To inspect the performance of D-GWO, the results of the proposed algorithm are compared with well-known metaheuristic algorithms such as Bat Algorithm(BA), Discrete Firefly Algorithm , Imperialist Competitive Algorithm and some other classical algorithms over several known TSP instances. In order to obtain unbiased and rigorous comparison, descriptive statistics such as mean and standard deviation are used as well as statistical tests such as Friedman test and Holm’s test are also conducted. The D-GWO is implemented in MATLAB environment. The computational result carried out in this study has shown that D-GWO outperforms significantly over other alternative algorithms.},
  archive      = {J_ASOC},
  author       = {Karuna Panwar and Kusum Deep},
  doi          = {10.1016/j.asoc.2021.107298},
  journal      = {Applied Soft Computing},
  pages        = {107298},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete grey wolf optimizer for symmetric travelling salesman problem},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Communication optimization for efficient dynamic task
allocation in swarm robotics. <em>ASOC</em>, <em>105</em>, 107297. (<a
href="https://doi.org/10.1016/j.asoc.2021.107297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation is a central idea to the usage of swarm robotics . It enables the solution of complex problems with a coordinated execution of simple tasks by a large group of small robot, which together lead to the achievement of the swarm common goal. This coordination is only possible with an efficient task allocation. Inspired by the strategy of the particle swarm optimization algorithm, we propose a novel algorithm called the Clustered Dynamic Task Allocation. This algorithm performs task allocation in a swarm robotic system in a fully distributed manner. It performs a guided search of the allocation space using the concept of adaptive speed. This search process requires information exchange between robots. This robot communication process must be planned carefully so as to achieve two conflicting objectives: the knowledge acquired by a given robot must flow throughout the swarm so that the optimization process may converge yet this communication must be limited so it does not hinder the efficiency of the task allocation process regarding large swarms. This paper proposes the use of a clustered communication topology between the swarm robots, aiming to optimize the underlying communication process, and thus enabling efficient task allocation for large robotic swarms. The results obtained with the cluster-based topology are compared to those obtained with the full mesh-based topology. On average, the results show a clear improvement in terms of execution time and battery charge requirements. Moreover, the performance of the proposed algorithm and the stability of the produced allocation are compared to other well-known models, demonstrating its better applicability to real-world swarm robotics based systems.},
  archive      = {J_ASOC},
  author       = {Nadia Nedjah and Luigi Maciel Ribeiro and Luiza de Macedo Mourelle},
  doi          = {10.1016/j.asoc.2021.107297},
  journal      = {Applied Soft Computing},
  pages        = {107297},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Communication optimization for efficient dynamic task allocation in swarm robotics},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustness and performance of deep reinforcement learning.
<em>ASOC</em>, <em>105</em>, 107295. (<a
href="https://doi.org/10.1016/j.asoc.2021.107295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (DRL) has recently obtained considerable attentions. It empowers Reinforcement Learning (RL) with Deep Learning (DL) techniques to address various difficult tasks. In this paper, a novel approach called the Genetic Algorithm of Neuron Coverage (GANC) is proposed. It is motivated for improving the robustness and performance of a DRL network. The GANC uses Genetic Algorithm (GA) to maximise the Neuron Coverage (NC) of a DRL network by producing augmented inputs. We apply this method in the self-driving car applications, where it is crucial to accurately provide a correct decision for different road tracking views. We evaluate our method on the SYNTHIA-SEQS-05 databases in four different driving environments. Our outcomes are very promising – the best driving accuracy reached 97.75\% – and are superior to the state-of-the-art results.},
  archive      = {J_ASOC},
  author       = {Raid Rafi Omar Al-Nima and Tingting Han and Saadoon Awad Mohammed Al-Sumaidaee and Taolue Chen and Wai Lok Woo},
  doi          = {10.1016/j.asoc.2021.107295},
  journal      = {Applied Soft Computing},
  pages        = {107295},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robustness and performance of deep reinforcement learning},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive elitist-ant system for solving combinatorial
optimization problems. <em>ASOC</em>, <em>105</em>, 107293. (<a
href="https://doi.org/10.1016/j.asoc.2021.107293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates the effects achieved by using an adaptive elite pool over a basic population-based approach, such as, the ant system approach, to different combinatorial optimization problems (COPs). Optimization methods are generally considered a good methodology for solving COPs, while population-based approaches are considered better for exploring the search space and local-based approaches are considered better for exploiting the search space. However, some of these approaches are also hybridized in order to balance between exploiting and exploring the search space. Using an adaptive elite pool offers further balance while also adaptively tuning the importance parameter. Taken together, these three processes may improve and enhance the efficiency of any given search. In this work, three COPs (i.e. CVRP, STSP and 0-1 MKP) are used as test domains. In order to evaluate the effectiveness of using an adaptive elite pool, a comparison is made among the original ant system algorithm, the hybrid Elitist-Ant system algorithm, and other approaches drawn from the literature. Experimental results indicate that using an adaptive elite pool enhances the algorithm and is often able to produce better-quality solutions than other approaches. Where the adaptive elitist-ant system approach obtained 10 out of 14 best known results for CVRP, 20 out of 29 optimal solution results for STSP, 9 out of 11 best known results for 0-1 MKP , and generally better than other individuals approaches in term of better, worse, and similar results.},
  archive      = {J_ASOC},
  author       = {Anmar Abuhamdah},
  doi          = {10.1016/j.asoc.2021.107293},
  journal      = {Applied Soft Computing},
  pages        = {107293},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive elitist-ant system for solving combinatorial optimization problems},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Priority weights acquisition of linear uncertain preference
relations and its application in the ranking of online shopping
platforms. <em>ASOC</em>, <em>105</em>, 107292. (<a
href="https://doi.org/10.1016/j.asoc.2021.107292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linear uncertain preference relations (LUPRs) uses an uncertain variable to represent the decision-maker’s pairwise comparison judgment of the scheme. The variable is subject to the linear uncertain distribution, which is the extension of the traditional fuzzy preference relations (FPRs) and interval fuzzy preference relations (IFPRs). This paper proposes a group decision modeling problem, constructs the priority weights acquisition models of the additively and multiplicatively consistent LUPRs, and especially solves the problem that the weight solution is negative value or no solution by using traditional methods to acquire weights in consistent FPRs and IFPRs. Based on these two types of consistent structure of LUPRs, this study constructs the crisp number, interval number weight solving models of LUPRs and the group decision ranking models with LUPRs. The results show that these new models are suitable for solving the weight vector of traditional FPRs and IFPRs. The case of online shopping platform selection compares the results obtained by various methods of calculating weights, further illustrating the effectiveness and rationality of the new methods.},
  archive      = {J_ASOC},
  author       = {Weiwei Guo and Zaiwu Gong and Enrique Herrera-Viedma and Qingsheng Li},
  doi          = {10.1016/j.asoc.2021.107292},
  journal      = {Applied Soft Computing},
  pages        = {107292},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Priority weights acquisition of linear uncertain preference relations and its application in the ranking of online shopping platforms},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting china’s sovereign CDS with a decomposition
reconstruction strategy. <em>ASOC</em>, <em>105</em>, 107291. (<a
href="https://doi.org/10.1016/j.asoc.2021.107291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precisely predicting sovereign credit default swaps (CDS) is important for preventing sovereign risks. However, sovereign CDS exhibit complex nonlinear characteristics, which make accurate predictions challenging. As a result, this paper proposes a hybrid ensemble forecasting framework based on a decomposition reconstruction strategy that considers multiple factors. The complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) is used to decompose the sovereign CDS into different components, and the reconstruction method based on sample entropy is employed to reconstruct the components into trend items, market fluctuation items and noise items. Motivated by the features of components reconstructed, we utilize the autoregressive integrated moving average (ARIMA) and relevance vector machine (RVM) models to predict each component. Finally, the forecasting results of the three sub-sequences are integrated as the final forecast. For verification, China’s 5-year daily sovereign CDS is used as sample data, and the empirical results reveal that the proposed model performs better than all benchmark models in terms of forecasting accuracy , including horizon and directional. Furthermore, the results indicate that the proposed model can be regarded as an effective and promising tool for forecasting China’s sovereign CDS.},
  archive      = {J_ASOC},
  author       = {Jianping Li and Jun Hao and Xiaolei Sun and Qianqian Feng},
  doi          = {10.1016/j.asoc.2021.107291},
  journal      = {Applied Soft Computing},
  pages        = {107291},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting china’s sovereign CDS with a decomposition reconstruction strategy},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A computational tool for trend analysis and forecast of the
COVID-19 pandemic. <em>ASOC</em>, <em>105</em>, 107289. (<a
href="https://doi.org/10.1016/j.asoc.2021.107289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a methodology and a computational tool to study the COVID-19 pandemic throughout the world and to perform a trend analysis to assess its local dynamics. Mathematical functions are employed to describe the number of cases and demises in each region and to predict their final numbers, as well as the dates of maximum daily occurrences and the local stabilization date. The model parameters are calibrated using a computational methodology for numerical optimization . Trend analyses are run, allowing to assess the effects of public policies. Easy to interpret metrics over the quality of the fitted curves are provided. Country-wise data from the European Centre for Disease Prevention and Control (ECDC) concerning the daily number of cases and demises around the world are used, as well as detailed data from Johns Hopkins University and from the Brasil.io project describing individually the occurrences in United States counties and in Brazilian states and cities, respectively. U. S. and Brazil were chosen for a more detailed analysis because they are the current focus of the pandemic. Illustrative results for different countries, U. S. counties and Brazilian states and cities are presented and discussed. The main contributions of this work lie in (i) a straightforward model of the curves to represent the data, which allows automation of the process without requiring interventions from experts; (ii) an innovative approach for trend analysis, whose results provide important information to support authorities in their decision-making process; and (iii) the developed computational tool, which is freely available and allows the user to quickly update the COVID-19 analyses and forecasts for any country, United States county or Brazilian state or city present in the periodic reports from the authorities.},
  archive      = {J_ASOC},
  author       = {Henrique Mohallem Paiva and Rubens Junqueira Magalhães Afonso and Fabiana Mara Scarpelli de Lima Alvarenga Caldeira and Ester de Andrade Velasquez},
  doi          = {10.1016/j.asoc.2021.107289},
  journal      = {Applied Soft Computing},
  pages        = {107289},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A computational tool for trend analysis and forecast of the COVID-19 pandemic},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Short-term wind power prediction based on EEMD–LASSO–QRNN
model. <em>ASOC</em>, <em>105</em>, 107288. (<a
href="https://doi.org/10.1016/j.asoc.2021.107288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing utilization of wind generation in power system , the improvement of wind power forecasting precision is attached vital importance. Owing to the stochastic and intermittent nature of wind power, the conventional methods no longer ensure sufficient accuracy of wind power prediction in majority of scenarios. Motivated by recent advancements of ensemble methods based on decomposition technologies, a novel ensemble method based on ensemble empirical mode decomposition (EEMD) and least absolute shrinkage and selection operator–quantile regression neural network (LASSO–QRNN) model for forecasting wind power is proposed in this paper. The model is an ingenious integration of data preprocessing technology, feature selection method, prediction model and data post-processing technology. Thereinto, EEMD is exploited to convert intricate and irregular wind power time series into a collection of subseries relatively easy to analyze; LASSO regression is combined with QRNN model to realize the filtering of important variables and provide more comprehensive and robust prediction results; the KDE method reprocesses the prediction results, greatly improves the prediction accuracy and effectively quantifies the uncertainty of the forecasting process. The suggested model and several benchmark models have been implemented on six wind power datasets, two are gathered from a wind farm in Spain and four from a competition to demonstrate the superiorities of the model proposed in this paper. The compared results reveal that the proposed method has adequate capacity to enhance the performance of wind power forecasting, measure and reduce the uncertainty of prediction process.},
  archive      = {J_ASOC},
  author       = {Yaoyao He and Yun Wang},
  doi          = {10.1016/j.asoc.2021.107288},
  journal      = {Applied Soft Computing},
  pages        = {107288},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term wind power prediction based on EEMD–LASSO–QRNN model},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mediative fuzzy logic mathematical model: A contradictory
management prediction in COVID-19 pandemic. <em>ASOC</em>, <em>105</em>,
107285. (<a href="https://doi.org/10.1016/j.asoc.2021.107285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a model based on mediative fuzzy logic in this COVID-19 pandemic. COVID-19 (novel coronavirus respiratory disease) has become a pandemic now and the whole world has been affected by this disease. Different methodologies and many prediction techniques based on various models have been developed so far. In the present article, we have developed a mediative fuzzy correlation technique based on the parameters for COVID-19 patients from different parts of India. The proposed mediative fuzzy correlation technique provides the relation between the increments of COVID-19 positive patients in terms of the passage of increment with respect to time. The peaks of infected cases in connection with the other condition are estimated from the available data. The mediative fuzzy logic mathematical model can be utilized to find a good fit or a contradictory model for any pandemic model. The proposed approach to the prediction in COVID-19 based on mediative fuzzy logic has produced promising results for the continuous contradictory prediction in India.},
  archive      = {J_ASOC},
  author       = {M.K. Sharma and Nitesh Dhiman and Vandana and Vishnu Narayan Mishra},
  doi          = {10.1016/j.asoc.2021.107285},
  journal      = {Applied Soft Computing},
  pages        = {107285},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mediative fuzzy logic mathematical model: A contradictory management prediction in COVID-19 pandemic},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An intelligent fault diagnosis method for roller bearing
using symplectic hyperdisk matrix machine. <em>ASOC</em>, <em>105</em>,
107284. (<a href="https://doi.org/10.1016/j.asoc.2021.107284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support matrix machine (SMM) is a new and effective classification method, which has been applied in the field of image processing . In this paper, an improved SMM called symplectic hyperdisk matrix machine (SHMM) is proposed and applied to the roller bearing fault diagnosis. In SHMM, the symplectic geometry similarity transformation (SGST) is used to obtain the dimensionless feature matrix, which protects the signal structure information while weakening the interference of noise. Then, different types of hyperdisks are constructed to divide different types of data, several more realistic hyperdisk prediction models can be obtained and the problem of under estimation is avoided. In order to fully mine the spatial structure information, the feature matrix is mapped to the high-dimensional space by kernel technology, and the decision function is established by using the structure information hidden of the input matrix in the SHMM. Experimental results of three datasets of roller bearing show that, compared with symplectic geometry matrix machine (SGMM), SMM, support vector machine (SVM) and radial basis function (RBF) neural network methods, the proposed SHMM has good application effect in roller bearing fault diagnosis.},
  archive      = {J_ASOC},
  author       = {Haiyang Pan and Jinde Zheng},
  doi          = {10.1016/j.asoc.2021.107284},
  journal      = {Applied Soft Computing},
  pages        = {107284},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intelligent fault diagnosis method for roller bearing using symplectic hyperdisk matrix machine},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive comparison of recent developed
meta-heuristic algorithms for streamflow time series forecasting
problem. <em>ASOC</em>, <em>105</em>, 107282. (<a
href="https://doi.org/10.1016/j.asoc.2021.107282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hydrological models play a crucial role in water planning and decision making. Machine Learning-based models showed several drawbacks for frequent high and a wide range of streamflow records. These models also experience problems during the training process such as over-fitting or trapping in searching for global optima To overcome these limitations, the current study attempts to hybridize the recently developed physics-inspired metaheuristic algorithms (MHAs) such as Equilibrium Optimization (EO), Henry Gases Solubility Optimization (HGSO), and Nuclear Reaction Optimization(NRO) with Multi-layer Perceptron (MLP). These models’ accuracy will be inspected to solve the streamflow forecasting problem where the streamflow dataset was collected through 130 years from a station located on the High Aswan Dam (HAD). The performance of proposed models then will be compared with two traditional neural network models(MLP and RNN), and nine well-known hybrid MLP-based models belong to the different branches of the metaheuristic field (evolutionary group, swarm group, and physics group). The internal parameters of the proposed models will be initialized and optimized. Different performance metrics will be used to examine the performance of the proposed models. The stability of the proposed models and the convergence speed will be evaluated. Finally, ranking these models based on different performance evaluations will be carried out. The results show that the models in the group of Physics-MLP are more reliable in capturing the streamflow patterns, followed by the Swarm-MLP group and then by the Evolutionary-MLP group. Finally, among the all employed methods, the NRO has the best accuracy with the lowest RMSE(2.35), MAE(1.356), MAPE(16.747), and the highest WI(0.957), R(0.924), and confidence in forecasting the streamflow of Aswan High Dam. It can be concluded that augmenting the NRO algorithm with MLP can be a reliable tool in forecasting the monthly streamflow with a high level of precision, speed convergence , and high constancy level.},
  archive      = {J_ASOC},
  author       = {Ali Najah Ahmed and To Van Lam and Nguyen Duy Hung and Nguyen Van Thieu and Ozgur Kisi and Ahmed El-Shafie},
  doi          = {10.1016/j.asoc.2021.107282},
  journal      = {Applied Soft Computing},
  pages        = {107282},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive comparison of recent developed meta-heuristic algorithms for streamflow time series forecasting problem},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian optimization algorithm based support vector
regression analysis for estimation of shear capacity of FRP reinforced
concrete members. <em>ASOC</em>, <em>105</em>, 107281. (<a
href="https://doi.org/10.1016/j.asoc.2021.107281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of fiber-reinforced polymer (FRP) rebars in lieu of steel rebars has led to some deviations in the shear behavior of concrete members. Several methods have been proposed to forecast the shear capacity of such members. Nonetheless, there are differences in the methods of considering the various parameters affecting shear capacity, and some of them provide widely scattered and conservative results. This paper presents a hybrid of the Bayesian optimization algorithm (BOA) and support vector regression (SVR) as a novel modeling tool for the prediction of the shear capacity of FRP-reinforced members with no stirrups. For this purpose, a large dataset of simply supported beams and unidirectional slabs reinforced with FRP were utilized. The model performance was assessed using several statistical performance indicators and compared with the Japan Society of Civil Engineers (JSCE), British Institution of Structural Engineers (BISE), Canadian Standard Association (CSA), and American Concrete Institute (ACI) design codes and guidelines, as well as some other artificial intelligence (AI) models. For development of the model, all the hyperparameters, i.e., kernel function type, epsilon, box constraint, and kernel scale, were optimized using the BOA technique. The k -fold cross validation approach was utilized to avoid overfitting of the model. It was found that the mean, median, standard deviation, minimum, maximum, and interquartile range of the developed hybrid model predictions are very close to the experimental results. The predicted results overlap the experimental data with a coefficient of determination of 95.5\%. The plot of relative deviations and residual plots are scattered around the zero reference line with low deviation, which indicates that the model is reliable and valid. The error terms (e.g., mean absolute error , root mean square error) obtained for all specimens were 4.85 and 11.03, which are very low values. The correlation coefficient ( R ) and fractional bias ( FB ) were found to be 0.977 and 0.0033, which are very close to 1 and 0, respectively, thus implying a reliable prediction. The comparative investigations with other codes and guidelines show that the hybrid BOA–SVR model predictions are more accurate and robust than those of the other models.},
  archive      = {J_ASOC},
  author       = {Md Shah Alam and N. Sultana and S.M. Zakir Hossain},
  doi          = {10.1016/j.asoc.2021.107281},
  journal      = {Applied Soft Computing},
  pages        = {107281},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian optimization algorithm based support vector regression analysis for estimation of shear capacity of FRP reinforced concrete members},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised learning-based artificial bee colony for
minimizing non-value-adding operations. <em>ASOC</em>, <em>105</em>,
107280. (<a href="https://doi.org/10.1016/j.asoc.2021.107280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced analytics benefits lean manufacturing by upgrading the scheduling problems into operational strategic tools that help minimize non-value-adding activities. Considering production environments with prevalent setup operations, this study develops an Unsupervised Learning-based Artificial Bee Colony (ULABC) algorithm to improve the effectiveness of minimizing idle times in unrelated parallel machine production settings. For this purpose, the k -means method is integrated into the approximation algorithm to address sequence-dependent setup operations. An exemplary case from the forging industry is provided to evaluate the performance of the ULABC algorithm. Reducing setup times through effective job clustering by the learning mechanism, it is shown that the solution quality is significantly improved in large-scale benchmark tests with 16 and 24 percentages of reduction in the makespan value of instances requiring short and long setup operations, respectively. The statistical analysis confirms the significance of the resulting improvements. This improvement is expected to be even more substantial when very-large industry-scale problems are solved. Overall, this study narrows the gap between scheduling theory and modern industrial applications through applications of advanced analytics in the production management context.},
  archive      = {J_ASOC},
  author       = {Chen-Yang Cheng and Pourya Pourhejazy and Kuo-Ching Ying and Chen-Fang Lin},
  doi          = {10.1016/j.asoc.2021.107280},
  journal      = {Applied Soft Computing},
  pages        = {107280},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised learning-based artificial bee colony for minimizing non-value-adding operations},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective artificial electric field optimization
algorithm for allocation of wind turbines in distribution systems.
<em>ASOC</em>, <em>105</em>, 107278. (<a
href="https://doi.org/10.1016/j.asoc.2021.107278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents wind turbine allocation in distribution systems to reduce active power loss and voltage deviations using a multi-objective Artificial Electric Field Algorithm (MOAEFA). The proposed method is a mathematical algorithm which is suitably capable to find optimal solutions based on the Pareto solution set using a fuzzy decision-making method. The proposed problem is implemented on 10, 33 and 69 bus IEEE radial distribution networks . The installation location, size and power factors of wind turbines are determined optimally using the MOAEFA method. Single and multi-objective allocation problem of wind turbines is implemented using AEFA, GWO, PSO and MOAEFA, MOGWO, MOPSO methods. The obtained the results of AEFA method achieves less power loss and voltage deviations compared to the conventional GWO and PSO methods. Moreover, the results of multi-objective fuzzy allocation show that there is a compromise between single-objective results and MOAEFA method provides better performance given the loss power and voltage deviation reduction in distribution networks . Furthermore, MOAEFA method has found a better voltage profile in the allocation of wind turbines in the distribution network compared to the other methods. The performance comparison between MOAEFA method and the previous methods given in the literature verifies the superiority of the MOAEFA method.},
  archive      = {J_ASOC},
  author       = {Amirreza Naderipour and Zulkurnain Abdul-Malek and Mohd Wazir Bin Mustafa and Josep M. Guerrero},
  doi          = {10.1016/j.asoc.2021.107278},
  journal      = {Applied Soft Computing},
  pages        = {107278},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective artificial electric field optimization algorithm for allocation of wind turbines in distribution systems},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Customizable and committee data mining framework for stock
trading. <em>ASOC</em>, <em>105</em>, 107277. (<a
href="https://doi.org/10.1016/j.asoc.2021.107277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a customizable and committee data mining (DM) framework for stock trading. The underlying conjecture is that different stocks may have different trading properties. A well-performing DM algorithm for a particular stock may not work well for other stocks. Therefore, the trading method designed for a particular stock should be customizable . To systematically obtain a customizable solution, we propose a research framework that can accommodate a large number of trading classifiers. Then, we select the well-performing ones to form a committee to make trading decisions for a particular stock. A committee decision is made because the best-performing trading classifier in a particular scenario (called validation phase) may not be the best one in another scenario (called testing phase). Therefore, some other well-performing classifiers afford their own advantages. We therefore extend the ensemble concept to form a customizable committee of well-performing classifiers that makes a trading decision by voting. To justify the idea of a customizable committee classifier, we slightly modify the framework and develop another trading method called customizable champion classifier in which only the best-performing classifier in the validation phase is used for stock trading. Using 12 stocks listed in the Taiwan Stock Exchange (TWSE) as test examples, the proposed customizable committee method outperforms the customizable champion method as well as six benchmarks methods (the buy-and-hold strategy and five popular technical analysis methods). To fairly compare a stock-trading method against the buy-and-hold strategy, two issues that have been rarely noted in literature are addressed in this study.},
  archive      = {J_ASOC},
  author       = {Hui-Chih Hung and Yu-Jen Chuang and Muh-Cherng Wu},
  doi          = {10.1016/j.asoc.2021.107277},
  journal      = {Applied Soft Computing},
  pages        = {107277},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Customizable and committee data mining framework for stock trading},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DJAYA: A discrete jaya algorithm for solving traveling
salesman problem. <em>ASOC</em>, <em>105</em>, 107275. (<a
href="https://doi.org/10.1016/j.asoc.2021.107275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jaya algorithm is a newly proposed stochastic population-based metaheuristic optimization algorithm to solve constrained and unconstrained continuous optimization problems . The main difference of this algorithm from the similar approaches, it uses best and worst solution in the population in order improve the intensification and diversification of the population, and this provides discovering potential solutions on the search space of the optimization problem. In this study, we propose discrete versions of the Jaya by using two major modifications in the algorithm. First is to generate initial solutions by using random permutations and nearest neighborhood approach to create population. Second is the update rule of the basic Jaya algorithm rearranged to solve discrete optimization problems. Due to characteristics of the discrete optimization problem, eight transformation operators are used for the discrete variants of the proposed algorithm. Based on these modifications, the discrete Jaya algorithm, called DJAYA, has been applied to solve fourteen different symmetric traveling salesman problem , which is one of the famous discrete problems in the discrete optimization. In order to improve the obtained best solution from DJAYA, 2-opt heuristic is also applied to the best solution of DJAYA. Once population size, search tendency and the other parameters of the proposed algorithm have been analyzed, it has been compared with the state-of-art algorithms and their variants, such as Simulated Annealing (SA), Tree-Seed Algorithm (TSA), State Transition Algorithm (STA) Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), Artificial Bee Colony (ABC), Genetic Algorithm (GA) and Black Hole (BH). The experimental results and comparisons show that the proposed DJAYA is highly competitive and robust optimizer for the problem dealt with the study.},
  archive      = {J_ASOC},
  author       = {Mesut Gunduz and Murat Aslan},
  doi          = {10.1016/j.asoc.2021.107275},
  journal      = {Applied Soft Computing},
  pages        = {107275},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DJAYA: A discrete jaya algorithm for solving traveling salesman problem},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Level set approach based on parzen window and floor of log
for edge computing object segmentation in digital images. <em>ASOC</em>,
<em>105</em>, 107273. (<a
href="https://doi.org/10.1016/j.asoc.2021.107273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific research into methodologies and algorithms to improve support for medical diagnoses remains in high demand on the agenda. Computer-aided diagnostic systems that use the Internet of Things (IoT) enable greater accessibility and integration between patients and specialists. One of the steps covered by IoT systems is the segmentation of Regions of Interest (ROI) in digital images. The challenge to segment these ROI in IoT systems is to apply methods that have low computational and storage costs with reliable results. Thus this work proposes the use of edge computing with a new approach based on active contours to target the ROI in medical images called FLog Parzen Level Set (FPLS). The method can be divided into two stages. First is the initialization of the seed point using Parzen Window and clusterization using Floor of Log. Second the growth and refinement of the region with probabilistic estimates of the regional contour using the Parzen Window. The proposed method was evaluated using the metrics of Accuracy, Precision, Sensitivity, Specificity, Matthews Coefficient Correlation, Hausdorff distance , Dice, and Jaccard Similarity Coefficient. The stable and satisfactory results can be highlighted despite the low computational times and costs. The proposed method was submitted to stroke, lung, and skin disease datasets. The proposed method achieved the fastest mean segmentation time of 1.64s for the three datasets used in this work. The method obtained the highest values for Sensitivity (98.57\%), Accuracy (98.77\%) and MCC (94.73\%) in the stroke dataset, the lowest value for Hausdorff distance (4.24) in the lung dataset, and the highest Dice Coefficient value (92.49\%) in the skin dataset. In conclusion, the proposed method is a promising tool for an edge computing system that segments regions of interest with the high precision of a level set and a fast convergence rate.},
  archive      = {J_ASOC},
  author       = {Elizângela de Souza Rebouças and Fátima Nelsizeuma Sombra de Medeiros and Regis Cristiano P. Marques and João Victor S. Chagas and Matheus T. Guimarães and Lucas O. Santos and Aldisio G. Medeiros and Solon A. Peixoto},
  doi          = {10.1016/j.asoc.2021.107273},
  journal      = {Applied Soft Computing},
  pages        = {107273},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Level set approach based on parzen window and floor of log for edge computing object segmentation in digital images},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification and feature transformation with fuzzy
cognitive maps. <em>ASOC</em>, <em>105</em>, 107271. (<a
href="https://doi.org/10.1016/j.asoc.2021.107271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Cognitive Maps (FCMs) are considered a soft computing technique combining elements of fuzzy logic and recurrent neural networks . They found multiple application in such domains as modeling of system behavior, prediction of time series, decision making and process control. Less attention, however, has been turned towards using them in pattern classification. In this work we propose an FCM based classifier with a fully connected map structure. In contrast to methods that expect reaching a steady system state during reasoning, we chose to execute a few FCM iterations (steps) before collecting output labels. Weights were learned with a gradient algorithm and logloss or cross-entropy were used as the cost function. Our primary goal was to verify, whether such design would result in a descent general purpose classifier, with performance comparable to off the shelf classical methods. As the preliminary results were promising, we investigated the hypothesis that the performance of d d -step classifier can be attributed to a fact that in previous d − 1 d−1 steps it transforms the feature space by grouping observations belonging to a given class, so that they became more compact and separable. To verify this hypothesis we calculated three clustering scores for the transformed feature space. We also evaluated performance of pipelines built from FCM-based data transformer followed by a classification algorithm . The standard statistical analyzes confirmed both the performance of FCM based classifier and its capability to improve data. The supporting prototype software was implemented in Python using TensorFlow library.},
  archive      = {J_ASOC},
  author       = {Piotr Szwed},
  doi          = {10.1016/j.asoc.2021.107271},
  journal      = {Applied Soft Computing},
  pages        = {107271},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification and feature transformation with fuzzy cognitive maps},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning based simulation optimisation for urban
routing problems. <em>ASOC</em>, <em>105</em>, 107269. (<a
href="https://doi.org/10.1016/j.asoc.2021.107269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real world routing problems, including those in tourism and surveillance, can be formulated as team orienteering problems. The goal in such problems is to maximise the rewards collected by a fleet of vehicles whose routes must be completed within a time limit. This work considers a team orienteering problem set within a traffic simulation. In the stochastic environment of a road network , travel times depend on network structure, the demands of road users, driver behaviour and the congestion that arises from these. As a result travel times are difficult to predict. In this work a learnheuristic solution approach is proposed. Learnheuristics integrate machine learning and optimisation for solving combinatorial problems with inherent parameter learning problems—in this case travel times. The machine learning component is used to predict travel times based on data obtained from a limited budget of traffic simulation runs, a budget that is used within the run-time learnheuristic algorithm. In each iteration of the learnheuristic, the optimisation component utilises the travel time predictions of the machine learning algorithm to rapidly generate candidate solutions. The strongest candidate is tested in the traffic simulator, and the results of which are used to train the machine learning component. In a range of test instances, the effectiveness of different combinations of machine learning and optimisation components are tested. Experiments reveal that different combinations of machine learning and optimisation components produce solutions with different characteristics in terms of total reward and reliability. Local search followed by biased randomisation combined with a neural network was found to be effective in multiple instances. The question of how best to use the run-time of a learnheuristic is also addressed.},
  archive      = {J_ASOC},
  author       = {Christopher Bayliss},
  doi          = {10.1016/j.asoc.2021.107269},
  journal      = {Applied Soft Computing},
  pages        = {107269},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine learning based simulation optimisation for urban routing problems},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neighborhood samples and surrogate assisted multi-objective
evolutionary algorithm for expensive many-objective optimization
problems. <em>ASOC</em>, <em>105</em>, 107268. (<a
href="https://doi.org/10.1016/j.asoc.2021.107268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many surrogate-assisted meta-heuristic algorithms have been proposed for single-objective expensive optimization problems , however, not so much attention has been paid to multi-objective expensive problems, especially for those with more than four objectives. In this paper, we use reference vector guided evolutionary algorithm (RVEA) to select suitable individuals, and radial basis function (RBF) networks are used to estimate the fitness of the original objective function to reduce the computational cost. These suitable individuals are optimized by the RBF network for several iterations. Then in surrogate model management, an infill strategy is proposed to select promising individuals for exact evaluations. Euclidean distance to origin or uncertainty is adaptively considered, according to the convergence degree of the current population in the infill strategy. The approximation uncertainty of each solution is calculated according to its distance to the modeling samples in the decision space. The experimental results on a number of many-objective optimization problems showed that the proposed method is competitive to three state-of-the-art algorithms for solving computationally expensive many-objective optimization problems.},
  archive      = {J_ASOC},
  author       = {Yi Zhao and Jianchao Zeng and Ying Tan},
  doi          = {10.1016/j.asoc.2021.107268},
  journal      = {Applied Soft Computing},
  pages        = {107268},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neighborhood samples and surrogate assisted multi-objective evolutionary algorithm for expensive many-objective optimization problems},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multistart solution-based tabu search for the set-union
knapsack problem. <em>ASOC</em>, <em>105</em>, 107260. (<a
href="https://doi.org/10.1016/j.asoc.2021.107260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NP-hard Set-Union Knapsack Problem is a general model able to formulate a number of practical problems. As a variant of the popular knapsack problem, SUKP is to find a subset of candidate items (an item is composed of several distinct weighted elements) such that a profit function is maximized while a knapsack capacity constraint is satisfied. We investigate for the first time a multistart solution-based tabu search algorithm for solving the problem. The proposed algorithm combines a solution-based tabu search procedure with a multistart strategy to ensure an effective examination of candidate solutions. We report computational results on 60 benchmark instances from the literature, including new best results (improved lower bounds) for 7 large instances. We show additional experiments to shed lights on the roles of the key composing ingredients of the algorithm. The code of the algorithm will be publicly available.},
  archive      = {J_ASOC},
  author       = {Zequn Wei and Jin-Kao Hao},
  doi          = {10.1016/j.asoc.2021.107260},
  journal      = {Applied Soft Computing},
  pages        = {107260},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multistart solution-based tabu search for the set-union knapsack problem},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A graph-based semi-supervised reject inference framework
considering imbalanced data distribution for consumer credit scoring.
<em>ASOC</em>, <em>105</em>, 107259. (<a
href="https://doi.org/10.1016/j.asoc.2021.107259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit scoring has been attracting increasing attention in the Chinese consumer financial industry. Traditional approaches are easily influenced by sample selection bias because they use accepted applicant samples only, while the applicant population also includes rejected applicants. Reject inference is a technique to infer good/bad labels for rejected applicants, which can overcome biases in credit scoring. However, previously proposed reject inference methods usually ignore the imbalanced distribution in accepted data, which means that good applicants are much more than bad ones in most practical consumer loan applications. Both the neglect of rejected data and the imbalanced distribution in accepted data weaken the performance of current credit scoring models. In this paper, we propose a novel reject inference framework that takes into account the imbalanced data distribution for consumer credit scoring. First, we use an advanced graph-based semi-supervised learning algorithm to solve the reject inference problem, which is called label spreading. Second, faced with an imbalanced distribution of good and bad samples in accepted applicants, we conduct imbalanced learning using a modified Synthetic Minority Over-sampling Technique before reject inference. Then, six binary classifiers are studied in our proposed framework for credit scoring modeling. Finally, we present the results of four exact experiments as well as online A/B tests for performance evaluation using data provided by a leading Chinese fintech company. Empirical results indicate that the proposed framework performs better than traditional scoring models across different evaluation metrics , representing a progressive method that promotes credit scoring research as well as improving fintech practices.},
  archive      = {J_ASOC},
  author       = {Yanzhe Kang and Ning Jia and Runbang Cui and Jiang Deng},
  doi          = {10.1016/j.asoc.2021.107259},
  journal      = {Applied Soft Computing},
  pages        = {107259},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A graph-based semi-supervised reject inference framework considering imbalanced data distribution for consumer credit scoring},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting handcrafted facial image manipulations and
GAN-generated facial images using shallow-FakeFaceNet. <em>ASOC</em>,
<em>105</em>, 107256. (<a
href="https://doi.org/10.1016/j.asoc.2021.107256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid progress of sophisticated image editing tools has made it easier to manipulate original face images and create fake media content by putting one’s face to another. In addition to image editing tools, creating natural-looking fake human faces can be easily achieved by Generative Adversarial Networks (GANs). However, malicious use of these new media generation technologies can lead to severe problems, such as the development of fake pornography, defamation, or fraud. In this paper, we introduce a novel Handcrafted Facial Manipulation (HFM) image dataset and soft computing neural network models (Shallow-FakeFaceNets) with an efficient facial manipulation detection pipeline . Our neural network classifier model , Shallow-FakeFaceNet (SFFN), shows the ability to focus on the manipulated facial landmarks to detect fake images. The detection pipeline only relies on detecting fake facial images based on RGB information, not leveraging any metadata, which can be easily manipulated. Our results show that our method achieves the best performance of 72.52\% in Area Under the Receiver Operating Characteristic (AUROC), gaining 3.99\% F1-score and 2.91\% AUROC on detecting handcrafted fake facial images , and 93.99\% on detecting small GAN-generated fake images, gaining 1.98\% F1-score and 10.44\% AUROC compared to the best performing state-of-the-art classifier. This study is targeted for developing an automated defense mechanism to combat fake images used in different online services and applications, leveraging our state-of-the-art hand-crafted fake facial dataset (HFM) and the neural network classifier Shallow-FakeFaceNet (SFFN). In addition, our work presents various experimental results that can help guide better applied soft computing research in the future to effectively combat and detect human and GAN-generated fake face images.},
  archive      = {J_ASOC},
  author       = {Sangyup Lee and Shahroz Tariq and Youjin Shin and Simon S. Woo},
  doi          = {10.1016/j.asoc.2021.107256},
  journal      = {Applied Soft Computing},
  pages        = {107256},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detecting handcrafted facial image manipulations and GAN-generated facial images using shallow-FakeFaceNet},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive online incremental learning for evolving data
streams. <em>ASOC</em>, <em>105</em>, 107255. (<a
href="https://doi.org/10.1016/j.asoc.2021.107255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed growing interests in online incremental learning. However, there are three major challenges in this area. The first major difficulty is concept drift, that is, the probability distribution in the streaming data would change as the data arrives. The second major difficulty is catastrophic forgetting, that is, forgetting what we have learned before when learning new knowledge. The last one we often ignore is the learning of the latent representation. Only good latent representation can improve the prediction accuracy of the model. Our research builds on this observation and attempts to overcome these difficulties. To this end, we propose an Adaptive Online Incremental Learning for evolving data streams (AOIL). We use auto-encoder with the memory module, on the one hand, we obtained the latent features of the input, on the other hand, according to the reconstruction loss of the auto-encoder with memory module, we could successfully detect the existence of concept drift and trigger the update mechanism, adjust the model parameters in time. In addition, we divide features, which are derived from the activation of the hidden layers, into two parts, which are used to extract the common and private features respectively. By means of this approach, the model could learn the private features of the new coming instances, but do not forget what we have learned in the past (shared features), which reduces the occurrence of catastrophic forgetting. At the same time, to get the fusion feature vector we use the self-attention mechanism to effectively fuse the extracted features, which further improved the latent representation learning . Moreover, in order to further improve the robustness of the algorithm, we add the de-noising auto-encoder to original framework. Finally, we conduct extensive experiments on different datasets, and show that the proposed AOIL gets promising results and outperforms other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Si-si Zhang and Jian-wei Liu and Xin Zuo},
  doi          = {10.1016/j.asoc.2021.107255},
  journal      = {Applied Soft Computing},
  pages        = {107255},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive online incremental learning for evolving data streams},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive knowledge transfer in multifactorial evolutionary
algorithm for the clustered minimum routing cost problem. <em>ASOC</em>,
<em>105</em>, 107253. (<a
href="https://doi.org/10.1016/j.asoc.2021.107253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Having tremendous promising impact on society, the Internet of Things refers to the connection of millions of worldwide devices to communicate and exchange data, which requires to aggregate similar devices into clusters for maximizing the network efficiency. Minimum Routing Cost Clustered Tree Problem (CluMRCT) is a lately investigated topic that has significant application towards enhancing the interconnectivity among various devices. Belonging to NP-Hard class, the CluMRCT problem can be solved effectively by a meta-heuristic approach such as Multifactorial Evolutionary Algorithm (MFEA), which can facilitate to find better solutions for multiple problems simultaneously. Recently, an improved framework (called MFEA-II) has been introduced to overcome the weakness of algorithm performance governed by the degree of underlying inter-task synergies in the previous version. Therefore, this paper proposes to apply MFEA-II for solving multiple CluMRCT problems concurrently with the population representation under a probabilistic distribution model adept at online learning. In the proposed algorithm, evolutionary operators are applied in two levels: the first level is to construct a spanning tree in each cluster, while the second one builds a spanning tree for connecting all clusters. To reduce consuming resources, this paper also introduces a new method to calculate the cost of CluMRCT solution in linear time complexity. Numerous types of test instances are implemented to demonstrate the effectiveness of the proposed algorithm when its performance surpassed other state-of-the-art algorithms in most of the datasets.},
  archive      = {J_ASOC},
  author       = {Ta Bao Thang and Nguyen Binh Long and Ngo Viet Hoang and Huynh Thi Thanh Binh},
  doi          = {10.1016/j.asoc.2021.107253},
  journal      = {Applied Soft Computing},
  pages        = {107253},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive knowledge transfer in multifactorial evolutionary algorithm for the clustered minimum routing cost problem},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial perturbation in remote sensing image
recognition. <em>ASOC</em>, <em>105</em>, 107252. (<a
href="https://doi.org/10.1016/j.asoc.2021.107252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have demonstrated that current deep neural networks suffer from small but intentional perturbation during the testing phase of the model. Such perturbations aiming at misclassifying the model’s inputs produce adversarial examples . To explain the origin of adversarial examples , the threat they pose towards deep learning networks, and their transferability property across models learning on the same data distribution, many works have been proposed without reaching any consensus. On the other hand, the study of adversarial examples in remote sensing imagery has gained popularity thanks to the availability of very high spatial resolution images , and the rapid development of deep learning algorithms in image recognition. Although many constructions aiming at mitigating adversarial examples in remote sensing imagery have been proposed, not all of them are realizable for they are based on unrealistic assumptions, or incur a lot of computation overhead. In this paper, we revisit adversarial examples in remote sensing imagery. First, we discuss the origin of adversarial perturbations, the various methods to generate them, as well as propose a framework to construct them under real-world constraints in remote sensing imagery. Second, we investigate the property of adversarial transferability or cross-model generalization, and detail its origin. Third, we revisit the various defense mechanisms to achieve certified robustness against adversarial perturbations in remote sensing imagery. Fourth, we discuss some important trade-offs regarding adversarial examples. Finally, we present the direction for future research aiming at more secure machine learning models against adversarial examples in remote sensing imagery.},
  archive      = {J_ASOC},
  author       = {Shan Ai and Arthur Sandor Voundi Koe and Teng Huang},
  doi          = {10.1016/j.asoc.2021.107252},
  journal      = {Applied Soft Computing},
  pages        = {107252},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial perturbation in remote sensing image recognition},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive large neighborhood search for the green mixed
fleet vehicle routing problem with realistic energy consumption and
partial recharges. <em>ASOC</em>, <em>105</em>, 107251. (<a
href="https://doi.org/10.1016/j.asoc.2021.107251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses a variant of the Electric Vehicle Routing Problem with Mixed Fleet, named as the Green Mixed Fleet Vehicle Routing Problem with Realistic Energy Consumption and Partial Recharges. This problem contains three important characteristics — realistic energy consumption, partial recharging policy, and carbon emissions. An adaptive Large Neighborhood Search heuristic is developed for the problem. Experimental results show that the proposed ALNS finds optimal solutions for most small-scale benchmark instances in a significantly faster computational time compared to the performance of CPLEX solver. Moreover, it obtains high quality solutions for all medium- and large-scale instances under a reasonable computational time. We also perform numerical studies to analyze the potential carbon emission reduction resulted from the proposed model.},
  archive      = {J_ASOC},
  author       = {Vincent F. Yu and Panca Jodiawan and Aldy Gunawan},
  doi          = {10.1016/j.asoc.2021.107251},
  journal      = {Applied Soft Computing},
  pages        = {107251},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive large neighborhood search for the green mixed fleet vehicle routing problem with realistic energy consumption and partial recharges},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral image sparse reconstruction model based on
collaborative multidimensional correlation. <em>ASOC</em>, <em>105</em>,
107250. (<a href="https://doi.org/10.1016/j.asoc.2021.107250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the sparse reconstruction of hyperspectral images based on compressed sensing theory has attracted great attention of the research community. How to better explore the sparsity priori that conforms to the statistical properties and structural properties of the original hyperspectral images is one of the key problems. To address this, in this paper, a novel hyperspectral image sparse reconstruction model based on collaborative multidimensional correlations is proposed. First, considering the high-dimensional and high-redundancy properties of hyperspectral data , the correlations of hyperspectral images are studied from multiple dimensions, except the “spectral correlation” between hyperspectral bands and the “local spatial correlation” within a single band, the “overall nonlocal correlation” is novel defined in band groups with highly correlated spectral features , where the “nonlocal correlation” of the key band in the band group can be used to guide the “nonlocal correlation” of other bands within the group. Then, the sparse measurement models are provided corresponding to the above correlations in different dimensions, at the same time, a band group collaborative sparse model which integrates the multidimensional correlations has been proposed. Finally, the proposed band group collaborative sparse model is used as the sparse constraint to construct the hyperspectral sparse reconstruction framework. Experimental results demonstrate that, the newly defined “global non-local correlation” can effectively enhance the detail information in the reconstruction process of hyperspectral images, which provides the possibility for better analyzing land-cover objects in the subsequent applications.},
  archive      = {J_ASOC},
  author       = {Xianghai Wang and Shun Wang and Yetao Li and Shicheng Xie and Jingzhe Tao and Derui Song},
  doi          = {10.1016/j.asoc.2021.107250},
  journal      = {Applied Soft Computing},
  pages        = {107250},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyperspectral image sparse reconstruction model based on collaborative multidimensional correlation},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local search-based metaheuristics for the robust distributed
permutation flowshop problem. <em>ASOC</em>, <em>105</em>, 107247. (<a
href="https://doi.org/10.1016/j.asoc.2021.107247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Permutation Flowshop Scheduling Problem (DPFSP) has become a hot issue in recent years. However, DPFSP with uncertain processing times (DPFSP_UPT) has not been addressed so far although real manufacturing systems often encounter various uncertain factors which make processing times uncertain. In this paper, two local-search based metaheuristics , i.e., an Iterated Greedy algorithm (sIG) and an Iterated Local Search algorithm (sILS), are presented to solve the DPFSP_UPT with makespan criterion. A robust model of expect-risk rule is used to describe the DPFSP_UPT. A valid heuristic based on the well-known NEH heuristic is proposed to initialize sIG and sILS. An acceleration algorithm is adapted to save computational efforts. A destruction procedure with dynamic sizes is presented to enhance the exploration capability for sIG. A feature with the characteristic that small changes in the solution can preserve the properties of good solutions in the iterative process is applied by sILS. Both sIG and sILS use multiple local search methods to produce promising solutions by exploiting diverse search areas. Extensive experiments show that the proposed sIG and sILS perform significantly better than the five competing algorithms adapted in the literature, but there is no significant difference between sIG and sILS and each has its own advantages for different instances.},
  archive      = {J_ASOC},
  author       = {Xue-Lei Jing and Quan-Ke Pan and Liang Gao},
  doi          = {10.1016/j.asoc.2021.107247},
  journal      = {Applied Soft Computing},
  pages        = {107247},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local search-based metaheuristics for the robust distributed permutation flowshop problem},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy image clustering incorporating local and region-level
information with median memberships. <em>ASOC</em>, <em>105</em>,
107245. (<a href="https://doi.org/10.1016/j.asoc.2021.107245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation with Fuzzy C-Means (FCM) clustering algorithm is a widely researched topic. In the literature, region-level information considers more redundancy information of image and becomes a more powerful FCM technique. However, noise pixels can be over-preserved via region-level information causing FCM produce fake segmentation areas. Fuzzification of partition is also vital in FCM and Kullback–Leibler (KL) information with membership mean template is successful to reduce fuzzification. However, existence of outliers influences final segmentation result due to the sum operation during calculation of mean template. In this paper, we propose an improved FCM with adaptive local and region-level information as well as KL information with locally median membership degrees. Firstly, followed by region-level information, a new distance measure incorporating both local and region-level information is proposed where local information of the image of region-level information is acquired to smooth the over-preserved noise pixels. Secondly, we renounce the sum operation in membership mean template and propose that locally median template is more reasonable to be considered as prior probability in KL information. Thirdly, adaptive constraints for conventional FCM and local as well as region-level information are introduced. In the beginning, differences between local variances of original image and image of region-level information in exponential form are computed. Then, reciprocals of the differences and the differences themselves are considered as constraints of conventional FCM and local as well as region-level information individually. Experiments of grayscale and color image segmentation show that the proposed method FALRCM (Fuzzy Adaptive Local and Region-level information C-Means) achieves better performance in terms of fuzzy partition coefficient (V PC PC ), fuzzy partition entropy (V PE PE ), Segmentation Accuracy (SA), mean Intersection-over-Union (mIoU), Peak Signal-to-Noise Ratio (PSNR) and visual effects compared with several state-of-the-art FCM variants. For example, in grayscale noise image segmentation, average V PC PC and mIoU are up to 99.92\% and 97.59\% with standard deviation of 0.05\% and 2.46\% respectively while the method with the closest performance provides V PC PC and mIoU of 95.27\% and 91.76\% with standard deviations of 15.59\% and 3.53\% respectively. The proposed method FALRCM achieves better robustness of noise and lower partition fuzzification. In the end, limitations are shown in discussion and application fields as well as future studies are proposed in conclusion part.},
  archive      = {J_ASOC},
  author       = {Qingsheng Wang and Xiaopeng Wang and Chao Fang and Jianjun Jiao},
  doi          = {10.1016/j.asoc.2021.107245},
  journal      = {Applied Soft Computing},
  pages        = {107245},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy image clustering incorporating local and region-level information with median memberships},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learned type-2 fuzzy neural network: Singular value
decomposition approach. <em>ASOC</em>, <em>105</em>, 107244. (<a
href="https://doi.org/10.1016/j.asoc.2021.107244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of this study is to present a novel dynamic fractional-order deep learned type-2 fuzzy logic system (FDT2-FLS) with improved estimation capability. The proposed FDT2-FLS is constructed based on the criteria of singular value decomposition and uncertainty bounds type-reduction. The upper and the lower singular values of the set of inputs are estimated by a simple filter and the output is obtained by fractional-order integral of the uncertainty bounds type-reduction. Using stability criteria of fractional-order systems, the adaptation rules of the consequent parameters are extracted such that the globally Mittag-Leffler stability is achieved. The proposed FDT2-FLS is employed for online dynamic identification of a hyperchaotic system, online prediction of chaotic time series and online prediction of glucose level in type-1 diabetes patients and its performance is compared with other well-known methods. It is shown that the proposed mechanism results in significantly better prediction and estimation performance with less tunable parameters in just one learning epoch.},
  archive      = {J_ASOC},
  author       = {Sultan Noman Qasem and Ardashir Mohammadzadeh},
  doi          = {10.1016/j.asoc.2021.107244},
  journal      = {Applied Soft Computing},
  pages        = {107244},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep learned type-2 fuzzy neural network: Singular value decomposition approach},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of particle swarm optimization for optimal
setting of phase shifting transformers to minimize unscheduled active
power flows. <em>ASOC</em>, <em>105</em>, 107243. (<a
href="https://doi.org/10.1016/j.asoc.2021.107243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s interconnected power systems require increased flexibility in control to ensure the capability for maintaining power flows at acceptable limits. Recently, this issue has become increasingly important, mainly due to the significant unscheduled flows that can violate system security. In response to this challenge, the transmission system operators introduced various measures to control unscheduled flows. The most effective are Phase Shifting Transformers (PSTs). A higher number of PSTs operating in the transmission grid increases the total control capabilities. Since the achieved control effects can be both strengthened and weakened because of various actions of individual PSTs, coordinating the settings of the PSTs will achieve the best results. This paper addresses a new, intelligent approach by using a combination of two methods: the discrete particle swarm optimization method for the optimal setting of the individual PSTs and the power flow method for calculating the value of the optimization objective . The case studies consider the IEEE 118-bus test system as well as a model of the real network of Central Eastern Europe. In both cases, the minimization of unscheduled flows was adopted as the objective. As a result of PSTs settings optimization, a significant reduction of unscheduled flows was obtained.},
  archive      = {J_ASOC},
  author       = {Roman Korab and Marcin Połomski and Robert Owczarek},
  doi          = {10.1016/j.asoc.2021.107243},
  journal      = {Applied Soft Computing},
  pages        = {107243},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of particle swarm optimization for optimal setting of phase shifting transformers to minimize unscheduled active power flows},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A weighted multi-feature transfer learning framework for
intelligent medical decision making. <em>ASOC</em>, <em>105</em>,
107242. (<a href="https://doi.org/10.1016/j.asoc.2021.107242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformative computing provides an emerging technology to data analysis and information processing, but how to effectively connect the data derived from different domains has aroused much of concern. Especially on medical areas, the scarcity of annotated medical data makes it hard to build a robust classification model , thus, the utilization of medical resources from different sources is particularly important. Transfer learning leverages the knowledge gained from the related domain to enhance the computational effectivity on the target domain. In this work, we extend transfer learning with ensemble learning to present a novel Weighted Multi-Feature Hybrid Transfer Learning Framework (W-MHTL) that builds a transformative approach to connect different domains and applies it to medical decision making. Our approach lessens the distribution variances from multiple perspectives by applying variant types of feature-based transfer learning methods. In each feature space, we construct the transfer model by evaluating the correlations and obtain the predicting result from each model. Finally, a feasible ensemble strategy is used to jointly consider each result. We evaluate our approach on synthetic datasets and UCI medical benchmarks, and a cerebral stroke dataset collected from local hospital. The experimental results reveal that our method achieves superior performances with the currently available alternatives.},
  archive      = {J_ASOC},
  author       = {Yun Yang and Jing Guo and Qiongwei Ye and Yuelong Xia and Po Yang and Amin Ullah and Khan Muhammad},
  doi          = {10.1016/j.asoc.2021.107242},
  journal      = {Applied Soft Computing},
  pages        = {107242},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A weighted multi-feature transfer learning framework for intelligent medical decision making},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-learning soft computing algorithms for prediction
machines of estimating crowd density. <em>ASOC</em>, <em>105</em>,
107240. (<a href="https://doi.org/10.1016/j.asoc.2021.107240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human needs motivate the improvement of computing paradigms , the emergence of soft computing is more effective in dealing with daily problems, while reducing costs, it also solves the problem of robustness and becomes easier to handle. Examples of this include collecting data on video data from smart sensors and can use that information to predict and monitor the behavior of crowd. Nowadays, with the development of cyber–physical systems and artificial intelligence , the traditional data collection and analysis system faces the risks of low transparency and high data security, making it difficult to obtain an accurate prediction result. Therefore, in this paper a novel prediction machine via self-learning generative adversarial network for soft computing application is proposed, which collects data through a series of high-precision IoT sensor devices and makes preliminary preprocessing, and further solves the crowd prediction problem based on deep learning algorithms and obtains a reliable and accurate prediction result by continuously optimizing internal parameters. The focus of this work is on the accuracy and preprocessing of data collection and crowd prediction algorithms. The prediction algorithm can be used to estimate and monitor the crowd flow in public places, and can prevent crowding, trampling and other traffic jams, such as stations, airports , large exhibitions, tourist attractions and other places. Therefore, the new prediction machine includes video capture, upload and display, data analysis and early warning operations in embedded devices, and automatically predicts crowd density. In terms of constructing the network, first, in order to obtain a clearer generated density map, the feature self-learning module is merged in the generator feature extraction stage. Secondly, in order to avoid the blur of the generated image, an adversarial loss is constructed between the generator and the discriminator , and finally to deal with multiple scales, two generator networks are constructed to adapt to large scale and small scale respectively in order to extract semantic information of different scales and use cross-scale consistency loss constraints to generate density maps.},
  archive      = {J_ASOC},
  author       = {Tao Zhang and Jiawei Yuan and Yeh-Cheng Chen and Wenjing Jia},
  doi          = {10.1016/j.asoc.2021.107240},
  journal      = {Applied Soft Computing},
  pages        = {107240},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-learning soft computing algorithms for prediction machines of estimating crowd density},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the origins of randomization-based feedforward neural
networks. <em>ASOC</em>, <em>105</em>, 107239. (<a
href="https://doi.org/10.1016/j.asoc.2021.107239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter identifies original independent works in the domain of randomization-based feedforward neural networks . In the most common approach, only the output layer weights require training while the hidden layer weights and biases are randomly assigned and kept fixed. The output layer weights are obtained using either iterative techniques or non-iterative closed-form solutions. The first such work (abbreviated as RWNN) was published in 1992 by Schmidt et al. for a single hidden layer neural network with sigmoidal activation. In 1994, a closed form solution was offered for the random vector functional link (RVFL) neural networks with direct links from the input to the output. On the other hand, for radial basis function neural networks, randomized selection of basis functions’ centers was used in 1988. Several works were published thereafter, employing similar techniques but with different names while failing to cite the original or relevant sources. In this letter, we make an attempt to identify and trace the origins of such randomization-based feedforward neural networks and give credits to the original works where due and hope that the future research publications in this field will provide fair literature review and appropriate experimental comparisons. We also briefly review the limited performance comparisons in the literature, two recently proposed new names, randomization-based multi-layer or deep neural networks and provide promising future directions.},
  archive      = {J_ASOC},
  author       = {Ponnuthurai N. Suganthan and Rakesh Katuwal},
  doi          = {10.1016/j.asoc.2021.107239},
  journal      = {Applied Soft Computing},
  pages        = {107239},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the origins of randomization-based feedforward neural networks},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Association rule-based malware classification using common
subsequences of API calls. <em>ASOC</em>, <em>105</em>, 107234. (<a
href="https://doi.org/10.1016/j.asoc.2021.107234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging malware pose increasing challenges to detection systems as their variety and sophistication continue to increase. Malware developers use complex techniques to produce malware variants, by removing, replacing, and adding useless API calls to the code, which are specifically designed to evade detection mechanisms, as well as do not affect the original functionality of the malicious code involved. In this work, a new recurring subsequences alignment-based algorithm that exploits associative rules has been proposed to infer malware behaviors. The proposed approach exploits the probabilities of transitioning from two API invocations in the call sequence, as well as it also considers their timeline, by extracting subsequence of API calls not necessarily consecutive and representative of common malicious behaviors of specific subsets of malware. The resulting malware classification scheme , capable to operate within dynamic analysis scenarios in which API calls are traced at runtime, is inherently robust against evasion/obfuscation techniques based on the API call flow perturbation . It has been experimentally compared with two detectors based on Markov chain and API call sequence alignment algorithms, which are among the most widely adopted approaches for malware classification. In such experimental assessment the proposed approach showed an excellent classification performance by outperforming its competitors.},
  archive      = {J_ASOC},
  author       = {Gianni D’Angelo and Massimo Ficco and Francesco Palmieri},
  doi          = {10.1016/j.asoc.2021.107234},
  journal      = {Applied Soft Computing},
  pages        = {107234},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Association rule-based malware classification using common subsequences of API calls},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trust-region based adaptive radial basis function algorithm
for global optimization of expensive constrained black-box problems.
<em>ASOC</em>, <em>105</em>, 107233. (<a
href="https://doi.org/10.1016/j.asoc.2021.107233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been a very challenging task to develop efficient and robust techniques to solve real-world engineering optimization problems due to the unknown function properties, complex constraints and severely limited computational budget. To address this issue, TARBF algorithm (trust-region based adaptive radial basis function interpolation) for solving expensive constrained black-box optimization problems is proposed in this paper. The approach successfully decomposes the original optimization problem into a sequence of sub-problems approximated by radial basis functions in a series of trust regions. Then, the solution of each sub-problem becomes the starting point for the next iteration. According to the values of objective and constraint functions, an effective online normalization technique is further developed to adaptively improve the model accuracy in the trust region, where the surrogate is updated iteratively. Averagely, TARBF has the ability to robustly solve the 21 G-problems (CEC’2006) and 4 engineering problems within 535.69 and 234.44 function evaluations, respectively. The comparison results with other state-of-the-art metamodel-based algorithms prove that TARBF is a convergent, efficient and accurate paradigm. Moreover, the sophisticated trust region strategy developed in TARBF, which is a major contribution to the field of the efficient constrained optimization , has the capability to facilitate an effective balance of exploration and exploitation for solving constrained black-box optimization problems.},
  archive      = {J_ASOC},
  author       = {Chengyang Liu and Zhiqiang Wan and Yijie Liu and Xuewu Li and Dianzi Liu},
  doi          = {10.1016/j.asoc.2021.107233},
  journal      = {Applied Soft Computing},
  pages        = {107233},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Trust-region based adaptive radial basis function algorithm for global optimization of expensive constrained black-box problems},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mitigating the impact of adversarial attacks in very deep
networks. <em>ASOC</em>, <em>105</em>, 107231. (<a
href="https://doi.org/10.1016/j.asoc.2021.107231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Network (DNN) models have vulnerabilities related to security concerns, with attackers usually employing complex hacking techniques to expose their structures. Data poisoning-enabled perturbation attacks are complex adversarial ones that inject false data into models. They negatively impact the learning process, with no benefit to deeper networks, as they degrade a model’s accuracy and convergence rates. In this paper, we propose an attack-agnostic-based defense method for mitigating their influence. In it, a Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture which assists in neutralizing the effects of illegitimate perturbation samples in the feature space. To boost the robustness and trustworthiness of this method for correctly classifying attacked input samples, we regularize the hidden space of a trained model with a discriminative loss function called Polarized Contrastive Loss (PCL). It improves discrimination among samples in different classes and maintains the resemblance of those in the same class. Also, we integrate a DFL and PCL in a compact model for defending against data poisoning attacks. This method is trained and tested using the CIFAR-10 and MNIST datasets with data poisoning-enabled perturbation attacks, with the experimental results revealing its excellent performance compared with those of recent peer techniques.},
  archive      = {J_ASOC},
  author       = {Mohammed Hassanin and Ibrahim Radwan and Nour Moustafa and Murat Tahtali and Neeraj Kumar},
  doi          = {10.1016/j.asoc.2021.107231},
  journal      = {Applied Soft Computing},
  pages        = {107231},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mitigating the impact of adversarial attacks in very deep networks},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Over-complete deep recurrent neutral network based on
wastewater treatment process soft sensor application. <em>ASOC</em>,
<em>105</em>, 107227. (<a
href="https://doi.org/10.1016/j.asoc.2021.107227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wastewater treatment process (WWTP) is a complex biochemical reaction process in which sensor data has strong nonlinear, non-Gaussian and time correlation characteristics. The traditional methods ignore to consider the aforementioned three characteristics simultaneously, which may have insufficient feature extraction of WWTP. In this work, an Over-Complete Deep Recurrent Neural Network (ODRNN) method is proposed to solve the above issues. The ODRNN combines the over-complete independent component analysis (OICA) and binary particle swarm optimization (BPSO) to efficiently extract the non-Gaussian information, and then the extracted information is fed into DRNN to obtain the time correlation characteristics. In this way, the method can not only capture the non-linear and non-Gaussian information but also extract temporal correlation of WWTP data. Simulation results on BSM1 showed that the ODRNN based soft sensor method has higher accuracy and robustness than other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Peng Chang and ZeYu Li},
  doi          = {10.1016/j.asoc.2021.107227},
  journal      = {Applied Soft Computing},
  pages        = {107227},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Over-complete deep recurrent neutral network based on wastewater treatment process soft sensor application},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal fuzzy logic-based control strategy for lower limb
rehabilitation exoskeleton. <em>ASOC</em>, <em>105</em>, 107226. (<a
href="https://doi.org/10.1016/j.asoc.2021.107226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, several control engineers have been working towards development of efficient rehabilitation exoskeletons for mobility impairments. This work aims at implementation of an optimal fuzzy logic-based control strategy for a lower limb exoskeleton application wherein the control parameters for the proposed control approach are obtained by a recently developed optimization technique named as dragon fly algorithm (DFA). For analysis of appropriately tuned closed-loop control, a comparative study between two optimization techniques namely DFA and genetic algorithm (GA) applied to a 2-degree of freedom (dof) nonlinear and coupled lower-limb exoskeleton, is presented. To see the practical aspects, a three-dimensional simscape model of the 4-dof lower limb exoskeleton is developed to observe the closed-loop performance of fuzzy logic proportional–integral–derivative (FLC-PID) controller for bipedal human walking. Experimental data for different speeds during treadmill walking is captured with electronic wireless goniometer, and is used to validate the bipedal walking control for the designed lower-limb exoskeleton. The results are further compared with traditional PID controllers in order to see the effectiveness of the proposed control approaches. Furthermore, the robustness testing of the proposed control schemes is also investigated for different speeds of human walking. This study presents a closed-loop control design for the development of a low-cost lower limb exoskeleton to restore normal gait for persons with mobility disorders, stroke or elderly persons.},
  archive      = {J_ASOC},
  author       = {Richa Sharma and Prerna Gaur and Shaurya Bhatt and Deepak Joshi},
  doi          = {10.1016/j.asoc.2021.107226},
  journal      = {Applied Soft Computing},
  pages        = {107226},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal fuzzy logic-based control strategy for lower limb rehabilitation exoskeleton},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A geometrical method for consensus building in GDM with
incomplete heterogeneous preference information. <em>ASOC</em>,
<em>105</em>, 107224. (<a
href="https://doi.org/10.1016/j.asoc.2021.107224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-life group decision-making (GDM) problems, the preferences given by decision-makers(DMs) are often incomplete, because the complexity of decision-making problems and the limitation of knowledge of DM make it difficult for DMs to take a determined evaluation of alternatives. In addition, preference relations provided by DMs are often heterogeneous because they always have different decision habits and hobbies. However, the consensus method for GDM under incomplete heterogeneous preference relations is rarely studied. For four common preference relations: utility values, preference orderings, and (incomplete) multiplicative preference relations and (incomplete) fuzzy preference relations, this paper proposes a geometrical method for consensus building in GDM. Specifically, we integrate incomplete heterogeneous preference structures using a similarity-based optimization model and set a corresponding geometrical consensus measurement. Then, preference modification and weighting processes are proposed to improve consensus degree. Finally, we conduct a comparison analysis based on a qualitative analysis and algorithm complexity analysis of existing consensus reaching methods. Numerical analyses and convergence tests show that our method can promote the improvement of the consensus degree in GDM, and has less time complexity than the previous methods. The proposed geometrical method is a more explainable model due to operability and simplicity.},
  archive      = {J_ASOC},
  author       = {Gang Kou and Yi Peng and Xiangrui Chao and Enrique Herrera-Viedma and Fawaz E. Alsaadi},
  doi          = {10.1016/j.asoc.2021.107224},
  journal      = {Applied Soft Computing},
  pages        = {107224},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A geometrical method for consensus building in GDM with incomplete heterogeneous preference information},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new integrated MCDM approach for improving QFD based on
DEMATEL and extended MULTIMOORA under uncertainty environment.
<em>ASOC</em>, <em>105</em>, 107222. (<a
href="https://doi.org/10.1016/j.asoc.2021.107222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality characteristics (QCs) prioritization is a criterial issue in QFD . However, due to its own inherent flaws, QFD is in a difficult situation in practical applications. As an effective method of multi-objective decision-making, the multi-criteria decision-making (MCDM) method can describe, evaluate, and rank the evaluation objects according to different criteria. Therefore, this paper proposes a new integrated MCDM method for improving QFD, which integrates the hesitant fuzzy linguistic term set (HFLTS), decision-making trial and evaluation laboratory (DEMATEL), and multi-objective optimization by ratio analysis plus full multiplicative form (MULTIMOORA). Firstly, the HFLTS is used to deal with the ambiguity in the evaluation process. Secondly, with regard to the interaction relationships among QCs, fuzzy DEMATEL technology is employed to capture their influence weights. Furthermore, an extended MULTIMOORA method is developed, which combines the entropy weight method to obtain the objective weights of customer requirements (CRs), and introduces the influence weights to prioritize QCs. Finally, the feasibility and practicability of the proposed method are verified by an example regarding the product design of CNC machine tool.},
  archive      = {J_ASOC},
  author       = {Yifan Chen and Yan Ran and Guangquan Huang and Liming Xiao and Genbao Zhang},
  doi          = {10.1016/j.asoc.2021.107222},
  journal      = {Applied Soft Computing},
  pages        = {107222},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new integrated MCDM approach for improving QFD based on DEMATEL and extended MULTIMOORA under uncertainty environment},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum squirrel inspired algorithm for gene selection in
methylation and expression data of prostate cancer. <em>ASOC</em>,
<em>105</em>, 107221. (<a
href="https://doi.org/10.1016/j.asoc.2021.107221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prostate cancer is the second most common type of cancer among men after skin cancer In this work, we present a comprehensive view on genomic and epigenomic changes following the incremental biological functionality. For gene selection, a new Feature Selection algorithm called Quantum Squirrel inspired Feature Selection is proposed here. While exploring the feature space, the proposed algorithm exploits the benefits of Squirrel Search Algorithm (a recently proposed swarm intelligence algorithm) along with Quantum mechanics. Moreover, a modified version of the end of winter concept is used to achieve effective dimension reduction capacity. Quantum Squirrel inspired Feature Selection is executed on both expression and methylation data of prostate cancer. The major challenge in gene selection is to bring down the number of selected features without compromising on accuracy. The proposed algorithm consistently achieves this goal and outperforms other state-of-the-art algorithms. The proposed algorithm has steadily attained 100\% accuracy while selecting a much lower number of features (around 4), which is a major improvement over others. The top selected genes are biologically validated in terms of Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway and Gene Ontologies (GO), which further demonstrates the usefulness of the proposed method. The genes selected by Quantum Squirrel inspired Feature Selection show an association with prostate carcinoma and most are known biomarkers. A few novel biomarkers selected by proposed algorithm have also been detailed in this work. Source code of this work is available at: Quantum Squirrel inspired Feature Selection .},
  archive      = {J_ASOC},
  author       = {Manosij Ghosh and Sagnik Sen and Ram Sarkar and Ujjwal Maulik},
  doi          = {10.1016/j.asoc.2021.107221},
  journal      = {Applied Soft Computing},
  pages        = {107221},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum squirrel inspired algorithm for gene selection in methylation and expression data of prostate cancer},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HANSEL: Adaptive horizontal scaling of microservices using
bi-LSTM. <em>ASOC</em>, <em>105</em>, 107216. (<a
href="https://doi.org/10.1016/j.asoc.2021.107216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of 5G network , business scenarios such as intelligent service and new retail are becoming more and more popular. The demand for more flexible and scalable real-time data processing , in particular, the AI-related data processing has also increased in edge computing . Therefore, how to meet such business development has become a major challenge. Focusing on this requirement, microservice architecture , proposed and developed by some big cloud computing companies’ platform, such as Google Kubernetes platform, has gradually become a mainstream technology solution in edge computing . However, many microservices used in edge computing cannot achieve an even time distribution, which is random or sudden. Kubernetes built-in Horizontal POD Autoscaling (HPA) is unable to well handle the change of microservice load, which inevitably leads to the waste of system resources and affects the SLA of microservice. To solve this issue, this paper proposes a HANSEL system based on Kubernetes platform, which can optimize the horizontal elastic scaling policy of Kubernetes by accurately predicting the load of microservices based on the Bi-LSTM load prediction algorithm with attention mechanism . Furthermore, active elastic scaling is realized through reinforcement learning method, and we design a hybrid elastic scaling mechanism through combining reactive and active methods, so as to construct an elastic scaling system for automatic scheduling of working nodes. Our experimental results show that HANSEL system can improve the system resource utilization by about 20\% 20\% when meeting the microservice SLA of edge computing.},
  archive      = {J_ASOC},
  author       = {Ming Yan and XiaoMeng Liang and ZhiHui Lu and Jie Wu and Wei Zhang},
  doi          = {10.1016/j.asoc.2021.107216},
  journal      = {Applied Soft Computing},
  pages        = {107216},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HANSEL: Adaptive horizontal scaling of microservices using bi-LSTM},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Balancing accuracy and diversity in ensemble learning using
a two-phase artificial bee colony approach. <em>ASOC</em>, <em>105</em>,
107212. (<a href="https://doi.org/10.1016/j.asoc.2021.107212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ensemble learning , it is necessary to build a balancing mechanism to balance the accuracy of individual learners with the diversity between individual learners to achieve excellent ensemble learning performance. In previous studies, diversity was regarded only as a regularization term, which does not sufficiently indicate that diversity should implicitly be treated as an accuracy factor. In this study, an ensemble learning approach based on balanced accuracy and diversity (ELBAD) that uses a two-phase artificial bee colony (ABC) algorithm is proposed to balance the accuracy and diversity of ensemble learners. In the first phase, the ABC algorithm is used to generate an ensemble classifier with appropriate diversity. In the second phase, the ABC algorithm is used to generate a weighted ensemble classifier. The ELBAD ensemble learning algorithm is significantly superior to other state-of-the-art popular ensemble learning algorithms, including AdaBoost , Bagging, Decorate, extremely randomized trees (ET), gradient boosting decision tree (GBDT), random forest (RF), and rotation forest (RoF) on 30 UCI datasets. In addition, this study proposes a systematic parameter tuning procedure for the ELBAD algorithm that reduces the time required to generate an ensemble classifier.},
  archive      = {J_ASOC},
  author       = {Yeou-Ren Shiue and Gui-Rong You and Chao-Ton Su and Hua Chen},
  doi          = {10.1016/j.asoc.2021.107212},
  journal      = {Applied Soft Computing},
  pages        = {107212},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Balancing accuracy and diversity in ensemble learning using a two-phase artificial bee colony approach},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EEG denoising through a wide and deep echo state network
optimized by UPSO algorithm. <em>ASOC</em>, <em>105</em>, 107149. (<a
href="https://doi.org/10.1016/j.asoc.2021.107149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the complex environment of telemedicine , Electroencephalogram (EEG) signals are easily overwhelmed by noise, which affects the intelligent diagnosis of diseases. Since the time-frequency domain characteristics of some noise in EEG signals are complex and the distribution is unknown, and the spectrum of some noise overlaps with the original EEG signal spectrum, it is difficult to filter those noise by traditional methods. To tackle this problem, and considering the large data characteristics of EEG signals in the context of telemedicine , a wide-deep echo state networks (WDESN) with multiple reservoirs in parallel and stacked configuration is proposed for multivariate time series denoising . Firstly, stacking and paralleling multiple reservoirs, the deep features of signals can be extracted to complete the task of reconstructing signal from the noisy signal . Then, the Uniform Search Particle Swarm Optimization (UPSO) is used to optimize reservoir parameters of WDESN. Finally, the effectiveness of UPSO-WDESN is verified through experiments. Experiment results show that compared with existing models, the proposed UPSO-WDESN model can achieve better noise removal performance, while keeping more nonlinear feature of signals.},
  archive      = {J_ASOC},
  author       = {Weitong Sun and Yuping Su and Xia Wu and Xiaojun Wu and Yumei Zhang},
  doi          = {10.1016/j.asoc.2021.107149},
  journal      = {Applied Soft Computing},
  pages        = {107149},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEG denoising through a wide and deep echo state network optimized by UPSO algorithm},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-layer selector(MLS): Dynamic selection based on
filtering some competence measures. <em>ASOC</em>, <em>104</em>, 107257.
(<a href="https://doi.org/10.1016/j.asoc.2021.107257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Classifier Systems (MCSs) have been widely studied as committee methods for increasing accuracy in pattern recognition. Recently, dynamic selection (DS) techniques have attracted undivided attention as effective types of MCSs in application. In DS techniques, the selection process is accomplished in two steps including measurement and selection. Normally, most DS methods in the literature have tried to focus on approaches adopted based on only one of the two options that are to offer a new measurement or a new selection method. On the other hand, some other papers have proposed frameworks in which both options were applied in combination with each other making use of previously existing measurement methods. For the first time, the idea of combining DS methods via multi-layer selectors is offered in this paper. In specific, two main tasks are performed in each layer of the multi-layer selector. First, the competence-level of classifiers are calculated. Then, the classifiers with high competence-level are passed to the next layer, and this way, best classifiers will be passed to the output in the last layer. During this procedure, some competence criteria are combined to measure the competence-level of classifiers. Furthermore, this paper introduces a novel taxonomy for dynamic selection methods and a new technique, called Probability Based (PB) method, to find the competence level of classifier. Experimental results show that the suggested framework improves the classification accuracy when compared against current state-of-the-art dynamic ensemble selection techniques. The Quade non-parametric statistical test confirms the capability of our proposed method to deal with classification problems.},
  archive      = {J_ASOC},
  author       = {Javad Elmi and Mahdi Eftekhari},
  doi          = {10.1016/j.asoc.2021.107257},
  journal      = {Applied Soft Computing},
  pages        = {107257},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-layer Selector(MLS): Dynamic selection based on filtering some competence measures},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance comparison of non-adaptive and adaptive
optimization algorithms for artificial neural network training applied
to damage diagnosis in civil structures. <em>ASOC</em>, <em>104</em>,
107254. (<a href="https://doi.org/10.1016/j.asoc.2021.107254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structural health monitoring of civil structures has been highlighted by the perception that the cost involved in the prevention of structural accidents is lower than the cost of correcting them. In addition, in the case of large structures (e.g. bridges, dams and industrial sheds), it is desired that the damage diagnosis should occur quickly from real-time monitoring without interrupting the use of the structure. The structural diagnosis may be performed based on vibration measurements , considering that the structural damages modify modal parameters of the structure (e.g. frequencies and mode shapes). With these dynamic responses, the application of appropriate computational tools to recognize and classify structural damage is intended, and artificial neural networks (ANN) have gained a lot of attention in achieving these goals. In this work, the methodology of diagnosing structures by real-time monitoring is originally developed and based on initial definition of the optimal topology , avoiding both the use of multiple hidden layers and the combination of several neural networks , and by applying of non-adaptive and adaptive first-order algorithms for agile network training, in order to be mathematically suitable for continuous and real-time monitoring, which allows updating both the dataset and neural parameters without greater computational effort. As some of these algorithms were not addressed in the diagnosis of civil structures with the aforementioned hypotheses, until this research, the performance of each algorithm was verified in case studies that simulate classic structural systems adopted in most civil buildings. Finally, the results endorse the feasibility of improving the structural diagnosis based on the training of a simple neural network, with one hidden layer, associated with non-adaptive or adaptive first-order optimizers that guarantee the agile assessment of structural integrity in real time.},
  archive      = {J_ASOC},
  author       = {Calebe Paiva Gomes de Souza and Paulo Roberto Gardel Kurka and Romulo Gonçalves Lins and José Medeiros de Araújo Júnior},
  doi          = {10.1016/j.asoc.2021.107254},
  journal      = {Applied Soft Computing},
  pages        = {107254},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Performance comparison of non-adaptive and adaptive optimization algorithms for artificial neural network training applied to damage diagnosis in civil structures},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A wrapper methodology to learn interval-valued fuzzy
rule-based classification systems. <em>ASOC</em>, <em>104</em>, 107249.
(<a href="https://doi.org/10.1016/j.asoc.2021.107249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning an interval-valued fuzzy rule-based classification system is a challenge as its success directly depends on the interval-valued fuzzy partition used. In fact, the learning of an interval-valued fuzzy system usually starts by creating a partition composed of numerical fuzzy sets, which are used to build an initial fuzzy classifier. Then, it is augmented with interval-valued fuzzy sets whose shape is subsequently optimized to improve the system’s performance. However, as in this methodology the fuzzy rules are learned using numerical fuzzy sets, the benefits of the interval-valued fuzzy sets may not be fully exploited. In this paper we define a new learning methodology that avoids building the initial fuzzy classifier but directly learns interval-valued fuzzy rules. To do so, we define a wrapper methodology to learn the interval-valued fuzzy partitions such that they lead to an interval-valued fuzzy rule-based classification system as accurate as possible. Moreover, our new method allows one to represent each membership function using the most proper type of fuzzy set for the sake of modeling the uncertainty in the best possible manner. Consequently, the antecedents of the rules can be formed of only numerical fuzzy sets, only interval-valued fuzzy sets or a mixture of both. The quality of the proposal is compared versus four state-of-the-art fuzzy classifiers like FARC-HD, IVTURS, FURIA and FARC-HD using an inference based on a generalization of the Choquet integral. We also compare our new approach besides its numerical fuzzy counterpart to clearly show the benefits of the usage of interval-valued fuzzy sets. Specifically, the average accuracy rate of our new method is 81.17\%, which is at least 0.66\% better than the remainder state-of-the-art fuzzy classifiers.},
  archive      = {J_ASOC},
  author       = {Jose Antonio Sanz and Humberto Bustince},
  doi          = {10.1016/j.asoc.2021.107249},
  journal      = {Applied Soft Computing},
  pages        = {107249},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A wrapper methodology to learn interval-valued fuzzy rule-based classification systems},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel adaptive learning deep belief network based on
automatic growing and pruning algorithms. <em>ASOC</em>, <em>104</em>,
107248. (<a href="https://doi.org/10.1016/j.asoc.2021.107248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel adaptive learning deep belief network (ALDBN) with a series of growing and pruning algorithms is proposed to dynamically adjust its structure when ALDBN is utilized for extracting features. Specifically, a neuron growing algorithm is designed considering the individual and macroscopical impacts on each neuron to detect unstable hidden neurons , and a new hidden neuron will be added around each unstable neuron to compensate for the inadequacy of the local structure for feature extraction. Moreover, the relations of network depth and information entropy with respect to the normal distribution of each weight between hidden layers are revealed. On basis of the relations revealed, a layer growing algorithm is designed considering the obedience rate of the normal distribution to control the number of hidden layers. In addition, a neuron pruning algorithm using the standard deviation of neuron activation probability is integrated in ALDBN to prune the redundant neurons with low discriminative ability. We first give the theoretical proof for the convergence of ALDBN, which is crucial to its stability. To exhibit its performance, parameter sensitivity analysis is then provided to investigate the effects of two key parameters in ALDBN. Finally, we compare ALDBN with five state-of-art methods on three benchmark datasets, and the comparative experimental results demonstrate that ALDBN outperforms the other five competitors in terms of the accuracies of common test, cross-validated test and holdout test.},
  archive      = {J_ASOC},
  author       = {Wei Song and Shiyu Zhang and Zijian Wen and Junhao Zhou},
  doi          = {10.1016/j.asoc.2021.107248},
  journal      = {Applied Soft Computing},
  pages        = {107248},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel adaptive learning deep belief network based on automatic growing and pruning algorithms},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discrete dynamic artificial bee colony with hyper-scout
for RESTful web service API test suite generation. <em>ASOC</em>,
<em>104</em>, 107246. (<a
href="https://doi.org/10.1016/j.asoc.2021.107246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microservices have become quite important because they decompose an application into a collection of highly maintainable services and reduce the complexity of monolithic architecture . Although the most common method of microservice development is the REST architecture, the number of studies on the automatic testing of RESTful APIs are very limited. For automatic testing of RESTful APIs, different algorithms have been proposed. However, their performance depends on the type of problem being addressed. This study implements the basic Artificial Bee Colony (ABC) algorithm for automatic testing of RESTful web services, and addresses some shortcomings for the RESTful test generation. To overcome these shortcomings, we propose a Discrete Dynamic Artificial Bee Colony with Hyper-Scout (DABC-HS) algorithm for the service-oriented problem. We enhanced the exploration ability of the algorithm by integrating a hyper-scout unit. The presence of solutions that correspond to covered targets but are not related to the remaining targets causes overburdening of the search. In the proposed approach, the onlooker bee phase generates a pool of all solutions and their mutants and applies a dominance-based selection operator to select high-quality solutions related to uncovered targets. The experimental results demonstrate that the DABC-HS recorded the highest performance in four of the seven problems while demonstrating similarly good performance in other problems. MOSA, which is the powerful algorithm in the literature, was the best in two of seven problems while ABC algorithm achieved the best performance in one problem. Although the method could not achieve the best result in all problems, it is more successful than the other published methods in the literature.},
  archive      = {J_ASOC},
  author       = {Omur Sahin and Bahriye Akay},
  doi          = {10.1016/j.asoc.2021.107246},
  journal      = {Applied Soft Computing},
  pages        = {107246},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete dynamic artificial bee colony with hyper-scout for RESTful web service API test suite generation},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A causal framework to determine the effectiveness of dynamic
quarantine policy to mitigate COVID-19. <em>ASOC</em>, <em>104</em>,
107241. (<a href="https://doi.org/10.1016/j.asoc.2021.107241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the start of the pandemic caused by the novel coronavirus, COVID-19, more than 106 million people have been infected and global deaths have surpassed 2.4 million. In Chile, the government restricted the activities and movement of people, organizations, and companies, under the concept of dynamic quarantine across municipalities for a predefined period of time. Chile is an interesting context to study because reports to have a higher quantity of infections per million people as well as a higher number of polymerize chain reaction (PCR) tests per million people. The higher testing rate means that Chile has good measurement of the contagious compared to other countries. Further, the heterogeneity of the social, economic, and demographic variables collected of each Chilean municipality provides a robust set of control data to better explain the contagious rate for each city. In this paper, we propose a framework to determine the effectiveness of the dynamic quarantine policy by analyzing different causal models (meta-learners and causal forest) including a time series pattern related to effective reproductive number. Additionally, we test the ability of the proposed framework to understand and explain the spread over benchmark traditional models and to interpret the Shapley Additive Explanations (SHAP) plots. The conclusions derived from the proposed framework provide important scientific information for government policymakers in disease control strategies, not only to analyze COVID-19 but to have a better model to determine social interventions for future outbreaks.},
  archive      = {J_ASOC},
  author       = {Werner Kristjanpoller and Kevin Michell and Marcel C. Minutolo},
  doi          = {10.1016/j.asoc.2021.107241},
  journal      = {Applied Soft Computing},
  pages        = {107241},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A causal framework to determine the effectiveness of dynamic quarantine policy to mitigate COVID-19},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated detection of covid-19 from chest x-ray scans using
an optimized CNN architecture. <em>ASOC</em>, <em>104</em>, 107238. (<a
href="https://doi.org/10.1016/j.asoc.2021.107238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel coronavirus termed as covid-19 has taken the world by its crutches affecting innumerable lives with devastating impact on the global economy and public health. One of the major ways to control the spread of this disease is identification in the initial stage, so that isolation and treatment could be initiated. Due to the lack of automated auxiliary diagnostic medical tools, availability of lesser sensitivity testing kits, and limited availability of healthcare professionals, the pandemic has spread like wildfire across the world. Certain recent findings state that chest X-ray scans contain salient information regarding the onset of the virus, the information can be analyzed so that the diagnosis and treatment can be initiated at an earlier stage. This is where artificial intelligence meets the diagnostic capabilities of experienced clinicians. The objective of the proposed research is to contribute towards fighting the global pandemic by developing an automated image analysis module for identifying covid-19 affected chest X-ray scans by employing an optimized Convolution Neural Network (CNN) model. The aforementioned objective is achieved in the following manner by developing three classification models , (i) ensemble of ResNet 50-Error Correcting Output Code (ECOC) model, (ii) CNN optimized using Grey Wolf Optimizer (GWO) and, (iii) CNN optimized using Whale Optimization + BAT algorithm. The novelty of the proposed method lies in the automatic tuning of hyper parameters considering a hierarchy of MultiLayer Perceptron (MLP), feature extraction, and optimization ensemble. A 100\% classification accuracy was obtained in classifying covid-19 images. Classification accuracy of 98.8\% and 96\% were obtained for dataset 1 and dataset 2 respectively for classification into covid-19, normal, and viral pneumonia cases. The proposed method can be adopted in a clinical setting for assisting radiologists and it can also be employed in remote areas to facilitate the faster screening of affected patients.},
  archive      = {J_ASOC},
  author       = {Sameena Pathan and P.C. Siddalingaswamy and Tanweer Ali},
  doi          = {10.1016/j.asoc.2021.107238},
  journal      = {Applied Soft Computing},
  pages        = {107238},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated detection of covid-19 from chest X-ray scans using an optimized CNN architecture},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BERT-ADLOC: A secure crowdsourced indoor localization system
based on BLE fingerprints. <em>ASOC</em>, <em>104</em>, 107237. (<a
href="https://doi.org/10.1016/j.asoc.2021.107237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourced indoor localization methods have grasped much attention in recent years as a method of reducing the cost of constructing the fingerprint database . In a crowdsourcing environment, however, the localization system is vulnerable to malicious attacks , which possibly lead to serious localization errors . In this paper, we conclude the potential attacks during fingerprint database updates and online inference phases and propose a secure indoor crowdsourced localization system, BERT-ADLOC, based on BLE fingerprints. Our system consists of two main parts: adversarial sample discriminator BERT-AD and indoor localization model BERT-LOC. Our proposed BERT-AD recognizes fake fingerprints during the database update phase, while BERT-LOC defends against attacks online, in which valid beacons are moved or malicious beacons are deployed. A tailored BERT model is introduced to extract deep hidden features through the self-attention mechanism. Our experiments show that BERT-ADLOC achieves a good localization performance against adversaries both in the fingerprint database update phase and online inference phase.},
  archive      = {J_ASOC},
  author       = {Xu Sun and Haojun Ai and Jingjie Tao and Tan Hu and Yusong Cheng},
  doi          = {10.1016/j.asoc.2021.107237},
  journal      = {Applied Soft Computing},
  pages        = {107237},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BERT-ADLOC: A secure crowdsourced indoor localization system based on BLE fingerprints},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale skeleton adaptive weighted GCN for
skeleton-based human action recognition in IoT. <em>ASOC</em>,
<em>104</em>, 107236. (<a
href="https://doi.org/10.1016/j.asoc.2021.107236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based human action recognition has become a hot topic due to its potential advantages. Graph convolution network (GCN) has obtained remarkable performances in the modeling of skeleton-based human action recognition in IoT. In order to capture robust spatial–temporal features from the human skeleton, a powerful feature extractor is essential. However, Most GCN-based methods use the fixed graph topology . Besides, only a single-scale feature is used, and the multi-scale information is ignored. In this paper, we propose a multi-scale skeleton adaptive weighted graph convolution network (MS-AWGCN) for skeleton-based action recognition. Specifically, a multi-scale skeleton graph convolution network is adopted to extract more abundant spatial features of skeletons. Moreover, we develop a simple graph vertex fusion strategy, which can learn the latent graph topology adaptively by replacing the handcrafted adjacency matrix with a learnable matrix. According to different sampling strategies, weighted learning method is adopted to enrich features while aggregating. Experiments on three large datasets illustrate that the proposed method achieves comparable performances to state-of-the-art methods. Our proposed method attains an improvement of 0.9\% and 0.7\% respectively over the recent GCN-based method on the NTU RGB+D and Kinetics dataset.},
  archive      = {J_ASOC},
  author       = {Weiyao Xu and Muqing Wu and Jie Zhu and Min Zhao},
  doi          = {10.1016/j.asoc.2021.107236},
  journal      = {Applied Soft Computing},
  pages        = {107236},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale skeleton adaptive weighted GCN for skeleton-based human action recognition in IoT},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CEEP-FL: A comprehensive approach for communication
efficiency and enhanced privacy in federated learning. <em>ASOC</em>,
<em>104</em>, 107235. (<a
href="https://doi.org/10.1016/j.asoc.2021.107235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is an emerging technique for collaboratively training machine learning models on distributed data under privacy constraints. However, recent studies have shown that FL significantly consumes plenty of communication resources during the global model update. In addition, participants’ private data can also be compromised by exploiting the shared parameters when uploading the local gradient updates to the central cloud server, which hinders FL to be implemented widely. To address these challenges, in this paper, we propose a novel comprehensive FL approach, namely, Communication Efficient and Enhanced Privacy (CEEP-FL). In particular, the proposed approach simultaneously aims to; (1) minimize the communication cost, (2) protect data from being compromised, and (3) maximize the global learning accuracy. To minimize the communication cost, we first apply a novel filtering mechanism on each local gradient update and upload only the important gradients. Then, we apply Non-Interactive Zero-Knowledge Proofs based Homomorphic-Cryptosystem (NIZKP-HC) in order to protect those local gradient updates while maintaining robustness in the network. Finally, we use Distributed Selective Stochastic Gradient Descent (DSSGD) optimization to minimize the computational cost and maximize the global learning accuracy. The experimental results on commonly used FL datasets demonstrate that CEEP-FL distinctively outperforms the existing approaches.},
  archive      = {J_ASOC},
  author       = {Muhammad Asad and Ahmed Moustafa and Muhammad Aslam},
  doi          = {10.1016/j.asoc.2021.107235},
  journal      = {Applied Soft Computing},
  pages        = {107235},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CEEP-FL: A comprehensive approach for communication efficiency and enhanced privacy in federated learning},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse elastic net multi-label rank support vector machine
with pinball loss and its applications. <em>ASOC</em>, <em>104</em>,
107232. (<a href="https://doi.org/10.1016/j.asoc.2021.107232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label rank support vector machine (RankSVM) is an effective technique to deal with multi-label classification problems, which has been widely used in various fields. However, it is sensitive to noise points and cannot delete redundant features for high dimensional problems. Therefore, to address the above two limitations, a sparse elastic net multi-label RankSVM with pinball loss (pin-ENR) is first proposed in this paper. On the one hand, pinball loss is employed to enhance the robustness. On the other hand, it adopts the sparse elastic net regularization , so that it can do variable selection. However, it still has challenges for large-scale problems with a huge number of features, samples, and labels. Therefore, motivated by the sparsity of pin-ENR, a safe simultaneous feature and label-pair elimination rule is further constructed for accelerating pin-ENR, which is termed as FLER-pin-ENR. Its main idea is to delete a large number of inactive features and label-pairs simultaneously before training without sacrificing accuracy. Numerical experiments on four synthetic and seven benchmark datasets demonstrate the feasibility and validity. Moreover, we apply our FLER-pin-ENR to the diagnosis of diabetes complications and the natural scene image classification problems, which further verifies the practicability of our proposed method.},
  archive      = {J_ASOC},
  author       = {Hongmei Wang and Yitian Xu},
  doi          = {10.1016/j.asoc.2021.107232},
  journal      = {Applied Soft Computing},
  pages        = {107232},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sparse elastic net multi-label rank support vector machine with pinball loss and its applications},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biomedical cross-sentence relation extraction via multihead
attention and graph convolutional networks. <em>ASOC</em>, <em>104</em>,
107230. (<a href="https://doi.org/10.1016/j.asoc.2021.107230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most biomedical information extraction efforts are focused on binary relations , there is a strong need to extract drug–gene–mutation n-ary relations among cross-sentences. In recent years, end-to-end biomedical relation extraction with sequence-based or dependency-based method has gained increasing attention. However, handling global dependencies and structural information remains challenges for sequence-based and dependency-based models. Joint exploitation of sequence and graph information may improve biomedical cross-sentence relation extraction. In this paper, we present a hybrid model for extracting biomedical relation in a cross-sentence which aims to address these problems. Our models rely on the self-attention mechanism that directly draws the global dependency relation of the sentence. Furthermore, to preserve the dependency structural information between the words that contain the syntactic dependency relations, we employ graph convolutional networks that encode the dependency structural information to guide the multihead attention to learn the dependency relation . Through extensive experiments on benchmark datasets, we demonstrated the effectiveness of our method.},
  archive      = {J_ASOC},
  author       = {Di Zhao and Jian Wang and Hongfei Lin and Xin Wang and Zhihao Yang and Yijia Zhang},
  doi          = {10.1016/j.asoc.2021.107230},
  journal      = {Applied Soft Computing},
  pages        = {107230},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Biomedical cross-sentence relation extraction via multihead attention and graph convolutional networks},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New benchmark algorithm for minimizing total completion time
in blocking flowshops with sequence-dependent setup times.
<em>ASOC</em>, <em>104</em>, 107229. (<a
href="https://doi.org/10.1016/j.asoc.2021.107229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just-in-time production in large enterprises along with the factory’s limited space highlights the need for scheduling tools that consider blocking conditions. This study contributes to the scheduling literature by developing an effective metaheuristic to address the Blocking Flowshop Scheduling Problems with Sequence-Dependent Setup-Times (BFSP with SDSTs). Including a new constructive heuristic and a local search mechanism customized for the blocking and setup time features, the Extended Iterated Greedy (EIG) algorithm effectively solves this highly intractable scheduling extension. The performance of the EIG algorithm is compared with that of the best-performing algorithms in the literature developed to solve the BFSP with SDSTs. Extensive numerical tests and statistical analyses verify EIG’s superiority over the benchmark algorithms and show that EIG performs steadily over various operational situations. Applications of the improved algorithm in this study are worthwhile topics to solve other complex scheduling problems.},
  archive      = {J_ASOC},
  author       = {Chen-Yang Cheng and Pourya Pourhejazy and Kuo-Ching Ying and Shi-Yao Huang},
  doi          = {10.1016/j.asoc.2021.107229},
  journal      = {Applied Soft Computing},
  pages        = {107229},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New benchmark algorithm for minimizing total completion time in blocking flowshops with sequence-dependent setup times},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flow interval prediction based on deep residual network and
lower and upper boundary estimation method. <em>ASOC</em>, <em>104</em>,
107228. (<a href="https://doi.org/10.1016/j.asoc.2021.107228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval prediction is an efficient approach for quantifying the uncertainties of future flow. In this paper, a novel method, based on the combination of a deep residual neural network (ResNet) and lower and upper bound estimation (LUBE), is proposed to forecast future flow and construct prediction intervals. LUBE is proposed by optimizing a LUBE-based objective function and adjusting the type and quantity of residual blocks to design a combined residual network. The final proposed interval prediction model is stResNet-LUBE p r o p o s e d proposed . The performance of the stResNet-LUBE p r o p o s e d proposed model is verified using the spatiotemporal dataset of Tunxi, which is a small and medium watershed. The performance of proposed model is mainly evaluated by the root mean square error (RMSE), coefficient of determination (R 2 2 ), and coverage width-based criterion (CWC). The experimental results show that the average values of RMSE, R 2 2 , and CWC of the proposed model are better than those of the spatiotemporal deep learning model stCNN-LUBE p r o p o s e d proposed , the deep learning model LSTM-LUBE p r o p o s e d proposed , and the machine learning model MLP-LUBE p r o p o s e d proposed by 1.881\%, 10.574\%, and 14.113\%; 2.198\%, 15.754\%, and 18.546\%; and 10.572\%, 35.907\%, and 46.819\%, respectively (predict 6-step).},
  archive      = {J_ASOC},
  author       = {Le Yan and Jun Feng and Tingting Hang and Yuelong Zhu},
  doi          = {10.1016/j.asoc.2021.107228},
  journal      = {Applied Soft Computing},
  pages        = {107228},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Flow interval prediction based on deep residual network and lower and upper boundary estimation method},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using following heroes operation in multi-objective
differential evolution for fast convergence. <em>ASOC</em>,
<em>104</em>, 107225. (<a
href="https://doi.org/10.1016/j.asoc.2021.107225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a new hybrid variant of multi-objective differential evolution is developed to improve the extent and the speed of convergence by combining the ability of DE/rand/1 strategy (NSDE1) to retain only better solutions in the selection operation with the ability of Following Heroes operation from adaptive social evolution algorithm to rapidly propagate the effect of the best solutions to the entire population. Such an algorithm is quite useful for fast convergence, particularly in solving computationally intensive multi-objective optimization problems for which it is highly desirable to obtain the better quality optimal solutions in a limited number of function calculations. The performance of the algorithm is first analyzed on thirty multi-objective test problems from ZDT, DTLZ, and WFG test suites for a limited number of function calculations, and the results are compared with four well-established multi-objective optimization algorithms: real-coded elitist non-dominated sorting genetic algorithm (RNSGA-II), non-dominated sorting particle swarm optimization (NSPSO), adaptive social evolution (ASE) and non-dominated sorting differential evolution with DE/rand/1 strategy (NSDE1) and found to be superior. The analysis is further extended to model-based algorithms, Pareto efficient global optimization (ParEGO), kriging for expensive evaluation Pareto (KEEP), and surrogate optimization of computationally expensive multi-objective problems (SOCEMO) which are commonly used for computationally intensive problems. These involve an additional computational cost of evolving the surrogate models . The performance of the proposed algorithm is found to be better than these, though, it has a relatively simple algorithmic structure and does not involve any additional computational cost. The algorithm is then evaluated on computationally intensive multi-objective optimization of bulk polymerization of methyl methacrylate. The numerical results obtained for this problem show better convergence for a limited number of function calculations.},
  archive      = {J_ASOC},
  author       = {Vibhu Trivedi and Manojkumar Ramteke},
  doi          = {10.1016/j.asoc.2021.107225},
  journal      = {Applied Soft Computing},
  pages        = {107225},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using following heroes operation in multi-objective differential evolution for fast convergence},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pattern similarity-based machine learning methods for
mid-term load forecasting: A comparative study. <em>ASOC</em>,
<em>104</em>, 107223. (<a
href="https://doi.org/10.1016/j.asoc.2021.107223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern similarity-based frameworks are widely used for classification and regression problems . Repeated, similar-shaped cycles observed in seasonal time series encourage the use of such frameworks for forecasting. In this paper, we use pattern similarity-based models for mid-term load forecasting. An integral part of these models is the use of patterns of time series sequences for time series representation. Pattern representation ensures input and output data unification through trend filtering and variance equalization. This simplifies the forecasting problem and allows us to use models based on pattern similarity. We consider four such models: nearest-neighbor model, fuzzy neighborhood model, kernel regression model, and general regression neural network . Three variants of the approach were proposed. A basic one and two hybrid solutions combining similarity-based and statistical methods (ARIMA and exponential smoothing). In the experimental part of the work, the proposed models were used to forecast the monthly electricity demand in 35 European countries . The results show the high performance of the proposed models, which outperform both the comparative classical statistical models and machine learning models in terms of accuracy, simplicity, and ease of optimization. Among the proposed variants, a hybrid approach combining similarity-based methods with exponential smoothing turned out to be the most accurate. The study highlights the many advantages of the proposed pattern similarity-based models such as clear operation principles, a small number of parameters to adjust, no training procedure, fast optimization procedure , good generalization ability , ability to work on the newest data without retraining, and delivery of multi-step forecasts.},
  archive      = {J_ASOC},
  author       = {Grzegorz Dudek and Paweł Pełka},
  doi          = {10.1016/j.asoc.2021.107223},
  journal      = {Applied Soft Computing},
  pages        = {107223},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pattern similarity-based machine learning methods for mid-term load forecasting: A comparative study},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting CO2 emissions from chinese marine fleets using
multivariable trend interaction grey model. <em>ASOC</em>, <em>104</em>,
107220. (<a href="https://doi.org/10.1016/j.asoc.2021.107220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reliable prediction of CO 2 emissions from marine fleets plays an important role in the low carbon development of shipping industry . However, CO 2 emissions from marine fleets have its own inherent trends and the influencing factors might have interaction effects, and these problems make it difficult to build an accurate prediction model. To this end, a novel multivariable trend interaction grey model , named TIGM(1, N ), is proposed in this paper. TIGM(1, N ) extends the grey prediction model by integrating three different terms, i.e., interaction, trends, and constant terms into the grey action terms of the classical multivariable grey model. Compared with the classical multivariable grey model, TIGM(1, N ) effectively reflects the impact of input variable interactions and trends on the system’s behavior. To increase the accuracy, the new model’s adjustment coefficient is optimized to obtain optimal time-response function values. The experimental results show that TIGM(1, N ) outperforms linear regression models and other variants of the grey prediction models in predicting the CO 2 emissions from Chinese marine fleets. Finally, the new model is applied to predict the marine fleets’ CO 2 emissions during 2016–2018 and the results demonstrates the feasibility of the proposed model in low carbon development plan of shipping industry and its value in formulating environmental policies.},
  archive      = {J_ASOC},
  author       = {Yun Cao and Kedong Yin and Xuemei Li and Chenchen Zhai},
  doi          = {10.1016/j.asoc.2021.107220},
  journal      = {Applied Soft Computing},
  pages        = {107220},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting CO2 emissions from chinese marine fleets using multivariable trend interaction grey model},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How to design the fair experimental classifier evaluation.
<em>ASOC</em>, <em>104</em>, 107219. (<a
href="https://doi.org/10.1016/j.asoc.2021.107219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many researchers working on classification problems evaluate the quality of developed algorithms based on computer experiments. The conclusions drawn from them are usually supported by the statistical analysis and chosen experimental protocol. Statistical tests are widely used to confirm whether considered methods significantly outperform reference classifiers. Usually, the tests are applied to stratified datasets, which could raise the question of whether data folds used for classification are really randomly drawn and how the statistical analysis supports robust conclusions. Unfortunately, some scientists do not realize the real meaning of the obtained results and overinterpret them. They do not see that inappropriate use of such analytical tools may lead them into a trap. This paper aims to show the commonly used experimental protocols’ weaknesses and discuss if we really can trust in such evaluation methodology, if all presented evaluations are fair and if it is possible to manipulate the experimental results using well-known statistical evaluation methods. We will present that it is possible to choose only such results, confirming the experimenter’s expectation. We will try to show what could be done to avoid such likely unethical behavior. At the end of this work, we will formulate recommendations on improving an experimental protocol to design fair experimental classifier evaluation.},
  archive      = {J_ASOC},
  author       = {Katarzyna Stapor and Paweł Ksieniewicz and Salvador García and Michał Woźniak},
  doi          = {10.1016/j.asoc.2021.107219},
  journal      = {Applied Soft Computing},
  pages        = {107219},
  shortjournal = {Appl. Soft. Comput.},
  title        = {How to design the fair experimental classifier evaluation},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Opposition-based JAYA with population reduction for
parameter estimation of photovoltaic solar cells and modules.
<em>ASOC</em>, <em>104</em>, 107218. (<a
href="https://doi.org/10.1016/j.asoc.2021.107218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To efficiently increase the conversion of solar energy into electricity, it is vitally important to find the appropriate equivalent circuit parameters to execute the modeling, evaluation, and maximum power point tracking on photovoltaic (PV) systems in high quality and efficiency. In this study, an enhanced JAYA (EJAYA) algorithm is proposed for accurately and efficiently estimating the PV system parameters. In EJAYA, it consists of three main improvements: (i) A modified evolution operator, based on the tendency factor adaption, is introduced to increase the probability of approaching the victory. (ii) The simple deterministic population resizing method is incorporated to control the convergence rate during the search. (iii) EJAYA employs generalized opposition-based learning mechanism to avoid being trapped in local optima. Experimental results tested over several different PV models demonstrate the excellence of EJAYA on accuracy, stability, and convergence speed. Additionally, to further highlight the effectiveness of EJAYA, other different modules from the data sheet are tested at different temperature and irradiance. Consequently, EJAYA is superior to become an alternative for the parameter detection of PV cells and modules at various practical conditions.},
  archive      = {J_ASOC},
  author       = {Xi Yang and Wenyin Gong},
  doi          = {10.1016/j.asoc.2021.107218},
  journal      = {Applied Soft Computing},
  pages        = {107218},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Opposition-based JAYA with population reduction for parameter estimation of photovoltaic solar cells and modules},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supply chain design to tackle coronavirus pandemic crisis by
tourism management. <em>ASOC</em>, <em>104</em>, 107217. (<a
href="https://doi.org/10.1016/j.asoc.2021.107217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of the COVID-19 pandemic in the world and the importance of controlling it in all regions have made managing this crisis a great challenge for all countries. In addition to imposing various monetary costs on countries, this pandemic has left many serious damages and casualties. Proper control of this crisis will provide better medical services. Controlling travel and tourists in this crisis is also an effective factor. Hence, the proposed model wants to control the crisis by controlling the volume of incoming tourists to each city and region by closing the entry points of that region, which reduces the inpatients. The proposed multi-objective model is designed to aim at minimizing total costs, minimizing the tourist patients, and maximizing the number of city patients. The Improved Multi-choice Goal programming (IMCGP) method has been used to solve the multi-objective problem. The model examines the results by considering a case study. Sensitivity analyses and managerial insight are also provided. According to the results obtained from the model and case study, two medical centers with the capacity of 300 and 700 should be opened if the entry points are not closed.},
  archive      = {J_ASOC},
  author       = {Faezeh Motevalli-Taher and Mohammad Mahdi Paydar},
  doi          = {10.1016/j.asoc.2021.107217},
  journal      = {Applied Soft Computing},
  pages        = {107217},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Supply chain design to tackle coronavirus pandemic crisis by tourism management},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social learning discrete particle swarm optimization based
two-stage x-routing for IC design under intelligent edge computing
architecture. <em>ASOC</em>, <em>104</em>, 107215. (<a
href="https://doi.org/10.1016/j.asoc.2021.107215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the core features of Intelligent Edge Computing (IEC) is real-time decision making, therefore low delay is more important for IC design under IEC architecture. And in very large scale integration routing, wirelength is one of the most important indexes affecting the final delay of the IC design . Therefore, this paper introduces the X-routing with more potential for wirelength optimization and the Steiner Minimum Tree (SMT), which is the best routing model in multi-terminal nets. Then, based on Particle Swarm Optimization (PSO) technique which has the strong global optimization ability in Soft Computing, an effective Two-Stage X-routing Steiner minimum tree construction algorithm is proposed. The proposed algorithm is divided into two stages: social learning discrete PSO searching and wirelength reduction . In the first stage, two excellent strategies are proposed to maintain a good balance between exploration and exploitation capabilities of the PSO technique: (1) Chaotic decreasing inertia weight combined with mutation operator is set to enhance the exploration capability. (2) A new social learning approach combined with crossover operator is designed to ensure the diverse evolution of the swarm while maintaining the exploitation capability. In the second stage, a strategy based on local topology optimization is proposed to further reduce the length of X-routing Steiner tree . Experiments show that the proposed algorithm can achieve the best wirelength optimization and has a strong stability, especially for large-scale SMT problem, so as to better satisfy the demand of low delay of IC design under IEC architecture.},
  archive      = {J_ASOC},
  author       = {Genggeng Liu and Xiaohua Chen and Ruping Zhou and Saijuan Xu and Yeh-Cheng Chen and Guolong Chen},
  doi          = {10.1016/j.asoc.2021.107215},
  journal      = {Applied Soft Computing},
  pages        = {107215},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Social learning discrete particle swarm optimization based two-stage X-routing for IC design under intelligent edge computing architecture},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generative adversarial neural network model for industrial
boiler data repair. <em>ASOC</em>, <em>104</em>, 107214. (<a
href="https://doi.org/10.1016/j.asoc.2021.107214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many achievements have been made in using neural networks to predict complex nonlinear industrial activities. The assumption that these prediction models established is that the various data collected by the sensors are available and accurate. However, accidents always happen beyond assumptions. The interference and damage of sensors are common phenomena in actual industrial production. In this case, the data collected by the sensors is incorrect and unavailable and cannot be fed into the prediction model for work. Incorrect data leads to inaccurate predictions, and the predictions lead to incorrect actions, which ultimately affect the activities of the entire industrial assembly line. How is this kind of problem data dealt with? This is the first time that generative adversarial networks (GAN) technology has been used to repair missing boiler data. Although GAN technology was originally used for image processing , we took the lead in transferring this technology to the issue of boiler data repair . Furthermore, we proposed a novel model structure that is more suitable for boiler data repair . Our method has rigorous theoretical feasibility and has passed the experimental test.},
  archive      = {J_ASOC},
  author       = {Xiaobin Hu and Guoqiang Li and Peifeng Niu and Jianmei Wang and Linlin Zha},
  doi          = {10.1016/j.asoc.2021.107214},
  journal      = {Applied Soft Computing},
  pages        = {107214},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A generative adversarial neural network model for industrial boiler data repair},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-platform dynamic goods recommendation system based on
reinforcement learning and social networks. <em>ASOC</em>, <em>104</em>,
107213. (<a href="https://doi.org/10.1016/j.asoc.2021.107213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of cold start, gray sheep and sparsity of the traditional collaborative filtering recommendation system, this paper proposes a cross-platform dynamic goods recommendation system based on reinforcement learning and edge computing . First of all, this system models the current friendship relationship networks and potential friendship relationship networks, it also constructs two layers preference prediction models. Then, we consider the frequent change characteristic of social networks and shopping platforms, we design a dynamic reinforcement learning method and edge computing to learn the minimized entropy loss error. Finally, we finish the validation experiments based on the real datasets, the results show the proposed system realizes better link prediction accuracy, and using our proposed system can obtain an obvious increase in the accuracy compared to the existing of collaborative filtering recommendation systems.},
  archive      = {J_ASOC},
  author       = {Gang Ke and Hong-Le Du and Yeh-Cheng Chen},
  doi          = {10.1016/j.asoc.2021.107213},
  journal      = {Applied Soft Computing},
  pages        = {107213},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-platform dynamic goods recommendation system based on reinforcement learning and social networks},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta deep learning based rotating machinery health
prognostics toward few-shot prognostics. <em>ASOC</em>, <em>104</em>,
107211. (<a href="https://doi.org/10.1016/j.asoc.2021.107211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven health prognostic is attracting more and more attention to machinery prognostic and health management. It enables machinery to realize predictive maintenance and rarely depends on prior knowledge of degradation mechanisms . However, cross-domain health prognostic may lack enough measured data as supports, and this bottleneck is particularly prominent in high-end manufacturing. As such, this paper aims to improve prediction performances under limited data coupled with variable working conditions. Meta learning is introduced into this field for the first time, and meta deep learning (MDL) based health prognostic methodologies toward few-shot prognostics are further proposed. To be specific, time–frequency images and time-series data are first picked up for abstracting domain-invariant degradation indicators based on the integration of covariance matrices and maximum mean discrepancy. Then the subtask and cross-subtask level gradient based optimization architecture is conducted to abstract more general degradation knowledge for prognostics models’ adaptation. Based on the architecture, two variants termed as meta convolutional neural network (meta CNN) and meta gated recurrent unit (meta GRU) are proposed to accomplish few-shot prognostics with different forms of degradation indicators. Thirdly, three cases of run-to-failed machinery experiments are employed for a large number of verifications to avoid unexpected results. Finally, appealing predictions compared with existing methods demonstrate the superiority of our proposed MDL health prognostics.},
  archive      = {J_ASOC},
  author       = {Peng Ding and Minping Jia and Xiaoli Zhao},
  doi          = {10.1016/j.asoc.2021.107211},
  journal      = {Applied Soft Computing},
  pages        = {107211},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Meta deep learning based rotating machinery health prognostics toward few-shot prognostics},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Utilizing IoT to design a relief supply chain network for
the SARS-COV-2 pandemic. <em>ASOC</em>, <em>104</em>, 107210. (<a
href="https://doi.org/10.1016/j.asoc.2021.107210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current universally challenging SARS-COV-2 pandemic has transcended all the social, logical, economic, and mortal boundaries regarding global operations. Although myriad global societies tried to address this issue, most of the employed efforts seem superficial and failed to deal with the problem, especially in the healthcare sector . On the other hand, the Internet of Things (IoT) has enabled healthcare system for both better understanding of the patient’s condition and appropriate monitoring in a remote fashion. However, there has always been a gap for utilizing this approach on the healthcare system especially in agitated condition of the pandemics. Therefore, in this study, we develop two innovative approaches to design a relief supply chain network is by using IoT to address multiple suspected cases during a pandemic like the SARS-COV-2 outbreak. The first approach (prioritizing approach) minimizes the maximum ambulances response time, while the second approach (allocating approach) minimizes the total critical response time. Each approach is validated and investigated utilizing several test problems and a real case in Iran as well. A set of efficient meta-heuristics and hybrid ones is developed to optimize the proposed models. The proposed approaches have shown their versatility in various harsh SARS-COV-2 pandemic situations being dealt with by managers. Finally, we compare the two proposed approaches in terms of response time and route optimization using a real case study in Iran. Implementing the proposed IoT-based methodology in three consecutive weeks, the results showed 35.54\% decrease in the number of confirmed cases.},
  archive      = {J_ASOC},
  author       = {Ali Zahedi and Amirhossein Salehi-Amiri and Neale R. Smith and Mostafa Hajiaghaei-Keshteli},
  doi          = {10.1016/j.asoc.2021.107210},
  journal      = {Applied Soft Computing},
  pages        = {107210},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Utilizing IoT to design a relief supply chain network for the SARS-COV-2 pandemic},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evolutionary/heuristic-based proof searching framework
for interactive theorem prover. <em>ASOC</em>, <em>104</em>, 107200. (<a
href="https://doi.org/10.1016/j.asoc.2021.107200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proof development process in interactive theorem provers (ITPs) requires the users to manually search for proofs by interacting with proof assistants. The activity of finding the correct proofs can become quite cumbersome and time consuming for users. To make the proof searching process easier in proof assistants, we provide an evolutionary/heuristic-based framework. The basic idea for the framework is to first generate random proof sequences from a population of frequently occurring proof steps that are discovered with sequential pattern mining. Generated proof sequences are then evolved till their fitness match the fitness of the target (or original) proof sequences. Three algorithms based on the proposed framework are developed using the Genetic Algorithm (GA), Simulated Annealing (SA) and Particle Swarm Optimization (PSO). Extensive experiments are performed to investigate the performance of the proposed algorithms using the HOL4 proof assistant. Results have shown that the proposed algorithms can efficiently evolve the random sequences to obtain the target sequences. In comparison, PSO performed better than SA and SA performed better than GA. In general, the experimental results suggest that combining evolutionary/heuristic algorithms with proof assistants allow efficient support for proof finding/optimization.},
  archive      = {J_ASOC},
  author       = {M. Saqib Nawaz and M. Zohaib Nawaz and Osman Hasan and Philippe Fournier-Viger and Meng Sun},
  doi          = {10.1016/j.asoc.2021.107200},
  journal      = {Applied Soft Computing},
  pages        = {107200},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary/heuristic-based proof searching framework for interactive theorem prover},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MARCOS technique under intuitionistic fuzzy environment for
determining the COVID-19 pandemic performance of insurance companies in
terms of healthcare services. <em>ASOC</em>, <em>104</em>, 107199. (<a
href="https://doi.org/10.1016/j.asoc.2021.107199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing and ranking private health insurance companies provides insurance agencies, insurance customers, and authorities with a reliable instrument for the insurance decision-making process. Moreover, because the world’s insurance sector suffers from a gap of evaluation of private health insurance companies during the COVID-19 outbreak, the need for a reliable, useful, and comprehensive decision tool is obvious. Accordingly, this article aims to identify insurance companies’ priority ranking in terms of healthcare services in Turkey during the COVID-19 outbreak through a multi-criteria performance evaluation methodology. Herein, alternatives are evaluated and then ranked as per 7 criteria and assessments of 5 experts. Experts’ judgments and assessments are full of uncertainties. We propose a Measurement of Alternatives and Ranking according to the Compromise Solution (MARCOS) technique under an intuitionistic fuzzy environment to rank insurance companies. The outcomes yielded ten insurance companies ranking in terms of healthcare services in the era of COVID-19. The payback period , premium price, and network are determined as the most crucial factors. Finally, a comprehensive sensitivity analysis is performed to verify the proposed methodology’s stability and effectiveness. The introduced approach met the insurance assessment problem during the COVID-19 pandemic very satisfactory manner based on sensitivity analysis findings.},
  archive      = {J_ASOC},
  author       = {Fatih Ecer and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2021.107199},
  journal      = {Applied Soft Computing},
  pages        = {107199},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MARCOS technique under intuitionistic fuzzy environment for determining the COVID-19 pandemic performance of insurance companies in terms of healthcare services},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolving simple and accurate symbolic regression models via
asynchronous parallel computing. <em>ASOC</em>, <em>104</em>, 107198.
(<a href="https://doi.org/10.1016/j.asoc.2021.107198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning , reducing the complexity of a model can help to improve its computational efficiency and avoid overfitting. In genetic programming (GP), the model complexity reduction is often achieved by reducing the size of evolved expressions. However, previous studies have demonstrated that the expression size reduction does not necessarily prevent model overfitting. Therefore, this paper uses the evaluation time – the computational time required to evaluate a GP model on data – as the estimate of model complexity. The evaluation time depends not only on the size of evolved expressions but also their composition , thus acting as a more nuanced measure of model complexity than the expression size alone. To discourage complexity, this study employs a novel method called asynchronous parallel GP (APGP) that introduces a race condition in the evolutionary process of GP; the race offers an evolutionary advantage to the simple solutions when their accuracy is competitive. To evaluate the proposed method, it is compared to the standard GP (GP) and GP with bloat control (GP+BC) methods on six challenging symbolic regression problems . APGP produced models that are significantly more accurate (on 6/6 problems) than those produced by both GP and GP+BC. In terms of complexity control, APGP prevailed over GP but not over GP+BC; however, GP+BC produced simpler solutions at the cost of test-set accuracy. Moreover, APGP took a significantly lower number of evaluations than both GP and GP+BC to meet a target training fitness in all tests. Our analysis of the proposed APGP also involved: (1) an ablation study that separated the proposed measure of complexity from the race condition in APGP and (2) the study of an initialisation scheme that encourages functional diversity in the initial population that improved the results for all the GP methods. These results question the overall benefits of bloat control and endorse the employment of both the evaluation time as an estimate of model complexity and the proposed APGP method for controlling it.},
  archive      = {J_ASOC},
  author       = {Aliyu Sani Sambo and R. Muhammad Atif Azad and Yevgeniya Kovalchuk and Vivek Padmanaabhan Indramohan and Hanifa Shah},
  doi          = {10.1016/j.asoc.2021.107198},
  journal      = {Applied Soft Computing},
  pages        = {107198},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving simple and accurate symbolic regression models via asynchronous parallel computing},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Expeditious COVID-19 similarity measure tool based on
consolidated SCA algorithm with mutation and opposition operators.
<em>ASOC</em>, <em>104</em>, 107197. (<a
href="https://doi.org/10.1016/j.asoc.2021.107197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is a global pandemic that aroused the interest of scientists to prevent it and design a drug for it. Nowadays, presenting intelligent biological data analysis tools at a low cost is important to analyze the biological structure of COVID-19. The global alignment algorithm is one of the important bioinformatics tools that measure the most accurate similarity between a pair of biological sequences. The huge time consumption of the standard global alignment algorithm is its main limitation especially for sequences with huge lengths. This work proposed a fast global alignment tool (G-Aligner) based on meta-heuristic algorithms that estimate similarity measurements near the exact ones at a reasonable time with low cost. The huge length of sequences leads G-Aligner based on standard Sine–Cosine optimization algorithm (SCA) to trap in local minima. Therefore, an improved version of SCA was presented in this work that is based on integration with PSO . Besides, mutation and opposition operators are applied to enhance the exploration capability and avoiding trapping in local minima. The performance of the improved SCA algorithm (SP-MO) was evaluated on a set of IEEE CEC functions. Besides, G-Aligner based on the SP-MO algorithm was tested to measure the similarity of real biological sequence. It was used also to measure the similarity of the COVID-19 virus with the other 13 viruses to validate its performance. The tests concluded that the SP-MO algorithm has superiority over the relevant studies in the literature and produce the highest average similarity measurements 75\% of the exact one.},
  archive      = {J_ASOC},
  author       = {Mohamed Issa},
  doi          = {10.1016/j.asoc.2021.107197},
  journal      = {Applied Soft Computing},
  pages        = {107197},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Expeditious COVID-19 similarity measure tool based on consolidated SCA algorithm with mutation and opposition operators},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An RUL prediction approach for lithium-ion battery based on
SADE-MESN. <em>ASOC</em>, <em>104</em>, 107195. (<a
href="https://doi.org/10.1016/j.asoc.2021.107195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting the remaining useful life of lithium-ion batteries is critical to battery health management systems . Aiming at the problems of low long-term prediction accuracy, unstable model output, and difficult key parameter selection, this paper proposes a self-adaptive differential evolution optimized monotonic echo state network prediction method. First, we analyze the life decay characteristics of Li-ion batteries and select appropriate indirect health indicators to replace the capacity based on the partial correlation coefficient analysis. Then use the self-adaptive differential evolution algorithm to optimize the free parameters of the monotonic echo state network to maintain the monotonic relationship between input and output. Finally, the remaining useful life indirect prediction model is established. This paper uses NASA Li-ion battery experimental data and independent experimental data to verify the feasibility, followed by the different starting points experiments and cut-off voltage experiments. The accuracy of the proposed method is compared with other commonly used artificial intelligence prediction algorithms. Experimental results prove that this method has high prediction accuracy and stable output.},
  archive      = {J_ASOC},
  author       = {Yufan Ji and Zewang Chen and Yong Shen and Ke Yang and Youren Wang and Jiang Cui},
  doi          = {10.1016/j.asoc.2021.107195},
  journal      = {Applied Soft Computing},
  pages        = {107195},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An RUL prediction approach for lithium-ion battery based on SADE-MESN},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of moth flame optimization heuristics for integrated
power plant system containing stochastic wind. <em>ASOC</em>,
<em>104</em>, 107193. (<a
href="https://doi.org/10.1016/j.asoc.2021.107193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this investigation, nature-inspired heuristic strategy exploiting moth flame optimization (MFO) algorithm combined with active-set algorithm (ASA), interior point algorithm (IPA) and sequential quadratic programming (SQP) are presented to take care of the enhancement issues of economic load dispatch (ELD) problem involving valve point loading effect (VPLE) and stochastic wind (SW). The strength of MFO algorithm is used as a global search mechanism that explore and exploit the entire search space while ASA, IPA and SQP are responsible for refinement of local optimum. The performance of the design system is based on 40 generating units including 37 thermal and 3 wind power units and is evaluated to verify the effectiveness of the scheme. The worth of the design integrated heuristic of MFO algorithm is endorsed through outcomes of the state of the art counterpart solvers in case of ELD problems integrated with wind power units in terms of cost minimization and computational complexity parameters.},
  archive      = {J_ASOC},
  author       = {Babar Sattar Khan and Muhammad Asif Zahoor Raja and Affaq Qamar and Naveed Ishtiaq Chaudhary},
  doi          = {10.1016/j.asoc.2021.107193},
  journal      = {Applied Soft Computing},
  pages        = {107193},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of moth flame optimization heuristics for integrated power plant system containing stochastic wind},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interval-valued q-rung orthopair fuzzy FMEA application to
improve risk evaluation process of tool changing manipulator.
<em>ASOC</em>, <em>104</em>, 107192. (<a
href="https://doi.org/10.1016/j.asoc.2021.107192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking actions to prevent the occurrence of failure in advance, rather than improving reliability through post-mortem testing is crucial for improvement of products’ quality and efficiency. As a typical prevention reliability analysis method , failure mode and effects analysis (FMEA) has an innate advantage in conducting this improvement. However, traditional FMEA also contains some deficiencies in rating risks, weighting risk factors and ranking failure modes. In this paper, a scientific risk evaluation method capable of solving these deficiencies is proposed, which combines interval-valued q -rung orthopair fuzzy-deviation maximization method (IV q -ROF-DMM) with interval-valued q -rung orthopair fuzzy-additive ratio assessment (IV q -ROF-ARAS) method. Moreover, the concept of sub-risk factors is developed to make evaluation results more practical. To make experts’ evaluation information more consistent, a consistency inspection and improvement process is conducted and experts’ evaluation information are fused by IV q -ROF weighted average (IV q -ROFWG) operator and IV q -ROF weighted Maclaurin symmetric mean (IV q -ROFWMSM) operator. Finally, a real case vis-à-vis tool changing manipulator of tool magazine is illustrated, and discussion results indicate that the proposed method is rational and valid.},
  archive      = {J_ASOC},
  author       = {Chuanxi Jin and Yan Ran and Genbao Zhang},
  doi          = {10.1016/j.asoc.2021.107192},
  journal      = {Applied Soft Computing},
  pages        = {107192},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval-valued q-rung orthopair fuzzy FMEA application to improve risk evaluation process of tool changing manipulator},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Homotopy-based hyper-heuristic searching approach for
reciprocal feedback inversion of groundwater contamination source and
aquifer parameters. <em>ASOC</em>, <em>104</em>, 107191. (<a
href="https://doi.org/10.1016/j.asoc.2021.107191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Groundwater contamination source identification is critical for taking effective measures to design remediation strategies, assess contamination risks, and confirm contamination responsibilities. To resolve the “equifinality” problem resulting from simultaneous inversion of contamination source characteristics and aquifer parameters at dense non-aqueous phase liquid-contaminated sites, two reciprocal optimization frames for separately identifying the contamination sources and aquifer parameters were designed and connected. The two sets of identification results were corrected stepwise by means of a feedback correction iteration process , thereby sufficiently improving the identification accuracy. The ensemble learning machine (ESLM) incorporating Kriging, radical basis function neural network , support vector regression , and wavelet kernel extreme learning machine with swarm intelligence (SI) algorithm was embedded into the reciprocal inversion iterations to replace the multiphase flow simulation model for significantly improving the computational efficiency. To improve the optimization efficiency, a hyper-heuristic homotopy algorithm was constructed for segmentally searching the global optimum in wider areas with low dependence on initial values. Results showed that the combined application of SI-based ensemble learning machine (SI-ESLM) and hyper-heuristic homotopy algorithm effectively accomplished the simultaneous identification of contamination sources and aquifer parameters with high efficiency, while maintaining high accuracy. The SI-ESLM sufficiently approximated the outputs of the multiphase flow simulation model with increased certainty ( R 2 = 0 R2=0 .9977), while the mean relative error was limited to 1.5388\%. Compared to traditional heuristic algorithms , this application of reciprocal inversion iterations and the hyper-heuristic homotopy algorithm significantly reduced the mean relative error of identification results from 6.51\% to 1.03\%.},
  archive      = {J_ASOC},
  author       = {Zeyu Hou and Wangmei Lao and Yu Wang and Wenxi Lu},
  doi          = {10.1016/j.asoc.2021.107191},
  journal      = {Applied Soft Computing},
  pages        = {107191},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Homotopy-based hyper-heuristic searching approach for reciprocal feedback inversion of groundwater contamination source and aquifer parameters},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An intelligent multi-objective framework for optimizing
friction-stir welding process parameters. <em>ASOC</em>, <em>104</em>,
107190. (<a href="https://doi.org/10.1016/j.asoc.2021.107190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The comprehensive intention of this paper is to evaluate the optimal welding parameters for joining two dissimilar materials by friction stir welding (FSW) process which is termed as a green manufacturing technology in order to generate quality joints . Conventionally, the optimization of process parameters experimentally is carried out by a time-consuming trial and error technique. Also, the effect of two or more parameters cannot be considered at the same time experimentally. Due to this, mathematical modelling is carried out to determine the optimal welding parameters. This paper focuses on a theory that hybridizes the exploring capability of non-dominated sorting genetic algorithm-II (NSGA-II) and exploitation capability of Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) method. The performance parameters of FSW process has a high level of nonlinearity due to which artificial neural network (ANN) is employed to mathematically model the performance measures . Full factorial design is employed to plan the experimentations. At random, ten dataset were used for testing the ANN structures and the remaining data were used to train the network for predicting the ultimate tensile strength (UTS), hardness and impact energy. The optimal networks had root mean square error (RMSE) and mean absolute error (MAE) of 0.7486 and 0.0074, 0.4045 and 0.003 and 0.1866 and 0.0354 respectively. The transfer equations developed from the ANN models is used as the fitness function for the NSGA-II algorithm. The optimal welding parameters obtained from the proposed hybrid algorithm are Tool rotation speed = 1693rpm, Traverse speed= 2.72 mm/s and Copper as advancing side material. Validation of the optimal result is done by carrying out experiments that are conducted at the simulated optimal parameter. Finally, a macro and microstructural study of the welded joint at the simulated optimal parameter is carried out in order to assess the behaviour of the weld.},
  archive      = {J_ASOC},
  author       = {Tanmoy Medhi and Syed Abou Iltaf Hussain and Barnik Saha Roy and Subhash Chandra Saha},
  doi          = {10.1016/j.asoc.2021.107190},
  journal      = {Applied Soft Computing},
  pages        = {107190},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intelligent multi-objective framework for optimizing friction-stir welding process parameters},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cluster-based information fusion for probabilistic risk
analysis in complex projects under uncertainty. <em>ASOC</em>,
<em>104</em>, 107189. (<a
href="https://doi.org/10.1016/j.asoc.2021.107189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a hybrid soft computing approach that integrates the Dempster–Shafer (D–S) evidence theory and cluster analysis for probabilistic risk analysis in complex projects under uncertainty. The fusion model tends to solve multi-criteria decision-making problems with a focus on the information content reflected from evidence. Risk factors are quantified into a continuous numeric scale for risk level classification and each factor value is turned into a basic probability assignment (BPA). A sorting operator is used to aggregate the evidence into risk level based clusters. The D–S evidence theory is first used to fuse similar evidence within each cluster, and then the weighted ratio method is used to fuse conflict evidence between clusters. The fused result is defuzzied into a crisp value to give a conveniently referred value for decision-making. Global sensitivity analysis is conducted to depict the effect of each risk factor on the overall estimated risk level. The developed approach is used to assess the water leakage condition of Line 2 of the Wuhan metro system in China to demo its feasibility. The tunnel is assessed to lie in a good condition with a tolerance of 5\% measurement error. The proposed two-step fusion process is capable to reserve more details through computation and enhances the confidence in risk classification results compared to that based on a separate piece of evidence. This research contributes to (a) a systematic classification and fusion-based quantitative risk analysis method; (b) practical risk assessment of water leakage in operational tunnels.},
  archive      = {J_ASOC},
  author       = {Limao Zhang and Ying Wang and Xianguo Wu},
  doi          = {10.1016/j.asoc.2021.107189},
  journal      = {Applied Soft Computing},
  pages        = {107189},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cluster-based information fusion for probabilistic risk analysis in complex projects under uncertainty},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised regression using diffusion on graphs.
<em>ASOC</em>, <em>104</em>, 107188. (<a
href="https://doi.org/10.1016/j.asoc.2021.107188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world machine learning applications, unlabeled training data are readily available, but labeled data are expensive and hard to obtain. Therefore, semi-supervised learning algorithms have gathered much attention. Previous studies in this area mainly focused on a semi-supervised classification problem, whereas semi-supervised regression has received less attention. In this paper, we proposed a novel semi-supervised regression algorithm using heat diffusion with a boundary-condition that guarantees a closed-form solution. Experiments from artificial and real datasets from business, biomedical, physical, and social domain show that the boundary-based heat diffusion method can effectively outperform the top state of the art methods.},
  archive      = {J_ASOC},
  author       = {Mohan Timilsina and Alejandro Figueroa and Mathieu d’Aquin and Haixuan Yang},
  doi          = {10.1016/j.asoc.2021.107188},
  journal      = {Applied Soft Computing},
  pages        = {107188},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-supervised regression using diffusion on graphs},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-attribute group decision making method considering
both the correlation coefficient and hesitancy degrees under
interval-valued intuitionistic fuzzy environment. <em>ASOC</em>,
<em>104</em>, 107187. (<a
href="https://doi.org/10.1016/j.asoc.2021.107187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel multiple attribute group decision making (MAGDM) method based on the correlation coefficient and hesitancy degrees under interval-valued intuitionistic fuzzy environment. Firstly, the conception of individual hesitancy degree and group hesitancy degree are defined to ensure the effective communication of information among group members and overcome the restrictions of the methodology of “majority rule” that some critical information of decision-making would be treated as conflicting opinions to be modified. Secondly, the correlation coefficients of interval-valued intuitionistic fuzzy set (IVIFS) are utilized to measure the similarity instead of distance functions in order to alleviate the drawbacks that do not consider the parameters of hesitancy. Thirdly, we extend the subjective assignment methods on utilizing the IVIFS to assign the weights of different decision makers in order to better address uncertainty in the MAGDM problem. Finally, TOPSIS and Linear programming optimization method are used to calculate the optimal attribute weight, which results in more accurate weights. A real-word application example has been presented to demonstrate the working of the proposed methodology. Moreover, a thorough comparison has been done with related existing works in order to show the validity of this methodology.},
  archive      = {J_ASOC},
  author       = {You Peng and Liu Xiaohe and Sun Jianbo},
  doi          = {10.1016/j.asoc.2021.107187},
  journal      = {Applied Soft Computing},
  pages        = {107187},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-attribute group decision making method considering both the correlation coefficient and hesitancy degrees under interval-valued intuitionistic fuzzy environment},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Higher-order terminal sliding mode controller for fault
accommodation of lipschitz second-order nonlinear systems using fuzzy
neural network. <em>ASOC</em>, <em>104</em>, 107186. (<a
href="https://doi.org/10.1016/j.asoc.2021.107186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a higher-order terminal sliding mode control is proposed for fault accommodation of a class of Lipschitz second-order nonlinear systems . This approach is designed based on a combining between a novel third-order fast terminal sliding mode surface (TOFTSMS), which is designed to preserve the merits of the PID sliding surface and the fast terminal sliding mode (FTSM) surface, and a continuous control law based on higher-order sliding mode (HOSM) control strategy. However, the proposed TOFTSMC requires an exact dynamics model of the system and the prior knowledge of the bounded value of the uncertainties and faults in the design. In order to exclude the requirements, an adaptive fuzzy neural network is integrated; yielding a novel adaptive fuzzy neural TOFTSMC (AFN-TOFTSMC). The proposed analytical results are then applied to the attitude control of a spacecraft. Simulation results clearly demonstrate the great performance of the proposed algorithm compared to other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Mien Van},
  doi          = {10.1016/j.asoc.2021.107186},
  journal      = {Applied Soft Computing},
  pages        = {107186},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Higher-order terminal sliding mode controller for fault accommodation of lipschitz second-order nonlinear systems using fuzzy neural network},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural networks for improving physical accuracy of 2D
and 3D multi-mineral segmentation of rock micro-CT images.
<em>ASOC</em>, <em>104</em>, 107185. (<a
href="https://doi.org/10.1016/j.asoc.2021.107185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of 3D micro-Computed Tomographic ( μ μ CT) images of rock samples is essential for further Digital Rock Physics (DRP) analysis, however, conventional methods such as thresholding and watershed segmentation are susceptible to user-bias. Deep Convolutional Neural Networks (CNNs) have produced accurate pixelwise semantic (multi-category) segmentation results with natural images and μ μ CT rock images, however, physical accuracy is not well documented. The performance of 4 CNN architectures is tested for 2D and 3D cases in 10 configurations. Manually segmented μ μ CT images of Mt. Simon Sandstone guided by QEMSCANs are treated as ground truth and used as training and validation data, with a high voxelwise accuracy (over 99\%) achieved. Downstream analysis is used to validate physical accuracy. The topology of each mineral is measured, the pore space absolute permeability and single/mixed wetting multiphase flow is modelled with direct simulation. These physical measures show high variance, with models that achieve 95\%+ in voxelwise accuracy possessing permeabilities and connectivities orders of magnitude off. A network architecture is introduced as a hybrid fusion of U-Net and ResNet , combining short and long skip connections in a Network-in-Network configuration, which overall outperforms U-Net and ResNet variants in some minerals, while outperforming SegNet in all minerals in voxelwise and physical accuracy measures. The network architecture and the dataset volume fractions influence accuracy trade-off since sparsely occurring minerals are over-segmented by lower accuracy networks such as SegNet at the expense of under-segmenting other minerals which can be alleviated with loss weighting. This is an especially important consideration when training a physically accurate model for segmentation.},
  archive      = {J_ASOC},
  author       = {Ying Da Wang and Mehdi Shabaninejad and Ryan T. Armstrong and Peyman Mostaghimi},
  doi          = {10.1016/j.asoc.2021.107185},
  journal      = {Applied Soft Computing},
  pages        = {107185},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep neural networks for improving physical accuracy of 2D and 3D multi-mineral segmentation of rock micro-CT images},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CoVNet-19: A deep learning model for the detection and
analysis of COVID-19 patients. <em>ASOC</em>, <em>104</em>, 107184. (<a
href="https://doi.org/10.1016/j.asoc.2021.107184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ongoing fight with Novel Corona Virus, getting quick treatment, and rapid diagnosis reports have become an act of high priority. With millions getting infected daily and a fatality rate of 2\%, we made it our motive to contribute a little to solve this real-world problem by accomplishing a significant and substantial method for diagnosing COVID-19 patients. The Exponential growth of COVID-19 cases worldwide has severely affected the health care system of highly populated countries due to proportionally a smaller number of medical practitioners, testing kits, and other resources, thus becoming essential to identify the infected people. Catering to the above problems, the purpose of this paper is to formulate an accurate, efficient, and time-saving method for detecting positive corona patients. In this paper, an Ensemble Deep Convolution Neural Network model “CoVNet-19” is being proposed that can unveil important diagnostic characteristics to find COVID-19 infected patients using X-ray images chest and help radiologists and medical experts to fight this pandemic. The experimental results clearly show that the overall classification accuracy obtained with the proposed approach for three-class classification among COVID-19, Pneumonia , and Normal is 98.28\%, along with an average precision and Recall of 98.33\% and 98.33\%, respectively. Besides this, for binary classification between Non-COVID and COVID Chest X-ray images, an overall accuracy of 99.71\% was obtained. Having a high diagnostic accuracy , our proposed ensemble Deep Learning classification model can be a productive and substantial contribution to detecting COVID-19 infected patients.},
  archive      = {J_ASOC},
  author       = {Priyansh Kedia and Anjum and Rahul Katarya},
  doi          = {10.1016/j.asoc.2021.107184},
  journal      = {Applied Soft Computing},
  pages        = {107184},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CoVNet-19: A deep learning model for the detection and analysis of COVID-19 patients},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive fuzzy logic with self-tuned membership functions
based repetitive learning control of robotic manipulators.
<em>ASOC</em>, <em>104</em>, 107183. (<a
href="https://doi.org/10.1016/j.asoc.2021.107183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing demand for using robotic manipulators in industrial applications, controllers specific for performing repeatable tasks are required. These controllers must also be robust to model uncertainties. To address this research issue, a repetitive learning control method fused with adaptive fuzzy logic techniques is designed. Specifically, modeling uncertainties are first modeled with a fuzzy logic network and an adaptive fuzzy logic strategy with online tuning is designed. The stability is investigated via Lyapunov type techniques where global uniform ultimate boundedness of closed loop system is guaranteed. Numerical simulation results obtained from a two degree of freedom robot manipulator model and experiments performed on a robot manipulator demonstrate the efficacy of the proposed control methodology.},
  archive      = {J_ASOC},
  author       = {B. Melih Yilmaz and Enver Tatlicioglu and Aydogan Savran and Musa Alci},
  doi          = {10.1016/j.asoc.2021.107183},
  journal      = {Applied Soft Computing},
  pages        = {107183},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive fuzzy logic with self-tuned membership functions based repetitive learning control of robotic manipulators},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An outlier detection algorithm for categorical matrix-object
data. <em>ASOC</em>, <em>104</em>, 107182. (<a
href="https://doi.org/10.1016/j.asoc.2021.107182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is a significant problem in data mining and machine learning which aims to discover objects in a data set that do not conform to well-defined notions of expected behavior. Generally, the input of the existing outlier detection algorithms is a collection of n n objects and each object is described by a feature vector. However, in many real world applications , an object is not only described by one feature vector, but a number of feature vectors. In this paper, we define an object described by more than one feature vector as a matrix-object. Inspired by the concepts of cohesion and coupling in software engineering , we define the coupling of a matrix-object based on the average distance between it and other matrix-objects, and define its cohesion based on information entropy and mutual information. On this basis, the outlier factor of a matrix-object is given, and an outlier detection algorithm for categorical matrix-object data is proposed. The experimental results on real and synthetic data sets have shown that the proposed outlier detection algorithm can effectively detect outliers for the matrix-object data set compared with other algorithms.},
  archive      = {J_ASOC},
  author       = {Fuyuan Cao and Xiaolin Wu and Liqin Yu and Jiye Liang},
  doi          = {10.1016/j.asoc.2021.107182},
  journal      = {Applied Soft Computing},
  pages        = {107182},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An outlier detection algorithm for categorical matrix-object data},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven time series prediction based on multiplicative
neuron model artificial neuron network. <em>ASOC</em>, <em>104</em>,
107179. (<a href="https://doi.org/10.1016/j.asoc.2021.107179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a hybrid approach combining the neural network and the nonlinear filtering to model and predict terrain profiles for both air and ground vehicles. To simplify the neural network structures and reduce the number of synaptic weights and biases, the multiplicative neuron model (MNM) is utilized to describe the relationship between the unknown elevation ahead and the last few height values on the terrain profile. This paper adopts the gradient descent algorithm (GDA) to train the MNM terrain model and stores the MNM parameters into a nonlinear state-space model. The state vector in the state-space model (i.e., parameters of MNM) evolve agilely once absorbing new observations and measurement of elevation values by the Bootstrap Particle Filter (BPF) algorithm. Data-driven predictions on terrain profiles can be achieved through the updated MNM model. This study utilizes two types of terrain profiles to verify the effectiveness of the proposed MNM–BPF approach. Experimental results on two public datasets indicate that the proposed approach not only overcomes the limitations of conventional terrain models that cannot dynamically tune model parameters according to the newly input information, but also provides a simple but effective single-layered network for modeling terrain profiles. The well-trained MNM–BPF model can achieve the lowest root mean square errors ( RMSE ) (i.e., 17.3211 on the NS profile, 19.0366 on the EW profile) and average error ( AE ) (i.e., 1.5852 on the NS profile, 0.14885 on the EW profile) in the low-resolution dataset. The lowest RMSE (i.e., 0.16549 on the left profile, 0.29926 on the right profile) and mean absolute error ( MAE ) (i.e., 0.13467 on the left profile, 0.23933 on the right profile) results are obtained in the high-resolution dataset. Overall, the developed model is superior to the state-of-the-art models in at least four of the six performance metrics and reduces RMSE by 40.8\%, 17.2\%, 13.1\%, and 6.8\% on average on the four testing terrain profiles, respectively. The developed approach can be used as a decision tool for the accurate prediction of terrain profiles with different resolutions.},
  archive      = {J_ASOC},
  author       = {Wenping Pan and Limao Zhang and Chunlin Shen},
  doi          = {10.1016/j.asoc.2021.107179},
  journal      = {Applied Soft Computing},
  pages        = {107179},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven time series prediction based on multiplicative neuron model artificial neuron network},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximizing minority accuracy for imbalanced pattern
classification problems using cost-sensitive localized generalization
error model. <em>ASOC</em>, <em>104</em>, 107178. (<a
href="https://doi.org/10.1016/j.asoc.2021.107178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional machine learning methods may not yield satisfactory generalization capability when samples in different classes are imbalanced. These methods tend to sacrifice the accuracy of the minority class to improve the overall accuracy without regarding the fact that misclassifications of minority samples usually costs more in many real world applications . Therefore, we propose a neural network training method via a minimization of the cost-sensitive localized generalization error-based objective function (c-LGEM) to achieve a better balance of error yielded by the minority and the majority classes. The c-LGEM emphasizes the minimization of the generalization error of the minority class in a cost-sensitive manner. Experimental results obtained on 16 UCI datasets show that neural networks trained by the c-LGEM yield better performance in comparison to the performance yielded by some existing methods.},
  archive      = {J_ASOC},
  author       = {Wing W.Y. Ng and Zhengxi Liu and Jianjun Zhang and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2021.107178},
  journal      = {Applied Soft Computing},
  pages        = {107178},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Maximizing minority accuracy for imbalanced pattern classification problems using cost-sensitive localized generalization error model},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary local search algorithm for the biclustering of
gene expression data based on biological knowledge. <em>ASOC</em>,
<em>104</em>, 107177. (<a
href="https://doi.org/10.1016/j.asoc.2021.107177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering is an unsupervised classification technique that plays an increasingly important role in the study of modern biology. This data mining technique has provided answers to several challenges raised by the analysis of biological data and more particularly the analysis of gene expression data . It aims to cluster simultaneously genes and conditions. These unsupervised techniques are based essentially on the assumption that the extraction of the co-expressed genes allows to have co-regulated genes. In addition, the integration of biological information in the search process may induce to the extraction of relevant and non-trivial biclusters. Therefore, this work proposes an evolutionary algorithm based on local search method that relies on biological knowledge. An experimental study is achieved on real microarray datasets to evaluate the performance of the proposed algorithm. The assessment and the comparison are based on statistical and biological criteria. A cross-validation experiment is also used to estimate its accuracy. Promising results are obtained. They demonstrate the importance of the integration of the biological knowledge in the biclustering process to foster the efficiency and to promote the discovery of non-trivial and biologically relevant biclusters.},
  archive      = {J_ASOC},
  author       = {Ons Maâtouk and Wassim Ayadi and Hend Bouziri and Béatrice Duval},
  doi          = {10.1016/j.asoc.2021.107177},
  journal      = {Applied Soft Computing},
  pages        = {107177},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary local search algorithm for the biclustering of gene expression data based on biological knowledge},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time hostile activities analyses and detection
system. <em>ASOC</em>, <em>104</em>, 107175. (<a
href="https://doi.org/10.1016/j.asoc.2021.107175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over recent years, the development of online social media has dramatically changed the way people connect and share information. It is undeniable that social platform has promoted the quickest type of spread for fake stories. Almost all the current online fact-checking sources and researches are concentrating on the validating political content and context. The proposed system in this paper provides a complete visual data analytics methods to assist users in achieving a comprehensive understanding of malicious activities at multiple levels such as adversary’s behavior, victim’s behavior, content, and context level. In this paper, we investigate a variety of datasets from different aspects such as role, vulnerabilities, influential level, and distribution pattern. The proposed method in this paper focuses on automatic fake/hostile activity detection by utilizing a variety of machine learning (ML) techniques, deep learning models, natural language processes (NLP), and social network analysis (SNA) techniques. Different auxiliary models, such as bot detection, user credibility, and text readability , are deployed to generate additional influential features. The classification performance of ten different machine learning algorithms using a variety of well-known datasets is evaluated by utilizing 10-fold cross-validation.},
  archive      = {J_ASOC},
  author       = {Sajjad Dadkhah and Farzaneh Shoeleh and Mohammad Mehdi Yadollahi and Xichen Zhang and Ali A. Ghorbani},
  doi          = {10.1016/j.asoc.2021.107175},
  journal      = {Applied Soft Computing},
  pages        = {107175},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A real-time hostile activities analyses and detection system},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A solution-driven multilevel approach for graph coloring.
<em>ASOC</em>, <em>104</em>, 107174. (<a
href="https://doi.org/10.1016/j.asoc.2021.107174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph coloring is one of the most studied NP-hard problems with a wide range of applications. In this work, the first solution-driven multilevel algorithm for this computationally challenging problem is investigated. Following the general idea of multilevel optimization, the proposed algorithm combines an original solution-driven coarsening procedure with an uncoarsening procedure as well as an effective refinement procedure. The algorithm is assessed on 47 popular DIMACS and COLOR benchmark graphs, and compared with 13 state-of-the-art coloring methods in the literature. We close one large graph (wap01a.col) by providing its chromatic number for the first time. Impacts of the key ingredients of the algorithm are also investigated.},
  archive      = {J_ASOC},
  author       = {Wen Sun and Jin-Kao Hao and Yuhao Zang and Xiangjing Lai},
  doi          = {10.1016/j.asoc.2021.107174},
  journal      = {Applied Soft Computing},
  pages        = {107174},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A solution-driven multilevel approach for graph coloring},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering algorithm based on nature-inspired approach for
energy optimization in heterogeneous wireless sensor network.
<em>ASOC</em>, <em>104</em>, 107171. (<a
href="https://doi.org/10.1016/j.asoc.2021.107171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a clustering model for energy optimization based on the nature-inspired behaviour of animals. This clustering model finds the optimal distance to send data packets from one location to another, either long or short distances, so as to maintain the lifetime of the sensor network. The challenge with sensor networks is how to balance the energy load, which can be achieved by selecting a sensor node with an adequate amount of energy from a cluster to compensate for those sensor nodes with limited amount of energy. Generally, the clustering technique is one of the approaches to solve this challenge because it optimizes energy to increase the lifetime of the sensor network. We focus on nodes with different energy makeup, and based on the number of nodes that send packets, and evaluated the network performance in terms of the stability period, network lifetime and network throughput. Two nature-inspired algorithms (that is, kestrel-based search algorithm and wolf search algorithm with minus step previous) were compared to evaluate which one is energy-efficient when used as a clustering algorithm. It was found that, the Kestrel-based Search Algorithm Distributed Energy Efficient Clustering (KSA-DEEC) model has the optimal network run time (in seconds) to send a higher number of packets to base station successfully. Consequently, The KSA-DEEC model has an optimal network lifetime performance as compared to the Wolf Search Algorithm with Minus Step Previous (WSAMP)-DEEC model. It also has the highest network throughput in the simulation that was performed while the WSAMP-DEEC model showed prospects of better performance in some of the cases.},
  archive      = {J_ASOC},
  author       = {Israel Edem Agbehadji and Richard C. Millham and Abdultaofeek Abayomi and Jason J. Jung and Simon James Fong and Samuel Ofori Frimpong},
  doi          = {10.1016/j.asoc.2021.107171},
  journal      = {Applied Soft Computing},
  pages        = {107171},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering algorithm based on nature-inspired approach for energy optimization in heterogeneous wireless sensor network},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Influence-aware graph neural networks. <em>ASOC</em>,
<em>104</em>, 107169. (<a
href="https://doi.org/10.1016/j.asoc.2021.107169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representation learning endeavors to learn low-dimensional dense representations for nodes in a network. With the rapid development of online social platforms, the analysis of social networks has become increasingly significant. Although network representation learning can facilitate the social network analysis , most existing algorithms merely exploit the explicit structure among nodes to obtain the node representations. Besides, traditional network representation learning techniques ignore the influence of nodes in a network when generating the representations of nodes. Motivated by this, we innovatively propose an influence-aware graph neural network (IAGNN) framework, which can learn the latent feature representations of nodes by incorporating both node influence and global structure information into the embedding process for encoding graph-structured data. The generated low-dimensional dense representations of the nodes in a network can be used for subsequent tasks such as user classification and user behavior prediction. Specifically, we assign different weights to each node according to different types of topology between their neighbors, and integrate with the basic influence of each node to generate an intermediate matrix with influence information. The intermediate matrix is encoded into low-dimensional and dense vector spaces by leveraging the attention mechanism and the graph convolution operation . Extensive experiments are conducted on five datasets, and IAGNN achieves an average accuracy of 3\% higher than the comparison algorithms on the node classification and link prediction tasks. The experimental results demonstrate that our model can significantly outperform the state-of-the-art network embedding methods such as GCN, GAT, GraphSage, AGNN on node classification and link prediction tasks.},
  archive      = {J_ASOC},
  author       = {Bin Yu and Yu Zhang and Yu Xie and Chen Zhang and Ke Pan},
  doi          = {10.1016/j.asoc.2021.107169},
  journal      = {Applied Soft Computing},
  pages        = {107169},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Influence-aware graph neural networks},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense conjugate initialization for deterministic PSO in
applications: ORTHOinit+. <em>ASOC</em>, <em>104</em>, 107121. (<a
href="https://doi.org/10.1016/j.asoc.2021.107121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a class of novel initializations in Deterministic Particle Swarm Optimization (DPSO) for approximately solving costly unconstrained global optimization problems . The initializations are based on choosing specific dense initial positions and velocities for particles. These choices tend to induce in some sense orthogonality of particles’ trajectories, in the early iterations, in order to better explore the search space. Our proposal is inspired by both a theoretical analysis on a reformulation of PSO iteration, and by possible limits of the proposals reported in Campana et al. (2010); Campana et al. (2013). We explicitly show that, in comparison with other initializations from the literature, our initializations tend to scatter PSO particles, at least in the first iterations. The latter goal is obtained by imposing that the initial choice of particles’ position/velocity satisfies specific conjugacy conditions, with respect to a matrix depending on the parameters of PSO. In particular, by an appropriate condition on particles’ velocities, our initializations also resemble and partially extend a general paradigm in the literature of exact methods for derivative-free optimization. Moreover, we propose dense initializations for DPSO, so that the final approximate global solution obtained is possibly not too sparse, which might cause troubles in some applications. Numerical results, on both Portfolio Selection and Computational Fluid Dynamics problems, validate our theory and prove the effectiveness of our proposal, which applies also in case different neighborhood topologies are adopted in DPSO.},
  archive      = {J_ASOC},
  author       = {Cecilia Leotardi and Andrea Serani and Matteo Diez and Emilio F. Campana and Giovanni Fasano and Riccardo Gusso},
  doi          = {10.1016/j.asoc.2021.107121},
  journal      = {Applied Soft Computing},
  pages        = {107121},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dense conjugate initialization for deterministic PSO in applications: ORTHOinit+},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A probabilistic multimodal optimization algorithm based on
buffon principle and nyquist sampling theorem for noisy environment.
<em>ASOC</em>, <em>104</em>, 107068. (<a
href="https://doi.org/10.1016/j.asoc.2020.107068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel method named probabilistic multimodal optimization (PMO) algorithm is proposed in this paper to competently optimize noisy objective function. In the PMO algorithm, we propose two new strategies to make the algorithm have the capability of probability prediction and multiple extreme points optimization. The first strategy is concerned with the partition strategy of search space based on Buffon principle, which provides the probability prediction of peak detection ratio according to the Buffon distance and extrema resolution. At the same time, several local scopes where multiple extreme points are located can be retained by the partition strategy. The second strategy deals with a same-peak detection method based on Nyquist sampling theorem to identify the position of the extreme point. Based on 12 benchmark functions , experiments are carried out from three aspects, including probabilistic property verification, analysis of the influence of sampling frequency on the same-peak detection method, and multiple extreme points optimization. The validity of PMO algorithm theory and the probabilistic characteristic of global optimization under noise interference are proved, and it is denoted that PMO algorithm can locate more optima and gain higher location precision.},
  archive      = {J_ASOC},
  author       = {Xia Wang and Yaomin Wang and Xinling Shi and Lian Gao and Peng Li},
  doi          = {10.1016/j.asoc.2020.107068},
  journal      = {Applied Soft Computing},
  pages        = {107068},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A probabilistic multimodal optimization algorithm based on buffon principle and nyquist sampling theorem for noisy environment},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating gaussian process surrogate modeling using
compositional kernel learning and multi-stage sampling framework.
<em>ASOC</em>, <em>104</em>, 106909. (<a
href="https://doi.org/10.1016/j.asoc.2020.106909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate modeling is becoming a popular tool to approximate computationally-expensive simulations for complex engineering problems. In practice, there are still difficulties in surrogate modeling as follows: (1) efficient learning for functional relationship of simulation models and (2) diagnostics for the surrogate model . In order to address these difficulties simultaneously, this paper proposes a new sequential surrogate modeling by integrating a Compositional Kernel Learning (CKL) method for Gaussian process into a sequential sampling strategy termed the Progressive Latin Hypercube Sampling (PLHS). The CKL enables efficient learning capability for complex response surfaces based on richly structured kernels, while the PLHS sequentially generates nested samples by maintaining desired properties for distribution. Furthermore, this sequential sampling framework allows users to monitor the diagnostics of the surrogate model and assess the stopping criteria for further sampling. In order to demonstrate useful features of the proposed method, nine test functions were assembled for numerical experiments to cover different types of problems (i.e., scale and complexity). The proposed method was evaluated with a set of surrogate modeling techniques and sampling methods in terms of performance, diagnostics and computational cost. The results show that (1) the proposed method can learn various response surfaces with fewer training samples than other methods; and (2) the proposed method only provides a reliable diagnostic measure for global accuracy over different types of problems.},
  archive      = {J_ASOC},
  author       = {Seung-Seop Jin},
  doi          = {10.1016/j.asoc.2020.106909},
  journal      = {Applied Soft Computing},
  pages        = {106909},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Accelerating gaussian process surrogate modeling using compositional kernel learning and multi-stage sampling framework},
  volume       = {104},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Binary team game algorithm based on modulo operation for
knapsack problem with a single continuous variable. <em>ASOC</em>,
<em>103</em>, 107180. (<a
href="https://doi.org/10.1016/j.asoc.2021.107180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For solving the knapsack problem with a single continuous variable (KPC), a binary team game algorithm (TGA) with one-way mutation strategy is proposed. Firstly, without changing the evolution mode of TGA, three basic operations are reconstructed based on modulo 2 operation. Then, a binary TGA (BTGA) suitable for solving binary optimization problem is proposed. In order to use BTGA to solve KPC problem effectively, a one-way mutation strategy to improve individual quality is subsequently developed. Finally, based on BTGA and the existing repair and optimization algorithm of eliminating infeasible solutions, a novel algorithm MOBTGA for solving KPC problem is proposed. For validating the performance of MOBTGA, Kruskal–Wallis test is used to determine the reasonable values of its parameters. The comparison of experimental results obtained by different algorithms for two sets of KPC instances is subsequently executed. The comparison results show that MOBTGA has better performance than the existing heuristic algorithms ETDE, S-HBDE and B-HBDE in terms of solution accuracy and stability for solving KPC. The same time, the solving speed of MOBTGA also has a certain degree of competitiveness. Thus, MOBTGA is a quick and efficient heuristic algorithm for solving large-scale KPC instances.},
  archive      = {J_ASOC},
  author       = {Yichao He and Xiang Hao and Wenbin Li and Qinglei Zhai},
  doi          = {10.1016/j.asoc.2021.107180},
  journal      = {Applied Soft Computing},
  pages        = {107180},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Binary team game algorithm based on modulo operation for knapsack problem with a single continuous variable},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven quantification of public–private partnership
experience levels under uncertainty with bayesian hierarchical model.
<em>ASOC</em>, <em>103</em>, 107176. (<a
href="https://doi.org/10.1016/j.asoc.2021.107176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public–private partnership (PPP) is increasingly encouraged to deliver public services in developing countries. Many studies have been conducted to identify factors that affect PPP contract failure. Although a country’s PPP experience is of great importance in controlling the contract failure rate, most of the current studies are based on a qualitative perspective. This research develops a data-driven approach to quantify countries’ PPP experience levels through the Bayesian hierarchical model with uncertainties considered. First, detailed data exploration and selection have been carried out to clean the data source. Second, the number of change points in the dataset is identified based on the binary segmentation method . Third, the Bayesian hierarchical model is developed to locate the positions of the change points, and different experience levels are divided based on the location of change points. Findings show that: (i) PPP experience level is widely varying depending on PPP sectors. Four experience levels are suggested for the energy sector, while five levels are found for the transportation sector, and water &amp; sewerage sector, (ii) PPP experience level is dispersed around the world, for example, Latin America and Caribbean (LAC) and East Asia and Pacific (EAP) regions have higher PPP experience levels than other regions, (iii) a country may have various experience levels in different sectors, such as India, and (iv) the learning rate will decreases as more PPP projects are initiated. This research can contribute to (a) a novel approach that could detect the change points in PPP project experience, and (b) support investors in the decision making process, such as selecting the most appropriate investment direction, contributing to the development of PPP projects in developing countries.},
  archive      = {J_ASOC},
  author       = {Yongqi Wang and Zengqi Xiao and Robert L.K. Tiong and Limao Zhang},
  doi          = {10.1016/j.asoc.2021.107176},
  journal      = {Applied Soft Computing},
  pages        = {107176},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven quantification of public–private partnership experience levels under uncertainty with bayesian hierarchical model},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A genetic programming-based feature selection and fusion for
facial expression recognition. <em>ASOC</em>, <em>103</em>, 107173. (<a
href="https://doi.org/10.1016/j.asoc.2021.107173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition has become one of the most active research areas in pattern recognition due to the emergence of human–machine interaction systems. Describing facial expression is a very challenging problem since it relies on the quality of the face representation. A multitude of features have been proposed in the literature to describe facial expression. None of these features is universal for accurately capturing all the emotions since facial expressions vary according to the person, gender and type of emotion (posed or spontaneous). Therefore, some research works have considered combining several features to enhance the recognition rate. But they faced significant problems because of information redundancy and high dimensionality of the resulting features. In this work, we propose a genetic programming framework for feature selection and fusion for facial expression recognition, which we called G P − F E R GP−FER . The main component of this framework is a tree-based genetic program with a three functional layers (feature selection, feature fusion and classification). The proposed genetic program is a binary classifier that performs discriminative feature selection and fusion differently for each pair of expression classes. The final emotion is captured by performing a unique tournament elimination between all the classes using the binary programs. Three different geometric and texture features were fused using the proposed G P − F E R GP−FER . The obtained results, on four posed and spontaneous facial expression datasets ( DISFA , DISFA+ , CK+ and MUG ), show that the proposed facial expression recognition method has outperformed, or achieved a comparable performance to the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Haythem Ghazouani},
  doi          = {10.1016/j.asoc.2021.107173},
  journal      = {Applied Soft Computing},
  pages        = {107173},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic programming-based feature selection and fusion for facial expression recognition},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transductive transfer learning based genetic programming for
balanced and unbalanced document classification using different types of
features. <em>ASOC</em>, <em>103</em>, 107172. (<a
href="https://doi.org/10.1016/j.asoc.2021.107172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document classification is one of the predominant tasks in Natural Language Processing . However, some document classification tasks do not have ground truth while other similar datasets may have ground truth. Transfer learning can utilize similar datasets with ground truth to train effective classifiers on the dataset without ground truth. This paper introduces a transductive transfer learning method for document classification using two different text feature representations—the term frequency (TF) and the semantic feature doc2vec. It has three main contributions. First, it enables the sharing knowledge in a dataset using TF and a dataset using doc2vec in transductive transfer learning for performance improvement. Second, it demonstrates that the partially learned programs from TFs and from doc2vecs can be alternatively used to “label then learn” and they improve each other. Lastly, it addresses the unbalanced dataset problem by considering the unbalanced distributions on categories for evolving proper Genetic Programming (GP) programs on the target domains. Our experimental results on two popular document datasets show that the proposed technique effectively transfers knowledge from the GP programs evolved from the source domains to the new GP programs on the target domains using TF or doc2vec. There are obviously more than 10 percentages improvement achieved by the GP programs evolved by the proposed method over the GP programs directly evolved from the source domains. Also, the proposed technique effectively utilizes GP programs evolved from unbalanced datasets (on the source and target domains) to evolve new GP programs on the target domains, which balances predictions on different categories.},
  archive      = {J_ASOC},
  author       = {Wenlong Fu and Bing Xue and Xiaoying Gao and Mengjie Zhang},
  doi          = {10.1016/j.asoc.2021.107172},
  journal      = {Applied Soft Computing},
  pages        = {107172},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transductive transfer learning based genetic programming for balanced and unbalanced document classification using different types of features},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistency and trust relationship-driven social network
group decision-making method with probabilistic linguistic information.
<em>ASOC</em>, <em>103</em>, 107170. (<a
href="https://doi.org/10.1016/j.asoc.2021.107170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike other linguistic modelings, probabilistic linguistic terms can clearly describe the importance of different linguistic terms . With respect to group decision-making (GDM) problems, it is convenient for experts to express their evaluation opinions with probabilistic linguistic preference relations (PLPRs), which can transform experts’ quantitative descriptions into qualitative probabilistic linguistic terms. The processes of consistency-adjustment and expert weights determination play a key role in GDM. Therefore, this paper aims at the design of a novel probabilistic linguistic GDM method with consistency-adjustment algorithm and trust relationship-driven expert weight determination model. First, we redefine the multiplicative consistency of PLPRs, which only involves changing the probabilities of linguistic terms. A new distance between PLPRs is presented to calculate the consistency index. Then, we propose a convergent consistency-adjustment algorithm to improve the consistency of a PLPR to an acceptable consistency level . Subsequently, a trust relationship-driven expert weight determination model is developed to derive the experts’ weights in a social network environment. Finally, a probabilistic linguistic GDM method is designed to determine the reliable ranking of alternatives. The advantages and applicability of the proposed method are illustrated by a case study concerning an evaluation of logistics service suppliers and associated comparative analyses.},
  archive      = {J_ASOC},
  author       = {Feifei Jin and Meng Cao and Jinpei Liu and Luis Martínez and Huayou Chen},
  doi          = {10.1016/j.asoc.2021.107170},
  journal      = {Applied Soft Computing},
  pages        = {107170},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consistency and trust relationship-driven social network group decision-making method with probabilistic linguistic information},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A weighting model based on best–worst method and its
application for environmental performance evaluation. <em>ASOC</em>,
<em>103</em>, 107168. (<a
href="https://doi.org/10.1016/j.asoc.2021.107168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analytic hierarchy process (AHP) is widely used as a multi-criteria decision-making method in practical applications. Several researchers have expanded the AHP method to D numbers AHP (D-AHP) to apply AHP to an uncertain decision-making environment. D numbers is an extension of the Dempster–Shafer (D–S) theory, which overcomes the shortcomings of the D–S theory and can effectively express uncertain information. With the deepening of research on the AHP method, the best–worst method (BWM) was proposed as an improvement to the AHP method. The BWM can lower the inconsistency in results and reduce the number of required pairwise comparisons . Although some researchers have extended the BWM method to an uncertain environment and proposed fuzzy BWM methods, these methods cannot handle some special situations, such as when the subjective evaluations of experts are conflicting or altogether missing. To apply the BWM method to these special situations, this study suggests combining the BWM with D numbers and proposes D numbers BWM (D-BWM) weighting model. First of all, we discuss D numbers extended fuzzy preference relations (DNFPRs). Afterwards, we design an algorithm to select the best and worst criteria based on the DNFPRs by calculating the out-degrees and in-degrees. Furthermore, we develop a linear programming model to derive the weights of criteria, and then propose a consistency ratio to check the reliability of the derived results. The experimental results show that the D-BWM method is more suitable for realistic decision-making because of its simplicity and sensitivity to subjective information. Finally, the proposed method is applied to evaluate the environmental performances of 30 provincial administrative regions of China.},
  archive      = {J_ASOC},
  author       = {Peide Liu and Baoying Zhu and Peng Wang},
  doi          = {10.1016/j.asoc.2021.107168},
  journal      = {Applied Soft Computing},
  pages        = {107168},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A weighting model based on best–worst method and its application for environmental performance evaluation},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modulo 9 model-based learning for missing data imputation.
<em>ASOC</em>, <em>103</em>, 107167. (<a
href="https://doi.org/10.1016/j.asoc.2021.107167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing Values Management is one of the challenges faced by Data Analysts. Therefore, the creation of effective data models will be the right decision for missing data imputation. However, learning, training, and Data Analysis must be implemented through machine learning algorithms . Missing Data is a problem with no feedback or variables. This problem (missing data) can result in serious Data Analysis, which may eventually lead to erroneous conclusions. This research paper first studies how missing data can affect Machine Learning Algorithms, and decision-making based on the Data Analysis’s output. Secondly, it proposes Modulo 9 as a novel method for handling missing data problems. The proposed novel method is assessed with wide-ranging experiments compared with robust Machine Learning techniques such as Support Vector Machine (SVM) Algorithm, Linear Regression (LR), K-Nearest Neighbors (KNN), Naïve Bayes (NB), Support Vector Classifier (SVC), Linear Support Vector Classifier (LSVC), Random Forest Classifier (RFC), Decision Tree Regressor (DTR), Deletion Method, Multi-Layer Perceptron (MLP), and the Mean Value. The results show that the novel method outperforms the eleven (11) existing methods.},
  archive      = {J_ASOC},
  author       = {Alladoumbaye Ngueilbaye and Hongzhi Wang and Daouda Ahmat Mahamat and Sahalu B. Junaidu},
  doi          = {10.1016/j.asoc.2021.107167},
  journal      = {Applied Soft Computing},
  pages        = {107167},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modulo 9 model-based learning for missing data imputation},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dynamic ensemble learning algorithm based on k-means for
ICU mortality prediction. <em>ASOC</em>, <em>103</em>, 107166. (<a
href="https://doi.org/10.1016/j.asoc.2021.107166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research proposes a Dynamic Ensemble Learning Algorithm based on K-means (DELAK) for intensive care unit (ICU) mortality prediction. Nowadays, the widely applied traditional scoring systems, which predict the mortality risk with some scores reflecting the severity of disease and physiological states of patients in ICU, have shown insufficient predictive performance when faced with large volume of data. Although taking advantage of large volume of data, single machine learning model and ensemble learning methods show an inadequate ability to make personalized predictions for each new patient. Dynamic ensemble selection (DES) methods which are widely studied in pattern recognition field can make personalized prediction but they only select a single classifier instead of ensemble of the results from a pool of classifiers. To overcome the limitations mentioned above, this research proposes an ensemble learning algorithm based on K-means sampling and distance-based dynamic ensemble. K-means sampling helps to achieve the diversity of base classifiers and the distance-based dynamic ensemble is a flexible fusion method which creates a personalized combination of results from base classifiers for each new test sample. To evaluate the performance for mortality prediction of the proposed algorithm, comprehensive experiments are conducted on MIMIC-III dataset. The experimental results show that DELAK achieves outstanding performance in term of AUROC and AUPRC compared with other six fusion strategies, traditional scoring systems, classical ensemble models, i.e. AdaBoost , Bagging and random forest , and dynamic ensemble selection methods in most mortality prediction tasks.},
  archive      = {J_ASOC},
  author       = {Chonghui Guo and Mucan Liu and Menglin Lu},
  doi          = {10.1016/j.asoc.2021.107166},
  journal      = {Applied Soft Computing},
  pages        = {107166},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic ensemble learning algorithm based on K-means for ICU mortality prediction},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of air-flow parameters and turbulent intensity in
hydraulic jump on rough bed using bayesian model averaging.
<em>ASOC</em>, <em>103</em>, 107165. (<a
href="https://doi.org/10.1016/j.asoc.2021.107165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hydraulic jump is an abrupt transition between subcritical and supercritical flows which is associated with energy dissipation , air entrainment , spray, splashing, and surface waves. Both physical and numerical modeling were largely applied to study hydrodynamics , turbulence and air-entrainment in the hydraulic jump, while the literature about the application of classifier models is quite limited. Determining air-flow parameters and turbulent intensity has been merely performed by costly and time-consuming experimental methods, while this study is the first attempt to estimate the mentioned parameters using a computer-based methodology with desired precision. In the present study, air-flow parameters including void fraction ( C ) and bubble count rate ( F ), as well as turbulent intensity ( Tu ) on rough bed were estimated using Bayesian model averaging (BMA) and three multilayer perceptron (MLP), support vector regression (SVR) and generalized regression neural network (GRNN) as classifier models . To develop the stated models, the experimental data from Felder and Chanson (2016) were divided into four classes based on longitudinal distance from the jump toe. Results highlighted that the MLP and GRNN models have more accurate results compared to the SVR model. For F and Tu , the GRNN model and for C , the MLP model showed better performance than other models in four classes. The average acceptance rate between 15 and 30\% of the BMA model performance for all classes proved the accuracy and efficiency of the proposed methodology. The average RMSE value of BMA results and the bests classifier models were 0.41 and 0.42, respectively, for the estimation of all three parameters. Results revealed that the BMA model by weighting individual classifier models could be able to estimate parameters with better accuracy than the best classifier model in each class. The significant outcome of this study is that the proposed model is able to render accurate results in a complex system such as hydraulic jump.},
  archive      = {J_ASOC},
  author       = {Narges Taravatrooy and Farhad Bahmanpouri and Mohammad Reza Nikoo and Carlo Gualtieri and Azizallah Izady},
  doi          = {10.1016/j.asoc.2021.107165},
  journal      = {Applied Soft Computing},
  pages        = {107165},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Estimation of air-flow parameters and turbulent intensity in hydraulic jump on rough bed using bayesian model averaging},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural network features fusion and selection based on
PLS regression with an application for crops diseases classification.
<em>ASOC</em>, <em>103</em>, 107164. (<a
href="https://doi.org/10.1016/j.asoc.2021.107164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The plants diseases affect both the production and quality of food in the agriculture sector. Computer vision techniques can contribute significantly by detecting the plant’s diseases at very early stages with more accuracy. In this work, we proposed an automated crop disease recognition system using partial least squares (PLS) regression for feature selection from an extracted deep feature set. The presented framework incorporates three primary phases : First, the deep features are extracted using a pre-trained Visual Geometry Group (VGG19) convolutional neural networks (CNN) model; Second, a PLS-based parallel fusion method combines the features extracted from the fully connected layers 6 and 7; Third, the best features are selected using a PLS projection method. The most discriminant features are finally plugged into the ensemble baggage tree classifier for final recognition. Three different crops (tomato, corn and potato) are selected from the Plant Village dataset for the algorithm’s evaluation. The average accuracy achieved using the proposed method is approximately 90.1\%. The proposed PLS based fusion and selection methods not only improve recognition accuracy but also reduce computational time. Further, based on the achieved results, we are confident that the proposed plan will work even under light variations and texture constraints.},
  archive      = {J_ASOC},
  author       = {Farah Saeed and Muhammad Attique Khan and Muhammad Sharif and Mamta Mittal and Lalit Mohan Goyal and Sudipta Roy},
  doi          = {10.1016/j.asoc.2021.107164},
  journal      = {Applied Soft Computing},
  pages        = {107164},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep neural network features fusion and selection based on PLS regression with an application for crops diseases classification},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A matheuristic algorithm for the vehicle routing problem
with cross-docking. <em>ASOC</em>, <em>103</em>, 107163. (<a
href="https://doi.org/10.1016/j.asoc.2021.107163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the integration of the vehicle routing problem with cross-docking (VRPCD). The aim is to find a set of routes to deliver products from a set of suppliers to a set of customers through a cross-dock facility, such that the operational and transportation costs are minimized, without violating the vehicle capacity and time horizon constraints. A two-phase matheuristic based on column generation is proposed. The first phase focuses on generating a set of feasible candidate routes in both pickup and delivery processes by implementing an adaptive large neighborhood search algorithm. A set of destroy and repair operators are used in order to explore a large neighborhood space. The second phase focuses on solving the set partitioning model to determine the final solution. The proposed matheuristic is tested on the available benchmark VRPCD instances and compared with the state-of-the-art algorithms. Experimental results show the competitiveness of the proposed matheuristic as it is able to improve the best known solutions for 80 instances and to obtain the same results for the remaining 10 instances, with an average improvement of 12.6\%. On new and larger instances, our proposed matheuristic maintains its solution quality within acceptable CPU times and outperforms a pure ALNS algorithm. We also explicitly analyze the performance of the matheuristic considering the solution quality and CPU time.},
  archive      = {J_ASOC},
  author       = {Aldy Gunawan and Audrey Tedja Widjaja and Pieter Vansteenwegen and Vincent F. Yu},
  doi          = {10.1016/j.asoc.2021.107163},
  journal      = {Applied Soft Computing},
  pages        = {107163},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A matheuristic algorithm for the vehicle routing problem with cross-docking},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile app recommendation via heterogeneous graph neural
network in edge computing. <em>ASOC</em>, <em>103</em>, 107162. (<a
href="https://doi.org/10.1016/j.asoc.2021.107162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new computing technology proposed with the development of 5G , IoT technologies and increasing requirement of mobile applications and services, edge computing enables mobile application developers and content providers to serve context-aware mobile services ( e.g., mobile app recommendation). Mobile app recommendation is known as an effective solution to overcome the information overload in mobile app markets. Most existing models only consider user-app interaction and feature modeling, and neglect the structural information which actually is a crucial part in the scenario of app recommendation. To fully exploit both structural and feature information for app recommendation, this paper proposes a novel heterogeneous graph neural network framework (HGNRec) including one inner module and one outer module. Specifically, the inner module is able to use a node-level attention to learn the importance between a node and its meta-path based neighbors. The outer module with a path-level attention can learn the importance of different meta-paths. With the learned importance from two modules, the comprehensive embeddings for user and app nodes can be generated by integrating features from meta-path based neighbors. Extensive experiments on the real-world Google Play mobile app dataset demonstrate the effectiveness of HGNRec.},
  archive      = {J_ASOC},
  author       = {Tingting Liang and Xuan Sheng and Li Zhou and Youhuizi Li and Honghao Gao and Yuyu Yin and Liang Chen},
  doi          = {10.1016/j.asoc.2021.107162},
  journal      = {Applied Soft Computing},
  pages        = {107162},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mobile app recommendation via heterogeneous graph neural network in edge computing},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting the dynamics of cumulative COVID-19 cases
(confirmed, recovered and deaths) for top-16 countries using statistical
machine learning models: Auto-regressive integrated moving average
(ARIMA) and seasonal auto-regressive integrated moving average (SARIMA).
<em>ASOC</em>, <em>103</em>, 107161. (<a
href="https://doi.org/10.1016/j.asoc.2021.107161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most countries are reopening or considering lifting the stringent prevention policies such as lockdowns, consequently, daily coronavirus disease (COVID-19) cases (confirmed, recovered and deaths) are increasing significantly. As of July 25th, there are 16.5 million global cumulative confirmed cases, 9.4 million cumulative recovered cases and 0.65 million deaths. There is a tremendous necessity of supervising and estimating future COVID-19 cases to control the spread and help countries prepare their healthcare systems. In this study, time-series models — Auto-Regressive Integrated Moving Average (ARIMA) and Seasonal Auto-Regressive Integrated Moving Average (SARIMA) are used to forecast the epidemiological trends of the COVID-19 pandemic for top-16 countries where 70\%–80\% of global cumulative cases are located. Initial combinations of the model parameters were selected using the auto-ARIMA model followed by finding the optimized model parameters based on the best fit between the predictions and test data. Analytical tools Auto-Correlation function (ACF), Partial Auto-Correlation Function (PACF), Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) were used to assess the reliability of the models. Evaluation metrics Mean Absolute Error (MAE), Mean Square Error (MSE), Root Mean Square Error (RMSE) and Mean Absolute Percent Error (MAPE) were used as criteria for selecting the best model. A case study was presented where the statistical methodology was discussed in detail for model selection and the procedure for forecasting the COVID-19 cases of the USA. Best model parameters of ARIMA and SARIMA for each country are selected manually and the optimized parameters are then used to forecast the COVID-19 cases. Forecasted trends for confirmed and recovered cases showed an exponential rise for countries such as the United States, Brazil , South Africa, Colombia, Bangladesh, India, Mexico and Pakistan. Similarly, trends for cumulative deaths showed an exponential rise for countries Brazil, South Africa, Chile, Colombia, Bangladesh, India, Mexico, Iran, Peru, and Russia. SARIMA model predictions are more realistic than that of the ARIMA model predictions confirming the existence of seasonality in COVID-19 data. The results of this study not only shed light on the future trends of the COVID-19 outbreak in top-16 countries but also guide these countries to prepare their health care policies for the ongoing pandemic. The data used in this work is obtained from publicly available John Hopkins University’s COVID-19 database.},
  archive      = {J_ASOC},
  author       = {K.E. ArunKumar and Dinesh V. Kalaga and Ch. Mohan Sai Kumar and Govinda Chilkoor and Masahiro Kawaji and Timothy M. Brenza},
  doi          = {10.1016/j.asoc.2021.107161},
  journal      = {Applied Soft Computing},
  pages        = {107161},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting the dynamics of cumulative COVID-19 cases (confirmed, recovered and deaths) for top-16 countries using statistical machine learning models: Auto-regressive integrated moving average (ARIMA) and seasonal auto-regressive integrated moving average (SARIMA)},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepCoroNet: A deep LSTM approach for automated detection of
COVID-19 cases from chest x-ray images. <em>ASOC</em>, <em>103</em>,
107160. (<a href="https://doi.org/10.1016/j.asoc.2021.107160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new coronavirus, known as COVID-19, first emerged in Wuhan, China, and since then has been transmitted to the whole world. Around 34 million people have been infected with COVID-19 virus so far, and nearly 1 million have died as a result of the virus. Resource shortages such as test kits and ventilator have arisen in many countries as the number of cases have increased beyond the control. Therefore, it has become very important to develop deep learning-based applications that automatically detect COVID-19 cases using chest X-ray images to assist specialists and radiologists in diagnosis. In this study, we propose a new approach based on deep LSTM model to automatically identify COVID-19 cases from X-ray images. Contrary to the transfer learning and deep feature extraction approaches, the deep LSTM model is an architecture, which is learned from scratch. Besides, the Sobel gradient and marker-controlled watershed segmentation operations are applied to raw images for increasing the performance of proposed model in the pre-processing stage. The experimental studies were performed on a combined public dataset constituted by gathering COVID-19, pneumonia and normal (healthy) chest X-ray images. The dataset was randomly separated into two sections as training and testing data. For training and testing, these separations were performed with the rates of 80\%–20\%, 70\%–30\% and 60\%–40\%, respectively. The best performance was achieved with 80\% training and 20\% testing rate. Moreover, the success rate was 100\% for all performance criteria, which composed of accuracy, sensitivity, specificity and F-score. Consequently, the proposed model with pre-processing images ensured promising results on a small dataset compared to big data. Generally, the proposed model can significantly improve the present radiology based approaches and it can be very useful application for radiologists and specialists to help them in detection, quantity determination and tracing of COVID-19 cases throughout the pandemic.},
  archive      = {J_ASOC},
  author       = {Fatih Demir},
  doi          = {10.1016/j.asoc.2021.107160},
  journal      = {Applied Soft Computing},
  pages        = {107160},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DeepCoroNet: A deep LSTM approach for automated detection of COVID-19 cases from chest X-ray images},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). User trustworthiness in online social networks: A systematic
review. <em>ASOC</em>, <em>103</em>, 107159. (<a
href="https://doi.org/10.1016/j.asoc.2021.107159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of social networks and their easy acceptance of new users have the unintended consequence of fostering an environment where anonymous users can act in malicious ways. Although these platforms have many incentives to prevent such occurrences, they have not been able to cope with the sheer volume of information that must be processed. Moreover, the tendency of attackers to rapidly change strategies in response to defensive measures also poses a challenge. Hence, research on issues related to user trustworthiness on social networks is gaining traction, with many interesting studies conducted in recent years. In this work, we aim to review the present state of this field and present an analysis of the studies published between 2012 and 2020 that attempt to address this problem using various methodologies. Some of the solutions discussed in the literature can be described as bot identification protocols, while others focus on anti-spam protection, recognition of fake news, or rating the truthfulness of user-generated content. Many of these solutions offer tangible benefits in various respects, however none of them are able to provide comprehensive all-around protection against all possible types of attacks. Monitoring this scientific field is thus a key task, and this review will hopefully lead to a better understanding of the concept of online user trustworthiness by highlighting recent works that deal with this issue.},
  archive      = {J_ASOC},
  author       = {Majed Alkhamees and Saleh Alsaleem and Muhammad Al-Qurishi and Majed Al-Rubaian and Amir Hussain},
  doi          = {10.1016/j.asoc.2021.107159},
  journal      = {Applied Soft Computing},
  pages        = {107159},
  shortjournal = {Appl. Soft. Comput.},
  title        = {User trustworthiness in online social networks: A systematic review},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimalistic fuzzy ontology reasoning: An application to
building information modeling. <em>ASOC</em>, <em>103</em>, 107158. (<a
href="https://doi.org/10.1016/j.asoc.2021.107158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a minimalistic reasoning algorithm to solve imprecise instance retrieval in fuzzy ontologies with application to querying Building Information Models (BIMs)—a knowledge representation formalism used in the construction industry. Our proposal is based on a novel lossless reduction of fuzzy to crisp reasoning tasks, which can be processed by any Description Logics reasoner . We implemented the minimalistic reasoning algorithm and performed an empirical evaluation of its performance in several tasks: interoperation with classical reasoners (Hermit and TrOWL), initialization time (comparing TrOWL and a SPARQL engine), and use of different data structures (hash tables, databases, and programming interfaces). We show that our software can efficiently solve very expressive queries not available nowadays in regular or semantic BIMs tools.},
  archive      = {J_ASOC},
  author       = {Ignacio Huitzil and Miguel Molina-Solana and Juan Gómez-Romero and Fernando Bobillo},
  doi          = {10.1016/j.asoc.2021.107158},
  journal      = {Applied Soft Computing},
  pages        = {107158},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Minimalistic fuzzy ontology reasoning: An application to building information modeling},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objectivization inspired metaheuristics for the
sum-of-the-parts combinatorial optimization problems. <em>ASOC</em>,
<em>103</em>, 107157. (<a
href="https://doi.org/10.1016/j.asoc.2021.107157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objectivization is a term used to describe strategies developed for optimizing single-objective problems by multi-objective algorithms. This paper focuses on multi-objectivizing the sum-of-the-parts combinatorial optimization problems , which include the traveling salesman problem , the unconstrained binary quadratic programming and other well-known combinatorial optimization problem. For a sum-of-the-parts combinatorial optimization problem, we propose to decompose its original objective into two sub-objectives with controllable correlation. Based on the decomposition method , two new multi-objectivization inspired single-objective optimization techniques called non-dominance search and non-dominance exploitation are developed, respectively. Non-dominance search is combined with two metaheuristics , namely iterated local search and iterated tabu search , while non-dominance exploitation is embedded within the iterated Lin–Kernighan metaheuristic. The resultant metaheuristics are called ILS+NDS , ITS+NDS and ILK+NDE , respectively. Empirical studies on some TSP and UBQP instances show that with appropriate correlation between the sub-objectives, there are more chances to escape from local optima when new starting solution is selected from the non-dominated solutions defined by the decomposed sub-objectives. Experimental results also show that ILS+NDS, ITS+NDS and ILK+NDE all significantly outperform their counterparts on most of the test instances.},
  archive      = {J_ASOC},
  author       = {Jialong Shi and Jianyong Sun and Qingfu Zhang},
  doi          = {10.1016/j.asoc.2021.107157},
  journal      = {Applied Soft Computing},
  pages        = {107157},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objectivization inspired metaheuristics for the sum-of-the-parts combinatorial optimization problems},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KDT-SPSO: A multimodal particle swarm optimisation algorithm
based on k-d trees for palm tree detection. <em>ASOC</em>, <em>103</em>,
107156. (<a href="https://doi.org/10.1016/j.asoc.2021.107156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palm tree detection, as any other problem of object localisation in an image, can be formulated mathematically as a multimodal optimisation problem that seeks for the best set of positions and scales that maximise the classification score. In recent decades, many bio-inspired multimodal optimisation algorithms using different niching techniques have been proven effective and robust in solving optimisation problems. In many niching-based algorithms, e.g., species-based particle swarm optimisation (SPSO), repetitive distance evaluations to search for nearest neighbours are unavoidable to find multiple optima in the search space, which are computationally expensive. In this paper, we propose an improved SPSO named KDT-SPSO to speed up the nearest neighbour search using a special tree-based structure, called k-d tree. KDT-SPSO reduces the computation complexity by only visiting the subtrees that most likely contain the neighbours. We statistically tested KDT-SPSO on a number of benchmark functions and the results showed that it was up to 48\% faster than the original SPSO. After proving its effectiveness, we optimised the parameters of KDT-SPSO for palm tree detection and introduced a restart mechanism to speed up its convergence and to improve the recall rate. The local binary pattern (LBP) feature extractor and modified support vector machine (SVM) classifier with radial basis function (RBF) kernel were used to compute the fitness value of palm trees. The optimised KDT-SPSO was able to achieve on average 91.80\% palm tree detection accuracy and 5 times faster than the traditional sliding window approach.},
  archive      = {J_ASOC},
  author       = {Zi Yan Chen and Iman Yi Liao and Amr Ahmed},
  doi          = {10.1016/j.asoc.2021.107156},
  journal      = {Applied Soft Computing},
  pages        = {107156},
  shortjournal = {Appl. Soft. Comput.},
  title        = {KDT-SPSO: A multimodal particle swarm optimisation algorithm based on k-d trees for palm tree detection},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An extended fuzzy decision-making framework using hesitant
fuzzy sets for the drug selection to treat the mild symptoms of
coronavirus disease 2019 (COVID-19). <em>ASOC</em>, <em>103</em>,
107155. (<a href="https://doi.org/10.1016/j.asoc.2021.107155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The whole world is presently under threat from Coronavirus Disease 2019 (COVID-19), a new disease spread by a virus of the corona family, called a novel coronavirus. To date, the cases due to this disease are increasing exponentially, but there is no vaccine of COVID-19 available commercially. However, several antiviral therapies are used to treat the mild symptoms of COVID-19 disease. Still, it is quite complicated and uncertain decision to choose the best antiviral therapy to treat the mild symptom of COVID-19. Hesitant Fuzzy Sets (HFSs) are proven effective and valuable structures to express uncertain information in real-world issues. Therefore, here we used the hesitant fuzzy decision-making (DM) method. This study has chosen five methods or medicines to treat the mild symptom of COVID-19. These alternatives have been ranked by seven criteria for choosing an optimal method. The purpose of this study is to develop an innovative Additive Ratio Assessment (ARAS) approach to elucidate the DM problems. Next, a divergence measure based procedure is developed to assess the relative importance of the criteria rationally. To do this, a novel divergence measure is introduced for HFSs. A case study of drug selection for COVID-19 disease is considered to demonstrate the practicability and efficacy of the developed idea in real-life applications. Afterward, the outcome shows that Remdesivir is the best medicine for patients with mild symptoms of the COVID-19. Sensitivity analysis is presented to ensure the permanence of the introduced framework. Moreover, a comprehensive comparison with existing models is discussed to show the advantages of the developed framework. Finally, the results prove that the introduced ARAS approach is more effective and reliable than the existing models.},
  archive      = {J_ASOC},
  author       = {Arunodaya Raj Mishra and Pratibha Rani and R. Krishankumar and K.S. Ravichandran and Samarjit Kar},
  doi          = {10.1016/j.asoc.2021.107155},
  journal      = {Applied Soft Computing},
  pages        = {107155},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An extended fuzzy decision-making framework using hesitant fuzzy sets for the drug selection to treat the mild symptoms of coronavirus disease 2019 (COVID-19)},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic optimization using grey wolf optimization with
optimal computing budget allocation. <em>ASOC</em>, <em>103</em>,
107154. (<a href="https://doi.org/10.1016/j.asoc.2021.107154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic optimization problems exist widely in many manufacturing and service systems. Due to the stochastic nature, these problems usually have no analytical solutions and are difficult to solve. This research proposes a hybrid approach that integrates the grey wolf optimization algorithm and the simulation optimization framework. In this hybrid approach, the grey wolf optimization algorithm is used to search for candidate solutions from the solution space, while the simulation helps the algorithm to identify the desired solutions such that the search is guided to more promising regions. To enhance the efficiency of simulation, this work designs a computing budget allocation rule that helps the grey wolf optimization algorithm to select the elite candidate solutions in each iteration. The proposed computing budget allocation rule is then integrated with the grey wolf optimization algorithm to solve stochastic optimization problems . Numerical experiments confirm that the proposed computing budget allocation rule performs better than extant allocation rules, and can find the better solution for stochastic optimization problems using fewer iterations by integration with the grey wolf optimization algorithm.},
  archive      = {J_ASOC},
  author       = {Yaping Fu and Hui Xiao and Loo Hay Lee and Min Huang},
  doi          = {10.1016/j.asoc.2021.107154},
  journal      = {Applied Soft Computing},
  pages        = {107154},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stochastic optimization using grey wolf optimization with optimal computing budget allocation},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent adaptive optimal control using incremental
model-based global dual heuristic programming subject to partial
observability. <em>ASOC</em>, <em>103</em>, 107153. (<a
href="https://doi.org/10.1016/j.asoc.2021.107153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scarcity of information regarding dynamics and full-state feedback increases the demand for a model-free control technique that can cope with partial observability . To deal with the absence of prior knowledge of system dynamics and perfect measurements, this paper develops a novel intelligent control scheme by combining global dual heuristic programming with an incremental model-based identifier. An augmented system consisting of the unknown nonlinear plant and unknown varying references is identified online using a locally linear regression technique . The actor–critic is implemented using artificial neural networks , and the actuator saturation constraint is addressed by exploiting a symmetrical sigmoid activation function in the output layer of the actor network. Numerical experiments are conducted by applying the proposed method to online adaptive optimal control tasks of an aerospace system. The results reveal that the developed method can deal with partial observability with performance comparable to the full-state feedback control , while outperforming the global model-based method in stability and adaptability.},
  archive      = {J_ASOC},
  author       = {Bo Sun and Erik-Jan van Kampen},
  doi          = {10.1016/j.asoc.2021.107153},
  journal      = {Applied Soft Computing},
  pages        = {107153},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent adaptive optimal control using incremental model-based global dual heuristic programming subject to partial observability},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective genetic programming for feature learning in
face recognition. <em>ASOC</em>, <em>103</em>, 107152. (<a
href="https://doi.org/10.1016/j.asoc.2021.107152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition is a challenging task due to high variations of pose, expression, ageing, and illumination. As an effective approach to face recognition, feature learning can be formulated as a multi-objective optimisation task of maximising classification accuracy and minimising the number of learned features. However, most of the existing algorithms focus on improving classification accuracy without considering the number of learned features. In this paper, we propose new multi-objective genetic programming (GP) algorithms for feature learning in face recognition. To achieve effective face feature learning, a new individual representation is developed to allow GP to select informative regions from the input image, extract features using various descriptors, and combine the extracted features for classification. Then two new multi-objective genetic programming (GP) algorithms, one with the idea of non-dominated sorting (NSGPFL) and the other with the idea of Strength Pareto (SPGPFL), are proposed to simultaneously optimise these two objectives. NSGPFL and SPGPFL are compared with a single-objective GP for feature learning (GPFL), a single-objective GP for weighting two objectives (GPFLW), and a large number of baseline methods . The experimental results show the effectiveness of the NSGPFL and SPGPFL algorithms by achieving better or comparable classification performance and learning a small number of features.},
  archive      = {J_ASOC},
  author       = {Ying Bi and Bing Xue and Mengjie Zhang},
  doi          = {10.1016/j.asoc.2021.107152},
  journal      = {Applied Soft Computing},
  pages        = {107152},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective genetic programming for feature learning in face recognition},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A GRASP-based algorithm for solving the emergency room
physician scheduling problem. <em>ASOC</em>, <em>103</em>, 107151. (<a
href="https://doi.org/10.1016/j.asoc.2021.107151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a physician scheduling problem in an Emergency Room (ER) requiring a long-term work calendar to allocate work days and types of shift among all the doctors. The mathematical model is created without simplifications, using the real calendar, including holidays. This precludes the possibility of cyclic-type solutions, and involves numerous and varied constraints (demand, workload, ergonomics , fairness, etc.). An effective solution to this very difficult practical problem cannot be obtained, for large instances, with exact solution methods. We formulate a mathematical representation of a real-world ER physician scheduling problem featuring a hybrid algorithm combining continuous linear programming with a greedy randomized adaptive search procedure (GRASP). Linear programming is used to model a general physician-demand covering problem, where the solution is used to guide the construction phase of the GRASP, to obtain initial full schedules for subsequent improvement by iterative application of Variable Neighborhood Descent Search (VNDS) and Network Flow Optimization (NFO). A computational study shows the superiority of our approach over the Integer Linear Programming method in a set of instances of varying size and difficulty inspired by a real setting. The methodology is embedded in a software tool for generating one-year-ahead physician schedules for a local ER. These solutions, which are now in use, outperform the manually-created schedules used previously.},
  archive      = {J_ASOC},
  author       = {M. Cildoz and F. Mallor and P.M. Mateo},
  doi          = {10.1016/j.asoc.2021.107151},
  journal      = {Applied Soft Computing},
  pages        = {107151},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A GRASP-based algorithm for solving the emergency room physician scheduling problem},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep transfer learning with limited data for machinery fault
diagnosis. <em>ASOC</em>, <em>103</em>, 107150. (<a
href="https://doi.org/10.1016/j.asoc.2021.107150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigation of deep transfer learning on machinery fault diagnosis is helpful to overcome the limitations of a large volume of training data, and accelerate the practical applications of diagnostic algorithms . However, previous reported methods, mainly including parameter transfer and domain adaptation , still require a few labeled or massive unlabeled fault samples, which are not always available. In general, only extremely limited fault data, namely sparse data (single or several samples), can be obtained, and the labeling is also easy to be processed. This paper presents a novel framework for disposing the problem of transfer diagnosis with sparse target data. In consideration of the unclear data distribution described by the sparse data, the main idea is to pair the source and target data with the same machine condition and conduct individual domain adaptation so as to alleviate the lack of target data, diminish the distribution discrepancy as well as avoid negative transfer. More impressive, the issue of label space mismatching can be appropriately addressed in our network. The extensive experiments on two case studies are used to verify the proposed method. Comprehensive transfer scenarios, i.e., diverse working conditions and diverse machines, are considered. The thorough evaluation shows that the proposed method presents superior performance with respect to traditional transfer learning methods.},
  archive      = {J_ASOC},
  author       = {Te Han and Chao Liu and Rui Wu and Dongxiang Jiang},
  doi          = {10.1016/j.asoc.2021.107150},
  journal      = {Applied Soft Computing},
  pages        = {107150},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep transfer learning with limited data for machinery fault diagnosis},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive methodology for quantification of bow-tie
under type II fuzzy data. <em>ASOC</em>, <em>103</em>, 107148. (<a
href="https://doi.org/10.1016/j.asoc.2021.107148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bow-Tie is an efficient and emerging safety management tool to identify the root causes of an accident. Conventional quantification approaches for bow-tie suffer from a lack of information, insufficient data, and uncertainty. Therefore, experts’ views and knowledge about the occurrence probability of the basic events and the success probability of safety measures can be utilized to deal with the lack of information and data insufficiency issues. However, experts prefer to provide their opinion in linguistic variables that are subjected to uncertainties. To deal with these data uncertainty issues, this study has adopted the concept of type II fuzzy set for converting the subjective linguistic opinions into objective quantitative value. Bow-tie is integrated with type II fuzzy set, called F2BTA, for risk quantification. Overall, this study provides a comprehensive methodology for identifying hazardous sources &amp; hazardous processes, developing bow-tie, quantifying top event, and providing cost-effective mitigation strategies . Besides, the Fussell–Vesely (FV) importance measure of basic events is performed to identify the strong and weak causal relations . A case study is also discussed to validate the proposed model. The results indicate that improper handling of cartridge case has the highest probability of occurrence. Accordingly, mitigation strategies are suggested to the safety management team of the case organization. The results also confirm the superiority of the proposed model. The proposed methodology also contributes significantly to safety-related decision making by the practitioners.},
  archive      = {J_ASOC},
  author       = {Souvik Das and Ashish Garg and J. Maiti and O.B. Krishna and Jitesh J. Thakkar and R.K. Gangwar},
  doi          = {10.1016/j.asoc.2021.107148},
  journal      = {Applied Soft Computing},
  pages        = {107148},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive methodology for quantification of bow-tie under type II fuzzy data},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning assistive application for users with speech
disorders. <em>ASOC</em>, <em>103</em>, 107147. (<a
href="https://doi.org/10.1016/j.asoc.2021.107147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates machine learning approaches toward the development of a speaker dependent keywords spotting system intended for users with speech disorders, in particular for those with dysarthria, i.e., a neuromotor speech impairment associated with severe physical disabilities. In the field of assistive technologies , nowadays automatic speech recognition (ASR) is an open challenge since standard voice recognition approaches and voice driven services are ineffective to recognize atypical speech. To address these issues, we focus our attention on keywords spotting task in presence of dysarthria and we exploit deep learning technology in conjunction with an existing convolutional neural network model to build a tailored ASR system for users with such speech disabilities. However, the usage of a machine learning approach requires enough data availability for the training of the model; to this aim, we introduce a mobile software (app) allowing those with speech disorders to collect their audio contribution in order to enrich the speech model. Considering Italian as main language, this approach allows us to build the first database containing speech samples from Italian native users with dysarthria. As discussed in the end of the article, early experiments show promising results and give us interesting perspectives for future research directions.},
  archive      = {J_ASOC},
  author       = {Davide Mulfari and Gabriele Meoni and Marco Marini and Luca Fanucci},
  doi          = {10.1016/j.asoc.2021.107147},
  journal      = {Applied Soft Computing},
  pages        = {107147},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine learning assistive application for users with speech disorders},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pinhole-imaging-based learning butterfly optimization
algorithm for global optimization and feature selection. <em>ASOC</em>,
<em>103</em>, 107146. (<a
href="https://doi.org/10.1016/j.asoc.2021.107146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Butterfly optimization algorithm (BOA), as a recently proposed meta-heuristic optimization technique, performs competitively in solving numerical optimization problems as well as real-world applications. However, BOA has low precision, slow convergence and may be prone to local optimum, when solving complex or high-dimensional optimization problems . To overcome these defects, a modified BOA (called PIL-BOA) with adaptive gbest-guided search strategy and pinhole-imaging-based learning is proposed. Firstly, a modified position updated equation by introducing the global best (gbest) solution and the inertia weight is designed to efficiently improve the exploitation capability and the solution precision. Secondly, a novel pinhole-imaging learning strategy based on the principle of optics is presented to effectively search the unknown regions and avoid premature convergence. 23 classical problems and 60 complex optimization tasks from CEC 2014 and CEC 2017 are used to further investigate the effectiveness of PIL-BOA. The comparison results demonstrate that PIL-BOA has better performance than most compared algorithms on benchmark test functions. Finally, PIL-BOA is applied to solve feature selection problems and fault diagnosis in real-world wind turbine . The results show that PIL-BOA is superior to other competitors in term of classification accuracy .},
  archive      = {J_ASOC},
  author       = {Wen Long and Jianjun Jiao and Ximing Liang and Tiebin Wu and Ming Xu and Shaohong Cai},
  doi          = {10.1016/j.asoc.2021.107146},
  journal      = {Applied Soft Computing},
  pages        = {107146},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pinhole-imaging-based learning butterfly optimization algorithm for global optimization and feature selection},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interval-valued fuzzy regression: Philosophical and
methodological issues. <em>ASOC</em>, <em>103</em>, 107145. (<a
href="https://doi.org/10.1016/j.asoc.2021.107145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revisits interval-valued fuzzy regression and proposes a new unified framework to address interval-valued type-1 and type-2 fuzzy regression models. The paper focuses on two main objectives. First, some philosophical and methodological reflections about interval-valued type-1 fuzzy regression (IV-T1FR) and interval-valued type-2 fuzzy regression (IV-T2FR) are discussed and analyzed. These reflections aim at positioning fuzzy regression to avoid misinterpretations that may sometimes lead to erroneous or ambiguous considerations in practical applications. Consequently, the interest, relevance, representativeness and typology of interval-valued fuzzy regression are established. Therefore, IV-T1FR generalizes conventional interval regression (CIR) and increases its specificity. However, if the IV-T1FR can fit fuzzy data, then its formalism is not able to address the uncertainty phenomenon in the IV-T1FS representation. In this context, the IV-T2FR can be regarded as an uncertain IV-T1FR, i.e., a generalization of the IV-T1FR in an uncertain environment. Second, a new unified methodology to address fuzzy regression models using the concepts of gradual intervals (GIs) and thick gradual intervals (TGIs) is proposed. The proposed view allows handling regression models via an extension of the standard interval arithmetic (SIA) – initially proposed for conventional intervals (CIs) – to GIs and TGIs. The originality of the proposed approach resides in the fact that all the CIR methodologies can be extended to the IV-T1FR methods. Furthermore, all the IV-T1FR methodologies can be extended to the IV-T2FR framework. Our view does not depend on the model shape and preserves the flexibility and rigor of SIA computations in the propagation of fuzzy quantities through regression models. The proposed concepts are validated using illustrative examples.},
  archive      = {J_ASOC},
  author       = {Reda Boukezzoula and Didier Coquin},
  doi          = {10.1016/j.asoc.2021.107145},
  journal      = {Applied Soft Computing},
  pages        = {107145},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval-valued fuzzy regression: Philosophical and methodological issues},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-hop reasoning over paths in temporal knowledge graphs
using reinforcement learning. <em>ASOC</em>, <em>103</em>, 107144. (<a
href="https://doi.org/10.1016/j.asoc.2021.107144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) are usually incomplete—many new facts can be inferred from KGs with existing information. In some traditional reasoning methods, temporal information is not taken into consideration, meaning that only triplets ( head , relation , tail ) are trained. In current dynamic knowledge graphs, it is a challenge to consider the temporal aspects of facts. Recent temporal reasoning methods embed temporal information into low-dimensional spaces. These methods mainly support implicitly reasoning, which means they cannot get the specific reasoning paths. These methods limit the accuracy of reasoning paths and ignore multiple explainable reasoning paths in temporal knowledge graphs (TKGs). To overcome this limitation, we propose a multi-hop reasoning model TPath in this paper. It is a reinforcement learning (RL) framework which can learn multi-hop reasoning paths and continuously adjust the reasoning paths in TKGs. More importantly, we add time vectors in reasoning paths, which further improve the accuracy of reasoning paths. Meanwhile, considering the diversity of temporal reasoning paths, we propose a new reward function. In TPath, the agent employs the Long Short-Term Memory networks (LSTM) to capture current observations from the environment, and it outputs action vectors (relation vectors and time vectors) to the environment through activation functions . Experimentally, our model outperforms other state-of-the-art reasoning methods in several aspects over two public temporal knowledge graph datasets.},
  archive      = {J_ASOC},
  author       = {Luyi Bai and Wenting Yu and Mingzhuo Chen and Xiangnan Ma},
  doi          = {10.1016/j.asoc.2021.107144},
  journal      = {Applied Soft Computing},
  pages        = {107144},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-hop reasoning over paths in temporal knowledge graphs using reinforcement learning},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel stochastic configuration networks for large-scale
data regression. <em>ASOC</em>, <em>103</em>, 107143. (<a
href="https://doi.org/10.1016/j.asoc.2021.107143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic configuration network (SCN) is a new powerful approach for large-scale data processing which introduces a supervisory mechanism to configure the parameters of each hidden node stochastically. To enhance the accuracy and robustness of SCN, a parallel stochastic configuration network (PSCN) based on the beetle antennae search (BAS) and fuzzy evidence theory is presented. Firstly, we propose a fuzzy evidence theory for the data fusion of multiple neural networks ; Secondly, to choose a suitable scale factor λ λ of weights and biases, BAS, as a meta-heuristic algorithm which only uses one individual to search for optimal parameter, it is appropriate for parameter selection of SCN, termed as BAS-SCN. Thirdly, parallel training of multiple BAS-SCNs with different objective functions, then several preliminary results of BAS-SCNs are fused by the fuzzy evidence theory to obtain the final results of PSCN. Finally, a complicated real-world dataset (IEEE 2012 PHM) is used to verify the performance of the PSCN. Numerical experimental results show that BAS-SCN performs well in parameter optimization of SCN, PSCN not only has the advantages of BAS-SCN, but also has a higher accuracy and stronger robustness. PSCN has a better performance in the prediction of bearing remaining useful life.},
  archive      = {J_ASOC},
  author       = {Chenglong Zhang and Shifei Ding and Jian Zhang and Weikuan Jia},
  doi          = {10.1016/j.asoc.2021.107143},
  journal      = {Applied Soft Computing},
  pages        = {107143},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel stochastic configuration networks for large-scale data regression},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint optimization of service chain caching and task
offloading in mobile edge computing. <em>ASOC</em>, <em>103</em>,
107142. (<a href="https://doi.org/10.1016/j.asoc.2021.107142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caching and offloading in Mobile Edge Computing (MEC) are hot topics recently. Existing caching strategies at the edge ignore the programming ability of edge network and design strategies independently thus network resource is under utilization and the quality of experience (QOE) for end users is far from satisfactory. In this paper, we design intelligently joint caching and offloading strategies under the assumption that applications can be in the form of divisible service chain. Different from common approaches that target on reducing response latency only for users, our system take the leasing cost into consideration thus is more efficient for Application Service Providers (ASP). To fulfill our design, we novelly utilize open Jackson queuing network to formulate this joint optimization problem under long term cost restrictions and design a pipeline of algorithm to search for the optimal solution. More specifically, we design a cost adaptive algorithm derived from Lyapunov drift-plus-penalty function so that the long-term problem can be optimized in the slot-by-slot basis. Moreover, we propose to exploit resource-based utility function and device-number-based relative distance to jointly find optimal caching and offloading scheme. Extensive simulation results demonstrate that our approach can effectively reduce the average service latency of the MEC system and keep a low average leasing cost.},
  archive      = {J_ASOC},
  author       = {Kai Peng and Jiangtian Nie and Neeraj Kumar and Chao Cai and Jiawen Kang and Zehui Xiong and Yang Zhang},
  doi          = {10.1016/j.asoc.2021.107142},
  journal      = {Applied Soft Computing},
  pages        = {107142},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint optimization of service chain caching and task offloading in mobile edge computing},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CASA-based speaker identification using cascaded GMM-CNN
classifier in noisy and emotional talking conditions. <em>ASOC</em>,
<em>103</em>, 107141. (<a
href="https://doi.org/10.1016/j.asoc.2021.107141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims at intensifying text-independent speaker identification performance in real application situations such as noisy and emotional talking conditions. This is achieved by incorporating two different modules: a Computational Auditory Scene Analysis (CASA) based pre-processing module for noise reduction and “cascaded Gaussian Mixture Model – Convolutional Neural Network (GMM-CNN) classifier for speaker identification” followed by emotion recognition. This research proposes and evaluates a novel algorithm to improve the accuracy of speaker identification in emotional and highly-noise susceptible conditions. Experiments demonstrate that the proposed model yields promising results in comparison with other classifiers when “Speech Under Simulated and Actual Stress (SUSAS) database, Emirati Speech Database (ESD), the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)” database and the “Fluent Speech Commands” database are used in a noisy environment .},
  archive      = {J_ASOC},
  author       = {Ali Bou Nassif and Ismail Shahin and Shibani Hamsa and Nawel Nemmour and Keikichi Hirose},
  doi          = {10.1016/j.asoc.2021.107141},
  journal      = {Applied Soft Computing},
  pages        = {107141},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CASA-based speaker identification using cascaded GMM-CNN classifier in noisy and emotional talking conditions},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved atom search optimization with dynamic opposite
learning and heterogeneous comprehensive learning. <em>ASOC</em>,
<em>103</em>, 107140. (<a
href="https://doi.org/10.1016/j.asoc.2021.107140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atom search optimization (ASO) is a newly developed metaheuristic algorithm inspired by molecular basis dynamics. The paramount challenge in ASO is that it is prone to stagnation in local optima due to premature convergence. To solve this issue, a modified atom search optimization with dynamic opposite learning and heterogeneous comprehensive learning is proposed in this paper, which is named DOLHCLASO. First, the asymmetry of dynamic opposite learning can increase the probability that the population will obtain an optimal solution, while dynamic characteristic can enrich population diversity and enhance exploration capability. Second, the heterogeneous comprehensive learning divides the population into two subpopulations, exploration-subpopulation and exploitation-subpopulation, and the division of labor and cooperation effectively balances exploration and exploitation. Finally, the proposed DOLHCLASO was evaluated in the CEC2017 benchmark functions and real-world engineering cases, compared with some classic and excellent variants of algorithms to confirm its performance. Experimental results and statistical analysis demonstrate that the performance of the DOLHCLASO is significantly better than other selected optimizers.},
  archive      = {J_ASOC},
  author       = {Pu Sun and Hao Liu and Yong Zhang and Qingyao Meng and Liangping Tu and Jian Zhao},
  doi          = {10.1016/j.asoc.2021.107140},
  journal      = {Applied Soft Computing},
  pages        = {107140},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved atom search optimization with dynamic opposite learning and heterogeneous comprehensive learning},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A stochastic approximation method for probability prediction
of docking success for aerial refueling. <em>ASOC</em>, <em>103</em>,
107139. (<a href="https://doi.org/10.1016/j.asoc.2021.107139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial refueling is capable of increasing the endurance and flight range of aircraft. However, during the docking phase of aerial refueling, the docking risk increases as the receiver aircraft approaches the tanker aircraft. A high docking risk indicates docking failure or even flight collision. In order to guarantee docking reliability and safety, a stochastic approximation method is proposed to predict the docking success probability during the docking phase of aerial refueling. If the success probability is lower than the specified value, the receiver aircraft needs to increase its relative distance from the tanker aircraft to reduce the risk. In our method, a stochastic differential equation is used to model the relative motion between the receiver and tanker aircrafts, where the receiver aircraft flies along a predefined flight path with influence of additive wind perturbations. The correlation between the magnitude of wind perturbations and the distance between the receiver and tanker aircrafts is considered. Then, based on the established relative motion model, the probability that the receiver aircraft enters the target set during a given time interval is predicted by the Markov chain stochastic approximation method. The docking success probability is computed by propagating the transition probabilities of the Markov chain backwards starting from the target set during the time interval. Comparing with the widely used Monte Carlo method in previous studies, our method demonstrates substantial effectiveness and efficiency.},
  archive      = {J_ASOC},
  author       = {Ying Liu and Zhiyao Zhao and Haibiao Ma and Quan Quan},
  doi          = {10.1016/j.asoc.2021.107139},
  journal      = {Applied Soft Computing},
  pages        = {107139},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A stochastic approximation method for probability prediction of docking success for aerial refueling},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IHC-net: A fully convolutional neural network for automated
nuclear segmentation and ensemble classification for allred scoring in
breast pathology. <em>ASOC</em>, <em>103</em>, 107136. (<a
href="https://doi.org/10.1016/j.asoc.2021.107136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Deep Learning (DL)-based nuclear segmentation and ensemble classification scheme for quantitative evaluation of hormone status namely estrogen or progesterone on IHC specimen. This will mainly assist pathologists in automatic analysis and interpretation of breast cancer prognosis route. The proposed method consists of two major steps i.e., segmentation and classification. Since invasive breast cancer images are associated with numerous stained cells including artifacts like stromal and inflammatory particulars, it is crucial to develop a computerized method for segmenting them. A new segmentation method has been presented based on deep learning network for precisely segmenting out the stained nuclei region from breast tissue images. Morphological post-processing on segmented results shows the splitting of overlapped nuclei. Finally, to improve individual classifier’s results, the ensemble method is used, which integrates the decision of three machine learning (ML) models for final Allred cancer score. Statistical analysis reveals that all three classifiers perform adequately but proposed approach shows the best accuracy (98.24\%), best correlation with the manual expert’s score (Pearson’s correlation coefficient = 0.908) and requires minimum computational time 44s/image ( ± ± 2.33) compared to state-of-the-art methods. The proposed framework can be used as a reliable alternative to manual methods for automatic Allred scoring and in the prognostic assessment of breast cancer.},
  archive      = {J_ASOC},
  author       = {Lipi B. Mahanta and Elima Hussain and Navarun Das and Lopamudra Kakoti and Manish Chowdhury},
  doi          = {10.1016/j.asoc.2021.107136},
  journal      = {Applied Soft Computing},
  pages        = {107136},
  shortjournal = {Appl. Soft. Comput.},
  title        = {IHC-net: A fully convolutional neural network for automated nuclear segmentation and ensemble classification for allred scoring in breast pathology},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Drug repositioning based on similarity constrained
probabilistic matrix factorization: COVID-19 as a case study.
<em>ASOC</em>, <em>103</em>, 107135. (<a
href="https://doi.org/10.1016/j.asoc.2021.107135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel coronavirus disease 2019 (COVID-19) pandemic has caused a massive health crisis worldwide and upended the global economy. However, vaccines and traditional drug discovery for COVID-19 cost too much in terms of time, manpower, and money. Drug repurposing becomes one of the promising treatment strategies amid the COVID-19 crisis. At present, there are no publicly existing databases for experimentally supported human drug–virus interactions, and most existing drug repurposing methods require the rich information, which is not always available, especially for a new virus. In this study, on the one hand, we put size-able efforts to collect drug–virus interaction entries from literature and build the Human Drug Virus Database (HDVD). On the other hand, we propose a new approach, called SCPMF (similarity constrained probabilistic matrix factorization), to identify new drug–virus interactions for drug repurposing. SCPMF is implemented on an adjacency matrix of a heterogeneous drug–virus network, which integrates the known drug–virus interactions, drug chemical structures, and virus genomic sequences. SCPMF projects the drug–virus interactions matrix into two latent feature matrices for the drugs and viruses, which reconstruct the drug–virus interactions matrix when multiplied together, and then introduces the weighted similarity interaction matrix as constraints for drugs and viruses. Benchmarking comparisons on two different datasets demonstrate that SCPMF has reliable prediction performance and outperforms several recent approaches. Moreover, SCPMF-predicted drug candidates of COVID-19 also confirm the accuracy and reliability of SCPMF.},
  archive      = {J_ASOC},
  author       = {Yajie Meng and Min Jin and Xianfang Tang and Junlin Xu},
  doi          = {10.1016/j.asoc.2021.107135},
  journal      = {Applied Soft Computing},
  pages        = {107135},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Drug repositioning based on similarity constrained probabilistic matrix factorization: COVID-19 as a case study},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying polyadenylation signals with biological
embedding via self-attentive gated convolutional highway networks.
<em>ASOC</em>, <em>103</em>, 107133. (<a
href="https://doi.org/10.1016/j.asoc.2021.107133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polyadenylation is responsible for regulating mRNA function and translation efficiency. The accurate polyadenylation signal (PAS) recognition is essential for understanding the genome annotation and function. Currently, computational methods mainly rely on human-designed features and do not extract complex discriminative patterns. In this work, we first adopt a shallow neural network to automatically learn biological embedding information with discriminative patterns from biological sequences alone. Next, we devise a hybrid deep learning framework termed PASNet to automatically extract underlying patterns by integrating gated convolutional highway networks with a self-attention mechanism and then identify PAS from genomic sequences. Specifically, we devise a gated convolutional highway unit by gated convolutional mechanisms, and the highway unit has significant paths to allow unimpeded information to pass between two adjacent layers. PASNet requires little prior knowledge and no laborious feature engineering. Besides, we evaluate PASNet on four different organism datasets through genome-wide experiments. Compared to published machine learning and deep learning predictors, results show that our framework achieves the improvement with the error rate decreasing by 4\% and 1.02\% respectively on average.},
  archive      = {J_ASOC},
  author       = {Yanbu Guo and Dongming Zhou and Weihua Li and Jinde Cao and Rencan Nie and Lei Xiong and Xiaoli Ruan},
  doi          = {10.1016/j.asoc.2021.107133},
  journal      = {Applied Soft Computing},
  pages        = {107133},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying polyadenylation signals with biological embedding via self-attentive gated convolutional highway networks},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probability-optimal leader comprehensive learning particle
swarm optimization with bayesian iteration. <em>ASOC</em>, <em>103</em>,
107132. (<a href="https://doi.org/10.1016/j.asoc.2021.107132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel comprehensive learning particle swarm optimization algorithm, which is based on the Bayesian iteration method and named as Bayesian comprehensive learning particle swarm optimization (BCLPSO), is proposed. In the original PSO, the flying direction of each particle is based on its own historical best position and global optimum. This updating mechanism, however, easily falls into the local optimum, and the potential optimum solution may be ignored in the iteration and update process. Therefore, the BCLPSO is designed to facilitate discovering potential solution and avoid the problem of premature convergence. In the BCLPSO algorithm, the exemplar of the swarm is not the global best position but the particle location with the largest posterior probability based on the Bayesian formula. The posterior probability is developed by historical prior information. This means that the posterior probability can inherit the historical information of particles that may be exploited. In this way, the swarm diversity can be preserved to prevent premature convergence. The BCLPSO is experimentally validated on the CEC2017 benchmark functions and compared with other state-of-the-art particle swarm optimization algorithms. The results show that BCLPSO outperforms other comparative PSO variants on the CEC 2017 test suite. Furthermore, the algorithm is applied to the quality control process of an automated welding production line for the automobile body and is found to exhibit superior performance.},
  archive      = {J_ASOC},
  author       = {Xing Zhang and Wei Sun and Min Xue and Anping Lin},
  doi          = {10.1016/j.asoc.2021.107132},
  journal      = {Applied Soft Computing},
  pages        = {107132},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Probability-optimal leader comprehensive learning particle swarm optimization with bayesian iteration},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AIGSA-based multi-jammer localization in wireless networks.
<em>ASOC</em>, <em>103</em>, 107131. (<a
href="https://doi.org/10.1016/j.asoc.2021.107131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jamming attacks, which exploit the shared and openness nature of the wireless mediums, are one of the major threats to wireless networks since they may significantly degrade the performance of or even corrupt wireless networks by continuously preventing wireless channels from serving intended users. To facilitate conducting anti-jamming countermeasures , a few single jammer localization algorithms have been introduced and evaluated. In contrast, only few efforts have been made for locating multiple jammers, for instance X-ray and Cluster algorithms . They are developed based on the assumption that a jammer’s affected area is roughly a circle; however, their localization accuracy is unsatisfactory if multiple jammers’ affected areas are overlapped. In this backdrop, a Multi-jammer Localization Algorithm based on Alternating Iteration and Gravitational Search Algorithm (MLA-AIGSA) is put forward in this paper. On the contrary to existing proposals, MLA-AIGSA neither relies on the mapping between the received signal power and two nodes’ mutual distance nor counts on the ideal assumption about the shape of a jammer’s affected area. MLA-AIGSA contains two steps, i.e. estimating the number of jammers and Multi-jammer localization . At first, Region Growth Algorithm (RGA) is adopted to derive the number of jammers based on collected Received Jamming Signal Strength (RJSS) values. Then, Alternating Iteration (AIt) and GSA are combined to estimate and refine multiple jammers’ positions in an iterative way. The overall time complexity of MLA-AIGSA is in quadratic time to the number of the particles and is in linear time to the number of AIt’s and GSA’s iterations. Extensive experiments are conducted to validate MLA-AIGSA’s correctness and effectiveness, and the results show that MLA-AIGSA’s average localization error (around 3 m) is nearly 70\% lower than state-of-the-art proposals, including X-ray (17 m) and Cluster (10 m) algorithms.},
  archive      = {J_ASOC},
  author       = {Xianglin Wei and Tongxiang Wang},
  doi          = {10.1016/j.asoc.2021.107131},
  journal      = {Applied Soft Computing},
  pages        = {107131},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AIGSA-based multi-jammer localization in wireless networks},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A type-2 neutrosophic-entropy-fusion based multiple
thresholding method for the brain tumor tissue structures segmentation.
<em>ASOC</em>, <em>103</em>, 107119. (<a
href="https://doi.org/10.1016/j.asoc.2021.107119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce an extension theory of the neutrosophic set (NS) called type-2 neutrosophic set (T2NS) . This new theory provides a granular representation of features and helps to model uncertainties with six different memberships very effectively. To demonstrate the real-time application of this theory, a new segmentation method for brain tumor tissue structures in magnetic resonance imaging (MRI) is presented. There are inconsistencies in the gray levels observed in MRIs due to their low illumination. The proposed theory addressed this problem by performing neutrosophication operation on gray levels based on six different membership functions called type-2 neutrosophic membership functions . During segmentation, a concept of T2NS entropy is used to quantify each gray level of MRIs. The proposed method is able to select multiple adaptive thresholds for segmentation of brain tumor tissue structures in MRIs from the locations of maximum entropy values of gray levels. Finally, an image fusion operation is performed on the segmented images with different thresholds to include all features and identify the location of brain tumor. The fusion images are compared with the segmented images obtained by five different methods, including fuzzy c-means algorithm, modified fuzzy c-means algorithm, fuzzy-K-means clustering algorithm, kernel intuitionistic fuzzy entropy c-means and neutrosophic entropy-based adaptive thresholding method. The proposed method achieves Jaccard similarity coefficients of 97.07\%, 97.92\% and 97.13\% in the case of three different sets of MRIs, namely Set I, Set II and Set III, respectively. The proposed method exhibits correlation coefficients of 0.9638, 0.9698 and 0.9610 for the Set I, Set II and Set III of MRIs, respectively. Similarly, the proposed method shows uniformity measures of 0.9624, 0.9633 and 0.9660 for the Set I, Set II and Set III of MRIs, respectively. These three performance evaluation metrics show the effectiveness of the proposed method compared to the existing methods.},
  archive      = {J_ASOC},
  author       = {Pritpal Singh},
  doi          = {10.1016/j.asoc.2021.107119},
  journal      = {Applied Soft Computing},
  pages        = {107119},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A type-2 neutrosophic-entropy-fusion based multiple thresholding method for the brain tumor tissue structures segmentation},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Data-driven product design evaluation method based on
multi-stage artificial neural network. <em>ASOC</em>, <em>103</em>,
107117. (<a href="https://doi.org/10.1016/j.asoc.2021.107117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of the new generation of IT technology in the manufacturing industry has brought a rich data foundation for the research of product life cycle management. However, in the product design stage, the existing design evaluation methods have not yet effectively used the data-driven advantages, which leads to over-reliance on personal experience and inefficiency. This paper presents a new data-driven product design evaluation (PDE) method to address this problem. First, a PDE model considering multi-stage evaluation indicators of the product life cycle is established. Second, to accurately and quickly realize the nonlinear mapping between the PDE model indicators, an improved multi-stage artificial neural network (ANN) based on a hybrid optimization algorithm combining particle swarm optimization and Adam (PSO-Adam) is proposed. The PSO is employed for a rapid convergence during the initial stage of a global search, while Adam is used to optimizing the training parameters around the global optimum adaptively. Finally, comparison experiments of the smartphone design are carried out to demonstrate the effectiveness of the multi-stage ANN and the PSO-Adam. The results demonstrate the proposed approach can help designers comprehensively consider design parameters and make fast and accurate design evaluation.},
  archive      = {J_ASOC},
  author       = {Lei Wang and Zhengchao Liu},
  doi          = {10.1016/j.asoc.2021.107117},
  journal      = {Applied Soft Computing},
  pages        = {107117},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven product design evaluation method based on multi-stage artificial neural network},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). M2FN: Multi-step modality fusion for advertisement image
assessment. <em>ASOC</em>, <em>103</em>, 107116. (<a
href="https://doi.org/10.1016/j.asoc.2021.107116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing advertisements, specifically on the basis of user preferences and ad quality, is crucial to the marketing industry. Although recent studies have attempted to use deep neural networks for this purpose, these studies have not utilized image-related auxiliary attributes, which include embedded text frequently found in ad images. We, therefore, investigated the influence of these attributes on ad image preferences. First, we analyzed large-scale real-world ad log data and, based on our findings, proposed a novel multi-step modality fusion network (M2FN) that determines advertising images likely to appeal to user preferences. Our method utilizes auxiliary attributes through multiple steps in the network, which include conditional batch normalization-based low-level fusion and attention-based high-level fusion. We verified M2FN on the AVA dataset, which is widely used for aesthetic image assessment, and then demonstrated that M2FN can achieve state-of-the-art performance in preference prediction using a real-world ad dataset with rich auxiliary attributes.},
  archive      = {J_ASOC},
  author       = {Kyung-Wha Park and Jung-Woo Ha and JungHoon Lee and Sunyoung Kwon and Kyung-Min Kim and Byoung-Tak Zhang},
  doi          = {10.1016/j.asoc.2021.107116},
  journal      = {Applied Soft Computing},
  pages        = {107116},
  shortjournal = {Appl. Soft. Comput.},
  title        = {M2FN: Multi-step modality fusion for advertisement image assessment},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural-dynamics-enabled jacobian inversion for model-based
kinematic control of multi-section continuum manipulators.
<em>ASOC</em>, <em>103</em>, 107114. (<a
href="https://doi.org/10.1016/j.asoc.2021.107114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuum manipulators are a new generation of robotic systems that possess infinite number of degrees of freedom associated with inherent compliance, unlike traditional robotic manipulators which consist of a finite number of rigid links. Because of this characteristic, controlling continuum manipulators is more complicated and difficult based on only traditional control theory. Soft computing techniques are solid alternative for improving the control performance of such kinds of robots. In this paper, we employ two types of neural dynamic approaches, i.e., gradient neural dynamics and zeroing neural dynamics, to solve the real-time Jacobian matrix pseudo-inversion problem, thereby achieving model-based kinematic control of multi-section continuum manipulators. Different kinds of neural dynamic models are investigated and their performances in terms of tracking accuracy are shown with and without noise disturbances. Simulation validations with a two-section and a three-section continuum manipulator demonstrate the feasibility and robustness of the proposed models.},
  archive      = {J_ASOC},
  author       = {Ning Tan and Mingwei Huang and Peng Yu and Tao Wang},
  doi          = {10.1016/j.asoc.2021.107114},
  journal      = {Applied Soft Computing},
  pages        = {107114},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neural-dynamics-enabled jacobian inversion for model-based kinematic control of multi-section continuum manipulators},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evidential reasoning rule based feature selection for
improving trauma outcome prediction. <em>ASOC</em>, <em>103</em>,
107112. (<a href="https://doi.org/10.1016/j.asoc.2021.107112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various demographic and medical factors can be linked to severe deterioration of patients suffering from traumatic injuries . Accurate identification of the most relevant variables is essential for building more accurate prediction models and making more rapid life-saving medical decision. The intention of this paper is to select a number of features that can be used to accurately predict patients’ outcomes through three feature selection methods: random forest , ReliefF and the evidential reasoning (ER) rule. The impact of an outcome’s class imbalance on feature selection is discussed, and synthetic minority over-sampling technique (SMOTE) is performed to show the differences in the selected features. The results show that length of stay in hospital, length of stay in intensive care unit , age, and Glasgow Coma Scale (GCS) are the most selected features across different techniques. The prediction models based on the features selected by the ER rule show the highest prediction performance represented by the area under the receiver operating characteristic curve (AUC) values, which has a median of 0.895 for the model employed by the ten highest-weighted variables, while the median AUC values are 0.827 and 0.885 if the ten highest-weighted variables are selected by ReliefF and random forest respectively. The results also show that after the ten most important features, increasing the number of the less important features has only a slight increase in prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Fatima Almaghrabi and Dong-Ling Xu and Jian-Bo Yang},
  doi          = {10.1016/j.asoc.2021.107112},
  journal      = {Applied Soft Computing},
  pages        = {107112},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evidential reasoning rule based feature selection for improving trauma outcome prediction},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed computation offloading method based on deep
reinforcement learning in ICV. <em>ASOC</em>, <em>103</em>, 107108. (<a
href="https://doi.org/10.1016/j.asoc.2021.107108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Intelligent Connected Vehicles (ICVs), more effective computation resources optimization schemes in task scheduling are exactly required for large-scale network implementation. We observe that an offloading scheme that almost all tasks are going to be executed in Multi-Access Edge Computing (MEC) servers, which lead to a lot of vehicle resources to be underutilized and put a great burden on severs, is not a good solution for resource utilization. So we first consider the scenario where MEC is not available or enough. We take surrounding vehicles as a Resource Pool (RP). And we propose a distributed computation offloading method to utilize all resources, in which a complex task can be split into many small sub-tasks. How to assign these minor tasks to get a better execution time in RP is a hard problem. The executing time of a complex computing task is a min–max problem. In this paper, a distributed computation offloading strategy based on Deep Q-learning Network (DQN) is proposed to find the best offloading method to minimize the execution time of a compound task. We can demonstrate that the model proposed in this paper can take full advantage of the computing resources of the surrounding vehicles and greatly reduce the execution time of the computation tasks.},
  archive      = {J_ASOC},
  author       = {Chen Chen and Yuru Zhang and Zheng Wang and Shaohua Wan and Qingqi Pei},
  doi          = {10.1016/j.asoc.2021.107108},
  journal      = {Applied Soft Computing},
  pages        = {107108},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distributed computation offloading method based on deep reinforcement learning in ICV},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient activity recognition using lightweight CNN and
DS-GRU network for surveillance applications. <em>ASOC</em>,
<em>103</em>, 107102. (<a
href="https://doi.org/10.1016/j.asoc.2021.107102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing human activities has become a trend in smart surveillance that contains several challenges, such as performing effective analyses of huge video data streams, while maintaining low computational complexity, and performing this task in real-time. Current activity recognition techniques are using convolutional neural network (CNN) models with computationally complex classifiers, creating hurdles in obtaining quick responses for abnormal activities. To address these challenges in real-time surveillance, this paper proposes a lightweight deep learning-assisted framework for activity recognition. First, we detect a human in the surveillance stream using an effective CNN model, which is trained on two surveillance datasets. The detected individual is tracked throughout the video stream via an ultra-fast object tracker called the ‘minimum output sum of squared error’ (MOSSE). Next, for each tracked individual, pyramidal convolutional features are extracted from two consecutive frames using the efficient LiteFlowNet CNN. Finally, a novel deep skip connection gated recurrent unit (DS-GRU) is trained to learn the temporal changes in the sequence of frames for activity recognition. Experiments are conducted over five benchmark activity recognition datasets, and the results indicate the efficiency of the proposed technique for real-time surveillance applications compared to the state-of-the-art.},
  archive      = {J_ASOC},
  author       = {Amin Ullah and Khan Muhammad and Weiping Ding and Vasile Palade and Ijaz Ul Haq and Sung Wook Baik},
  doi          = {10.1016/j.asoc.2021.107102},
  journal      = {Applied Soft Computing},
  pages        = {107102},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient activity recognition using lightweight CNN and DS-GRU network for surveillance applications},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preface: Data-driven decision making - theory, methods, and
applications. <em>ASOC</em>, <em>102</em>, 107261. (<a
href="https://doi.org/10.1016/j.asoc.2021.107261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Ching-Ter Chang and Chun-Che Huang (Managing editor) and Tzu-Liang (Bill) Tseng},
  doi          = {10.1016/j.asoc.2021.107261},
  journal      = {Applied Soft Computing},
  pages        = {107261},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Preface: Data-driven decision making - theory, methods, and applications},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bee-foraging learning particle swarm optimization.
<em>ASOC</em>, <em>102</em>, 107134. (<a
href="https://doi.org/10.1016/j.asoc.2021.107134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous particle swarm optimization (PSO) algorithms have been developed for solving numerical optimization problems in recent years. However, most of existing PSO algorithms have only one search phase. There is no strengthened search phase for the well-performed particles, and also no re-initialization phase for the exhausted particles. These issues may still restrict the performance of PSO for complex optimization problems . In this paper, inspired by the bee-foraging search mechanism of artificial bee colony algorithm , a novel bee-foraging learning PSO (BFL-PSO) algorithm is proposed. Different from existing PSO algorithms, the proposed BFL-PSO has three different search phases, namely employed learning, onlooker learning and scout learning. The employed learning phase works like traditional one-phase-based PSO, while the onlooker learning phase performs strengthened search around those well-performed particles to exploit promising solutions, and the scout learning phase re-initializes those exhausted particles to introduce new diversity. The proposed BFL-PSO is comprehensively evaluated on CEC2014 benchmark functions , and compared with state-of-the-art PSO algorithms as well as artificial bee colony algorithms. The experimental results show that BFL-PSO achieves very competitive performance in terms of solution accuracy. In addition, the effectiveness of the newly introduced onlooker learning and scout learning phases in BFL-PSO is verified.},
  archive      = {J_ASOC},
  author       = {Xu Chen and Hugo Tianfield and Wenli Du},
  doi          = {10.1016/j.asoc.2021.107134},
  journal      = {Applied Soft Computing},
  pages        = {107134},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bee-foraging learning particle swarm optimization},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum inspired particle swarm optimization with guided
exploration for function optimization. <em>ASOC</em>, <em>102</em>,
107122. (<a href="https://doi.org/10.1016/j.asoc.2021.107122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex function optimization problems , which have properties like multimodality, high dimensionality , non-differentiability, non-linear parameter interactions, are challenging and hard to solve. A number of meta-heuristic algorithms are proposed to find near optimal solution to these complex problems. However, most of them suffer from poor exploration and get caught in local optima. In order to overcome this problem, we present an enhanced Quantum behaved Particle Swarm Optimization ( e-QPSO) algorithm, which improves the exploration and the exploitation properties of the original QPSO for function optimization. We introduce the adaptive balance among the personal best and the global best positions using the parameter alpha and achieve the balance between the diversification and the intensification using the parameter gamma . Further, we re-initialize a percentage of the worst performing population to help escape the particle from the local optima. These three modifications in the e-QPSO play crucial role to enhance the performance of the original QPSO algorithm. The e-QPSO is validated on 59 well-known challenging benchmark problems including 5 engineering problems. The results of e-QPSO outperform those of existing twelve QPSO variants, two adaptive variants of PSO, as well as nine well-known evolutionary algorithms . Statistical tests also demonstrate the statistically significant performance of the e-QPSO compared to the other meta-heuristic methods.},
  archive      = {J_ASOC},
  author       = {R.K. Agrawal and Baljeet Kaur and Parul Agarwal},
  doi          = {10.1016/j.asoc.2021.107122},
  journal      = {Applied Soft Computing},
  pages        = {107122},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum inspired particle swarm optimization with guided exploration for function optimization},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-step multi-view and multi-label learning with missing
label via subspace learning. <em>ASOC</em>, <em>102</em>, 107120. (<a
href="https://doi.org/10.1016/j.asoc.2021.107120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-view and multi-label learning, each example can be represented by multiple data view features and annotated with a set of discrete non-exclusive labels. Missing label learning is an important branch of multi-label learning, which can handle incomplete labels with annotations. Previous work on multi-label learning with missing labels mainly considered data in a single view representation. Based on intuitive understanding, we propose a T wo-step M ulti-view and M ulti-label M issing L abel learning optimization solution(TM3L). The first step is to solve the multi-view learning problem by finding the data representation of the common low-dimensional space of all views through subspace learning. While fully considering the complementary information between multiple views, the different degrees of contribution combined with different views are weighted differently. The second step is to solve the multi-label missing label learning problem by using the label matrix completion method in combination with the kernel extreme learning machine classifier. The kernel extreme learning machine can effectively enhance the robustness of the algorithm to missing labels. The experimental results and analysis on multiple benchmark multi-view and multi-label data sets verify the effectiveness of TM3L compared with the state-of-the-art solutions.},
  archive      = {J_ASOC},
  author       = {Dawei Zhao and Qingwei Gao and Yixiang Lu and Dong Sun},
  doi          = {10.1016/j.asoc.2021.107120},
  journal      = {Applied Soft Computing},
  pages        = {107120},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-step multi-view and multi-label learning with missing label via subspace learning},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated multi-criteria decision making approach with
linguistic hesitant fuzzy sets for e-learning website evaluation and
selection. <em>ASOC</em>, <em>102</em>, 107118. (<a
href="https://doi.org/10.1016/j.asoc.2021.107118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network teaching has been widely developed under the influence of COVID-19 pandemic to guarantee the implementation of teaching plans and protect the learning rights of students. Selecting a particular website for network teaching can directly affects end users’ performance and promote network teaching quality. Normally, e-learning website selection can be considered as a complex multi-criteria decision making (MCDM) problem, and experts’ evaluations over the performance of e-learning websites are often imprecise and fuzzy due to the subjective nature of human thinking. In this article, we propose a new integrated MCDM approach on the basis of linguistic hesitant fuzzy sets (LHFSs) and the TODIM (an acronym in Portuguese of interactive and multi-criteria decision making) method to evaluate and select the best e-learning website for network teaching. This introduced method deals with the linguistic assessments of experts based on the LHFSs, determines the weights of evaluation criteria with the best–worst method (BWM), and acquires the ranking of e-learning websites utilizing an extended TODIM method. The applicability and superiority of the presented linguistic hesitant fuzzy TODIM (LHF-TODIM) approach are demonstrated through a realistic e-learning website selection example. Results show that the LHF-TODIM model being proposed is more practical and effective for solving the e-learning website selection problem under vague and uncertain linguistic environment.},
  archive      = {J_ASOC},
  author       = {Jia-Wei Gong and Hu-Chen Liu and Xiao-Yue You and Linsen Yin},
  doi          = {10.1016/j.asoc.2021.107118},
  journal      = {Applied Soft Computing},
  pages        = {107118},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated multi-criteria decision making approach with linguistic hesitant fuzzy sets for E-learning website evaluation and selection},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ball bonding inspections using a conjoint framework with
machine learning and human judgement. <em>ASOC</em>, <em>102</em>,
107115. (<a href="https://doi.org/10.1016/j.asoc.2021.107115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ball bonding inspections with human vision are essential in manufacturing processes of semiconductors devices and integrated circuits (ICs). The inspections are an intensive task which involves human labours to detect poor bonds. Prolonged visual inspections cause poor inspection integrity due to eye-fatigue. However, inspections nowadays are mostly conducted manually by humans which cannot satisfy the demanding productions. Motivated by the extraordinary performance of machine learning for manufacturing inspections, a detection framework integrated with machine learning and human judgement is proposed to aid bonding inspections based on visual images. The detection framework is incorporated with the convolution neural network (CNN), support vector machine (SVM) and circle hough transform algorithm (CHT); human judgement is only used when the detection uncertainty is below the threshold. The novel machine learning integration is proposed on the detection framework to improve the generalization capabilities. The CNN architecture is redeveloped by incorporating with the SVM which is generally more effective than the fully connected network in the classical CNN. Also a novel training function is proposed based on the CHT to ensure the inspection reliability; the function not only takes into account real image captures, but also locates important features using pattern analysis of the ball bondings. Experimental results show that significantly better classifications can be achieved by the proposed framework compared with the classical CNN and other commonly used classifiers. Only the machine learning determinations below the threshold are reassessed by human judgements.},
  archive      = {J_ASOC},
  author       = {Kit Yan Chan and Ka Fai Cedric Yiu and Hak-Keung Lam and Bert Wei Wong},
  doi          = {10.1016/j.asoc.2021.107115},
  journal      = {Applied Soft Computing},
  pages        = {107115},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ball bonding inspections using a conjoint framework with machine learning and human judgement},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed grey wolf optimizer for scheduling of workflow
applications in cloud environments. <em>ASOC</em>, <em>102</em>, 107113.
(<a href="https://doi.org/10.1016/j.asoc.2021.107113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal scheduling of workflows in cloud computing environments is an essential element to maximize the utilization of Virtual Machines (VMs). In practice, scheduling of dependent tasks in a workflow requires distributing the tasks to the available VMs on the cloud. This paper introduces a discrete variation of the Distributed Grey Wolf Optimizer (DGWO) for scheduling dependent tasks to VMs. The scheduling process in DGWO is modeled as a minimization problem for two objectives: computation and data transmission costs. DGWO uses the largest order value (LOV) method to convert the continuous candidate solutions produced by DGWO to discrete candidate solutions. DGWO was experimentally tested and compared to well-known optimization-based scheduling algorithms (Particle Swarm Optimization (PSO), Grey Wolf Optimizer). The experimental results suggest that DGWO distributes tasks to VMs faster than the other tested algorithms. Besides, DGWO was compared to PSO and Binary PSO (BPSO) using WorkflowSim and scientific workflows of different sizes. The obtained simulation results suggest that DGWO provides the best makespan compared to the other algorithms.},
  archive      = {J_ASOC},
  author       = {Bilal H. Abed-alguni and Noor Aldeen Alawad},
  doi          = {10.1016/j.asoc.2021.107113},
  journal      = {Applied Soft Computing},
  pages        = {107113},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distributed grey wolf optimizer for scheduling of workflow applications in cloud environments},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time series forecasting based on echo state network and
empirical wavelet transformation. <em>ASOC</em>, <em>102</em>, 107111.
(<a href="https://doi.org/10.1016/j.asoc.2021.107111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo state network (ESN) is a reservoir computing structure consisting randomly generated hidden layer which enables a rapid learning and extrapolation process. On the other hand, the determination of inputs is still an outstanding question similar to other neural networks . Instead of utilizing the original historical data, wavelet transformations can be used to extract and reflect features with predictability (periodicity) and eliminate random oscillations (i.e. noise) which would distort the underlying predictive structure. In this regard, we propose a two-stage predictive algorithm in which the empirical wavelet transformation (EWT) has been implemented to transform the data, and ESN is utilized to execute the overall predictive process. The proposed method has been validated by twelve public datasets with different mean-volatility features. Out of sample forecasts have been compared to the baseline persistence model and conventional benchmarks. The empirical study demonstrates the superiority of the EWT–ESN model compared with other models.},
  archive      = {J_ASOC},
  author       = {Ruobin Gao and Liang Du and Okan Duru and Kum Fai Yuen},
  doi          = {10.1016/j.asoc.2021.107111},
  journal      = {Applied Soft Computing},
  pages        = {107111},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time series forecasting based on echo state network and empirical wavelet transformation},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An artificial-neural-network based prediction of heat
transfer behaviors for in-tube supercritical CO2 flow. <em>ASOC</em>,
<em>102</em>, 107110. (<a
href="https://doi.org/10.1016/j.asoc.2021.107110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supercritical CO 2 flowing in tubes has been researched intensively to confirm their potential applications in energy conversion systems. However its performance prediction is always damaged by the non-linear variable thermo-physical properties and conventional correlation methods. To this context, this paper considers GA-BP model performed in MATLAB software to improve prediction accuracy. Firstly, three heat-transfer regimes are defined and parametrical effects are evaluated. Then, an architecture of 2-5-5-1 network is well-trained for accurately redistributed reconstruction of non-linear density of sCO 2 and achieves a reasonable root-mean-square error of 1.112 kg/m 3 and a regression coefficient of 0.99. Finally, using 5895 sets of reliable experimental data with a verified architecture of 5-150-1 network makes the ANN model more adaptive and precise in interval predictions of heat-transfer behaviors, with a mean-absolute-percent error and as root-mean-square error far less than 2.97\% and 3.11 °C, respectively. Results also suggest that the ANN model is technically superior to those correlations by I. Pioro, T. Preda, H. Kim, J.D. Jackson and Bringer-Smith under established test 1–5 groups. This study can provide a methodological guidance and shed a new insight for the further prediction of heat-transfer problems with supercritical fluids .},
  archive      = {J_ASOC},
  author       = {Feng Sun and Gongnan Xie and Shulei Li},
  doi          = {10.1016/j.asoc.2021.107110},
  journal      = {Applied Soft Computing},
  pages        = {107110},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An artificial-neural-network based prediction of heat transfer behaviors for in-tube supercritical CO2 flow},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven preference learning in multiple criteria
decision making in the evidential reasoning context. <em>ASOC</em>,
<em>102</em>, 107109. (<a
href="https://doi.org/10.1016/j.asoc.2021.107109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some situations, such as the diagnosis of thyroid nodules, a decision maker considers observations on multiple criteria to provide the overall assessments and advice on what will be done in the next step. To guarantee the quality of the assessments and advice and their consistency with observations, this paper proposes a method of learning the preferences of the decision maker from the observations on multiple criteria and the overall assessments provided. The constraints on preferences are learned first to avoid extreme and irrational preferences. Within the feasible region formed by the constraints, the preferences are learned. When gold standards, which can be used to judge the correctness of the overall assessments, are available, the issue of how to learn the constraints and the preferences that satisfy the constraints is presented. With and without the consideration of gold standards, the way in which solutions can be generated using the learned preferences is introduced. To demonstrate the process of preference learning based on observations and overall assessments, a case study is conducted using the examination reports generated by three radiologists from 2013 to 2017 in a hospital located in Hefei, Anhui, China.},
  archive      = {J_ASOC},
  author       = {Chao Fu and Min Xue and Weiyong Liu and Dongling Xu and Jianbo Yang},
  doi          = {10.1016/j.asoc.2021.107109},
  journal      = {Applied Soft Computing},
  pages        = {107109},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven preference learning in multiple criteria decision making in the evidential reasoning context},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interval type-2 fuzzy ARAS method for recycling facility
location problems. <em>ASOC</em>, <em>102</em>, 107107. (<a
href="https://doi.org/10.1016/j.asoc.2021.107107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The management of end-of-life vehicles (ELVs) is currently one of the most important ecological topics. The recycling process has essential importance for the environmental and economic sustainability of the ELV management. Istanbul has the highest rate of car ownership population in Turkey as well as an old vehicle fleet. There is a strong motivation to open an additional ELV recycling facility in this mega-city. Facility location is one of the crucial strategic problems for decision-makers. Addressing multi-criteria and highly uncertain nature of the ELV recycling facility location problem, this paper introduces a novel approach to support the facility location process. For the first time, an extension of the Additive ratio assessment (ARAS) method under the interval type-2 fuzzy environment is presented. The novel method is utilized for solving the ELV recycling facility location problem. The potentials and applicability of the presented interval type-2 fuzzy ARAS method are demonstrated throughout the real-life case study of Istanbul. The comparison with the available state-of-the-art interval type-2 fuzzy set based MCDM methods approves its validity and consistency.},
  archive      = {J_ASOC},
  author       = {Selman Karagöz and Muhammet Deveci and Vladimir Simic and Nezir Aydin},
  doi          = {10.1016/j.asoc.2021.107107},
  journal      = {Applied Soft Computing},
  pages        = {107107},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval type-2 fuzzy ARAS method for recycling facility location problems},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pricing of satellite image data products: Neutrosophic fuzzy
pricing approaches under different game scenarios. <em>ASOC</em>,
<em>102</em>, 107106. (<a
href="https://doi.org/10.1016/j.asoc.2021.107106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the gradual commercialization of satellite image data products, their pricing decisions become an important link for providers to achieve profits. This study aims to investigate the pricing problems of data products in a neutrosophic fuzzy environment. A main contribution is that two game scenarios, including the Bertrand and Stackelberg game scenarios, are discussed by considering the specific features of satellite image data products (SIDPs) market. Two main SIDPs providers are regarded as two players in the game. The innovation is that the concept of neutrosophic fuzzy variables is presented to fully express the uncertainty existing in real-world problems. As a result, the truth, indeterminacy and falsity degrees of decision makers for some problem parameters can determined in an intuitive and convenient fashion. Additionally, the expected value of neutrosophic variables is defined to facilitate the establishment of pricing models. After some equilibrium formulas are derived, a numerical example is provided to obtain the optimal pricing and expected profits. The effects of different power structures of providers are also analyzed and some interesting conclusions are made. Last, the advantages of our methods are demonstrated through comparative analyses. Furthermore, sensitivity analyses are conducted to identify the influence of important problem parameters and weight coefficients on final pricing decisions. The results demonstrate that the proposed approaches are feasible and can provide guidelines for the pricing of satellite image data products.},
  archive      = {J_ASOC},
  author       = {Suizhi Luo and Witold Pedrycz and Lining Xing},
  doi          = {10.1016/j.asoc.2021.107106},
  journal      = {Applied Soft Computing},
  pages        = {107106},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pricing of satellite image data products: Neutrosophic fuzzy pricing approaches under different game scenarios},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving a novel designed second order nonlinear lane–emden
delay differential model using the heuristic techniques. <em>ASOC</em>,
<em>102</em>, 107105. (<a
href="https://doi.org/10.1016/j.asoc.2021.107105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of the present study is to present a new model based on the nonlinear singular second order delay differential equation of Lane–Emden type and numerically solved by using the heuristic technique . Four different examples are presented based on the designed model and numerically solved by using artificial neural networks optimized by the global search, local search methods and their hybrid combinations, respectively, named as genetic algorithm (GA), sequential quadratic programming (SQP) and GA-SQP. The numerical results of the designed model are compared for the proposed heuristic technique with the exact/explicit results that demonstrate the performance and correctness. Moreover, statistical investigations/assessments are presented for the accuracy and performance of the designed model implemented with heuristic methodology.},
  archive      = {J_ASOC},
  author       = {Zulqurnain Sabir and Juan L.G. Guirao and Tareq Saeed},
  doi          = {10.1016/j.asoc.2021.107105},
  journal      = {Applied Soft Computing},
  pages        = {107105},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving a novel designed second order nonlinear Lane–Emden delay differential model using the heuristic techniques},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequence encoding incorporated CNN model for email document
sentiment classification. <em>ASOC</em>, <em>102</em>, 107104. (<a
href="https://doi.org/10.1016/j.asoc.2021.107104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document sentiment classification is an area of study that has been developed for decades. However, sentiment classification of Email data is rather a specialized field that has not yet been thoroughly studied. Compared to typical social media and review data, Email data has characteristics of length variance, duplication caused by reply and forward messages, and implicitness in sentiment indicators. Due to these characteristics, existing techniques are incapable of fully capturing the complex syntactic and relational structure among words and phrases in Email documents. In this study, we introduce a dependency graph-based position encoding technique enhanced with weighted sentiment features, and incorporate it into the feature representation process. We combine encoded sentiment sequence features with traditional word embedding features as input for a revised deep CNN model for Email sentiment classification. Experiments are conducted on three sets of real Email data with adequate label conversion processes. Empirical results indicate that our proposed SSE-CNN model obtained the highest accuracy rate of 88.6\%, 74.3\% and 82.1\% for three experimental Email datasets over other comparative state-of-the-art algorithms. Furthermore, our performance evaluations on the preprocessing and sentiment sequence encoding justify the effectiveness of Email preprocessing and sentiment sequence encoding with dependency-graph based position and SWN features on the improvement of Email document sentiment classification.},
  archive      = {J_ASOC},
  author       = {Sisi Liu and Ickjai Lee},
  doi          = {10.1016/j.asoc.2021.107104},
  journal      = {Applied Soft Computing},
  pages        = {107104},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sequence encoding incorporated CNN model for email document sentiment classification},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Review fuzzy multi-criteria decision-making in construction
management using a network approach. <em>ASOC</em>, <em>102</em>,
107103. (<a href="https://doi.org/10.1016/j.asoc.2021.107103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With numerous and ambiguous information and often-conflicting requirements, construction management (CM) is regarded as a complex process with uncertainty. Fuzzy multi-criteria decision making (FMCDM) is thus becoming popular as an efficient approach to addressing complex problems with diverse decision-makers’ interests, conflicting objectives, and numerous but uncertain information. Previous studies mainly focused on the proposition and application of FMCDM methods in construction management. Although there have been some works to review FMCDM methods and applications, they mainly relied on bibliometrics and taxonomy to summarise and present critiques of existing literature without digesting the relationships between fuzzy sets (FSs), MCDM and associated applications. This paper aims to comprehensively review and analyse the literature of FMCDM in CM from 2007 to 2017 using a network approach for both summarising the development of FMCDM in CM and providing insights into the relationships between FSs, MCDM and associated applications. A total of 165 published journal articles were first chosen, involving 37 single-hybrid and 17 multiple-hybrid FMCDM methods. Current practices of these methods are introduced and discussed, disclosing their characteristics, strengths and limitations. Network meta-analysis is then performed based on the literature to explore the correlations between FSs, MCDM methods and construction applications for establishing a FSs-MCDM-CM network. Finally, based on such a network, the current practice of FMCDM in CM is summarised and discussed. It also suggests a two-step way to select appropriate FMCDM methods for addressing CM problems and recommends new directions for future research and applications. The results will inform and guide both practitioners and researchers in FMCDM methods selection and exploration in CM. This research contributes to the body of knowledge through providing a new vision to review, analyse and critique the FMCDM methods and application in CM from the network perspective.},
  archive      = {J_ASOC},
  author       = {Long Chen and Wei Pan},
  doi          = {10.1016/j.asoc.2021.107103},
  journal      = {Applied Soft Computing},
  pages        = {107103},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Review fuzzy multi-criteria decision-making in construction management using a network approach},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Att-net: Enhanced emotion recognition system using
lightweight self-attention module. <em>ASOC</em>, <em>102</em>, 107101.
(<a href="https://doi.org/10.1016/j.asoc.2021.107101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) is an active research field of digital signal processing and plays a crucial role in numerous applications of Human–computer interaction (HCI). Nowadays, the baseline state of the art systems has quite a low accuracy and high computations, which needs upgrading to make it reasonable for real-time industrial uses such as detection of content from speech data. The main intent for low recognition rate and high computational cost is a scarceness of datasets, model configuration , and patterns recognition that is the supreme stimulating work for building a robust SER system. In this study, we address these problems and propose a simple and lightweight deep learning-based self-attention module (SAM) for SER system. The transitional features map is given to SAM, which produces efficiently the channel and spatial axes attention map with insignificant overheads. We use a multi-layer perceptron (MLP) in channel attention to extracting global cues and a special dilated convolutional neural network (CNN) in spatial attention to extract spatial info from input tensor. Moreover, we merge, spatial and channel attention maps to produce a combine attention weights as a self-attention module. We placed SAM in the middle of convolutional and connected layers and trained it in an end-to-end mode. The ablation study and comprehensive experimentations are accompanied over IEMOCAP, RAVDESS, and EMO-DB speech emotion datasets. The proposed SER system shows consistent improvements in overall experiments for all datasets and shows 78.01\%, 80.00\%, and 93.00\% average recall, respectively.},
  archive      = {J_ASOC},
  author       = {Mustaqeem and Soonil Kwon},
  doi          = {10.1016/j.asoc.2021.107101},
  journal      = {Applied Soft Computing},
  pages        = {107101},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Att-net: Enhanced emotion recognition system using lightweight self-attention module},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization strategies of neural networks for impact damage
classification of RC panels in a small dataset. <em>ASOC</em>,
<em>102</em>, 107100. (<a
href="https://doi.org/10.1016/j.asoc.2021.107100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting damage modes of reinforced concrete (RC) panels subjected to impact loading is a difficult task and often involves considerable effort in doing experiments or simulations. The development of missiles in terms of strength and destructive power requires an accurate estimation of future damage levels. In addition, data collected from the experiment are often small. Therefore, this study aims to build an artificial neural network (ANN) model to classify the damage modes of RC panel under impact loading and enhance its performance by optimizing the model’s hyperparameters when learning a small dataset (254 observes for four classes in this study). To address this a novel optimization strategy was proposed and two metaheuristic optimization algorithms , i.e., genetic algorithm (GA) and particle swarm optimization (PSO) were presented for automatic selection of hyperparameters to increase the accuracy of the ANN model. The proposed optimization strategy was developed based on the incorporation of a stepwise gridsearch (SG) method into a nested cross-validation (NCV) process to find the optimal parameters for the ANN model, named SG-NCV-ANN. The efficiency of the proposed SG-NCV-ANN model and two hybrid models including ANN-GA, ANN-PSO are evaluated by comparing to each other and other machine-learning-based classification methods including ANN using a randomized cross-validation search (RCV-ANN), oblique random forest (oRF), support vector machine (SVM) and k -nearest neighbors ( k -NN). Accuracy, micro f1 score, receiver operating characteristic (ROC) curve, and area under the ROC curve (AUC) were employed to thoroughly assess the obtained results from the model. The experimental results indicated that the ANN-GA model achieves the highest AUC and f1 score compared to other state-of-the-art methods, following by the ANN-PSO model. While the proposed SG-NCV-ANN model obtained the best generalization performance on the present small dataset.},
  archive      = {J_ASOC},
  author       = {Quoc Hoan Doan and Tuong Le and Duc-Kien Thai},
  doi          = {10.1016/j.asoc.2021.107100},
  journal      = {Applied Soft Computing},
  pages        = {107100},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization strategies of neural networks for impact damage classification of RC panels in a small dataset},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On robust asymmetric lagrangian ν-twin support vector
regression using pinball loss function. <em>ASOC</em>, <em>102</em>,
107099. (<a href="https://doi.org/10.1016/j.asoc.2021.107099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of twin support vector regression (TSVR) is to find the optimum regression function based on the ε ε -insensitive up- and down-bound with equal influences on the regression function where all the data points have a different location above the up-bound points and below the down-bound points. However, the effects of all data points must be distinct based on their distribution in the regression function. Recently, asymmetric ν ν -twin support vector regression (Asy- ν ν -TSVR) is encouraged on the same subject but still, the present matrices in the mathematical formulation have faced the problem of semi-definite. In order to handle this problem effectively, a new regressor model named as robust asymmetric Lagrangian ν ν -twin support vector regression using pinball loss function (URALTSVR) proposes as a pair of the unconstrained minimization problem to handle not only the noise sensitivity and instability of re-sampling but also consist positive definite matrices . Here, we suggest the proposed model URALTSVR in such a way where the pinball loss function is playing a vital role to control the fitting error inside the asymmetric tube. One of the advantages is that unlike TSVR and Asy- ν ν -TSVR, it considers the concept of structural risk minimization principle through the inclusion of regularization term as well as change the one-norm of the vector of the slack variable by two-norm, which yields the dual problem to be strongly convex, stable and well-posed. Aforementioned, the proposed formulation has a continuous and piecewise quadratic problem that is solved by their gradients based iterative approaches. Specifically, we analyze the three implementations of URALTSVR with the baselines approaches support vector regression (SVR), TSVR and Asy- ν ν -TSVR, which discard the dependencies to solve a pair of quadratic programming problem (QPP) for obtaining the unique global solution. Overall, SRALTSVR1 based on smooth approximation function performs outstanding for artificial and real-world datasets.},
  archive      = {J_ASOC},
  author       = {Deepak Gupta and Umesh Gupta},
  doi          = {10.1016/j.asoc.2021.107099},
  journal      = {Applied Soft Computing},
  pages        = {107099},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On robust asymmetric lagrangian ν-twin support vector regression using pinball loss function},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-criteria decision-making framework for large-scale
rooftop photovoltaic project site selection based on intuitionistic
fuzzy sets. <em>ASOC</em>, <em>102</em>, 107098. (<a
href="https://doi.org/10.1016/j.asoc.2021.107098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Site selection plays a critical role in the success of large-scale rooftop photovoltaic (LSRPV) projects. The complexity associated with the site selection of LSRPV projects comes from the uncertain and heterogeneous decision information, subjective preferences of decision-makers, differences among experts’ opinions and interactions between criteria. However, previous research works failed to solve these issues thoroughly. To fill such a gap, a fuzzy multi-criteria decision-making framework is established involving the intuitionistic fuzzy sets, score function, linear weighting method, prospect theory and analytical network process. Thereafter, to validate the rationality and effectiveness of this framework, a case study is conducted in some cities of China with comparative and sensitivity analysis; the ranking results demonstrate that the site located in the city Hefei is the best option, followed by Taizhou, Nanchang, Hangzhou, Dongguan, Ningbo, Jinan, Foshan, Guangzhou and Lishui. This study can effectively improve the site selection decision-making quality and efficiency, and provide a reference for investors to carry out reasonable managerial measures.},
  archive      = {J_ASOC},
  author       = {Jianwei Gao and Fengjia Guo and Zeyang Ma and Xin Huang},
  doi          = {10.1016/j.asoc.2021.107098},
  journal      = {Applied Soft Computing},
  pages        = {107098},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria decision-making framework for large-scale rooftop photovoltaic project site selection based on intuitionistic fuzzy sets},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary multi-objective set cover problem for task
allocation in the internet of things. <em>ASOC</em>, <em>102</em>,
107097. (<a href="https://doi.org/10.1016/j.asoc.2021.107097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient distribution of tasks in an Internet of Things (IoT) network ensures the fulfillment for all objects to dynamically cooperate with their limited energy, processing and memory capabilities. The main contribution of this paper is threefold. Firstly, we address the task allocation in the IoT as an optimization problem with a new formulation derived from the context of set cover problem. To the best of our knowledge, no such study has been considered in the literature. Secondly, we extend the set cover problem to further express the conflict that meets with both operational period and stability. Thirdly, an evolutionary single objective and multi-objective algorithms are developed to tackle the formulated problem. Two heuristic operators are also introduced and injected within the framework of the evolutionary algorithms where the need arises to harness their strength in terms of both operational period and network stability. Performance evaluation is reported while different problem dimensions are experimented with in the simulations. The results show that the proposed multi-objective evolutionary algorithm is quite appropriate to converge to more accurate solutions than the counterpart single objective evolutionary algorithm. Further, the results give plausible evidence supporting the importance of the proposed heuristic operators to mitigate against the contradictory nature of the network lifetime in terms of operational period and stability.},
  archive      = {J_ASOC},
  author       = {Hussein M. Burhan and Bara’a A. Attea and Amenah D. Abbood and Mustafa N. Abbas and Mayyadah Al-Ani},
  doi          = {10.1016/j.asoc.2021.107097},
  journal      = {Applied Soft Computing},
  pages        = {107097},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary multi-objective set cover problem for task allocation in the internet of things},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A network security situation assessment method based on
adversarial deep learning. <em>ASOC</em>, <em>102</em>, 107096. (<a
href="https://doi.org/10.1016/j.asoc.2021.107096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the poor performance and flexibility of traditional assessment methods of network security in dealing with a large number of network attack data, this paper proposes a network security situation assessment method based on adversarial deep learning . Firstly, establish the Deep Autoencoder-Deep Neural Network (AEDNN) model based on Deep Autoencoder (DAE) and Deep Neural Network (DNN). Conduct feature learning on the DAE network and apply the DNN network as a network attacks classifier. In the training process, for considering the results of feature learning in DAE, this paper builds an adversarial training process by changing the training weights. Besides, to increase the performance of the model on the minority network attacks, the Under–Over Sampling Weighted (UOSW) algorithm is designed; Finally, conduct model testing and calculate the network security situation value. The compared results of other models show the proposed model is more accurate for identifying network attacks and can evaluate the network situation more comprehensively and flexibly.},
  archive      = {J_ASOC},
  author       = {Hongyu Yang and Renyun Zeng and Guangquan Xu and Liang Zhang},
  doi          = {10.1016/j.asoc.2021.107096},
  journal      = {Applied Soft Computing},
  pages        = {107096},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A network security situation assessment method based on adversarial deep learning},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A secure distributed machine learning protocol against
static semi-honest adversaries. <em>ASOC</em>, <em>102</em>, 107095. (<a
href="https://doi.org/10.1016/j.asoc.2021.107095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been successfully applied to various fields over the last few years. However, it still faces two critical challenges. On one hand, the concern for the security issue in machine learning is increased. On the other hand, data exists in the form of isolated islands across different organizations. In this work, we focus on the privacy-preserving issue on a non-parametric machine learning algorithmize, i.e., the k Nearest Neighbor Classification ( k NNC) in which, training data are split vertically among multiple servers. We propose a novel protocol that is secure against static semi-honest adversaries. In specific, the clients can obtain the label of his/her query without disclosing the servers’ data, the client’s query, and the client’s output to others. We use the state-of-the-art lattice-based fully homomorphic encryption to realize the privacy-preserving distance computation. In order to protect data access patterns, permutation technique and Oblivious Transfer are used in the top- k selection phase. We proved the security via the simulation paradigm. Meanwhile, we implemented our protocol and performed extensive experiments. Results show that our protocol performs well, especially in a large-width environment. Compared to the existing solution, our protocol leaks no information about the participants’ private input and output in both centralized and distributed architectures. Meanwhile, our protocol runs faster than existing solutions.},
  archive      = {J_ASOC},
  author       = {Maohua Sun and Ruidi Yang and Lei Hu},
  doi          = {10.1016/j.asoc.2021.107095},
  journal      = {Applied Soft Computing},
  pages        = {107095},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A secure distributed machine learning protocol against static semi-honest adversaries},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thermally-induced error compensation of spindle system based
on long short term memory neural networks. <em>ASOC</em>, <em>102</em>,
107094. (<a href="https://doi.org/10.1016/j.asoc.2021.107094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal error has become a key reason hindering machine tool’s thermal stability improvement. The error compensation is carried out from the view of error mechanism of spindle systems to increase the thermal stability of machine tools. The hysteresis effect of the thermal expansion is revealed with theoretical modeling of error mechanism, and long-term memory characteristics of thermal error on historical thermal information are demonstrated. Then the applicability of long short term memory (LSTM) neural networks for the training of the error model is proved. The variational mode decomposition (VMD) decomposes error data into several inherent modal function (IMF) components to remove the coupling effect of high- and low-frequency data, improving the robustness and generalization capability of the error model. Moreover, the hyper-parameters of LSTM neural networks are optimized with grey wolf (GW) algorithms to remove the sensitivity of the predictive performance to its hyper-parameters. Finally, error models are trained with VMD-GW-LSTM neural networks, VMD-LSTM neural networks, and recurrent neural networks (RNNs). To verify the effectiveness of compensation methods, the error compensation and machining were performed at different working conditions. The results show that compensation rates of the VMD-GW-LSTM network model are 77.78\%, 75.00\%, and 77.78\% for Sizes 1, 2, and 3, respectively. Moreover, the predictive performance and compensation performance of the VMD-GW-LSTM network model is far better than that of VMD-LSTM network and RNN models .},
  archive      = {J_ASOC},
  author       = {Jialan Liu and Chi Ma and Hongquan Gui and Shilong Wang},
  doi          = {10.1016/j.asoc.2021.107094},
  journal      = {Applied Soft Computing},
  pages        = {107094},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Thermally-induced error compensation of spindle system based on long short term memory neural networks},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soil micromorphological image classification using deep
learning: The porosity parameter. <em>ASOC</em>, <em>102</em>, 107093.
(<a href="https://doi.org/10.1016/j.asoc.2021.107093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying components and microstructures in soil and sediment thin sections is one of the many subjects of analysis in archeological research, as these features can provide information regarding the deposit from which they were extracted, such as its origin and nature, clues about their associated human contexts or alteration processes they might have undergone over time. This article presents a Deep Learning system based on Convolutional Neural Networks (CNN) to classify different porosity types of structures in photomicrographs from archeological soils and sediment thin sections, as a first step to build and expand a database that will boost research in this field of archeological research. The results obtained are encouraging and show that the presented models can be successfully applied to this classification task . The trained models have been used to estimate the quantity of the different microstructures in test images, obtaining a median error of around 2\%.},
  archive      = {J_ASOC},
  author       = {Rafael Arnay and Javier Hernández-Aceituno and Carolina Mallol},
  doi          = {10.1016/j.asoc.2021.107093},
  journal      = {Applied Soft Computing},
  pages        = {107093},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soil micromorphological image classification using deep learning: The porosity parameter},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-effective fault diagnosis of a multi-component dynamic
system under corrective maintenance. <em>ASOC</em>, <em>102</em>,
107092. (<a href="https://doi.org/10.1016/j.asoc.2021.107092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintenance planning and execution are challenging tasks for every system with complex structure. Interdependent nature of the components that builds up the system may have significant effect on system integrity. While preventive maintenance actions can be carried out in a more planned fashion, corrective actions are more time sensitive as they directly affect the availability of the system. This study proposes a cost-effective dynamic Bayesian network modeling scheme to be used in the planning of corrective maintenance actions on systems having hidden components which have stochastic and structural dependencies. In such context, the regenerative air heater system which is a key element of a power plant is taken into consideration. The proposed maintenance framework offers several methods, each aiming to balance the cost with the probability effect using a normalization procedure. The methodologies are extensively simulated for sensitivity analysis under various downtime cost values. Fault effect methods with worst state probability efficiency measures give the least total cost for all downtime cost values and their distinction becomes significant as this value increases. Further statistical analysis concludes that considerable gains on maintenance costs can be achieved by the proposed approach.},
  archive      = {J_ASOC},
  author       = {Demet Özgür-Ünlüakın and Busenur Türkali and S. Çağlar Aksezer},
  doi          = {10.1016/j.asoc.2021.107092},
  journal      = {Applied Soft Computing},
  pages        = {107092},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cost-effective fault diagnosis of a multi-component dynamic system under corrective maintenance},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing the artificial neural network parameters using a
biased random key genetic algorithm for time series forecasting.
<em>ASOC</em>, <em>102</em>, 107091. (<a
href="https://doi.org/10.1016/j.asoc.2021.107091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Neural Networks (ANN) is one of the most used methods in time series forecasting. Mostly, it is hard to determine the design and weight parameters of ANNs by experience. For this reason, ANN optimization is considered to find the best network design and parameter sets. In this study, a novel hybrid algorithm based on biased random key genetic algorithms is proposed for ANN optimization. The algorithm, which is named as BRKGA–NN, determines the number of hidden neurons , bias values of hidden neurons and the connection weights between nodes. To test the performance of the BRKGA–NN, the algorithm is compared with genetic algorithm based ANN, ANN with back-propagation, Support Vector Regression and Autoregressive Integrated Moving Average on some of the most known time series datasets. According to the results of forecasts, BRKGA–NN algorithm can produce better forecasts than the compared methods. In addition to the time series datasets, the results and comparisons of the forecasts with the proposed algorithm using real-life data are also presented.},
  archive      = {J_ASOC},
  author       = {Zeynep Idil Erzurum Cicek and Zehra Kamisli Ozturk},
  doi          = {10.1016/j.asoc.2021.107091},
  journal      = {Applied Soft Computing},
  pages        = {107091},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing the artificial neural network parameters using a biased random key genetic algorithm for time series forecasting},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Agile trends in chinese global software development
industry: Fuzzy AHP based conceptual mapping. <em>ASOC</em>,
<em>102</em>, 107090. (<a
href="https://doi.org/10.1016/j.asoc.2021.107090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global Software Development (GSD) has gained great attention during the past decade or so, and it is also being adopted by most of the software development organizations. The GSD approach is practiced by the software development organizations in China and a wide range of firms have established its operations in various Chinese cities. However, GSD practices becomes more challenging for the projects based on the agile concepts. The aim of this study is to develop a taxonomy of the factors that could positively impact the scaling process of agile methods in the Chinese GSD industry. The factors are identified by exploring the available literature and conducting industrial study with the Chinese agile and GSD practitioners. The finally reported factors are categorized, prioritized and developed their taxonomy using the multi-criterion decision making (MCDM) Fuzzy AHP approach. The given taxonomy is significant and progressive for GSD industry to assess and improve the scaling process of agile methods.},
  archive      = {J_ASOC},
  author       = {Arif Ali Khan and Mohammad Shameem and Mohammad Nadeem and Muhammad Azeem Akbar},
  doi          = {10.1016/j.asoc.2021.107090},
  journal      = {Applied Soft Computing},
  pages        = {107090},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Agile trends in chinese global software development industry: Fuzzy AHP based conceptual mapping},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining the features of spatial adjacency relationships to
improve the classification of high resolution remote sensing images
based on complex network. <em>ASOC</em>, <em>102</em>, 107089. (<a
href="https://doi.org/10.1016/j.asoc.2021.107089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of spatial and structural patterns of remote sensing images, land cover classification is still an active, ongoing problem in the field of image classification. The limited spectrum and texture features can be used often lead to low accuracy in remote sensing image classifications. Spatial structure information, which is commonly regarded as very useful information for improving land cover classifications, characterizes a composite of several types of objects or land covers. However, it is rarely used because it is difficult to describe and calculate. Generally, images contain the relationships of spatial adjacency, spatial direction and spatial distance among multiple objects. This study selects spatial adjacency relationships as an example, mines the features of spatial adjacency, and uses to improve classification. Therefore, this paper (1) presents a modeling method of remote sensing images based on complex networks to express the spatial structure information and make it computable firstly; (2) gives network expression and calculation methods of spectrum, texture and object features in images; (3) discusses a discovery method of spatial adjacency feature based on network. Finally, this paper gives a strategy to improve the classification for high-resolution image by using the spatial adjacency features. Two kinds of data in coastal zones and city areas are used to verify the method. The results show that the proposed approach, which considers the features of spatial adjacencies, improves the classification accuracy by at least 4\% compared with the approaches that do not consider spatial adjacencies. Comparing with the results of deep learning method, our method has higher overall accuracy when the numbers of samples are small.},
  archive      = {J_ASOC},
  author       = {Changying Wang and Fengjing Shao and Zhimei Zhang and Yi Sui and Shujing Li},
  doi          = {10.1016/j.asoc.2021.107089},
  journal      = {Applied Soft Computing},
  pages        = {107089},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mining the features of spatial adjacency relationships to improve the classification of high resolution remote sensing images based on complex network},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A combination of FA and SRPSO algorithm for combined heat
and power economic dispatch. <em>ASOC</em>, <em>102</em>, 107088. (<a
href="https://doi.org/10.1016/j.asoc.2021.107088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper represents a hybrid Firefly and Self-Regulating Particle Swarm Optimization (FSRPSO) algorithm to solve optimal Combined Heat and Power Economic Dispatch (CHPED) problem. Valve point effect on fuel cost function of pure generation units, electrical power losses in transmission systems and feasible operating zones are taken into account in the CHPED problem. The CHPED refers to minimize total costs of fuel for electricity and heat generation supply to load demand. The proposed FSRPSO attempts to determine the start of the local search process properly by checking the previous global best. Thus, the FSRPSO is able to exploit strong points of both Firefly Algorithm (FA) and SRPSO mechanisms in order to balance between exploration and exploitation phases. Besides, for the sake of validation the proposed hybrid method , the FSRPSO is examined on 21 well-known benchmarks, and also a real engineering case i.e., two power systems for evaluating its performance compared with the SRPSO, FA, PSO, and other state-of-the-art algorithms. The obtained optimization results show that the proposed FSRPSO provides fast, mature and reliable optimum solutions and outperform other compared algorithms in diverse categories of benchmarks along with the studied CHPED problem.},
  archive      = {J_ASOC},
  author       = {Mohammad Nasir and Ali Sadollah and İbrahim Berkan Aydilek and Afshin Lashkar Ara and Seyed Ali Nabavi-Niaki},
  doi          = {10.1016/j.asoc.2021.107088},
  journal      = {Applied Soft Computing},
  pages        = {107088},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combination of FA and SRPSO algorithm for combined heat and power economic dispatch},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical exploratory analysis of mask-fill reproduction
operators of genetic algorithms. <em>ASOC</em>, <em>102</em>, 107087.
(<a href="https://doi.org/10.1016/j.asoc.2021.107087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To be successful, a search algorithm needs to balance exploration and exploitation. In Genetic Algorithms (GAs) this is achieved through proportionate selection of individuals and reproduction operators. GAs can suffer premature convergence, when the diversity of the population decreases over time and search gets trapped in a local optimum, returning a very poor quality solution. Mask-fill reproduction operators utilizes bit-masking oriented data structure (BMODS) that maintains a good ratio between exploration and exploitation. Mask-fill reproduction operators have been utilized in various applications, but the exploration and exploitation ability of mask-fill reproduction operators have not been compared against other reproduction operators. This paper describes a rigorous and practical statistical methodology for the exploratory analysis of the mask-fill reproduction operators. First, the issues of robust experimental design and setting the control parameters for implementing a GA is addressed. Second, the impact of various reproduction operator combinations are analysed. In this study, three crossover operators and five mutation operators are considered which creates fifteen crossover–mutation operator combinations. Third, the methodology is demonstrated by considering grammatical inference (GI) problem as domain of enquiry. A hybrid genetic algorithm integrated with Sequitur algorithm (GAWS) is proposed for GI. Numerical results are presented to describe the effect of crossover–mutation operator combinations. The performance of the proposed GAWS is compared against the state-of-the-art algorithms. Statistical test are conducted to determine the performance significance of both crossover–mutation operator combinations and the proposed GAWS algorithm.},
  archive      = {J_ASOC},
  author       = {Hari Mohan Pandey and Marcello Trovati and Nik Bessis},
  doi          = {10.1016/j.asoc.2021.107087},
  journal      = {Applied Soft Computing},
  pages        = {107087},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Statistical exploratory analysis of mask-fill reproduction operators of genetic algorithms},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel evolutionary approach for learning syntactic
features for cross domain opinion target extraction. <em>ASOC</em>,
<em>102</em>, 107086. (<a
href="https://doi.org/10.1016/j.asoc.2021.107086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-Based Sentiment Analysis (ABSA) is the problem of mining textual user reviews to gauge the orientation of a user towards the various characteristics of a product. Identifying words in the review towards which the user holds a sentiment, known as Aspect Target Extraction (ATE) or Opinion Target Extraction (OTE), is a crucial step in ABSA. Several start-of-the-art techniques, mostly employing Deep Learning methods, have been proposed to tackle this problem. However, when attempting ATE in a new domain, most of the supervised methods are crippled by a lack of labeled data within the domain and do not perform well with models built on other unrelated domains. Other unsupervised methods based on linguistic rules framed by experts have been proposed to mitigate the problem of the lack of data but this requires manual engineering of rules, which may not capture all aspect terms. Different from the previous approaches, we propose evolutionary methods to generate pattern graphs, which encode rules to identify cross-domain aspect terms. The proposed methods aim to mine linguistic patterns, which are quality indicators of the presence of aspect words. The automated pattern learning technique using the evolutionary approach enables automated learning of rules. This approach eliminates the need for human experts to coin the same, while at the same time enabling the production of complicated rules, which can identify cross-domain aspects effectively. Two slightly different approaches are proposed — the first based on identifying opinion targets by matching the learned pattern graphs against existing sentences and the second based on using the learned pattern graphs to construct features for building classification models for aspect identification. We conduct extensive experiments using 10 real-life datasets of varying sizes and different characteristics, and demonstrate the superiority of the proposed methods for cross-domain ATE. We find that the proposed approaches perform better as compared to the existing methods in 69 of the 90 cases, or in other words, 76\% of the cases.},
  archive      = {J_ASOC},
  author       = {Deepa Anand and Bonson Sebastian Mampilli},
  doi          = {10.1016/j.asoc.2021.107086},
  journal      = {Applied Soft Computing},
  pages        = {107086},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel evolutionary approach for learning syntactic features for cross domain opinion target extraction},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discrete shuffled frog-leaping algorithm based on
heuristic information for traveling salesman problem. <em>ASOC</em>,
<em>102</em>, 107085. (<a
href="https://doi.org/10.1016/j.asoc.2021.107085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A discrete shuffled frog-leaping algorithm based on heuristic information is proposed and the traveling salesman problem is used as a test problem. First, a new individual generation operator utilizing the nearest neighbor information is designed. Then, four improved searching strategies are designed to improve the algorithm performance: (1) the opposite roulette selection that makes the candidate solutions participate in the evolution alternately and keeps the population diversity; (2) establishment of an independent elite set which helps to strengthen the exploring ability, balance the searching ability among subgroups and reduce the population assimilation speed; (3) mutation of the local optimal solution in each subgroup which is beneficial to jump out of the local extremum in the search space; and (4) enhancement of the local search on some best solutions in the shuffled population which aims to strengthen the exploiting ability and improve the searching precision. These defined operators make a good balance between exploration and exploitation. Finally, a large number of TSP instances in the open data set TSPLIB are selected to validate the effectiveness of the new individual generation operator and four improved strategies. Comparison results with three classical and six state-of-the-art algorithms indicate that the proposed algorithm can obtain solutions with a higher accuracy and show a better stability.},
  archive      = {J_ASOC},
  author       = {Yao Huang and Xiao-Ning Shen and Xuan You},
  doi          = {10.1016/j.asoc.2021.107085},
  journal      = {Applied Soft Computing},
  pages        = {107085},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete shuffled frog-leaping algorithm based on heuristic information for traveling salesman problem},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guided parallelized stochastic gradient descent for delay
compensation. <em>ASOC</em>, <em>102</em>, 107084. (<a
href="https://doi.org/10.1016/j.asoc.2021.107084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) algorithm and its variations have been effectively used to optimize neural network models . However, with the rapid growth of big data and deep learning , SGD is no longer the most suitable choice due to its natural behavior of sequential optimization of the error function. This has led to the development of parallel SGD algorithms, such as asynchronous SGD (ASGD) and synchronous SGD (SSGD) to train deep neural networks . However, it introduces a high variance due to the delay in parameter (weight) update. We address this delay in our proposed algorithm and try to minimize its impact. We employed guided SGD (gSGD) that encourages consistent examples to steer the convergence by compensating the unpredictable deviation caused by the delay. Its convergence rate is also similar to A/SSGD, however, some additional (parallel) processing is required to compensate for the delay. The experimental results demonstrate that our proposed approach has been able to mitigate the impact of delay for the quality of classification accuracy . The guided approach with SSGD clearly outperforms sequential SGD and even achieves an accuracy close to sequential SGD for some benchmark datasets.},
  archive      = {J_ASOC},
  author       = {Anuraganand Sharma},
  doi          = {10.1016/j.asoc.2021.107084},
  journal      = {Applied Soft Computing},
  pages        = {107084},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Guided parallelized stochastic gradient descent for delay compensation},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Short-term daily precipitation forecasting with
seasonally-integrated autoencoder. <em>ASOC</em>, <em>102</em>, 107083.
(<a href="https://doi.org/10.1016/j.asoc.2021.107083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term precipitation forecasting is essential for planning of human activities in multiple scales, ranging from individuals’ planning, urban management to flood prevention. Yet the short-term atmospheric dynamics are highly nonlinear that it cannot be easily captured with classical time series models. On the other hand, deep learning models are good at learning nonlinear interactions , but they are not designed to deal with the seasonality in time series. In this study, we aim to develop a forecasting model that can both handle the nonlinearities and detect the seasonality hidden within the daily precipitation data. To this end, we propose a seasonally-integrated autoencoder (SSAE) consisting of two long short-term memory (LSTM) autoencoders: one for learning short-term dynamics, and the other for learning the seasonality in the time series. Our experimental results show that not only does the SSAE outperform various time series models regardless of the climate type, but it also has low output variance compared to other deep learning models. The results also show that the seasonal component of the SSAE helped improve the correlation between the forecast and the actual values from 4\% at horizon 1 to 37\% at horizon 3.},
  archive      = {J_ASOC},
  author       = {Donlapark Ponnoprat},
  doi          = {10.1016/j.asoc.2021.107083},
  journal      = {Applied Soft Computing},
  pages        = {107083},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term daily precipitation forecasting with seasonally-integrated autoencoder},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational intelligence in the hospitality industry: A
systematic literature review and a prospect of challenges.
<em>ASOC</em>, <em>102</em>, 107082. (<a
href="https://doi.org/10.1016/j.asoc.2021.107082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work presents a detailed survey about Computational Intelligence (CI) applied to various Hotel and Travel Industry areas. Currently, the hospitality industry’s interest in data science is growing exponentially because of their expected margin of profit growth. In order to provide precise state of the art content, this survey analyzes more than 160 research works from which a detailed categorization and taxonomy have been produced. We have studied the different approaches on the various forecasting methods and subareas where CI is currently being used. This research work also shows an actual distribution of these research efforts in order to enhance the understanding of the reader about this topic and to highlight unexploited research niches. A set of guidelines and recommendations for future research areas and promising applications are also presented in a final section.},
  archive      = {J_ASOC},
  author       = {Juan Guerra-Montenegro and Javier Sanchez-Medina and Ibai Laña and David Sanchez-Rodriguez and Itziar Alonso-Gonzalez and Javier Del Ser},
  doi          = {10.1016/j.asoc.2021.107082},
  journal      = {Applied Soft Computing},
  pages        = {107082},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Computational intelligence in the hospitality industry: A systematic literature review and a prospect of challenges},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Daily streamflow forecasting in sobradinho reservoir using
machine learning models coupled with wavelet transform and
bootstrapping. <em>ASOC</em>, <em>102</em>, 107081. (<a
href="https://doi.org/10.1016/j.asoc.2021.107081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving forecasting techniques for streamflow time series is of extreme importance for water resource planning. Among the available techniques, those based on machine learning models have sparked the interest of the scientific community in recent decades due to their good adaptability and forecasting accuracy , particularly when coupled with data pretreatment methods. These algorithms include artificial neural networks (ANNs) and support vector machines (SVMs). In the present study, a comparative analysis of a set of machine learning models, namely, an ANN and an SVM coupled with wavelet transform and data resampling with the bootstrap method , was performed. The analysis of these models was performed with daily streamflow time series for Sobradinho Reservoir, which is located in northeastern Brazil , corresponding to the period between 1931 and 2015. The results show that higher accuracy is achieved by the ANN than by the SVM. Furthermore, the best combination for predicting the streamflow into the Sobradinho Reservoir was the bootstrap, wavelet and neural network (BWNN) method. For a forecast 3 to 15 days ahead, this method yielded a mean square error (MSE) 15 to 25 times lower than that of the purely neural model (ANN), and the R 2 and mean absolute error (MAE) coefficients increased by more than 7 to 28\% and 13 to 50\%, respectively. Therefore, the BWNN is an alternative approach for future time series forecasting studies.},
  archive      = {J_ASOC},
  author       = {Samuel Vitor Saraiva and Frede de Oliveira Carvalho and Celso Augusto Guimarães Santos and Lucas Costa Barreto and Paula Karenina de Macedo Machado Freire},
  doi          = {10.1016/j.asoc.2021.107081},
  journal      = {Applied Soft Computing},
  pages        = {107081},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Daily streamflow forecasting in sobradinho reservoir using machine learning models coupled with wavelet transform and bootstrapping},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Syntax-type-aware graph convolutional networks for natural
language understanding. <em>ASOC</em>, <em>102</em>, 107080. (<a
href="https://doi.org/10.1016/j.asoc.2021.107080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structure of a sentence conveys rich linguistic knowledge and has proven useful for natural language understanding . In this paper, we aim to incorporate syntactical constraints and long-range word dependencies into the sentence encoding procedure using the widely applied Graph Convolutional Network (GCN) and word dependency trees. Existing syntax-aware GCN methods construct the adjacency matrix by referring to whether two words are connected in the dependency tree. But they fail to model the word dependency type, which reflects how the words are linked in dependency trees. They cannot distinguish the different contributions of different word dependency paths . To avoid introducing redundant word dependencies that harm language understanding, we propose a GCN version that is extended by a novel Word Dependency Gate mechanism. Word Dependency Gate can adaptively maintain the balance between the inclusion and exclusion of specific word dependency paths based on the word dependency type and its word context. Experiments show that our approach can effectively incorporate the relevant syntactical dependency in BERT and achieve a state-of-the-art performance in the End-to-End Aspect-Based Sentiment Analysis and Relation Triple Extraction tasks.},
  archive      = {J_ASOC},
  author       = {Chunning Du and Jingyu Wang and Haifeng Sun and Qi Qi and Jianxin Liao},
  doi          = {10.1016/j.asoc.2021.107080},
  journal      = {Applied Soft Computing},
  pages        = {107080},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Syntax-type-aware graph convolutional networks for natural language understanding},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). CELOF: Effective and fast memory efficient local outlier
detection in high-dimensional data streams. <em>ASOC</em>, <em>102</em>,
107079. (<a href="https://doi.org/10.1016/j.asoc.2021.107079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is an important and challenging problem in industrial automation, where data are often collected in large amounts but with little labeled information. To realize real-time outlier detection on data streams, many models have been proposed in the academic. However, most existing outlier detection algorithms still have two main limitations: (1) Need a large amount of memory to store data. (2) Poor detection of high-dimensional data in application scenarios. In this paper, we propose a new algorithm, called CELOF which can effectively overcome the two limitations. In CELOF, We first use information entropy to construct a new index weight calculation method, which can distinguish the influencing factors of different indexes and improve the detection accuracy of multi-dimensional data. Next, we designed a new reachable distance factor discrimination method to extract the original data information and then proposed a new strategy for outlier detection, which can greatly reduce the amount of data storage. Finally, the final experiment result shows that the CELOF algorithm has an average improvement of 15\% in accuracy compared to the state-of-the-art algorithms, and the CELOF’s running time less than 1\% of the original LOF. Additionally, our comprehensive experiments use different real data sets for simulation, and the results show that our algorithm can be widely used in different practical application scenarios without any prior information and data distribution.},
  archive      = {J_ASOC},
  author       = {Liang Chen and Wei Wang and Yun Yang},
  doi          = {10.1016/j.asoc.2021.107079},
  journal      = {Applied Soft Computing},
  pages        = {107079},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CELOF: Effective and fast memory efficient local outlier detection in high-dimensional data streams},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization and estimation of reliability indices and cost
of power distribution system of an urban area by a noble fuzzy-hybrid
algorithm. <em>ASOC</em>, <em>102</em>, 107078. (<a
href="https://doi.org/10.1016/j.asoc.2021.107078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power Distribution System (PDS) in an urban populated area suffers from interruptions due to uncertain faults, which degrades its reliability. Better performance of the PDS can be achieved by minimizing the most commonly used reliability indices like System Average Interruption Frequency Index (SAIFI) or System Average Interruption Duration Index (SAIDI) as well as minimizing PDS cost (both installation and maintenance cost). Number and position of Reclosers and fuses (protective devices) and switches in a PDS play an important role in enhances the performance of a PDS while lowering its cost. As the problem is inherently related to real-life activities, it can be modelled better by fuzzy logic. In this paper, the single, as well as a multi-objective optimization problem of minimizing the cost and reliability metrics, has been solved by the help of a noble fuzzy-hybrid algorithm. The outcome of the proposed algorithm is a PDS with a modified number of a recloser, fuse and switch and their arrangement in the network. Two well-known and well-organized meta-heuristic algorithms named as improved Genetic Algorithm (GA) and Quantum mechanized Particle Swarm Optimization (QPSO) are hybridized with fuzzified inputs and the outputs are defuzzified in order to minimize reliability metrics and cost. The obtained result set is compared with existing works of literature to establish that the model developed is more reliable in terms of fault tolerance as a fuzzy repair rate and the fuzzy failure rate has been considered in the experiment. As an offshoot, a rule-set is also obtained that may help the future decision process.},
  archive      = {J_ASOC},
  author       = {Avishek Banerjee and Samiran Chattopadhyay and Mihai Gavrilas and Gheorghe Grigoras},
  doi          = {10.1016/j.asoc.2021.107078},
  journal      = {Applied Soft Computing},
  pages        = {107078},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization and estimation of reliability indices and cost of power distribution system of an urban area by a noble fuzzy-hybrid algorithm},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A KNN quantum cuckoo search algorithm applied to the
multidimensional knapsack problem. <em>ASOC</em>, <em>102</em>, 107077.
(<a href="https://doi.org/10.1016/j.asoc.2020.107077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization algorithms and particularly metaheuristics are constantly improved with the goal of reducing execution times, increasing the quality of solutions, and addressing larger target instances. Hybridizing techniques are one of these methods particularly interesting for the broad scope of problems to which they can be adapted. In this work, we assessed a hybrid algorithm that uses the k-nearest neighbor technique to improve the results of a quantum cuckoo search algorithm for resource allocation. The k-nearest neighbor technique is used to direct the movement of solutions. Numerical experiments were performed to obtain insights from the contribution of the k-nearest neighbor technique in the final result of solutions. The well-known multidimensional knapsack problem was addressed in order to validate our procedure; a comparison is made with state-of-the-art algorithms. Our results show that our hybrid algorithm consistently produces better results in most of the analyzed instances.},
  archive      = {J_ASOC},
  author       = {José García and Carlos Maureira},
  doi          = {10.1016/j.asoc.2020.107077},
  journal      = {Applied Soft Computing},
  pages        = {107077},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A KNN quantum cuckoo search algorithm applied to the multidimensional knapsack problem},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A clustering and ensemble based classifier for data stream
classification. <em>ASOC</em>, <em>102</em>, 107076. (<a
href="https://doi.org/10.1016/j.asoc.2020.107076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of data mining, the research industry has great attention to data stream mining as well as it has a great impact on a wide range of applications like networking, telecommunication, education, banking, weather forecasting, a stock market, and so on. Because of these data stream mining having more attention from researchers. The handling of concept drifting data streams is one of the major issues and challenges in the data stream mining field. In the presence of the concept drift, the performance of the learning algorithm always degrades. In this paper, a hybrid method has been proposed which are the combination of an ensemble, and grid and density-based clustering methods . The proposed method is tested on both synthetic as well as real data. The proposed method works well in the presence of concept drift and performance is measured in terms of time, accuracy, and memory. As compared with the state-of-art algorithms, the proposed method performed well and gave better accuracy using synthetic datasets like 88.29\%, 71.34\%, and 75.39\% for Hyperplane , RBF , and LED respectively and for real datasets 86.17\%, 86.28\%, 95.15\%, and 99.83\% for Adult, Census-Income, KDDCup99\%–10\%, and Covertype respectively.},
  archive      = {J_ASOC},
  author       = {Kapil K. Wankhade Ph.D. and Kalpana C. Jondhale Ph.D. and Snehlata S. Dongre Ph.D.},
  doi          = {10.1016/j.asoc.2020.107076},
  journal      = {Applied Soft Computing},
  pages        = {107076},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A clustering and ensemble based classifier for data stream classification},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi objective taguchi–grey relational analysis and krill
herd algorithm approaches to investigate the parametric optimization in
abrasive water jet drilling of stainless steel. <em>ASOC</em>,
<em>102</em>, 107075. (<a
href="https://doi.org/10.1016/j.asoc.2020.107075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This novel study deals with the investigation of abrasive mixtures on abrasive water jet (AWJ) drilling of stainless steel 304 using multi objective soft computing techniques . In this study, the drilling parameters such as abrasive mixture, stand-off distance and feed rate were varied. The abrasive mixture was prepared with the composition of different types of abrasives such as silicon carbide &amp; garnet, and aluminium oxide &amp; garnet, and different mixture ratios. This mixing ratio was done based on the total mass of the abrasive mixture. The effect of abrasive water jet drilling parameters was examined on the hole features such as the hole diameter, circularity , cylindricity, and surface roughness. Multi objective optimization and algorithm techniques were employed in this study, namely Taguchi–Grey Relational Analysis (TGRA) and Krill Herd Algorithm (KHA). In this research work, the performance of KHA method was also compared with another recent metaheuristic technique i.e. grey wolf optimization (GWO) based on the quality measurement tools such as Spacing and Inverted generational distance. For this approach, the main parameters of metaheuristics algorithms were tuned using a robust design approach to acquire the best feasible solution. Besides, different multiple linear regression model equations were established to determine the best model for the KHA method based on the similarity between experimental and calculated attributes. With the assistance of these approaches, it is found that the abrasive mixtures have improved the performance of the AWJ drilling process in SS 304 rather than the use of a single type abrasive such as 100\% Garnet. The results of this study proved that the KHA optimization technique is successfully utilized to find the best configuration parameter setting for AWJ drilling process, and that results are found to be efficient than the TGRA. To validate the predicted results of KHA, confirmation test was conducted. The results of the confirmation test showed that the predicted hole features of KHA were acceptable as that the error deviation was found as less than 2\% with the experimental results. It is also noticed that the computational time and the selected quality metrics of KHA are found to be lower than the GWO method. Hence, it is confirmed that a new metaheuristic algorithm namely, KHA was found suitable for AWJ drilling process. The outcome of the present work explores a new paradigm to the AWJ machining to improve performance features in various operations.},
  archive      = {J_ASOC},
  author       = {K. Balaji and M. Siva Kumar and N. Yuvaraj},
  doi          = {10.1016/j.asoc.2020.107075},
  journal      = {Applied Soft Computing},
  pages        = {107075},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi objective taguchi–grey relational analysis and krill herd algorithm approaches to investigate the parametric optimization in abrasive water jet drilling of stainless steel},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soft-sensor design for vacuum distillation bottom product
penetration classification. <em>ASOC</em>, <em>102</em>, 107072. (<a
href="https://doi.org/10.1016/j.asoc.2020.107072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Petroleum oil refineries are complex systems that convert crude into subproducts of value. The profit of the refinery depends on the quality of the resultant subproducts, which are usually determined by a laboratory analysis called “Needle penetration”. Normally, this laboratory analysis is costly and time-consuming since entails around four hours for its accomplishment. In order to solve this limitation, this paper proposes a novel soft-sensor design for online vacuum distillation bottom product penetration classification. The design of the soft-sensor is based on a new approach, the two-stage methodology, that considers the joint effect of both Normalization and Supervised Filter Feature Weighting methods to transform the features. This methodology stands on the analysis of the real impact of applying normalization methods on the contribution of each feature, providing results that significantly differ from the traditional premises of the state-of-the-art. The analysis includes the impact of normalization on distance metrics such as the Euclidean. Also, a new adaptation of Pearson correlation for the estimation of the feature weights respect to categorical labels is proposed in this work. Once the features are transformed, five well-known Machine Learning (ML) algorithms (K-means, K-NN, RFc, SVC and MLP) are considered for the design of the soft-sensor. The final soft-sensor design is selected based on the feature space transformation strategy and the ML algorithm that achieves the best results in terms of: accuracy, precision, generalization and explicability. In order to validate the proposal, real monitored data from a petroleum refinery plant sited in The Basque Country is employed. Results show that the proposed two-stage methodology improves the results obtained by the Normalization methods.},
  archive      = {J_ASOC},
  author       = {Iratxe Niño-Adan and Itziar Landa-Torres and Diana Manjarres and Eva Portillo},
  doi          = {10.1016/j.asoc.2020.107072},
  journal      = {Applied Soft Computing},
  pages        = {107072},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft-sensor design for vacuum distillation bottom product penetration classification},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning feature exploration for android malware
detection. <em>ASOC</em>, <em>102</em>, 107069. (<a
href="https://doi.org/10.1016/j.asoc.2020.107069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Android mobile devices and applications are widely deployed and used in industry and smart city. Malware detection is one of the most powerful and effective approaches to guarantee security of Android systems, especially for industrial platform and smart city. Recently, researches using machine learning-based techniques for Android malware detection increased rapidly. Nevertheless, most of the appeared approaches have to perform feature analysis and selection, so-called feature engineering, which is time-consuming and relies on artificial experience. To solve the inefficiency problem of feature engineering, we propose TC-Droid, an automatic framework for Android malware detection based on text classification method. The core idea of TC-Droid is derived from the field of text classification . TC-Droid feeds on the text sequence of APPs analysis reports generated by AndroPyTool, applies a convolutional neural network (CNN) to explore significant information (or knowledge) under original report text, instead of manual feature engineering. In an evaluation with different number of real-world samples, TC-Droid outperforms state-of-the-art model (Drebin) and several classic models (NB, LR , KNN, RF) as well. With multiple experimental settings and corresponding comparisons, TC-Droid achieves effective and flexible performance in Android malware detection task.},
  archive      = {J_ASOC},
  author       = {Nan Zhang and Yu-an Tan and Chen Yang and Yuanzhang Li},
  doi          = {10.1016/j.asoc.2020.107069},
  journal      = {Applied Soft Computing},
  pages        = {107069},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning feature exploration for android malware detection},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Device-free single-user activity recognition using
diversified deep ensemble learning. <em>ASOC</em>, <em>102</em>, 107066.
(<a href="https://doi.org/10.1016/j.asoc.2020.107066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based human activity recognition (HAR) aims to recognize human activities in an off-the-shelf manner that only relies on the commercial Wi-Fi devices already installed in environments. The recent trend in HAR research is to train classifiers on top of statistical or deep neural features extracted from channel state information (CSI) data. Unfortunately, existing methods only take into account the temporal-correlation within each CSI subcarrier , while ignoring the spatial-correlation between different subcarriers. This issue has not been fully exploited yet, resulting a limited performance. To address this issue, we propose WiAReS, a WiFi-based device-free activity recognition system that takes both temporal-correlation and spatial-correlation into account. WiAReS embarks on diversified deep ensemble methods 2̌for single-user activity recognition where one user performs a single activity at a given time. More specifically, it adopts convolutional neural network (CNN) to automatically extract features from CSI measurements with the preservation of the locality of both spatial patterns and temporal patterns. To further improve recognition accuracy upon CNN-extracted features, we propose a novel ensemble architecture that fuses a multiple layer perception (MLP), a random forest (RF) and a support vector machine (SVM). Our system obtains the CSI data in PHY layer of off-the-shelf WiFi devices by installing Atheros-CSI-Tool on AR9590 based WiFi network interface cards (NICs). Comprehensive experiments have been conducted in three real environments with environmental variation to evaluate the performance of the proposed WiAReS. The experimental results demonstrate that the proposed WiARes system significantly outperforms existing methods.},
  archive      = {J_ASOC},
  author       = {Wei Cui and Bing Li and Le Zhang and Zhenghua Chen},
  doi          = {10.1016/j.asoc.2020.107066},
  journal      = {Applied Soft Computing},
  pages        = {107066},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Device-free single-user activity recognition using diversified deep ensemble learning},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probability granular distance-based fuzzy rough set model.
<em>ASOC</em>, <em>102</em>, 107064. (<a
href="https://doi.org/10.1016/j.asoc.2020.107064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy rough set theory is sensitive to noisy samples as the fuzzy approximations are proposed based on sensitive statistics, i.e. minimum and maximum. Here, we develop a robust fuzzy rough set model called probability granular distance-based fuzzy rough sets (PGDFRS), in which the similarity between samples is substituted by that between granules to reduce the impact of noise on the statistical minimum and maximum. The robust principle is to take the probability density values of samples as weights for computing probability distances between granules. By using PGDFRS, a feature selection algorithm is created. This algorithm limits feature selection to two-dimensional space and avoids the difficulty of parameter setting in high-dimensional space. The experimental results indicate that the designed feature selection algorithm is effective and robust. Additionally, it confirms that the proposed PGDFRS model is more robust than some existing fuzzy rough set models.},
  archive      = {J_ASOC},
  author       = {Shuang An and Qinghua Hu and Changzhong Wang},
  doi          = {10.1016/j.asoc.2020.107064},
  journal      = {Applied Soft Computing},
  pages        = {107064},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Probability granular distance-based fuzzy rough set model},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fusion of region based extracted features for instance- and
class-based CBIR applications. <em>ASOC</em>, <em>102</em>, 107063. (<a
href="https://doi.org/10.1016/j.asoc.2020.107063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major challenges in content-based image retrieval (CBIR) is understanding the actual semantic meaning of images. Generally, natural images possess a rich collection of different visual features, each of which have different contributions to the image semantics . These vary drastically in their local sub-regions. However, most of the state-of-the-art CBIR techniques give equal importance to all the visual features, which are not always effective for generating relevant retrieval outcomes. To overcome these issues, a CBIR scheme has been proposed which considers the image semantic properties of the different regions with their positional significance in the overall image. In this paper, a novel regions-of-attention (ROA) based feature extraction and fusion scheme for efficient image retrieval is suggested. Initially, a salient key-points and opponent-color feature based scheme is used to locate ROA of the image. Multi-directional texture, and spatial correlation-based color features are extracted from this ROA of the image to obtain most of the image’s semantic meanings. Spatial features from salient non-ROA local regions are also obtained to capture vital background information. A dissimilarity factor-based fusion scheme then is utilized to fuse the ROA and non-ROA features effectively, considering the importance and uniqueness of each feature to support the positional invariance notion in their respective sub-regions. Finally, the fused feature vector is used to perform instance- and class-based image retrieval in five different image datasets using different classifiers. The outcomes of the retrieval experiment have shown competitive performances as compared to the current state-of-the-art architectures.},
  archive      = {J_ASOC},
  author       = {Jitesh Pradhan and Arup Kumar Pal and Haider Banka and Prabhat Dansena},
  doi          = {10.1016/j.asoc.2020.107063},
  journal      = {Applied Soft Computing},
  pages        = {107063},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fusion of region based extracted features for instance- and class-based CBIR applications},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Swarm ascending: Swarm intelligence-based exemplar group
detection for robust clustering. <em>ASOC</em>, <em>102</em>, 107062.
(<a href="https://doi.org/10.1016/j.asoc.2020.107062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An exemplar is a representative observation for each cluster. Exemplar-based clustering algorithms , which find the exemplars and assign data points to the nearest exemplar, have exhibited promising performance. However, the single- and multi-exemplar methods become inadequate for clustering data points with nonlinear and local patterns because one exemplar (or a set of sparse exemplars for a nonlinear cluster) is insufficient to represent the cluster. In this paper, we propose a swarm intelligence-based exemplar group detection method that ascends data points to local high-density points and groups the merged points. The proposed method is robust to nonlinear and local patterns because it detects the intrinsic structure of each cluster more sufficiently than sparse exemplars. We use simulation and real-world data to demonstrate the usefulness of the proposed method by comparing it to existing methods in terms of clustering accuracy. The comparison results demonstrate that the proposed method outperforms the alternatives.},
  archive      = {J_ASOC},
  author       = {Younghoon Kim and Minjung Lee and Seoung Bum Kim},
  doi          = {10.1016/j.asoc.2020.107062},
  journal      = {Applied Soft Computing},
  pages        = {107062},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Swarm ascending: Swarm intelligence-based exemplar group detection for robust clustering},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new version of the deterministic dendritic cell algorithm
based on numerical differential and immune response. <em>ASOC</em>,
<em>102</em>, 107055. (<a
href="https://doi.org/10.1016/j.asoc.2020.107055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of the promising deterministic dendritic cell algorithm (dDCA) in terms of anomaly detection has been demonstrated in large number of studies. However, the results of these studies also show that the signal calculation phase of dDCA depends on artificial experience, while the algorithm also lacks adaptability. To overcome these limitations, we propose a new approach for anomaly detection based on dDCA, in which numerical differential and immune responses are applied. The numerical differential is introduced to provide a new method of calculating the signal values that is independent of artificial experience. In the immune response method, moreover, the immune nonlinear model is adopted to adjust the weight signal matrix of dDCA. The experimental results show that the proposed algorithm has significant advantages over the compared immune anomaly detection approaches in terms of signal calculation and adaptability.},
  archive      = {J_ASOC},
  author       = {Wen Zhou and Yiwen Liang},
  doi          = {10.1016/j.asoc.2020.107055},
  journal      = {Applied Soft Computing},
  pages        = {107055},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new version of the deterministic dendritic cell algorithm based on numerical differential and immune response},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An agile and intelligent dynamic economic emission
dispatcher based on multi-objective proximal policy optimization.
<em>ASOC</em>, <em>102</em>, 107047. (<a
href="https://doi.org/10.1016/j.asoc.2020.107047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent development in power systems claims the improvement of the efficiency to solve dynamic economic emission dispatch (DEED) problem. However, the popular pure optimization framework based on evolutionary algorithms only starts the compute-intensive optimization process after receiving the power demand, which causes significant delays To address this limitation, this research defines a novel dynamic economic emission dispatcher (DEEDer) problem and the dispatcher learning framework. It is different from previous research and the most current algorithms in the advantage of transferring the on-line compute-intensive optimization task to off-line. To solve the dispatcher, we model the dispatching process as a conditional deterministic Markov Decision Process (MDP) and propose the multi-objective proximal policy optimization (MOPPO). The Benchmark Test Set with 10, 000 different dispatching tasks for 5-unit and 10-unit system is released to evaluate the generalization of the dispatcher. The experiment results indicate that the neural network dispatcher trained with MOPPO is hundreds to thousands of times faster than the state-of-the-art pure optimization algorithms . Meanwhile, the dispatcher not only has comparable performance as the state-of-the-art multi-objective optimization algorithms in standard dispatching task but also shows generalization on generating Pareto optimal solutions given any possible dispatching task. Following the DEEDer framework, the proposed method makes it possible to dispatch power in a more agile way as the timely response to the changing of power demand while still controlling economic and emission.},
  archive      = {J_ASOC},
  author       = {Zhuang Shao and Fengqi Si and Huaijiang Wu and Xiaozhong Tong},
  doi          = {10.1016/j.asoc.2020.107047},
  journal      = {Applied Soft Computing},
  pages        = {107047},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An agile and intelligent dynamic economic emission dispatcher based on multi-objective proximal policy optimization},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-start granular skewed variable neighborhood tabu
search for the roaming salesman problem. <em>ASOC</em>, <em>102</em>,
107024. (<a href="https://doi.org/10.1016/j.asoc.2020.107024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel hybrid metaheuristic algorithm for the Roaming Salesman Problem (RSP), called Multi-Start Granular Skewed Variable Neighborhood Tabu Search (MS-GSVNTS). The objective in RSP is to design daily tours for a traveling campaigner who collects rewards from activities in cities during a fixed planning horizon. RSP exhibits a number of exclusive features: It is selective which implies that not every node needs a visit. The rewards of cities are time-dependent. Daily tours can be either an open or a closed tour which implies the absence of a fixed depot. Instead, there is a campaign base that is to be attended frequently. Multiple visits are allowed for certain cities. The proposed method MS-GSVNTS is tested on 45 real-life instances from Turkey which are built with actual travel distances and times and on 10 large scale instances. Computational results suggest that MS-GSVNTS is superior to the existing solution methods developed for RSP. It produces 50 best known solutions including 18 ties and 32 new ones. The performance of MS-GSVNTS can be attributed to its multi-start feature, rich neighborhood structures, skewed moves, and granular neighborhoods.},
  archive      = {J_ASOC},
  author       = {Masoud Shahmanzari and Deniz Aksen},
  doi          = {10.1016/j.asoc.2020.107024},
  journal      = {Applied Soft Computing},
  pages        = {107024},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-start granular skewed variable neighborhood tabu search for the roaming salesman problem},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A ranking-based feature selection for multi-label
classification with fuzzy relative discernibility. <em>ASOC</em>,
<em>102</em>, 106995. (<a
href="https://doi.org/10.1016/j.asoc.2020.106995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a crucial pre-processing step for learning tasks to mitigate the “curse of dimensionality”, which is caused by irrelevant and redundant features in high-dimensional feature space. The fuzzy rough set is an effective tool in feature selection, and the fuzzy discernibility matrix, one of the generalized model of the fuzzy rough set , has attracted significant attention recently. However, with the complicated discrimination relation of multiple labels, the traditional fuzzy discernibility matrix does not fit well for multi-label data. To address this problem, in this paper, the fuzzy label discernibility relation and the fuzzy relative discernibility relation are defined firstly, which are derived from fuzzy discernibility relation. Subsequently, we present the feature discernibility significance which measures the discernibility ability of the conditional feature and selects the most relevant features using the value of discernibility significance. On this basis, we propose a ranking-based feature selection algorithm for multi-label classification with fuzzy relative discernibility. Finally, in terms of six widely-accepted multi-label evaluation metrics , a series of experiments is conducted based on the multi-label classifier with ten different multi-label datasets from Mulan library and MLL resource, to compare the performance of the proposed algorithm with four state-of-the-art multi-label feature selection algorithms. The experimental results demonstrate the superiority and effectiveness of the proposed algorithm in multi-label classification.},
  archive      = {J_ASOC},
  author       = {Wenbin Qian and Chuanzhen Xiong and Yinglong Wang},
  doi          = {10.1016/j.asoc.2020.106995},
  journal      = {Applied Soft Computing},
  pages        = {106995},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A ranking-based feature selection for multi-label classification with fuzzy relative discernibility},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent modeling strategies for forecasting air quality
time series: A review. <em>ASOC</em>, <em>102</em>, 106957. (<a
href="https://doi.org/10.1016/j.asoc.2020.106957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the deterioration of air quality, the frequent events of the air contaminants , and the health impacts from that have caused continuous attention by the government and the public. Based on that, suitable and effective forecasting tools are urgently needed in scientific research. In this study, the basic forecasting algorithms are introduced as the simple forecasting models with their background, applications, advantages, and limitations, which include shallow predictors and deep learning predictors. Then, to enhance the forecasting ability, the data processing methods and two commonly used auxiliary methods (the ensemble learning and the metaheuristic optimization) in the hybrid models have been reviewed. The recent articles of the spatiotemporal aspects have also brought changes in both the analysis and the modeling methods. Furthermore, the representative models are summarized to present the structures of efficient predictive models . Some possible research directions of the air pollution forecasting are given at the end. This review aims to provide a comprehensive literature summary of the intelligent modeling strategies in the air quality forecasting, which may be helpful for subsequent study.},
  archive      = {J_ASOC},
  author       = {Hui Liu and Guangxi Yan and Zhu Duan and Chao Chen},
  doi          = {10.1016/j.asoc.2020.106957},
  journal      = {Applied Soft Computing},
  pages        = {106957},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent modeling strategies for forecasting air quality time series: A review},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On removing potential redundant constraints for SVOR
learning. <em>ASOC</em>, <em>102</em>, 106941. (<a
href="https://doi.org/10.1016/j.asoc.2020.106941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extension of support vector machine in ordinal regression problem , support vector ordinal regression (SVOR) finds ( r − 1 r−1 ) parallel hyperplanes by solving a quadratic programming which has n ( r − 1 ) n(r−1) constraints. Here, n n and r r represent the number of training sample and ranks, respectively. Therefore, it would costs much more time to train SVOR than SVC or SVR . Fortunately, the solution of SVOR is only decided by minor constraints which are associated with non-zero Lagrange multipliers . Other constraints with zero Lagrange multipliers have no influence on the solution. Because a training sample is associated with ( r − 1 ) (r−1) constraints, retaining potential support vector may still induce that many redundant constraints are reserved. In this paper, we try to remove these potential redundant constraints for SVOR learning. For the j j th parallel hyperplane, the potential constraints with non-zero Lagrange multipliers are associated with the samples near the j j th parallel hyperplane. These samples can be identified by a chain near the j j th parallel hyperplane. Then, other constraints for the j j th parallel hyperplane can be discarded before learning. The number of the constraints can be reduced to less than 17 percent of the original in our experiments. Additionally, it only executes once to find potential critical constraints. Obviously, it is easy to tune parameters after removing potential redundant constraints. The experimental results on several datasets demonstrate that SVOR becomes much faster after removing potential redundant constraints and the performance does not degrade seriously.},
  archive      = {J_ASOC},
  author       = {Fa Zhu and Ye Ning and Xingchi Chen and Yongbin Zhao and Yining Gang},
  doi          = {10.1016/j.asoc.2020.106941},
  journal      = {Applied Soft Computing},
  pages        = {106941},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On removing potential redundant constraints for SVOR learning},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). URNet: A u-net based residual network for image dehazing.
<em>ASOC</em>, <em>102</em>, 106884. (<a
href="https://doi.org/10.1016/j.asoc.2020.106884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low visibility in hazy weather causes the loss of image details in digital images captured by some imaging devices such as monitors. This paper proposes an end-to-end U-Net based residual network (URNet) to improve the visibility of hazy images. The encoder module of URNet uses hybrid convolution combining standard convolution with dilated convolution to expand the receptive field for extracting image features with more details. The URNet embeds several building blocks of ResNet into the junction between the encoder module and the decoder module. This prevents network performance degradation due to the vanishing gradient. After considering large absolute difference on image saturation and value components between hazy images and haze-free images in the HSV color space, the URNet defines a new loss function to better guide the network training. Experimental results on synthetic hazy images and real hazy images show that the URNet significantly improves the image dehazing effect compared to the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Ting Feng and Chuansheng Wang and Xinwei Chen and Haoyi Fan and Kun Zeng and Zuoyong Li},
  doi          = {10.1016/j.asoc.2020.106884},
  journal      = {Applied Soft Computing},
  pages        = {106884},
  shortjournal = {Appl. Soft. Comput.},
  title        = {URNet: A U-net based residual network for image dehazing},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of web browser prototype with embedded
classification capability for mitigating cross-site scripting attacks.
<em>ASOC</em>, <em>102</em>, 106873. (<a
href="https://doi.org/10.1016/j.asoc.2020.106873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mitigation of Cross-Site Scripting (XSS) with machine learning techniques is the recent interest of researchers. A large amount of research work is reported in this domain. A lack of real-time tools working on the basis of these approaches is a gap in this domain. In this work, a web browser that works on machine learning classification to mitigate XSS attacks is developed. This browser classifies webpages into malicious and non-malicious pages using features identified by observation of malicious web pages and features collected from the different authors works. Classification experiments are conducted to evaluate the effectiveness of these features, and it is found that this approach performs better than other proposed methods in terms of classification accuracy , precision, recall, and F1-score. A web browser is implemented with the open-source browser WebKit . Experiments are conducted to assess the overhead created by the added functionality of classification in the web browser. The browser is found effective in classifying web pages and in real-time browsing scenarios with very less generated overhead. This makes web browser better than other proposed solutions to mitigate (XSS) attacks with minimal overhead. This developed web browser will be beneficial not only for researchers working in this domain but also for the users who can be the victims of XSS attacks.},
  archive      = {J_ASOC},
  author       = {Vikas K. Malviya and Sawan Rai and Atul Gupta},
  doi          = {10.1016/j.asoc.2020.106873},
  journal      = {Applied Soft Computing},
  pages        = {106873},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of web browser prototype with embedded classification capability for mitigating cross-site scripting attacks},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated approach to predict scalar fields of a
simulated turbulent jet diffusion flame using multiple fully connected
variational autoencoders and MLP networks. <em>ASOC</em>, <em>101</em>,
107074. (<a href="https://doi.org/10.1016/j.asoc.2020.107074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel integrated deep learning approach for data-driven surrogate modelling of combustion computational fluid dynamics (CFD) simulations is presented. It combines variational autoencoders (VAEs) with deep neural networks (DNNs) to predict detail cell-by-cell two-dimensional distributions of temperature, velocity and species mass fractions from high level inputs such as velocity and fuel and air mass fractions. The VAE model is used to generate low dimensional encodings of the CFD data and the DNN is used in turn to map boundary conditions to the encodings. The results show that regularization is required during all training phases. Sufficiently accurate results were achieved for the reproduced species mass fractions with mean average errors below 0.3 [\%wt.]. The validation mean average percentage errors for the temperature and velocity fields are 1.7\% and 7.1\% respectively. It is therefore possible to predict detail two-dimensional contours of CFD solution data with adequate generalizability and accuracy.},
  archive      = {J_ASOC},
  author       = {Ryno Laubscher and Pieter Rousseau},
  doi          = {10.1016/j.asoc.2020.107074},
  journal      = {Applied Soft Computing},
  pages        = {107074},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated approach to predict scalar fields of a simulated turbulent jet diffusion flame using multiple fully connected variational autoencoders and MLP networks},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recursive multilayer perceptron-based data-driven
identification for a parameterized polarization model of rechargeable
li-ion battery. <em>ASOC</em>, <em>101</em>, 107073. (<a
href="https://doi.org/10.1016/j.asoc.2020.107073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A battery model is intended to reflect the dynamics of a battery in terms of terminal voltage under different stress factors. The terminal voltage further depends on the open-circuit voltage (OCV) and polarizations of the battery. Therefore, an important concern of battery modeling is how to accurately map the impact of stress factors on the OCV and polarizations of the battery. Commonly, the OCV curve obtained experimentally by the DC pulse technique is fitted empirically to model the OCV as a function of the operating conditions by using different regression techniques , which may be suitable for offline modeling. However, for online implementation, intelligent fitting techniques are necessary to adjust the model according to changing conditions. Similarly, different combinations of resistors and capacitors are selected to model the polarizations, and these components are parameterized by curve fitting of the terminal voltage. Although the cumulative effect of the components may result in good curve fitting, the first-principle justification of the selection of components is lacking. To address this problem, data-driven identification of the parametric model of a battery and its subsequent parameterization by a multilayer perceptron (MLP) with a gradient-descent back-propagation algorithm are introduced. The proposed algorithm is intelligent, and it provides justification for the selection of components to model polarizations. Furthermore, this algorithm can be easily extended to develop a model adaptable to changes in battery life and operating conditions.},
  archive      = {J_ASOC},
  author       = {Mazhar Abbas and Inho Cho and Jonghoon Kim},
  doi          = {10.1016/j.asoc.2020.107073},
  journal      = {Applied Soft Computing},
  pages        = {107073},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recursive multilayer perceptron-based data-driven identification for a parameterized polarization model of rechargeable li-ion battery},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Condensing the solution of support vector machines via
radius-margin bound. <em>ASOC</em>, <em>101</em>, 107071. (<a
href="https://doi.org/10.1016/j.asoc.2020.107071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with the problem of how to condense efficiently the used basis vectors in the decision function of support vector machines (SVM). Most existing methods learn the basis vectors and the corresponding coefficients by maximizing the margin between different classes. They ignore the key fact that condensing the solution of SVM is equivalent to constructing the SVM model in the transformation space determined by the basis vectors. Thus, the radius-margin bound, which is related with the generalization ability of SVM, changes with them. In the paper, we propose a novel method called sparse support vector machine guided by radius-margin bound (RMB-SSVM) to learn the condensed solution for SVM. The key characteristic of RMB-SSVM is that it employs a criterion that exploits integratedly the margin and the radius of the minimum hypersphere enclosing the data to guide the selecting of the basis vectors and the learning of the corresponding coefficients. Thus, the learning criterion of RMB-SSVM is directly related with the generalization ability of SVM and so it can yield better performance. We develop an effective method to handle the proposed optimization model, and propose a heuristic scheme to select the basis vectors used in the decision function. Further, we explore how to use the maximum pairwise distance over the pairs of data points to approximate the radius in our methodology. This can simplify the proposed model and reduce the training time while not drastically impairing the performance. Finally, we conduct the comprehensive experiments to demonstrate the effectiveness and superiority of the proposed methods by comparing them with the counterparts.},
  archive      = {J_ASOC},
  author       = {Xiaoming Wang and Shitong Wang and Zengxi Huang and Yajun Du},
  doi          = {10.1016/j.asoc.2020.107071},
  journal      = {Applied Soft Computing},
  pages        = {107071},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Condensing the solution of support vector machines via radius-margin bound},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A virtual sample generation approach based on a modified
conditional GAN and centroidal voronoi tessellation sampling to cope
with small sample size problems: Application to soft sensing for
chemical process. <em>ASOC</em>, <em>101</em>, 107070. (<a
href="https://doi.org/10.1016/j.asoc.2020.107070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the chemical industries, it is occasionally hard to acquire plenty of samples for developing a soft sensor due to physical limitations and high cost of measurements. To overcome this issue, we come up with a virtual sample generation approach to synthesis new samples to rationally enlarge training sets for soft sensing. Firstly, by applying the centroidal Voronoi tessellation sampling, uniformly distributed new samples x are obtained, for the sake of as possible filling up data scarcity regions. Secondly, the corresponding output of those new samples is determined by the conditional distribution P ( y | x ) P(y|x) captured by a modified conditional GAN implicitly. The negative logarithmic prediction density is then taken to be a measure of closeness between generated samples and real samples. To examine the effectiveness of our approach, numerical simulations over a benchmarking function and a chemical process application were carried out. Experimental results suggested that in contrast to other existing state-of-the-art approaches, our approach can yield more authentic samples but also give rise to significant improvement in soft sensor’s performance.},
  archive      = {J_ASOC},
  author       = {Zhong-Sheng Chen and Kun-Rui Hou and Mei-Yu Zhu and Yuan Xu and Qun-Xiong Zhu},
  doi          = {10.1016/j.asoc.2020.107070},
  journal      = {Applied Soft Computing},
  pages        = {107070},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A virtual sample generation approach based on a modified conditional GAN and centroidal voronoi tessellation sampling to cope with small sample size problems: Application to soft sensing for chemical process},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A health assessment framework of lithium-ion batteries for
cyber defense. <em>ASOC</em>, <em>101</em>, 107067. (<a
href="https://doi.org/10.1016/j.asoc.2020.107067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the existence of cascading failures in the vehicle system, the vehicle telematics system would cause the failure of lithium-ion batteries under the threats of cyber-attacks. This paper presents a new health assessment framework for lithium-ion batteries to construct an efficient defense mechanism. The framework could mitigate the effects of variable operation conditions to the evaluating process. Specifically, it extracts the geometrical characteristics of charging and discharging curves of the lithium-ion batteries. Furthermore, it adopts a multiple dimensionality reduction method to assess the state of health of lithium-ion batteries. Moreover, the long short-term memory network is introduced to predict the state of health. Finally, the example illustrates the effectiveness of the proposed framework.},
  archive      = {J_ASOC},
  author       = {Sheng Hong and Yining Zeng},
  doi          = {10.1016/j.asoc.2020.107067},
  journal      = {Applied Soft Computing},
  pages        = {107067},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A health assessment framework of lithium-ion batteries for cyber defense},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNN-based multivariate data analysis for bitcoin trend
prediction. <em>ASOC</em>, <em>101</em>, 107065. (<a
href="https://doi.org/10.1016/j.asoc.2020.107065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin is the most widely known blockchain , a distributed ledger that records an increasing number of transactions based on the bitcoin cryptocurrency . New bitcoins are created at a predictable and decreasing rate, which means that the demand must follow this level of inflation to keep the price stable. Actually, the price is highly volatile, because it is affected by many factors including the supply of bitcoin, its market demand, the cost of the mining process, as well as economic and political world-class news. In this work, we illustrate a novel approach for bitcoin trend prediction, based on the One-Dimensional Convolutional Neural Network (1D CNN). First, we propose a methodology for building useful datasets that take into account social media data , the full blockchain transaction history, and a number of financial indicators. Moreover, we present a cloud-based system characterized by a highly efficient distributed architecture, which allowed us to collect a huge amount of data in order to build thousands of different datasets, using the aforementioned methodology. To the best of our knowledge, this is the first work that uses 1D CNN for bitcoin trend prediction. Remarkably, an efficient and low-cost implementation is feasible due to the simple and compact configuration of 1D CNN models that perform one-dimensional convolutions (i.e., scalar multiplications and additions). We show that the 1D CNN model we implemented, trained, validated and tested using the aforementioned datasets, allow one to predict the bitcoin trend with higher accuracy compared to LSTM models. Last but not least, we introduce and simulate a trading strategy based on the proposed 1D CNN model, which increases the profit when the bitcoin trend is bullish and reduces the loss when the trend is bearish.},
  archive      = {J_ASOC},
  author       = {Stefano Cavalli and Michele Amoretti},
  doi          = {10.1016/j.asoc.2020.107065},
  journal      = {Applied Soft Computing},
  pages        = {107065},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CNN-based multivariate data analysis for bitcoin trend prediction},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Hybrid particle swarm and grey wolf optimizer and its
application to clustering optimization. <em>ASOC</em>, <em>101</em>,
107061. (<a href="https://doi.org/10.1016/j.asoc.2020.107061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grey Wolf Optimizer (GWO) and Particle Swarm Optimization (PSO) algorithm are two popular swarm intelligence optimization algorithms and these two algorithms have their own search mechanisms. Based on their unique search mechanisms and their advantages after the improvements on them, this paper proposes a novel hybrid algorithm based on PSO and GWO (Hybrid GWO with PSO , HGWOP). Firstly, GWO is simplified and a novel differential perturbation strategy is embedded in the search process of the simplified GWO to form a Simplified GWO with Differential Perturbation (SDPGWO) so that it can improve the global search ability while retaining the strong exploitation ability of GWO. Secondly, a stochastic mean example learning strategy is applied to PSO to create a Mean Example Learning PSO (MELPSO) to enhance the global search ability of PSO and prevent the algorithm from falling into local optima. Finally, a poor-for-change strategy is proposed to organically integrate SDPGWO and MELPSO to obtain an efficient hybrid algorithm of GWO and PSO. HGWOP can give full play to the advantages of these two improved algorithms, overcome the shortcomings of GWO and PSO and maximize the whole performance. A large number of experiments on the complex functions from CEC2013 and CEC2015 test sets reveal that HGWOP has better optimization performance and stronger universality compared with quite a few state-of-the-art algorithms. Experimental results on K-means clustering optimization show that HGWOP has obvious advantages over the comparison algorithms.},
  archive      = {J_ASOC},
  author       = {Xinming Zhang and Qiuying Lin and Wentao Mao and Shangwang Liu and Zhi Dou and Guoqi Liu},
  doi          = {10.1016/j.asoc.2020.107061},
  journal      = {Applied Soft Computing},
  pages        = {107061},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid particle swarm and grey wolf optimizer and its application to clustering optimization},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive ranking based ensemble learning of gaussian process
regression models for quality-related variable prediction in process
industries. <em>ASOC</em>, <em>101</em>, 107060. (<a
href="https://doi.org/10.1016/j.asoc.2020.107060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proper monitoring of quality-related but hard-to-measure variables is currently one of the bottlenecks limiting the safe and efficient operations of industrial processes. This paper proposes a novel ensemble learning algorithm by coordinating global and local Gaussian process regression (GPR) models, and this algorithm is able to capture global and local process behaviours for accurate prediction and timely process monitoring. To further address the deterioration in predictions when using the off-line training and online testing strategy, this paper proposes an adaptive ranking strategy to perform ensemble learning for the sub-GPR models. In this adaptive strategy, we use the moving-window technique to rank and select several of the best sub-model predictions and then average them together to make the final predictions. Last but not least, the least absolute shrinkage and selection operator (Lasso) works together with factor analysis (FA) in a two-step variable selection method to remove under-correlated model input variables in the first stage and to compress over-correlated model input variables in the second stage. The proposed prediction model is validated in two real wastewater treatment plants (WWTPs) with stationary and nonstationary behaviours. The results show that the proposed methodology achieves better performance than other standard methods in the context of their predictions of quality-related variables.},
  archive      = {J_ASOC},
  author       = {Yiqi Liu and Daoping Huang and Bin Liu and Qiang Feng and Baoping Cai},
  doi          = {10.1016/j.asoc.2020.107060},
  journal      = {Applied Soft Computing},
  pages        = {107060},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive ranking based ensemble learning of gaussian process regression models for quality-related variable prediction in process industries},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time energy management for PV–battery–wind based
microgrid using on-line sequential kernel based robust random vector
functional link network. <em>ASOC</em>, <em>101</em>, 107059. (<a
href="https://doi.org/10.1016/j.asoc.2020.107059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper comprises of an effective Local Energy Management System (LEMS) implementing the Generalized Power Prediction Model (GPPM) for the uncertain performance of various Distributed Generations of a microgrid . The primary DGs considered to compensate intermittency is Wind Power Generation System (WPGS) and Photovoltaic (PV) along with Battery Energy Storage (BES). Solar and wind power are considered here as prediction targets and to cope with the intermittent nature of the Renewable Energy Sources (RESs), Power calculation from the RES data is included in the proposed prediction model. In order to establish arobust reduction in the predicted error, areal-time Online Sequential Kernel based Robust Random Vector Functional Link Network (OS-KRRVFLN) prediction algorithm is developed. Here, the initial randomness of the proposed OS-KRRVFLN is reduced (robust performance) by Hampel’s cost function ( e P eP based) minimization. The PV-BES and WPGS based DGs are designed to be operated under certain uncertainties like PV arc, bus faults etc. as uninterrupted power supply with minimum grid power dependency. An efficient Distributed Adaptive Droop (DAD) control is thus opted as Primary Controller (PC). Addictiveness of DAD’s operational region is estimated while compensating e P eP ”- . The coordination of GPPM with DAD-PC based accurate reference estimation for Independent DG Controllers (IDGCs) operation is considered as LEMS operation.},
  archive      = {J_ASOC},
  author       = {Irani Majumder and P.K. Dash and Snehamoy Dhar},
  doi          = {10.1016/j.asoc.2020.107059},
  journal      = {Applied Soft Computing},
  pages        = {107059},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time energy management for PV–battery–wind based microgrid using on-line sequential kernel based robust random vector functional link network},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective chaos-driven differential evolution for
multi-objective unbalanced transportation problem considering fuel
consumption. <em>ASOC</em>, <em>101</em>, 107058. (<a
href="https://doi.org/10.1016/j.asoc.2020.107058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on a special multi-objective unbalanced transportation problem (MOUTP) considering fuel consumption. MOUTP involves finding a transportation decision to minimize the supply–demand deviation (SDD) and the total transportation cost, which is different from conventional transportation problems. In this paper, the mathematical model of MOUTP is established, in which a new mechanism model of the fuel consumption based on the automobile theory is presented. To solve MOUTP, we propose an effective chaos-driven differential evolution (chaos-driven DE, CdDE). In CdDE, the solution representation and problem-specific heuristics based on the solution structures of MOUTP are presented first. Then, a decomposition-based multi-objective search framework integrating with DE is developed to execute the global exploration. Thereafter, a chaos-driven local search procedure based on the well-known Logistic map is proposed to enhance the local exploitation ability of CdDE. Numerical results and comparisons on 100 random instances and a real-life case study verify the effectiveness and practical values of the proposed CdDE.},
  archive      = {J_ASOC},
  author       = {Rongjuan Luo and Shoufeng Ji and Tingting Ji},
  doi          = {10.1016/j.asoc.2020.107058},
  journal      = {Applied Soft Computing},
  pages        = {107058},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective chaos-driven differential evolution for multi-objective unbalanced transportation problem considering fuel consumption},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topic detection and sentiment analysis in twitter content
related to COVID-19 from brazil and the USA. <em>ASOC</em>,
<em>101</em>, 107057. (<a
href="https://doi.org/10.1016/j.asoc.2020.107057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter is a social media platform with more than 500 million users worldwide. It has become a tool for spreading the news, discussing ideas and comments on world events. Twitter is also an important source of health-related information, given the amount of news, opinions and information that is shared by both citizens and official sources. It is a challenge identifying interesting and useful content from large text-streams in different languages, few works have explored languages other than English. In this paper, we use topic identification and sentiment analysis to explore a large number of tweets in both countries with a high number of spreading and deaths by COVID-19, Brazil , and the USA . We employ 3, 332, 565 tweets in English and 3, 155, 277 tweets in Portuguese to compare and discuss the effectiveness of topic identification and sentiment analysis in both languages. We ranked ten topics and analyzed the content discussed on Twitter for four months providing an assessment of the discourse evolution over time. The topics we identified were representative of the news outlets during April and August in both countries. We contribute to the study of the Portuguese language, to the analysis of sentiment trends over a long period and their relation to announced news, and the comparison of the human behavior in two different geographical locations affected by this pandemic. It is important to understand public reactions, information dissemination and consensus building in all major forms, including social media in different countries.},
  archive      = {J_ASOC},
  author       = {Klaifer Garcia and Lilian Berton},
  doi          = {10.1016/j.asoc.2020.107057},
  journal      = {Applied Soft Computing},
  pages        = {107057},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Topic detection and sentiment analysis in twitter content related to COVID-19 from brazil and the USA},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-criterion intelligent decision support system for
COVID-19. <em>ASOC</em>, <em>101</em>, 107056. (<a
href="https://doi.org/10.1016/j.asoc.2020.107056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is a buzz word nowadays. The deadly virus that started in China has spread worldwide. The fundamental principle is “if the disease can travel faster information has to travel even faster”. The sequence of events reveals the upheaval need to strengthen the ability of the early warning system , risk reduction, and management of national and global risks. Digital contact tracing apps like Aarogya setu (India) and Pan-European privacy preserving proximity tracing (German) has somehow helped but they are more effective in the initial stage and less relevant in the community spread phase. Thus, there is a need to devise a Decision Support System (DSS) based on machine learning algorithms . In this paper, we have attempted to propose an Additive Utility Assumption Approach for Criterion Comparison in Multi-criterion Intelligent Decision Support system for COVID-19. The dataset of Covid-19 has been taken from government link for validating the results. In this paper, an additive utility assumption-based approach for multi-criterion decision support system (MCDSS) with an accurate prediction of identified risk factors on certain well-defined input parameters is proposed and validated empirically using the standard SEIR model approach (Susceptible, Exposed, Infected and Recovered). The results includes comparative analysis in tabular form with already existing approaches to illustrate the potential of the proposed approach including the parameters such as Precision, Recall and F-Score. Other advanced parameters such as, MCC (Matthews Correlation Coefficient), ROC (Receiver Operating Characteristics) and PRC (Precision Recall) have also been considered for validation and the graphs are illustrated using Jupyter notebook. The statistical analysis of the most affected top eight states of India is undertaken effectively using the Weka software tool and IBM Cognos software to correctly predict the outbreak of pandemic situation due to Covid-19. Finally, the article has immense potential to contribute to the COVID-19 situation and may prove to be instrumental in propelling the research interest of researchers and providing some useful insights for the current pandemic situation.},
  archive      = {J_ASOC},
  author       = {Lakshita Aggarwal and Puneet Goswami and Shelly Sachdeva},
  doi          = {10.1016/j.asoc.2020.107056},
  journal      = {Applied Soft Computing},
  pages        = {107056},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criterion intelligent decision support system for COVID-19},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive binary artificial bee colony algorithm.
<em>ASOC</em>, <em>101</em>, 107054. (<a
href="https://doi.org/10.1016/j.asoc.2020.107054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristics and swarm intelligence algorithms are bio-inspired algorithms, which have long standing track record of success in problem solving. Due to the nature and the complexity of the problems, problem solving approaches may not achieve the same success level in every type of problems. Artificial bee colony (ABC) algorithm is a swarm intelligence algorithm and has originally been developed to solve numerical optimisation problems . It has a sound track record in numerical problems, but has not yet been tested sufficiently for combinatorial and binary problems. This paper proposes an adaptive hybrid approach to devise ABC algorithms with multiple and complementary binary operators for higher efficiency in solving binary problems. Three prominent operator selection schemes have been comparatively investigated for the best configuration in this regard. The proposed approach has been applied to uncapacitated facility location problems, a renown NP-Hard combinatorial problem type modelled with 0–1 programming, and successfully solved the well-known benchmarks outperforming state-of-art algorithms.},
  archive      = {J_ASOC},
  author       = {Rafet Durgut and Mehmet Emin Aydin},
  doi          = {10.1016/j.asoc.2020.107054},
  journal      = {Applied Soft Computing},
  pages        = {107054},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive binary artificial bee colony algorithm},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-sensor edge computing architecture for identification
of failures short-circuits in wind turbine generators. <em>ASOC</em>,
<em>101</em>, 107053. (<a
href="https://doi.org/10.1016/j.asoc.2020.107053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Towards connectivity and development of reliable systems, smarting sensors are vastly applied in hi-tech industries. The wind energy is a growing market that could benefit from edge processing technology by enhancing monitoring systems, decreasing downtime and guiding predictive maintenance . We proposed an embedded multi-sensor architecture to detect incipient short-circuit in wind turbine electrical generators, that is robust to both false positives and negatives. Five different sensor settings are tested in three feature extraction methods and four classifiers. An analysis of variance (ANOVA) and a Tukey’s honestly significant difference (HSD) statistical tests are used to determine which architectures should be embedded in a Raspberry Pi 3, NVIDIA Jetson TX2 and NVIDIA Xavier boards. A three current sensor setting with Fourier-MLP is the most suitable approach, achieving 81.20\% of accuracy, 0\% of false positive rate (FPR) and 0.08\% of false negative rate (FNR), also detecting generator’s normal conditions 100\% of the time. For a single sensor configuration, current sensor is the most suitable method for detecting fault or non-fault conditions, being 16 times more robust to false negatives than using an axial flux sensor . Comparing the processing time, the system embedded in a NVIDIA Xavier predicts a fault condition 37\% faster than in a Raspberry Pi 3, with Fourier-MLP and using a single current sensor, thus being the most suitable configuration in fault detection.},
  archive      = {J_ASOC},
  author       = {Yongzhao Xu and Navar Medeiros M. Nascimento and Pedro H. Feijó de Sousa and Fabrício G. Nogueira and Bismark C. Torrico and Tao Han and Chuanyu Jia and Pedro P. Rebouças Filho},
  doi          = {10.1016/j.asoc.2020.107053},
  journal      = {Applied Soft Computing},
  pages        = {107053},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-sensor edge computing architecture for identification of failures short-circuits in wind turbine generators},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 x-ray images classification based on enhanced
fractional-order cuckoo search optimizer using heavy-tailed
distributions. <em>ASOC</em>, <em>101</em>, 107052. (<a
href="https://doi.org/10.1016/j.asoc.2020.107052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of COVID-19 X-ray images to determine the patient’s health condition is a critical issue these days since X-ray images provide more information about the patient’s lung status. To determine the COVID-19 case from other normal and abnormal cases, this work proposes an alternative method that extracted the informative features from X-ray images, leveraging on a new feature selection method to determine the relevant features. As such, an enhanced cuckoo search optimization algorithm (CS) is proposed using fractional-order calculus (FO) and four different heavy-tailed distributions in place of the Lévy flight to strengthen the algorithm performance during dealing with COVID-19 multi-class classification optimization task . The classification process includes three classes, called normal patients, COVID-19 infected patients, and pneumonia patients. The distributions used are Mittag-Leffler distribution, Cauchy distribution , Pareto distribution , and Weibull distribution . The proposed FO-CS variants have been validated with eighteen UCI data-sets as the first series of experiments. For the second series of experiments, two data-sets for COVID-19 X-ray images are considered. The proposed approach results have been compared with well-regarded optimization algorithms. The outcomes assess the superiority of the proposed approach for providing accurate results for UCI and COVID-19 data-sets with remarkable improvements in the convergence curves, especially with applying Weibull distribution instead of Lévy flight.},
  archive      = {J_ASOC},
  author       = {Dalia Yousri and Mohamed Abd Elaziz and Laith Abualigah and Diego Oliva and Mohammed A.A. Al-qaness and Ahmed A. Ewees},
  doi          = {10.1016/j.asoc.2020.107052},
  journal      = {Applied Soft Computing},
  pages        = {107052},
  shortjournal = {Appl. Soft. Comput.},
  title        = {COVID-19 X-ray images classification based on enhanced fractional-order cuckoo search optimizer using heavy-tailed distributions},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tucker decomposition based knowledge distillation for
intelligent edge applications. <em>ASOC</em>, <em>101</em>, 107051. (<a
href="https://doi.org/10.1016/j.asoc.2020.107051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation(KD) has been proven an effective method in intelligent edge computing and have achieved extensive study in recent deep learning research. However, when the teacher network is too stronger compared to the student network, the effect of knowledge distillation is not ideal. Aiming at resolving this problem, an improved method of knowledge distillation (TDKD) is proposed, which enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. Firstly, the tucker-2 decomposition was performed on the convolutional layers of the original teacher model to reduce the capacity variance between the teacher network and student network. Then, the decomposed model will be used as a new teacher to participate in knowledge distillation for the student model. The experimental results show that the TDKD method can effectively solve the problem of poor distillation performance, which not only get better results if the KD method is effective, but also can reactivate the invalid KD method to some extents.},
  archive      = {J_ASOC},
  author       = {Cheng Dai and Xingang Liu and Zhuolin Li and Mu-Yen Chen},
  doi          = {10.1016/j.asoc.2020.107051},
  journal      = {Applied Soft Computing},
  pages        = {107051},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A tucker decomposition based knowledge distillation for intelligent edge applications},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advanced machine learning techniques for fake news (online
disinformation) detection: A systematic mapping study. <em>ASOC</em>,
<em>101</em>, 107050. (<a
href="https://doi.org/10.1016/j.asoc.2020.107050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news has now grown into a big problem for societies and also a major challenge for people fighting disinformation. This phenomenon plagues democratic elections, reputations of individual persons or organizations, and has negatively impacted citizens, (e.g., during the COVID-19 pandemic in the US or Brazil). Hence, developing effective tools to fight this phenomenon by employing advanced Machine Learning (ML) methods poses a significant challenge. The following paper displays the present body of knowledge on the application of such intelligent tools in the fight against disinformation. It starts by showing the historical perspective and the current role of fake news in the information war. Proposed solutions based solely on the work of experts are analysed and the most important directions of the application of intelligent systems in the detection of misinformation sources are pointed out. Additionally, the paper presents some useful resources (mainly datasets useful when assessing ML solutions for fake news detection) and provides a short overview of the most important R&amp;D projects related to this subject. The main purpose of this work is to analyse the current state of knowledge in detecting fake news; on the one hand to show possible solutions, and on the other hand to identify the main challenges and methodological gaps to motivate future research.},
  archive      = {J_ASOC},
  author       = {Michał Choraś and Konstantinos Demestichas and Agata Giełczyk and Álvaro Herrero and Paweł Ksieniewicz and Konstantina Remoundou and Daniel Urda and Michał Woźniak},
  doi          = {10.1016/j.asoc.2020.107050},
  journal      = {Applied Soft Computing},
  pages        = {107050},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advanced machine learning techniques for fake news (online disinformation) detection: A systematic mapping study},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skills2Job: A recommender system that encodes job offer
embeddings on graph databases. <em>ASOC</em>, <em>101</em>, 107049. (<a
href="https://doi.org/10.1016/j.asoc.2020.107049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a recommender system that, starting from a set of users’ skills, identifies the most suitable jobs as they emerge from a large dataset of Online Job Vacancies (OJVs). To this aim, we process 2.5M+ OJVs posted in three different countries (United Kingdom, France, and Germany), training several embeddings and performing an intrinsic evaluation of their quality. Besides, we compute a measure of skill importance for each occupation in each country, the Revealed Comparative Advantage ( rca ). The best vector model, one for each country, together with the rca , is used to feed a graph database , which will serve as the keystone for the recommender system . Results are evaluated through a user study of 10 labor market experts, using P@3 and nDCG as scores. Results show a high precision for the recommendations provided by skills2job , and the high values of nDCG (0.985 and 0.984 in a [0, 1] range) indicate a strong correlation between the experts’ scores and the rankings generated by skills2job .},
  archive      = {J_ASOC},
  author       = {Anna Giabelli and Lorenzo Malandri and Fabio Mercorio and Mario Mezzanzanica and Andrea Seveso},
  doi          = {10.1016/j.asoc.2020.107049},
  journal      = {Applied Soft Computing},
  pages        = {107049},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Skills2Job: A recommender system that encodes job offer embeddings on graph databases},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data gap analysis of ship and maritime data using meta
learning. <em>ASOC</em>, <em>101</em>, 107048. (<a
href="https://doi.org/10.1016/j.asoc.2020.107048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the developments in new Internet of Things (IoT) technology, ship owners and managers can access and analyze vast amounts of real-time onboard data. However, data collected from the ship’s engine and navigation information include various types of errors owing to imprecision , bias, sensor failure, or human error caused by manual recording by the ship operator’s hands. Such abnormal data make accurate ship performance prediction and monitoring difficult; thus, the original data need to be carefully refined through data preprocessing . Machine learning models have been used to detect and remove abnormal data; however, this yields inconsistent predictions in results depending on the types of machine learning models used. In this study, a robust data gap analysis method is proposed to detect abnormal data among ship and marine data collected in real time. Data preprocessing, including rule-based data imputation, clustering, denoising , and dimension reduction, is systematically performed; subsequently, a meta-learning model, which is a combination of various machine learning models, is used to predict ship performance and detect its abnormal data. This study compared the performance of single-machine learning models and meta models through various error analysis methods that utilize actual ship operation data. The single machine leading models have various error values depending on types of data and models, while the meta model consistently has values of less than 5\% of mean absolute percentage error (MAPE) and relative root-mean-square error (RRMSE), showing a Nash–Sutcliffe efficiency coefficient(NSE) value of 0.7 or higher. Our proposed data gap analysis framework, including the meta-learning model, can be useful for monitoring the condition of ships, as it can more accurately and robustly classify them as normal or abnormal.},
  archive      = {J_ASOC},
  author       = {Miyeon Jeon and Yoojeong Noh and Kyunghwan Jeon and Sangbong Lee and Inwon Lee},
  doi          = {10.1016/j.asoc.2020.107048},
  journal      = {Applied Soft Computing},
  pages        = {107048},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data gap analysis of ship and maritime data using meta learning},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty in bayesian deep label distribution learning.
<em>ASOC</em>, <em>101</em>, 107046. (<a
href="https://doi.org/10.1016/j.asoc.2020.107046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though widely used, most deep convolutional neural networks fail to capture prediction uncertainty, which can be crucial in scenarios such as automotive applications and disease diagnosis. Aleatoric and epistemic uncertainties have been proposed in the Bayesian deep learning framework for regression and classification, which require training images with unambiguous labels for success. Some situations do not have precise labels by nature, such as age estimation or lesion contour annotation by different physicians in the real world. Label distribution learning (LDL) has been proposed to account for the label ambiguity. However, uncertainty estimation has not been studied for LDL. This study presents a Bayesian deep label distribution learning (BLDL) to obtain the uncertainties of LDL tasks. We define semantic uncertainty to account for the incompleteness of data acquisition and inaccurate data labeling and unify the three types of uncertainties’ mathematical expressions for classification and LDL. We further demonstrate that applying the three types of uncertainties in the loss functions significantly improves model performance, and using semantic uncertainty as a criterion to select the training samples substantially promotes active learning.},
  archive      = {J_ASOC},
  author       = {Rui Zheng and Shulin Zhang and Lei Liu and Yuhao Luo and Mingzhai Sun},
  doi          = {10.1016/j.asoc.2020.107046},
  journal      = {Applied Soft Computing},
  pages        = {107046},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty in bayesian deep label distribution learning},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A linguistic multi-criteria decision making methodology for
the evaluation of tourist services considering customer opinion value.
<em>ASOC</em>, <em>101</em>, 107045. (<a
href="https://doi.org/10.1016/j.asoc.2020.107045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a consequence of the exponential growth in online data, tourism sector has experimented a radical transformation. From this large amount of information, opinion makers can be benefited for decision making in their purchase process. However, it can also harm them according to the information they consult. In fact, being benefited or harmed by the information translates into greater or lesser satisfaction after the purchase. This will largely depend on the published opinions that they take into account, which in turn depend on the value of the opinioner who publishes said information. In this paper, the authors propose a methodology that integrates multiple decision-making techniques and with which it is intended to obtain a ranking of hotels through the opinions of their past clients. To do this, the customer value is obtained using the Recency, Frequency, Helpfulness model. The information about the users found in the social networks is managed and aggregated using the fuzzy linguistic approach 2-tuples multi-granular. In addition, we have verified the functionality of this methodology by presenting a business case by applying it on TripAdvisor data.},
  archive      = {J_ASOC},
  author       = {Itzcóatl Bueno and Ramón A. Carrasco and Carlos Porcel and Gang Kou and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2020.107045},
  journal      = {Applied Soft Computing},
  pages        = {107045},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A linguistic multi-criteria decision making methodology for the evaluation of tourist services considering customer opinion value},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The context-based distance measure for intuitionistic fuzzy
set with application in marine energy transportation route decision
making. <em>ASOC</em>, <em>101</em>, 107044. (<a
href="https://doi.org/10.1016/j.asoc.2020.107044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distance measure is a classical topic in the intuitionistic fuzzy set theory . Although plenty of distance measures have been proposed and successfully applied to the decision-making problems, it is found that there still exists the counter-intuitive phenomenon where the context information in the alternatives is seldom considered in the existing distance measures. A context-based distance measure for the intuitionistic fuzzy set is proposed to solve this problem in this paper. The domination and competition relationships of the alternatives are integrated into the new distance measure. To fully take advantage of the proposed distance measure, a new similarity measure that utilizes the nonlinear function of the distance and additional parameters to control its discrimination capability is also defined. To demonstrate the effectiveness of the proposed information measures and their practical applications, an extended decision making method based on the order preference by similarity to ideal solutions is proposed. A practical case study of the marine energy transportation route decision making problem is provided as the validation. The comparative results comparing with other methods demonstrate the fine discrimination ability and effectiveness of the proposed methods.},
  archive      = {J_ASOC},
  author       = {Zhinan Hao and Zeshui Xu and Hua Zhao and Ren Zhang},
  doi          = {10.1016/j.asoc.2020.107044},
  journal      = {Applied Soft Computing},
  pages        = {107044},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The context-based distance measure for intuitionistic fuzzy set with application in marine energy transportation route decision making},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New imbalanced bearing fault diagnosis method based on
sample-characteristic oversampling TechniquE (SCOTE) and multi-class
LS-SVM. <em>ASOC</em>, <em>101</em>, 107043. (<a
href="https://doi.org/10.1016/j.asoc.2020.107043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In actual industrial production, the historical data sets used for bearing fault diagnosis are generally limited and imbalanced and consist of multiple classes. These problems present challenges in the field of bearing fault diagnosis, for which traditional fault diagnosis methods (e.g., multi-class least squares support vector machine (multi-class LS-SVM)) are not very effective. Therefore, we propose a new multi-class imbalanced fault diagnosis method based on Sample-characteristic Oversampling Technique (SCOTE) and multi-class LS-SVM, where SCOTE is a new oversampling method proposed by us. SCOTE transforms multi-class imbalanced problems into multiple binary imbalanced problems. In each binary imbalanced problem, first, SCOTE uses the k-nearest neighbours (knn) noise processing method to filter out noisy points. Second, samples are trained by LS-SVM, and minority samples are sorted by importance according to the misclassification error of the minority classes in the training sets. Moreover, based on the importance sorting of minority samples, SCOTE performs a sample synthesis method based on the k* information nearest neighbours (k*inn) to address the binary imbalanced problems. Thus, when all the binary imbalance problems are addressed, the multi-class imbalanced problem will also be addressed. The 20 fault diagnosis examples represented by Case Western Reserve University (CWRU) bearing data and Intelligent Maintenance Systems (IMS) bearing data show that the proposed method has higher fault diagnosis recognition rates and algorithm robustness than 8 oversampling algorithms and 8 multi-class imbalanced algorithms.},
  archive      = {J_ASOC},
  author       = {Jianan Wei and Haisong Huang and Liguo Yao and Yao Hu and Qingsong Fan and Dong Huang},
  doi          = {10.1016/j.asoc.2020.107043},
  journal      = {Applied Soft Computing},
  pages        = {107043},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New imbalanced bearing fault diagnosis method based on sample-characteristic oversampling TechniquE (SCOTE) and multi-class LS-SVM},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective evolutionary algorithm for steady-state
constrained multi-objective optimization problems. <em>ASOC</em>,
<em>101</em>, 107042. (<a
href="https://doi.org/10.1016/j.asoc.2020.107042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many multi-objective evolutionary algorithms (MOEAs) are developed to solve constrained multi-objective optimization problems (CMOPs). However, they encounter low efficiency for steady-state CMOPs which are to optimize a known feasible solution named the current steady-state operation point. This paper proposes a multi-objective evolutionary algorithm (MOEA), termed FACE, for tackling the steady-state CMOPs. In FACE, the known feasible solution is maintained in the second population and evolves together with the main population. The main population is evolved by global search without handling constraints to accelerate the convergence. And the second population is evolved by local search to hold and achieve more feasible solutions. Two kinds of performance comparison between FACE and five representative MOEAs are made: the first using two cases of steady-state multi-source compressed-air pipeline optimization problems to evaluate the performance of FACE on real life applications, and the second using a set of bench-mark test problems of CMOPs assigned with a known feasible solution to further assess the scalability of FACE. The results demonstrate the efficiency and the scalability of FACE with a potential for steady-state CMOPs.},
  archive      = {J_ASOC},
  author       = {Yongkuan Yang and Jianchang Liu and Shubin Tan},
  doi          = {10.1016/j.asoc.2020.107042},
  journal      = {Applied Soft Computing},
  pages        = {107042},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective evolutionary algorithm for steady-state constrained multi-objective optimization problems},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extracting salient information from discarded features via
attribute selection and pruning. <em>ASOC</em>, <em>101</em>, 107041.
(<a href="https://doi.org/10.1016/j.asoc.2020.107041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel two-step proposal for attribute subset selection. The first step is able to extract handy information from non-selected features after an initial feature subset selection to be added to the preliminary subset selected; for this step the feature subset selector relies on the Correlation-based Feature Selection (CFS). The second step tries to prune the solution given that in the first step some extra attributes out of initially non-selected features have been added to the relevant features; the ultimate goal of this pruning is to optimise the attribute subset prior to conduct a classification task . In accordance with the experimental results, the simplification of solutions is effective since an improvement takes place in most of the cases where the new method does determine some attributes to be removed. Some performance measures are reported within a test bed composed of a good number of binary and multi-class classification problems. Comparisons with other attribute selection procedures such as CFS, Fast Correlation-based Feature Selection and Gain Ratio revealed that the new approach is very competitive and may be taken into account by the data preparation community.},
  archive      = {J_ASOC},
  author       = {Antonio J. Tallón-Ballesteros and Sung-Bae Cho},
  doi          = {10.1016/j.asoc.2020.107041},
  journal      = {Applied Soft Computing},
  pages        = {107041},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extracting salient information from discarded features via attribute selection and pruning},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Agent-based automated dynamic SLA negotiation framework in
the cloud using the stochastic optimization approach. <em>ASOC</em>,
<em>101</em>, 107040. (<a
href="https://doi.org/10.1016/j.asoc.2020.107040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization of the broker negotiation strategy is one of the challenging issues observed in the cloud-based e-commerce negotiation framework. This strategy can be optimized either in the context of pre-request optimization or long-term optimization. Most researchers have focused on the context of pre-request optimization using various utility functions (such as time, opportunity, and competition). In the context of long-term optimization, the current state-of-the-art negotiation strategies can increase the utility value and success rate of the parties to some extent, but they do not guarantee minimization of the negotiation states (rounds) involved during the negotiation process. In addition, the existing strategies cannot react to the stochastic, rational, emotional, and unknown behaviour of the opponents due to their deterministic behaviour, which may lead to a negotiation break-off between parties. To overcome such limitations, a novel stochastic behavioural learning negotiation (SBLN) strategy is proposed to further maximize the utility value and success rate.},
  archive      = {J_ASOC},
  author       = {Rajkumar Rajavel and Mala Thangarathanam},
  doi          = {10.1016/j.asoc.2020.107040},
  journal      = {Applied Soft Computing},
  pages        = {107040},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Agent-based automated dynamic SLA negotiation framework in the cloud using the stochastic optimization approach},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time measurement of the uncertain epidemiological
appearances of COVID-19 infections. <em>ASOC</em>, <em>101</em>, 107039.
(<a href="https://doi.org/10.1016/j.asoc.2020.107039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virus diseases are a continued threat to human health in both community and healthcare settings. The current virus disease COVID-19 outbreak raises an unparalleled public health issue for the world at large. Wuhan is the city in China from where this virus came first and, after some time the whole world was affected by this severe disease. It is a challenge for every country’s people and higher authorities to fight with this battle due to the insufficient number of resources. On-going assessment of the epidemiological features and future impacts of the COVID-19 disease is required to stay up-to-date of any changes to its spread dynamics and foresee needed resources and consequences in different aspects as social or economic ones. This paper proposes a prediction model of confirmed and death cases of COVID-19. The model is based on a deep learning algorithm with two long short-term memory (LSTM) layers. We consider the available infection cases of COVID-19 in India from January 22, 2020, till October 9, 2020, and parameterize the model. The proposed model is an inference to obtain predicted coronavirus cases and deaths for the next 30 days, taking the data of the previous 260 days of duration of the pandemic. The proposed deep learning model has been compared with other popular prediction methods (Support Vector Machine, Decision Tree and Random Forest) showing a lower normalized RMSE . This work also compares COVID-19 with other previous diseases (SARS, MERS, h1n1, Ebola, and 2019-nCoV). Based on the mortality rate and virus spread, this study concludes that the novel coronavirus (COVID-19) is more dangerous than other diseases.},
  archive      = {J_ASOC},
  author       = {Meenu Gupta and Rachna Jain and Soham Taneja and Gopal Chaudhary and Manju Khari and Elena Verdú},
  doi          = {10.1016/j.asoc.2020.107039},
  journal      = {Applied Soft Computing},
  pages        = {107039},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time measurement of the uncertain epidemiological appearances of COVID-19 infections},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A stacking-based ensemble learning method for earthquake
casualty prediction. <em>ASOC</em>, <em>101</em>, 107038. (<a
href="https://doi.org/10.1016/j.asoc.2020.107038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of the loss and prediction of the casualties in earthquake-stricken areas are vital for making rapid and accurate decisions during rescue efforts. The number of casualties is determined by various factors, necessitating a comprehensive system for earthquake-casualty prediction. To obtain accurate prediction results, an effective prediction method based on stacking ensemble learning and improved swarm intelligence algorithm is proposed in this study, which comprises three parts: (1) applying multiple base learners for training, (2) using a stacking strategy to integrate the results generated by multiple base learners to obtain the final prediction results, and (3) developing an improved swarm intelligence algorithm to optimize the key parameters in the prediction model. To verify the effectiveness of the model, we collected data pertaining to earthquake destruction from 1966 to 2017 in China. Experiments were conducted to compare the proposed method with popular machine learning methods. It was found that the stacking ensemble learning method can effectively integrate the prediction results of the base learner to improve the performance of the model, and the improved swarm intelligence algorithm can further improve the prediction accuracy. Moreover, the importance of each feature was evaluated, which has important implications for future work such as casualty prevention and rescue during earthquakes.},
  archive      = {J_ASOC},
  author       = {Shaoze Cui and Yunqiang Yin and Dujuan Wang and Zhiwu Li and Yanzhang Wang},
  doi          = {10.1016/j.asoc.2020.107038},
  journal      = {Applied Soft Computing},
  pages        = {107038},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A stacking-based ensemble learning method for earthquake casualty prediction},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient method using whale optimization algorithm for
reliability-based design optimization of labyrinth spillway.
<em>ASOC</em>, <em>101</em>, 107036. (<a
href="https://doi.org/10.1016/j.asoc.2020.107036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spillways are essential parts of dams, in which the main task of these structures is to allow the passing of excess water and floods from the upstream to the downstream. In this regards, the main goal of this paper is proposing a novel framework for the probabilistic design of labyrinth spillway structures using a new developed reliability-based design optimization (RBDO) approach. In this RBDO approach, the total volume of the spillway is considered as the objective function of the optimization problem under uncertainties, while the labyrinth spillway parameters are considered as the design variables. Hereafter, to solve the formulated RBDO problem of the labyrinth spillway design, a new proposed model that consist of coupling the Monte Carlo Simulation (MCS) with a hybrid Artificial Neural Network (ANN) based Whale Optimization Algorithm (WOA) model is developed. The hybrid ANN-WOA is utilized to approximate the labyrinth spillway response in order to reduce the computational cost during the RBDO analysis. The proposed MCS-ANN-WOA model was implemented on the Ute dam labyrinth spillway at Logan, New Mexico (USA). The obtained results showed that the proposed RBDO model performance is more accurate and robust compared to the deterministic optimization (DO) approaches for an optimal design of the labyrinth spillway shape with the consideration of the safety levels.},
  archive      = {J_ASOC},
  author       = {Jafar Jafari-Asl and Mohamed El Amine Ben Seghier and Sima Ohadi and Pieter van Gelder},
  doi          = {10.1016/j.asoc.2020.107036},
  journal      = {Applied Soft Computing},
  pages        = {107036},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient method using whale optimization algorithm for reliability-based design optimization of labyrinth spillway},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linguistic scale consistency issues in multi-granularity
decision making contexts. <em>ASOC</em>, <em>101</em>, 107035. (<a
href="https://doi.org/10.1016/j.asoc.2020.107035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The symbolic model based on the linguistic scale has been widely used to represent linguistic knowledge to deal with various linguistic decision problems. However, linguistic scales with different granularity may yield inconsistent decision outcomes in the linguistic decision making. Thus, this paper systematically studies the linguistic scale consistency issues in multi-granularity decision making contexts. We first define the concepts of the consistent multi-granularity representation, consistent multi-granularity aggregation and consistent multi-granularity ranking. After that, we analytically present a necessary and sufficient condition to guarantee the consistent multi-granularity representation and a sufficient condition to characterize the intrinsic mechanism of the consistent multi-granularity aggregation. Then, an attitude-based linguistic representation method (ALRM) is proposed to improve the consistent multi-granularity ranking. Finally, a detailed numerical analysis and simulation experiments are presented to show the advantages of the ALRM over the traditional linguistic approach. These results will provide new insights into the use of linguistic scales in the linguistic decision making.},
  archive      = {J_ASOC},
  author       = {Sihai Zhao and Yucheng Dong and Siqi Wu and Luis Martínez},
  doi          = {10.1016/j.asoc.2020.107035},
  journal      = {Applied Soft Computing},
  pages        = {107035},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Linguistic scale consistency issues in multi-granularity decision making contexts},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective retrieval method for 3D models in plastic
injection molding for process reuse. <em>ASOC</em>, <em>101</em>,
107034. (<a href="https://doi.org/10.1016/j.asoc.2020.107034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide application of 3D models in computer-aided engineering (CAE) has created an urgent need for 3D model retrieval systems in manufacturing. However, recent methods are mainly based on the shape similarity of models, limiting reuse of the manufacturing process information associated with retrieved models. In this paper, we present a novel 3D model retrieval method for plastic injection molding involving process-related features. An effective feature is proposed to characterize both the geometry and process information of the 3D model, using the pressure profile based on the molding process . A variational autoencoder (VAE) is utilized to refine process-related features through unsupervised learning to improve retrieval efficiency. A 3D model database containing 120 models in actual production was built for validation experiments. The experimental results show that the proposed encoded pressure feature outperforms conventional methods with an accuracy of 86.61\% compared to 78.57\% using shape distribution and 73.21\% using numerical features. A retrieval application proves that the information of retrieved models can be reused by a new product through the proposed method. There is considerable potential for utilizing the proposed method in similar manufacturing fields.},
  archive      = {J_ASOC},
  author       = {Fei Guo and Jiahuan Liu and Xiaowei Zhou and Hui Wang and Yun Zhang and Dequn Li and Huamin Zhou},
  doi          = {10.1016/j.asoc.2020.107034},
  journal      = {Applied Soft Computing},
  pages        = {107034},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective retrieval method for 3D models in plastic injection molding for process reuse},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EUSC: A clustering-based surrogate model to accelerate
evolutionary undersampling in imbalanced classification. <em>ASOC</em>,
<em>101</em>, 107033. (<a
href="https://doi.org/10.1016/j.asoc.2020.107033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from imbalanced datasets is highly demanded in real-world applications and a challenge for standard classifiers that tend to be biased towards the classes with the majority of the examples. Undersampling approaches reduce the size of the majority class to balance the class distributions. Evolutionary-based approaches are prominent, treating undersampling as a binary optimisation problem that determines which examples are removed. However, their utilisation is limited to small datasets due to fitness evaluation costs. This work proposes a two-stage clustering-based surrogate model that enables evolutionary undersampling to compute fitness values faster. The main novelty lies in the development of a surrogate model for binary optimisation which is based on the meaning (phenotype) rather than their binary representation (genotype). We conduct an evaluation on 44 imbalanced datasets, showing that in comparison with the original evolutionary undersampling, we can save up to 83\% of the runtime without significantly deteriorating the classification performance.},
  archive      = {J_ASOC},
  author       = {Hoang Lam Le and Dario Landa-Silva and Mikel Galar and Salvador Garcia and Isaac Triguero},
  doi          = {10.1016/j.asoc.2020.107033},
  journal      = {Applied Soft Computing},
  pages        = {107033},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EUSC: A clustering-based surrogate model to accelerate evolutionary undersampling in imbalanced classification},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-objective genetic algorithm for assembly planning
and supplier selection with capacity constraints. <em>ASOC</em>,
<em>101</em>, 107030. (<a
href="https://doi.org/10.1016/j.asoc.2020.107030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study evaluates the supplier selection problem encountered when using multiple assembly plants with production capacity constraints to produce multiple products. It also applies assembly sequence planning (ASP) and assembly task assignment to production planning for the overall supply chain to determine optimal supplier combinations and production resource allocation. A multi-objective optimization mathematical model was constructed and a modified multi-objective algorithm was used to solve the optimization model. Developed on the basis of the nondominated sorting genetic algorithm II (NSGA-II), this modified algorithm incorporates an algorithmic and determination mechanism involving the greatest number of following tasks and the longest task time when initial solutions are generated. This increases the efficiency of solutions derived from the NSGA-II, here referred to as the W-NSGA2. The W-NSGA2 is then applied to the actual case of a faucet assembly task, and solutions derived by the W-NSGA2 are compared with those derived by the NSGA-II and the nondominated sorting particle swarm optimizer (NSPSO). Results show that the W-NSGA2 yields better solutions.},
  archive      = {J_ASOC},
  author       = {Z.H. Che and Tzu-An Chiang and Tzu-Ting Lin},
  doi          = {10.1016/j.asoc.2020.107030},
  journal      = {Applied Soft Computing},
  pages        = {107030},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective genetic algorithm for assembly planning and supplier selection with capacity constraints},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WSMN: An optimized multipurpose blind watermarking in
shearlet domain using MLP and NSGA-II. <em>ASOC</em>, <em>101</em>,
107029. (<a href="https://doi.org/10.1016/j.asoc.2020.107029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital watermarking is a remarkable issue in the field of information security to avoid the misuse of images in multimedia networks. Although access to unauthorized persons can be prevented through cryptography, it cannot be simultaneously used for copyright protection or content authentication with the preservation of image integrity. Hence, this paper presents an optimized multipurpose blind watermarking in the Shearlet domain with the help of smart algorithms, including MLP and NSGA-II. In this method, four copies of the robust copyright logo are embedded in the approximate coefficients of Shearlet by using an effective quantization technique. Furthermore, an embedded random sequence as a semi-fragile authentication mark is effectively extracted from details by the neural network . Due to performing an effective optimization algorithm for selecting optimum embedding thresholds and also distinguishing the texture of blocks, the imperceptibility and robustness have been preserved. The experimental results reveal the superiority of scheme with regard to the quality of watermarked images and robustness against hybrid attacks over other state-of-the-art schemes. The average PSNR and SSIM of the dual watermarked images are 38 dB and 0.95, respectively; Besides, it can effectively extract the copyright logo and locates forgery regions under severe attacks with satisfactory accuracy.},
  archive      = {J_ASOC},
  author       = {Behrouz Bolourian Haghighi and Amir Hossein Taherinia and Ahad Harati and Modjtaba Rouhani},
  doi          = {10.1016/j.asoc.2020.107029},
  journal      = {Applied Soft Computing},
  pages        = {107029},
  shortjournal = {Appl. Soft. Comput.},
  title        = {WSMN: An optimized multipurpose blind watermarking in shearlet domain using MLP and NSGA-II},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous emotion recognition during music listening using
EEG signals: A fuzzy parallel cascades model. <em>ASOC</em>,
<em>101</em>, 107028. (<a
href="https://doi.org/10.1016/j.asoc.2020.107028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A controversial issue in artificial intelligence is human emotion recognition. This paper presents a fuzzy parallel cascades (FPC) model for predicting the continuous subjective emotional appraisal of music by time-varying spectral content of electroencephalogram (EEG) signals. The EEG, along with an emotional appraisal of 15 subjects, was recorded during listening to seven musical excerpts. The emotional appraisement was recorded along the valence and arousal emotional axes as a continuous signal. The FPC model was composed of parallel cascades with each cascade containing a fuzzy logic-based system. The FPC model performance was evaluated using linear regression (LR), support vector regression (SVR), and Long–Short-Term-Memory recurrent neural network (LSTM-RNN) models by 4 fold cross-validation. The root mean square error (RMSE) of the FPC was lower than other models in the estimation of both valence and arousal of all musical excerpts. The lowest obtained RMSE was 0.082, which was acquired by the FPC model. The analysis of mutual information of frontal EEG with the valence confirms the role of frontal channels in the theta frequency band in emotion recognition. Considering the dynamic variations of musical features during songs, employing a modeling approach to predict dynamic variations of the emotional appraisal can be a plausible substitute for the classification of musical excerpts into predefined labels.},
  archive      = {J_ASOC},
  author       = {Fatemeh Hasanzadeh and Mohsen Annabestani and Sahar Moghimi},
  doi          = {10.1016/j.asoc.2020.107028},
  journal      = {Applied Soft Computing},
  pages        = {107028},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Continuous emotion recognition during music listening using EEG signals: A fuzzy parallel cascades model},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A heredity-based adaptive variation operator for
reinitialization in dynamic multi-objective problems. <em>ASOC</em>,
<em>101</em>, 107027. (<a
href="https://doi.org/10.1016/j.asoc.2020.107027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reinitialization approach is an effective way of generalizing a static multi-objective optimization method to a dynamic one. It is usually comprised of a prediction operator for predicting the approximate location(s) of the optimal solution(s) and a variation operator for enhancing the diversity of the reinitialized solution(s) after a change. While many recent studies have focused on prediction methods, the importance of the variation operator has usually been overlooked. This study systematically explores the effects of the accuracy of the prediction method employed as well as the frequency and severity of the change on the optimal strength of the variation used for reinitialization. Subsequently, it introduces an adaptive variation operator for dynamic multi-objective optimization which can learn the optimal variation strength on-the-fly. To develop this method, firstly, a heredity measure for evolutionary algorithms is formulated to quantify the contribution of each reinitialized solution to the optimization process by measuring the presence of its traits in the final population. Some carefully designed descriptive simulations are performed to explore the capability of the proposed method to learn the optimal variation strength and its sensitivity to the change severity, initial variation strength, and accuracy of the employed prediction method. Finally, the performance of this variation operator on 42 dynamic multi-objective test problems is compared with those of five other popular ones, with numerical comparisons revealing its superior learning capability.},
  archive      = {J_ASOC},
  author       = {Ali Ahrari and Saber Elsayed and Ruhul Sarker and Daryl Essam and Carlos A. Coello Coello},
  doi          = {10.1016/j.asoc.2020.107027},
  journal      = {Applied Soft Computing},
  pages        = {107027},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A heredity-based adaptive variation operator for reinitialization in dynamic multi-objective problems},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binary biogeography-based optimization based SVM-RFE for
feature selection. <em>ASOC</em>, <em>101</em>, 107026. (<a
href="https://doi.org/10.1016/j.asoc.2020.107026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid data growth presents many challenges for Machine Learning (ML) tasks as it can include lots of irrelevant, noisy, and redundant features. Thus, it is vital to select the most relevant features to the classification task , known as Feature Selection (FS). The main goal of FS techniques is to maximize the performance of a classification task while keeping the number of features to a minimum. In this study, a hybrid metaheuristic model is designed to solve FS problems based on Binary Biogeography Optimization (BBO) followed by the application of Support Vector Machine Recursive Feature Elimination (SVM-RFE), known as BBO-SVM-RFE. The SVM-RFE is embedded into the BBO to improve the quality of the obtained solutions in the mutation operator in order to enhance the exploitation capability as well as striking an adequate balance between exploitation and exploration of the original BBO. The proposed BBO-SVM-RFE method for solving FS problems was assessed on eighteen benchmark datasets. Comparative results showed that the BBO-SVM-RFE method outperforms the BBO method and other existing wrapper and filter methods in terms of accuracy and number of selected features. The obtained results reveal the high potentiality of BBO-SVM-RFE in reliably searching the feature space to obtain the optimal combination of features.},
  archive      = {J_ASOC},
  author       = {Dheeb Albashish and Abdelaziz I. Hammouri and Malik Braik and Jaffar Atwan and Shahnorbanun Sahran},
  doi          = {10.1016/j.asoc.2020.107026},
  journal      = {Applied Soft Computing},
  pages        = {107026},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Binary biogeography-based optimization based SVM-RFE for feature selection},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing the effectiveness of semi-supervised learning
approaches for opinion spam classification. <em>ASOC</em>, <em>101</em>,
107023. (<a href="https://doi.org/10.1016/j.asoc.2020.107023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opinion spam detection is concerned with identifying fake reviews that are deliberately placed to either promote or discredit a product. Opinionated social media like product reviews are increasingly important resources for people as well as businesses in the decision-making process and can be easily manipulated by opportunistic individuals. To reduce this increasing impact of opinion spams , opinion spam detection approaches have been proposed, which adopt mostly supervised classification methods. However, in practice, the provided data is largely not labeled and therefore semi-supervised learning approaches are required instead. To this end, this study aims to analyze the effectiveness of several semi-supervised learning approaches for opinion spam classification. Four different semi-supervised methods are evaluated on a dataset of both genuine and deceptive hotel reviews. The results are compared with several traditional classification methods using the same amount of labeled data. According to this study, the self-training algorithm with Naive Bayes as the base classifier yields 93\% accuracy. Results show that self-training is the only approach, out of the four tested semi-supervised models, that outperforms traditional supervised classification models when limited data is available. This study further shows that self-training can mitigate labeling efforts while retaining high model performance, which is useful for scenarios where limited data is available or retrieving labeled data is more costly.},
  archive      = {J_ASOC},
  author       = {Alexander Ligthart and Cagatay Catal and Bedir Tekinerdogan},
  doi          = {10.1016/j.asoc.2020.107023},
  journal      = {Applied Soft Computing},
  pages        = {107023},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analyzing the effectiveness of semi-supervised learning approaches for opinion spam classification},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SARA: A memetic algorithm for high-dimensional biomedical
data. <em>ASOC</em>, <em>101</em>, 107009. (<a
href="https://doi.org/10.1016/j.asoc.2020.107009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past two decades, large amounts of biomedical and clinical data have been generated. These high dimensional datasets contain thousands of genes. However, such datasets contain many irrelevant genes which influence the predictive accuracy of diagnosis. Therefore, to select the relevant genes from the dataset and to accurately identify the patterns in the genes, it is necessary to employ some gene selection and classification algorithms . In this work, a hybrid algorithm is proposed using simulated annealing (SA) and Rao algorithm (RA) for selecting the optimal gene subset and classifying cancer. SA works as a local search strategy and RA works as a global optimization framework. The reason for combining SA in RA is to improve the exploitation capability of RA. The proposed method consists of two stages. In the first stage, minimum redundancy maximum relevance (mRMR) is employed to select the relevant gene subsets from the microarray dataset. Then, SA is hybridized with RA to improve the quality of solutions after every iteration of RA. Log sigmoidal function is introduced as an encoding scheme to transform the continuous version of Simulated annealing-Rao algorithm (SARA) to a discrete optimization algorithm . The performance of our approach is tested on three binary-class and four multi-class datasets. A comparative study is carried out with eighteen existing techniques. Results from the experiments have shown that our proposed approach selects discriminating genes with high classification accuracy . Particularly, it achieves high classification accuracy on the SRBCT dataset with 99.81\% with only five informative genes.},
  archive      = {J_ASOC},
  author       = {Santos Kumar Baliarsingh and Khan Muhammad and Sambit Bakshi},
  doi          = {10.1016/j.asoc.2020.107009},
  journal      = {Applied Soft Computing},
  pages        = {107009},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SARA: A memetic algorithm for high-dimensional biomedical data},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy best-worst method and its application in initial water
rights allocation. <em>ASOC</em>, <em>101</em>, 107007. (<a
href="https://doi.org/10.1016/j.asoc.2020.107007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Allocating initial water rights in a river basin is not only a significant problem but also an essential research topic. It plays a crucial role in promoting the construction of a system of water rights and water-saving society as well as alleviating the contradiction between water supply and demand. Thus, the effective initial allocation of water rights has theoretical and practical significance for enhancing water rights management in China. First, this paper analyzes the impact of the strictest water resources management system on the establishment of the main criteria in the initial water rights allocation index system of a basin, including social, economic and environmental benefit criteria. Then, a new fuzzy comparison-based method, namely the fuzzy best-worst method (FBWM), is proposed to determine the weights of criteria and a procedure for group decision-making by FBWM is put forward. Finally, a case study about the initial water rights allocation in the Huaihe River Basin is illustrated to show how the developed FBWM works for real-world applications. Sensitivity analysis and comparative analysis with the existing methods are also provided to show the performance of the proposed method.},
  archive      = {J_ASOC},
  author       = {Yejun Xu and Xiaotong Zhu and Xiaowei Wen and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2020.107007},
  journal      = {Applied Soft Computing},
  pages        = {107007},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy best-worst method and its application in initial water rights allocation},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WBC-net: A white blood cell segmentation network based on
UNet++ and ResNet. <em>ASOC</em>, <em>101</em>, 107006. (<a
href="https://doi.org/10.1016/j.asoc.2020.107006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The counting and identification of white blood cells (WBCs, i.e., leukocytes) in blood smear images play a crucial role in the diagnosis of certain diseases, including leukemia, infections, and COVID-19 (corona virus disease 2019). WBC image segmentation lays a firm foundation for automatic WBC counting and identification. However, automated WBC image segmentation is challenging due to factors such as background complexity and variations in appearance caused by histological staining conditions. To improve WBC image segmentation accuracy , we propose a deep learning network called WBC-Net, which is based on UNet++ and ResNet . Specifically, WBC-Net designs a context-aware feature encoder with residual blocks to extract multi-scale features, and introduces mixed skip pathways on dense convolutional blocks to obtain and fuse image features at different scales. Moreover, WBC-Net uses a decoder incorporating convolution and deconvolution to refine the WBC segmentation mask. Furthermore, WBC-Net defines a loss function based on cross-entropy and the Tversky index to train the network. Experiments on four image datasets show that the proposed WBC-Net achieves better WBC segmentation performance than several state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Yan Lu and Xuejun Qin and Haoyi Fan and Taotao Lai and Zuoyong Li},
  doi          = {10.1016/j.asoc.2020.107006},
  journal      = {Applied Soft Computing},
  pages        = {107006},
  shortjournal = {Appl. Soft. Comput.},
  title        = {WBC-net: A white blood cell segmentation network based on UNet++ and ResNet},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A knowledge coverage-based trust propagation for
recommendation mechanism in social network group decision making.
<em>ASOC</em>, <em>101</em>, 107005. (<a
href="https://doi.org/10.1016/j.asoc.2020.107005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust is a typical relationship in social network, which in group decision making problems relates to the inner relationship among experts. To obtain a complete trust relationship of a networked group of experts, firstly, a novel knowledge coverage-based trust propagation operator is proposed to estimate the trust relationship between pairs of unknown experts. The novelty of this trust propagation operator resides in its account of the domain knowledge coverage of experts. Desirable properties regarding boundary conditions, generalisation and knowledge coverage absorption are studied. The comparison with existing operators of boundary conditions shows the rationality of the proposed operator. Next, a knowledge coverage-based multi-paths trust propagation model for constructing complete trust network is investigated. The proposed approach aggregates all trust paths to collect all trust information and penalise trust decay. Secondly, a trust order induced recommendation mechanism is proposed by combining subjective and objective weights. Thus, experts can accept consensus recommendations by subjective and objective trust. This recommendation mechanism allows the inconsistent experts to accept the advices they trust. The validity and rationality of the proposed recommendation mechanism is mathematically proved, and a numerical example is utilised to illustrate the calculation process of the proposed method.},
  archive      = {J_ASOC},
  author       = {Yujia Liu and Changyong Liang and Francisco Chiclana and Jian Wu},
  doi          = {10.1016/j.asoc.2020.107005},
  journal      = {Applied Soft Computing},
  pages        = {107005},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A knowledge coverage-based trust propagation for recommendation mechanism in social network group decision making},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective evolution strategy for multimodal
multi-objective optimization. <em>ASOC</em>, <em>101</em>, 107004. (<a
href="https://doi.org/10.1016/j.asoc.2020.107004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades, various effective and efficient multi-objective evolutionary algorithms (MOEAs) have been proposed for solving multi-objective optimization problems. However, existing MOEAs cannot satisfactorily address multimodal multi-objective optimization problems that demand to find multiple groups of optimal solutions simultaneously. In this paper, we propose an evolution strategy to solve multimodal multi-objective optimization problems, named MMO-MOES. This paper focus on searching for well-converged and well-distributed solutions in the decision space. Firstly, a novel niching strategy in the decision space, which imitates the repulsive force among isotropic magnetic particles, is adopted to drive the individuals to preserve uniform distances from each other and spread to the whole Pareto set automatically. This strategy is effective in finding multiple groups of optimal solutions simultaneously. Secondly, MMO-MOES requires only a very small population size to obtain a well-distributed and well-converged set of Pareto optimal solutions in the decision space. The greater the population size, the clearer contour of the approximate Pareto sets and Pareto front will be. Finally, the MMO-MOES is compared against some chosen leading-edge MMOEAs. The experimental results demonstrate that MMO-MOES provides exceptional performance in searching for the complete Pareto subsets and Pareto front on Omni-test problem, Symmetrical Parts (SYM-PART) problems, and CEC 2019 Multimodal Multi-Objective Optimization Problems (MMOPs) test suite.},
  archive      = {J_ASOC},
  author       = {Kai Zhang and Minshi Chen and Xin Xu and Gary G. Yen},
  doi          = {10.1016/j.asoc.2020.107004},
  journal      = {Applied Soft Computing},
  pages        = {107004},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective evolution strategy for multimodal multi-objective optimization},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedded stacked group sparse autoencoder ensemble with l1
regularization and manifold reduction. <em>ASOC</em>, <em>101</em>,
107003. (<a href="https://doi.org/10.1016/j.asoc.2020.107003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning useful representations from original features is a key issue in classification tasks . Stacked autoencoders (SAEs) are easy to understand and realize, and they are powerful tools that learn deep features from original features, so they are popular for classification problems. The deep features can further combine the original features to construct more representative features for classification. However, existing SAEs do not consider the original features within the network structure and during training, so the deep features have low complementarity with the original features. To solve the problem, this paper proposes an embedded stacked group sparse autoencoder (ESGSAE) for more effective feature learning . Different from traditional stacked autoencoders , the ESGSAE model considers the complementarity between the original feature and the hidden outputs by embedding the original features into hidden layers. To alleviate the impact of the small sample problem on the generalization of the proposed ESGSAE model, an L 1 L1 regularization-based feature selection strategy is designed to further improve the feature quality. After that, an ensemble model with support vector machine (SVM) and weighted local discriminant preservation projection (w_LPPD) is designed to further enhance the feature quality. Based on the designs above, an embedded stacked group sparse autoencoder ensemble with L 1 regularization and manifold reduction is proposed to obtain deep features with high complementarity in the context of the small sample problem. At the end of this paper, several representative public datasets are used for verification of the proposed algorithm. The results demonstrate that the ESGSAE ensemble model with L 1 L1 regularization and manifold reduction yields superior performance compared to other existing and state-of-the-art feature learning algorithms, including some representative deep stacked autoencoder methods. Specifically, compared with the original features, the representative feature extraction algorithms and the improved autoencoders, the algorithm proposed in this paper can improve the classification accuracy by up to 13.33\%, 7.33\%, and 9.55\%, respectively. The data and codes can be found in: https://share.weiyun.com/Jt7qeORm},
  archive      = {J_ASOC},
  author       = {Yongming Li and Yan Lei and Pin Wang and Mingfeng Jiang and Yuchuan Liu},
  doi          = {10.1016/j.asoc.2020.107003},
  journal      = {Applied Soft Computing},
  pages        = {107003},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Embedded stacked group sparse autoencoder ensemble with l1 regularization and manifold reduction},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective optimization using self-organizing
decomposition and its application to crashworthiness design.
<em>ASOC</em>, <em>101</em>, 107002. (<a
href="https://doi.org/10.1016/j.asoc.2020.107002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are three main classic decomposition approaches, i.e., weighted sum (WS), Tchebycheff (TCH) and penalty-boundary based intersection (PBI), which have different improvement regions (IRs) during the evolutionary search process. Hence, these three decomposition approaches have their own corresponding advantages and weaknesses on the performance of convergence or diversity due to their different features of IRs. In this paper, in order to take advantages of different decomposition methods , a novel multi-objective evolutionary algorithm using a self-organizing decomposition selection strategy (called MOEA/D-SDSS) is presented for solving multi-objective optimization problems (MOPs) and the crashworthiness design problem. In this approach, two competitive populations are respectively run based on the TCH and PBI decomposition methods . Then, based on their respective performance on both diversity and convergence, a self-organizing selection strategy is performed during the whole evolutionary process, which aims to select one suitable decomposition approach for the next generation. In other words, the two populations based on different decomposition approaches are complementary with each other, which is crucial and beneficial for improving the robustness of evolutionary algorithms on solving various kinds of MOPs. At last, one population with a better performance will be outputted as the final result. In order to evaluate the performance of our proposed MOEA/D-SDSS, thirty-six well-known benchmark problems and the crashworthiness design problem are studied in our experiments. The experimental results validate the promising performance of our proposed algorithm over six state-of-the-art decomposition-based evolutionary algorithms and six other competitive evolutionary algorithms.},
  archive      = {J_ASOC},
  author       = {Lingjie Li and Qiuzhen Lin and Zhong Ming},
  doi          = {10.1016/j.asoc.2020.107002},
  journal      = {Applied Soft Computing},
  pages        = {107002},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimization using self-organizing decomposition and its application to crashworthiness design},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bi-objective particle swarm optimization algorithm for the
search and track tasks in the distributed multiple-input and
multiple-output radar. <em>ASOC</em>, <em>101</em>, 107000. (<a
href="https://doi.org/10.1016/j.asoc.2020.107000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The resource allocation strategy plays an important role in the multifunctional performance of distributed multiple-input and multiple-output radar (D-MIMO-R) systems. In applications, different functions may be conflicting, due to the competition of the same resource. In this paper, a bi-objective hybrid particle swarm optimization (BHPSO) algorithm is proposed for the simultaneous optimization of the search and track functions, under the constraint of limited active subarrays and power budget. In the BHPSO, the nonlinear time-varying coefficients are used to balance the exploitation and exploration abilities in different stages. The heuristic mapping scheme is designed to cope with the constraints. The crossover and mutation operators are devised to the break the swarm stagnation. The distance-based crowding function with the local guider scheme is incorporated to preserve the swam diversity. In addition, by exploiting the unique structures of the objective functions, we fully prove that the solutions, each of which corresponds to a specific power budget for the search or track function, have a proportional relationship. Thus, the best-known Pareto subset (BKPS) can be obtained by parallel solving two convex optimization models only once. Extensive simulation results show the effectiveness and efficiency of the proposed BHPSO, in comparison with the state-of-the-art algorithms. The statistical results also indicate that the target reflectivity is a main influence factor on the resource allocation, and the BHPSO can provide competitive results on standard test functions.},
  archive      = {J_ASOC},
  author       = {Haowei Zhang and Junwei Xie and Binfeng Zong},
  doi          = {10.1016/j.asoc.2020.107000},
  journal      = {Applied Soft Computing},
  pages        = {107000},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-objective particle swarm optimization algorithm for the search and track tasks in the distributed multiple-input and multiple-output radar},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Velocity prediction using markov chain combined with driving
pattern recognition and applied to dual-motor electric vehicle energy
consumption evaluation. <em>ASOC</em>, <em>101</em>, 106998. (<a
href="https://doi.org/10.1016/j.asoc.2020.106998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle velocity prediction is of great significance in electric vehicle (EV) energy consumption evaluation. However, vehicle velocity prediction is complicated due to the uncertainty of vehicle velocity and driving pattern. To address the challenges, a vehicle velocity prediction model based on driving pattern recognition (DPR) and Markov Chain (MC) is proposed. Firstly, three typical driving cycles are adopted to constructed sample driving cycle. K-means algorithm is used for clustering the constructed driving cycle segments. And Learning Vector Quantization (LVQ) neural network (NN) is applied to recognizing the driving pattern in real-time, then the Markov Transition Matrix (MTM) corresponding to three clustered driving patterns are adopted to predict vehicle velocity. Finally, the velocity prediction results are applied to dual-motor EV energy consumption evaluation model which is established by Multiple Linear Regression (MLR). Velocity prediction results show that the Root Mean Square Error (RMSE) value of velocity prediction based on DPR and MC decreased by 24.1\% compared with MC prediction without DPR. The velocity prediction is applied to energy consumption evaluation, the results shows that the error is 2.33\%, which is sufficient to demonstrate the accuracy of the velocity prediction model and the energy consumption evaluation model.},
  archive      = {J_ASOC},
  author       = {Xinyou Lin and Guangji Zhang and Shenshen Wei},
  doi          = {10.1016/j.asoc.2020.106998},
  journal      = {Applied Soft Computing},
  pages        = {106998},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Velocity prediction using markov chain combined with driving pattern recognition and applied to dual-motor electric vehicle energy consumption evaluation},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed embodied evolution over networks. <em>ASOC</em>,
<em>101</em>, 106993. (<a
href="https://doi.org/10.1016/j.asoc.2020.106993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In several network problems the optimal behavior of the agents (i.e., the nodes of the network) is not known before deployment. Furthermore, the agents might be required to adapt, i.e. change their behavior based on the environment conditions. In these scenarios, offline optimization is usually costly and inefficient, while online methods might be more suitable. In this work, we use a distributed Embodied Evolution approach to optimize spatially distributed, locally interacting agents by allowing them to exchange their behavior parameters and learn from each other to adapt to a certain task within a given environment. Our results on several test scenarios show that the local exchange of information, performed by means of crossover of behavior parameters with neighbors, allows the network to conduct the optimization process more efficiently than the cases where local interactions are not allowed, even when there are large differences on the optimal behavior parameters within each agent’s neighborhood.},
  archive      = {J_ASOC},
  author       = {Anil Yaman and Giovanni Iacca},
  doi          = {10.1016/j.asoc.2020.106993},
  journal      = {Applied Soft Computing},
  pages        = {106993},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distributed embodied evolution over networks},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting fake news with capsule neural networks.
<em>ASOC</em>, <em>101</em>, 106991. (<a
href="https://doi.org/10.1016/j.asoc.2020.106991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news has increased dramatically in social media in recent years. This has prompted the need for effective fake news detection algorithms . Capsule neural networks have been successful in computer vision and are receiving attention for use in Natural Language Processing (NLP). This paper aims to use capsule neural networks in the fake news detection task. We use different embedding models for news items of different lengths. Static word embedding is used for short news items, whereas non-static word embeddings that allow incremental uptraining and updating in the training phase are used for medium length or long news statements. Moreover, we apply different levels of n-grams for feature extraction. Our proposed models are evaluated on two recent well-known datasets in the field, namely ISOT and LIAR. The results show encouraging performance, outperforming the state-of-the-art methods by 7.8\% on ISOT and 3.1\% on the validation set, and 1\% on the test set of the LIAR dataset.},
  archive      = {J_ASOC},
  author       = {Mohammad Hadi Goldani and Saeedeh Momtazi and Reza Safabakhsh},
  doi          = {10.1016/j.asoc.2020.106991},
  journal      = {Applied Soft Computing},
  pages        = {106991},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detecting fake news with capsule neural networks},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Genetic programming for development of cost-sensitive
classifiers for binary high-dimensional unbalanced classification.
<em>ASOC</em>, <em>101</em>, 106989. (<a
href="https://doi.org/10.1016/j.asoc.2020.106989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic programming (GP) has the built-in ability for feature selection when developing classifiers for classification with high-dimensional data. However, due to the problem of class imbalance, the developed classifiers by GP are prone to be biased towards the majority class. Cost-sensitive learning has shown to be effective in addressing the problem of class imbalance. In cost-sensitive learning, cost matrices are often manually designed and then considered by classification algorithms to treat different mistakes differently. However, in many real-world applications, cost matrices are unknown because of the limited domain knowledge in complex situations. Therefore, in this paper, we propose a novel GP method to develop cost-sensitive classifiers, where a cost matrix is automatically learned, instead of requiring it from domain experts. The proposed method is examined and compared with existing methods on ten high-dimensional unbalanced datasets. Experimental results show that the proposed method outperforms the compared GP methods in most cases.},
  archive      = {J_ASOC},
  author       = {Wenbin Pei and Bing Xue and Lin Shang and Mengjie Zhang},
  doi          = {10.1016/j.asoc.2020.106989},
  journal      = {Applied Soft Computing},
  pages        = {106989},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Genetic programming for development of cost-sensitive classifiers for binary high-dimensional unbalanced classification},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evolutionary multi-objective approach to learn from
positive and unlabeled data. <em>ASOC</em>, <em>101</em>, 106986. (<a
href="https://doi.org/10.1016/j.asoc.2020.106986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive and unlabeled (PU) learning has attracted increasing interests in recent years. Despite that a number of PU learning algorithms have been proposed, most of them are subject to some assumptions about unlabeled sample distribution and objective functions, which makes them difficult to be adopted for real applications. To this end, in this paper, an evolutionary multi-objective approach, namely MOEA-PUL, is suggested for PU learning, whose aim is to build a PU classifier without any prior assumption for data distribution and objective functions. To be specific, the PU learning is formulated as a bi-objective optimization problem , where the true positive rate (TPR) and a new metric, termed unlabeled accuracy rate (UAR) are used as the two objectives. A multi-objective evolutionary algorithm is proposed to solve this bi-objective optimization problem , under the framework of NSGA-II, where a PU similarity based initialization strategy and an elite label based learning strategy are developed. Empirical studies on 12 datasets demonstrate the superiority of MOEA-PUL over the existing PU learning algorithms.},
  archive      = {J_ASOC},
  author       = {Jianfeng Qiu and Xiaoqiang Cai and Xingyi Zhang and Fan Cheng and Shenzhi Yuan and Guanglong Fu},
  doi          = {10.1016/j.asoc.2020.106986},
  journal      = {Applied Soft Computing},
  pages        = {106986},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary multi-objective approach to learn from positive and unlabeled data},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparative analysis of bio-inspired optimization
algorithms for automated test pattern generation in sequential circuits.
<em>ASOC</em>, <em>101</em>, 106967. (<a
href="https://doi.org/10.1016/j.asoc.2020.106967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, bio-inspired optimization algorithms have demonstrated the ability to produce optimal solutions to numerous complex computational problems in science and engineering. In this work, a comparative analysis of bio-inspired algorithms is presented to understand and quantify the performance of algorithms in guiding the search process toward better solutions over all feasible solutions. Three evolutionary algorithms , namely, the Genetic Algorithm (GA), Particle Swarm Optimization (PSO) and Differential Evolution (DE), are implemented to generate an optimized test sequence set for digital sequential circuits to investigate how they explore and the exploitation realized in their search spaces. The merit of using a comparative method to analyze and optimize search spaces is to reach a reliable and quantifiable conclusion about the relative performance of each algorithm. An improvement in the quality of solutions was achieved, particularly in terms of testing time, number of test vectors, and fault coverage of tested sequential circuits in comparison with other algorithmic test generators that have been presented in the literature. The study shows how to effectively reduce the search space without negatively affecting the results and guides the search process over a large space. In addition, the experiments highlight the limitations of each optimization algorithm and offer some constructive methods of improvement. Moreover, several recommendations and guidelines regarding the use of optimization algorithms as test pattern generators to improve their performance and increase their efficiency are presented. Finally, we emphasize the relevance of bio-inspired algorithms in solving complex computational problems, data manipulation, and optimization objectives .},
  archive      = {J_ASOC},
  author       = {Majed Alateeq and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2020.106967},
  journal      = {Applied Soft Computing},
  pages        = {106967},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comparative analysis of bio-inspired optimization algorithms for automated test pattern generation in sequential circuits},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corrigendum to “a new soft computing model for estimating
and controlling blast-produced ground vibration based on hierarchical
k-means clustering and cubist algorithms” [appl. Soft comput. 77 (2019)
376–386]. <em>ASOC</em>, <em>100</em>, 107123. (<a
href="https://doi.org/10.1016/j.asoc.2021.107123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Hoang Nguyen and Xuan-Nam Bui and Quang-Hieu Tran and Ngoc-Luan Mai},
  doi          = {10.1016/j.asoc.2021.107123},
  journal      = {Applied Soft Computing},
  pages        = {107123},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “A new soft computing model for estimating and controlling blast-produced ground vibration based on hierarchical K-means clustering and cubist algorithms” [Appl. soft comput. 77 (2019) 376–386]},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distinguishing computer-generated images from photographic
images using two-stream convolutional neural network. <em>ASOC</em>,
<em>100</em>, 107025. (<a
href="https://doi.org/10.1016/j.asoc.2020.107025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s advanced multimedia tools allow us to create photorealistic computer graphic images, effortlessly. There are various fields such as the film industry, virtual reality, video games where computer-generated (CG) images are used widely. CG images can also be misused in many ways. Therefore, there is a need of distinguishing CG images from real photographic (PG) images. This paper proposes a method to distinguish CG images from PG images using a two-stream convolutional neural network (CNN). In the proposed method, the first stream takes the advantage of the knowledge learned by the pre-trained VGG-19 network, and then this knowledge is transferred to learn the distinct features of CG and PG images. Here, we propose a second stream, that preprocesses the images using three high-pass filters which aim to help the network to focus on noise-based distinct features of CG and PG images. Finally, we propose an ensemble model to merge the outcomes of the proposed two streams. Comparative and self-analysis experiments demonstrates that the proposed method outperforms the state-of-the-art methods in terms of classification accuracy . The experimental results also show that the proposed method performs satisfactorily under the additive white Gaussian noise postprocessing operation in the images.},
  archive      = {J_ASOC},
  author       = {Kunj Bihari Meena and Vipin Tyagi},
  doi          = {10.1016/j.asoc.2020.107025},
  journal      = {Applied Soft Computing},
  pages        = {107025},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distinguishing computer-generated images from photographic images using two-stream convolutional neural network},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of hydraulics performance in drain envelopes
using kmeans based multivariate adaptive regression spline.
<em>ASOC</em>, <em>100</em>, 107008. (<a
href="https://doi.org/10.1016/j.asoc.2020.107008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study suggests a new modeling strategy, hybridized multivariate adaptive regression spline (MARS) and Kmeans clustering, for estimating coefficients of hydraulic conductivity using various input combinations of the useful variables, hydraulic head H (cm), geotextile filters size O90 (nm), time T (min) and discharge of drain Q (cm 3 /s). The results of the newly developed method (MARS-Kmeans) were compared with the single MARS, M5 model tree (M5 Tree) and group method of data handling (GMDH) with respect to four assessing statistics of root mean square errors (RMSE), mean absolute errors (MSE), Nash Sutcliffe efficiency (NSE), and coefficient and determination (R 2 ) together with Wilcoxon rank-sum test and visual evaluation via scatterplots , boxplots, and Taylor diagram. The results indicated the superiority of the MARS-Kmeans method over the M5 Tree, MARS, and GMDH in estimating envelope hydraulic conductivity and soil-envelope hydraulic conductivity. The accuracy of the M5 Tree, MARS and, GMDH methods were improved using MARS-Kmeans in estimating Kse by 45\%, 57\%, and 77\% and estimating Ke by 31\%, 38\%, and 45\% for RMSE.},
  archive      = {J_ASOC},
  author       = {Rana Muhammad Adnan and Payam Khosravinia and Bakhtiar Karimi and Ozgur Kisi},
  doi          = {10.1016/j.asoc.2020.107008},
  journal      = {Applied Soft Computing},
  pages        = {107008},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of hydraulics performance in drain envelopes using kmeans based multivariate adaptive regression spline},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-layer surrogate-assisted differential evolution with
better and nearest option for optimizing the spring of hydraulic series
elastic actuator. <em>ASOC</em>, <em>100</em>, 107001. (<a
href="https://doi.org/10.1016/j.asoc.2020.107001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The series elastic actuator (SEA) becomes popular recently because of its compliance However, most SEAs are oversized due to the helical compression springs . In this study, a smaller SEA with a new structure is proposed, and a planar torsion spring is designed for the proposed SEA. The shape of the planar torsion spring is described by the cubic spline interpolation method. The planar torsion spring optimization problem is an expensive black box problem ; therefore, a two-layer surrogate-assisted differential evolution with a better and nearest option (TLSANbDE) is proposed to optimize the shape of the planar spring. The superiority of TLSANbDE is demonstrated by a comparison between TLSANbDE and several other algorithms. Our simulation results show that TLSANbDE can design the planar spring with specific stiffness, less working stress, and is expected to perform well in other engineering problems.},
  archive      = {J_ASOC},
  author       = {Haozhen Dong and Xinyu Li and Zan Yang and Liang Gao and Yan Lu},
  doi          = {10.1016/j.asoc.2020.107001},
  journal      = {Applied Soft Computing},
  pages        = {107001},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-layer surrogate-assisted differential evolution with better and nearest option for optimizing the spring of hydraulic series elastic actuator},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic collaborative fireworks algorithm and its
applications in robust pole assignment optimization. <em>ASOC</em>,
<em>100</em>, 106999. (<a
href="https://doi.org/10.1016/j.asoc.2020.106999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pole assignment optimization is important for improving system stability. This paper considers the regional pole assignment optimization based on the swam intelligence optimization algorithm . Regular pole assignment cannot overcome the effect of system disturbance and cannot reach the best control performance. And the firework algorithm (FWA) often ignore valuable local search opportunity and its selection mechanism taken much computation effort. Therefore, a Dynamic Collaborative Fireworks Algorithm (DCFWA) which reserves the advantages of Fireworks Algorithm (FWA) is presented for solve these problems, and a few features of the proposed algorithm as follows. First, a new explosion radius scaling strategy is designed by adjusting the scaling coefficient according to the distribution of optimal value points, which effectively enhances the optimal firework’s search capability. Second, a new offspring fireworks selection method is built for avoiding the algorithm falling into local optimum. Third, a new initialization method is embedded into the algorithm for improving global search capability. Some well-known algorithms and our proposed algorithm are applied to several types of pole assignment optimization of control systems. The comparison results indicate that the proposed algorithm does outperform the other ones.},
  archive      = {J_ASOC},
  author       = {Wenqi Wei and Haibin Ouyang and Chunliang Zhang and Steven Li and Liyun Fu and Liqun Gao},
  doi          = {10.1016/j.asoc.2020.106999},
  journal      = {Applied Soft Computing},
  pages        = {106999},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic collaborative fireworks algorithm and its applications in robust pole assignment optimization},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient metaheuristic algorithm based feature selection
and recurrent neural network for DoS attack detection in cloud computing
environment. <em>ASOC</em>, <em>100</em>, 106997. (<a
href="https://doi.org/10.1016/j.asoc.2020.106997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of Denial of Service (DoS) attack is one of the most critical issues in cloud computing . The attack detection framework is very complex due to the nonlinear thought of interruption activities, unusual conduct of systems traffic, and many attributes in the issue space. This paper proposes an efficient DoS attack detection system that uses the Oppositional Crow Search Algorithm (OCSA), which integrates the Crow Search Algorithm (CSA) and Opposition Based Learning (OBL) method to address such type of issues. The proposed system consists of two stages viz. selection of features using OCSA and classification using Recurrent Neural Network (RNN) classifier. The essential features are selected using the OCSA algorithm and then given to RNN classifier. In the subsequent testing process, incoming data is classified using the RNN classifier. It ensures the separation of standard data (saved in cloud) and the removal of compromised data Using the benchmark data set, the results of experimental evaluation demonstrate that the proposed technique outperforms the other conventional methods by 98.18\%, 95.13\%, 93.56\%, and 94.12\% in terms of Precision, Recall, F-Measure, and Accuracy respectively. Further, the proposed work outperforms existing works by 3\% on an average for all the metrics used.},
  archive      = {J_ASOC},
  author       = {Reddy SaiSindhuTheja and Gopal K. Shyam},
  doi          = {10.1016/j.asoc.2020.106997},
  journal      = {Applied Soft Computing},
  pages        = {106997},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient metaheuristic algorithm based feature selection and recurrent neural network for DoS attack detection in cloud computing environment},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new hybrid model for wind speed forecasting combining long
short-term memory neural network, decomposition methods and grey wolf
optimizer. <em>ASOC</em>, <em>100</em>, 106996. (<a
href="https://doi.org/10.1016/j.asoc.2020.106996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable and accurate wind speed forecasting (WSF) is fundamental for efficient exploitation of wind power. In particular, high accuracy short-term WSF (ST-WSF) has a significant impact on the efficiency of wind power generation systems . Due to the non-stationarity and stochasticity of the wind speed (WS), a single model is often not sufficient in practice for the accurate estimation of the WS. Hybrid models are being proposed to overcome the limitations of single models and increase the WS forecasting performance. In this paper, a new hybrid WSF model is developed based on long short-term memory (LSTM) network and decomposition methods with grey wolf optimizer (GWO). In the pre-processing stage, the missing data is filled by the weighted moving average (WMA) method, the WS time series (WSTS) data are smoothed by WMA filtering and the smoothed data are used as model input after Z Z -score normalization. The forecasting model is formed by the combination of a single model, a decomposition method and an advanced optimization algorithm . Successively, the hybrid WSF model is developed by combining the LSTM and decomposition methods , and optimizing the intrinsic mode function (IMF) estimated outputs with a grey wolf optimizer (GWO). The developed non-linear hybrid model is utilized on the data collected from five wind farms in the Marmara region, Turkey. The obtained experimental results indicate that the proposed combined model can capture non-linear characteristics of WSTS, achieving better forecasting performance than single forecasting models, in terms of accuracy.},
  archive      = {J_ASOC},
  author       = {Aytaç Altan and Seçkin Karasu and Enrico Zio},
  doi          = {10.1016/j.asoc.2020.106996},
  journal      = {Applied Soft Computing},
  pages        = {106996},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new hybrid model for wind speed forecasting combining long short-term memory neural network, decomposition methods and grey wolf optimizer},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A gene selection algorithm using simplified swarm
optimization with multi-filter ensemble technique. <em>ASOC</em>,
<em>100</em>, 106994. (<a
href="https://doi.org/10.1016/j.asoc.2020.106994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene expression data has been successfully applied to various purposes, especially for cancer classification. The challenges confronted by the development of effective classifiers for expression data are the curse of dimensionality and over-fitting. Gene selection is an efficient and effective method for overcoming the above difficulties and enhancing the predictive accuracy of a classifier. This work presents a hybrid gene selection method based on a novel multi-filter ensemble technique and simplified swarm optimization (SSO). This proposed method was comprised of two phases. In the first phase, the ensemble technique was developed via VIKOR to integrate the outcomes of multiple filters to preselect the most informative gene subset from the original dataset. In the second phase, SSO was tailored with a new gene pruning strategy, re-initialization scheme, and support vector machine as a wrapper to further search the optimal gene subset on the space of the candidate genes selected in the first phase. As evidence of the performance of the proposed method, extensive experiments on nineteen microarray datasets were carried out. The computational results compared favorably with well-known methods in the literature. Statistical analysis indicated that the proposed method was highly competitive and could be an effective tool for microarray analysis.},
  archive      = {J_ASOC},
  author       = {Chyh-Ming Lai and Hsin-Ping Huang},
  doi          = {10.1016/j.asoc.2020.106994},
  journal      = {Applied Soft Computing},
  pages        = {106994},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gene selection algorithm using simplified swarm optimization with multi-filter ensemble technique},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A conditional factor VAE model for pump degradation
assessment under varying conditions. <em>ASOC</em>, <em>100</em>,
106992. (<a href="https://doi.org/10.1016/j.asoc.2020.106992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance degradation assessment (PDA) of hydraulic pumps is of great importance to preserve operational reliability and ensure safety of hydraulic systems. PDA of hydraulic pumps relies heavily on degradation monitoring data such as vibration data, acoustic emission data and oil data. However, besides the inherent degradation of pumps, the time-varying load conditions (e.g. output pressure) have a significant influence on the behavior of degradation data. This makes PDA more difficult under varying conditions. To address this issue, this paper proposes a conditional factor variational auto-encoder (CFVAE) model whereby variational theory is firstly applied to degradation data decoupling and degradation characteristics extraction. In the training phase, degradation data decoupling is realized by punishing the total correlation term of the model with a hyper-parameter γ γ and the degradation characteristics can be extracted from latent code units of the model. However, the punishment can result in large reconstruction losses of the model which is not conducive to degradation assessment. Then the degradation label information is integrated into the decoder of the model to decrease reconstruction loss caused by data decoupling. Moreover, a new metric based on inter-class distance and intra-class divergence is introduced to optimize the hyper-parameter and select the best latent code units for degradation characteristics extraction and description. Using these techniques, the CFVAE models corresponding to all degradation states can be well trained. In the testing phase, degradation data from unknown degradation states is fed into all of the trained CFVAE models respectively and minimum distance criterion is introduced to predict the actual degradation state. Taking return oil flow as degradation data, the proposed CFVAE model along with several advanced methodologies is applied to degradation assessment for hydraulic pumps under varying conditions. The assessment results of ten experimental cases verify that the proposed CFVAE model can adapt better to change of conditions and has higher assessment accuracy than other methodologies. The proposed model also shows good robustness in multiple cases, making it likely to apply this model to other mechanical components.},
  archive      = {J_ASOC},
  author       = {He Yu and Hongru Li},
  doi          = {10.1016/j.asoc.2020.106992},
  journal      = {Applied Soft Computing},
  pages        = {106992},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A conditional factor VAE model for pump degradation assessment under varying conditions},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalized average linkage criterion for hierarchical
agglomerative clustering. <em>ASOC</em>, <em>100</em>, 106990. (<a
href="https://doi.org/10.1016/j.asoc.2020.106990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical agglomerative clustering (HAC) is among the most widely adopted algorithms in unsupervised learning. This method employs a linkage criterion to measure the similarity between two clusters and the correct selection of this criterion highly influences the performance of HAC. This paper presents and evaluates a novel linkage criterion for HAC, which is a generalization of the Average Linkage (GAL) and it aims at improving the quality of the similarity computation of the original average linkage criterion. In order to assess the liability of the proposed criterion, an empirical analysis is conducted, which is performed on 28 datasets from the literature. In a comparative analysis, the proposed criterion is compared to seven reference methods from the literature. Our findings indicate that the results obtained by the proposed criterion are promising, surpassing all the existing reference methods.},
  archive      = {J_ASOC},
  author       = {Leonardo Ramos Emmendorfer and Anne Magaly de Paula Canuto},
  doi          = {10.1016/j.asoc.2020.106990},
  journal      = {Applied Soft Computing},
  pages        = {106990},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A generalized average linkage criterion for hierarchical agglomerative clustering},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward a scalable type-2 fuzzy model for
resource-constrained project scheduling problem. <em>ASOC</em>,
<em>100</em>, 106988. (<a
href="https://doi.org/10.1016/j.asoc.2020.106988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a scalable method for the Resource-Constrained Project Scheduling Problem (RCPSP) with a type-2 fuzzy uncertain activity time-span. The objective of this method is the project completion time minimization. This problem has been solved through a scalable type-2 fuzzy Dijkstra algorithm inside different activities rules. The Dijkstra algorithm produces a different permutation of activities alongside each selected path that should be examined in optimization function using the type-2 fuzzy simplex algorithm . This method generates the best permutation using run-time optimization which significantly satisfies lower project completion time inside the project cost minimization . Also, the method proposed a resource workload for both tactical and operational levels of planning. The presented approach is conducted by the gas-related industry project and a case on satellite design of the aerospace industry on 3 different Resource/Process/Environment strategies. The results convey the defined method has a better performance compared to type-1 fuzzy for covering the largest uncertainty domain. This method could be considered in every engineering project management.},
  archive      = {J_ASOC},
  author       = {Amirhosein Mahdavi and Babak Shirazi and Javad Rezaeian},
  doi          = {10.1016/j.asoc.2020.106988},
  journal      = {Applied Soft Computing},
  pages        = {106988},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Toward a scalable type-2 fuzzy model for resource-constrained project scheduling problem},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient compression algorithm using learning networks for
remote sensing images. <em>ASOC</em>, <em>100</em>, 106987. (<a
href="https://doi.org/10.1016/j.asoc.2020.106987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image compression plays a vital role in the high-resolution imaging of an on-orbit optical camera. The post-transform-based compression method is of particular importance for remote sensing on-orbit images because it can remove remaining redundancies among high-amplitude coefficients in the wavelet transform , specifically in high-frequency areas. However, current post-transforms are inefficient because the post-transform has to access a large-scale wavelet domain . In this paper, we propose a low-dimensional visual representation convolution neural network (LVR-CNN) for efficient post-transform-based image compression . The LVR-CNN is used to transform the wavelet domain from a large-scale representation to a new wavelet version with a small-scale. We obtain the optimized small-scale wavelet representation by minimization between the original and reconstructed wavelet representations through LVR-CNN. The multi-basis dictionary post-transform is applied to the optimized wavelet representation to achieve high compression performance and calculation efficiency. We experimentally confirm the proposed method and results with test remote sensing images. The experimental results indicate that the LVR-CNN post-transform-based compression method yields high compression performance and low post-transform resource utilization. Compared with conventional methods, the proposed method can increase the peak-signal-noise-ratio (PSNR) by 1.2 dB ∼ ∼ 2.7 dB. These merits indicate the proposed compression method is efficient for remote sensing images.},
  archive      = {J_ASOC},
  author       = {Jin Li and Zilong Liu},
  doi          = {10.1016/j.asoc.2020.106987},
  journal      = {Applied Soft Computing},
  pages        = {106987},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient compression algorithm using learning networks for remote sensing images},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural review text interaction for recommendation
systems. <em>ASOC</em>, <em>100</em>, 106985. (<a
href="https://doi.org/10.1016/j.asoc.2020.106985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users’ reviews contain valuable information which are not taken into account in most recommender systems . According to the latest studies in this field, using review texts could not only improve the performance of recommendation, but it can also alleviate the impact of data sparsity and help to tackle this problem. In this paper, we present a neural recommender model which recommends items by leveraging user reviews. In order to predict user rating for each item, our proposed model, named M a t c h P y r a m i d R e c o m m e n d e r S y s t e m MatchPyramidRecommenderSystem (MPRS), represents each user and item with their corresponding review texts. Thus, the problem of recommendation is viewed as a text matching problem such that the matching score obtained from matching user and item texts could be considered as a good representative of their joint extent of similarity. To solve the text matching problem, inspired by MatchPyramid (Pang et al., 2016), we employed an interaction-based approach according to which a matching matrix is constructed given a pair of input texts. The matching matrix, which has the property of hierarchical matching patterns , is then fed into a Convolutional Neural Network (CNN) to compute the matching score for the given user–item pair. Our experiments on the small data categories of Amazon review dataset show that our proposed model gains from 1.76\% to 21.72\% relative improvement compared to DeepCoNN model (Zheng et al., 2017), and from 0.83\% to 3.15\% relative improvement compared to TransNets model (Catherine and Cohen, 2017). Also, on two large categories, namely AZ-CSJ and AZ-Mov , our model achieves relative improvements of 8.08\% and 7.56\% compared to the DeepCoNN model, and relative improvements of 1.74\% and 0.86\% compared to the TransNets model, respectively.},
  archive      = {J_ASOC},
  author       = {Parisa Abolfath Beygi Dezfouli and Saeedeh Momtazi and Mehdi Dehghan},
  doi          = {10.1016/j.asoc.2020.106985},
  journal      = {Applied Soft Computing},
  pages        = {106985},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep neural review text interaction for recommendation systems},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalized fuzzy multiple-layer NDEA: An application to
performance-based budgeting. <em>ASOC</em>, <em>100</em>, 106984. (<a
href="https://doi.org/10.1016/j.asoc.2020.106984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network data envelopment analysis (NDEA) is capable of considering operations and interdependence of a system’s component processes to measure efficiencies. There are numerous performance evaluation applications in which some indicators have hierarchical structures with a considerable number of sub-indicators. This problem of ignoring the hierarchical structure of indicators weakens the discrimination power of NDEA models and may result in inaccurate efficiency scores. In this paper we propose a generalized fuzzy Multiple-Layer NDEA (GFML-NDEA) model and GFML-NDEA-based composite indicators (GFML-NDEA-CI) to incorporate the hierarchical structures of indicators in the ambit of the particular two-stage NDEA models. To demonstrate the usefulness of the GFML-NDEA-CI model proposed, its application was tested by evaluating the efficiency of the performance-based budgeting (PBB) system in 14 governmental agencies in Iran. The comparative analysis results obtained from the GFML-NDEA-CI (multi-layer) model with those from the single-layer fuzzy NDEA-CI model indicate that the number of efficient decision-making units (DMUs) in the one-layer model is eight, whereas it is solely one DMU in the multi-layer model. The discrimination power of the multi-layer model proposed is significantly increased by observing that standard deviation of efficiency scores are increased by 41\%, 61\%, and 84\% for possibility levels 0, 0.5, and 1, respectively. This is obtained while reducing information entropy, thus suggesting that the proposed model yields more reliable scores.},
  archive      = {J_ASOC},
  author       = {Mohamad Reza Amini and Adel Azar and Hamidreza Eskandari and Peter F. Wanke},
  doi          = {10.1016/j.asoc.2020.106984},
  journal      = {Applied Soft Computing},
  pages        = {106984},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A generalized fuzzy multiple-layer NDEA: An application to performance-based budgeting},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple features based approach for automatic fake news
detection on social networks using deep learning. <em>ASOC</em>,
<em>100</em>, 106983. (<a
href="https://doi.org/10.1016/j.asoc.2020.106983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the rise of Online Social Networks has led to proliferation of social news such as product advertisement, political news, celebrity’s information, etc. Some of the social networks such as Facebook, Instagram and Twitter affected by their user through fake news. Unfortunately, some users use unethical means to grow their links and reputation by spreading fake news in the form of texts, images, and videos. However, the recent information appearing on an online social network is doubtful, and in many cases, it misleads other users in the network. Fake news is spread intentionally to mislead readers to believe false news, which makes it difficult for detection mechanism to detect fake news on the basis of shared content. Therefore, we need to add some new information related to user’s profile, such as user’s involvement with others for finding a particular decision. The disseminated information and their diffusion process create a big problem for detecting these contents promptly and thus highlighting the need for automatic fake news detection. In this paper, we are going to introduce automatic fake news detection approach in chrome environment on which it can detect fake news on Facebook. Specifically, we use multiple features associated with Facebook account with some news content features to analyze the behavior of the account through deep learning . The experimental analysis of real-world information demonstrates that our intended fake news detection approach has achieved higher accuracy than the existing state of art techniques.},
  archive      = {J_ASOC},
  author       = {Somya Ranjan Sahoo and B.B. Gupta},
  doi          = {10.1016/j.asoc.2020.106983},
  journal      = {Applied Soft Computing},
  pages        = {106983},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple features based approach for automatic fake news detection on social networks using deep learning},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy region-based active contour driven by global and local
fitting energy for image segmentation. <em>ASOC</em>, <em>100</em>,
106982. (<a href="https://doi.org/10.1016/j.asoc.2020.106982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel global and local fuzzy image fitting (GLFIF) based active contour model for image segmentation . First, we design two fitted images: global fuzzy fitted image (GFFI) and local fuzzy fitted image (LFFI). Next, the energy function including a global term and a fitting term is constructed. The global term comes from the fuzzy energy-based active contour (FEAC) model to balance the importance of the object and background while the fitting term is based on GFFI and LFFI to handle the intensity inhomogeneity in given images. Then, we prove the energy function to be convex, which can ensure the segmentation results independent of initialization. Finally, unlike the FEAC model by computing the change of pixel-by-pixel energy function, a direct method is utilized to calculate the difference between the old and new energy functions in the whole image for each iteration to update the pseudo level set function . Experiments on synthetic and real images show that the proposed model is more robust than the popular active contour models for segmenting the images with noise, blurred boundaries, and intensity inhomogeneity . The code is available at: https://github.com/fangchj2002/GLFIF .},
  archive      = {J_ASOC},
  author       = {Jiangxiong Fang and Huaxiang Liu and Jun Liu and Haiying Zhou and Liting Zhang and Hesheng Liu},
  doi          = {10.1016/j.asoc.2020.106982},
  journal      = {Applied Soft Computing},
  pages        = {106982},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy region-based active contour driven by global and local fitting energy for image segmentation},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two graph-regularized fuzzy subspace clustering methods.
<em>ASOC</em>, <em>100</em>, 106981. (<a
href="https://doi.org/10.1016/j.asoc.2020.106981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new fuzzy subspace clustering (FSC) method which finds some subspaces as clusters such that each point belongs to the nearest subspace with a certain weight or probability. Then we propose two graph-regularized versions for it, in which two points are more likely to be assigned to the same cluster if they are close spatially or with the same labels. In the proposed two graph regularizations , one encodes the weight (or probability) of a point assigned to a cluster and the other encodes the projection coefficients of a point on a subspace. We develop iterative solutions for these methods through constructing a surrogate, which monotonically decrease the cost function with a simple structure. The experimental results, using both synthetic and real-world databases, demonstrate the effectiveness and flexibility of the proposed methods.},
  archive      = {J_ASOC},
  author       = {Yueyang Teng and Shouliang Qi and Fangfang Han and Lisheng Xu and Yudong Yao and Wei Qian},
  doi          = {10.1016/j.asoc.2020.106981},
  journal      = {Applied Soft Computing},
  pages        = {106981},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two graph-regularized fuzzy subspace clustering methods},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Balancing convergence and diversity in resource allocation
strategy for decomposition-based multi-objective evolutionary algorithm.
<em>ASOC</em>, <em>100</em>, 106968. (<a
href="https://doi.org/10.1016/j.asoc.2020.106968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposition-based multi-objective evolutionary algorithm decomposes a multi-objective optimization problem into a set of scalar subproblems and then optimizes them simultaneously. However, it does not take into account that subproblems of different difficulties need unequal computing resources. The resource allocation (RA) strategy based on MOEA/D was proposed to solve this problem. But most RA strategies generate a large number of solutions around some easy optimization subproblems, which cause the deterioration of the distribution. And the way they measure the subproblem difficulty ignores that subproblems may lose their evolvability in an evolutionary period. This paper aims to balance the convergence and diversity in RA strategy. Firstly, we introduce the accumulated escape probability (AEP) to calculate the historical improvement probability of each subproblem and then measure the subproblem evolvability, which can detect whether the subproblem has lost its evolvability in an evolutionary period. Secondly, we propose a density penalty-based individual screening mechanism (ISM) to enhance diversity. It gives priority to update the subproblem surrounded with more solutions and greater relative improvement on aggregated function. Finally, the above two methods are cooperated in MOEA/D and named it MOEA/D-BRA. Then the effectiveness of these two methods and the comprehensive performance of MOEA/D-BRA are tested. Furthermore, the BRA strategy is applied to more cases to balance convergence and diversity. Several experimental results indicate that the BRA strategy performers well on tackling a set of complicated optimization problems.},
  archive      = {J_ASOC},
  author       = {Liping Wang and Xiaotian Pan and Xiao Shen and Peipei Zhao and Qicang Qiu},
  doi          = {10.1016/j.asoc.2020.106968},
  journal      = {Applied Soft Computing},
  pages        = {106968},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Balancing convergence and diversity in resource allocation strategy for decomposition-based multi-objective evolutionary algorithm},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved PSO algorithm for smooth path planning of mobile
robots using continuous high-degree bezier curve. <em>ASOC</em>,
<em>100</em>, 106960. (<a
href="https://doi.org/10.1016/j.asoc.2020.106960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new strategy is developed to plan the smooth path for mobile robots through an improved PSO algorithm in combination with the continuous high-degree Bezier curve. Rather than connecting several low-degree Bezier curve segments, the use of continuous high-degree Bezier curves facilitates the fulfillment of the requirement of high-order continuity such as the continuous curvature derivative, which is critical for the motion control of the mobile robots. On the other hand, the smooth path planning of mobile robots is mathematically an optimization problem that can be dealt with by evolutionary computation algorithms. In this regard, an improved particle swarm optimization (PSO) algorithm is proposed to tackle the local trapping and premature convergence issues. In the improved PSO algorithm, an adaptive fractional-order velocity is introduced to enforce some disturbances on the particle swarm according to its evolutionary state, thereby enhancing its capability of jumping out of the local minima and exploring the searching space more thoroughly. The superiority of the improved PSO algorithm is verified by comparing with several standard and modified PSO algorithms on some benchmark functions , and the advantages of the new strategy is also confirmed by several comprehensive simulation experiments for the smooth path planning of mobile robots.},
  archive      = {J_ASOC},
  author       = {Baoye Song and Zidong Wang and Lei Zou},
  doi          = {10.1016/j.asoc.2020.106960},
  journal      = {Applied Soft Computing},
  pages        = {106960},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved PSO algorithm for smooth path planning of mobile robots using continuous high-degree bezier curve},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of LSTM approach for modelling stress–strain
behaviour of soil. <em>ASOC</em>, <em>100</em>, 106959. (<a
href="https://doi.org/10.1016/j.asoc.2020.106959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new trial to reproduce soil stress–strain behaviour by adapting a long short-term memory (LSTM) deep learning method. LSTM is an approach that employs time sequence data to predict future occurrences, and it can be used to consider the stress history of soil behaviour. The proposed LSTM method includes the following three steps: data preparation, architecture determination, and optimisation. The capacity of the adapted LSTM method is compared with that of feedforward and feedback neural networks using a new numerical benchmark dataset. The performance of the proposed LSTM method is verified through a dataset collected from laboratory tests. The results indicate that the LSTM deep-learning method outperforms the feed forward and feedback neural networks based on both accuracy and the convergence rate when reproducing the soil’s stress–strain behaviour. One new phenomenon referred to as “bias at low stress levels”, which was not noticed before, is first discovered and discussed for all neural network-based methods.},
  archive      = {J_ASOC},
  author       = {Ning Zhang and Shui-Long Shen and Annan Zhou and Yin-Fu Jin},
  doi          = {10.1016/j.asoc.2020.106959},
  journal      = {Applied Soft Computing},
  pages        = {106959},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of LSTM approach for modelling stress–strain behaviour of soil},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regret theory-based fuzzy multi-objective portfolio
selection model involving DEA cross-efficiency and higher moments.
<em>ASOC</em>, <em>100</em>, 106958. (<a
href="https://doi.org/10.1016/j.asoc.2020.106958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with fuzzy portfolio selection problems under the framework of bounded rationality, in which the higher moments and DEA cross-efficiency are taken into consideration. To this end, a regret cross-efficiency (RCE) evaluation model is first proposed to evaluate cross-efficiency scores of all assets by integrating several important financial information: return on equity, earnings per share, current ratio, price/earnings ratio, price/book ratio, and beta value. Then, by incorporating cross-efficiency into the mean–variance–skewness framework, a regret theory-based multi-objective portfolio selection model is developed. The model aims to maximize investor’s perceived utility (composed of utility, regret, and rejoice functions) with respect to the four objectives, namely mean–variance–skewness-efficiency, bounded by several realistic constraints. Additionally, considering the inherent uncertainty in data, the criteria related to RCE evaluation as well as asset returns are characterized as fuzzy variables. The current study not only empowers investors with an overall control over the preferences regarding all portfolio objectives, but also gives investors an opportunity to tweak their intrinsic behavioral tendencies from the perspective of regret aversion and rejoice preference, thus achieving the results that are compatible with the actual preferences of investors. A real-world empirical application is presented to demonstrate the effectiveness of the proposed model.},
  archive      = {J_ASOC},
  author       = {Xiaomin Gong and Changrui Yu and Liangyu Min and Zhipeng Ge},
  doi          = {10.1016/j.asoc.2020.106958},
  journal      = {Applied Soft Computing},
  pages        = {106958},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Regret theory-based fuzzy multi-objective portfolio selection model involving DEA cross-efficiency and higher moments},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferring absolutely non-circular attribute grammars with a
memetic algorithm. <em>ASOC</em>, <em>100</em>, 106956. (<a
href="https://doi.org/10.1016/j.asoc.2020.106956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When valid syntactical structures are additionally constrained with context-sensitive information the Grammar Inference needs to be extended to the Semantic Inference. In this paper, it is shown that a complete compiler/interpreter for small Domain-Specific Languages (DSLs) can be generated automatically solely from given programs and their associated meanings using Semantic Inference. In this work a wider class of Attribute Grammars has been learned, while only S-attributed and L-attributed Grammars have previously been inferred successfully. Inferring Absolutely Non-Circular Attribute Grammars (ANC-AG) with complex dependencies among attributes has been achieved by integrating a Memetic Algorithm (MA) into the L I S A . S I LISA.SI tool. The results show that the proposed Memetic Algorithm is at least four times faster on the selected benchmark than the previous method.},
  archive      = {J_ASOC},
  author       = {Miha Ravber and Željko Kovačević and Matej Črepinšek and Marjan Mernik},
  doi          = {10.1016/j.asoc.2020.106956},
  journal      = {Applied Soft Computing},
  pages        = {106956},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inferring absolutely non-circular attribute grammars with a memetic algorithm},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial bee colony algorithm based on adaptive
neighborhood search and gaussian perturbation. <em>ASOC</em>,
<em>100</em>, 106955. (<a
href="https://doi.org/10.1016/j.asoc.2020.106955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial bee colony (ABC) is a type of popular swarm intelligence optimization algorithm . It is widely concerned because of its easy implementation, few parameters and strong global search ability. However, there are some limitations for ABC, such as weak exploitation ability and slow convergence. In this paper, a novel ABC with adaptive neighborhood search and Gaussian perturbation (called ABCNG) is proposed to overcome these shortcomings. Firstly, an adaptive method is used to dynamically adjust the neighborhood size. Then, a modified global best solution guided search strategy is constructed based on the neighborhood structure. Finally, a new Gaussian perturbation with evolutionary rate is designed to evolve the unchanged solutions at each iteration. Performance of ABCNG is tested on two benchmark sets and compared with some excellent ABC variants. Results show ABCNG is more competitive than six other ABCs.},
  archive      = {J_ASOC},
  author       = {Songyi Xiao and Hui Wang and Wenjun Wang and Zhikai Huang and Xinyu Zhou and Minyang Xu},
  doi          = {10.1016/j.asoc.2020.106955},
  journal      = {Applied Soft Computing},
  pages        = {106955},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Artificial bee colony algorithm based on adaptive neighborhood search and gaussian perturbation},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EEG emotion recognition using fusion model of graph
convolutional neural networks and LSTM. <em>ASOC</em>, <em>100</em>,
106954. (<a href="https://doi.org/10.1016/j.asoc.2020.106954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph convolutional neural networks have become research focus and inspired new ideas for emotion recognition based on EEG. Deep learning has been widely used in emotion recognition, but it is still challenging to construct models and algorithms in practical applications. In this paper, we propose a novel emotion recognition method based on a novel deep learning model (ERDL). Firstly, EEG data is calibrated by 3s baseline data and divided into segments with 6s time window, and then differential entropy is extracted from each segment to construct feature cube. Secondly, the feature cube of each segment serves as input of the novel deep learning model which fuses graph convolutional neural network (GCNN) and long-short term memories neural networks (LSTM). In the fusion model, multiple GCNNs are applied to extract graph domain features while LSTM cells are used to memorize the change of the relationship between two channels within a specific time and extract temporal features, and Dense layer is used to attain the emotion classification results . At last, we conducted extensive experiments on DEAP dataset and experimental results demonstrate that the proposed method has better classification results than the state-of-the-art methods. We attained the average classification accuracy of 90.45\% and 90.60\% for valence and arousal in subject-dependent experiments while 84.81\% and 85.27\% in subject-independent experiments.},
  archive      = {J_ASOC},
  author       = {Yongqiang Yin and Xiangwei Zheng and Bin Hu and Yuang Zhang and Xinchun Cui},
  doi          = {10.1016/j.asoc.2020.106954},
  journal      = {Applied Soft Computing},
  pages        = {106954},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEG emotion recognition using fusion model of graph convolutional neural networks and LSTM},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mitigating the risk of infection spread in manual order
picking operations: A multi-objective approach. <em>ASOC</em>,
<em>100</em>, 106953. (<a
href="https://doi.org/10.1016/j.asoc.2020.106953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the aftermath of the COVID-19 pandemic, supply chains experienced an unprecedented challenge to fulfill consumers’ demand. As a vital operational component, manual order picking operations are highly prone to infection spread among the workers, and thus, susceptible to interruption. This study revisits the well-known order batching problem by considering a new overlap objective that measures the time pickers work in close vicinity of each other and acts as a proxy of infection spread risk. For this purpose, a multi-objective optimization model and three multi-objective metaheuristics with an effective seeding procedure are proposed and are tested on the data obtained from a major US-based logistics company. Through extensive numerical experiments and comparison with the company’s current practices, the results are discussed, and some managerial insights are offered. It is found that the picking capacity can have a determining impact on reducing the risk of infection spread through minimizing the picking overlap.},
  archive      = {J_ASOC},
  author       = {Ehsan Ardjmand and Manjeet Singh and Heman Shakeri and Ali Tavasoli and William A. Young II},
  doi          = {10.1016/j.asoc.2020.106953},
  journal      = {Applied Soft Computing},
  pages        = {106953},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mitigating the risk of infection spread in manual order picking operations: A multi-objective approach},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Failure prediction by regularized fuzzy learning with
intelligent parameters selection. <em>ASOC</em>, <em>100</em>, 106952.
(<a href="https://doi.org/10.1016/j.asoc.2020.106952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software failure time prediction, which is critical in software dependable evaluation, is among one of the most critical issues in software quality assurance research. Conventional software reliability predictions often use single models to illustrate the overall characteristic of the process of software failure. However, those models often fail to produce good prediction performance owing to the constant changes in software failure patterns during different time periods. Fuzzy systems like Takagi–Sugeno–Kang are multimodel approaches that combine simple submodels to represent the overall characteristic of dynamic nonlinear complex systems. This paper presents a novel Fuzzy system based software reliability prediction model, called regularized extreme learning adaptive neuro-fuzzy inference system, and employs the quantum-inspired binary gravitational search algorithm to determine its suitable regularization parameters . The new model can effectively reduce randomness, computational complexity as well as avoid local optimization when searching for optimal parameters. An experimentation using four real-world software failure data sets was carried out to compare the performance of the proposed model with that of certain representative software reliability prediction models. The results showed that the proposed model with the optimized parameters exhibits superior performance.},
  archive      = {J_ASOC},
  author       = {Qing Shen and Jungang Lou and Xiongtao Zhang and Yunliang Jiang},
  doi          = {10.1016/j.asoc.2020.106952},
  journal      = {Applied Soft Computing},
  pages        = {106952},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Failure prediction by regularized fuzzy learning with intelligent parameters selection},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An exploratory analysis of data noisy scenarios in a
pareto-front based dynamic feature selection method. <em>ASOC</em>,
<em>100</em>, 106951. (<a
href="https://doi.org/10.1016/j.asoc.2020.106951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has become a mandatory step in several data exploration and Machine Learning applications since data quality can have a strong impact in the performance of machine learning models. Many feature selection strategies have been developed in the past decades, using different criteria to select the most relevant features. The use of dynamic feature selection, however, has showed that the use of multiple simultaneously criteria to determine the best attribute subset for similar instances can deliver encouraging results. In this context, this paper proposes to analyze the performance of a pareto-front based dynamic feature selection (PF-DFS) method under data noise scenarios. In order to do this, we intentionally added noise in 15 datasets and evaluated the PF-DFS performance in order to measure its stability under two different data noise scenarios. The obtained results are compared to some state-of-the-art algorithms and show that, in terms of accuracy, the PF-DFS method is more robust to the other methods for the majority of the analyzed scenarios.},
  archive      = {J_ASOC},
  author       = {Jhoseph Jesus and Anne Canuto and Daniel Araújo},
  doi          = {10.1016/j.asoc.2020.106951},
  journal      = {Applied Soft Computing},
  pages        = {106951},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An exploratory analysis of data noisy scenarios in a pareto-front based dynamic feature selection method},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combined economic and emission power dispatch problems
through multi-objective squirrel search algorithm. <em>ASOC</em>,
<em>100</em>, 106950. (<a
href="https://doi.org/10.1016/j.asoc.2020.106950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, due to the increasing deterioration of environmental problems, combined economic and environmental power dispatch has become one of the active research areas in the power system operation and control. This paper proposes a new multi-objective squirrel search algorithm to solve the combined economic and environmental power dispatch problem. The proposed multi-objective squirrel search algorithm integrates squirrel search algorithm along with Pareto-dominance principle to generate non-dominated solutions. It uses an external elitist depository mechanism with crowding distance sorting to maintain the distribution diversity of Pareto-optimal solutions as the evolution proceeds. In addition, a fuzzy decision-making strategy is used to select the best compromised solution from the obtained Pareto frontiers . The combined economic and environmental power dispatch problem is also solved by squirrel search algorithm based weighted sum approach, which transfers both the fuel cost and emission objectives into a single-objective function. The performances of multi-objective squirrel search algorithm are tested on three test systems with diverse complexities. The simulation results and comparisons with the other state-of-the-art heuristic algorithms confirm that the proposed multi-objective squirrel search algorithm for combined economic and environmental power dispatching can obtain a better trade-off between fuel cost and emission objectives.},
  archive      = {J_ASOC},
  author       = {V.P. Sakthivel and M. Suman and P.D. Sathya},
  doi          = {10.1016/j.asoc.2020.106950},
  journal      = {Applied Soft Computing},
  pages        = {106950},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combined economic and emission power dispatch problems through multi-objective squirrel search algorithm},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-effective subsidy policy for growers and
biofuels-plants in closed-loop supply chain of herbs and herbal
medicines: An interactive bi-objective optimization in t-environment.
<em>ASOC</em>, <em>100</em>, 106949. (<a
href="https://doi.org/10.1016/j.asoc.2020.106949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of present study is twofold. Firstly, this introduces a bi-objective closed-loop supply chain (CLSC) network by deliberating on farming of herbs and production plus distribution of associated herbal medicines in manufacturing-chain layer. Reverse-chain layer (RC-layer) recycles used substrates and unused herbs, thus producing biofuels and casing soil. Though majority of well-established CLSC models comprise sole pecuniary objective function, this aims to validates the economic self-reliance of RC-layer. Besides, Governments pay the subsidy to various entities, including the growers to encourage countrywide farming of herbs and the biofuels-plants for the extent of clean fuel produced. Thus, two different partners of same network earn subsidy without analysing trade-off. Secondly, this study develops an emended minmax method based interactive bi-objective optimization algorithm by employing absolute difference function and newly introduced semi-autonomized desired levels in real-life oriented T-environment. In pessimistic business scenario, whereas proposed model turns infeasible in classical interactive fuzzy multi-objective optimization algorithm , proposed algorithm determines corresponding Pareto optimal solution . In optimistic business scenario, optimal net profits to both layers of proposed model are more desirable than respective goals in proposed algorithm. Vulnerability analysis of twelve parameters to proposed model plus twelve subfigures pleads legislators to offer more subsidy to biofuels-plants and slightly lesser to growers; and to enact regulations to intensify the recycling of discarded substrates and herbs, thereby ensuring financial feasibility of RC-layer. Again, reduced subsidy to biofuels-plants and little more subsidy to growers, plus multi-angled modernization of all partners except landfills elevate the profitability of proposed network.},
  archive      = {J_ASOC},
  author       = {Arindam Garai and Sriparna Chowdhury and Biswajit Sarkar and Tapan Kumar Roy},
  doi          = {10.1016/j.asoc.2020.106949},
  journal      = {Applied Soft Computing},
  pages        = {106949},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cost-effective subsidy policy for growers and biofuels-plants in closed-loop supply chain of herbs and herbal medicines: An interactive bi-objective optimization in T-environment},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discrete artificial bee colony algorithm for the
distributed heterogeneous no-wait flowshop scheduling problem.
<em>ASOC</em>, <em>100</em>, 106946. (<a
href="https://doi.org/10.1016/j.asoc.2020.106946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of globalization, distributed manufacturing has become one of the main modes of manufacturing. The situation in which a number of heterogeneous factories coproduce a batch of jobs is ubiquitous in the field of distributed manufacturing. Compared with isomorphic factories, heterogeneous factories bring further difficulty in assigning jobs to factories. This paper considers the heterogeneity between factories in distributed flow-shop scheduling for the first time. This paper addresses a distributed heterogeneous no-wait flowshop scheduling problem (DHNWFSP) to minimize the makespan, where the factories have differences among them, including the number of machines, machine technology, raw material supply, and transportation conditions. In this problem, the numbers and types of machines in each factory are different, and this means that the jobs have to be processed through different processing paths. To effectively solve this DHNWFSP, a discrete artificial bee colony algorithm (DABC) is proposed. Firstly, to obtain a feasible neighborhood solution, four neighborhood search operators based on the characteristics of this problem are presented to search neighborhoods during the employed bee phase and onlooker bee phase. Then, a new method to accelerate the evaluation of the obtained neighborhood is proposed to reduce the computation time. Moreover, an efficient population update method is designed in the onlooker bee phase. Finally, a variable neighborhood descent (VND) algorithm based on four local-search methods is embedded into the scout bee phase to strengthen the local search ability of the overall algorithm. To validate the performance of the proposed algorithm, a series of numerical experiments are executed for small- and large-scale problems to compare the DABC with some state-of-art algorithms in terms of solving the DHNWFSP. The results show that the proposed DABC obtains the highest-quality solutions.},
  archive      = {J_ASOC},
  author       = {Haoran Li and Xinyu Li and Liang Gao},
  doi          = {10.1016/j.asoc.2020.106946},
  journal      = {Applied Soft Computing},
  pages        = {106946},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete artificial bee colony algorithm for the distributed heterogeneous no-wait flowshop scheduling problem},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mean–variance portfolio optimization using machine
learning-based stock price prediction. <em>ASOC</em>, <em>100</em>,
106943. (<a href="https://doi.org/10.1016/j.asoc.2020.106943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of portfolio construction depends primarily on the future performance of stock markets. Recent developments in machine learning have brought significant opportunities to incorporate prediction theory into portfolio selection. However, many studies show that a single prediction model is insufficient to achieve very accurate predictions and affluent returns. In this paper, a novel portfolio construction approach is developed using a hybrid model based on machine learning for stock prediction and mean–variance (MV) model for portfolio selection. Specifically, two stages are involved in this model: stock prediction and portfolio selection. In the first stage, a hybrid model combining eXtreme Gradient Boosting (XGBoost) with an improved firefly algorithm (IFA) is proposed to predict stock prices for the next period. The IFA is developed to optimize the hyperparameters of the XGBoost . In the second stage, stocks with higher potential returns are selected, and the MV model is employed for portfolio selection. Using the Shanghai Stock Exchange as the study sample, the obtained results demonstrate that the proposed method is superior to traditional ways (without stock prediction) and benchmarks in terms of returns and risks.},
  archive      = {J_ASOC},
  author       = {Wei Chen and Haoyu Zhang and Mukesh Kumar Mehlawat and Lifen Jia},
  doi          = {10.1016/j.asoc.2020.106943},
  journal      = {Applied Soft Computing},
  pages        = {106943},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mean–variance portfolio optimization using machine learning-based stock price prediction},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building trust/distrust relationships on signed social
service network through privacy-aware link prediction process.
<em>ASOC</em>, <em>100</em>, 106942. (<a
href="https://doi.org/10.1016/j.asoc.2020.106942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-increasing popularity of social software, we can easily establish a s igned s ocial n etwork (SSN) by capturing users’ attitudes (i.e., trust/distrust, friend/enemies, consent/opposition) toward other people. However, the social relationships among users are often very sparse in an SSN, which impede the effective extension of the users’ social circle significantly. To tackle this issue, researchers often use link prediction methods to search for missing links and predict new links in the network. However, existing link prediction methods cannot protect user’s private information well. Considering this shortcoming, we propose a Simhash-based link prediction method with privacy-preservation. Concretely, we first apply Simhash to build less-sensitive user indices and then determine the ”probably similar” friends (i.e., candidates) of a target user based on his or her indices. Through theoretical analysis, it can be known that the method proposed in this paper can effectively protect users’ proprietary information . Second, for each candidate, we calculate his/her trust and distrust values with the target user. Third, we use Social Balance Theory to evaluate the possibility of building a link between the candidate and the target user based on the trust and distrust values. Finally, we conducted a set of experiments on the real-world Epinions dataset. Experimental results prove the advantages of our proposal in terms of overcoming the sparsity problem, compared to other competitive approaches.},
  archive      = {J_ASOC},
  author       = {Huaizhen Kou and Hanwen Liu and Yucong Duan and Wenwen Gong and Yanwei Xu and Xiaolong Xu and Lianyong Qi},
  doi          = {10.1016/j.asoc.2020.106942},
  journal      = {Applied Soft Computing},
  pages        = {106942},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Building trust/distrust relationships on signed social service network through privacy-aware link prediction process},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A feature disentangling approach for person
re-identification via self-supervised data augmentation. <em>ASOC</em>,
<em>100</em>, 106939. (<a
href="https://doi.org/10.1016/j.asoc.2020.106939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problem of insufficient training data in person ReID , this paper proposes a data augmentation method based on image channels shuffling, by which a large volume of diversified training samples sharing similar edges can be produced. In the meantime, a soft label assignment strategy is designed to characterize the correlations between the original image and the generated counterparts. Furthermore, we design an encoder–decoder based learning structure for the person ReID task, where the encoder module tackles feature disentangling according to the introduced correlations, and the decoder module handles reconstruction using the combinations of decoupled features. Extensive experiments on four benchmark datasets demonstrate the effectiveness and robustness of the proposed method by attaining significant improvement over some state-of-the-art approaches. Source code is released at: https://github.com/flychen321/feature_disentangle_reid .},
  archive      = {J_ASOC},
  author       = {Feng Chen and Nian Wang and Jun Tang and Fan Zhu},
  doi          = {10.1016/j.asoc.2020.106939},
  journal      = {Applied Soft Computing},
  pages        = {106939},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature disentangling approach for person re-identification via self-supervised data augmentation},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A cyclic dynamic trust-based consensus model for large-scale
group decision making with probabilistic linguistic information.
<em>ASOC</em>, <em>100</em>, 106937. (<a
href="https://doi.org/10.1016/j.asoc.2020.106937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a consensus reaching process (CRP) considering dynamic trust in large-scale group decision making (LSGDM). In the traditional trust-based consensus model, it is assumed that the trust relationship generated by decision makers (DMs)’ previous knowledge remain unchanged during the whole decision process. However, this relationship will be dynamic rather than static especially in a social network with a new decision problem. This study explores the dynamic nature of trust through two stages. In the first stage, the trust degree will be functionally reformed by the conflict caused by DM’s opposite preferences. In the second stage, it will be effected by surroundings according to the “assimilation effect” in network. To handle the CRP with large-scale decision settings, a clustering technique is used to classify DMs with similar preference and preference accuracy. Based on the classifications, an optimization model is constructed to obtain the trust degrees between subgroups. The consensus measurements are investigated from similarity network within subgroups and min–max programming model between subgroups, respectively. Moreover, preference modification will effect trust in the aggregation and next iteration, the cyclic dynamic trust mechanism is established. The feasibility of the proposed model is verified by a numerical example. Comparisons declare the constructed consensus model’s universality without any essential conditions, as well as superiority with fully consideration of DM’s utility and centrality in network.},
  archive      = {J_ASOC},
  author       = {Xiao Tan and Jianjun Zhu and Francisco Javier Cabrerizo and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2020.106937},
  journal      = {Applied Soft Computing},
  pages        = {106937},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cyclic dynamic trust-based consensus model for large-scale group decision making with probabilistic linguistic information},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards minimization of overall inconsistency involved in
criteria weights for improved decision making. <em>ASOC</em>,
<em>100</em>, 106936. (<a
href="https://doi.org/10.1016/j.asoc.2020.106936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In any multi-criteria decision making (MCDM) process, the role of criteria weights is highly critical, as any inconsistency associated with their weights prompts an off-base assessment and leads to non-optimal selections. In this work, a novel subjective method for determining optimal criteria weights is presented, in which a newly proposed measure of inconsistency termed as ‘Error Index’ is used to formulate a non-linear constrained optimization model. The model was solved using Particle Swarm Optimization (PSO) technique, due to its faster convergence and simpler computation as compared to Genetic Algorithm , Ant Colony and, Firefly optimization methods. For the sake of simulation, a user-friendly solver named ‘Criteria Weight Estimation using Non-Linear Programming (CWENLiP)’ is also developed. Since the error index incorporates both the global and the local characteristics of the inconsistent matrix, it is more indicative of the overall inconsistency present in the comparison matrix, in contrast with the frequently employed consistency ratio index. Several numerical problems were solved to illustrate the procedure and to explore the potential applications of the proposed method. The robustness check, the statistical significance test and, the comparative analysis were also performed. The results revealed that the proposed method is highly robust and efficient. Hence, the Error Index based inconsistency quantification, combined with user-friendly CWENLiP solver, is a more reliable tool for criteria weight estimation which is crucial for accurate ranking and improved decision making , especially for problems encompassing large sets of criteria.},
  archive      = {J_ASOC},
  author       = {Sameera Mufazzal and Sarfaraz Masood and Noor Zaman Khan and S.M. Muzakkir and Zahid A. Khan},
  doi          = {10.1016/j.asoc.2020.106936},
  journal      = {Applied Soft Computing},
  pages        = {106936},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards minimization of overall inconsistency involved in criteria weights for improved decision making},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting COVID-19 daily cases using phone call data.
<em>ASOC</em>, <em>100</em>, 106932. (<a
href="https://doi.org/10.1016/j.asoc.2020.106932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need to forecast COVID-19 related variables continues to be pressing as the epidemic unfolds. Different efforts have been made, with compartmental models in epidemiology and statistical models such as AutoRegressive Integrated Moving Average (ARIMA), Exponential Smoothing (ETS) or computing intelligence models. These efforts have proved useful in some instances by allowing decision makers to distinguish different scenarios during the emergency, but their accuracy has been disappointing, forecasts ignore uncertainties and less attention is given to local areas. In this study, we propose a simple Multiple Linear Regression model, optimised to use phone call data to forecast the number of daily confirmed cases. Moreover, we produce a probabilistic forecast that allows decision makers to better deal with risk. Our proposed approach outperforms ARIMA, ETS, Seasonal Naive, Prophet and a regression model without call data, evaluated by three point forecast error metrics, one prediction interval and two probabilistic forecast accuracy measures. The simplicity, interpretability and reliability of the model, obtained in a careful forecasting exercise, is a meaningful contribution to decision makers at local level who acutely need to organise resources in already strained health services. We hope that this model would serve as a building block of other forecasting efforts that on the one hand would help front-line personal and decision makers at local level, and on the other would facilitate the communication with other modelling efforts being made at the national level to improve the way we tackle this pandemic and other similar future challenges.},
  archive      = {J_ASOC},
  author       = {Bahman Rostami-Tabar and Juan F. Rendon-Sanchez},
  doi          = {10.1016/j.asoc.2020.106932},
  journal      = {Applied Soft Computing},
  pages        = {106932},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting COVID-19 daily cases using phone call data},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ordered search with a large margin classifier for feature
selection. <em>ASOC</em>, <em>100</em>, 106930. (<a
href="https://doi.org/10.1016/j.asoc.2020.106930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a novel feature selection method for binary classification problems based on an ordered search process to explore the space of feature subsets. The method, called Admissible Ordered Search (AOS), uses a monotone evaluation function based on margin values calculated by a large margin classifier with an arbitrary L p Lp norm. This includes large margin classifiers with the L ∞ L∞ norm, which minimize the L 1 L1 norm and are very useful in feature selection, since they produce sparse solutions . An important contribution of this paper is the development of the projected margin concept. This value is computed as the maximal margin vector projected into a lower-dimensional subspace and it is used as an upper bound for the hypothesis value. This enables great economy in runtime and consequently efficiency in the search process as a whole. AOS was tested on several problems and its results were compared to other feature selection methods. The experiments demonstrate the competitive performance of the proposed method in terms of generalization power and computational efficiency.},
  archive      = {J_ASOC},
  author       = {Saulo Moraes Villela and Saul de Castro Leite and Adilson Elias Xavier and Raul Fonseca Neto},
  doi          = {10.1016/j.asoc.2020.106930},
  journal      = {Applied Soft Computing},
  pages        = {106930},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ordered search with a large margin classifier for feature selection},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Component-based 2-/3-dimensional nearest neighbor search
based on elias method to GPU parallel 2D/3D euclidean minimum spanning
tree problem. <em>ASOC</em>, <em>100</em>, 106928. (<a
href="https://doi.org/10.1016/j.asoc.2020.106928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present improved data parallel approaches working on graphics processing unit (GPU) compute unified device architecture (CUDA) platform to build hierarchical Euclidean minimum spanning forest or tree (EMSF/EMST) for applications whose input only contains N N points with arbitrary data distribution in 2D/3D Euclidean space. Characteristic of the proposed parallel algorithms follows “data parallelism , decentralized control and O ( 1 ) O(1) local memory occupied by each GPU thread”. This research has to solve GPU parallelism of component-based nearest neighbor search (component-based NNS), tree traversal , and other graph operations like union-find. For exact NNS, instead of using classical K-d tree search or brute-force computing method, we propose a K-d search method working based on dividing the Euclidean K-dimensional space into congruent and non-overlapping square/cubic cells where size of points in each cell is bounded. For component-based NNS, with the uniqueness property based on 2D/3D square/cubic space partition, we propose dynamic and static pruning techniques to prune unnecessary neighbor cells’ search. For tree traversal , instead of using breadth-first-search, this paper proposes CUDA kernels working with a distributed dynamic link list for selecting a local spanning tree’s shortest outgoing edge since size of local EMSTs in EMSF cannot be predicted. Source code is provided online and experimental comparisons are conducted on both 2D and 3D benchmarks with up to 1 0 7 107 points to build final EMST . Results show that applying K-d search with static pruning technique and the proposed operators totally working in parallel on GPU, our current implementation runs faster than our previous work and current optimal sequential dual-tree mlpack EMST library.},
  archive      = {J_ASOC},
  author       = {Wen-Bao Qiao and Jean-Charles Créput},
  doi          = {10.1016/j.asoc.2020.106928},
  journal      = {Applied Soft Computing},
  pages        = {106928},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Component-based 2-/3-dimensional nearest neighbor search based on elias method to GPU parallel 2D/3D euclidean minimum spanning tree problem},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of different metaheuristics for the quadratic
assignment problem in accelerated systems. <em>ASOC</em>, <em>100</em>,
106927. (<a href="https://doi.org/10.1016/j.asoc.2020.106927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although metaheuristics are used for solving complex and real-world problems, they do not provide an exact solution, but only an approximate solution in feasible time. However, for very large instances, a metaheuristic may take a considerable amount of time. Therefore, we utilize a highly parallel metaheuristic to run on a modern massively parallel graphics processing unit ( G P U ) (GPU) to further reduce the execution time. Additionally, because metaheuristics are not problem-specific, it is interesting to determine which metaheuristic is best for each problem type. In this study, using a massive parallel machine such as a GPU, we evaluate the performance of different metaheuristics such as the iterated local search , simulated annealing, genetic algorithm , tabu search , particle swarm optimization , and crow search algorithm with respect to the quadratic assignment problem .},
  archive      = {J_ASOC},
  author       = {Manoj Kumar and Aryabartta Sahu and Pinaki Mitra},
  doi          = {10.1016/j.asoc.2020.106927},
  journal      = {Applied Soft Computing},
  pages        = {106927},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comparison of different metaheuristics for the quadratic assignment problem in accelerated systems},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). A signed network analysis-based consensus reaching process
in group decision making. <em>ASOC</em>, <em>100</em>, 106926. (<a
href="https://doi.org/10.1016/j.asoc.2020.106926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the previous Consensus Reaching Process (CRP) models, although it has been a common understanding that positive social relationships between experts can prompt cooperation, meanwhile the existence of negative relations and the consequent antagonistic behaviors have been largely overlooked. In an attempt to fill in that gap, in this paper a signed network analysis-based CRP is proposed, where both the positive and negative relations can be taken into full account. In our proposed model, two types of centrality are taken advantage of to derive the experts’ importance degrees from their signed social network. Such importance degrees are then incorporated into the aggregation process via an Importance-Induced Ordered Weighting Average operator, where experts with higher importance degrees are assigned more weights to reflect their credibility. Consensus indices are defined to measure each expert’s consensus level to the group. And to help those experts with insufficient consensus indices, a feedback mechanism is developed where personalized preference revision advices are generated based on two rules: first, experts are attracted to the opinions of their positive social connections; and second, experts are repelled by the opinions of their negative social connections. An illustrative example is implemented to demonstrate the practicality of our model, and to investigate possible factors that may affect the CRP outcome. Finally, comparative analyses to several existing social network-based CRP methods are conducted.},
  archive      = {J_ASOC},
  author       = {Zi-xuan Zhang and You-wei Xu and Wen-ning Hao and Xiao-han Yu},
  doi          = {10.1016/j.asoc.2020.106926},
  journal      = {Applied Soft Computing},
  pages        = {106926},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A signed network analysis-based consensus reaching process in group decision making},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating public transport service quality using picture
fuzzy analytic hierarchy process and linear assignment model.
<em>ASOC</em>, <em>100</em>, 106920. (<a
href="https://doi.org/10.1016/j.asoc.2020.106920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group decision-making on a public service problem is often biased by untrustworthy responses of the participants, which affects negatively the outcome of the procedure. To eliminate this problem, the state-of-the-art methodologies apply fuzzy sets, generally within the frames of hybrid methodologies. The objective of the recent paper is to introduce a new hybrid model based on picture fuzzy sets and linear assignment and its first real-world application on a public transport development problem. The advantage of the proposed methodology is that it simultaneously considers the indeterminacy level of respondent evaluations on decision alternatives, and applies linear assignment to avoid subjectivity in responses. With the aim of testing the reliability of the new methodology, the results are compared with the generally applied intuitionistic fuzzy Technique of Order Preference Similarity to the Ideal Solution model. The comparative results have shown that the final outcomes of the proposed method are very similar to the well-proven reference technique, thus the hybrid picture fuzzy AHP-linear assignment model has been validated.},
  archive      = {J_ASOC},
  author       = {Fatma Kutlu Gündoğdu and Szabolcs Duleba and Sarbast Moslem and Serhat Aydın},
  doi          = {10.1016/j.asoc.2020.106920},
  journal      = {Applied Soft Computing},
  pages        = {106920},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating public transport service quality using picture fuzzy analytic hierarchy process and linear assignment model},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering-based anomaly detection in multivariate time
series data. <em>ASOC</em>, <em>100</em>, 106919. (<a
href="https://doi.org/10.1016/j.asoc.2020.106919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series data come as a collection of time series describing different aspects of a certain temporal phenomenon. Anomaly detection in this type of data constitutes a challenging problem yet with numerous applications in science and engineering because anomaly scores come from the simultaneous consideration of the temporal and variable relationships. In this paper, we propose a clustering-based approach to detect anomalies concerning the amplitude and the shape of multivariate time series. First, we use a sliding window to generate a set of multivariate subsequences and thereafter apply an extended fuzzy clustering to reveal a structure present within the generated multivariate subsequences. Finally, a reconstruction criterion is employed to reconstruct the multivariate subsequences with the optimal cluster centers and the partition matrix . We construct a confidence index to quantify a level of anomaly detected in the series and apply Particle Swarm Optimization as an optimization vehicle for the problem of anomaly detection. Experimental studies completed on several synthetic and six real-world datasets suggest that the proposed methods can detect the anomalies in multivariate time series. With the help of available clusters revealed by the extended fuzzy clustering , the proposed framework can detect anomalies in the multivariate time series and is suitable for identifying anomalous amplitude and shape patterns in various application domains such as health care, weather data analysis, finance, and disease outbreak detection.},
  archive      = {J_ASOC},
  author       = {Jinbo Li and Hesam Izakian and Witold Pedrycz and Iqbal Jamal},
  doi          = {10.1016/j.asoc.2020.106919},
  journal      = {Applied Soft Computing},
  pages        = {106919},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering-based anomaly detection in multivariate time series data},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A heuristic method to rank the alternatives in the AHP
synthesis. <em>ASOC</em>, <em>100</em>, 106916. (<a
href="https://doi.org/10.1016/j.asoc.2020.106916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a heuristic method (Bayesian cosine maximization method (BCCM)) to rank the alternatives in the Analytic Hierarchy Process (AHP) synthesis, based on the multiplicative AHP model, which focuses on the revision of the pair-wise comparison matrices (PCMs) and derivation of the priority vectors from the PCMs in whole hierarchy, considering both the consistency of the PCMs and total consistency. An Eight-step algorithm for the AHP synthesis is developed to how to revise the PCMs in the uncertainty context and generate the final priority vector of the alternatives, which obtains more accurate estimates of the priority vectors and provides a global AHP framework based on the multiplicative AHP model. Finally, two numerical examples and corresponding comparison with several other methods are implemented to illustrate the application and efficiency of the proposed BCCM.},
  archive      = {J_ASOC},
  author       = {Changsheng Lin and Gang Kou},
  doi          = {10.1016/j.asoc.2020.106916},
  journal      = {Applied Soft Computing},
  pages        = {106916},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A heuristic method to rank the alternatives in the AHP synthesis},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast energy-centered and QoS-aware service composition
approach for internet of things. <em>ASOC</em>, <em>100</em>, 106914.
(<a href="https://doi.org/10.1016/j.asoc.2020.106914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many scholars have paid more attention to the service-oriented computing for Internet of Things (IoT). With the increasing smart devices are deployed in IoT, it has become a challenging problem that how to quickly combine the services provided by IoT components into a comprehensive composite service to balance energy and quality of service (QoS). In this article, a fast energy-centered and QoS-aware service composition approach (FSCA-EQ) is proposed for IoT service composition. The FSCA-EQ employs the idea of hierarchical optimization. First, the compromise ratio method (CRM) is used to pre-select services satisfying user’s QoS requirements. Then, to reduce the energy consumption and extend the lifetime of IoT devices, the concept of relative dominance is applied to select the optimal service as the final composite service. The relative dominance of the service depends on its energy profile, QoS attributes, and user’s preferences. The simulation results show that the proposed algorithm performs better in terms of selection time, energy consumption, and optimality .},
  archive      = {J_ASOC},
  author       = {Zheng-yi Chai and Meng-meng Du and Guo-zhi Song},
  doi          = {10.1016/j.asoc.2020.106914},
  journal      = {Applied Soft Computing},
  pages        = {106914},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fast energy-centered and QoS-aware service composition approach for internet of things},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fuzzy matrix factor recommendation method with forgetting
function and user features. <em>ASOC</em>, <em>100</em>, 106910. (<a
href="https://doi.org/10.1016/j.asoc.2020.106910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a variety of recommendation algorithms have been proposed. However, there are still some issues such as cold start, sparsity and timeliness. On the basis of traditional collaborative filtering, matrix decomposition technologies can greatly reduce the computational complexity and address the issues of cold start and sparsity. Based on forgetting functions and users’ multi-attribute features, in this paper, a fuzzy matrix decomposition and trace norm minimization method is proposed. Firstly, a forgetting function is incorporated into the historical scores, which makes existing scoring information more efficient. Then the user features are applied to find trusted users, which will give some valuable suggestions and possible scores to target items. The user features are also applied to list the relationships among users. In this paper, the relations between users and items are represented by the scores between them. Secondly, both time and feature information is integrated into the proposed optimization model. Then a fuzzy matrix decomposition and trace norm minimization model is established and further solved by our proposed algorithm to give better recommendations. Moreover, the convergence of the proposed algorithm is also proved in theoretically. Finally, we investigate the empirical recoverability properties of our model and analyze its advantages over classical norm minimization models. Extensive experimental results on synthetic and real-world data sets verify the efficiency and effectiveness of our method compared with the state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Jianrui Chen and Yanqing Lu and Fanhua Shang and Yuyang Wang},
  doi          = {10.1016/j.asoc.2020.106910},
  journal      = {Applied Soft Computing},
  pages        = {106910},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy matrix factor recommendation method with forgetting function and user features},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WhoReview: A multi-objective search-based approach for code
reviewers recommendation in modern code review. <em>ASOC</em>,
<em>100</em>, 106908. (<a
href="https://doi.org/10.1016/j.asoc.2020.106908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary software development is distributed and characterized by high dynamics with continuous and frequent changes to fix defects, add new user requirements or adapt to other environmental changes. To manage such changes and ensure software quality, modern code review is broadly adopted as a common and effective practice. Yet several open-source as well as commercial software projects have adopted peer code review as a crucial practice to ensure the quality of their software products using modern tool-based code review. Nevertheless, the selection of peer reviewers is still merely a manual and hard task especially with the growing size of distributed development teams. Indeed, it has been proven that inappropriate peer reviewers selection can consume more time and effort from both developers and reviewers and increase the development costs and time to market. To address this problem, we introduce a multi-objective search-based approach, named WhoReview, to find the optimal set of peer reviewers for code changes. We use the Indicator-Based Evolutionary Algorithm (IBEA) to find the best set of code reviewers that are (1) most experienced with the code change to be reviewed, while (2) considering their current workload, i.e. , the number of open code reviews they are working on. We conduct an empirical study on 4 long-lived open source software projects to evaluate our approach. The obtained results show that WhoReview outperforms state-of-the-art approach by an average precision of 68\% and recall of 77\%. Moreover, we deployed our approach in an industrial context and evaluated it qualitatively from developers perspective. Results show the effectiveness of our approach with a high acceptance ratio in identifying relevant reviewers.},
  archive      = {J_ASOC},
  author       = {Moataz Chouchen and Ali Ouni and Mohamed Wiem Mkaouer and Raula Gaikovina Kula and Katsuro Inoue},
  doi          = {10.1016/j.asoc.2020.106908},
  journal      = {Applied Soft Computing},
  pages        = {106908},
  shortjournal = {Appl. Soft. Comput.},
  title        = {WhoReview: A multi-objective search-based approach for code reviewers recommendation in modern code review},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fault diagnosis based on deep learning for current-carrying
ring of catenary system in sustainable railway transportation.
<em>ASOC</em>, <em>100</em>, 106907. (<a
href="https://doi.org/10.1016/j.asoc.2020.106907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the intelligent traffic transportation, the security and stability are vital for the sustainable transportation and efficient logistics. The fault diagnosis on the catenary system is crucial for the railway transportation. For purpose of improving the detection capability for the faulted current-carrying ring and maintaining the efficiency of the railway system , a fault diagnosis method for the current-carrying ring based on an improved RetinaNet model with the spatial attention map and channel weight map is proposed. The local and global features are utilized respectively. The spatial attention maps are embedded into the original convolutional neural network to emphasize the interested local features and weaken the influence of other objects and background. The channel weight maps are introduced into the feature pyramid network of detection network to weight the multi-scale feature maps in channels. The representative global features are focused and unnecessary features are suppressed. The experimental results indicate that the proposed method has increased fault diagnosis accuracy for faulted current-carrying rings compared with the original detection network based on different backbones. It can be used in improving efficiency and safety of railway transport system.},
  archive      = {J_ASOC},
  author       = {Yuwen Chen and Bin Song and Yuan Zeng and Xiaojiang Du and Mohsen Guizani},
  doi          = {10.1016/j.asoc.2020.106907},
  journal      = {Applied Soft Computing},
  pages        = {106907},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault diagnosis based on deep learning for current-carrying ring of catenary system in sustainable railway transportation},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differential evolution algorithm with wavelet basis function
and optimal mutation strategy for complex optimization problem.
<em>ASOC</em>, <em>100</em>, 106724. (<a
href="https://doi.org/10.1016/j.asoc.2020.106724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization performance of differential evolution(DE) algorithm significantly depends on control parameters and mutation strategy. However, it is difficult to set suitable control parameters and select reasonable mutation strategy for DE in solving an actual engineering optimization problem . To solve these problems, a new optimal mutation strategy based on the complementary advantages of five mutation strategies is designed to develop a novel improved DE algorithm with the wavelet basis function, named WMSDE, which can improve the search quality, accelerate convergence and avoid fall into local optimum and stagnation. In the proposed WMSDE, the initial population is divided into several subpopulations to exchange search information between the different subpopulations and improve the population diversity to a certain extent. The wavelet basis function and normal distribution function are used to control the scaling factor and the crossover rate respectively in order to ensure the diversity of solutions and accelerate convergence. The new optimal mutation strategy is used to improve the local search ability and ensure the global search ability. Finally, the proposed WMSDE is compared with five state-of-the-art DE variants by 11 benchmark functions . The experiment results indicate that the proposed WMSDE can avoid premature convergence, balance local search ability and global search ability, accelerate convergence, improve the population diversity and the search quality. Additionally, a real-world airport gate assignment problem is employed to further prove the effectiveness of the proposed WMSDE. The results show that it can effectively solve the complex airport gate assignment problem, and obtain airport gate assignment rate of 97.6\%.},
  archive      = {J_ASOC},
  author       = {Wu Deng and Junjie Xu and Yingjie Song and Huimin Zhao},
  doi          = {10.1016/j.asoc.2020.106724},
  journal      = {Applied Soft Computing},
  pages        = {106724},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution algorithm with wavelet basis function and optimal mutation strategy for complex optimization problem},
  volume       = {100},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AHP integrated TOPSIS and VIKOR methods with pythagorean
fuzzy sets to prioritize risks in self-driving vehicles. <em>ASOC</em>,
<em>99</em>, 106948. (<a
href="https://doi.org/10.1016/j.asoc.2020.106948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-driving vehicles are of critical importance to a future sustainable transport system, which is expected to become widespread around the world. However a substantial amount of risk is associated with self-driving vehicles which must be considered by decision-makers effectively. Given that automated driving technology and how it will interact with the mobility system are substantially risky, the risks involved in self-driving vehicles need to be addressed appropriately. The identified knowledge gap of the pre-literature review is that an overview of the identification which completely considers all types of risks related to self-driving vehicles does not exist. In response to this knowledge gap, this study aims to prioritize the risks in self-driving vehicles. Risk prioritization is a complicated multi-criteria decision making (MCDM) problem that requires consideration of multiple feasible alternatives and conflicting tangible and intangible criteria. This study addresses the prioritization of risks involved with self-driving vehicles by proposing new hybrid MCDM methods based on the Analytic Hierarchy Process (AHP), the Technique for order preference by similarity to an ideal solution (TOPSIS) and Vlse Kriterijumska Optimizacija I Kompromisno Resenje (VIKOR) under Pythagorean fuzzy environment. The result of the proposed model is validated by performing sensitivity analysis. The performance of proposed methodology with Pythagorean fuzzy sets is also compared with those with ordinary fuzzy sets and it is revealed that the proposed method produces reliable and informative outcomes better representing the impreciseness of decision making problems. The findings of this study will provide useful insight to the planners and policymakers for decision making in self-driving vehicles.},
  archive      = {J_ASOC},
  author       = {Gozde Bakioglu and Ali Osman Atahan},
  doi          = {10.1016/j.asoc.2020.106948},
  journal      = {Applied Soft Computing},
  pages        = {106948},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AHP integrated TOPSIS and VIKOR methods with pythagorean fuzzy sets to prioritize risks in self-driving vehicles},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large-scale many-objective particle swarm optimizer with
fast convergence based on alpha-stable mutation and logistic function.
<em>ASOC</em>, <em>99</em>, 106947. (<a
href="https://doi.org/10.1016/j.asoc.2020.106947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenges of the most multi-objective particle swarm optimization (MOPSO) algorithms are to improve the selection pressure, equilibrate the convergence and diversity when tackling large-scale many-objective problems. To overcome these challenges, this paper proposes a novel PSO-based large-scale many-objective algorithm, named as LMPSO. In LMPSO, the Alpha-stable mutation is performed to enhance the diversity of swarm for avoiding premature convergence. And the parameters of PSO and Alpha-stable mutation are dynamically set following the Logistic function , which emphasize different convergence and diversity at different optimization stages . Moreover, LMPSO adopts a fitness to maintain the external archive , and the calculation of fitness is based on binary additive epsilon indicator. The binary indicator is also used to update the personal best of particles to avoid wrongly selecting dominance resistance solutions (DRSs). Aims for improving the selection pressure, the proposed algorithm employs a concept of dominance resistance error to identify the DRSs. To verify this idea, the DTLZ, ZDT, and LSMOP test suites with up to 1000 decision variables and 10-objective are used to qualify the performance of LMPSO. The simulations reveal the fact that the LMPSO significantly outruns the several chosen state-of-the-art algorithms when solving large-scale many-objective test instances.},
  archive      = {J_ASOC},
  author       = {Shixin Cheng and Hao Zhan and Huiqin Yao and Huayu Fan and Yan Liu},
  doi          = {10.1016/j.asoc.2020.106947},
  journal      = {Applied Soft Computing},
  pages        = {106947},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Large-scale many-objective particle swarm optimizer with fast convergence based on alpha-stable mutation and logistic function},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective iterated greedy algorithm for solving a
multi-compartment AGV scheduling problem in a matrix manufacturing
workshop. <em>ASOC</em>, <em>99</em>, 106945. (<a
href="https://doi.org/10.1016/j.asoc.2020.106945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address a multi-compartment automatic guided vehicle scheduling (MC-AGVS) problem from a matrix manufacturing workshop that has attracted more and more attention of manufacturing firms in recent years. The problem aims to determine a solution to minimize the total cost including the travel cost, the service cost, and the cost of vehicles involved. For this purpose, a mixed-integer linear programming model is first constructed. Then, a novel iterated greedy (IG) algorithm including accelerations for evaluating objective functions of neighboring solutions; an improved nearest-neighbor-based constructive heuristic; an improved sweep-based constructive heuristic; an improved destruction procedure; and a simulated annealing type of acceptance criterion is proposed. At last, a series of comparative experiments are implemented based on some real-world instances from an electronic equipment manufacturing enterprise. The computational results demonstrate that the proposed IG algorithm has generated substantially better solutions than the existing algorithms in solving the problem under consideration.},
  archive      = {J_ASOC},
  author       = {Wen-Qiang Zou and Quan-Ke Pan and M. Fatih Tasgetiren},
  doi          = {10.1016/j.asoc.2020.106945},
  journal      = {Applied Soft Computing},
  pages        = {106945},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective iterated greedy algorithm for solving a multi-compartment AGV scheduling problem in a matrix manufacturing workshop},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel dependency definition exploiting boundary samples in
rough set theory for hyperspectral band selection. <em>ASOC</em>,
<em>99</em>, 106944. (<a
href="https://doi.org/10.1016/j.asoc.2020.106944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction is considered to be a primary task for effective classification of hyperspectral images . In this work, a novel feature (band) selection technique based on rough set theory is presented to reduce the dimensionality of hyperspectral images . Here, a new definition of dependency measure in rough set theory is proposed by not only considering the objects in the positive region but also some objects from the boundary region. The proposed dependency definition is completely parameter free and computationally very cheap. Our technique, first, defines a novel criterion by combining the relevance and significance measure computed using the proposed dependency definition. Then, a first-order incremental search is adopted to select the most informative bands by maximizing the defined criterion. The proposed band selection technique shows better result compared to the existing rough set based band selection techniques on three real hyperspectral data sets.},
  archive      = {J_ASOC},
  author       = {Swarnajyoti Patra and Barnali Barman},
  doi          = {10.1016/j.asoc.2020.106944},
  journal      = {Applied Soft Computing},
  pages        = {106944},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel dependency definition exploiting boundary samples in rough set theory for hyperspectral band selection},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CFPS: Collaborative filtering based source projects
selection for cross-project defect prediction. <em>ASOC</em>,
<em>99</em>, 106940. (<a
href="https://doi.org/10.1016/j.asoc.2020.106940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction aims at helping developers allocate existing resources by predicting defect-prone modules prior to the testing phase. In the past decade, cross-project defect prediction (CPDP) have gained more attention than within-project defect prediction (WPDP) as WPDP is usually inefficient with the scarcity of training data due to the absence of historical defect data. Currently most CPDP studies focus on selecting appropriate training instances for improving the performance of defect prediction while few studies pay attention to the selection of appropriate source projects. However, in practice, source projects selection is the basis and prerequisite of training instances selection as an increasing number of open source software defect data are now available. In present study, we propose a C ollaborative F iltering based source P rojects S election (CFPS) method for cross-project defect prediction. For a given new project, the similarity between it and each historical project is firstly calculated and thus the corresponding similarity repository could be obtained. Then CFPS mines the applicability among historical projects for constructing an applicability repository. Finally, with the aforementioned applicability and similarity repository, the popular user-based collaborative filtering algorithm is employed to recommend the appropriate source projects for the given new project. In the experiment, we have empirically validated the importance and necessity of selecting appropriate source projects. Furthermore, the experimental results also demonstrate that the proposed CFPS method is feasible and effective.},
  archive      = {J_ASOC},
  author       = {Zhongbin Sun and Junqi Li and Heli Sun and Liang He},
  doi          = {10.1016/j.asoc.2020.106940},
  journal      = {Applied Soft Computing},
  pages        = {106940},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CFPS: Collaborative filtering based source projects selection for cross-project defect prediction},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Surrogate-assisted teaching-learning-based optimization for
high-dimensional and computationally expensive problems. <em>ASOC</em>,
<em>99</em>, 106934. (<a
href="https://doi.org/10.1016/j.asoc.2020.106934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a surrogate-assisted teaching-learning-based optimization algorithm is presented for high-dimensional and computationally expensive black-box optimization problems . In the presented method, a two-phase searching framework is proposed to realize the exploitation of surrogates and the metaheuristic exploration. Specifically, radial basis functions are used to build the dynamically updated surrogate models . Moreover, a surrogate-assisted knowledge mining strategy is proposed to sufficiently collect valuable information in each cycle. In this strategy, multiple groups of promising points are selected from the expensive sample set to construct the subspaces and local surrogate models , from which the most potential points are captured for the subsequent updates and optimization. At the same time, a population composed of the present best expensive samples carries out the teaching/learning-based search, accelerating local convergence and promoting global exploration. In the surrogate-assisted teaching phase, both of individual and mean behaviors are considered to make learners go close to the teacher efficiently. In the surrogate-assisted learning phase, a more extensive search range is defined to improve sampling diversity. The cooperation of the surrogate-assisted knowledge mining and the modified teaching-learning-based exploration makes the new method have excellent performance on 21 benchmark cases with 30–100 design variables. Furthermore, this method is used for the shape optimization of a blended-wing-body underwater glider, and gets impressive results.},
  archive      = {J_ASOC},
  author       = {Huachao Dong and Peng Wang and Xinkai Yu and Baowei Song},
  doi          = {10.1016/j.asoc.2020.106934},
  journal      = {Applied Soft Computing},
  pages        = {106934},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-assisted teaching-learning-based optimization for high-dimensional and computationally expensive problems},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An algorithm for automatic dormant tree pruning.
<em>ASOC</em>, <em>99</em>, 106931. (<a
href="https://doi.org/10.1016/j.asoc.2020.106931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree pruning is a labor and cost-intensive task. Still, it is a necessary activity that ensures a high yield of good quality products in horticulture and increases the overall health of trees in general. Extensive research has been done attempting to automate this labor-intensive procedure, lower the cost, and demand a skilled workforce. We introduce a new algorithm based on discrete differential evolution that simulates the pruning of virtual trees. Although pruning driven by differential evolution alone optimizes the overall tree light intake, it cannot maintain the distance between individual trees, nor can it shape trees into any of the growing forms. In the article, we show that adding additional steps into the pruning process, which is an initial trimming of the tree into a desired shape, can be improved significantly. We demonstrate our method by simulating the pruning of virtual trees and show that it provides results comparable to the results obtained by a human expert. By simulating the tree pruning over a few consecutive years, We show that our method is also capable of autonomous tree training toward the desired growing form.},
  archive      = {J_ASOC},
  author       = {Simon Kolmanič and Damjan Strnad and Štefan Kohek and Bedrich Benes and Peter Hirst and Borut Žalik},
  doi          = {10.1016/j.asoc.2020.106931},
  journal      = {Applied Soft Computing},
  pages        = {106931},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An algorithm for automatic dormant tree pruning},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic clustering method using multi-objective genetic
algorithm with gene rearrangement and cluster merging. <em>ASOC</em>,
<em>99</em>, 106929. (<a
href="https://doi.org/10.1016/j.asoc.2020.106929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an unsupervised approach of machine learning , clustering is an important method to understand and learn structural information from data. However, current adaptive clustering approach based on multi-objective genetic algorithm have two apparent limitations. The first is that prior knowledge, i.e., sample information is needed to get the correct cluster number. The second is that no effective method can be found to select the best clustering solution from the Pareto Optimal Front (POF) generated by a multi-objective optimization. These problems become severer in applications applied on non-category datasets. Therefore, the primary goal of this research is to establish a genetic optimization based multi-objective clustering framework, in which multiple clustering validity indexes (CVIs) can be tested simultaneously to automatically obtain the optimal cluster number without knowing any sample label information in advance. In this effort, we will not only be able to consider clustering measurements such as cluster cohesion and separation, but also take other aspects, such as compactness, connectivity, variation among data elements, into consideration as well. Then, we aim to design a procedure to recommend three best solutions from the POF by using appropriate combination of CVIs without increasing computational cost. This procedure is expected to control the cluster number in a reasonable range and consequently decrease the difficulty in best solution recommendation. Finally, since we have the knowledge that using gene rearrangement in the genetic optimization does not affect partition, we take this advantage to merge clusters effectively and significantly speed the convergence of the algorithm. Our approach can outperform the state-of-the-art counterparts across diverse benchmark datasets in terms of partitioning accuracy and performance, as demonstrated in three experiments conducted on both artificial and typical real-world datasets.},
  archive      = {J_ASOC},
  author       = {Hongchun Qu and Li Yin and Xiaoming Tang},
  doi          = {10.1016/j.asoc.2020.106929},
  journal      = {Applied Soft Computing},
  pages        = {106929},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An automatic clustering method using multi-objective genetic algorithm with gene rearrangement and cluster merging},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic evolutionary model based on a multi-sampling
inherited HAPFNN for an aluminium electrolysis manufacturing system.
<em>ASOC</em>, <em>99</em>, 106925. (<a
href="https://doi.org/10.1016/j.asoc.2020.106925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is technically difficult to accurately establish a dynamic adaptive model of an aluminium electrolysis manufacturing system (AEMS) because the system has many complex characteristics, such as multiple parameters, dynamic time variance, and a non-Gaussian distribution of process data. Inspired by the overall superiority of particle filtering theory in dealing with non-linear non-Gaussian problems, this paper presents a novel method based on a multi-sampling inherited hybrid annealed particle filter neural network (MSI-HAPFNN). Firstly, the neural network’s (NN’s) weights and thresholds are used as the state variables of hybrid annealed particle filter; Secondly, the hybrid proposal distribution obtained by sampling the above state variables is employed to replace the posterior proposal distribution in the standard particle filter (PF) algorithm as the importance density function, thereby adjusting the NN’s weights and thresholds in real time. Thirdly, the model achieves the features of multi-sampling and inheritance by introducing NN and PF weights, and using adaptive inheritance method. Therefore, this paper systematically proposes the theoretical construction framework and experimental procedure of MSI-HAPFNN. Furthermore, this article also introduces a genetic algorithm to thoroughly evaluate the prediction potential. The proposed model has been tested on the real-world system for aluminium electrolysis manufacturing and compared with several closely related frameworks. The experimental results show that the MSI-HAPFNN model can significantly improve the self-adaptive ability of the object system to working conditions and the prediction accuracy of power consumption , which is helpful in finding optimal design parameters in an AEMS.},
  archive      = {J_ASOC},
  author       = {Wei Ding and Lizhong Yao and Yanyan Li and Wei Long and Jun Yi and Tiantian He},
  doi          = {10.1016/j.asoc.2020.106925},
  journal      = {Applied Soft Computing},
  pages        = {106925},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic evolutionary model based on a multi-sampling inherited HAPFNN for an aluminium electrolysis manufacturing system},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-singleton fuzzy control for multi-synchronization of
chaotic systems. <em>ASOC</em>, <em>99</em>, 106924. (<a
href="https://doi.org/10.1016/j.asoc.2020.106924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper the problem of multi-synchronization with time-varying switching modes is introduced and a novel adaptive control method is designed. Unlike to the most studies, in addition to the unknown dynamics, the values of fractional-orders and control input nonlinearities are also considered to be unknown. A new fractional-order dynamic model on basis of non-singleton interval type-3 fuzzy logic system (NIT3-FLS) is presented to estimate the uncertainties. A fractional-order tuning rule is derived to optimize NIT3-FLS. An adaptive compensator is designed to tackle the effects of approximation errors, switching of synchronization modes and dynamic uncertainties and perturbations. The superiority of the suggested control scheme is verified through the several simulations and comparisons with the other control systems and other kind of FLSs.},
  archive      = {J_ASOC},
  author       = {Mohammad Ahmadi Balootaki and Hossein Rahmani and Hossein Moeinkhah and Ardashir Mohammadzadeh},
  doi          = {10.1016/j.asoc.2020.106924},
  journal      = {Applied Soft Computing},
  pages        = {106924},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Non-singleton fuzzy control for multi-synchronization of chaotic systems},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy multi-hop clustering protocol: Selection fuzzy input
parameters and rule tuning for WSNs. <em>ASOC</em>, <em>99</em>, 106923.
(<a href="https://doi.org/10.1016/j.asoc.2020.106923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the most important aspects of wireless sensor networks (WSNs) are to make optimal use of and direct the limited energy of sensor nodes towards the desired application and prolong the network lifetime for that application. Although a few studies exist that have addressed these special goals, they have been mostly focused on the process of selecting cluster heads (CHs) and forwarders. No studies have been conducted so far on the selection of fuzzy input parameters in clustering and routing processes as well as the application-based parameter selection process. Generally, a fixed number of parameters have always been selected by designers. Hence, the shuffled frog leaping algorithm (SFLA) was employed in this paper to propose a technique for selecting fuzzy input parameters in a fuzzy multi-hop clustering protocol named the PS-SFLA. This technique includes three main phases, introduced in three versions for the sake of stepwise evaluation. Based on the literature review, the most frequent and diverse parameters were extracted and formulated in the first version. The proposed technique used the SFLA in the second version to select the appropriate parameters fitting the application specifics and scenario and determine the coefficients of parameters simultaneously so that they could be used as the inputs of the fuzzy inference system . It was also utilized in the final version for the automated, accurate, application-based tuning of fuzzy rules before the network was set up. By design, different versions of the PS-SFLA act as the starting points of the next version in addition to the fact that they can be evaluated separately. The PS-SFLA was compared with the LEACH, ASLPR, SIF, ERA, and FSFLA in different scenarios and two applications from the perspective of the alive nodes, the number of packets received by the BS , lifetime, and other factors. According to the simulation results, the PS-SFLA outperformed all of the other methods greatly in all scenarios and applications and PS-SFLA increases the lifetime due to the appropriate selection of fuzzy input parameters based on application and purpose.},
  archive      = {J_ASOC},
  author       = {Fakhrosadat Fanian and Marjan Kuchaki Rafsanjani and Arsham Borumand Saeid},
  doi          = {10.1016/j.asoc.2020.106923},
  journal      = {Applied Soft Computing},
  pages        = {106923},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy multi-hop clustering protocol: Selection fuzzy input parameters and rule tuning for WSNs},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A positive real order weakening buffer operator and its
applications in grey prediction model. <em>ASOC</em>, <em>99</em>,
106922. (<a href="https://doi.org/10.1016/j.asoc.2020.106922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the idea of fractional calculus, reverse accumulated generating operators of real number’s order are proposed in this paper. We claim the set of these operators forms a 1-dimensional additive real Lie group, which is isomorphic to R R . As a direct consequence, the weakening buffer operators of positive real number’s order are constructed. Compared with traditional weakening buffer operators of integer number’s order, this formulation could make finer adjustments on weight distributions between original data sequence by varying the order of weakening buffer operators, which can take a balance between the influence of disturbance components and the tracking speed of parameter estimation. Applications of this construction are used in GM ( 1 , 1 ) (1, 1) model and multiple linear regression models, experimental results show that the proposed buffer operator can effectively improve the prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Jianke Chen and Zhengpeng Wu},
  doi          = {10.1016/j.asoc.2020.106922},
  journal      = {Applied Soft Computing},
  pages        = {106922},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A positive real order weakening buffer operator and its applications in grey prediction model},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). StackPDB: Predicting DNA-binding proteins based on XGB-RFE
feature optimization and stacked ensemble classifier. <em>ASOC</em>,
<em>99</em>, 106921. (<a
href="https://doi.org/10.1016/j.asoc.2020.106921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA-binding proteins (DBPs) not only play an important role in all aspects of genetic activities such as DNA replication , recombination, repair, and modification but also are used as key components of antibiotics, steroids, and anticancer drugs in the field of drug discovery . Identifying DBPs becomes one of the most challenging problems in the domain of proteomics research. Considering the high-priced and inefficient of the experimental method, constructing a detailed DBPs prediction model becomes an urgent problem for researchers. In this paper, we propose a stacked ensemble classifier based method for predicting DBPs called StackPDB. Firstly, pseudo amino acid composition (PseAAC), pseudo-position-specific scoring matrix (PsePSSM), position-specific scoring matrix-transition probability composition (PSSM-TPC), evolutionary distance transformation (EDT), and residue probing transformation (RPT) are applied to extract protein sequence features. Secondly, extreme gradient boosting-recursive feature elimination (XGB-RFE) is employed to gain an excellent feature subset. Finally, the best features are applied to the stacked ensemble classifier composed of XGBoost , LightGBM, and SVM to construct StackPDB. After applying leave-one-out cross-validation (LOOCV), StackPDB obtains high ACC and MCC on PDB1075, 93.44\% and 0.8687, respectively. Besides, the ACC of the independent test datasets PDB186 and PDB180 are 84.41\% and 90.00\%, respectively. The MCC of the independent test datasets PDB186 and PDB180 are 0.6882 and 0.7997, respectively. The results on the training dataset and the independent test dataset show that StackPDB has a great predictive ability to predict DBPs.},
  archive      = {J_ASOC},
  author       = {Qingmei Zhang and Peishun Liu and Xue Wang and Yaqun Zhang and Yu Han and Bin Yu},
  doi          = {10.1016/j.asoc.2020.106921},
  journal      = {Applied Soft Computing},
  pages        = {106921},
  shortjournal = {Appl. Soft. Comput.},
  title        = {StackPDB: Predicting DNA-binding proteins based on XGB-RFE feature optimization and stacked ensemble classifier},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Evaluating community question-answering websites using
interval-valued intuitionistic fuzzy DANP and TODIM methods.
<em>ASOC</em>, <em>99</em>, 106918. (<a
href="https://doi.org/10.1016/j.asoc.2020.106918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As important knowledge-sharing platforms, community question-answering (CQA) websites have attracted the attention of many users. An evaluation of the quality of CQA websites can assist users in selecting high-quality CQA websites and can help the operators of CQA websites achieve the most beneficial improvements. An approach to the evaluation of CQA websites is proposed in this study. First, a CQA service quality model is constructed to evaluate the quality of CQA websites based on the E-SERVQUAL model. A novel multicriteria decision-making (MCDM) model is proposed to address the ratings. In the model, uncertainty and consistency are combined in an interval-valued intuitionistic fuzzy (IVIF) environment to determine two kinds of expert weights. The DANP method, along with experts’ weights, is extended to the IVIF environment to derive the criteria weights. The IVIF TODIM method, along with experts’ weights, is used to rank the CQA websites in terms of quality. Finally, the proposed approach is applied to evaluate the quality of five CQA websites. The experimental results of the application, along with sensitivity analysis and comparative analysis, demonstrate the feasibility and practicality of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Ming Li and Ying Li and Qijin Peng and Jie Wang and Chunxia Yu},
  doi          = {10.1016/j.asoc.2020.106918},
  journal      = {Applied Soft Computing},
  pages        = {106918},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating community question-answering websites using interval-valued intuitionistic fuzzy DANP and TODIM methods},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A self-adaptive kernel extreme learning machine for
short-term wind speed forecasting. <em>ASOC</em>, <em>99</em>, 106917.
(<a href="https://doi.org/10.1016/j.asoc.2020.106917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind speed forecasting with artificial neural networks (ANNs) plays important role in safely utilizing and integrating the wind power. With the rapid updated wind speed data, however, the only way to guarantee forecasting accuracy for these ANN models is re-training from scratch with an updated training dataset. Obviously, it is an inefficient work due to the resumption of constructing the new training dataset and re-training the model. To enhance training efficiency, reduce re-training cost and improve forecasting accuracy , a self-adaptive kernel extreme learning machine (KELM) is proposed in this paper. With an advanced and efficient learning process, the self-adaptive KELM could simultaneously obsolete old data and learn from new data by reserving overlapped information between the updated and old training datasets. To evaluate the efficiency and accuracy of the self-adaptive KELM, the wind speed data from three different stations are employed as a numerical experiment. The Mean Absolute Error (MAE), Mean Square Error (MSE) and Mean Absolute Percentage Error (MAPE) showed that the self-adaptive KELM with a simple structure could obtain more accurate forecasting results at a faster calculation speed than comparison models, where the proposed model decreased the MAPE values with 7.4776\%, 3.5329\% and 2.0900\% in 1-step, 3-step and 5-step forecasting, respectively},
  archive      = {J_ASOC},
  author       = {Liye Xiao and Wei Shao and Fulong Jin and Zhuochun Wu},
  doi          = {10.1016/j.asoc.2020.106917},
  journal      = {Applied Soft Computing},
  pages        = {106917},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-adaptive kernel extreme learning machine for short-term wind speed forecasting},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot learning through observation via coarse-to-fine
grained video summarization. <em>ASOC</em>, <em>99</em>, 106913. (<a
href="https://doi.org/10.1016/j.asoc.2020.106913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning human daily behavior is important for enabling robots to perform tasks and assist people. However, most prior work either requires specific sensors for capturing data or heavily relies on prior knowledge of human motion , which can be difficult to obtain. To alleviate the above problems, we propose a novel pipeline for robots to learn human behavior based on coarse-to-fine video summarization using a single Kinect camera. Specifically, the robot first retrieves information of general interest followed by a task-specific content retrieval, then focuses on fine-grained motion clips of human behavior, and guides itself by using an object-centric learning method to complete the desired task. Our work has three unique advantages: (1) it enables the robot to effectively capture granularity hierarchies of human behavior which efficiently exploits multi-stage information while alleviating disturbances and redundancies in visual data; (2) it obtains knowledge by focusing on object movements in summarized motion clips which does not require any prior knowledge of human motion ; (3) it only requires a single Kinect sensor for the robot to learn human behavior which is fully accessible and easy to equip. Experiments in an office environment were performed to validate the efficiency and effectiveness of the proposed framework, and the results indicate that this approach exhibits good learning efficacy for the robot to understand human behavior and learn to perform tasks.},
  archive      = {J_ASOC},
  author       = {Yujia Zhang and Qianzhong Li and Xiaoguang Zhao and Min Tan},
  doi          = {10.1016/j.asoc.2020.106913},
  journal      = {Applied Soft Computing},
  pages        = {106913},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robot learning through observation via coarse-to-fine grained video summarization},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An eigenspace divide-and-conquer approach for large-scale
optimization. <em>ASOC</em>, <em>99</em>, 106911. (<a
href="https://doi.org/10.1016/j.asoc.2020.106911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Divide-and-conquer-based (DC-based) evolutionary algorithms (EAs) have achieved notable success in dealing with large-scale optimization problems (LSOPs). However, the appealing performance of this type of algorithms generally requires a high-precision decomposition of the optimization problem , which is still a challenging task for existing decomposition methods . This study attempts to address the above issue from a different perspective and proposes an eigenspace divide-and-conquer (EDC) approach. Different from existing DC-based algorithms that perform decomposition and optimization in the original solution space, EDC first establishes an eigenspace by conducting singular value decomposition on a set of high-quality solutions selected from recent generations. Then it transforms the optimization problem into the eigenspace, and thus significantly weakens the dependencies among the corresponding eigenvariables. Accordingly, these eigenvariables can be efficiently grouped by a simple random decomposition strategy and each of the resulting subproblems can be addressed more easily by a traditional EA. To verify the efficiency of EDC, comprehensive experimental studies were conducted on two sets of benchmark functions . Experimental results indicate that EDC is robust to its parameters and has good scalability to the problem dimension. The comparison with several state-of-the-art algorithms further confirms that EDC is pretty competitive and performs better on complicated LSOPs.},
  archive      = {J_ASOC},
  author       = {Zhigang Ren and Yongsheng Liang and Muyi Wang and Yang Yang and An Chen},
  doi          = {10.1016/j.asoc.2020.106911},
  journal      = {Applied Soft Computing},
  pages        = {106911},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An eigenspace divide-and-conquer approach for large-scale optimization},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting COVID-19 patients based on fuzzy inference engine
and deep neural network. <em>ASOC</em>, <em>99</em>, 106906. (<a
href="https://doi.org/10.1016/j.asoc.2020.106906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19, as an infectious disease, has shocked the world and still threatens the lives of billions of people. Recently, the detection of coronavirus (COVID-19) is a critical task for the medical practitioner. Unfortunately, COVID-19 spreads so quickly between people and approaches millions of people worldwide in few months. It is very much essential to quickly and accurately identify the infected people so that prevention of spread can be taken. Although several medical tests have been used to detect certain injuries, the hopefully detection efficiency has not been accomplished yet. In this paper, a new Hybrid Diagnose Strategy (HDS) has been introduced. HDS relies on a novel technique for ranking selected features by projecting them into a proposed Patient Space (PS). A Feature Connectivity Graph (FCG) is constructed which indicates both the weight of each feature as well as the binding degree to other features. The rank of a feature is determined based on two factors; the first is the feature weight, while the second is its binding degree to its neighbors in PS. Then, the ranked features are used to derive the classification model that can classify new persons to decide whether they are infected or not. The classification model is a hybrid model that consists of two classifiers; fuzzy inference engine and Deep Neural Network (DNN). The proposed HDS has been compared against recent techniques. Experimental results have shown that the proposed HDS outperforms the other competitors in terms of the average value of accuracy, precision, recall, and F-measure in which it provides about of 97.658\%, 96.756\%, 96.55\%, and 96.615\% respectively. Additionally, HDS provides the lowest error value of 2.342\%. Further, the results were validated statistically using Wilcoxon Signed Rank Test and Friedman Test.},
  archive      = {J_ASOC},
  author       = {Warda M. Shaban and Asmaa H. Rabie and Ahmed I. Saleh and M.A. Abo-Elsoud},
  doi          = {10.1016/j.asoc.2020.106906},
  journal      = {Applied Soft Computing},
  pages        = {106906},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detecting COVID-19 patients based on fuzzy inference engine and deep neural network},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incomplete data ensemble classification using
imputation-revision framework with local spatial neighborhood
information. <em>ASOC</em>, <em>99</em>, 106905. (<a
href="https://doi.org/10.1016/j.asoc.2020.106905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing machine learning techniques require complete data. However, incomplete patterns are common in many real-world scenarios due to the missing values (MVs). Various Missing value imputation (MVI) methods have been proposed to recover the MVs. Each of them has its own advantages in some scenarios. However, on the one hand, few of them consider taking advantages of different MVI methods; on the other hand, how to improve the imputation performance with local information is still an open problem. This paper proposes an imputation-revision framework with local spatial neighborhood information for incomplete data classification . The proposed method endeavors to combine the advantages of several imputation methods. It first obtains several complete datasets which are pre-filled by various MVI methods. Then, it detects the local neighborhood information (LNI) of samples and revises MVs based on the LNI. Finally, ensemble technique is employed to give a final decision. Numerical experiments have verified the superiority of the proposed method in terms of both prediction accuracy and algorithm stability.},
  archive      = {J_ASOC},
  author       = {Yuanting Yan and Yaya Wu and Xiuquan Du and Yanping Zhang},
  doi          = {10.1016/j.asoc.2020.106905},
  journal      = {Applied Soft Computing},
  pages        = {106905},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incomplete data ensemble classification using imputation-revision framework with local spatial neighborhood information},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of air-overpressure induced by blasting using an
ANFIS-PNN model optimized by GA. <em>ASOC</em>, <em>99</em>, 106904. (<a
href="https://doi.org/10.1016/j.asoc.2020.106904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blasting operations typically have several negative impacts upon human beings and constructions in adjacent region. Among all, air-overpressure (AOp) has been persistently attractive to practitioners and researchers. To control the AOp-induced damage, its strength should be predicted before conducting a blasting operation. This paper analyzes the AOp consequences through the use of the Fuzzy Delphi method (FDM). The method was adopted to identify the key variables with the deepest influence on AOp based on the experts’ opinions. Then, the most effective parameters on AOp were selected to be used in developing a new hybrid intelligent technique, i.e., adaptive neuro-fuzzy inference system (ANFIS)-polynomial neural network (PNN) optimized by the genetic algorithm (GA), called ANFIS-PNN-GA. From FDM and experts’ opinions, four parameters, i.e., amount of explosive charge , powder factor, stemming, and distance from the blast-face were identified as the most effective ones on AOp. In fact, in ANFIS-PNN-GA system, GA was used to optimize the ANFIS-PNN structure. The new framework of ANFIS-PNN-GA was developed, trained, and tested on actual datasets collected from a total of 62 blasting events. To show capability of the newly-proposed model, the ANFIS and PNN predictive models were also constructed to estimate AOp, and the performance prediction of the proposed models were evaluated through the use of several performance indices, e.g., correlation coefficient ( R ) and mean square error (MSE). R values of (0.94, 0.72, and 0.84) and (0.92, 0.58, and 0.77) and MSE values of (0.003, 0.03, and 0.021) and (0.005, 0.066, and 0.05) were obtained for training and testing datasets of ANFIS-PNN-GA, PNN, and ANFIS models, respectively. Accordingly, because of the role of GA as a practical optimization algorithm in improving the efficiency of both PNN and ANFIS models, results obtained by the ANFIS-PNN-GA model are more accurate compared to other implemented methods.},
  archive      = {J_ASOC},
  author       = {Hooman Harandizadeh and Danial Jahed Armaghani},
  doi          = {10.1016/j.asoc.2020.106904},
  journal      = {Applied Soft Computing},
  pages        = {106904},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of air-overpressure induced by blasting using an ANFIS-PNN model optimized by GA},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A combinatorial social group whale optimization algorithm
for numerical and engineering optimization problems. <em>ASOC</em>,
<em>99</em>, 106903. (<a
href="https://doi.org/10.1016/j.asoc.2020.106903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents two new hybrid swarm-human based meta-heuristic optimization algorithms benefitting from the synergy of whale optimization algorithm (WOA) and social group optimization (SGO) known as Hybrid Social Whale Optimization Algorithm (HS-WOA and HS-WOA+). HS-WOA and HS-WOA+ are hybridized combining the exploratory capabilities of WOA and convergence capabilities of SGO with a perfect balance between exploration and exploitation. A comparative analysis of the new proposed hybrid algorithm is performed through various benchmark functions . Various test cases to analyze the algorithm’s performance like influence of population size, effect of dimensionality, effect of iterative count is performed and compared. The proposed algorithms are compared with modern-meta-heuristics and variants of WOA and SGO to justify its performance. The performance is evaluated statistically through the Wilcoxon’s rank-sum test and Friedman’s non-parametric test while the convergence curves and acceleration rates are provided to demonstrate the convergence capabilities of the proposed hybrid algorithms and the computational times are recorded to showcase the computational speeds of all the algorithms used in comparison. Composite benchmarking functions are considered to analyze the exploratory prowess and the algorithms’ capability to avoid local entrapment. To assess and evaluate the performance of the proposed algorithms with real world optimization tasks , four standard engineering problems with penalty constraints are added to the test bench. Further, a multi-unit production planning problem with correction constraints is deployed through the proposed algorithms. The benchmarking results prove that HS-WOA and HS-WOA+ s’ performance is competitive and better than the various algorithms tested against and had a statistically significant performance with lower computational times. The algorithms performed well for both standard engineering problems and the multi-unit production planning problem outperforming the various algorithms in the literature.},
  archive      = {J_ASOC},
  author       = {Vamsi Krishna Reddy Aala Kalananda and Venkata Lakshmi Narayana Komanapalli},
  doi          = {10.1016/j.asoc.2020.106903},
  journal      = {Applied Soft Computing},
  pages        = {106903},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combinatorial social group whale optimization algorithm for numerical and engineering optimization problems},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic keystroke pattern analysis and classifiers with
competence for user recognition. <em>ASOC</em>, <em>99</em>, 106902. (<a
href="https://doi.org/10.1016/j.asoc.2020.106902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of biometric data is always a difficult task because these data are often unstable and depends on behavioural condition of a person. Personal keystroke activity is difficult to imitate and can be used for identity authentication . In this article, we suggest that the analysis of keystroke dynamics can dramatically increase the level of security of a computer system without disturbing the user’s comfort. First, the user’s established behavioural profile is loaded and then this profile is continuously compared with the current use of the keyboard. If a user’s temporary behavioural profile differs from their established profile, then the access to the computer is blocked. If a current profile of the user is recognized as legitimate, then profile can be dynamically changed, which allows to adjust profiles according to users abilities. Security based on keystroke dynamics analysis depends primarily on the quality of recognizing changes in a user’s profile. Recognition is performed on the dynamic analysis of the feature vector recorded while using the keyboard. In the presented approach, we propose using classifiers with their competence selection, which significantly improves the recognition of users. To select the most appropriate classifiers, the competences of the classifiers in a pool are calculated and then, the most powerful classifiers are selected. Conducted experiments, supported by the proper statistical analysis, confirm the usefulness of the proposed strategy in authorized user and intruder recognition.},
  archive      = {J_ASOC},
  author       = {Piotr Porwik and Rafal Doroz and Tomasz Emanuel Wesolowski},
  doi          = {10.1016/j.asoc.2020.106902},
  journal      = {Applied Soft Computing},
  pages        = {106902},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic keystroke pattern analysis and classifiers with competence for user recognition},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combined HCS–RBFNN for energy management of multiple
interconnected microgrids via bidirectional DC–DC converters.
<em>ASOC</em>, <em>99</em>, 106901. (<a
href="https://doi.org/10.1016/j.asoc.2020.106901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this manuscript, a hybrid control method is implemented to model and design bidirectional DC–DC converters for the energy management of interconnected renewable energy sources (RES). The proposed hybrid control method is the joint execution of Hybrid Crow Search Algorithm (HCSA) and Radial Basis Function Neural Network (RBFNN). The modeling and design of the Bidirectional DC–DC converter modeling and design topology is developed with the improved efficiency of the converter, efficient use of renewable energy sources , and the reduction of switching loss. In the proposed manner, the HCSA runs the evaluation procedure for establishing correct control signals for the system and creates the control signal database for offline mode in light of power range among source side (RES, battery and supercapacitor (SCAP)) and load side. Here, the crow’s seeking behavior is modified by crossover and mutation. The dataset obtained is utilized to operate the AI approach for the online method and leads the control process on less execution time. In the proposed method, the intention function is defined using the data of the system subject to equality and inequality constraints . The constraint is the availability of RES, power requirement and charge level of storage elements. Batteries and SCAP are utilized as an energy source to allow renewable energy system units to operate continuously in stable and constant power output . At that point, the proposed model is implemented on MATLAB/Simulink work platform and the implementation is assessed to the existing techniques like base method, ALORNN and CSA. Furthermore, switching losses, conduction losses and converter efficiency are also analyzed. Switching losses, conduction losses and converter efficiency of the proposed technique are 0.19W, 0.43W and 98\%.},
  archive      = {J_ASOC},
  author       = {R. Rajasekaran and P. Usha Rani},
  doi          = {10.1016/j.asoc.2020.106901},
  journal      = {Applied Soft Computing},
  pages        = {106901},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combined HCS–RBFNN for energy management of multiple interconnected microgrids via bidirectional DC–DC converters},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Electrical load forecasting: A deep learning approach based
on k-nearest neighbors. <em>ASOC</em>, <em>99</em>, 106900. (<a
href="https://doi.org/10.1016/j.asoc.2020.106900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches have shown superior advantages than shallow techniques in the field of electrical load forecasting; however, their applications in existing studies encounter thorny issues despite their excellent forecasting performance: heavy computing costs due to complicated network structure and restricted to the deterministic point forecasting. This paper aims to solve above two problems by proposing a deep learning approach based on K -nearest neighbors to capture uncertainty and reflect the range of electrical load fluctuation. First, the K -nearest neighbors algorithm is applied to seek features of historical electrical load time series that are similar to the future values by calculating the distance between the training and testing datasets . Then the second generation of non-dominated sorting genetic algorithm is adopted for multi-objective optimization to find out the smallest category number of K -nearest neighbors and the highest forecasting accuracy . Based on the forecasting results of the deep belief network , modified non-parameter kernel density estimation is used to obtain the prediction intervals. Five datasets collected from Australia are employed to examine the effectiveness of the proposed model. By a series of comparisons with other state-of-the-art models, experimental results confirm that the proposed interval forecasting model cannot only improve the forecasting efficiency and accuracy, but also simplify the forecasting process of deep learning approaches, which can provide great referential value for future work.},
  archive      = {J_ASOC},
  author       = {Yunxuan Dong and Xuejiao Ma and Tonglin Fu},
  doi          = {10.1016/j.asoc.2020.106900},
  journal      = {Applied Soft Computing},
  pages        = {106900},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Electrical load forecasting: A deep learning approach based on K-nearest neighbors},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and coordinated optimization method featuring
coupling relationship among subsystems for improving safety and
efficiency of drilling process. <em>ASOC</em>, <em>99</em>, 106899. (<a
href="https://doi.org/10.1016/j.asoc.2020.106899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drilling is an important means of obtaining resources. Since the complicated bottom hole conditions and coupling relationship among subsystems, it is difficult to accurately predict the drilling states and simultaneously improve drilling efficiency and safety. A novel modeling and coordinated optimization method has been developed to solve that. In the modeling part, based on the analysis of the correlation between subsystems, two online support vector regression (OSVR) models are established to predict drilling states of rate of penetration (ROP) and mud pit volume (MPV) respectively. In the optimization part, based on the prediction models, the problem of how to improve drilling efficiency and safety is described into a two-objective optimization issue, which includes improving ROP and maintaining MPV at same time. Then a coordinated optimization strategy is developed to finish it, which adopts the nondominated sorting genetic algorithm II (NSGA-II) to find out pareto set , and the technique for order preference by similarity to an ideal solution (TOPSIS) method to determine the final optimal operating parameters. Finally, the verification of modeling and coordinated optimization method based on the actual drilling data show that it can improve ROP by an average of 39.8\% and reduce the fluctuation of the MPV by an average of 69.3\%, which demonstrates effectiveness and practicability for our method.},
  archive      = {J_ASOC},
  author       = {Yang Zhou and Xin Chen and Min Wu and Weihua Cao},
  doi          = {10.1016/j.asoc.2020.106899},
  journal      = {Applied Soft Computing},
  pages        = {106899},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling and coordinated optimization method featuring coupling relationship among subsystems for improving safety and efficiency of drilling process},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Financial market prediction under deep learning framework
using auto encoder and kernel extreme learning machine. <em>ASOC</em>,
<em>99</em>, 106898. (<a
href="https://doi.org/10.1016/j.asoc.2020.106898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technical indicators are highly uncertain therefore possess greater influence on the stock market prediction. Among different techniques developed for effective prediction of the financial market the AI techniques show better prediction efficiency. In this paper, a hybrid model combined with auto encoder (AE) and kernel extreme learning machine (KELM) is proposed for further improvement in the quality of financial market prediction. This study mainly emphasizes on a precise prediction of the financial market, the main motive behind stock price prediction is minimizing the substantial losses faced by investors, and analysing the profitability with the help of buying and selling amount. The prime advantage of the proposed technique over the conventional SAE is robust prediction of different financial market with reduction in error. To authenticate the performance of the proposed deep learning (DL) technique (KELM-AE), high-frequency data of different financial market like Yes Bank, SBI, ASHR, and DJI are taken into consideration and the performance of the proposed technique is investigated in MATLAB based simulation in accordance with MAPE (Mean Absolute Percentage Error), MAE (Mean Absolute Error) and RMSE (Root Mean Square Error). The application of SAE is new in the field of predicting different bank data. The validation of the model is performed by comparing it with other traditional methods based on different performance indexes. The simulation result indicates that the proposed DL based technique (KELM-AE) outperforms other models with a MAPE value of less than 2\%for future prediction, irrespective of the financial market. For example the MAPE value for KELM-AE is observed to be 1.074\%, 0.888\%, 1.021\% for YES, SBI and BOI respectively which is much lower as compared to other model like ELM that shows a MAPE value of 1.714\%, 1.473\% and 1.550\% for the above mentioned bank.},
  archive      = {J_ASOC},
  author       = {D.K. Mohanty and Ajaya Kumar Parida and Shelly Suman Khuntia},
  doi          = {10.1016/j.asoc.2020.106898},
  journal      = {Applied Soft Computing},
  pages        = {106898},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Financial market prediction under deep learning framework using auto encoder and kernel extreme learning machine},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive fault detector strategy for scientific workflow
scheduling based on improved differential evolution algorithm in cloud.
<em>ASOC</em>, <em>99</em>, 106895. (<a
href="https://doi.org/10.1016/j.asoc.2020.106895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity and acceptance of cloud computing , it is being applied in services like executing large-scale applications, where cloud environment is selected by the scientific associations to easily execute the computation intensive workflows. However, cloud computing can have higher failure rates due to the larger number of servers and components filled with the intensive workloads. These failures may lead to the unavailability of virtual machines (VMs) for computation. Hence, this issue of fault occurrences can be tolerated by adopting an effective and efficient fault tolerant strategy. The goal of our research in this paper is to develop an adaptive fault detector strategy based on Improved Differential Evolution (IDE) algorithm in cloud computing that can minimize the energy consumption, the makespan, the total cost and, at the same time, tolerate up faults when scheduling scientific workflows. This proposed work applies an adaptive network-based fuzzy inference system (ANFIS) prediction model to proactively control resource load fluctuation that increases the failure prediction accuracy before fault/failure occurrence. In addition, it applies a reactive fault tolerance technique for when a processor fails and the scheduler must allocate a new VM to execute the workflow tasks. The experimental results show that compared with existing techniques, the proposed approach significantly improves the overall scheduling performance, achieves a higher degree of fault tolerance with high HyperVolume (HV) compared with the ICFWS, IDE, and ACO algorithms, minimizes the makespan, the energy consumption and task fault ratio, and reduces the total cost.},
  archive      = {J_ASOC},
  author       = {Mani Alaei and Reihaneh Khorsand and Mohammadreza Ramezanpour},
  doi          = {10.1016/j.asoc.2020.106895},
  journal      = {Applied Soft Computing},
  pages        = {106895},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive fault detector strategy for scientific workflow scheduling based on improved differential evolution algorithm in cloud},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QPSO algorithm based on lévy flight and its application in
fuzzy portfolio. <em>ASOC</em>, <em>99</em>, 106894. (<a
href="https://doi.org/10.1016/j.asoc.2020.106894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve constrained portfolio selection model effectively, an improved quantum-behaved particle swarm optimization algorithm(LQPSO) is presented. Firstly, considering its practicality in real dealing process, a class of fuzzy portfolio models with transaction costs and background risk is established. Then in the design of improved algorithm, Lévy flight strategy and contraction–expansion coefficient with nonlinear structure are taken into account for enhancing particle’s exploration ability, and premature prevention mechanism is used to increase population diversity. According to the following performance test, LQPSO demonstrates better convergence and robustness than PSO with inertia weight, QPSO and QPSO with a hybrid probability distribution in 12 benchmark functions . Furthermore, experimental results indicate that LQPSO outperforms several metaheuristics when seeking optimal solution for the fuzzy portfolio model with constraints.},
  archive      = {J_ASOC},
  author       = {Xiao-li Lu and Guang He},
  doi          = {10.1016/j.asoc.2020.106894},
  journal      = {Applied Soft Computing},
  pages        = {106894},
  shortjournal = {Appl. Soft. Comput.},
  title        = {QPSO algorithm based on lévy flight and its application in fuzzy portfolio},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The new extension of the MULTIMOORA method for sustainable
supplier selection with intuitionistic linguistic rough numbers.
<em>ASOC</em>, <em>99</em>, 106893. (<a
href="https://doi.org/10.1016/j.asoc.2020.106893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing awareness of environmental and social issues, sustainable supplier selection (SSS) becomes an important problem. In order to scientifically evaluation the SSS, the aim of this paper is to develop a novel SSS method considering the robustness and relationship among experts. Firstly, a novel concept of intuitionistic linguistic rough numbers (ILRNs) is proposed to accurately express the opinions of expert groups for SSS and to consider the interactive relationship among experts. Then we present a process to construct ILRNs and introduce the arithmetic operations , distance measure, correlation measure, ranking rules, aggregation operators, and some corresponding properties. In addition, based on the correlation coefficients between attributes, we introduce a weight determining method. Moreover, considering the robustness of the ranking method, the multiplicative Multi-objective Optimization by Ratio Analysis (MULTIMOORA) is enhanced by developing a new aggregated model and the improved Borda rule, which can consider both the subordinate utility values and ranks, and an intuitionistic linguistic MULTIMOORA method and an intuitionistic linguistic rough MULTIMOORA method are developed respectively. Finally, a real case of a shared power bank SSS is conducted to illustrate the application of the proposed method, and a comparison is performed to explain the superiority and feasibility of the proposed method.},
  archive      = {J_ASOC},
  author       = {Peide Liu and Hui Gao and Hamido Fujita},
  doi          = {10.1016/j.asoc.2020.106893},
  journal      = {Applied Soft Computing},
  pages        = {106893},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The new extension of the MULTIMOORA method for sustainable supplier selection with intuitionistic linguistic rough numbers},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving an EOQ model under fuzzy reasoning. <em>ASOC</em>,
<em>99</em>, 106892. (<a
href="https://doi.org/10.1016/j.asoc.2020.106892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general fuzzy system, the nature of a fuzzy number is usually viewed as specific and deterministic but, in fuzzy reasoning the membership function is developed in a randomized​ sense. Here, we have studied the concept of fuzzy approximate reasoning over the modelling of a cost minimization classical economic order quantity (EOQ) inventory management problem. We have developed the model where all parameters assume randomized fuzzy set by means of fuzzy approximate reasoning. First of all, considering the probability density function of the fuzzy variable, utilizing possibility measures of fuzzy sets within we have formulated the expectations of the fuzzy membership functions then we split the model into seven several sub models accordingly. To defuzzify the fuzzy functions the traditional α α -cuts and its dual k-cuts have been utilized over several feasible spaces of dual spaces of fuzzy variables. Numerical illustrations under different α α -dual k cuts of the objective functions are done with the help of LINGO software via solution algorithm . Moreover, a comparative study has been done with the existing general fuzzy solution. Managerial insights are also highlighted by showing the superiority of the proposed approach. Finally, sensitivity analysis and graphical illustrations are made to justify the model.},
  archive      = {J_ASOC},
  author       = {Sujit Kumar De},
  doi          = {10.1016/j.asoc.2020.106892},
  journal      = {Applied Soft Computing},
  pages        = {106892},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving an EOQ model under fuzzy reasoning},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A MFO-based conformable fractional nonhomogeneous grey
bernoulli model for natural gas production and consumption forecasting.
<em>ASOC</em>, <em>99</em>, 106891. (<a
href="https://doi.org/10.1016/j.asoc.2020.106891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for natural gas is expected to continuously increase due to its significant role in the transition towards a low-carbon energy structure. Based on the nonhomogeneous grey model , a new method for estimating natural gas production and consumption is developed, namely, the conformable fractional nonhomogeneous grey Bernoulli model (denoted as CFNHGBM(1, 1, k) for short). In the new method, the Bernoulli equation is first introduced into the existing differential equation. The traditional accumulation is then replaced with conformable fractional accumulation. Finally, the moth flame optimization (MFO) algorithm is applied to determine the structural parameters for the novel model. Moreover, when taking different values, the novel model will be changed into the existing grey serial models. Based on natural gas production and consumption from 2008 to 2018, we use the proposed model to predict future data from 2019 to 2021 in North America, and the forecasts show that the novel model performs better than other competitors. Furthermore, natural gas production and consumption maintain steady increasing trends with average annual growth rates of 3.29\% and 2.02\%, respectively.},
  archive      = {J_ASOC},
  author       = {Chengli Zheng and Wen-Ze Wu and Wanli Xie and Qi Li},
  doi          = {10.1016/j.asoc.2020.106891},
  journal      = {Applied Soft Computing},
  pages        = {106891},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A MFO-based conformable fractional nonhomogeneous grey bernoulli model for natural gas production and consumption forecasting},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative feature-based adaptive distribution alignment
(DFADA) for rotating machine fault diagnosis under variable working
conditions. <em>ASOC</em>, <em>99</em>, 106886. (<a
href="https://doi.org/10.1016/j.asoc.2020.106886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, cross-domain fault diagnosis of rotating machinery has been a hot topic, and various kinds of methods taking advantage of transfer learning are proposed correspondingly. Despite their success, they mainly focus on marginal distribution alignments, which ignore weighing between marginal and conditional distributions in network training. However, this kind of weighting can boost diagnosis network performance further and make it more robust. Hence, a novel transfer learning method called discriminative feature-based adaptive distribution alignment (DFADA) is proposed, which can extract discriminative features and conduct a two-stage adaptive distribution alignment on L2 ball. In DFADA, maximum mean discrepancy (MMD) and graph Laplacian regularization are fused to extract discriminative and task-specific features. Meanwhile, for comprehensive and adaptive distribution alignments, the distributions of datasets are pre-matched via MMD and further matched in feature classifier via dynamic distribution alignment (DDA), which can not only reduce both marginal and conditional distribution discrepancies but also weigh their importance adaptively. Finally, a DFADA-based fault diagnosis method for rotating machinery with volatile working conditions is constructed correspondingly. The validity of the proposed method is also confirmed by extensive experiments and comparisons with some state of the arts on 18 transfer learning cases.},
  archive      = {J_ASOC},
  author       = {Weiwei Qian and Shunming Li and Tong Yao and Kun Xu},
  doi          = {10.1016/j.asoc.2020.106886},
  journal      = {Applied Soft Computing},
  pages        = {106886},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discriminative feature-based adaptive distribution alignment (DFADA) for rotating machine fault diagnosis under variable working conditions},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble of deep sequential models for credit card fraud
detection. <em>ASOC</em>, <em>99</em>, 106883. (<a
href="https://doi.org/10.1016/j.asoc.2020.106883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent years, the fast development of e-commerce technologies made it possible for people to select the most desirable items in terms of suggested price, quality and quantity among various services, facilities, shops and stores from all around the world. However, it also made it easier for fraudsters to abuse this huge opportunity. As credit card has become the most popular mode of payment, the fraudulent activities using credit card payment technologies are rapidly increasing as a result. Therefore, it is obligatory for financial institution to think of an automatic deterrent mechanism to prevent these fraudulent actions. Although many works have been done in this area using traditional statistical and machine learning methods, most of them do not take the sequential nature of transactional data into account. In this paper, we proposed an ensemble model based on sequential modeling of data using deep recurrent neural networks and a novel voting mechanism based on artificial neural network to detect fraudulent actions. In addition, we present a novel algorithm for training the aforementioned voting approach. Our experimental results on two real world datasets demonstrate that the proposed model outperforms the state-of-the-art models in all evaluation criteria. Moreover, the time analysis of the proposed model shows that it is more efficient in terms of real-time performance versus the recent models in the field.},
  archive      = {J_ASOC},
  author       = {Javad Forough and Saeedeh Momtazi},
  doi          = {10.1016/j.asoc.2020.106883},
  journal      = {Applied Soft Computing},
  pages        = {106883},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble of deep sequential models for credit card fraud detection},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cascade knowledge diffusion network for skin lesion
diagnosis and segmentation. <em>ASOC</em>, <em>99</em>, 106881. (<a
href="https://doi.org/10.1016/j.asoc.2020.106881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate diagnosis and segmentation of skin lesion is critical for early detection and diagnosis of skin cancer. Recent multi-task learning methods require expensive annotations for skin lesion analysis while single-task driven models cannot fully utilize the potential knowledge. The aim of this study is to utilize the neglected knowledge by a flexible architecture in dermoscopy skin lesion classification and segmentation. In this work, we propose a cascade knowledge diffusion network (CKDNet) to transfer and aggregate knowledge learnt from different tasks to simultaneously boost the performances of classification and segmentation. CKDNet consists of a sequence of coarse-level segmentation, classification, and fine-level segmentation networks . We design two novel feature entanglement modules, Entangle-Cls and Entangle-Seg, for classification and segmentation. The Entangle-Cls module aggregates the diffused features from initial segmentation to drive the classification network’s attention to image regions relevant to the disease. The Entangle-Seg module integrates the cascaded context knowledge learnt from classification to benefit fine-level segmentation, especially at uncertain boundaries. The entanglement modules can adaptively control the knowledge that can be diffused from one task to another, which avoids the empirical selection of weights for different learning tasks compared to other multi-task methods. We perform extensive evaluations and comparisons with state-of-the-art methods on skin lesion classification and segmentation with challenge datasets, ISIC2017 and ISIC2018. Our CKDNet demonstrated superior performance without using any ensemble approaches or any external datasets. The effectiveness of each component and loss functions are demonstrated by interpretable results using class activation maps (CAM), t-SNE, and classification and segmentation results.},
  archive      = {J_ASOC},
  author       = {Qiangguo Jin and Hui Cui and Changming Sun and Zhaopeng Meng and Ran Su},
  doi          = {10.1016/j.asoc.2020.106881},
  journal      = {Applied Soft Computing},
  pages        = {106881},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cascade knowledge diffusion network for skin lesion diagnosis and segmentation},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance analysis and prediction of asymmetric two-level
priority polling system based on BP neural network. <em>ASOC</em>,
<em>99</em>, 106880. (<a
href="https://doi.org/10.1016/j.asoc.2020.106880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concerning the needs of multi-service and network performance prediction in the Internet of Things (IoT), we propose an asymmetric two-priority polling control system model, and use the neural network algorithm to predict and analyze its performance. Firstly, the mathematical model of the system in the continuous time state is established by using the embedded Markov chain theory and the probability generating function. Meanwhile, the characteristics like the average queue length and average cycle of the system are accurately analyzed, and verified in simulation experiments. Subsequently, a three-layer multi-input single-output backpropagation (BP) network model is constructed to predict the performance of the polling system. The results show that the model can not only distinguish multi-business tasks, but also ensure the system delay. BP neural network prediction algorithm can accurately predict the performance of the system, which has a guiding significance for its performance evaluation, and provides a new method for the research of the polling system.},
  archive      = {J_ASOC},
  author       = {Zhijun Yang and Lei Mao and Bin Yan and Jun Wang and Wei Gao},
  doi          = {10.1016/j.asoc.2020.106880},
  journal      = {Applied Soft Computing},
  pages        = {106880},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Performance analysis and prediction of asymmetric two-level priority polling system based on BP neural network},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A consensus model to manage the non-cooperative behaviors of
individuals in uncertain group decision making problems during the
COVID-19 outbreak. <em>ASOC</em>, <em>99</em>, 106879. (<a
href="https://doi.org/10.1016/j.asoc.2020.106879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has brought lots of losses to the global economy. Within the context of COVID-19 outbreak, many emergency decision-making problems with uncertain information arose and a number of individuals were involved to solve such complicated problems. For instance, the selection of the first entry point to China is important for oversea flights during the epidemic outbreak given that reducing imported virus from abroad becomes the top priority of China since China has achieved remarkable achievements regarding the epidemic control. In such a large-scale group decision making problem, the non-cooperative behaviors of experts are common due to the different backgrounds of the experts. The non-cooperative behaviors of experts have a negative impact on the efficiency of a decision-making process in terms of decision time and cost. Given that the non-cooperative behaviors of experts were rarely considered in existing large-scale group decision making methods, this study aims to propose a novel consensus model to manage the non-cooperative behaviors of experts in large-scale group decision making problems. A group consistency index simultaneously considering fuzzy preference values and cooperation degrees is introduced to detect the non-cooperative behaviors of experts. We combine the cooperation degrees and fuzzy preference similarities of experts when clustering experts. To reduce the negative influence of the experts with low degrees of cooperation on the quality of a decision-making process, we implement a dynamic weight punishment mechanism to non-cooperative experts so as to improve the consensus level of a group. An illustrative example about the selection of the first point of entry for the flights entering Beijing from Toronto during the COVID-19 outbreak is presented to show the validity of the proposed model.},
  archive      = {J_ASOC},
  author       = {Xiaofang Li and Huchang Liao and Zhi Wen},
  doi          = {10.1016/j.asoc.2020.106879},
  journal      = {Applied Soft Computing},
  pages        = {106879},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A consensus model to manage the non-cooperative behaviors of individuals in uncertain group decision making problems during the COVID-19 outbreak},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal uncertainty-guided neural network training.
<em>ASOC</em>, <em>99</em>, 106878. (<a
href="https://doi.org/10.1016/j.asoc.2020.106878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neural network (NN)-based direct uncertainty quantification (UQ) methods have achieved the state of the art performance since the first inauguration, known as the lower–upper-bound estimation (LUBE) method. However, currently-available cost functions for uncertainty guided NN training are not always converging, and all converged NNs do not generate optimized prediction intervals (PIs). In recent years researchers have proposed different quality criteria for PIs that raise a question about their relative effectiveness. Most of the existing cost functions of uncertainty guided NN training are not customizable, and the convergence of the NN training is uncertain. Therefore, in this paper, we propose a highly customizable smooth cost function for developing NNs to construct optimal PIs. The method computes the optimized average width of PIs, PI-failure distances, and the PI coverage probability (PICP) for the test dataset . We examine the performance of the proposed method for wind power generation , electricity demand, and temperature forecast datasets. Results show that the proposed method reduces variation in the quality of PIs, accelerates the training, and improves convergence probability from 99.2\% to 99.8\%.},
  archive      = {J_ASOC},
  author       = {H. M. Dipu Kabir and Abbas Khosravi and Abdollah Kavousi-Fard and Saeid Nahavandi and Dipti Srinivasan},
  doi          = {10.1016/j.asoc.2020.106878},
  journal      = {Applied Soft Computing},
  pages        = {106878},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal uncertainty-guided neural network training},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational model for identifying stereotyped behaviors
and determining the activation level of pseudo-autistic. <em>ASOC</em>,
<em>99</em>, 106877. (<a
href="https://doi.org/10.1016/j.asoc.2020.106877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective state recognition of an individual is based on the emotional cues, such as the activation level . Body expression is a modal able to convey emotions and can be used for autism diagnosis through the presence of stereotyped behaviors (SBs). These behaviors are atypical and repetitive movements of the body, which can be related to a low mental health condition. The development of systems able to both recognize SBs and inferring activation level can automatically aid some therapeutic approaches. In this paper, a computational model of low intrusiveness is proposed to infer activation levels from recognized SBs, Machine Learning Algorithms (MLAs) are for identifying the SBs and for determining the related activation levels. A metric performance is also proposed to evaluate the performance of MLAs considering the time for classification of the SBs, accuracy, and precision. For classifying the SBs, the Hidden Markov Models and Multilayer Perceptron presented the best performance than Support Vector Machine and Convolutional Neural Network . The Adaptive Neuro-Fuzzy technique based on the Fuzzy C-Means algorithm allowed one to determine and differentiate the activation levels of the stereotyped behaviors considered in the present study. The experiments were performed with non-autistic participants, here referred to as pseudo-autistic.},
  archive      = {J_ASOC},
  author       = {Marcos Y.O. Camada and Jés J.F. Cerqueira and Antonio M.N. Lima},
  doi          = {10.1016/j.asoc.2020.106877},
  journal      = {Applied Soft Computing},
  pages        = {106877},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Computational model for identifying stereotyped behaviors and determining the activation level of pseudo-autistic},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Resource provisioning in scalable cloud using bio-inspired
artificial neural network model. <em>ASOC</em>, <em>99</em>, 106876. (<a
href="https://doi.org/10.1016/j.asoc.2020.106876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource assignment is one of the emerging research area in the cloud scenario. Cloud computing provides a shared pool of resources in a distributed environment. It supports the features of utility-based computing. Efficient task provisioning on virtual machines is the major concern in an extensible cloud computing environment. The task provisioning minimizes the performance metrics total completion time (ms), average start time, average finish time, average execution time , scheduling time, and simulation time respectively. The scheduling is an important problem which becomes more complicated when various parameters consider. The key issue in virtual machine level scheduling is execution time overhead and scalability in a real-time scenario. Our objective is to make an optimal schedule of tasks on a virtual machine inside the datacenter using neural-bio inspired GA-ANN technique. This work presents a scheduler based on a genetic approach and an artificial neural network. The presented approach performs optimal scheduling of tasks on an appropriate virtual machine. The reliability of the system improves by reducing the number of tasks failed. The presented work uses a genetic algorithm to generated huge data sets and trains the neural model using the data set generated by using a genetic approach. The accuracy of the model is improved using back propagation with 98\% accuracy. The set of experiments are performed using a scalable cloud computing environment. The presented bio-inspired technique is compared against nature-inspired, bio-inspired cost-aware BB–BC, GA-Cost, and GA-Exe based efficient task scheduling techniques. The results are obtained using real workload logs and synthetic data sets. Results indicate that the proposed GA-ANN bio-inspired predictive approach outperforms the considered nature-inspired scheduling approaches. The proposed algorithm is compared using various performance metrics total completion time, average start time, average finish time, and the fault rate, execution time, and scheduling time respectively. The proposed model reduces the fault rate by 82.63\%, successfully completed tasks count improves by 26.81\% and execution time improves by 10.66\% and scheduling time improves by 69.94\%. The scheduling time improves by 85.76\% with an increasing number of iterations and constant numbers of tasks. Hence the presented GA-ANN scheduling technique outperformed the GA cost, GA EXE, and BB–BC COST scheduling approaches.},
  archive      = {J_ASOC},
  author       = {Pradeep Singh Rawat and Priti Dimri and Punit Gupta and G.P. Saroha},
  doi          = {10.1016/j.asoc.2020.106876},
  journal      = {Applied Soft Computing},
  pages        = {106876},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Resource provisioning in scalable cloud using bio-inspired artificial neural network model},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy-tuned model predictive control for dynamic eco-driving
on hilly roads. <em>ASOC</em>, <em>99</em>, 106875. (<a
href="https://doi.org/10.1016/j.asoc.2020.106875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing optimal control systems for vehicles that consider the effect of road slopes use a cost function with fixed weights related to speed deviation, regardless of driving states on slopes. As a result, gravitational potential energy is not efficiently exploited and braking at down-slopes (which wastes energy) becomes unavoidable. Thus, there is still significant scope to improve fuel saving behavior on slopes. To address this opportunity, in this paper, we present a dynamic eco-driving system (EDS) for a (host) vehicle based on model predictive control (MPC) with fuzzy-tuned weights, which helps efficiently utilize the gravitational potential energy . In the proposed EDS, we formulate a nonlinear optimization problem with an appropriate prediction horizon and an objective function based on the factors affecting vehicle fuel consumption. The objective function’s weight is tuned via fuzzy inference techniques using information of the vehicle’s instantaneous velocity and the road slope angle. By considering the vehicle longitudinal dynamics , preceding vehicle’s state, and road slope information (obtained from the digital road map), the optimization generates velocity trajectories for the host vehicle that minimizes fuel consumption and CO 2 2 emission. We also investigate the traffic flow performance of following vehicles (behind the host vehicle) in dense traffic; this was not considered in existing works on hilly roads. The effectiveness of the proposed EDS is evaluated using microscopic traffic simulations on a real road stretch in Fukuoka City, Japan , and the results demonstrate that the fuzzy-tuned MPC EDS significantly reduces fuel consumption and CO 2 2 emission of the host vehicle compared to the traditional driving (human-based) system (TDS) for the same travel time. In dense traffic, the fuel consumption and CO 2 2 emission of following vehicles are noticeably reduced.},
  archive      = {J_ASOC},
  author       = {A.S.M. Bakibillah and M.A.S. Kamal and Chee Pin Tan and Tomohisa Hayakawa and Jun-ichi Imura},
  doi          = {10.1016/j.asoc.2020.106875},
  journal      = {Applied Soft Computing},
  pages        = {106875},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy-tuned model predictive control for dynamic eco-driving on hilly roads},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A convolutional neural network model for abnormality
diagnosis in a nuclear power plant. <em>ASOC</em>, <em>99</em>, 106874.
(<a href="https://doi.org/10.1016/j.asoc.2020.106874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosing abnormal events in nuclear power plants (NPPs) is a challenging issue given the hundreds of possible abnormal events that can occur and the thousands of plant parameters that require monitoring. This study proposes a convolutional neural network model for abnormality diagnosis in an NPP. The distinct feature of the proposed approach is the use of two-channel two-dimensional images to deal with (1) the massive amount of data that individual systems generate in real time, and (2) the dynamics of the states of individual systems. One channel represents the current NPP state values, while the other channel represents the changing patterns of the state values during a prescribed time period in the past. Experimental results from a full-scope simulator confirm, with statistically significant outcomes, that the developed model outperforms other classification models in terms of accuracy and reliability and is robust across different contexts of analysis, and thus has the potential to be adopted by actual NPP systems for real-time diagnosis.},
  archive      = {J_ASOC},
  author       = {Gyumin Lee and Seung Jun Lee and Changyong Lee},
  doi          = {10.1016/j.asoc.2020.106874},
  journal      = {Applied Soft Computing},
  pages        = {106874},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A convolutional neural network model for abnormality diagnosis in a nuclear power plant},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An indicator and adaptive region division based evolutionary
algorithm for many-objective optimization. <em>ASOC</em>, <em>99</em>,
106872. (<a href="https://doi.org/10.1016/j.asoc.2020.106872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problems involving four or more objectives are termed Many-objective problems (MaOPs), which pose serious challenges to existing evolutionary algorithms (EAs). Although EAs via decomposition exhibit encouraging performance in handling MaOPs, they need a set of predefined weight vectors , which is not well adaptable to problems possessing various Pareto front (PF) shapes. Besides, the nature of subproblem formulations renders the overwhelming convergence property . In this paper, we introduce an indicator and adaptive region division based EA, referred to as IREA, which is free from the presetting of weight vectors . To be specific, an angular distance based space division module and a proximity-oriented indicator are incorporated into IREA, where the former highlights diversity adaptively via maximum angular distance while the latter emphasizes convergence in a local manner. Furthermore, the quality of mating pool is leveraged by a coordinate transformation assisted niche technique. The proposed IREA is compared with several prevalent many-objective EAs on scalable MaOPs with varying characteristics, as well as on the many-objective cloud manufacturing service composition problems . The experimental results demonstrate that IREA is highly competitive and can be used as an alternative for handling MaOPs.},
  archive      = {J_ASOC},
  author       = {Jiajun Zhou and Xifan Yao and Liang Gao and Chengyu Hu},
  doi          = {10.1016/j.asoc.2020.106872},
  journal      = {Applied Soft Computing},
  pages        = {106872},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An indicator and adaptive region division based evolutionary algorithm for many-objective optimization},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Short search space and synthesized-reference re-ranking for
face image retrieval. <em>ASOC</em>, <em>99</em>, 106871. (<a
href="https://doi.org/10.1016/j.asoc.2020.106871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face image retrieval underpins numerous applications in many computer vision domains, however facial appearance variations including age, gender and race make this task challenging. Prior art methods rely on geometric properties and relationship between local features . However, their performance is still short of what is needed, mainly because (1) they ignore the demographic information, and (2) lack age-invariant re-ranking while retrieving face images. In this paper, we aim to build a two-stage face retrieval approach. First, we search for candidate face images using demographic-assisted clustering resulting in a short search space. Second, we develop a generative model to compensate aging variations between query and candidate face images resulting into an independent aging synthesized face images reference set. We then use this reference set to re-rank candidate face images resulting into the final retrieval of face images. We show that the proposed face retrieval approach outperforms the state-of-the-art methods in terms of both the precision and scalability on publicly available longitudinal datasets including CACD and MORPH II.},
  archive      = {J_ASOC},
  author       = {Muhammad Sajid and Nouman Ali and Saadat Hanif Dar and Bushra Zafar and Muhammad Kashif Iqbal},
  doi          = {10.1016/j.asoc.2020.106871},
  journal      = {Applied Soft Computing},
  pages        = {106871},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short search space and synthesized-reference re-ranking for face image retrieval},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of binary PSO for public cloud resources
allocation system of video on demand (VoD) services. <em>ASOC</em>,
<em>99</em>, 106870. (<a
href="https://doi.org/10.1016/j.asoc.2020.106870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video streaming, whether on demand or live, has become one of the most popular internet applications. However, financial investments required for it is a severe problem since it needs more real time storage, higher data transfer and a significant amount of computation than other kinds of multimedia data. To tackle this problem, cloud computing , offering services without investing in hardware or software, emerges as a preferred technology. However, there are a large number of cloud service providers and they offer different pricing strategies for various applications in various regions. Therefore, it is of great importance for them that incoming service requests are assigned to the appropriate cloud services at minimum cost and provide maximum user satisfaction [quality of service (QoS) attributes]. Due to the issues, such as multiple cloud providers , different QoS requirements, different service level agreements and uncertainties in demand, price and availability, the optimization of resource allocation present further challenges. The objective of our study is to optimize the cost and performance of video on demand applications using cloud content delivery networks , storage and transcoders based on the QoS requirements of users. To solve the NP-hard problem, Particle Swarm Optimization (PSO) technique is used due to the easiness in its concept and coding, less sensitive to the nature of the objective function, limited number of parameters and generating high quality solution within a short time. We propose a new method in which the optimum solution is affected not only by the best solution of the particle and global best solution but also by the best solution of the neighborhood particles in that iteration. This ternary approach is implemented into the well-known discrete and constrained PSO, achieving the minimum cost with user satisfaction for allocation of video requests to cloud resources. Although the proposed method yields better results in terms of accuracy, execution time of the algorithm is not reasonable. To overcome this inefficiency; ternary approach is embedded into multi-swarm PSO and it is parallelized and combined with greedy heuristic algorithms . The results of the comparison with the benchmarking algorithms show that our proposed method yields better results from the standpoint of both accuracy and execution time.},
  archive      = {J_ASOC},
  author       = {Betul Aygun and Banu Gunel Kilic and Nursal Arici and Ahmet Cosar and Bedriye Tuncsiper},
  doi          = {10.1016/j.asoc.2020.106870},
  journal      = {Applied Soft Computing},
  pages        = {106870},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of binary PSO for public cloud resources allocation system of video on demand (VoD) services},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved non-dominated sorting biogeography-based
optimization algorithm for the (hybrid) multi-objective flexible
job-shop scheduling problem. <em>ASOC</em>, <em>99</em>, 106869. (<a
href="https://doi.org/10.1016/j.asoc.2020.106869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of intelligent manufacturing and industry 4.0 , production scheduling has become a significant problem that most enterprises must deal with. Thereinto, (hybrid) multi-objective flexible job-shop scheduling problem, widely existing in the real-life manufacturing systems , is one of the NP-hard problems in various scheduling problems. Consequently, in this paper, an improved non-dominated sorting biogeography-based optimization (INSBBO) algorithm has been proposed to solve the problem. First of all, to overcome the pressure scarcity of individual selection in the Pareto dominance principle, especially in the late iteration of the algorithm, a novel V-dominance principle based on the volume enclosed by the normalized objective function values has been developed to enhance the convergence speed. Then, a hybrid variable neighborhood search (HVNS) structure is designed as a local search algorithm to amend the local search ability. Thereafter, for avoiding the loss of the partial (sub-)optimal solutions in the iteration, an elite storage strategy (ESS) is constructed to store the (sub-)optimal solutions. Additionally, we modify the internal habitat suitability index (HSI), migration and mutation operators of the NSBBO algorithm to further improve its performance. To evaluate the effectiveness of the above improved operations and the robustness of parameter setting, we compare the performances of each modified operation and critical parameter combination through multiple independent running the typical scheduling instance from the literature. The statistical results exhibit that each amended operation has a significant influence on the performance of INSBBO and its key parameter configuration is robust. Meanwhile, INSBBO has a better or similar performance among other state-of-the-art intelligent algorithms by comparing three classical benchmark scheduling datasets.},
  archive      = {J_ASOC},
  author       = {Youjun An and Xiaohui Chen and Yinghe Li and Yaoyao Han and Ji Zhang and Haohao Shi},
  doi          = {10.1016/j.asoc.2020.106869},
  journal      = {Applied Soft Computing},
  pages        = {106869},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved non-dominated sorting biogeography-based optimization algorithm for the (hybrid) multi-objective flexible job-shop scheduling problem},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-level label recovery-based label embedding for
multi-label classification with missing labels. <em>ASOC</em>,
<em>99</em>, 106868. (<a
href="https://doi.org/10.1016/j.asoc.2020.106868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from single-label learning, multi-label learning has rich semantic information . Label embedding obtains the inherent intelligence of the label space by projecting the label space into a latent one. However, the label space is inevitably incomplete. The missing label data will lead to the label embedding model capturing incomplete inherent information. Therefore, label data recovery becomes particularly important. However, the label correlations recovery mechanism only considers the label correlations but ignores the objective existence of instance-correlation information. The information we obtain from the perspective of label correlations alone will produce inadequate and incorrect phenomena when a large number of labels are lost or wrong. In this paper, a two-level label recovery mechanism is used not only to recover label data from the label perspective but also to take full advantage of the instance correlations in the label space. Although the two-level label recovery mechanism considers both instances and labels, it ignores the label space after recovery to capture its essential information. Therefore, combining the two-level semantic information , we propose a label embedding for multi-label classification based on a two-level label recovery mechanism about missing labels model for incomplete datasets, which is the two-level label recovery. Firstly, the two-level label space is projected to the inherent space, and then the instance and label correlations information are captured in the inherent space. Finally, it is applied to the label prediction of classifiers. The algorithm is trained and tested on several complete and incomplete multi-label datasets. It shows that the proposed algorithm has a good classification effect on the largely missed datasets. The results and statistical hypothesis tests further verify the effectiveness of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Yibin Wang and Weijie Zheng and Yusheng Cheng and Dawei Zhao},
  doi          = {10.1016/j.asoc.2020.106868},
  journal      = {Applied Soft Computing},
  pages        = {106868},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-level label recovery-based label embedding for multi-label classification with missing labels},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). InstaCovNet-19: A deep learning classification model for the
detection of COVID-19 patients using chest x-ray. <em>ASOC</em>,
<em>99</em>, 106859. (<a
href="https://doi.org/10.1016/j.asoc.2020.106859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the whole world became infected by the newly discovered coronavirus (COVID-19). SARS-CoV-2, or widely known as COVID-19, has proved to be a hazardous virus severely affecting the health of people. It causes respiratory illness, especially in people who already suffer from other diseases. Limited availability of test kits as well as symptoms similar to other diseases such as pneumonia has made this disease deadly, claiming the lives of millions of people. Artificial intelligence models are found to be very successful in the diagnosis of various diseases in the biomedical field In this paper, an integrated stacked deep convolution network InstaCovNet-19 is proposed. The proposed model makes use of various pre-trained models such as ResNet101, Xception, InceptionV3, MobileNet, and NASNet to compensate for a relatively small amount of training data. The proposed model detects COVID-19 and pneumonia by identifying the abnormalities caused by such diseases in Chest X-ray images of the person infected. The proposed model achieves an accuracy of 99.08\% on 3 class (COVID-19, Pneumonia, Normal) classification while achieving an accuracy of 99.53\% on 2 class (COVID, NON-COVID) classification. The proposed model achieves an average recall, F1 score, and precision of 99\%, 99\%, and 99\%, respectively on ternary classification, while achieving a 100\% precision and a recall of 99\% on the binary class., while achieving a 100\% precision and a recall of 99\% on the COVID class. InstaCovNet-19’s ability to detect COVID-19 without any human intervention at an economical cost with high accuracy can benefit humankind greatly in this age of Quarantine.},
  archive      = {J_ASOC},
  author       = {Anunay Gupta and Anjum and Shreyansh Gupta and Rahul Katarya},
  doi          = {10.1016/j.asoc.2020.106859},
  journal      = {Applied Soft Computing},
  pages        = {106859},
  shortjournal = {Appl. Soft. Comput.},
  title        = {InstaCovNet-19: A deep learning classification model for the detection of COVID-19 patients using chest X-ray},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measure of influences in social networks. <em>ASOC</em>,
<em>99</em>, 106858. (<a
href="https://doi.org/10.1016/j.asoc.2020.106858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Society may be assumed to be a combination of several small networks. Naturally, an individual from one network is connected to several other networks (associated networks). The concept of associated networks is introduced here in a fuzzy environment. Although individuals of a given (focal) network are more likely to interact with others in the network, individuals from an associated network(s) also play an essential role in influencing decisions in the focal network. In this study, a measure of the influence of an individual (node) on/from individuals in the focal network and that in the context of associated networks have been developed using fuzzy systems. Mathematical formulations for the notion of the influence of a node have been developed based on the structure of a network. Also, fuzzy parameters that capture real-life situation-based characteristics (for example, characteristics of a connected associated network) have been included. Thus, the objective (structure-based) and subjective (using fuzzy membership parameters) nature of the network have been captured. We collected Facebook data to illustrate the proposed approach. We consider new features: (a) a fuzzy definition of centrality measures , (b) power measure, (c) notion of associated network and a measure for linking it to the main network, (d) in addition, we provide a mechanism ( through subjective parameters) to adapt our approach to a given situation, thus making our approach adaptable to a variety of applications. In this study, another application on the spreading of COVID19 affected regions has been discussed.},
  archive      = {J_ASOC},
  author       = {Sovan Samanta and Vivek Kumar Dubey and Biswajit Sarkar},
  doi          = {10.1016/j.asoc.2020.106858},
  journal      = {Applied Soft Computing},
  pages        = {106858},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Measure of influences in social networks},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient three-level parallel ABC algorithm for
secondary structure prediction of complex RNA sequences. <em>ASOC</em>,
<em>99</em>, 106848. (<a
href="https://doi.org/10.1016/j.asoc.2020.106848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces 3L-PABCfold, a parallel artificial bee colony algorithm with three-Level strategy for efficient secondary structure prediction of complex RNA sequences, by multi-processor parallel computing . Designed 3L-PABCfold is a combination of additionally proposed three sub-algorithms: three-Level parallel ABC algorithm (3L-PABC), set based ABC algorithm and discrete ABC (DABC) algorithm. Proposed 3L-PABCfold is an extension of author’s earlier developed TL-PSOfold Lalwani et al. (2016), a two-Level particle swarm optimization algorithm for RNA secondary structure prediction. 3L-PABCfold is furthermore enriched with complexity evaluation of highly obscure sequence families followed by makespan minimization and algorithm implementation in parallel computing environment. As the complex dataset, 1, 00, 000 RNA sequences are randomly generated with length range 139–6358 nucleotides. Level-I of the algorithm reduces the machines idle time by implementing an efficient schedule, whereas, Level-II maximizes the bonded pairs of RNA sequences and Level-III provides optimum secondary structure of RNA sequence. Moreover, three more parallel versions are developed i . e . i.e. parallel version of TL-PSOfold (TL-PPSOfold), three Level parallel version of TL-PSOfold (3L-PPSOfold) and two-Level parallel version of 3L-PABCfold (TL-PABCfold), so as to avail a fair comparison and to identify the competent strategy for the addressed problem. The performance evaluated at the criteria of sensitivity, specificity, F-measure along with the descriptive statistics and statistical significance, yields significantly better prediction accuracy for 3L-PABCfold. Further, the efficiency at the processing time criteria proves 3L-PABCfold as the most competent algorithm than TL-PPSOfold, TL-PABCfold and 3L-PPSOfold.},
  archive      = {J_ASOC},
  author       = {Soniya Lalwani and Rajesh Kumar},
  doi          = {10.1016/j.asoc.2020.106848},
  journal      = {Applied Soft Computing},
  pages        = {106848},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient three-level parallel ABC algorithm for secondary structure prediction of complex RNA sequences},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Jointly optimization for activity recognition in secure
IoT-enabled elderly care applications. <em>ASOC</em>, <em>99</em>,
106788. (<a href="https://doi.org/10.1016/j.asoc.2020.106788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elderly care is a significant livelihood project in the increasingly serious aging society. Nowadays, the wider application of the Internet of Things (IoT) technology on assistant means has making an importance contribution to elderly care in institutions and at home. On the broader data foundation collected by IoT devices, Human Activity Recognition (HAR) with its high demand in various elderly care applications also has grabbed considerable research attentions. However, the elderly’s sensitive data transmitted over the wireless communication channel need to address many security concerns, and the accuracy of activity recognition is susceptible to many influencing factors, especially, the employed feature selection method and classifier. In this paper, to ensure the confidentiality of the elderly’s sensitive data, a secure and efficient group-based key establishment and authentication framework is first proposed. Subsequently, activity recognition is investigated within the security data sensitive framework, where a feature reorganization based feature selection method is proposed with the demonstrated relationship between the recognition accuracy and strongly correlative features, and three classifiers are investigated based on classical ones. By contrast, an improved convolutional neural network (CNN) enabled classifier is singled out to conduct the activity recognition with the feature reorganization method. Finally, security proofs show that the proposed security framework can ensure data confidentiality and be resilient to possible attacks, and experimental analyses show investigations in activity recognition can achieve more cost-effective results.},
  archive      = {J_ASOC},
  author       = {Ming Tao and Xueqiang Li and Wenhong Wei and Huaqiang Yuan},
  doi          = {10.1016/j.asoc.2020.106788},
  journal      = {Applied Soft Computing},
  pages        = {106788},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Jointly optimization for activity recognition in secure IoT-enabled elderly care applications},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparative study of swarm intelligence and evolutionary
algorithms on urban land readjustment problem. <em>ASOC</em>,
<em>99</em>, 106753. (<a
href="https://doi.org/10.1016/j.asoc.2020.106753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land Readjustment and redistribution (LR) is a land management tool that helps regular urban development with the contribution of landowners. The main purpose of LR is to transform irregularly developed land parcels into suitable forms. Since it is necessary to handle many criteria simultaneously to solve LR problems, classical mathematical methods can be insufficient due to time limitation. Since LR problems are similar to traveling salesman problems and typical scheduling problems in terms of structure, they are kinds of NP-hard problems in combinatorial optimization . Therefore, metaheuristic algorithms are used in order to solve NP-hard problems instead of classical methods. At first, in this study, an effective problem-specific objective function is proposed to address the main criteria of the problem. In addition, a map-based crossover operator and three different mutation operators are proposed for the LR, and then a hybrid approach is implemented by utilizing those operators together. Furthermore, since the optimal value of the problem handled in real world cannot be exactly estimated, a synthetic dataset is proposed as a benchmarking set in LR which makes the success of algorithms can be objectively evaluated. This dataset consists of 5 different problems according to number of parcel which are 20, 40, 60, 80 and 100. Each problem set consists of 4 sub-problems in terms of number of landowners per-parcel which are 1, 2, 3 and 4. Therefore, the dataset consists of 20 kinds of problems. In this study, artificial bee colony , particle swarm optimization , differential evolution, genetic and tree seed algorithm are used. In the experimental studies, five algorithms are set to run under equal conditions using the proposed synthetic dataset . When the acquired experimental results are examined, genetic algorithm seems to be the most effective algorithm in terms of both speed and performance. Although artificial bee colony has better results from genetic algorithm in a few problems, artificial bee colony is the second most successful algorithm after genetic algorithm in terms of performance. However, in terms of time, artificial bee colony is an algorithm nearly as successful as genetic algorithm. On the other hand, the results of differential evolution, particle swarm optimization and tree seed algorithms are similar to each other in terms of solution quality. In conclusion, the statistical tests clearly show that genetic algorithm is the most effective technique in solving LR problems in terms of speed, performance and robustness.},
  archive      = {J_ASOC},
  author       = {Ismail Koc and Ismail Babaoglu},
  doi          = {10.1016/j.asoc.2020.106753},
  journal      = {Applied Soft Computing},
  pages        = {106753},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comparative study of swarm intelligence and evolutionary algorithms on urban land readjustment problem},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning distinctive filters for COVID-19 detection from
chest x-ray using shuffled residual CNN. <em>ASOC</em>, <em>99</em>,
106744. (<a href="https://doi.org/10.1016/j.asoc.2020.106744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is a deadly viral infection that has brought a significant threat to human lives. Automatic diagnosis of COVID-19 from medical imaging enables precise medication, helps to control community outbreak, and reinforces coronavirus testing methods in place. While there exist several challenges in manually inferring traces of this viral infection from X-ray, Convolutional Neural Network (CNN) can mine data patterns that capture subtle distinctions between infected and normal X-rays. To enable automated learning of such latent features, a custom CNN architecture has been proposed in this research. It learns unique convolutional filter patterns for each kind of pneumonia . This is achieved by restricting certain filters in a convolutional layer to maximally respond only to a particular class of pneumonia/COVID-19. The CNN architecture integrates different convolution types to aid better context for learning robust features and strengthen gradient flow between layers. The proposed work also visualizes regions of saliency on the X-ray that have had the most influence on CNN’s prediction outcome. To the best of our knowledge, this is the first attempt in deep learning to learn custom filters within a single convolutional layer for identifying specific pneumonia classes. Experimental results demonstrate that the proposed work has significant potential in augmenting current testing methods for COVID-19. It achieves an F1-score of 97.20\% and an accuracy of 99.80\% on the COVID-19 X-ray set.},
  archive      = {J_ASOC},
  author       = {R. Karthik and R. Menaka and Hariharan M.},
  doi          = {10.1016/j.asoc.2020.106744},
  journal      = {Applied Soft Computing},
  pages        = {106744},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning distinctive filters for COVID-19 detection from chest X-ray using shuffled residual CNN},
  volume       = {99},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Retraction notice to “video associated cross-modal
recommendation algorithm based on deep learning” [appl. Soft comput. J.
82 (2019) 105597]. <em>ASOC</em>, <em>98</em>, 106980. (<a
href="https://doi.org/10.1016/j.asoc.2020.106980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Fan Yang and Hao Xie and Huxiong Li},
  doi          = {10.1016/j.asoc.2020.106980},
  journal      = {Applied Soft Computing},
  pages        = {106980},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Retraction notice to “Video associated cross-modal recommendation algorithm based on deep learning” [Appl. soft comput. j. 82 (2019) 105597]},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tree–seed algorithm based on intelligent search mechanisms
for continuous optimization. <em>ASOC</em>, <em>98</em>, 106938. (<a
href="https://doi.org/10.1016/j.asoc.2020.106938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the recently proposed metaheuristic algorithms is tree–seed algorithm, TSA for short. TSA is developed by inspiring the relation between trees and their seeds in order to solve continuous optimization problems , and it has a simple but effective algorithmic structure. The algorithm uses two different solution generating mechanisms in order to improve balance local and global search abilities. However, when the algorithm is analyzed in detail, it is seen that there are some issues in the basic algorithm. These are (i) when trees in the stand approaches to each other, the diversification in the stand is lost, (ii) there is no mechanism to get rid of local minima for a tree, (iii) some of the fitness calculation goes to waste due to seed generation mechanism of basic TSA. In order to address these issues, four different approaches (withering process, sequential seed generation, best-based solution update rule and dimensional selection for the solution update rule) have been proposed for the basic TSA, and all these approaches have been also integrated within algorithmic framework of TSA, named new tree–seed algorithm briefly NTSA, and each of them has been used to solve 28 CEC2013 benchmark functions . In the experimental comparisons, the variants of TSA have been compared with each other, and the better algorithm, NTSA, has been compared with 17 state-of-art algorithms such as artificial bee colony , particle swarm optimization , differential evolution, genetic algorithm , covariance matrix adaptation evolutionary strategy etc. The experimental analysis and comparisons show that the NTSA shows better or similar performance than/with the compared algorithms in terms of solution quality and robustness.},
  archive      = {J_ASOC},
  author       = {Mustafa Servet Kiran and Huseyin Hakli},
  doi          = {10.1016/j.asoc.2020.106938},
  journal      = {Applied Soft Computing},
  pages        = {106938},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A tree–seed algorithm based on intelligent search mechanisms for continuous optimization},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ensemble-based hotel recommender system using sentiment
analysis and aspect categorization of hotel reviews. <em>ASOC</em>,
<em>98</em>, 106935. (<a
href="https://doi.org/10.1016/j.asoc.2020.106935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a suitable hotel based on user’s need and affordability is a complex decision-making process. Nowadays, the availability of an ample amount of online reviews made by the customers helps us in this regard. This very fact gives us a promising research direction in the field of tourism called hotel recommendation system which also helps in improving the information processing of consumers. Real-world reviews may showcase different sentiments of the customers towards a hotel and each review can be categorized based on different aspects such as cleanliness, value, service, etc. Keeping these facts in mind, in the present work, we have proposed a hotel recommendation system using Sentiment Analysis of the hotel reviews, and aspect-based review categorization which works on the queries given by a user. Furthermore, we have provided a new rich and diverse dataset of online hotel reviews crawled from Tripadvisor.com. We have followed a systematic approach which first uses an ensemble of a binary classification called Bidirectional Encoder Representations from Transformers (BERT) model with three phases for positive–negative, neutral–negative, neutral–positive sentiments merged using a weight assigning protocol. We have then fed these pre-trained word embeddings generated by the BERT models along with other different textual features such as word vectors generated by Word2vec, TF–IDF of frequent words, subjectivity score, etc. to a Random Forest classifier . After that, we have also grouped the reviews into different categories using an approach that involves fuzzy logic and cosine similarity . Finally, we have created a recommender system by the aforementioned frameworks. Our model has achieved a Macro F1-score of 84\% and test accuracy of 92.36\% in the classification of sentiment polarities. Also, the results of the categorized reviews have formed compact clusters . The results are quite promising and much better compared to state-of-the-art models. The relevant codes and notebooks can be found here .},
  archive      = {J_ASOC},
  author       = {Biswarup Ray and Avishek Garain and Ram Sarkar},
  doi          = {10.1016/j.asoc.2020.106935},
  journal      = {Applied Soft Computing},
  pages        = {106935},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble-based hotel recommender system using sentiment analysis and aspect categorization of hotel reviews},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dimension by dimension dynamic sine cosine algorithm for
global optimization problems. <em>ASOC</em>, <em>98</em>, 106933. (<a
href="https://doi.org/10.1016/j.asoc.2020.106933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve global optimization problems , this paper proposed a novel improved version of sine cosine algorithm — the dimension by dimension dynamic sine cosine algorithm (DDSCA). In the update equation of sine cosine algorithm (SCA), the dimension by dimension strategy evaluates the solutions in each dimension, and the greedy strategy is used to form new solutions after combined them with other dimensions. Moreover, in order to balance the exploration and exploitation of SCA, a dynamic control parameter is designed to modify the position equation of this algorithm. To evaluate the effectiveness of DDSCA in solving global optimization problems , it is compared with state-of-art algorithms and modified SCA on 23 benchmark functions . The experimental results reveal the DDSCA has better robustness and efficiency. The IEEE CEC2010 large-scale functions are selected to solve high-dimensional optimization problem, the results show that the performance of the DDSCA is better than other algorithms. In addition, five engineering optimization problems are also verified the effectiveness of the DDSCA. The results of accuracy and speed show that the improved sine cosine algorithm (DDSCA) is competitive in solving global optimization problems.},
  archive      = {J_ASOC},
  author       = {Yu Li and Yiran Zhao and Jingsen Liu},
  doi          = {10.1016/j.asoc.2020.106933},
  journal      = {Applied Soft Computing},
  pages        = {106933},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dimension by dimension dynamic sine cosine algorithm for global optimization problems},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A soft computing approach based on critical chain for
project planning and control in real-world applications with interval
data. <em>ASOC</em>, <em>98</em>, 106915. (<a
href="https://doi.org/10.1016/j.asoc.2020.106915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource-constrained project scheduling problem (RCPSP) has been one of the most important topics in project scheduling in recent decades. RCPSP, due to the strategic importance of the projects and internal and external pressures for timely completion, is a very challenging task. When executing a project, controlling and monitoring it also becomes vital. This paper aims to present a new soft computing framework that incorporates decision making about RCPSP parameters, RCPSP modeling, adding project and activities buffer, and monitoring the project. In the decision-making procedure, the activities durations are interval, but resource requirements are real numbers. So, the decision-making problem needs a hybrid procedure. To overcome this matter, the hybrid projection measure is extended to obtain the experts weights and build the aggregated decision matrix . In the RCPSP section, the activities durations are not determined and vary between certain ranges. The resource requirements and range of activities durations are obtained from group decision-making method. In addition, this model is solved with simulated annealing (SA) algorithm. In the third step, buffers are considered in a way that allocating project buffer to activities’ buffers becomes based on a new normalized important factor. The normalized important factor is introduced by considering activity duration and resource requirements. Finally, a novel controlling procedure is extended by activity buffer monitoring. Two buffer threshold sets and violations are applied, and each one of them sends a particular alarm to the project manager. Project manager’s decisions in optimistic and pessimistic situations are discussed. Ultimately, the method is solved in a real case study, and the results are discussed. The application shows that the presented method is flexible in many situations. It also increases the probability of timely completion of the project in addition to tracking the deviations from the plan. The proposed method introduces a comprehensive framework, so it gives project managers a better vision. It can also act as an ideal monitoring tool to control schedule deviations and to help project manager for proper actions during the project execution.},
  archive      = {J_ASOC},
  author       = {S. Aramesh and S.M. Mousavi and V. Mohagheghi and E.K. Zavadskas and J. Antucheviciene},
  doi          = {10.1016/j.asoc.2020.106915},
  journal      = {Applied Soft Computing},
  pages        = {106915},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A soft computing approach based on critical chain for project planning and control in real-world applications with interval data},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNN-based transfer learning–BiLSTM network: A novel approach
for COVID-19 infection detection. <em>ASOC</em>, <em>98</em>, 106912.
(<a href="https://doi.org/10.1016/j.asoc.2020.106912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-2019), which emerged in Wuhan, China in 2019 and has spread rapidly all over the world since the beginning of 2020, has infected millions of people and caused many deaths. For this pandemic, which is still in effect, mobilization has started all over the world, and various restrictions and precautions have been taken to prevent the spread of this disease. In addition, infected people must be identified in order to control the infection. However, due to the inadequate number of Reverse Transcription Polymerase Chain Reaction (RT-PCR) tests, Chest computed tomography (CT) becomes a popular tool to assist the diagnosis of COVID-19. In this study, two deep learning architectures have been proposed that automatically detect positive COVID-19 cases using Chest CT X-ray images. Lung segmentation (preprocessing) in CT images, which are given as input to these proposed architectures, is performed automatically with Artificial Neural Networks (ANN). Since both architectures contain AlexNet architecture, the recommended method is a transfer learning application. However, the second proposed architecture is a hybrid structure as it contains a Bidirectional Long Short-Term Memories (BiLSTM) layer, which also takes into account the temporal properties . While the COVID-19 classification accuracy of the first architecture is 98.14\%, this value is 98.70\% in the second hybrid architecture . The results prove that the proposed architecture shows outstanding success in infection detection and, therefore this study contributes to previous studies in terms of both deep architectural design and high classification success.},
  archive      = {J_ASOC},
  author       = {Muhammet Fatih Aslan and Muhammed Fahri Unlersen and Kadir Sabanci and Akif Durdu},
  doi          = {10.1016/j.asoc.2020.106912},
  journal      = {Applied Soft Computing},
  pages        = {106912},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CNN-based transfer learning–BiLSTM network: A novel approach for COVID-19 infection detection},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AI-assisted CT imaging analysis for COVID-19 screening:
Building and deploying a medical AI system. <em>ASOC</em>, <em>98</em>,
106897. (<a href="https://doi.org/10.1016/j.asoc.2020.106897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sudden outbreak of novel coronavirus 2019 (COVID-19) increased the diagnostic burden of radiologists. In the time of an epidemic crisis, we hope artificial intelligence (AI) to reduce physician workload in regions with the outbreak, and improve the diagnosis accuracy for physicians before they could acquire enough experience with the new disease. In this paper, we present our experience in building and deploying an AI system that automatically analyzes CT images and provides the probability of infection to rapidly detect COVID-19 pneumonia. The proposed system which consists of classification and segmentation will save about 30\%–40\% of the detection time for physicians and promote the performance of COVID-19 detection. Specifically, working in an interdisciplinary team of over 30 people with medical and/or AI background, geographically distributed in Beijing and Wuhan, we are able to overcome a series of challenges ( e.g. data discrepancy, testing time-effectiveness of model, data security, etc.) in this particular situation and deploy the system in four weeks. In addition, since the proposed AI system provides the priority of each CT image with probability of infection, the physicians can confirm and segregate the infected patients in time. Using 1, 136 training cases (723 positives for COVID-19) from five hospitals, we are able to achieve a sensitivity of 0.974 and specificity of 0.922 on the test dataset, which included a variety of pulmonary diseases.},
  archive      = {J_ASOC},
  author       = {Bo Wang and Shuo Jin and Qingsen Yan and Haibo Xu and Chuan Luo and Lai Wei and Wei Zhao and Xuexue Hou and Wenshuo Ma and Zhengqing Xu and Zhuozhao Zheng and Wenbo Sun and Lan Lan and Wei Zhang and Xiangdong Mu and Chenxi Shi and Zhongxiao Wang and Jihae Lee and Zijian Jin and Minggui Lin and Jiahong Dong},
  doi          = {10.1016/j.asoc.2020.106897},
  journal      = {Applied Soft Computing},
  pages        = {106897},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task processing oriented production layout based on
evolutionary programming mechanism. <em>ASOC</em>, <em>98</em>, 106896.
(<a href="https://doi.org/10.1016/j.asoc.2020.106896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional job scheduling is an independent optimization phase without considering material flow among machines. However, in actual production, the machine layout will affect material flow time, therefore affecting job completion time. In this paper, a novel multi-task-oriented production layout problem is proposed, with the goal to optimize material flow by adjusting machine placement. To solve this problem, a production layout evolution (PLEV) framework is established, which models it as a two-stage optimization problem . In the first stage, a concept of ‘relevance between machines (RBM)’ is proposed. The RBM measures the relationship between two machines according to the processing data. A machine placement scheme is designed based on RBM. In the second stage, a job scheduling rule based on the earliest finish time (EFT) is adopted to evaluate production layouts. Then, through an evolutionary programming mechanism, the production layout can evolve to optimize makespan. Experiments are carried out to verify the feasibility and performance of the proposed PLEV framework. The results demonstrate that promising product layouts for multi-task processing can be obtained by using this framework.},
  archive      = {J_ASOC},
  author       = {Zhao-Hui Sun and Di Liang and Zilong Zhuang and Liang Chen and Xinguo Ming},
  doi          = {10.1016/j.asoc.2020.106896},
  journal      = {Applied Soft Computing},
  pages        = {106896},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-task processing oriented production layout based on evolutionary programming mechanism},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A consensus measure for group decision making with hesitant
linguistic preference information based on double alpha-cut.
<em>ASOC</em>, <em>98</em>, 106890. (<a
href="https://doi.org/10.1016/j.asoc.2020.106890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hesitant fuzzy linguistic term set, which is characterized by a set of possible linguistic terms, becomes a useful tool to describe the complex cognitions of experts. In many cases, it is much more flexible for experts to provide evaluation information via pairwise comparisons rather than give their evaluation information on options directly. In this regard, the hesitant fuzzy linguistic preference relation (HFLPR) whose elements are hesitant fuzzy linguistic elements was introduced. Besides, to make a reasonable decision, a group of experts are usually involved in practice and thus how to reach an agreement among experts whose perceptions are represented by HFLPRs turns to be significantly important. To meet this challenge, in this study, we propose a novel consensus measure for group decision making with HFLPRs. Firstly, we define an alpha-cut-based method to calculate the similarities among different hesitant fuzzy linguistic evaluations and then develop an alpha-cut-based consensus measure for group decision making with HFLPRs. By converting the linguistic terms of hesitant fuzzy linguistic elements into numerical values of 0 or 1, the consensus measure can be simplified. Finally, to illustrate the effectiveness of the method, we apply it to a practical case study. A comparative analysis is provided to demonstrate the advantages and significance in practical applications of the proposed method.},
  archive      = {J_ASOC},
  author       = {Huchang Liao and Lisheng Jiang and Ran Fang and Rui Qin},
  doi          = {10.1016/j.asoc.2020.106890},
  journal      = {Applied Soft Computing},
  pages        = {106890},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A consensus measure for group decision making with hesitant linguistic preference information based on double alpha-cut},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The ensemble deep learning model for novel COVID-19 on CT
images. <em>ASOC</em>, <em>98</em>, 106885. (<a
href="https://doi.org/10.1016/j.asoc.2020.106885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid detection of the novel coronavirus disease, COVID-19, has a positive effect on preventing propagation and enhancing therapeutic outcomes. This article focuses on the rapid detection of COVID-19. We propose an ensemble deep learning model for novel COVID-19 detection from CT images. 2933 lung CT images from COVID-19 patients were obtained from previous publications, authoritative media reports, and public databases. The images were preprocessed to obtain 2500 high-quality images. 2500 CT images of lung tumor and 2500 from normal lung were obtained from a hospital. Transfer learning was used to initialize model parameters and pretrain three deep convolutional neural network models: AlexNet, GoogleNet, and ResNet . These models were used for feature extraction on all images. Softmax was used as the classification algorithm of the fully connected layer. The ensemble classifier EDL-COVID was obtained via relative majority voting. Finally, the ensemble classifier was compared with three component classifiers to evaluate accuracy, sensitivity, specificity, F value, and Matthews correlation coefficient. The results showed that the overall classification performance of the ensemble model was better than that of the component classifier. The evaluation indexes were also higher. This algorithm can better meet the rapid detection requirements of the novel coronavirus disease COVID-19.},
  archive      = {J_ASOC},
  author       = {Tao Zhou and Huiling Lu and Zaoli Yang and Shi Qiu and Bingqiang Huo and Yali Dong},
  doi          = {10.1016/j.asoc.2020.106885},
  journal      = {Applied Soft Computing},
  pages        = {106885},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The ensemble deep learning model for novel COVID-19 on CT images},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive optimal fuzzy logic based energy management in
multi-energy microgrid considering operational uncertainties.
<em>ASOC</em>, <em>98</em>, 106882. (<a
href="https://doi.org/10.1016/j.asoc.2020.106882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intelligent decision making of multi-energy management in a microgrid is a non-trivial task due to the intermittent and stochastic nature of highly penetrated renewable energy sources and demand. To address such a challenge, the energy management system often adopts the prediction based day-ahead energy scheduling and real-time energy dispatch to optimally coordinate the operation of dispatchable components, e.g., battery-based energy storage and thermal units. This paper presents an adaptive optimal fuzzy logic based energy management solution to develop appropriate day-ahead fuzzy rules for real-time energy dispatch adaptively in the presence of operational uncertainties. The solution determines the optimal fuzzy inference system (e.g., the membership function shape and the inference rules set) based on the predicted information over a certain period through a novel offline meta-heuristic optimization algorithm. The real-time energy dispatch based on the obtained optimal fuzzy logic rules can be further carried out to meet the various operational criteria, e.g., minimal power fluctuation and operational cost. The proposed solution is extensively evaluated through simulation experiments in comparison with two existing approaches: the online rule-based dispatch method and the meta-heuristic optimization-based offline scheduling method. The numerical results demonstrate the superior performance of the proposed energy management solution.},
  archive      = {J_ASOC},
  author       = {Wei Dong and Qiang Yang and Xinli Fang and Wei Ruan},
  doi          = {10.1016/j.asoc.2020.106882},
  journal      = {Applied Soft Computing},
  pages        = {106882},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive optimal fuzzy logic based energy management in multi-energy microgrid considering operational uncertainties},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Equilibrium optimization algorithm for network
reconfiguration and distributed generation allocation in power systems.
<em>ASOC</em>, <em>98</em>, 106867. (<a
href="https://doi.org/10.1016/j.asoc.2020.106867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is imperative to distribution system operators to provide quantitative as well as qualitative power demand and satisfy consumers’ satisfaction. So, it is important to address one of the most promising combinatorial optimization problems for the optimal integration of power distribution network reconfiguration (PDNR) with distributed generations (DGs). In this regard, this paper proposes an improved equilibrium optimization algorithm (IEOA) combined with a proposed recycling strategy for configuring the power distribution networks with optimal allocation of multiple distributed generators . The recycling strategy is augmented to explore the solution space more effectively during iterations. The effectiveness of the proposed algorithm is checked on 23 standard benchmark functions . Simultaneous integration of PDNR and DG are carried out considering the 33 and 69-bus distribution test systems at three different load levels and its superiority is established. Verification of the proposed technique on large scale distribution system with a variety of control variables is introduced on a 137-bus large scale distribution system. These simulations lead to enhanced distribution system performance, quality and reliability. While, the integration represents a challenge for complexity and disability to achieve optimal solutions of the considered problem especially for multi-objective framework. To solve this challenge, a multi-objective function is developed considering total active power loss and overall voltage enhancement with respecting the system limitations. The proposed algorithm is contrasted with harmony search , genetic, refined genetic, fireworks, and firefly optimization algorithms. The obtained results confirm the effectiveness and robustness of the proposed technique compared with the competitive algorithms.},
  archive      = {J_ASOC},
  author       = {A.M. Shaheen and A.M. Elsayed and Ragab A. El-Sehiemy and Almoataz Y. Abdelaziz},
  doi          = {10.1016/j.asoc.2020.106867},
  journal      = {Applied Soft Computing},
  pages        = {106867},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Equilibrium optimization algorithm for network reconfiguration and distributed generation allocation in power systems},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A knee-guided differential evolution algorithm for unmanned
aerial vehicle path planning in disaster management. <em>ASOC</em>,
<em>98</em>, 106857. (<a
href="https://doi.org/10.1016/j.asoc.2020.106857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles are instrumental in monitoring and analyzing information and searching for people in disaster relief scenarios. In this paper, path planning is constructed as a multiobjective optimization problem with constraints in a three-dimensional terrain disaster scenario. The objective functions involve the distance and risk of the path, which are calculated based on Bézier theory. The constraints include the turning angle and flight altitude . To solve this problem in an effective and efficient manner, a differential evolution algorithm that is based solely on the knee point is proposed, in which the knee solution would guide the search direction of the algorithm. According to the minimal Manhattan distance approach, the algorithm can quickly identify an optimal solution to generating a smooth path for decision-makers. Experimental results have confirmed the superiority of the proposed algorithm, and the rankings of the minimal Manhattan distance approach are consistent with multicriteria decision-making methods.},
  archive      = {J_ASOC},
  author       = {Xiaobing Yu and Chenliang Li and Gary G. Yen},
  doi          = {10.1016/j.asoc.2020.106857},
  journal      = {Applied Soft Computing},
  pages        = {106857},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A knee-guided differential evolution algorithm for unmanned aerial vehicle path planning in disaster management},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A greedy belief rule base generation and learning method for
classification problem. <em>ASOC</em>, <em>98</em>, 106856. (<a
href="https://doi.org/10.1016/j.asoc.2020.106856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among many rule-based systems employed in classification problems, the belief rule-based (BRB) system has been significant for its ability to deal with both quantitative and qualitative information under uncertainty. However, it may face excessive information and low accuracy in some applications due to the limitations of the conventional BRB generation method. To this end, a greedy-based BRB learning method is proposed in this paper. Firstly, the BRB is generated by selecting a reduced number of belief rules from a set of candidate rules. Then, the BRB learning process is conducted by exploiting a selection and reduction strategy, which searches and selects the optimal rules from candidate rules as well as removes noise and redundant rules. Moreover, the original procedures of the inference process and class estimation are retained from conventional BRB systems. Thirty standard classification benchmarks are tested to validate the effectiveness and efficiency of the proposed method, and the classification results are compared with existing rule-based systems, novel belief rule-based systems, and conventional machine learning methods . The comparison results show that the proposed method could achieve relatively satisfactory accuracy while having a significantly smaller BRB. Furthermore, the results derived from benchmarks with two or three classes show the superior performance of the proposed method compared with some state-of-the-art classification methods.},
  archive      = {J_ASOC},
  author       = {Fei Gao and An Zhang and Wenhao Bi and Junwen Ma},
  doi          = {10.1016/j.asoc.2020.106856},
  journal      = {Applied Soft Computing},
  pages        = {106856},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A greedy belief rule base generation and learning method for classification problem},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Standardized variable distances: A distance-based machine
learning method. <em>ASOC</em>, <em>98</em>, 106855. (<a
href="https://doi.org/10.1016/j.asoc.2020.106855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, machine learning algorithms are an important research area capable of analyzing and modeling data in any field. Information obtained through machine learning methods helps researchers and planners to understand and review systematic problems of their current strategies. Thus, it is very important to work fully in every field that facilitates human life, such as early and correct diagnosis, correct choice, fully functioning autonomous systems . In this paper, a novel machine learning algorithm for multiclass classification is presented. The proposed method is designed based on the Minimum Distance Classifier (MDC) algorithm. The MDC is variance-insensitive because it classifies input vectors by calculating their distances/similarities with respect to class-centroids (average value of input vectors of a class). As it is known, real-world data contains certain proportions of noise. This situation negatively affects the performance of the MDC. To overcome this problem, we developed a variance-sensitive model, which we call Standardized Variable Distances (SVD), considering the standard deviation and z-score (standardized variable) factors. To ensure the accuracy of the SVD, we used Wisconsin Breast Cancer Original (WBCO) and LED Display Domain (led7digit) datasets, which we obtained from UCI machine learning repository, with 5-fold cross validation. It was compared and analyzed classification performance of the SVD with Decision Tree (DT), Random Forest (RF), k-Nearest Neighbor (k-NN), Multinomial Logistic Regression (MLR), Naïve Bayes (NB), Support Vector Machine (SVM), and the Minimum Distance Classifier (MDC), which are well-known in the literature. It has also been compared thirteen different studies using the same datasets over the past five years. Our results in the experimental studies have shown that the SVD can classify better than traditional and state-of-the-art methods, compared in this study. The proposed method reached over 97\% classification accuracy (CACC), F-measure (FM) and area under the curve (AUC) on the WBCO dataset. On the led7digit dataset, approximately 74\% CACC, 75.1\% FM and 82.2\% AUC scores were obtained. It has been observed that the classification scores obtained with the SVD are higher than other ML algorithms used in the experimental studies.},
  archive      = {J_ASOC},
  author       = {Abdullah Elen and Emre Avuçlu},
  doi          = {10.1016/j.asoc.2020.106855},
  journal      = {Applied Soft Computing},
  pages        = {106855},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Standardized variable distances: A distance-based machine learning method},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Balancing the user-driven feature selection and their
incidence in the clustering structure formation. <em>ASOC</em>,
<em>98</em>, 106854. (<a
href="https://doi.org/10.1016/j.asoc.2020.106854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The feature selection represents a key step in mining high-dimensional data: the significance of features in maintaining the data structure while ignoring the feature redundancy is crucial to improve the final performance of classification methods. At the same time, an accurate understanding of feature domains may need human intervention to balance the importance of structure-based features with those one dictated by human expertise. To address this issue, this work introduces a human-driven feature selection method for data clustering . The algorithm, called Feature Selection EFCM (FS-EFCM in short), aims at supporting the relevance of some features from the domain of interest, but preserving their incidence in the natural clustering structure . The relevance and incidence of each feature are measure assessed during the FS-EFCM execution, in order to find a balance between the human suggestions about the feature importance and the by feature incidence in the natural cluster-based data distribution. Experimental results and comparisons highlight how the algorithm is robust in the presence of not very significant features, and the classification performance shows the effectiveness of the proposed feature selection method compared with the well-known feature selection algorithms .},
  archive      = {J_ASOC},
  author       = {Ferdinando Di Martino and Sabrina Senatore},
  doi          = {10.1016/j.asoc.2020.106854},
  journal      = {Applied Soft Computing},
  pages        = {106854},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Balancing the user-driven feature selection and their incidence in the clustering structure formation},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A thorough review of aircraft landing operation from
practical and theoretical standpoints at an airport which may include a
single or multiple runways. <em>ASOC</em>, <em>98</em>, 106853. (<a
href="https://doi.org/10.1016/j.asoc.2020.106853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The organization of landing operation and related issues within an Air Traffic Control (ATC) system in the airport Terminal Maneuvering Area (TMA) are hitherto performed by human air traffic controllers. These latter skilled buffers are responsible for aligning and controlling a continuous inbound flow of aircraft. These tasks are beginning from the time of notification of the getting of arrivals to the TMA entry fixes until the end of landings on the assigned runway(s). The whole duties carried out by air traffic controllers, such as arrival control, sequencing, scheduling, and assignment solutions are frequently implemented according to the information shown on various instruments, and some simple rules such as First Come First Served (FCFS) discipline. It is also worth noting that the training and past experience of the air traffic controllers are playing a leading role, and are essential part of decision-making processes. Since the demand for air traffic has continued to rise and with the emergence of congested airports, it became clear that the mission entrusted to the air traffic controller and its ability to undertake in its job in the right way has become increasingly more exacting. Consequently, there has been a greater demand to make the ATC more automated by improving aircraft scheduling/sequencing techniques, and developing further decision support systems, to which the challenge intends to alleviate the human intervention and not to replace the air traffic controller. Here of, the core goal is to provide a system review of past and the most up-to-date research related to the Aircraft Landing Problem (ALP) for an airport , which may include a single or multiple runways. Recognizing that the theoretical aspect relating to the ALP is dealt with this study, the evaluation of several solutions for that specific problem is furthermore covered. This evaluation will not succeed without explaining first the landing operation from a practical standpoint, which is a fundamental way to better understand the reasons why research effort has been devoted to suggest solutions for the overall arrival management. A comparative study which selects the distinguishing features that can be recognized in some research works against other related works is then proposed. In the last, a summary of the findings of published literature and recommendations for the future trend of ALP are pointed up as conclusions.},
  archive      = {J_ASOC},
  author       = {Meriem Ben Messaoud},
  doi          = {10.1016/j.asoc.2020.106853},
  journal      = {Applied Soft Computing},
  pages        = {106853},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A thorough review of aircraft landing operation from practical and theoretical standpoints at an airport which may include a single or multiple runways},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new deep learning ensemble credit risk evaluation model
with an improved synthetic minority oversampling technique.
<em>ASOC</em>, <em>98</em>, 106852. (<a
href="https://doi.org/10.1016/j.asoc.2020.106852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, research has found that in many credit risk evaluation domains, deep learning is superior to traditional machine learning methods and classifier ensembles perform significantly better than single classifiers. However, credit evaluation model based on deep learning ensemble algorithm has rarely been studied. Moreover, credit data imbalance still challenges the performance of credit scoring models. Therefore, to go some way to filling this research gap, this study developed a new deep learning ensemble credit risk evaluation model to deal with imbalanced credit data. First, an improved synthetic minority oversampling technique (SMOTE) method was developed to overcome known SMOTE shortcomings, after which a new deep learning ensemble classification method combined with the long-short-term-memory (LSTM) network and the adaptive boosting (AdaBoost) algorithm was developed to train and learn the processed credit data. Then, area under the curve (AUC), the Kolmogorov–Smirnov (KS) and the non-parametric Wilcoxon test were employed to compare the performance of the proposed model and other widely used credit scoring models on two imbalanced credit datasets. The experimental test results indicated that the proposed deep learning ensemble model was generally more competitive when addressing imbalanced credit risk evaluation problems than other models.},
  archive      = {J_ASOC},
  author       = {Feng Shen and Xingchao Zhao and Gang Kou and Fawaz E. Alsaadi},
  doi          = {10.1016/j.asoc.2020.106852},
  journal      = {Applied Soft Computing},
  pages        = {106852},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new deep learning ensemble credit risk evaluation model with an improved synthetic minority oversampling technique},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Liger: A cross-platform open-source integrated optimization
and decision-making environment. <em>ASOC</em>, <em>98</em>, 106851. (<a
href="https://doi.org/10.1016/j.asoc.2020.106851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world optimization problems involving multiple conflicting objectives are commonly best solved using multi-objective optimization as this provides decision-makers with a family of trade-off solutions. However, the complexity of using multi-objective optimization algorithms often impedes the optimization process. Knowing which optimization algorithm is the most suitable for the given problem, or even which setup parameters to pick, requires someone to be an optimization specialist. The lack of supporting software that is readily available, easy to use and transparent can lead to increased design times and increased cost. To address these challenges, Liger is presented. Liger has been designed for ease of use in industry by non-specialists in optimization. The user interacts with Liger via a visual programming language to create an optimization workflow, enabling the user to solve an optimization problem. Liger contains a novel optimization library known as Tigon. The library utilizes the concept of design patterns to enable the composition of optimization algorithms by making use of simple reusable operator nodes. The library offers a varied range of multi-objective evolutionary algorithms which cover different paradigms in evolutionary computation; and supports a wide variety of problem types, including support for using more than one programming language at a time to implement the optimization model. Additionally, Liger functionality can be easily extended by plugins that provide access to state-of-the-art visualization tools and are responsible for managing the graphical user interface . Lastly, new user-driven interactive capabilities are shown to facilitate the decision-making process and are demonstrated on a control engineering optimization problem.},
  archive      = {J_ASOC},
  author       = {João A. Duro and Yiming Yan and Ioannis Giagkiozis and Stefanos Giagkiozis and Shaul Salomon and Daniel C. Oara and Ambuj K. Sriwastava and Jacqui Morison and Claire M. Freeman and Robert J. Lygoe and Robin C. Purshouse and Peter J. Fleming},
  doi          = {10.1016/j.asoc.2020.106851},
  journal      = {Applied Soft Computing},
  pages        = {106851},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Liger: A cross-platform open-source integrated optimization and decision-making environment},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective optimization-based TOPSIS method for
sustainable product design under epistemic uncertainty. <em>ASOC</em>,
<em>98</em>, 106850. (<a
href="https://doi.org/10.1016/j.asoc.2020.106850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable product design has captured considerable attention over recent years due to the growing customer demands of sustainability. To improve the environmental performance of products at the early stage of product design, a variety of economic, social, and environmental factors, such as manufacturing cost and time, product yield, capacity, customer preferences, and pollutant emissions, have to be taken into account jointly. However, due to the lack of knowledge and ambiguity of customers and experts, some of these factors may contain epistemic uncertainties , overlooking them may lead to an infeasible design . To fill the gap, we propose a new multi-objective optimization-based technique for order preference by similarity to ideal solution (TOPSIS) method to facilitate sustainable product design under epistemic uncertainty. In the proposed method, we develop a fuzzy Mahalanobis–Taguchisystem method to address the epistemic uncertainty of customer preferences on optimization objectives . Meanwhile, we introduce the Me measure to manipulate the epistemic uncertainty of experts’ judgments on process parameters and variables during the manufacturing process . Subsequently, we implement the new TOPSIS method to obtain the optimal design scheme. We provide an example of sustainable substrate design, along with sensitivity analysis scenarios and comparative studies, to elaborate on the performance of the proposed method.},
  archive      = {J_ASOC},
  author       = {Jing Zhou and Tangfan Xiahou and Yu Liu},
  doi          = {10.1016/j.asoc.2020.106850},
  journal      = {Applied Soft Computing},
  pages        = {106850},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimization-based TOPSIS method for sustainable product design under epistemic uncertainty},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing a decision support system for logistics service
provider selection employing fuzzy MULTIMOORA &amp; BWM in mining
equipment manufacturing. <em>ASOC</em>, <em>98</em>, 106849. (<a
href="https://doi.org/10.1016/j.asoc.2020.106849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, maximizing the production by reducing the associated risks in the supply chain and enhancing the final product quality by selecting the best providers are among the most fundamental challenges encountered within the equipment manufacturing industry worldwide. The lack of timely delivery of machines to customers and unregulated purchase of goods associated with the delivery of the machines are among the many problems faced by the manufactures. The proposed research aims to evaluate a Decision Support System (DSS) for selecting the most appropriate logistic service provider out of three service providers companies. Three companies X1, X2 and X3 were weighted and ranked using two decision-making methods, namely Fuzzy best–worst​ Method (FBWM) and Multiple Objective Optimizations on the basis of Ratio Analysis plus full Multiplicative Form (MULTIMOORA), considering eight criteria and their corresponding sub-criteria, respectively. Once finished with constructing the decision matrix , the analytical data being obtained from the two methods were processed using Microsoft Excel and the Lingo software. According to the results, it is concluded that Company X3 is the best logistics service provider.},
  archive      = {J_ASOC},
  author       = {Elnaz Poormohammad Sarabi and Soroush Avakh Darestani},
  doi          = {10.1016/j.asoc.2020.106849},
  journal      = {Applied Soft Computing},
  pages        = {106849},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing a decision support system for logistics service provider selection employing fuzzy MULTIMOORA &amp; BWM in mining equipment manufacturing},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing customer satisfaction of O2O takeaway based on
online reviews by integrating fuzzy comprehensive evaluation with AHP
and probabilistic linguistic term sets. <em>ASOC</em>, <em>98</em>,
106847. (<a href="https://doi.org/10.1016/j.asoc.2020.106847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, O2O takeaway has a rapid development with increasingly fierce market competitions. The online reviews of these takeaway platforms give restaurants an opportunity to better understand their customers and find their own shortages. However, how to dig customer satisfaction information from online reviews of restaurants is a challenge. Therefore, we establish an O2O takeaway customer satisfaction evaluation system and evaluate the satisfaction degree for restaurants by analyzing online reviews. First, we use European customer satisfaction index and Service performance-aware service quality measurement method as a logical framework. Meantime, we adopt content analysis to construct the evaluation system of customer satisfaction. Taking probabilistic linguistic term sets and the frequency into consideration, we improve analytic hierarchy process and develop a combination way to calculate the index weights of the evaluation system. The fuzzy comprehensive evaluation method is further designed to calculate the customer satisfaction level. Finally, we have an empirical analysis on Meituan and Ele.me platforms to validate our method.},
  archive      = {J_ASOC},
  author       = {Decui Liang and Zhuoyin Dai and Mingwei Wang},
  doi          = {10.1016/j.asoc.2020.106847},
  journal      = {Applied Soft Computing},
  pages        = {106847},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessing customer satisfaction of O2O takeaway based on online reviews by integrating fuzzy comprehensive evaluation with AHP and probabilistic linguistic term sets},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smooth pinball loss nonparallel support vector machine for
robust classification. <em>ASOC</em>, <em>98</em>, 106840. (<a
href="https://doi.org/10.1016/j.asoc.2020.106840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a robust smooth pinball loss nonparallel support vector machine (SpinNSVM) for binary classification . We first define a smooth pinball loss function, which is insensitive to the feature noise of samples, especially for the samples located nearby the boundary, and is able to capture the distribution of samples well simultaneously. Due to its differentiability , SpinNSVM model is formulated with two convex optimization problems . Furthermore, an efficient dual coordinate descent algorithm is adopted to solve the dual formulations of SpinNSVM. Numerical experiments are provided to demonstrate the effectiveness and the efficiency of the proposed method.},
  archive      = {J_ASOC},
  author       = {Ming-Zeng Liu and Yuan-Hai Shao and Chun-Na Li and Wei-Jie Chen},
  doi          = {10.1016/j.asoc.2020.106840},
  journal      = {Applied Soft Computing},
  pages        = {106840},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Smooth pinball loss nonparallel support vector machine for robust classification},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward an adaptive protection scheme in active distribution
networks: Intelligent approach fault detector. <em>ASOC</em>,
<em>98</em>, 106839. (<a
href="https://doi.org/10.1016/j.asoc.2020.106839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional protection schemes have proven insufficient for the protection of Active Distribution Networks (ADN). Novel protection schemes with an adaptive approach should be developed to guarantee the protection of ADN under all their operating conditions. This paper proposes an ADN adaptive protection methodology, which is based on an intelligent approach fault detector over locally available measurements. This approach uses Machine Learning (ML) based techniques to reduce the strong dependence of the adaptive protection schemes on the availability of communication systems and to determine if, over a fault condition, an Intelligent Electronic Device (IED) should operate considering the changes in operational conditions of an ADN. Additionally, the methodology takes into account different and remarkable recommendations for the use of ML techniques . The proposed methodology is validated on the modified IEEE 34-nodes test feeder. Additionally, it takes into consideration typical features of ADN and micro-grids like the load imbalance, reconfiguration, changes in impedance upstream from the micro-grid, and off-grid/on-grid operation modes. The results demonstrate the flexibility and simplicity of the methodology to determine the best accuracy performance among several ML models. Besides, they show the methodology’s versatility to find the suitable ML model for IEDs located on different zones of an ADN. The ease of design’s implementation, formulation of parameters, and promising test results indicate the potential for real-life applications.},
  archive      = {J_ASOC},
  author       = {J. Marín-Quintero and C. Orozco-Henao and W.S. Percybrooks and Juan C. Vélez and Oscar Danilo Montoya and W. Gil-González},
  doi          = {10.1016/j.asoc.2020.106839},
  journal      = {Applied Soft Computing},
  pages        = {106839},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Toward an adaptive protection scheme in active distribution networks: Intelligent approach fault detector},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autoencoder-based multi-task learning for imputation and
classification of incomplete data. <em>ASOC</em>, <em>98</em>, 106838.
(<a href="https://doi.org/10.1016/j.asoc.2020.106838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of missing values in real-world datasets increases the difficulty of data analysis . In this paper, we propose an autoencoder (AE)-based multi-task learning (MTL) model and optimize missing values dynamically to classify incomplete datasets having interdependencies among attributes. Specifically, we first design the input structure of hidden neurons in a dynamic way to enhance the imputation performance of AE, and then reorganize the output layer and construct an MTL model to achieve imputation and classification simultaneously. During network training and prediction, missing values are treated as variables and optimized dynamically accompanying with network parameters under the consideration of the incomplete model input. The optimization of missing values promotes the MTL model to match the regression and classification structures implied in incomplete data, thus reducing the impact of the perturbation caused by missing values effectively. The experiments on several datasets validate the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Xiaochen Lai and Xia Wu and Liyong Zhang},
  doi          = {10.1016/j.asoc.2020.106838},
  journal      = {Applied Soft Computing},
  pages        = {106838},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Autoencoder-based multi-task learning for imputation and classification of incomplete data},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A distributed density estimation algorithm and its
application to naive bayes classification. <em>ASOC</em>, <em>98</em>,
106837. (<a href="https://doi.org/10.1016/j.asoc.2020.106837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning a density function from observations of an unknown underlying model in a distributed setting, where the observations are partitioned into different sites. Applying commonly used density estimation methods such as Gaussian Mixture Model (GMM) or Kernel Density Estimation (KDE) to distributed data leads to an extensive amount of communication. A familiar approach to address this issue is to sample a small subset of data and collect them into a central node to run the density estimation algorithms on them. In this paper, we follow an alternative to the sub-sampling approach by proposing the nested Log-Poly model. This model provides an accurate density estimation from a small sized statistic of the entire data. In distributed settings, it transfers the small sized statistics from the client nodes to a central node. The estimation process is then run in the central node. The proposed model can be used in different learning tasks such as classification in supervised learning and clustering in unsupervised learning . However, the properties of nested Log-Poly make it a suitable model for one-dimensional density estimations in the distributed settings. This makes Log-Poly a good choice for naive Bayes classifier , where one-dimensional density estimation is required for every feature conditioned on the class label. We provide a theoretical analysis of the efficiency of our model in estimating a wide range of probability density functions . Our experiments show that nested Log-Poly outperforms the state of the art density estimators on several synthetic datasets . We compare the accuracy and the communication load of naive Bayes classifier using nested Log-Poly and other related density estimators on several real datasets. The experimental outcomes depict that nested Log-Poly has less communication load, while maintaining a competitive classification accuracy compared to similar methods that use the entire data. Moreover, we present a comprehensive comparison between nested Log-Poly and validated KDE with sub-sampling, in terms of the number of communicated variables and the number of bytes transferred between the clients and the central node. Nested Log-Poly provides comparable accuracy with the validated KDE with sub-sampling, while communicating fewer variables. However, our method needs to compute and transmit the variables with a high precision in order to accurately capture the details of the underlying distributions.},
  archive      = {J_ASOC},
  author       = {Ahmad Khajenezhad and Mohammad Ali Bashiri and Hamid Beigy},
  doi          = {10.1016/j.asoc.2020.106837},
  journal      = {Applied Soft Computing},
  pages        = {106837},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A distributed density estimation algorithm and its application to naive bayes classification},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning for arabic subjective sentiment analysis:
Challenges and research opportunities. <em>ASOC</em>, <em>98</em>,
106836. (<a href="https://doi.org/10.1016/j.asoc.2020.106836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fields of machine learning and Web technologies have witnessed significant development in the last years. This caused a ceaseless and rapid growth in sharing of the views and experience regarding services or products over the Internet in different domains. Therefore, a torrential flow of online data is available for analytical studies. Sentiment Analysis (SA) is a subtask of Natural Language Processing (NLP) that aims to analyze huge data for detecting people opinions and emotions. This field has gained growing interest by public and private sectors that led to the occurrence of many challenges, especially that related to the Arabic language. The purpose of this study is to conduct a systematic review from year 2000 until June, 2020 to analyze the status of deep Learning for Arabic NLP (ANLP) task in Arabic Subjective Sentiment Analysis (ASSA) to highlight the challenges and propose research opportunities in this field. Extensive number of research studies were reviewed to investigate deep learning techniques applied in subjective sentiment analysis for the Arabic language. We observed that CNN and RNN (LSTM) models were the most common methods used for ASSA. The results of this review show that there is a need for more efforts to implement modernized deep learning methods for Arabic sentiment analysis systems.},
  archive      = {J_ASOC},
  author       = {Ali Bou Nassif and Ashraf Elnagar and Ismail Shahin and Safaa Henno},
  doi          = {10.1016/j.asoc.2020.106836},
  journal      = {Applied Soft Computing},
  pages        = {106836},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning for arabic subjective sentiment analysis: Challenges and research opportunities},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Failure mode and effect analysis: An interval-valued
intuitionistic fuzzy cloud theory-based method. <em>ASOC</em>,
<em>98</em>, 106834. (<a
href="https://doi.org/10.1016/j.asoc.2020.106834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure mode and effect analysis (FMEA) is a proactive quality management instrument to improve the reliability of systems. Nevertheless, the classical FMEA technique has suffered from many weaknesses, e.g., inability to handle inaccurate information, strong sensitiveness to variations in assessments. Although fuzzy theories are utilized to enhance the classical FMEA, they still have some deficiencies, e.g., requiring extra assumptions, lacking mechanism to describe the hesitation and randomness of assessment information simultaneously, ignoring the psychological effects of experts, and considering only three risk aspects among most of them. Hence, this work presents a novel concept of interval-valued intuitionistic fuzzy clouds (IVIFCs), which combines the merit of interval-valued intuitionistic fuzzy set in reflecting vagueness and hesitation of decision information and the strength of cloud model in manipulating randomness of quantitative information, and a new FMEA based on IVIFCs. Then, the individual bounded rationalities are determined by a developed weighting method considering both subjective and objective importance. Moreover, a hierarchical structure containing eight risk elements is established to identify risk orders of failures. Additionally, a well-defined Excel computational program is presented to reduce the calculation burden effectively. Finally, a real application of a machine tool is conducted by the proposed FMEA to illustrate its effectiveness and superiority.},
  archive      = {J_ASOC},
  author       = {Guangquan Huang and Liming Xiao},
  doi          = {10.1016/j.asoc.2020.106834},
  journal      = {Applied Soft Computing},
  pages        = {106834},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Failure mode and effect analysis: An interval-valued intuitionistic fuzzy cloud theory-based method},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A carnivorous plant algorithm for solving global
optimization problems. <em>ASOC</em>, <em>98</em>, 106833. (<a
href="https://doi.org/10.1016/j.asoc.2020.106833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel metaheuristic algorithm , namely, carnivorous plant algorithm (CPA), inspired by how the carnivorous plants adapting to survive in the harsh environment, was proposed. The CPA was first evaluated on thirty well-known benchmark functions with different characteristics and seven CEC 2017 test functions. Its convergence characteristic and computational time were analysed and compared with seven widely used metaheuristic algorithms, with the superiority was validated using the Wilcoxon signed-rank test. The applicability of the CPA was further examined on mechanical engineering design problems and a real-world challenging application of controlling the orientation of a five degree-of-freedom robotic arm . Experimental simulations demonstrated the supremacy of the CPA in solving global optimization problems .},
  archive      = {J_ASOC},
  author       = {Kok Meng Ong and Pauline Ong and Chee Kiong Sia},
  doi          = {10.1016/j.asoc.2020.106833},
  journal      = {Applied Soft Computing},
  pages        = {106833},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A carnivorous plant algorithm for solving global optimization problems},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discrete particle swarm optimization method for assignment
of supermarket resources to urban residential communities under the
situation of epidemic control. <em>ASOC</em>, <em>98</em>, 106832. (<a
href="https://doi.org/10.1016/j.asoc.2020.106832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When contagious diseases hit a city, such as MERS, SARS, and COVID-19, the problem arises as how to assign the limited supermarket resources to urban residential communities for government measures. In this study, in order to solve the assignment problem from supermarket resources to urban residential communities under the situation of the epidemic control, the discrete multi-objective particle swarm algorithm can be improved by introducing some new strategies, and the probability matrix can be used to simulate the many-to-many assignment relationship between residential communities and supermarkets. The ultimate purpose of this research is to achieve an optimal way to balance the two conflicting objectives, i.e. minimization of the cross-infection risk and maximization of the service coverage rate. Also, the optimization considers the accessible distance limit and the service capacity constraints of supermarkets for the feasible scheme. For this aim, we redefine the subtraction operator, add operator and multiply operator to generate the Pareto optimal solutions , and introduce a new study strategy based on the idea of differential evolution in the particle swarm algorithm (PSO-DE). In this work, we take the COVID-19 epidemic outbreak in Wuhan city of China as an example in the experiment. The simulation results are compared with the Genetic Algorithm (GA), Simulated Annealing (SA), Ant Colony Algorithm (ACO) and the Particle Swarm Optimization with Roulette Wheel Selection (PSO-R), and these results have been shown that the algorithm PSO-DE proposed in this work has a better optimization performance in both objectives.},
  archive      = {J_ASOC},
  author       = {Xinyan Zou and Zhixiang Fang and Shengwu Xiong},
  doi          = {10.1016/j.asoc.2020.106832},
  journal      = {Applied Soft Computing},
  pages        = {106832},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete particle swarm optimization method for assignment of supermarket resources to urban residential communities under the situation of epidemic control},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A soft-computing framework for automated optimization of
multiple product quality criteria with application to micro-fluidic chip
production. <em>ASOC</em>, <em>98</em>, 106827. (<a
href="https://doi.org/10.1016/j.asoc.2020.106827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a general strategy for optimizing the quality of products of industrial batch processes that comprise multiple production stages. We focus on the particularities of applying this strategy in the field of micro-fluidic chip production. Our approach is based on three interacting components: (i) a new hybrid design of experiments (DoE) strategy that combines expert- and distribution-based space exploration with model-based uncertainty criteria to obtain a representative set of initial samples (i.e., settings of essential machining process parameters), (ii) construction of linear and non-linear predictive mappings from these samples to describe the relation between machining process parameters and resulting quality control (QC) values and (iii) incorporation of these mappings as surrogate fitness estimators into a multi-objective optimization process to discover settings that outperform those routinely used by operators. These optimized settings lead to final products with better quality and/or higher functionality for the clients. The optimization module employs a co-evolutionary strategy we developed that is able to deliver better Pareto non-dominated solutions than the renowned NSGA-II multi-objective solver. We applied our proposed high-level surrogate-based multi-objective strategy both in a single/late-stage optimization scenario and in a more challenging multi-stage scenario, yielding final optimization results that improved parameter settings and thus product quality compared to standard expert-based production process parameterizations.},
  archive      = {J_ASOC},
  author       = {Alexandru-Ciprian Zăvoianu and Edwin Lughofer and Robert Pollak and Christian Eitzinger and Thomas Radauer},
  doi          = {10.1016/j.asoc.2020.106827},
  journal      = {Applied Soft Computing},
  pages        = {106827},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A soft-computing framework for automated optimization of multiple product quality criteria with application to micro-fluidic chip production},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporation of multimodal multiobjective optimization in
designing a filter based feature selection technique. <em>ASOC</em>,
<em>98</em>, 106823. (<a
href="https://doi.org/10.1016/j.asoc.2020.106823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current work reports about the development of a new feature selection technique fusing two concepts, multimodal multiobjective optimization and filter-based feature selection. Use of the concept of multimodality in multiobjective based feature selection helps in generating diverse set of feature subsets on final Pareto front . This approach evaluates the quality of the reduced feature set by utilizing different quality measures. This process of feature selection focuses on achieving two discrete objectives: (1) identification of a large number of Pareto-optimal solutions along with achieving a good distribution in both objective and decision spaces; (2) making a selection of feature subset with minimal redundancy and high correlation with classes. To achieve the second objective, a variety of objective functions based on information-theoretic measures like normalized mutual information and correlation with class attributes are utilized. A multiobjective ring-based Particle swarm Optimization (PSO) and non-dominated sorting with special crowding distance are employed to cover the aspects corresponding to the first objective. An evaluation is carried out on seven publicly available datasets concerning different classifiers. The results of these experiments illustrate that the multimodal PSO based feature selection approach finds more feature subsets than its simple PSO counterpart in multiobjective environment. And, the results are also compared with those of existing wrapper based multimodal multiobjective feature selection methods.},
  archive      = {J_ASOC},
  author       = {Kanchan Jha and Sriparna Saha},
  doi          = {10.1016/j.asoc.2020.106823},
  journal      = {Applied Soft Computing},
  pages        = {106823},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incorporation of multimodal multiobjective optimization in designing a filter based feature selection technique},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cycle-consistent GAN-based stain translation of renal
pathology images with glomerulus detection application. <em>ASOC</em>,
<em>98</em>, 106822. (<a
href="https://doi.org/10.1016/j.asoc.2020.106822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivation: Renal biopsy is an irreplaceable diagnostic tool for kidney diseases. Glomeruli provide important information for an accurate disease diagnosis. This paper applies deep learning techniques to automate translation of renal pathology images and glomerulus detection to improve the efficiency and accuracy on pathological diagnoses. Methods: This paper first proposes a new method for automatic translation of different renal pathology staining styles using the cycle-consistent Generative Adversarial Network (GAN). This paper then proposes the combination of faster region-based convolutional neural network (R-CNN) with an aspect ratio filter to detect glomeruli in light microscopy images processed with four different stains at various optical magnifications. Finally, this paper improves glomerulus detection at different stains by using translated image stains from the CycleGAN. Results: To show the effectiveness of the translation and detection methods, in addition to quantitative analysis of the results, the involvement of assessment from four physicians is also performed. Experimental results show that the physicians fail to differentiate real and translated stains and the automatic glomerulus detection method outperforms that manually labeled by the physicians. Conclusion: The proposed method works well and improves the efficiency of renal pathological diagnosis. This work contributes in the area of automated medical diagnosis.},
  archive      = {J_ASOC},
  author       = {Ying-Chih Lo and I-Fang Chung and Shin-Ning Guo and Mei-Chin Wen and Chia-Feng Juang},
  doi          = {10.1016/j.asoc.2020.106822},
  journal      = {Applied Soft Computing},
  pages        = {106822},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cycle-consistent GAN-based stain translation of renal pathology images with glomerulus detection application},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corrigendum to “maintaining filter structure: A gabor-based
convolutional neural network for image analysis” [appl. Soft comput. 88
(2020) 105960]. <em>ASOC</em>, <em>98</em>, 106820. (<a
href="https://doi.org/10.1016/j.asoc.2020.106820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Somayeh Molaei and Mohammad Ebrahim Shiri Ahmad Abadi},
  doi          = {10.1016/j.asoc.2020.106820},
  journal      = {Applied Soft Computing},
  pages        = {106820},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Maintaining filter structure: A gabor-based convolutional neural network for image analysis” [Appl. soft comput. 88 (2020) 105960]},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid prediction strategy to predict agricultural
information. <em>ASOC</em>, <em>98</em>, 106811. (<a
href="https://doi.org/10.1016/j.asoc.2020.106811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crop yield prediction (CYP) has a high significance in agriculture. Early crop yield predictions assist the farmers, decision-makers in making timely decisions during the actual growing season. In many developing countries such as India, the process of crop yield prediction is done manually, based on surveys and field visits which are time-consuming, expensive and prone to human error. To overcome these drawbacks, we propose a hybrid prediction strategy which can be applied to predict agricultural information such as crop yield and air temperature with a critical focus on crop yield prediction. The weighted principal component analysis (w-PCA) is used as the feature extraction strategy to extract the relevant features. A hybrid prediction strategy integrating artificial neural network (ANN) with modified-particle swarm optimization (m-PSO) is proposed. Initial parameters of ANN are selected using m-PSO with modified inertia weight and velocity update equations. This hybridized ANN then performs prediction on the selected features. This proposed prediction model which we call as hybrid-ANN (H-ANN) comprises of w-PCA as feature extractor, m-PSO for selecting initial weights and biases of ANN. Experiments were performed on eight real world and two benchmark agriculture data sets for crop yield and air temperature prediction. Results show that the proposed prediction model (H-ANN) performed with improvements in the range of 2 to 30\%, 0.2 to 4\% and 0.12 to 3\% with respect to R-squared ( R 2 R2 ), root mean square error ( R M S E RMSE ) and mean absolute error ( M A E MAE ) respectively when compared to other prediction models such as ANN, ANN trained using GA (GA-ANN), ANN trained using standard PSO (SPSO-ANN), multiple linear regression (MLR), support vector regression (SVR) and ensemble of bagged regression trees (ET) on benchmark and real-world agricultural datasets.},
  archive      = {J_ASOC},
  author       = {K. Aditya Shastry and H.A. Sanjay},
  doi          = {10.1016/j.asoc.2020.106811},
  journal      = {Applied Soft Computing},
  pages        = {106811},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid prediction strategy to predict agricultural information},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting stock price trends based on financial news
articles and using a novel twin support vector machine with fuzzy
hyperplane. <em>ASOC</em>, <em>98</em>, 106806. (<a
href="https://doi.org/10.1016/j.asoc.2020.106806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study extracts the hidden topic model and emotional information from news articles. A novel fuzzy twin support vector machine is also developed to merge the large volume of information from on-line news, using this to predict stock price trends. Fuzzy set theory is very useful for this approach because the texts use fuzzy terminology, such as high/low and big/small, and the boundary between rising and falling categories is poorly defined. By using fuzzy partial ordering relation, the decision function of our approach is generalized such that the values assigned to the stocks fall within a specified range and indicate the membership grade of these stocks in the positive class (rising trends). The proposed approach is useful for dealing with outliers. Outliers only slightly increase the spread of the fuzzy decision boundary and the estimation of the mode of fuzzy hyperplane remains unchanged. Therefore, our model is more robust compared with other methods when the data contain some outliers. Besides, the membership grade estimated by the fuzzy decision boundary gives a better insight into the degree of confidence in the predicted outputs. The explicable characteristic for the degree of confidence is a vital aspect for decision-making applications.},
  archive      = {J_ASOC},
  author       = {Pei-Yi Hao and Chien-Feng Kung and Chun-Yang Chang and Jen-Bing Ou},
  doi          = {10.1016/j.asoc.2020.106806},
  journal      = {Applied Soft Computing},
  pages        = {106806},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting stock price trends based on financial news articles and using a novel twin support vector machine with fuzzy hyperplane},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature subset selection via an improved
discretization-based particle swarm optimization. <em>ASOC</em>,
<em>98</em>, 106794. (<a
href="https://doi.org/10.1016/j.asoc.2020.106794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data analysis has attracted increasingly attention in machine learning or data mining tasks . Due to the existence of irrelevant and redundant features, classification accuracy is often degraded seriously. Feature selection (FS), which aims to improve the predictive accuracy by selecting a subset of features, plays a very important role. In this paper, we proposed an improved discretization-based particle swarm optimization (PSO) for FS. In our method, we applied a moderate pre-screening process to obtain a reduced size of features at first. Then, a ranking-based cut-point table that stores multiple cut-points sorted by an entropy-based cut-point priority for each feature was obtained. To find the optimal combination of the cut-points that could best distinguish the data samples, a simple yet efficient encoding and decoding approach in PSO was used to select a flexible number of cut-points. Moreover, a probability-guided local search strategy was applied to search for better combination of cut-points to achieve promising feature subset. Comprehensive simulation results on 19 benchmark datasets demonstrate the effectiveness of several improved strategies in our proposed method and the advantages of our proposed method over some state-of-the-art PSO-based competitors.},
  archive      = {J_ASOC},
  author       = {Yu Zhou and Jiping Lin and Hainan Guo},
  doi          = {10.1016/j.asoc.2020.106794},
  journal      = {Applied Soft Computing},
  pages        = {106794},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature subset selection via an improved discretization-based particle swarm optimization},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic coloring method for ethnic costume sketches
based on generative adversarial networks. <em>ASOC</em>, <em>98</em>,
106786. (<a href="https://doi.org/10.1016/j.asoc.2020.106786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an automatic coloring model for ethnic costume sketches based on a generative adversarial networks . The proposed model is composed of a 6-layer U-net structure generator and a discriminator with 5-layer convolutional neural network . Then the loss function and the weights of true reliability value in the discriminator are optimized. And, finally the constructed sketch database is used for training to get an automatic coloring model. The results of a large number of sketch coloring experiments show that the model has good learning ability on exploring color law of ethnic costumes and can achieve better coloring effects.},
  archive      = {J_ASOC},
  author       = {Bo Liu and Jianhou Gan and Bin Wen and Yiping LiuFu and Wei Gao},
  doi          = {10.1016/j.asoc.2020.106786},
  journal      = {Applied Soft Computing},
  pages        = {106786},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An automatic coloring method for ethnic costume sketches based on generative adversarial networks},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A linguistic distribution behavioral multi-criteria group
decision making model integrating extended generalized TODIM and quantum
decision theory. <em>ASOC</em>, <em>98</em>, 106757. (<a
href="https://doi.org/10.1016/j.asoc.2020.106757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most existing multi-criteria group decision making (MCGDM) problems, the decision makers (DMs) are usually regarded as independent and DMs’ psychological behaviors are rarely considered. However, in many cases, the opinions of DMs are likely to influence each other and the DMs often hold subjective bounded rationality in the decision making process. To address this issue, a linguistic distribution behavioral MCGDM model integrating extended generalized TODIM (an acronym in Portuguese of interactive and multi-criteria decision making) method and quantum decision theory is developed. First, linguistic distribution assessments considering the sample size information are used to gather the group linguistic evaluations, which can integrally depict the quantitative distribution and qualitative vagueness. Second, an extended generalized linguistic distribution TODIM method is presented to reflect the DMs’ bounded rational behaviors. It is used to calculate the dominance of each alternative. Third, a quantum possibilistic aggregation framework is constructed to explore the interference effects among DMs’ opinions. In this process, DMs’ opinions are viewed as synchronously occurred wave functions that interfere with each other and influence the aggregated result. Finally, a case study is examined to recommend the optimal automobile for customers based on distributed online reviews in newcar.xcar.com and autohome.com. Three separate scenarios with conjunctive, neural, and disjunctive modes are considered respectively. The sensitivity analysis is given to show the effect of risk factors and interference factors. The parameters show to be dominant in the final best automobile recommendation. The comparisons with some existing methods confirm the validity and stability of the proposed MCGDM method.},
  archive      = {J_ASOC},
  author       = {Qun Wu and Xinwang Liu and Jindong Qin and Weizhong Wang and Ligang Zhou},
  doi          = {10.1016/j.asoc.2020.106757},
  journal      = {Applied Soft Computing},
  pages        = {106757},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A linguistic distribution behavioral multi-criteria group decision making model integrating extended generalized TODIM and quantum decision theory},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimized deep learning architecture for the diagnosis of
COVID-19 disease based on gravitational search optimization.
<em>ASOC</em>, <em>98</em>, 106742. (<a
href="https://doi.org/10.1016/j.asoc.2020.106742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel approach called GSA-DenseNet121-COVID-19 based on a hybrid convolutional neural network (CNN) architecture is proposed using an optimization algorithm . The CNN architecture that was used is called DenseNet121, and the optimization algorithm that was used is called the gravitational search algorithm (GSA). The GSA is used to determine the best values for the hyperparameters of the DenseNet121 architecture. To help this architecture to achieve a high level of accuracy in diagnosing COVID-19 through chest x-ray images. The obtained results showed that the proposed approach could classify 98.38\% of the test set correctly. To test the efficacy of the GSA in setting the optimum values for the hyperparameters of DenseNet121. The GSA was compared to another approach called SSD-DenseNet121, which depends on the DenseNet121 and the optimization algorithm called social ski driver (SSD). The comparison results demonstrated the efficacy of the proposed GSA-DenseNet121-COVID-19. As it was able to diagnose COVID-19 better than SSD-DenseNet121 as the second was able to diagnose only 94\% of the test set. The proposed approach was also compared to another method based on a CNN architecture called Inception-v3 and manual search to quantify hyperparameter values. The comparison results showed that the GSA-DenseNet121-COVID-19 was able to beat the comparison method, as the second was able to classify only 95\% of the test set samples. The proposed GSA-DenseNet121-COVID-19 was also compared with some related work. The comparison results showed that GSA-DenseNet121-COVID-19 is very competitive.},
  archive      = {J_ASOC},
  author       = {Dalia Ezzat and Aboul Ella Hassanien and Hassan Aboul Ella},
  doi          = {10.1016/j.asoc.2020.106742},
  journal      = {Applied Soft Computing},
  pages        = {106742},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized deep learning architecture for the diagnosis of COVID-19 disease based on gravitational search optimization},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal microgrid’s protection coordination considering n-1
contingency and optimum relay characteristics. <em>ASOC</em>,
<em>98</em>, 106741. (<a
href="https://doi.org/10.1016/j.asoc.2020.106741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a knowledge gap about the development of optimal coordination of microgrids , which considers all N-1 contingencies by using the smart selection of standard relay characteristics. This paper tries to fill such a knowledge gap by contributing to introduce a novel method to optimize the coordination of microgrids’ directional overcurrent relays (DOCRs), which smartly selects the time-current characteristic of relays and considers all system topologies. Achieving an optimal protection scheme of microgrids without any selectivity constraint under various topologies is the main purpose of this research. The proposed objective function of total operating time of DOCRs under various system topologies is linearized as the TDSs. The hybrid heuristic-linear programming algorithms (HHLPAs) are used to solve the mixed-integer non-linear programming (MINLP) problem. The decrease in the number of heuristic algorithm’s decision variables improves the performance of the proposed HHLPAs. The soft computing-based comparison of the hybrid genetic algorithm-linear programming (GA-LP) and the hybrid particle swarm optimization-linear programming (PSO-LP) is another contribution of this paper. About 80\% decrease in the DOCRs’ operating time has been achieved by applying the proposed smart selection of standard relay characteristics (normally inverse, very inverse, and extremely inverse) in comparison to use of just normally inverse curve based on existing methods. The satisfaction of coordination constraints of optimum relay settings is validated based on the DIgSILENT protection simulations.},
  archive      = {J_ASOC},
  author       = {Amir Mohammad Entekhabi-Nooshabadi and Hamed Hashemi-Dezaki and Seyed Abbas Taher},
  doi          = {10.1016/j.asoc.2020.106741},
  journal      = {Applied Soft Computing},
  pages        = {106741},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal microgrid’s protection coordination considering N-1 contingency and optimum relay characteristics},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerator for crosswise computing reduct. <em>ASOC</em>,
<em>98</em>, 106740. (<a
href="https://doi.org/10.1016/j.asoc.2020.106740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction , as a technique for selecting qualified attributes which can satisfy the intended constraint related to considered measure, has been widely explored. Notably, one and only one reduct is derived through using one searching strategy in most cases. Nevertheless, only one reduct may be not enough for us to evaluate its effectiveness. To fill such gap, an approach of crosswise computing reduct is proposed for obtaining multiple reducts. The computation of reduct is realized through partitioning the whole data into several groups, and crosswise selecting some groups to form different subsets of data, then computing reducts over these different subsets of data. Moreover, to speed up the process of crosswise computing reduct, an acceleration strategy is designed. The main thinking of our acceleration strategy is to compute the reduct over different subsets of data on the basis of reduct over the whole data. The experimental results over 16 data sets show the following superiorities of our strategy: (1) our approach can decrease the elapsed time of crosswise computing reducts significantly; (2) our approach can not only provide reduct with higher stability, but also maintain the classification performance; (3) the attributes in reduct can provide more stable classification results .},
  archive      = {J_ASOC},
  author       = {Zehua Jiang and Keyu Liu and Jingjing Song and Xibei Yang and Jinhai Li and Yuhua Qian},
  doi          = {10.1016/j.asoc.2020.106740},
  journal      = {Applied Soft Computing},
  pages        = {106740},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Accelerator for crosswise computing reduct},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Balancing and sequencing problem of mixed-model u-shaped
robotic assembly line: Mathematical model and dragonfly algorithm based
approach. <em>ASOC</em>, <em>98</em>, 106739. (<a
href="https://doi.org/10.1016/j.asoc.2020.106739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic assembly line outperforms labor assembly line in terms of efficiency and flexibility. This paper focuses on the mixed-model U-shaped robotic assembly line balancing and sequencing problem (MURALBSP). Different from most reported works, this paper includes the energy consideration. Two conflict objectives, i.e., energy consumption and makespan, are studied for energy saving and efficient production. To this end, a hybrid multi-objective dragonfly algorithm (HMODA) is proposed. First, the mathematical model of this bi-objective problem is formulated. Second, the basic dragonfly algorithm is improved to solve the problem. The specific encoding and decoding method is designed and the chaotic map is used to improve algorithm randomness. Besides, the solution update method is amended and multi-point crossover mechanism is employed. Finally, several multiple size benchmark problems are designed and comparisons are conducted, the sensitivity of decision variables is analyzed to provide managerial insights. The results suggest that HMODA is more efficient in solving the proposed problem than compared algorithms.},
  archive      = {J_ASOC},
  author       = {Beikun Zhang and Liyun Xu and Jian Zhang},
  doi          = {10.1016/j.asoc.2020.106739},
  journal      = {Applied Soft Computing},
  pages        = {106739},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Balancing and sequencing problem of mixed-model U-shaped robotic assembly line: Mathematical model and dragonfly algorithm based approach},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparative study of statistical and soft computing
techniques for reliability prediction of automotive manufacturing.
<em>ASOC</em>, <em>98</em>, 106738. (<a
href="https://doi.org/10.1016/j.asoc.2020.106738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability and safety analyses are the most important activities for reducing risk of failure events and upgrading availability of manufacturing industries. The traditional statistical models have been currently used; however, the complexity growth and diversity of systems as well as uncertainty of their functions result in extreme difficulties in analyzing the reliability by such models. To overcome such drawbacks, the soft computing techniques are useful alternative for modeling of complex systems and prediction applications. Hence, this paper provides a comparative structure for predicting the operational reliability in automotive manufacturing industry, using soft computing + statistical techniques. The results of comparative structure revealed that the soft computing techniques can estimate the reliability function with the lowest error in all cases. Based on the performance criteria, it was observed that among the soft computing techniques, the Adaptive Neuro-Fuzzy Inference System (ANFIS) model yields better results in most cases and thus can be used for predicting operational reliability, since it predicts the reliability more accurately and precisely than the statistical models. Ultimately, the maintenance intervals based on the ANFIS model are proposed to upgrade the reliability and safety of automotive manufacturing process .},
  archive      = {J_ASOC},
  author       = {Hamzeh Soltanali and Abbas Rohani and Mohammad Hossein Abbaspour-Fard and José Torres Farinha},
  doi          = {10.1016/j.asoc.2020.106738},
  journal      = {Applied Soft Computing},
  pages        = {106738},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comparative study of statistical and soft computing techniques for reliability prediction of automotive manufacturing},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A variable neighborhood search algorithm for transshipment
scheduling of multi products at a single station. <em>ASOC</em>,
<em>98</em>, 106736. (<a
href="https://doi.org/10.1016/j.asoc.2020.106736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we investigate a transshipment scheduling problem in which a set of loading and unloading jobs must be performed at a single station of a transshipment terminal. We assume that there is a set of product types and an inventory storage center with a constant capacity for each. Each loading job decreases the inventory level of a particular product type and has a predetermined due date, while each unloading job increases the inventory level of a product type and has a given release date. An inbound truck delivers a set of products, which have the same release dates and must be unloaded, to the inventory storage center. In contrast, an outbound truck conveys a set of goods, which have identical completion times and must be loaded. We aim to minimize the total freight cost of trucks as well as the total weighted tardiness of products. To this end, we develop a linear integer programming model and a variable neighborhood search algorithm including a set of efficient local search procedures. Moreover, we run the two previously developed metaheuristics , i.e. a genetic algorithm and a particle swarm algorithm so as to solve the problem. Using a set of randomly generated test instances, we perform extensive computational experiments and statistical analyses to highlight the efficiency and superiority of the developed algorithm.},
  archive      = {J_ASOC},
  author       = {Mohammad Ranjbar and Reza Ghorbani Saber},
  doi          = {10.1016/j.asoc.2020.106736},
  journal      = {Applied Soft Computing},
  pages        = {106736},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A variable neighborhood search algorithm for transshipment scheduling of multi products at a single station},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance analysis of nonlinear activated zeroing neural
networks for time-varying matrix pseudoinversion with application.
<em>ASOC</em>, <em>98</em>, 106735. (<a
href="https://doi.org/10.1016/j.asoc.2020.106735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By exploiting two simplified nonlinear activation functions , two zeroing neural network (ZNN) models are designed and studied to efficiently tackle the time-varying matrix pseudoinversion problem. Compared with ZNN activated by previously presented activation functions , these two simplified finite-time ZNN (SFTZNN) models (called SFTZNN1 and SFTZNN2) not only achieve faster finite-time convergence, but also possess better robustness. In addition, the SFTZNN1 and SFTZNN2 models have simpler structure compared with the widely used sign-bi-power activated ZNN model. Theoretical analysis is presented to obtain the maximum convergence time for the SFTZNN models in ideal conditions. Besides, when external perturbations are injected into the proposed SFTZNN models, upper bounds of the steady-state residual error are theoretically calculated. Comparative simulations and one engineering application case validate the feasibility and superiority of the two new SFTZNN models when solving time-varying matrix pseudoinversion.},
  archive      = {J_ASOC},
  author       = {Zeshan Hu and Lin Xiao and Kenli Li and Keqin Li and Jichun Li},
  doi          = {10.1016/j.asoc.2020.106735},
  journal      = {Applied Soft Computing},
  pages        = {106735},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Performance analysis of nonlinear activated zeroing neural networks for time-varying matrix pseudoinversion with application},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperation search algorithm: A novel metaheuristic
evolutionary intelligence algorithm for numerical optimization and
engineering optimization problems. <em>ASOC</em>, <em>98</em>, 106734.
(<a href="https://doi.org/10.1016/j.asoc.2020.106734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a novel population-based evolutionary method called cooperation search algorithm (CSA) to address the complex global optimization problem . Inspired by the team cooperation behaviors in modern enterprise, the CSA method randomly generates a set of candidate solutions in the problem space, and then three operators are repeatedly executed until the stopping criterion is met: the team communication operator is used to improve the global exploration and determine the promising search area; the reflective learning operator is used to achieve a comprise between exploration and exploitation; the internal competition operator is used to choose solutions with better performances for the next cycle. Firstly, three kinds of mathematical optimization problems (including 24 famous test functions, 25 CEC2005 test problems and 30 CEC2014 test problems) are used to test the convergence speed and search accuracy of the CSA method. Then, several famous engineering optimization problems (like Gear train design, Welded beam design and Speed reducer design) are chosen to testify the engineering practicality of the CSA method. The results in different scenarios demonstrate that as compared with several existing evolutionary algorithms , the CSA method can effectively explore the decision space and produce competitive results in terms of various performance evaluation indicators. Thus, an effective tool is provided for solving the complex global optimization problems.},
  archive      = {J_ASOC},
  author       = {Zhong-kai Feng and Wen-jing Niu and Shuai Liu},
  doi          = {10.1016/j.asoc.2020.106734},
  journal      = {Applied Soft Computing},
  pages        = {106734},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperation search algorithm: A novel metaheuristic evolutionary intelligence algorithm for numerical optimization and engineering optimization problems},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A clustering-based differential evolution with different
crowding factors for nonlinear equations system. <em>ASOC</em>,
<em>98</em>, 106733. (<a
href="https://doi.org/10.1016/j.asoc.2020.106733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving nonlinear equations systems (NESs) is one of the most important tasks in numerical computation. It is common that most NESs contain more than one root. Generally, these roots are equally important. Therefore, locating as many of these roots as possible is extremely useful; however, it is a difficult task. Recently, the use of evolutionary algorithms (EAs) for NESs has been given more consideration. There are several issues that need to be considered when applying EAs to solve NESs: (1) How to guide the population towards multiple roots, (2) how to tackle individuals which trip into local optimum, and (3) how to minimize replacement errors. In this paper, we deal with the first issue by introducing a one-step k-means clustering method combined with niching. In this way, individuals will search for different roots in their respective directions. For the second issue, we propose two methods to form species, the goal of which is to promote individuals to get rid of local optima. Finally, different crowding factors are adopted to reduce replacement errors. By assembling these improvements, a one-step k-means clustering based differential evolution, namely KSDE, is proposed. To evaluate the effectiveness of the proposal, we use 30 NES problems from the literature as the test suite. Experimental results demonstrate KSDE has great potential to locate multiple roots in a single run. Furthermore, according to the evaluation criteria, KSDE shows better performance compared with other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Jianye Wu and Wenyin Gong and Ling Wang},
  doi          = {10.1016/j.asoc.2020.106733},
  journal      = {Applied Soft Computing},
  pages        = {106733},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A clustering-based differential evolution with different crowding factors for nonlinear equations system},
  volume       = {98},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
