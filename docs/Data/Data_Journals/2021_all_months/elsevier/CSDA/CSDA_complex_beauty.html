<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSDA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="csda---166">CSDA - 166</h2>
<ul>
<li><details>
<summary>
(2021). Inference for partially observed epidemic dynamics guided by
kalman filtering techniques. <em>CSDA</em>, <em>164</em>, 107319. (<a
href="https://doi.org/10.1016/j.csda.2021.107319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent development of methods dealing with partially observed epidemic dynamics (unobserved model coordinates, discrete and noisy outbreak data), limitations remain in practice, mainly related to the quantity of augmented data and calibration of numerous tuning parameters. In particular, as coordinates of dynamic epidemic models are coupled, the presence of unobserved coordinates leads to a statistically difficult problem. The aim is to propose an easy-to-use and general inference method that is able to tackle these issues. First, using the properties of epidemics in large populations, a two-layer model is constructed. Via a diffusion-based approach, a Gaussian approximation of the epidemic density-dependent Markovian jump process is obtained, representing the state model. The observational model, consisting of noisy observations of certain model coordinates, is approximated by Gaussian distributions . Then, an inference method based on an approximate likelihood using Kalman filtering recursion is developed to estimate parameters of both the state and observational models. The performance of estimators of key model parameters is assessed on simulated data of SIR epidemic dynamics for different scenarios with respect to the population size and the number of observations. This performance is compared with that obtained using the well-known maximum iterated filtering method. Finally, the inference method is applied to a real data set on an influenza outbreak in a British boarding school in 1978.},
  archive      = {J_CSDA},
  author       = {Romain Narci and Maud Delattre and Catherine Larédo and Elisabeta Vergu},
  doi          = {10.1016/j.csda.2021.107319},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107319},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Inference for partially observed epidemic dynamics guided by kalman filtering techniques},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A more powerful test of equality of high-dimensional
two-sample means. <em>CSDA</em>, <em>164</em>, 107318. (<a
href="https://doi.org/10.1016/j.csda.2021.107318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new test is proposed for testing the equality of two sample means in high dimensional data in which the sample sizes may be much less than the dimension. The test is constructed based on a studentized average of squared component-wise t-statistics. Asymptotic normality of the test statistic was derived under H 0 . Theoretical properties of the power function were given under local alternatives. The new test has much better type I error control and power compared to a similarly constructed competing test in recent literature as a result of a more efficient scaling parameter estimate in the test statistic. Monte Carlo experiments show that the new test outperforms several popular competing tests under various data settings, especially when components of the data vector have high correlations. The results are established under the condition that there exists a permutation of the component indices such that the correlation decays suitably fast (at least with polynomial rate). The test is further evaluated with a real-data task of identifying differently expressed Gene Ontology terms with the acute lymphoblastic leukemia gene expression data . The new test provides more consistent results on random samples of the dataset.},
  archive      = {J_CSDA},
  author       = {Huaiyu Zhang and Haiyan Wang},
  doi          = {10.1016/j.csda.2021.107318},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107318},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A more powerful test of equality of high-dimensional two-sample means},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Outlier detection in networks with missing links.
<em>CSDA</em>, <em>164</em>, 107308. (<a
href="https://doi.org/10.1016/j.csda.2021.107308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers arise in networks due to different reasons such as fraudulent behaviour of malicious users or default in measurement instruments and can significantly impair network analyses. In addition, real-life networks are likely to be incompletely observed, with missing links due to individual non-response or machine failures. Therefore, identifying outliers in the presence of missing links is a crucial problem in network analysis . A new algorithm is introduced to detect outliers in a network and simultaneously predict the missing links. The proposed method is statistically sound: under fairly general assumptions, this algorithm exactly detects the outliers, and achieves the best known error for the prediction of missing links with polynomial computational cost. The sub-linear convergence of the algorithm is proven, which confirms its computational efficiency. A simulation study is provided, which demonstrates the good behaviour of the algorithm in terms of outlier detection and prediction of the missing links. The method is also illustrated with an application in epidemiology and with the analysis of a political Twitter network. The algorithm is freely available as an R package on the Comprehensive R Archive Network.},
  archive      = {J_CSDA},
  author       = {Solenne Gaucher and Olga Klopp and Geneviève Robin},
  doi          = {10.1016/j.csda.2021.107308},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107308},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Outlier detection in networks with missing links},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlation for tree-shaped datasets and its bayesian
estimation. <em>CSDA</em>, <em>164</em>, 107307. (<a
href="https://doi.org/10.1016/j.csda.2021.107307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-shaped datasets have arisen in various research and industrial fields, such as gene expression data measured on a cell lineage tree and information spreading on tree-shaped paths. Certain correlation measure between two tree-shaped datasets, i.e., how the values increase or decrease together along corresponding paths of the two trees, is desired; but the tree topology prohibits the use of classical vector-based correlation measures such as Pearson correlation coefficient. To this end, a statistical framework for measuring such tree correlation is proposed. As a specific model in this framework, a parametric model based on bivariate Gaussian distributions is provided, and a Bayesian approach for parameter estimation is introduced. The model allows the coupling degree of corresponding nodes to change with the depth of the tree. It provides an intuitive mapping of the trend similarity of the values along two trees to the classical Pearson correlation. A Metropolis-within-Gibbs algorithm is used to obtain the posterior estimates. Extensive simulations and in-depth sensitivity analyses are performed to demonstrate the validity and robustness of the method. Furthermore, an application to embryonic gene expression datasets shows that this tree similarity measure aligns well with the biological properties.},
  archive      = {J_CSDA},
  author       = {Shanjun Mao and Xiaodan Fan and Jie Hu},
  doi          = {10.1016/j.csda.2021.107307},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107307},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Correlation for tree-shaped datasets and its bayesian estimation},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric least-squares regression with doubly-censored
data. <em>CSDA</em>, <em>164</em>, 107306. (<a
href="https://doi.org/10.1016/j.csda.2021.107306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Double censoring often occurs in biomedical research, such as HIV/AIDS clinical trials, when an outcome of interest is subject to both left censoring and right censoring. It can also be seen as a mixture of exact and current status data and has long been investigated by several authors for theoretical and practical purposes. In this article, we propose the Buckley-James method for an accelerated failure time model under double random censoring. For the semiparametric inference, where the error distribution of the censored linear model is left unspecified, we develop an efficient EM-based self-consistency procedure to estimate the regression parameter and the unknown residual distribution function. Asymptotic properties , including the uniform consistency and weak convergence, are established for the proposed estimators. Simulation studies demonstrate that the proposed procedure works well under various censoring schemes and outperforms the inverse-probability weighting method in terms of accuracy and efficiency. The method is applied to the HIV/AIDS study.},
  archive      = {J_CSDA},
  author       = {Taehwa Choi and Arlene K.H. Kim and Sangbum Choi},
  doi          = {10.1016/j.csda.2021.107306},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107306},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric least-squares regression with doubly-censored data},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On efficient exact experimental designs for ordered
treatments. <em>CSDA</em>, <em>164</em>, 107305. (<a
href="https://doi.org/10.1016/j.csda.2021.107305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent paper Singh and Davidov (2019) derive approximate optimal designs for experiments with ordered treatments. Specifically, maxi–min and intersection–union designs were explored. These designs, which address different types of hypothesis testing problems , provide a substantial improvement over standard designs in terms of power, or equivalently, sample size requirements . In practice however, exact, not approximate designs are used. Therefore, in this paper, we develop methods for finding efficient exact designs for the testing problems considered in Singh and Davidov (2019) . The proposed designs are compared numerically to some well known existing designs and it is shown that the new designs require fewer experimental units to attain prespecified power. A thorough sensitivity analysis shows that the proposed designs are robust against possible misspecification of the parameters under the alternative and the order relation among the treatment groups.},
  archive      = {J_CSDA},
  author       = {Satya Prakash Singh and Ori Davidov},
  doi          = {10.1016/j.csda.2021.107305},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107305},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On efficient exact experimental designs for ordered treatments},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Equivalence class selection of categorical graphical models.
<em>CSDA</em>, <em>164</em>, 107304. (<a
href="https://doi.org/10.1016/j.csda.2021.107304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the structure of dependence relations between variables is a pervasive issue in the statistical literature. A directed acyclic graph (DAG) can represent a set of conditional independencies , but different DAGs may encode the same set of relations and are indistinguishable using observational data . Equivalent DAGs can be collected into classes, each represented by a partially directed graph known as essential graph (EG). Structure learning directly conducted on the EG space , rather than on the allied space of DAGs, leads to theoretical and computational benefits. Still, the majority of efforts has been dedicated to Gaussian data, with less attention to methods designed for multivariate categorical data . A Bayesian methodology for structure learning of categorical EGs is then proposed. Combining a constructive parameter prior elicitation with a graph-driven likelihood decomposition, a closed-form expression for the marginal likelihood of a categorical EG model is derived. Asymptotic properties are studied, and an MCMC sampler scheme developed for approximate posterior inference. The methodology is evaluated on both simulated scenarios and real data, with appreciable performance in comparison with state-of-the-art methods.},
  archive      = {J_CSDA},
  author       = {Federico Castelletti and Stefano Peluso},
  doi          = {10.1016/j.csda.2021.107304},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107304},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Equivalence class selection of categorical graphical models},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph informed sliced inverse regression. <em>CSDA</em>,
<em>164</em>, 107302. (<a
href="https://doi.org/10.1016/j.csda.2021.107302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new method is developed for performing sufficient dimension reduction when probabilistic graphical models are being used to estimate parameters. The procedure enriches the domain of application of dimension reduction techniques to settings where (i) p the number of variables in the model is much larger than the available sample size n , (ii) p is much larger than the number of slices H the model uses and (iii) D the number of projection vectors can be larger than the number of slices H . The methodology is developed for the case of the sliced inverse regression model, but extensions to other dimension reduction techniques such as sliced average variance estimation or other methods are straightforward.},
  archive      = {J_CSDA},
  author       = {Eugen Pircalabelu and Andreas Artemiou},
  doi          = {10.1016/j.csda.2021.107302},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107302},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Graph informed sliced inverse regression},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Projection-averaging-based cumulative covariance and its use
in goodness-of-fit testing for single-index models. <em>CSDA</em>,
<em>164</em>, 107301. (<a
href="https://doi.org/10.1016/j.csda.2021.107301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A projection-averaging-based cumulative divergence to characterize the conditional mean independence is proposed. As a natural extension of Zhou et al. (2020) , the new metric has several appealing features. It ranges from zero to one, and equals zero if and only if the conditional mean independence holds. It has an elegant closed-form expression that involves no tuning parameters, making it easy to implement. The sample estimator of new metric is n -consistent under the conditional mean independence and root- n -consistent otherwise. A goodness-of-fit test for single-index models based on the variant of the proposed metric is further introduced, which generalizes the projected-based test of Escanciano (2006) to a semiparametric regression setting that allows an unspecified link function. The proposed test is consistent against any global alternatives and can detect the local alternatives distinct from the null at the parametric rate of O ( n − 1 / 2 ) . The effectiveness of our proposals is demonstrated through simulation examples and a real application.},
  archive      = {J_CSDA},
  author       = {Kai Xu and Yeqing Zhou},
  doi          = {10.1016/j.csda.2021.107301},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107301},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Projection-averaging-based cumulative covariance and its use in goodness-of-fit testing for single-index models},
  volume       = {164},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covariate balancing functional propensity score for
functional treatments in cross-sectional observational studies.
<em>CSDA</em>, <em>163</em>, 107303. (<a
href="https://doi.org/10.1016/j.csda.2021.107303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis , which handles data arising from curves, surfaces, volumes, manifolds and beyond in a variety of scientific fields, is a rapidly developing area in modern statistics and data science in the recent decades. The effect of a functional variable on an outcome is an essential theme in functional data analysis, but a majority of related studies are restricted to correlational effects rather than causal effects . As the first attempt in the literature, the causal effect is studied for a functional variable as a treatment in cross-sectional observational studies. Despite the lack of a probability density function for the functional treatment, the propensity score is properly defined in terms of its top functional principal component scores which can represent the functional treatment approximately. Two covariate balancing methods are proposed to estimate the propensity score, which minimize the correlation between the treatment and covariates. The appealing performance of the proposed method in both covariate balance and causal effect estimation is demonstrated by a simulation study. The proposed method is applied to study the causal effect of body shape on human visceral adipose tissue.},
  archive      = {J_CSDA},
  author       = {Xiaoke Zhang and Wu Xue and Qiyue Wang},
  doi          = {10.1016/j.csda.2021.107303},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107303},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Covariate balancing functional propensity score for functional treatments in cross-sectional observational studies},
  volume       = {163},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active set algorithms for estimating shape-constrained
density ratios. <em>CSDA</em>, <em>163</em>, 107300. (<a
href="https://doi.org/10.1016/j.csda.2021.107300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many instances, imposing a constraint on the shape of a density is a reasonable and flexible assumption. It offers an alternative to parametric models , which can be too rigid, and to other nonparametric methods , which require the choice of tuning parameters. The nonparametric estimation of log-concave or log-convex density ratios is treated by means of active set algorithms in a unified framework. In the setting of log-concave densities, the new algorithm is similar to, but substantially faster than, previously considered active set methods. Log-convexity, on the other hand, is a less common shape-constraint, described by some authors as “tail inflation”. The active set method proposed here is novel in this context. As a by-product, new goodness-of-fit tests of single hypotheses are formulated and are shown to be more powerful than higher criticism tests in a simulation study.},
  archive      = {J_CSDA},
  author       = {Lutz Dümbgen and Alexandre Mösching and Christof Strähl},
  doi          = {10.1016/j.csda.2021.107300},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107300},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Active set algorithms for estimating shape-constrained density ratios},
  volume       = {163},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Categorical CVA biplots. <em>CSDA</em>, <em>163</em>,
107299. (<a href="https://doi.org/10.1016/j.csda.2021.107299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Techniques to visualise and understand large amounts of data are of paramount importance. In most settings, this data is usually multivariate, which further stresses the need for effective visualisation techniques. Multivariate visualisation techniques such as canonical variate analysis (CVA) biplots allow for simultaneous lower-dimensional visualisation and data classification by incorporating class-specific data. CVA biplots , however, are currently restricted to numerical data. Through combining concepts from both CVA and non-linear principal component analysis (PCA) biplots, a new biplot construction methodology that improves on the traditional CVA biplot by allowing for categorical variables is proposed. This technique, named CVA(H r ), is showcased using the established mushroom data set, which contains a mix of categorical and ordinal variables. This novel method improves upon existing biplot construction in terms of classification accuracy and class separation.},
  archive      = {J_CSDA},
  author       = {D.T. Rodwell and C.J. van der Merwe and S. Gardner-Lubbe},
  doi          = {10.1016/j.csda.2021.107299},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107299},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Categorical CVA biplots},
  volume       = {163},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal bayesian registration of noisy functions using
hamiltonian monte carlo. <em>CSDA</em>, <em>163</em>, 107298. (<a
href="https://doi.org/10.1016/j.csda.2021.107298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data registration is a necessary processing step for many applications. The observed data can be inherently noisy, often due to measurement error or natural process uncertainty; which most functional alignment methods cannot handle. A pair of functions can also have multiple optimal alignment solutions, which is not addressed in current literature. In this paper, a flexible Bayesian approach to functional alignment is presented, which appropriately accounts for noise in the data without any pre-smoothing required. Additionally, by running parallel MCMC chains, the method can account for multiple optimal alignments via the multi-modal posterior distribution of the warping functions . To most efficiently sample the warping functions, the approach relies on a modification of the standard Hamiltonian Monte Carlo to be well-defined on the infinite-dimensional Hilbert space. This flexible Bayesian alignment method is applied to both simulated data and real data sets to show its efficiency in handling noisy functions and successfully accounting for multiple optimal alignments in the posterior; characterizing the uncertainty surrounding the warping functions.},
  archive      = {J_CSDA},
  author       = {J. Derek Tucker and Lyndsay Shand and Kenny Chowdhary},
  doi          = {10.1016/j.csda.2021.107298},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107298},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multimodal bayesian registration of noisy functions using hamiltonian monte carlo},
  volume       = {163},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing dynamic effects on a bayesian matrix-variate
dynamic linear model: An application to task-based fMRI data analysis.
<em>CSDA</em>, <em>163</em>, 107297. (<a
href="https://doi.org/10.1016/j.csda.2021.107297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A modeling procedure for task-based functional magnetic resonance imaging (fMRI) data analysis using a Bayesian matrix-variate dynamic linear model (MVDLM) is presented. With this type of model, less complex than the more traditional temporal-spatial models, it is possible to take into account the temporal and, at least locally, the spatial structures that are usually present in this type of data. Thus, every voxel in the brain image is jointly modeled with its nearest neighbors, as defined by a Euclidean metric. MVDLM&#39;s have been widely used in applications where the interest lies in performing predictions and/or analysis of covariance structures among time series. However, in this context, the interest is rather to assess the dynamic effects related to voxel activation. In order to do so, two algorithms are developed and an already-existing one is adapted to simulate on-line trajectories related to the state parameter. With those curves or simulated trajectories, a Monte Carlo evidence for voxel activation is computed. Through two practical examples of auditory- and motor-cortex activations and two different types of assessments using resting-state and simulated fMRI data, it is shown that the proposed method can be viewed by practitioners as a reliable tool for task-based fMRI data analysis. Despite the assessments and applications being illustrated just for a single-subject analysis, a description is given of how general group analysis can be implemented, exemplified with a group analysis for 21 subjects.},
  archive      = {J_CSDA},
  author       = {Johnatan Cardona Jiménez and Carlos A. de B. Pereira},
  doi          = {10.1016/j.csda.2021.107297},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107297},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Assessing dynamic effects on a bayesian matrix-variate dynamic linear model: An application to task-based fMRI data analysis},
  volume       = {163},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature filter for estimating central mean subspace and its
sparse solution. <em>CSDA</em>, <em>163</em>, 107285. (<a
href="https://doi.org/10.1016/j.csda.2021.107285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction, replacing the original predictors with a few linear combinations while keeping all the regression information, has been widely studied. A key goal is to find the central mean subspace, the intersection of all subspaces that provide such a reduction. To this end, a new sufficient dimension reduction method is proposed, with two estimation procedures, through a novel approach of feature filter, applicable to both univariate and multivariate responses. Asymptotic results are established. Estimation methods to determine the structural dimension, to obtain sparse estimator and to deal with large p small n data are provided. The efficacy of the method is demonstrated by simulations and a real data example.},
  archive      = {J_CSDA},
  author       = {Pei Wang and Xiangrong Yin and Qingcong Yuan and Richard Kryscio},
  doi          = {10.1016/j.csda.2021.107285},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107285},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Feature filter for estimating central mean subspace and its sparse solution},
  volume       = {163},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-sample high dimensional mean test based on prepivots.
<em>CSDA</em>, <em>163</em>, 107284. (<a
href="https://doi.org/10.1016/j.csda.2021.107284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing equality of mean vectors is a very commonly used criterion when comparing two multivariate random variables . Traditional tests such as Hotelling&#39;s T 2 T2 become either unusable or output small power when the number of variables is greater than the combined sample size. A novel method is proposed using both prepivoting and Edgeworth expansion for testing the equality of two population mean vectors in a “large p , small n” setting. The asymptotic null distribution of the test statistic is derived and it is shown that the power of suggested test converges to one under certain alternatives when both n and p increase to infinity. Finite sample performance of the proposed test statistic is compared with other recently developed tests designed to also handle the “large p , small n ” situation through simulations. The proposed test achieves competitive rates for both type I error rate and power. The usefulness of suggested test is illustrated by applications to two microarray gene expression data sets .},
  archive      = {J_CSDA},
  author       = {Santu Ghosh and Deepak Nag Ayyala and Rafael Hellebuyck},
  doi          = {10.1016/j.csda.2021.107284},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107284},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Two-sample high dimensional mean test based on prepivots},
  volume       = {163},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing the effective sample size for large spatial
datasets: A block likelihood approach. <em>CSDA</em>, <em>162</em>,
107282. (<a href="https://doi.org/10.1016/j.csda.2021.107282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of new techniques for sample size reduction has attracted growing interest in recent decades. Recent findings allow us to quantify the amount of duplicated information within a sample of spatial data through the so-called effective sample size (ESS), whose definition arises from the Fisher information that is associated with maximum likelihood estimation. However, in all circumstances where the sample size is very large, maximum likelihood estimation and ESS evaluation are challenging from a computational viewpoint. An alternative definition of the ESS, in terms of the Godambe information from a block likelihood estimation approach, is presented. Several theoretical properties satisfied by this quantity are investigated. Our proposal is evaluated in some parametric correlation structures , including the intraclass, AR(1), Matérn, and simultaneous autoregressive models . Simulation experiments show that our proposal provides accurate approximations of the full likelihood-based ESS while maintaining a moderate computational cost. A large dataset is analyzed to quantify the effectiveness and limitations of the proposed framework in practice.},
  archive      = {J_CSDA},
  author       = {Jonathan Acosta and Alfredo Alegría and Felipe Osorio and Ronny Vallejos},
  doi          = {10.1016/j.csda.2021.107282},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107282},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Assessing the effective sample size for large spatial datasets: A block likelihood approach},
  volume       = {162},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast multivariate empirical cumulative distribution function
with connection to kernel density estimation. <em>CSDA</em>,
<em>162</em>, 107267. (<a
href="https://doi.org/10.1016/j.csda.2021.107267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of computing empirical cumulative distribution functions (ECDF) efficiently on large, multivariate datasets, is revisited. Computing an ECDF at one evaluation point requires O ( N ) O(N) operations on a dataset composed of N data points. Therefore, a direct evaluation of ECDFs at N evaluation points requires a quadratic O ( N 2 ) O(N2) operations, which is prohibitive for large-scale problems. Two fast and exact methods are proposed and compared. The first one is based on fast summation in lexicographical order , with a O ( N log ⁡ N ) O(Nlog⁡N) complexity and requires the evaluation points to lie on a regular grid. The second one is based on the divide-and-conquer principle, with a O ( N log ⁡ ( N ) ( d − 1 ) ∨ 1 ) O(Nlog⁡(N)(d−1)∨1) complexity and requires the evaluation points to coincide with the input points. The two fast algorithms are described and detailed in the general d -dimensional case, and numerical experiments validate their speed and accuracy. Secondly, a direct connection between cumulative distribution functions and kernel density estimation (KDE) is established for a large class of kernels. This connection paves the way for fast exact algorithms for multivariate kernel density estimation and kernel regression. Numerical tests with the Laplacian kernel validate the speed and accuracy of the proposed algorithms. A broad range of large-scale multivariate density estimation, cumulative distribution estimation, survival function estimation and regression problems can benefit from the proposed numerical methods.},
  archive      = {J_CSDA},
  author       = {Nicolas Langrené and Xavier Warin},
  doi          = {10.1016/j.csda.2021.107267},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107267},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast multivariate empirical cumulative distribution function with connection to kernel density estimation},
  volume       = {162},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fitting jump additive models. <em>CSDA</em>, <em>162</em>,
107266. (<a href="https://doi.org/10.1016/j.csda.2021.107266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jump regression analysis (JRA) provides a useful tool for estimating discontinuous functional relationships between a response and predictors. Most existing JRA methods consider the problems where there is only one or two predictors. It is unclear whether these methods can be directly extended to cases where there are multiple predictors . A jump additive model and a jump-preserving backfitting procedure are proposed. Jump additive models have the appeal that they make no restrictive parametric assumptions and allow possible discontinuities in the functional relationships, as with univariate JRA methods, but unlike them, jump additive models easily accommodate multiple predictors and the effects of individual predictors on the response can still be visually interpreted, regardless of the number of predictors. The proposed fitting procedure achieves the jump-preserving property by adaptively choosing, in each iteration of the backfitting algorithm , among two one-sided local linear estimates and a two-sided local linear estimate. Theoretical justifications and numerical studies show that it works well in applications. The procedure is also illustrated in analyzing a real data set .},
  archive      = {J_CSDA},
  author       = {Yicheng Kang and Yueyong Shi and Yuling Jiao and Wendong Li and Dongdong Xiang},
  doi          = {10.1016/j.csda.2021.107266},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107266},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fitting jump additive models},
  volume       = {162},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed one-step upgraded estimation for non-uniformly
and non-randomly distributed data. <em>CSDA</em>, <em>162</em>, 107265.
(<a href="https://doi.org/10.1016/j.csda.2021.107265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-shot-type (or divide-and-conquer) estimators have been widely used for distributed statistical analysis. However, their outstanding statistical efficiency hinges on two critical conditions. The first is the uniformity condition, which requires that the sample sizes allocated to different Workers should be as comparable as possible. The second one is the randomness condition, which requires that the data should be distributed across Workers as randomly as possible. Both conditions are often violated in practice. The violation of either condition can be seriously degrade the statistical efficiency of one-shot estimators, or even make them inconsistent. To fix this problem, a novel one-step upgraded pilot (OSUP) method is proposed. In the first step of the algorithm, a pilot estimate is computed based on randomly selected samples from different Workers. In the second step, one-step updating is conducted based on the pilot estimate by summarizing the derivative information on each Worker. The resulting OSUP estimator is theoretically proved to be as statistically efficient as the whole sample maximum likelihood estimator without any restrictive assumption about distribution uniformity and randomness . Extensive numerical studies are presented to demonstrate the finite sample performance of the OSUP estimator. Finally, by way of an illustration, an American Airlines dataset is analyzed on a Spark cluster.},
  archive      = {J_CSDA},
  author       = {Feifei Wang and Yingqiu Zhu and Danyang Huang and Haobo Qi and Hansheng Wang},
  doi          = {10.1016/j.csda.2021.107265},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107265},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Distributed one-step upgraded estimation for non-uniformly and non-randomly distributed data},
  volume       = {162},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and scalable computations for gaussian hierarchical
models with intrinsic conditional autoregressive spatial random effects.
<em>CSDA</em>, <em>162</em>, 107264. (<a
href="https://doi.org/10.1016/j.csda.2021.107264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast algorithms are developed for Bayesian analysis of Gaussian hierarchical models with intrinsic conditional autoregressive (ICAR) spatial random effects. To achieve computational speed-ups, first a result is proved on the equivalence between the use of an improper CAR prior with centering on the fly and the use of a sum-zero constrained ICAR prior. This equivalence result then provides the key insight for the algorithms, which are based on rewriting the hierarchical model in the spectral domain . The two novel algorithms are the Spectral Gibbs Sampler (SGS) and the Spectral Posterior Maximizer (SPM). Both algorithms are based on one single matrix spectral decomposition computation. After this computation, the SGS and SPM algorithms scale linearly with the sample size. The SGS algorithm is preferable for smaller sample sizes, whereas the SPM algorithm is preferable for sample sizes large enough for asymptotic calculations to provide good approximations. Because the matrix spectral decomposition needs to be computed only once, the SPM algorithm has computational advantages over algorithms based on sparse matrix factorizations (which need to be computed for each value of the random effects variance parameter) in situations when many models need to be fitted. Three simulation studies are performed: the first simulation study shows improved performance in computational speed in estimation of the SGS algorithm compared to an algorithm that uses the spectral decomposition of the precision matrix ; the second simulation study shows that for model selection computations with 10 regressors and sample sizes varying from 49 to 3600, when compared to the current fastest state-of-the-art algorithm implemented in the R package INLA, SPM computations are 550 to 1825 times faster; the third simulation study shows that, when compared to default INLA settings, SGS and SPM combined with reference priors provide much more adequate uncertainty quantification . Finally, the application of the novel SGS and SPM algorithms is illustrated with a spatial regression study of county-level median household income for 3108 counties in the contiguous United States in 2017.},
  archive      = {J_CSDA},
  author       = {Marco A.R. Ferreira and Erica M. Porter and Christopher T. Franck},
  doi          = {10.1016/j.csda.2021.107264},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107264},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast and scalable computations for gaussian hierarchical models with intrinsic conditional autoregressive spatial random effects},
  volume       = {162},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A motif building process for simulating random networks.
<em>CSDA</em>, <em>162</em>, 107263. (<a
href="https://doi.org/10.1016/j.csda.2021.107263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A simple stochastic process is described which provides a useful basis for generating some types of random networks. The process is based on an iterative building block technique that uses a motif profile as a conditional probability model. The conditional iterative form of the algorithm insures that the calculations required to simulate an observed random network are relatively simple and does not require complicated models to be fit to an observed network. Bounds on the theoretical cohesiveness of the realized networks are established and empirical studies provide indications on more general properties of the resulting network, suggesting the types of applications where the process would be useful. The algorithm is used to generate networks similar to those observed in several examples.},
  archive      = {J_CSDA},
  author       = {Alan M. Polansky and Paramahansa Pramanik},
  doi          = {10.1016/j.csda.2021.107263},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107263},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A motif building process for simulating random networks},
  volume       = {162},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian subgroup analysis in regression using mixture
models. <em>CSDA</em>, <em>162</em>, 107252. (<a
href="https://doi.org/10.1016/j.csda.2021.107252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneity occurs in many regression problems , where members from different latent subgroups respond differently to the covariates of interest (e.g., treatments) even after adjusting for other covariates . A Bayesian model called the mixture of finite mixtures (MFM) can be used to identify these subgroups, a key feature of which is that the number of subgroups is modeled as a random variable and its distribution is learned from the data. The Bayesian MFM model was not commonly used in earlier applications largely due to computational difficulties. In comparison, an alternative infinite mixture model called the Dirichlet Process Mixture (DPM) model has been a main Bayesian tool for clustering even though it is a mis-specified model for many applications. The popularity of DPM is partly due to its convenient mathematical properties that enable efficient computing algorithms. A class of Bayesian models tailored to regression problems , the conditional MFMs (cMFM), are described and studied. Computing for the cMFM is developed by extending the efficient MCMC algorithms for general MFMs. Using simulation and real data examples, the cMFM is compared to existing frequentist methods , the conditional DPM, and the original MFM and DPM models that model response and covariates jointly. The cMFM is shown to be favorable in clustering accuracy and is robust to different covariates and noise distributions.},
  archive      = {J_CSDA},
  author       = {Yunju Im and Aixin Tan},
  doi          = {10.1016/j.csda.2021.107252},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107252},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian subgroup analysis in regression using mixture models},
  volume       = {162},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust communication-efficient distributed composite
quantile regression and variable selection for massive data.
<em>CSDA</em>, <em>161</em>, 107262. (<a
href="https://doi.org/10.1016/j.csda.2021.107262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical analysis of massive data is becoming more and more common. Distributed composite quantile regression (CQR) for massive data is proposed in this paper. Specifically, the global CQR loss function is approximated by a surrogate one on the first machine, which relates to the local data only through their gradients, then the estimator is obtained on the first machine by minimizing the surrogate loss. Because the gradients of local datasets can be efficiently communicated, the communication cost is significantly reduced. In order to reduce the computational burdens, the induced smoothing method is applied. Theoretically, the resulting estimator is proved to be statistically as efficient as the global CQR estimator. What is more, as a direct application, a smooth-threshold distributed CQR estimating equations for variable selection is proposed. The new methods inherit the robustness and efficiency advantages of CQR. The promising performances of the new methods are supported by extensive numerical examples and real data analysis.},
  archive      = {J_CSDA},
  author       = {Kangning Wang and Shaomin Li and Benle Zhang},
  doi          = {10.1016/j.csda.2021.107262},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107262},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust communication-efficient distributed composite quantile regression and variable selection for massive data},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian multivariate latent class profile analysis:
Exploring the developmental progression of youth depression and
substance use. <em>CSDA</em>, <em>161</em>, 107261. (<a
href="https://doi.org/10.1016/j.csda.2021.107261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate latent class profile analysis (MLCPA) is a useful tool for exploring the stage-sequential process of multiple latent class variables, but the inference can be challenging due to the high-dimensional latent structure of the model. In this paper, a Bayesian approach via Markov chain Monte Carlo (MCMC) is proposed for MLCPA as an alternative to the maximum-likelihood (ML) method. Compared to the ML solution, Bayesian estimates are less sensitive to the set of initial values as well as easier to obtain standard errors. We also address issues in MCMC such as label-switching problem with a dynamic data-dependent prior and computational complexity with a recursive formula . Simulation studies revealed the validity and efficiency of the proposed algorithm. An empirical analysis of MLCPA using the National Longitudinal Survey of Youth 97 (NLSY97) identified a small number of representative developmental progressions of adolescent depression and substance use.},
  archive      = {J_CSDA},
  author       = {Jung Wun Lee and Hwan Chung and Saebom Jeon},
  doi          = {10.1016/j.csda.2021.107261},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107261},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian multivariate latent class profile analysis: Exploring the developmental progression of youth depression and substance use},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Harmless label noise and informative soft-labels in
supervised classification. <em>CSDA</em>, <em>161</em>, 107253. (<a
href="https://doi.org/10.1016/j.csda.2021.107253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual labelling of training examples is common practice in supervised learning. When the labelling task is of non-trivial difficulty, the supplied labels may not be equal to the ground-truth labels, and label noise is introduced into the training dataset. If the manual annotation is carried out by multiple experts, the same training example can be given different class assignments by different experts, which is indicative of label noise. In the framework of model-based classification, a simple, but key observation is that when the manual labels are sampled using the posterior probabilities of class membership, the noisy labels are as valuable as the ground-truth labels in terms of statistical information. A relaxation of this process is a random effects model for imperfect labelling by a group that uses approximate posterior probabilities of class membership. The relative efficiency of logistic regression using the noisy labels compared to logistic regression using the ground-truth labels can then be derived. The main finding is that logistic regression can be robust to label noise when label noise and classification difficulty are positively correlated. In particular, when classification difficulty is the only source of label errors, multiple sets of noisy labels can supply more information for the estimation of a classification rule compared to the single set of ground-truth labels.},
  archive      = {J_CSDA},
  author       = {Daniel Ahfock and Geoffrey J. McLachlan},
  doi          = {10.1016/j.csda.2021.107253},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107253},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Harmless label noise and informative soft-labels in supervised classification},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Communication-efficient distributed m-estimation with
missing data. <em>CSDA</em>, <em>161</em>, 107251. (<a
href="https://doi.org/10.1016/j.csda.2021.107251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the big data era, practical applications often encounter incomplete data. Current distributed methods, ignoring missingness , may cause inconsistent estimates. Motivated by that, a distributed algorithm is developed for M -estimation with missing data. The proposed algorithm is communication-efficient, where only gradient information is transferred to the central machine. The parameters of interest and the nuisance parameters are simultaneously updated. Theoretically, it is shown that the proposed algorithm achieves a full sample performance after a moderate number of iterations. The influence of nuisance parameters on distributed M -estimation is also investigated. Simulations via synthetic data illustrate the effectiveness of the algorithm. At last, the algorithm is applied to a real data set.},
  archive      = {J_CSDA},
  author       = {Jianwei Shi and Guoyou Qin and Huichen Zhu and Zhongyi Zhu},
  doi          = {10.1016/j.csda.2021.107251},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107251},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Communication-efficient distributed M-estimation with missing data},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast bayesian inference using laplace approximations in
nonparametric double additive location-scale models with right- and
interval-censored data. <em>CSDA</em>, <em>161</em>, 107250. (<a
href="https://doi.org/10.1016/j.csda.2021.107250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Penalized B-splines are commonly used in additive models to describe smooth changes in a response with quantitative covariates . This is usually done through the conditional mean in the exponential family using generalized additive models with an indirect impact on other conditional moments. Another common strategy is to focus on several low-order conditional moments, leaving the full conditional distribution unspecified. Alternatively, a multi-parameter distribution could be assumed for the response with several of its parameters jointly regressed on covariates using additive expressions. The latter proposal for a right- or interval-censored continuous response with a highly flexible and smooth nonparametric density is considered. The focus is on location-scale models with additive terms in the conditional mean and standard deviation. Starting from recent results in the Bayesian framework, a fast converging algorithm is proposed to select penalty parameters from their marginal posteriors . It is based on Laplace approximations of the conditional posterior of the spline parameters. Simulations suggest that the estimators obtained in this way have excellent frequentist properties and superior efficiencies compared to approaches with a working Gaussian assumption . The methodology is illustrated by the analysis of right- and interval-censored income data.},
  archive      = {J_CSDA},
  author       = {Philippe Lambert},
  doi          = {10.1016/j.csda.2021.107250},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107250},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast bayesian inference using laplace approximations in nonparametric double additive location-scale models with right- and interval-censored data},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A class of birnbaum–saunders type kernel density estimators
for nonnegative data. <em>CSDA</em>, <em>161</em>, 107249. (<a
href="https://doi.org/10.1016/j.csda.2021.107249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric density estimation using a class of deformed skew Birnbaum–Saunder (BS) type kernels is suggested for nonnegative data. A remarkable feature of new skew BS type kernel density estimators lies in its general formulation via asymmetry parameter as well as density generator. Mean integrated squared errors of the proposed estimators are investigated, together with strong consistency and asymptotic normality . Simulation studies and applications to real data sets are presented.},
  archive      = {J_CSDA},
  author       = {Yoshihide Kakizawa},
  doi          = {10.1016/j.csda.2021.107249},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107249},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A class of Birnbaum–Saunders type kernel density estimators for nonnegative data},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized accelerated hazards mixture cure models with
interval-censored data. <em>CSDA</em>, <em>161</em>, 107248. (<a
href="https://doi.org/10.1016/j.csda.2021.107248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing semiparametric mixture cure models with interval-censored data often assume a survival model, such as the Cox proportional hazards model , proportional odds model, accelerated failure time model, or their transformations for the susceptible subjects. There are cases in practice that such conventional assumptions may be inappropriate for modeling survival outcomes of susceptible subjects. We propose a more flexible class of generalized accelerated hazards mixture cure models for analysis of interval-censored failure times in the presence of a cure fraction. We develop a sieve maximum likelihood estimation in which the unknown cumulative baseline hazard function is approximated by means of B-splines and bundled with regression parameters . The proposed estimator possesses the properties of consistency and asymptotic normality , and can achieve the optimal global convergence rate under some conditions. Simulation results demonstrate that the proposed estimator performs satisfactorily in finite samples. The application of the proposed method is illustrated by the analysis of smoking cessation data from a lung health study.},
  archive      = {J_CSDA},
  author       = {Xiaoyu Liu and Liming Xiang},
  doi          = {10.1016/j.csda.2021.107248},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107248},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized accelerated hazards mixture cure models with interval-censored data},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing the first-order separability hypothesis for
spatio-temporal point patterns. <em>CSDA</em>, <em>161</em>, 107245. (<a
href="https://doi.org/10.1016/j.csda.2021.107245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {First-order separability of a spatio-temporal point process plays a fundamental role in the analysis of spatio-temporal point pattern data. While it is often a convenient assumption that simplifies the analysis greatly, existing non-separable structures should be accounted for in the model construction. Three different tests are proposed to investigate this hypothesis as a step of preliminary data analysis. The first two tests are exact or asymptotically exact for Poisson processes. The first test based on permutations and global envelopes allows one to detect at which spatial and temporal locations or lags the data deviate from the null hypothesis. The second test is a simple and computationally cheap χ 2 χ2 -test. The third test is based on stochastic reconstruction method and can be generally applied for non-Poisson processes. The performance of the first two tests is studied in a simulation study for Poisson and non-Poisson models. The third test is applied to the real data of the UK 2001 epidemic foot and mouth disease. 1},
  archive      = {J_CSDA},
  author       = {Mohammad Ghorbani and Nafiseh Vafaei and Jiří Dvořák and Mari Myllymäki},
  doi          = {10.1016/j.csda.2021.107245},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107245},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Testing the first-order separability hypothesis for spatio-temporal point patterns},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian semiparametric vector multiplicative error model.
<em>CSDA</em>, <em>161</em>, 107242. (<a
href="https://doi.org/10.1016/j.csda.2021.107242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactions among multiple time series of positive random variables are crucial in diverse financial applications, from spillover effects to volatility interdependence. A popular model in this setting is the vector Multiplicative Error Model (vMEM) which poses a linear iterative structure on the dynamics of the conditional mean, perturbed by a multiplicative innovation term. A main limitation of vMEM is however its restrictive assumption on the distribution of the random innovation term. A Bayesian semiparametric approach that models the innovation vector as an infinite location-scale mixture of multidimensional kernels with support on the positive orthant is used to address this major shortcoming of vMEM. Computational complications arising from the constraints to the positive orthant are avoided through the formulation of a slice sampler on the parameter-extended unconstrained version of the model. The method is applied to simulated and real data and a flexible specification is obtained that outperforms the classical ones in terms of fitting and predictive power .},
  archive      = {J_CSDA},
  author       = {Nicola Donelli and Stefano Peluso and Antonietta Mira},
  doi          = {10.1016/j.csda.2021.107242},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107242},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A bayesian semiparametric vector multiplicative error model},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ensemble of inverse moment estimators for sufficient
dimension reduction. <em>CSDA</em>, <em>161</em>, 107241. (<a
href="https://doi.org/10.1016/j.csda.2021.107241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction (SDR) is known to be a useful tool in data visualization and information retrieval for high dimensional data . Many well-known SDR approaches investigate the inverse conditional moments of the predictors given the response. Motivated by the idea of the aggregate dimension reduction, we propose an ensemble of inverse moment estimators to explore the central subspace. The new approach can substantially improve the estimation accuracy for the directions beyond the regression mean functions. A ladle estimator is proposed to determine the structural dimension of the central subspace. We further present two variable selection procedures to improve the interpretability of the reduced variables. Both simulation studies and a real data application show the efficacy of the newly proposed method.},
  archive      = {J_CSDA},
  author       = {Qin Wang and Yuan Xue},
  doi          = {10.1016/j.csda.2021.107241},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107241},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An ensemble of inverse moment estimators for sufficient dimension reduction},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining heterogeneous spatial datasets with process-based
spatial fusion models: A unifying framework. <em>CSDA</em>,
<em>161</em>, 107240. (<a
href="https://doi.org/10.1016/j.csda.2021.107240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern spatial statistics , the structure of data has become more heterogeneous. Depending on the types of spatial data, different modeling strategies are used. For example, kriging approaches for geostatistical data; Gaussian Markov random field models for lattice data; or log Gaussian Cox process models for point-pattern data. Despite these different modeling choices, the nature of underlying data-generating (latent) processes is often the same, which can be represented by some continuous spatial surfaces. A unifying framework is introduced for process-based multivariate spatial fusion models. The framework can jointly analyze all three aforementioned types of spatial data or any combinations thereof. Moreover, the framework accommodates different likelihoods for geostatistical and lattice data. It is shown that some established approaches, such as linear models of coregionalization, can be viewed as special cases of the proposed framework. A flexible and scalable implementation using R-INLA is provided. Simulation studies confirm that the prediction of latent processes improves as one moves from univariate spatial models to multivariate spatial fusion models. The framework is illustrated via a case study using datasets from a cross-sectional study linked with a national cohort in Switzerland. The differences in underlying spatial risks between respiratory disease and lung cancer are examined in the case study.},
  archive      = {J_CSDA},
  author       = {Craig Wang and Reinhard Furrer and SNC Study Group},
  doi          = {10.1016/j.csda.2021.107240},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107240},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Combining heterogeneous spatial datasets with process-based spatial fusion models: A unifying framework},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Copula particle filters. <em>CSDA</em>, <em>161</em>,
107230. (<a href="https://doi.org/10.1016/j.csda.2021.107230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel analysis of the state space model is presented. It is shown that by modifying the standard recursive update it is possible to apply a copula model to eliminate a particular integral, which is typically performed using importance sampling. With Bayesian models , copulas have recently been shown to provide predictive densities directly, avoiding integrals altogether. As in every particle filter algorithm particles are generated; hence the proposed algorithm is named the Copula Particle Filter (CPF). As a by-product, the likelihood function of the model is obtained and used for parameter inference. Several illustrations and comparisons made with the standard updating schemes are provided. Supplementary material for this article, containing code, are available online.},
  archive      = {J_CSDA},
  author       = {Carlos E. Rodríguez and Stephen G. Walker},
  doi          = {10.1016/j.csda.2021.107230},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107230},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Copula particle filters},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayes linear analysis for ordinary differential equations.
<em>CSDA</em>, <em>161</em>, 107228. (<a
href="https://doi.org/10.1016/j.csda.2021.107228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential equation models are used in a wide variety of scientific fields to describe the behaviour of physical systems. Commonly, solutions to given systems of differential equations are not available in closed-form; in such situations, the solution to the system is generally approximated numerically. The numerical solution obtained will be systematically different from the (unknown) true solution implicitly defined by the differential equations. Even if it were known, this true solution would be an imperfect representation of the behaviour of the real physical system that it was designed to represent. A Bayesian framework is proposed which handles all sources of numerical and structural uncertainty encountered when using ordinary differential equation (ODE) models to represent real-world processes. The model is represented graphically, and the graph proves to be useful tool, both for deriving a full prior belief specification and for inferring model components given observations of the real system. A general strategy for modelling the numerical discrepancy induced through choice of a particular solver is outlined, in which the variability of the numerical discrepancy is fixed to be proportional to the length of the solver time-step and a grid-refinement strategy is used to study its structure in detail. A Bayes linear adjustment procedure is presented, which uses a junction tree derived from the originally specified directed graphical model to propagate information efficiently between model components, lessening the computational demands associated with the inference. The proposed framework is illustrated through application to two examples: a model for the trajectory of an airborne projectile moving subject to gravity and air resistance , and a model for the coupled motion of a set of ringing bells and the tower which houses them.},
  archive      = {J_CSDA},
  author       = {Matthew Jones and Michael Goldstein and David Randell and Philip Jonathan},
  doi          = {10.1016/j.csda.2021.107228},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107228},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayes linear analysis for ordinary differential equations},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing error heterogeneity in censored linear regression.
<em>CSDA</em>, <em>161</em>, 107207. (<a
href="https://doi.org/10.1016/j.csda.2021.107207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In censored linear regression, a key assumption is that the error is independent of predictors. We develop an omnibus test to check error heterogeneity in censored linear regression. Our approach is based on testing the variance component in a working kernel machine regression model. The limiting null distribution of the proposed test statistic is shown to be a weighted sum of independent chi-squared distributions with one degree of freedom. A resampling scheme is derived to approximate the null distribution. The empirical performance of the proposed tests is evaluated via simulation and two real data sets .},
  archive      = {J_CSDA},
  author       = {Caiyun Fan and Wenbin Lu and Yong Zhou},
  doi          = {10.1016/j.csda.2021.107207},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107207},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Testing error heterogeneity in censored linear regression},
  volume       = {161},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust MAVE through nonconvex penalized regression.
<em>CSDA</em>, <em>160</em>, 107247. (<a
href="https://doi.org/10.1016/j.csda.2021.107247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensionality has been a significant feature in modern statistical modeling . Sufficient dimension reduction (SDR) as an efficient tool aims at reducing the original high dimensional predictors without losing any regression information. Minimum average variance estimation (MAVE) is a popular approach in SDR among others. However, it is not robust to outliers in the response due to the use of least squares . A robust estimation through regularization with case-specific parameters is proposed to achieve robust estimation and outlier detection simultaneously. Under the nonconvex penalized regression framework, two efficient computational strategies are introduced. Simulation studies and a real data application show the efficacy of the proposed approach. Compared with existing methods, the proposed approach is less sensitive to the choice of initial estimators.},
  archive      = {J_CSDA},
  author       = {Jing Zhang and Qin Wang and D&#39;Arcy Mays},
  doi          = {10.1016/j.csda.2021.107247},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107247},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust MAVE through nonconvex penalized regression},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A kernel-based measure for conditional mean dependence.
<em>CSDA</em>, <em>160</em>, 107246. (<a
href="https://doi.org/10.1016/j.csda.2021.107246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel metric, called kernel-based conditional mean dependence (KCMD), is proposed to measure and test the departure from conditional mean independence between a response variable Y and a predictor variable X , based on the reproducing kernel embedding and the Hilbert-Schmidt norm of a tensor operator. The KCMD has several appealing merits. It equals zero if and only if the conditional mean of Y given X is independent of X , i.e. E ( Y | X ) = E ( Y ) E(Y|X)=E(Y) almost surely, provided that the employed kernel is characteristic; it can be used to detect all kinds of conditional mean dependence with an appropriate choice of kernel; it has a simple expectation form and allows an unbiased empirical estimator. A class of test statistics based on the estimated KCMD is constructed, and a wild bootstrap test procedure to the conditional mean independence is presented. The limit distributions of the test statistics and the bootstrapped statistics under null hypothesis, fixed alternative hypothesis and local alternative hypothesis are given respectively, and a data-driven procedure to choose a suitable kernel is suggested. Simulation studies indicate that the tests based on the KCMD have close powers to the tests based on martingale difference divergence in monotone dependence, but excel in the cases of nonlinear relationships or the moment restriction on X is violated. Two real data examples are presented for the illustration of the proposed method.},
  archive      = {J_CSDA},
  author       = {Tingyu Lai and Zhongzhan Zhang and Yafei Wang},
  doi          = {10.1016/j.csda.2021.107246},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107246},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A kernel-based measure for conditional mean dependence},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In the pursuit of sparseness: A new rank-preserving penalty
for a finite mixture of factor analyzers. <em>CSDA</em>, <em>160</em>,
107244. (<a href="https://doi.org/10.1016/j.csda.2021.107244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A finite mixture of factor analyzers is an effective method for achieving parsimony in model-based clustering. Introducing a penalization term for the factor loading can lead to sparse estimates. However, in the pursuit of sparseness, one can end up with rank-deficient solutions regardless of the number of factors assumed. In light of this issue, a new penalty-based method that can fit a finite mixture of sparse factor analyzers with full-rank factor loading estimates is developed. In addition, the extension of an existing penalized factor analyzer model to a finite mixture is introduced.},
  archive      = {J_CSDA},
  author       = {Nam-Hwui Kim and Ryan P. Browne},
  doi          = {10.1016/j.csda.2021.107244},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107244},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {In the pursuit of sparseness: A new rank-preserving penalty for a finite mixture of factor analyzers},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel integrative learning for large-scale multi-response
regression with incomplete outcomes. <em>CSDA</em>, <em>160</em>,
107243. (<a href="https://doi.org/10.1016/j.csda.2021.107243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning is increasingly used to investigate the association structure between multiple responses and a single set of predictor variables in many applications. In the era of big data, the coexistence of incomplete outcomes, large number of responses, and high dimensionality in predictors poses unprecedented challenges in estimation, prediction and computation. In this paper, we propose a scalable and computationally efficient procedure, called PEER, for large-scale multi-response regression with incomplete outcomes, where both the numbers of responses and predictors can be high-dimensional. Motivated by sparse factor regression, we convert the multi-response regression into a set of univariate-response regressions, which can be efficiently implemented in parallel. Under some mild regularity conditions , we show that PEER enjoys nice sampling properties including consistency in estimation, prediction, and variable selection. Extensive simulation studies show that our proposal compares favorably with several existing methods in estimation accuracy, variable selection, and computation efficiency.},
  archive      = {J_CSDA},
  author       = {Ruipeng Dong and Daoji Li and Zemin Zheng},
  doi          = {10.1016/j.csda.2021.107243},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107243},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Parallel integrative learning for large-scale multi-response regression with incomplete outcomes},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Marginal false discovery rate for a penalized transformation
survival model. <em>CSDA</em>, <em>160</em>, 107232. (<a
href="https://doi.org/10.1016/j.csda.2021.107232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis that involves moderate/high dimensional covariates has become common. Most of the existing analyses have been focused on estimation and variable selection, using penalization and other regularization techniques. To draw more definitive conclusions, a handful of studies have also conducted inference. The recently developed mFDR (marginal false discovery rate) technique provides an alternative inference perspective and can be advantageous in multiple aspects. The existing inference studies for regularized estimation of survival data with moderate/high dimensional covariates assume the Cox and other specific models, which may not be sufficiently flexible. To tackle this problem, the analysis scope is expanded to the transformation model, which is robust and has been shown to be desirable for practical data analysis. Statistical validity is rigorously established. Two data analyses are conducted. Overall, an alternative inference approach has been developed for survival analysis with moderate/high dimensional data.},
  archive      = {J_CSDA},
  author       = {Weijuan Liang and Shuangge Ma and Cunjie Lin},
  doi          = {10.1016/j.csda.2021.107232},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107232},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Marginal false discovery rate for a penalized transformation survival model},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Composite quantile regression for ultra-high dimensional
semiparametric model averaging. <em>CSDA</em>, <em>160</em>, 107231. (<a
href="https://doi.org/10.1016/j.csda.2021.107231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To estimate the joint multivariate regression function , a robust ultra-high dimensional semiparametric model averaging approach is developed. Specifically, a three-stage estimation procedure is proposed. In the first step, the joint multivariate function can be approximated by a weighted average of one-dimensional marginal regression functions which can be estimated robustly by the composite quantile marginal regression. In the second step, a nonparametric composite quantile correlation screening technique is proposed to robustly choose relative important regressors whose marginal regression functions have significant effects on estimating the joint regression function . In the third step, based on these significant regressors that survive the screening procedure, a penalized composite quantile model averaging marginal regression is considered to further achieve sparse model weights and estimate the joint regression function. The sure independence screening property of the proposed screening procedure and sparse property of the penalized estimator are established under some regularity conditions . Numerical studies including both extensive simulation studies and an empirical application are considered to verify the merits of our proposed approach.},
  archive      = {J_CSDA},
  author       = {Chaohui Guo and Jing Lv and Jibo Wu},
  doi          = {10.1016/j.csda.2021.107231},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107231},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Composite quantile regression for ultra-high dimensional semiparametric model averaging},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent association graph inference for binary transaction
data. <em>CSDA</em>, <em>160</em>, 107229. (<a
href="https://doi.org/10.1016/j.csda.2021.107229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach to the problem of statistical inference for multivariate binary transaction data is proposed. A fundamental question that arises from this data, often referred to as market basket data, is how the items relate to one another. These relationships are naturally expressed by a graph and transactions can be modeled as samples of cliques from this association graph . A hierarchical model is developed that follows from this generative idea, along with an MCMC sampling procedure that handles large datasets and allows inference on a broad set of parameters. This model provides a sparser representation of associations between items as compared with frequent itemset mining (FIM) output, without sacrificing predictive accuracy . Additionally, by allowing inference on a broad set of parameters, the model provides a deeper level of insight into transaction data. Empirical results are provided on applications of this model to simulated data and real transaction data from Instacart.},
  archive      = {J_CSDA},
  author       = {David Reynolds and Luis Carvalho},
  doi          = {10.1016/j.csda.2021.107229},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107229},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Latent association graph inference for binary transaction data},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequentist delta-variance approximations with mixed-effects
models and TMB. <em>CSDA</em>, <em>160</em>, 107227. (<a
href="https://doi.org/10.1016/j.csda.2021.107227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measures of uncertainty are investigated for estimates and predictions using nonlinear mixed-effects models including state–space models in particular. These nonlinear mixed-effects models include fixed parameters and random effects. Maximum likelihood estimation of the parameters and conditional mean predictors of random effects are commonly used to estimate important quantities for a wide spectrum of applications. These quantities of interest may be functions of the parameters and random effects. In this case, software packages such as TMB and glmmTMB use a generalized delta method to provide standard errors and statistical inference . In the frequentist framework, it is clarified that these packages actually provide estimates of mean squared errors (MSE’s) based on a multivariate normal approximation of the distribution of the random effects given data. It is further shown that the MSE’s are not the variance of estimates due to repeated sampling of the data and the random effects. Equations are provided for that variance, including orders of approximations . In many cases the MSE’s will be more appropriate to use for statistical inference , but not always, and this is demonstrated for a simple random-walk state–space model example.},
  archive      = {J_CSDA},
  author       = {Nan Zheng and Noel Cadigan},
  doi          = {10.1016/j.csda.2021.107227},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107227},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Frequentist delta-variance approximations with mixed-effects models and TMB},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time stable empirical best predictors under a unit-level
model. <em>CSDA</em>, <em>160</em>, 107226. (<a
href="https://doi.org/10.1016/j.csda.2021.107226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparability as well as stability over time are highly desirable properties of regularly published statistics , specially when they are related to important issues such as people’s living conditions. For instance, poverty statistics displaying drastic changes from one period to the next for the same area have low credibility. In fact, longitudinal surveys that collect information on the same phenomena at several time points are indeed very popular, specially because they allow analyzing changes over time. Data coming from those surveys are likely to present correlation over time, which should be accounted for by the considered statistical procedures, and methods that account for it are expected to yield more stable estimates over time. A unit-level temporal linear mixed model is considered for small area estimation using historical information. The proposed model includes random time effects nested within the usual area effects, following an autoregressive process of order 1, AR(1). Based on the proposed model, empirical best predictors (EBPs) of small area parameters that are comparable for different time points and are expected to be more stable are derived. Explicit expressions are provided for the EBPs of some common poverty indicators. A parametric bootstrap method is also proposed for estimation of the mean square errors under the model. The proposed methods are studied through different simulation experiments, and are illustrated in an application to poverty mapping in Spanish provinces using survey data on living conditions from years 2004–2006.},
  archive      = {J_CSDA},
  author       = {María Guadarrama and Domingo Morales and Isabel Molina},
  doi          = {10.1016/j.csda.2021.107226},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107226},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Time stable empirical best predictors under a unit-level model},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust distributed modal regression for massive data.
<em>CSDA</em>, <em>160</em>, 107225. (<a
href="https://doi.org/10.1016/j.csda.2021.107225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modal regression is a good alternative of the mean regression and likelihood based methods, because of its robustness and high efficiency. A robust communication-efficient distributed modal regression for the distributed massive data is proposed in this paper. Specifically, the global modal regression objective function is approximated by a surrogate one at the first machine, which relates to the local datasets only through gradients. Then the resulting estimator can be obtained at the first machine and other machines only need to calculate the gradients, which can significantly reduce the communication cost. Under mild conditions, the asymptotical properties are established, which show that the proposed estimator is statistically as efficient as the global modal regression estimator . What is more, as a specific application, a penalized robust communication-efficient distributed modal regression variable selection procedure is developed. Simulation results and real data analysis are also included to validate our method.},
  archive      = {J_CSDA},
  author       = {Kangning Wang and Shaomin Li},
  doi          = {10.1016/j.csda.2021.107225},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107225},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust distributed modal regression for massive data},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bias-corrected kullback–leibler distance criterion based
model selection with covariables missing at random. <em>CSDA</em>,
<em>160</em>, 107224. (<a
href="https://doi.org/10.1016/j.csda.2021.107224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A model selection problem for the conditional probability function of the response variable Y Y given the covariable vector ( X , Z ) (X,Z) is considered under the case where X X is missing at random . And two novel model selection criteria are suggested. It is shown that the model selection by these two criteria is consistent and that the population parameter estimators, corresponding to the selected model, are also consistent and asymptotically normal. Extensive simulation studies are conducted to investigate the finite-sample performances of the proposed two criteria and a thorough comparison is made with some related model selection strategies. Moreover, two real data analyses are presented for illustrating the practical application of the proposed two criteria.},
  archive      = {J_CSDA},
  author       = {Yuting Wei and Qihua Wang and Xiaogang Duan and Jing Qin},
  doi          = {10.1016/j.csda.2021.107224},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107224},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bias-corrected Kullback–Leibler distance criterion based model selection with covariables missing at random},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust tests for time series comparison based on laplace
periodograms. <em>CSDA</em>, <em>160</em>, 107223. (<a
href="https://doi.org/10.1016/j.csda.2021.107223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical comparison of time series is useful for the detection of mechanical damage and many other real-world applications. New methods have been proposed to check whether two semi-stationary time series have the same normalized dynamics. The proposed methods differ from traditional methods in that they are based on the Laplace periodogram , which is a robust tool to analyze the serial dependence of time series. Via the method of estimating equations, a generalized score statistic and an order selected statistic are developed for the comparison. Their asymptotic distributions under the null are obtained. The proposed methods are applicable to compare two semi-stationary time series which may be dependent on each other. They also can be used to compare two time series whose traditional spectral densities or autocovariance structures may not exist. A Monte Carlo simulation study illustrates the validity of the asymptotic results and the finite sample performance. The proposed methods have been applied to an analysis of non-stationary vibration signals for mechanical damage detection.},
  archive      = {J_CSDA},
  author       = {Lei Jin},
  doi          = {10.1016/j.csda.2021.107223},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107223},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust tests for time series comparison based on laplace periodograms},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FunCC: A new bi-clustering algorithm for functional data
with misalignment. <em>CSDA</em>, <em>160</em>, 107219. (<a
href="https://doi.org/10.1016/j.csda.2021.107219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of bi-clustering functional data, which has recently been addressed in literature, is considered. A definition of ideal functional bi-cluster is given and a novel bi-clustering method, called Functional Cheng and Church (FunCC), is developed. The introduced algorithm searches for non-overlapping and non-exhaustive bi-clusters in a set of functions which are naturally ordered in matrix structure through a non-parametric deterministic iterative procedure. Moreover, the possible misalignment of the data, which is a common problem when dealing with functions, is taken into account. Hence, the FunCC algorithm is extended obtaining a model able to jointly bi-cluster and align curves. Different simulation studies are performed to show the potential of the introduced method and to compare it with state-of-the-art methods. The model is also applied on a real case study allowing to discover the spatio-temporal patterns of a bike-sharing system.},
  archive      = {J_CSDA},
  author       = {Marta Galvani and Agostino Torti and Alessandra Menafoglio and Simone Vantini},
  doi          = {10.1016/j.csda.2021.107219},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107219},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {FunCC: A new bi-clustering algorithm for functional data with misalignment},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-sample test in high dimensions through random selection.
<em>CSDA</em>, <em>160</em>, 107218. (<a
href="https://doi.org/10.1016/j.csda.2021.107218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the equality for two-sample means with high dimensional distributions is a fundamental problem in statistics . In the past two decades, many efforts have been devoted to comparing the mean vectors of two populations. Many existing tests rely on naive diagonal or trace estimators of the covariance matrix , ignoring the dependence structure between variables. To make more use of the dependence structure , a new nonparametric test based on random selections is proposed to test the population mean vector of nonnormal high-dimensional multivariate data . This makes more efficient use of the covariance structure to deal with dependent variables. The asymptotic null distribution of the proposed test is standard normal, regardless of the parent distributions of the random samples and the relations between data dimensions and sample sizes. Extensive simulations show that the power performance of the proposed test is encouraging compared with some existing methods.},
  archive      = {J_CSDA},
  author       = {Tao Qiu and Wangli Xu and Liping Zhu},
  doi          = {10.1016/j.csda.2021.107218},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107218},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Two-sample test in high dimensions through random selection},
  volume       = {160},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble sparse estimation of covariance structure for
exploring genetic disease data. <em>CSDA</em>, <em>159</em>, 107220. (<a
href="https://doi.org/10.1016/j.csda.2021.107220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data often occur nowadays in various areas, such as genetic and microarray data. The covariance matrix is of fundamental importance in analyzing the relationship between multivariate variables. A powerful tool for estimating a covariance matrix is the modified Cholesky decomposition , which allows for unconstrained estimation and guarantees the positive definiteness of the estimate. However, it requires a pre-specified ordering of variables before analysis, which is often not available in the real data. Hence, an ensemble Cholesky-based sparse estimation is proposed for a high-dimensional covariance matrix by adopting the model averaging idea. The asymptotically theoretical convergence rate is established under some regularity conditions . The merits of the proposed model are illustrated by the numerical study and two genetic disease data.},
  archive      = {J_CSDA},
  author       = {Xiaoning Kang and Mingqiu Wang},
  doi          = {10.1016/j.csda.2021.107220},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107220},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Ensemble sparse estimation of covariance structure for exploring genetic disease data},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized k-means in GLMs with applications to the
outbreak of COVID-19 in the united states. <em>CSDA</em>, <em>159</em>,
107217. (<a href="https://doi.org/10.1016/j.csda.2021.107217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized k k -means can be combined with any similarity or dissimilarity measure for clustering. Using the well known likelihood ratio or F F -statistic as the dissimilarity measure, a generalized k k -means method is proposed to group generalized linear models (GLMs) for exponential family distributions . Given the number of clusters k k , the proposed method is established by the uniform most powerful unbiased (UMPU) test statistic for the comparison between GLMs. If k k is unknown, then the proposed method can be combined with generalized liformation criterion (GIC) to automatically select the best k k for clustering. Both AIC and BIC are investigated as special cases of GIC. Theoretical and simulation results show that the number of clusters can be correctly identified by BIC but not AIC. The proposed method is applied to the state-level daily COVID-19 data in the United States, and it identifies 6 clusters. A further study shows that the models between clusters are significantly different from each other, which confirms the result with 6 clusters.},
  archive      = {J_CSDA},
  author       = {Tonglin Zhang and Ge Lin},
  doi          = {10.1016/j.csda.2021.107217},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107217},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized k-means in GLMs with applications to the outbreak of COVID-19 in the united states},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Promote sign consistency in the joint estimation of
precision matrices. <em>CSDA</em>, <em>159</em>, 107210. (<a
href="https://doi.org/10.1016/j.csda.2021.107210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gaussian graphical model is a popular tool for inferring the relationships among random variables , where the precision matrix provides a natural interpretation of conditional independence . With high-dimensional data, sparsity of the precision matrix is often assumed, and various regularization methods have been applied for estimation. In several scenarios, it is desirable to conduct the joint estimation of multiple precision matrices. In joint estimation, entries corresponding to the same element of multiple precision matrices form a group, and group regularization methods have been applied for the estimation and identification of sparsity structures. In many practical examples, it can be difficult to interpret the results when parameters within the same group have conflicting signs. Unfortunately, existing methods lack an explicit mechanism in regards to sign consistency of group parameters. To tackle this problem, a novel regularization method is developed for the joint estimation of multiple precision matrices. It effectively enhances the sign consistency of group parameters and hence can lead to more interpretable results, while still allowing for conflicting signs to achieve full flexibility. The method’s consistency properties are rigorously established. Simulations show that the proposed method outperforms competing alternatives under a variety of settings. For the two data examples, the proposed approach leads to interpretable results that are different from the alternatives.},
  archive      = {J_CSDA},
  author       = {Qingzhao Zhang and Shuangge Ma and Yuan Huang},
  doi          = {10.1016/j.csda.2021.107210},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107210},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Promote sign consistency in the joint estimation of precision matrices},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tests for differential gaussian bayesian networks based on
quadratic inference functions. <em>CSDA</em>, <em>159</em>, 107209. (<a
href="https://doi.org/10.1016/j.csda.2021.107209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypotheses testing procedures based on quadratic inference functions are proposed to test whether two Gaussian Bayesian networks are differential in structure, strength of associations between nodes, or both. Bootstrap procedures are developed to estimate p p -values to quantify the statistical significance of the tests. Operating characteristics of these testing procedures are investigated using synthetic data in simulation experiments. Additionally, the proposed methods are applied to flow cytometry data from a designed experiment, and data of bile acids from an observational study in the Alzheimer’s Disease Neuroimaging Initiative.},
  archive      = {J_CSDA},
  author       = {Xianzheng Huang and Hongmei Zhang},
  doi          = {10.1016/j.csda.2021.107209},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107209},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Tests for differential gaussian bayesian networks based on quadratic inference functions},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hidden semi-markov-switching quantile regression for time
series. <em>CSDA</em>, <em>159</em>, 107208. (<a
href="https://doi.org/10.1016/j.csda.2021.107208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hidden semi-Markov-switching quantile regression model is introduced as an extension of the hidden Markov-switching one. The proposed model allows for arbitrary sojourn-time distributions in the states of the Markov-switching chain. Parameters estimation is carried out via maximum likelihood estimation method using the Asymmetric Laplace distribution . As a by product of the model specification, the formulae and methods for forecasting, the state prediction, decoding and model checking that exist for ordinary hidden Markov-switching models can be applied to the proposed model. A simulation study to investigate the behaviour of the proposed model is performed covering several experimental settings. The empirical analysis studies the relationship between the stock index from the emerging market of China and those from the advanced markets, and investigates the determinants of high levels of pollution in an Italian small city.},
  archive      = {J_CSDA},
  author       = {Antonello Maruotti and Lea Petrella and Luca Sposito},
  doi          = {10.1016/j.csda.2021.107208},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107208},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hidden semi-markov-switching quantile regression for time series},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Censored mean variance sure independence screening for
ultrahigh dimensional survival data. <em>CSDA</em>, <em>159</em>,
107206. (<a href="https://doi.org/10.1016/j.csda.2021.107206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature screening has become an indispensable statistical modeling tool for ultrahigh dimensional data analysis. This article introduces a new model-free marginal feature screening approach for ultrahigh dimensional survival data with right censoring. The new procedure could be used for survival data with both ultrahigh dimensional categorical and continuous covariates . Motivated by Cui et al. (2015), a censored mean variance index (cMV) is proposed to measure the dependence between a survival outcome and a categorical covariate. Then a slice-and-fuse method is exploited to modify the cMV index adaptive to continuous covariates . The sure independence screening based on the censored mean variance index (cMV-SIS) is proposed to identify the important covariates for ultrahigh dimensional data with censored survival outcomes. It enjoys many appealing merits inherited in the mean variance index. It is model-free and thus robust to model misspecification . It is also robust to heavy tails and outliers in covariates. Moreover, the sure screening properties are theoretically investigated for both categorical and continuous covariates under some mild technical conditions. Extensive numerical simulations and a real data example have demonstrated the competitive performances of the proposed feature screening method.},
  archive      = {J_CSDA},
  author       = {Wei Zhong and Jiping Wang and Xiaolin Chen},
  doi          = {10.1016/j.csda.2021.107206},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107206},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Censored mean variance sure independence screening for ultrahigh dimensional survival data},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tuning-free ridge estimators for high-dimensional
generalized linear models. <em>CSDA</em>, <em>159</em>, 107205. (<a
href="https://doi.org/10.1016/j.csda.2021.107205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge estimators regularize the squared Euclidean lengths of parameters. Such estimators are mathematically and computationally attractive but involve tuning parameters that need to be calibrated. It is shown that ridge estimators can be modified such that tuning parameters can be avoided altogether, and the resulting estimator can improve on the prediction accuracies of standard ridge estimators combined with cross-validation.},
  archive      = {J_CSDA},
  author       = {Shih-Ting Huang and Fang Xie and Johannes Lederer},
  doi          = {10.1016/j.csda.2021.107205},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107205},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Tuning-free ridge estimators for high-dimensional generalized linear models},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypothesis testing of varying coefficients for regional
quantiles. <em>CSDA</em>, <em>159</em>, 107204. (<a
href="https://doi.org/10.1016/j.csda.2021.107204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the behavior of varying coefficients (VC) over a range of quantiles is important in the field of regression analysis . This study tests whether coefficient functions in varying quantile regression share common structural information across a certain range of quantile levels, even when linear combinations of covariates are unspecified in the null hypothesis. Our approach allows varying the coefficients, β ( τ , t ) β(τ,t) , as a function of the quantile level, τ ∈ Δ τ∈Δ , and a random variable , t ∈ T t∈T , where Δ Δ is the quantile region of interest and T T is a domain of t t . By incorporating an interval of quantiles in the inference, the proposed method can test whether the model can be justified by using a VC quantile model against other important reduced models, such as the linear quantile model, varying coefficient linear model, or linear model with homogeneous errors. We use bivariate B-splines to approximate the varying quantile functions, β ( τ , t ) β(τ,t) , and utilize composite quantile regression to estimate the parameters. Furthermore, we develop constrained composite quantile regression to provide a more efficient estimate in case the null hypothesis is not rejected. We show that the proposed test admits normal approximations. Using simulation and real data analysis, we demonstrate the superiority of the proposed test over other tests designed for a finite number of quantile levels.},
  archive      = {J_CSDA},
  author       = {Seyoung Park and Eun Ryung Lee},
  doi          = {10.1016/j.csda.2021.107204},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107204},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hypothesis testing of varying coefficients for regional quantiles},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep distribution regression. <em>CSDA</em>, <em>159</em>,
107203. (<a href="https://doi.org/10.1016/j.csda.2021.107203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their flexibility and predictive performance , machine-learning based regression methods have become an important tool for predictive modeling and forecasting. However, most methods focus on estimating the conditional mean or specific quantiles of the target quantity and do not provide the full conditional distribution , which contains uncertainty information that might be crucial for decision making. A general solution consists of transforming a conditional distribution estimation problem into a constrained multi-class classification problem, in which tools such as deep neural networks can be applied. A novel joint binary cross-entropy loss function is proposed to accomplish this goal. Its performance is compared to current state-of-the-art methods via simulation. The approach also shows improved accuracy in a probabilistic solar energy forecasting problem.},
  archive      = {J_CSDA},
  author       = {Rui Li and Brian J. Reich and Howard D. Bondell},
  doi          = {10.1016/j.csda.2021.107203},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107203},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Deep distribution regression},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric density estimation and bandwidth selection
with b-spline bases: A novel galerkin method. <em>CSDA</em>,
<em>159</em>, 107202. (<a
href="https://doi.org/10.1016/j.csda.2021.107202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A general and efficient nonparametric density estimation procedure for local bases, including B-splines, is proposed, which employs a novel statistical Galerkin method combined with basis duality theory . To select the bandwidth, an efficient cross-validation procedure is introduced, based on closed-form expressions in terms of the primal and dual B-spline basis. By utilizing a closed-form expression for the dual basis, the least-squares cross validation formula is calculated in closed-form, enabling an efficient estimation of the optimal bandwidth. The full computational procedure achieves optimal complexity, and is very accurate in comparison with existing estimation procedures, including state-of-the-art kernel density estimators . The presented theoretical results are supported by extensive numerical experiments, which demonstrate the efficiency and accuracy of the new methodology. This new approach provides a complete and optimally efficient framework for density estimation with a B-spline basis, based on simple and elegant closed-form estimators with theoretical convergence results that are substantiated in numerical experiments.},
  archive      = {J_CSDA},
  author       = {J. Lars Kirkby and Álvaro Leitao and Duy Nguyen},
  doi          = {10.1016/j.csda.2021.107202},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107202},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric density estimation and bandwidth selection with B-spline bases: A novel galerkin method},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dissimilarity functions for rank-invariant hierarchical
clustering of continuous variables. <em>CSDA</em>, <em>159</em>, 107201.
(<a href="https://doi.org/10.1016/j.csda.2021.107201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A theoretical framework is presented for a (copula-based) notion of dissimilarity between continuous random vectors and its main properties are studied. The proposed dissimilarity assigns the smallest value to a pair of random vectors that are comonotonic. Various properties of this dissimilarity are studied, with special attention to those that are prone to the hierarchical agglomerative methods, such as reducibility. Some insights are provided for the use of such a measure in clustering algorithms and a simulation study is presented. Real case studies illustrate the main features of the whole methodology.},
  archive      = {J_CSDA},
  author       = {Sebastian Fuchs and F. Marta L. Di Lascio and Fabrizio Durante},
  doi          = {10.1016/j.csda.2021.107201},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107201},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dissimilarity functions for rank-invariant hierarchical clustering of continuous variables},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subgroup causal effect identification and estimation via
matching tree. <em>CSDA</em>, <em>159</em>, 107188. (<a
href="https://doi.org/10.1016/j.csda.2021.107188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring causal effect from observational studies is a central topic in many scientific fields, including social science, health and medicine. The statistical methodology for estimating population average causal effect has been well established. However, the methods for identifying and estimating subpopulation causal effects are relatively less developed. Part of the challenge is that the subgroup structure is usually unknown, therefore, methods working well for population level effect need to be modified to address this. A tree method based on a matched design is proposed to identify subgroups with differential treatment effects. To remove observed confounding , propensity-score-matched pairs are first created. Then the classification and regression tree is applied to the within-pair outcome differences to identify the subgroup structure. This nonparametric approach is robust to model misspecification , which is important because it becomes much harder to specify a parametric outcome model in the presence of subgroup effects. In addition to describing assumptions under which our matching estimator is unbiased, algorithms for identifying subgroup structures are provided. Simulation results indicate that the proposed approach compares favorably, in terms of the percentage of correctly identifying true tree structure , with other competing tree-based methods, including causal trees, causal inference trees and the virtual twins approach. Finally the proposed method is implemented to examine the potential subgroup effect of the timing of Tobramycin use on chronic infection among pediatric Cystic Fibrosis patients.},
  archive      = {J_CSDA},
  author       = {Yuyang Zhang and Patrick Schnell and Chi Song and Bin Huang and Bo Lu},
  doi          = {10.1016/j.csda.2021.107188},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107188},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Subgroup causal effect identification and estimation via matching tree},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new class of stochastic EM algorithms. Escaping local
maxima and handling intractable sampling. <em>CSDA</em>, <em>159</em>,
107159. (<a href="https://doi.org/10.1016/j.csda.2020.107159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expectation–maximization (EM) algorithm is a powerful computational technique for maximum likelihood estimation in incomplete data models. When the expectation step cannot be performed in closed form, a stochastic approximation of EM (SAEM) can be used. The convergence of the SAEM toward critical points of the observed likelihood has been proved and its numerical efficiency has been demonstrated. However, sampling from the posterior distribution may be intractable or have a high computational cost. Moreover, despite appealing features, the limit position of this algorithm can strongly depend on its starting one. Sampling from an approximation of the distribution in the expectation phase of the SAEM allows coping with these two issues. This new procedure is referred to as approximated-SAEM and is proved to converge toward critical points of the observed likelihood. Experiments on synthetic and real data highlight the performance of this algorithm in comparison to the SAEM and the EM when feasible.},
  archive      = {J_CSDA},
  author       = {Stéphanie Allassonnière and Juliette Chevallier},
  doi          = {10.1016/j.csda.2020.107159},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107159},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A new class of stochastic EM algorithms. escaping local maxima and handling intractable sampling},
  volume       = {159},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clusterwise functional linear regression models.
<em>CSDA</em>, <em>158</em>, 107192. (<a
href="https://doi.org/10.1016/j.csda.2021.107192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical clusterwise linear regression is a useful method for investigating the relationship between scalar predictors and scalar responses with heterogeneous variation of regression patterns for different subgroups of subjects. This paper extends the classical clusterwise linear regression to incorporate multiple functional predictors by representing the functional coefficients in terms of a functional principal component basis. We estimate the functional principal component coefficients based on M-estimation and K K -means clustering algorithm , which can classify the data into clusters and estimate clusterwise coefficients simultaneously. One advantage of the proposed method is that it is robust and flexible by adopting a general loss function, which can be broadly applied to mean regression, median regression, quantile regression and robust mean regression. A Bayesian information criterion is proposed to select the unknown number of groups and shown to be consistent in model selection. We also obtain the convergence rate of the set of estimators to the set of true coefficients for all clusters. Simulation studies and real data analysis show that the proposed method is easily implemented, and it consequently improves previous works and also requires much less computing burden than existing methods.},
  archive      = {J_CSDA},
  author       = {Ting Li and Xinyuan Song and Yingying Zhang and Hongtu Zhu and Zhongyi Zhu},
  doi          = {10.1016/j.csda.2021.107192},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107192},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Clusterwise functional linear regression models},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High dimensional regression for regenerative time-series: An
application to road traffic modeling. <em>CSDA</em>, <em>158</em>,
107191. (<a href="https://doi.org/10.1016/j.csda.2021.107191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A statistical predictive model in which a high-dimensional time-series regenerates at the end of each day is used to model road traffic. Due to the regeneration, prediction is based on a daily modeling using a vector autoregressive model that combines linearly the past observations of the day. Due to the high-dimension, the learning algorithm follows from an ℓ 1 ℓ1 -penalization of the regression coefficients . Excess risk bounds are established under the high-dimensional framework in which the number of road sections goes to infinity with the number of observed days. Considering floating car data observed in an urban area, the approach is compared to state-of-the-art methods including neural networks . In addition of being highly competitive in terms of prediction, it enables the identification of the most determinant sections of the road network .},
  archive      = {J_CSDA},
  author       = {Mohammed Bouchouia and François Portier},
  doi          = {10.1016/j.csda.2021.107191},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107191},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {High dimensional regression for regenerative time-series: An application to road traffic modeling},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering with the average silhouette width. <em>CSDA</em>,
<em>158</em>, 107190. (<a
href="https://doi.org/10.1016/j.csda.2021.107190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Average Silhouette Width (ASW) is a popular cluster validation index to estimate the number of clusters. The question whether it also is suitable as a general objective function to be optimized for finding a clustering is addressed. Two algorithms (the standard version OSil and a fast version FOSil) are proposed, and they are compared with existing clustering methods in an extensive simulation study covering known and unknown numbers of clusters. Real data sets are analysed, partly exploring the use of the new methods with non-Euclidean distances. The ASW is shown to satisfy some axioms that have been proposed for cluster quality functions. The new methods prove useful and sensible in many cases, but some weaknesses are also highlighted. These also concern the use of the ASW for estimating the number of clusters together with other methods, which is of general interest due to the popularity of the ASW for this task.},
  archive      = {J_CSDA},
  author       = {Fatima Batool and Christian Hennig},
  doi          = {10.1016/j.csda.2021.107190},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107190},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Clustering with the average silhouette width},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust designs for dose–response studies: Model and
labelling robustness. <em>CSDA</em>, <em>158</em>, 107189. (<a
href="https://doi.org/10.1016/j.csda.2021.107189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for the construction of dose–response designs are presented that are robust against possible model misspecifications and mislabelled responses. The asymptotic properties are studied, leading to asymptotically minimax designs that minimize the maximum – over neighbourhoods of both types of model inadequacies – value of the mean squared error of the predictions. Both sequential and adaptive approaches are studied. Finite sample simulations and examples illustrate the gains to be made by adaptivity.},
  archive      = {J_CSDA},
  author       = {Douglas P. Wiens},
  doi          = {10.1016/j.csda.2021.107189},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107189},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust designs for dose–response studies: Model and labelling robustness},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Response adaptive designs for phase II trials with binary
endpoint based on context-dependent information measures. <em>CSDA</em>,
<em>158</em>, 107187. (<a
href="https://doi.org/10.1016/j.csda.2021.107187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many rare disease Phase II clinical trials, two objectives are of interest to an investigator: maximising the statistical power and maximising the number of patients responding to the treatment. These two objectives are competing, therefore, clinical trial designs offering a balance between them are needed. Recently, it was argued that response-adaptive designs such as families of multi-arm bandit (MAB) methods could provide the means for achieving this balance. Furthermore, response-adaptive designs based on a concept of context-dependent (weighted) information criteria were recently proposed with a focus on Shannon’s differential entropy . The information-theoretic designs based on the weighted Renyi, Tsallis and Fisher informations are also proposed. Due to built-in parameters of these novel designs, the balance between the statistical power and the number of patients that respond to the treatment can be tuned explicitly. The asymptotic properties of these measures are studied in order to construct intuitive criteria for arm selection. A comprehensive simulation study shows that using the exact criteria over asymptotic ones or using information measures with more parameters, namely Renyi and Tsallis entropies, brings no sufficient gain in terms of the power or proportion of patients allocated to superior treatments. The proposed designs based on information-theoretical criteria are compared to several alternative approaches. For example, via tuning of the built-in parameter, one can find designs with power comparable to the fixed equal randomisation’s but a greater number of patients responded in the trials.},
  archive      = {J_CSDA},
  author       = {Ksenia Kasianova and Mark Kelbert and Pavel Mozgunov},
  doi          = {10.1016/j.csda.2021.107187},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107187},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Response adaptive designs for phase II trials with binary endpoint based on context-dependent information measures},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust variable selection for model-based learning in
presence of adulteration. <em>CSDA</em>, <em>158</em>, 107186. (<a
href="https://doi.org/10.1016/j.csda.2021.107186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of identifying the most discriminating features when performing supervised learning has been extensively investigated. In particular, several methods for variable selection have been proposed in model-based classification. The impact of outliers and wrongly labeled units on the determination of relevant predictors has instead received far less attention, with almost no dedicated methodologies available. Two robust variable selection approaches are introduced: one that embeds a robust classifier within a greedy-forward selection procedure and the other based on the theory of maximum likelihood estimation and irrelevance. The former recasts the feature identification as a model selection problem, while the latter regards the relevant subset as a model parameter to be estimated. The benefits of the proposed methods, in contrast with non-robust solutions, are assessed via an experiment on synthetic data. An application to a high-dimensional classification problem of contaminated spectroscopic data is presented.},
  archive      = {J_CSDA},
  author       = {Andrea Cappozzo and Francesca Greselin and Thomas Brendan Murphy},
  doi          = {10.1016/j.csda.2021.107186},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107186},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust variable selection for model-based learning in presence of adulteration},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence intervals for spatial scan statistic.
<em>CSDA</em>, <em>158</em>, 107185. (<a
href="https://doi.org/10.1016/j.csda.2021.107185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial scan statistic is a popular statistical tool to detect geographical clusters of diseases. The basic problem of constructing confidence intervals for the relative risk of the most likely cluster has remained an open question. To cover this lack, a Monte Carlo based interval estimator for the relative risk of the primary cluster is derived. The method works for the circular spatial scan statistic applied to binomial data, and it ensures, by construction, an analytical control of the coverage probability under the nominal confidence coefficient . In addition, its performance is illustrated on simulated and real data of birth defects in New York State.},
  archive      = {J_CSDA},
  author       = {Ivair R. Silva and Luiz Duczmal and Martin Kulldorff},
  doi          = {10.1016/j.csda.2021.107185},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107185},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Confidence intervals for spatial scan statistic},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computation of projection regression depth and its induced
median. <em>CSDA</em>, <em>158</em>, 107184. (<a
href="https://doi.org/10.1016/j.csda.2021.107184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Notions of depth in regression have been introduced and studied in the literature. The most famous example is Regression Depth (RD), which is a direct extension of location depth to regression. The projection regression depth (PRD) is the extension of another prevailing location depth, the projection depth, to regression. The computation issues of the RD have been discussed in the literature. The computation issues of the PRD and its induced median (maximum depth estimator) in a regression setting, never considered before, are addressed. For a given β ∈ R p exact algorithms for the PRD with cost O ( n 2 log n ) ( p = 2 ) and O ( N ( n , p ) ( p 3 + n log n + n p 1 . 5 + n p N I t e r ) ) ( p &gt; 2 ) and approximate algorithms for the PRD and its induced median with cost respectively O ( N v n p ) and O ( R p N β ( p 2 + n N v N I t e r ) ) are proposed. Here N ( n , p ) is a number defined based on the total number of ( p − 1 ) dimensional hyperplanes formed by points induced from sample points and the β ; N v is the total number of unit directions v utilized; N β is the total number of candidate regression parameters β employed; N I t e r is the total number of iterations carried out in an optimization algorithm ; R is the total number of replications. Furthermore, as the second major contribution, three PRD induced estimators, which can be computed up to 30 times faster than that of the PRD induced median while maintaining a similar level of accuracy are introduced. Examples and simulation studies reveal that the depth median induced from the PRD is favorable in terms of robustness and efficiency, compared to the maximum depth estimator induced from the RD, which is the current leading regression median.},
  archive      = {J_CSDA},
  author       = {Yijun Zuo},
  doi          = {10.1016/j.csda.2021.107184},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107184},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Computation of projection regression depth and its induced median},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explicit-duration hidden markov models for quantum state
estimation. <em>CSDA</em>, <em>158</em>, 107183. (<a
href="https://doi.org/10.1016/j.csda.2021.107183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An explicit-duration Hidden Markov Model with a nonparametric kernel estimator of the state duration distribution is specified. The motivation comes from the physical problem of extracting the maximum information from an open quantum system subject to an external perturbation, which induces a change in the dynamics of the system. A nonparametric kernel estimator for discrete data is introduced, which is consistent and improves the estimates accuracy in presence of sparse data. To reconstruct the hidden dynamics, a Viterbi algorithm is used, which is robust against the underflow problem . Finite sample properties are investigated through an extensive Monte Carlo study showing that our formulation outperforms the original one both in small and in large samples.},
  archive      = {J_CSDA},
  author       = {Alessandra Luati and Marco Novelli},
  doi          = {10.1016/j.csda.2021.107183},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107183},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Explicit-duration hidden markov models for quantum state estimation},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixture of linear experts model for censored data: A novel
approach with scale-mixture of normal distributions. <em>CSDA</em>,
<em>158</em>, 107182. (<a
href="https://doi.org/10.1016/j.csda.2021.107182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture of linear experts (MoE) model is one of the widespread statistical frameworks for modeling, classification, and clustering of data. Built on the normality assumption of the error terms for mathematical and computational convenience, the classical MoE model has two challenges: (1) it is sensitive to atypical observations and outliers, and (2) it might produce misleading inferential results for censored data . The aim is then to resolve these two challenges, simultaneously, by proposing a robust MoE model for model-based clustering and discriminant censored data with the scale-mixture of normal (SMN) class of distributions for the unobserved error terms. An analytical expectation–maximization (EM) type algorithm is developed in order to obtain the maximum likelihood parameter estimates. Simulation studies are carried out to examine the performance, effectiveness, and robustness of the proposed methodology. Finally, a real dataset is used to illustrate the superiority of the new model.},
  archive      = {J_CSDA},
  author       = {Elham Mirfarah and Mehrdad Naderi and Ding-Geng Chen},
  doi          = {10.1016/j.csda.2021.107182},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107182},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Mixture of linear experts model for censored data: A novel approach with scale-mixture of normal distributions},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating robot strengths with application to selection of
alliance members in FIRST robotics competitions. <em>CSDA</em>,
<em>158</em>, 107181. (<a
href="https://doi.org/10.1016/j.csda.2021.107181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the inception of the FIRST Robotics Competition (FRC) and its special playoff system, robotics teams have longed to appropriately quantify the strengths of their designed robots. The FRC includes a playground draft-like phase (alliance selection), arguably the most game-changing part of the competition, in which the top-8 robotics teams in a tournament based on the FRC’s ranking system assess potential alliance members for the opportunity of partnering in a playoff stage. In such a three-versus-three competition, several measures and models have been used to characterize actual or relative robot strengths. However, existing models are found to have poor predictive performance due to their imprecise estimates of robot strengths caused by a small ratio of the number of observations to the number of robots. A more general regression model with latent clusters of robot strengths is, thus, proposed to enhance their predictive capacities. Two effective estimation procedures are further developed to simultaneously estimate the number of clusters, clusters of robots, and robot strengths. Meanwhile, some measures are used to assess the predictive ability of competing models, the agreement between published FRC measures of strength and model-based robot strengths of all, playoff, and FRC top-8 robots, and the agreement between FRC top-8 robots and model-based top robots. Moreover, the stability of estimated robot strengths and accuracies is investigated to determine whether the scheduled matches are excessive or insufficient. In the analysis of qualification data from the 2018 FRC Houston and Detroit championships, the predictive ability of our model is also shown to be significantly better than those of existing models. Teams who adopt the new model can now appropriately rank their preferences for playoff alliance partners with greater predictive capability than before.},
  archive      = {J_CSDA},
  author       = {Alejandro Lim and Chin-Tsang Chiang and Jen-Chieh Teng},
  doi          = {10.1016/j.csda.2021.107181},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107181},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimating robot strengths with application to selection of alliance members in FIRST robotics competitions},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable selection in finite mixture of regression models
with an unknown number of components. <em>CSDA</em>, <em>158</em>,
107180. (<a href="https://doi.org/10.1016/j.csda.2021.107180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian framework for finite mixture models to deal with model selection and the selection of the number of mixture components simultaneously is presented. For that purpose, a feasible reversible jump Markov Chain Monte Carlo algorithm is proposed to model each component as a sparse regression model. This approach is made robust to outliers by using a prior that induces heavy tails and works well under multicollinearity and with high-dimensional data. Finally, the framework is applied to cross-sectional data investigating early warning indicators. The results reveal two distinct country groups for which estimated effects of vulnerability indicators vary considerably.},
  archive      = {J_CSDA},
  author       = {Kuo-Jung Lee and Martin Feldkircher and Yi-Chi Chen},
  doi          = {10.1016/j.csda.2021.107180},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107180},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection in finite mixture of regression models with an unknown number of components},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A stochastic block model approach for the analysis of
multilevel networks: An application to the sociology of organizations.
<em>CSDA</em>, <em>158</em>, 107179. (<a
href="https://doi.org/10.1016/j.csda.2021.107179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multilevel network is defined as the junction of two interaction networks, one level representing the interactions between individuals and the other the interactions between organizations. The levels are linked by an affiliation relationship, each individual belonging to a unique organization. A new Stochastic Block Model is proposed as a unified probabilistic framework tailored for multilevel networks. This model contains latent blocks accounting for heterogeneity in the patterns of connection within each level and introducing dependencies between the levels. The sought connection patterns are not specified a priori which makes this approach flexible. Variational methods are used for the model inference and an Integrated Classified Likelihood criterion is developed for choosing the number of blocks and also for deciding whether the two levels are dependent or not. A comprehensive simulation study exhibits the benefit of considering this approach, illustrates the robustness of the clustering and highlights the reliability of the criterion used for model selection. This approach is applied on a sociological dataset collected during a television program trade fair, the inter-organizational level being the economic network between companies and the inter-individual level being the informal network between their representatives. It brings a synthetic representation of the two networks unraveling their intertwined structure and confirms the coopetition at stake.},
  archive      = {J_CSDA},
  author       = {Saint-Clair Chabert-Liddell and Pierre Barbillon and Sophie Donnet and Emmanuel Lazega},
  doi          = {10.1016/j.csda.2021.107179},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107179},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A stochastic block model approach for the analysis of multilevel networks: An application to the sociology of organizations},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised image segmentation with gaussian pairwise
markov fields. <em>CSDA</em>, <em>158</em>, 107178. (<a
href="https://doi.org/10.1016/j.csda.2021.107178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling strongly correlated random variables is a critical task in the context of latent variable models . A new probabilistic model, called Gaussian Pairwise Markov Field, is presented to generalize existing Markov Fields latent variables models, and to introduce more correlations between variables. This is done by considering the correlations within Gaussian Markov Random Fields models which are much richer than in the classical Markov Field models. The assets of the Gaussian Pairwise Markov Field model are explained. In particular, it offers a generalization of the classical Markov Field modelization that is highlighted. The new model is also considered in the practical case of unsupervised segmentation of images corrupted by long-range spatially-correlated noise, producing interesting new results.},
  archive      = {J_CSDA},
  author       = {Hugo Gangloff and Jean-Baptiste Courbot and Emmanuel Monfrini and Christophe Collet},
  doi          = {10.1016/j.csda.2021.107178},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107178},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Unsupervised image segmentation with gaussian pairwise markov fields},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing conditional mean through regression model sequence
using yanai’s generalized coefficient of determination. <em>CSDA</em>,
<em>158</em>, 107168. (<a
href="https://doi.org/10.1016/j.csda.2021.107168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional data analysis such as in genomics, repeated univariate regression for each variable is utilized to screen useful variables. However, signals jointly detectable with other variables may be overlooked. While the saturated model using all variables may not work in high-dimensional data, based on prior knowledge, group-wise analysis for a pre-defined group is often developed, but the power will be limited if the knowledge is insufficient. A flexible test procedure is thus proposed for conditional mean applicable to a variety of model sequences that bridge between low and high complexity models as in penalized regression. The test is based on the model that maximizes a generalization of the Yanai’s generalized coefficient of determination by exploiting the tendency for the dimensionality to be large under the null hypothesis. The test does not require complicated null distribution computation, thereby enabling large-scale testing application. Numerical studies demonstrated that the proposed test applied to the lasso and elastic net had a high power regardless of the simulation scenarios. Applied to a group-wise analysis in real genome-wide association study data from Alzheimer’s Disease Neuroimaging Initiative, the proposal gave a higher association signal than the existing methods.},
  archive      = {J_CSDA},
  author       = {Masao Ueki and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.csda.2021.107168},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107168},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Testing conditional mean through regression model sequence using yanai’s generalized coefficient of determination},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal treatment regimes for competing risk data using
doubly robust outcome weighted learning with bi-level variable
selection. <em>CSDA</em>, <em>158</em>, 107167. (<a
href="https://doi.org/10.1016/j.csda.2021.107167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of the optimal treatment regime is maximizing treatment benefits via personalized treatment assignments based on the observed patient and treatment characteristics. Parametric regression-based outcome learning approaches require exploring complex interplay between the outcome and treatment assignments adjusting for the patient and treatment covariates , yet correctly specifying such relationships is challenging. Thus, a robust method against misspecified models is desirable in practice. Parsimonious models are also desired to pursue a concise interpretation and to avoid including spurious predictors of the outcome or treatment benefits. These issues have not been comprehensively addressed in the presence of competing risks. Recognizing that competing risks and group variables are frequently present, we propose a doubly robust estimation with adaptive L 1 L1 penalties to select important variables at both group and within-group levels for competing risks data. The proposed method is applied to hematopoietic cell transplantation data to personalize the graft source choice for treatment-related mortality (TRM). While the existing medical literature attempts to find a uniform solution ignoring the heterogeneity of the graft source effects on TRM, the analysis results show the effect of the graft source on TRM could be different depending on the patient-specific characteristics.},
  archive      = {J_CSDA},
  author       = {Yizeng He and Soyoung Kim and Mi-Ok Kim and Wael Saber and Kwang Woo Ahn},
  doi          = {10.1016/j.csda.2021.107167},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107167},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Optimal treatment regimes for competing risk data using doubly robust outcome weighted learning with bi-level variable selection},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An exchange algorithm for optimal calibration of items in
computerized achievement tests. <em>CSDA</em>, <em>157</em>, 107177. (<a
href="https://doi.org/10.1016/j.csda.2021.107177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of large scale achievement tests, like national tests in school, eligibility tests for university, or international assessments for evaluation of students, is increasing. Pretesting of questions for the above mentioned tests is done to determine characteristic properties of the questions by adding them to an ordinary achievement test. If computerized tests are used, it has been shown using optimal experimental design methods that it is efficient to assign pretest questions to examinees based on their abilities. The specific distribution of abilities of the available examinees are considered and restricted optimal designs are applied. A new algorithm is developed which builds on an equivalence theorem . It discretizes the design space with the possibility to change the grid adaptively during the run, makes use of an exchange idea and filters computed designs. It is illustrated how the algorithm works through some examples as well as how convergence can be checked. The new algorithm is flexible and can be used even if different models are assumed for different questions.},
  archive      = {J_CSDA},
  author       = {Mahmood Ul Hassan and Frank Miller},
  doi          = {10.1016/j.csda.2021.107177},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107177},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An exchange algorithm for optimal calibration of items in computerized achievement tests},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partition-based feature screening for categorical data via
RKHS embeddings. <em>CSDA</em>, <em>157</em>, 107176. (<a
href="https://doi.org/10.1016/j.csda.2021.107176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new screening procedure for the ultrahigh dimensional data with a categorical response. By exploiting the group structure among predictors, a new partition-based screening approach is developed via the reproducing kernel Hilbert space (RKHS) embeddings in the maximum mean discrepancy framework. Consequently, the new method is able to identify the influential group of predictors that may be overlooked by the marginal screening methods. Moreover, by using the RKHS embedding, the new ranking index has a very simple form, and thus can be evaluated easily. As a by-product, the new method is model-free without specifying any relationship between the predictors and the response. The sure screening property of the proposed method is proved and the effectiveness of the new method is also illustrated via numerical studies and a real data analysis.},
  archive      = {J_CSDA},
  author       = {Jun Lu and Lu Lin and WenWu Wang},
  doi          = {10.1016/j.csda.2021.107176},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107176},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Partition-based feature screening for categorical data via RKHS embeddings},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Normal variance mixtures: Distribution, density and
parameter estimation. <em>CSDA</em>, <em>157</em>, 107175. (<a
href="https://doi.org/10.1016/j.csda.2021.107175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient algorithms for computing the distribution function, (log-)density function and for estimating the parameters of multivariate normal variance mixtures are introduced. For the evaluation of the distribution function, randomized quasi-Monte Carlo (RQMC) methods are utilized in a way that improves upon existing methods proposed for the special case of normal and t t distributions. For evaluating the log-density function, an adaptive RQMC algorithm that similarly exploits the superior convergence properties of RQMC methods is introduced. This allows the parameter estimation task to be accomplished via an expectation–maximization-like algorithm where all weights and log-densities are numerically estimated. Numerical examples demonstrate that the suggested algorithms are quite fast. Even for high dimensions around 1000 the distribution function can be estimated with moderate accuracy using only a few seconds of run time. Also, even log-densities around −100 can be estimated accurately and quickly. An implementation of all algorithms presented is available in the R package nvmix (version ≥ ≥ 0.0.4).},
  archive      = {J_CSDA},
  author       = {Erik Hintz and Marius Hofert and Christiane Lemieux},
  doi          = {10.1016/j.csda.2021.107175},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107175},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Normal variance mixtures: Distribution, density and parameter estimation},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate computation of projection depths. <em>CSDA</em>,
<em>157</em>, 107166. (<a
href="https://doi.org/10.1016/j.csda.2020.107166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data depth is a concept in multivariate statistics that measures the centrality of a point in a given data cloud in R d . If the depth of a point can be represented as the minimum of the depths with respect to all one-dimensional projections of the data, then the depth satisfies the so-called projection property. Such depths form an important class that includes many of the depths that have been proposed in literature. For depths that satisfy the projection property an approximate algorithm can easily be constructed since taking the minimum of the depths with respect to only a finite number of one-dimensional projections yields an upper bound for the depth with respect to the multivariate data . Such an algorithm is particularly useful if no exact algorithm exists or if the exact algorithm has a high computational complexity , as is the case with the halfspace depth or the projection depth. To compute these depths in high dimensions , the use of an approximate algorithm with better complexity is surely preferable. Instead of focusing on a single method we provide a comprehensive and fair comparison of several methods, both already described in the literature and original.},
  archive      = {J_CSDA},
  author       = {Rainer Dyckerhoff and Pavlo Mozharovskyi and Stanislav Nagy},
  doi          = {10.1016/j.csda.2020.107166},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107166},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Approximate computation of projection depths},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sum of kronecker products representation and its cholesky
factorization for spatial covariance matrices from large grids.
<em>CSDA</em>, <em>157</em>, 107165. (<a
href="https://doi.org/10.1016/j.csda.2020.107165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sum of Kronecker products (SKP) representation for spatial covariance matrices from gridded observations and a corresponding adaptive-cross-approximation-based framework for building the Kronecker factors are investigated. The time cost for constructing an n n -dimensional covariance matrix is O ( n k 2 ) O(nk2) and the total memory footprint is O ( n k ) O(nk) , where k k is the number of Kronecker factors. The memory footprint under the SKP representation is compared with that under the hierarchical representation and found to be one order of magnitude smaller. A Cholesky factorization algorithm under the SKP representation is proposed and shown to factorize a one-million dimensional covariance matrix in under 600 seconds on a standard scientific workstation. With the computed Cholesky factor , simulations of Gaussian random fields in one million dimensions can be achieved at a low cost for a wide range of spatial covariance functions .},
  archive      = {J_CSDA},
  author       = {Jian Cao and Marc G. Genton and David E. Keyes and George M. Turkiyyah},
  doi          = {10.1016/j.csda.2020.107165},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107165},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sum of kronecker products representation and its cholesky factorization for spatial covariance matrices from large grids},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principal component analysis using frequency components of
multivariate time series. <em>CSDA</em>, <em>157</em>, 107164. (<a
href="https://doi.org/10.1016/j.csda.2020.107164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimension reduction techniques for multivariate time series decompose the observed series into a few useful independent/orthogonal univariate components. A spectral domain method is developed for multivariate second-order stationary time series that linearly transforms the observed series into several groups of lower-dimensional multivariate subseries. These multivariate subseries have non-zero spectral coherence among components within a group but have zero spectral coherence among components across groups. The observed series is expressed as a sum of frequency components whose variances are proportional to the spectral matrices at the respective frequencies. The demixing matrix is then estimated using an eigendecomposition on the sum of the variance matrices of these frequency components and its asymptotic properties are derived. Finally, a consistent test on the cross-spectrum of pairs of components is used to find the desired segmentation into the lower-dimensional subseries. The numerical performance of the proposed method is illustrated through simulation examples and an application to modeling and forecasting wind data is presented.},
  archive      = {J_CSDA},
  author       = {Raanju R. Sundararajan},
  doi          = {10.1016/j.csda.2020.107164},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107164},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Principal component analysis using frequency components of multivariate time series},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Community detection via an efficient nonconvex optimization
approach based on modularity. <em>CSDA</em>, <em>157</em>, 107163. (<a
href="https://doi.org/10.1016/j.csda.2020.107163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximizing modularity is a widely used method for community detection, which is generally solved by approximate or greedy search because of its high complexity. In this paper, we propose a method, named MSM, for modularity maximization, which reformulates the modularity maximization problem as a subset identification problem and maximizes the surrogate of the modularity. The surrogate of the modularity is constructed by replacing the discontinuous indicator functions in the reformulated modularity function with the continuous truncated L 1 L1 function. This makes the NP-hard problem of maximizing the modularity function approximately become a non-convex optimization problem, which can be efficiently solved via the DC (Difference of Convex Functions) Programming. The proposed MSM method can be used for community detection when the number of communities is given, and it can also be applied to the situation where the number of communities is unknown. Then, we demonstrate the advantages of the proposed MSM method by some simulation results and real data analyses .},
  archive      = {J_CSDA},
  author       = {Quan Yuan and Binghui Liu},
  doi          = {10.1016/j.csda.2020.107163},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107163},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Community detection via an efficient nonconvex optimization approach based on modularity},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A flexible factor analysis based on the class of
mean-mixture of normal distributions. <em>CSDA</em>, <em>157</em>,
107162. (<a href="https://doi.org/10.1016/j.csda.2020.107162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor analysis is a statistical technique for data reduction and structure detection that traditionally relies on the normality assumption for factors. However, due to the presence of non-normal features such as asymmetry and heavy tails in many practical situations, the first two moments cannot adequately explain the factors. An extension of the factor analysis model is introduced by assuming a generalization of the multivariate restricted skew-normal distribution for the vector of unobserved factors . An efficient and computationally tractable EM-type algorithm is adopted for computing the maximum likelihood estimates by presenting a hierarchical representation of the proposed model. Finally, the efficiency and advantages of the proposed novel methodology are demonstrated through both simulated and real benchmark datasets.},
  archive      = {J_CSDA},
  author       = {Farzane Hashemi and Mehrdad Naderi and Ahad Jamalizadeh and Andriette Bekker},
  doi          = {10.1016/j.csda.2020.107162},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107162},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A flexible factor analysis based on the class of mean-mixture of normal distributions},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression analysis of asynchronous longitudinal data with
informative observation processes. <em>CSDA</em>, <em>157</em>, 107161.
(<a href="https://doi.org/10.1016/j.csda.2020.107161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A great deal of literature has been established for regression analysis of longitudinal data but most of the existing methods assume that covariates can be observed completely or at the same observation times for the response variable, and the observation process is independent of the response variable completely or given covariates . As pointed out by many authors, in practice, one may face the situation where the response variable and covariates are observed intermittently at different time points , leading to sparse asynchronous longitudinal data , or the observation process may be related to the response variable even given covariates. It is apparent that sometimes both issues can occur in the same time and although some literature has been developed to address each of the two issues, it does not seem to exist an established approach that can deal with both together. To address this, in this paper, a flexible semiparametric transformation conditional model is presented and for estimation, a kernel-weighted estimating equation-based approach is proposed. The proposed estimators of regression parameters are shown to be consistent and asymptotically follow the normal distribution . For the assessment of the finite sample performance of the method, an extensive simulation study is carried out and suggests that it performs well for practical situations. The approach is applied to a prospective HIV study that motivated this investigation.},
  archive      = {J_CSDA},
  author       = {Dayu Sun and Hui Zhao and Jianguo Sun},
  doi          = {10.1016/j.csda.2020.107161},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107161},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Regression analysis of asynchronous longitudinal data with informative observation processes},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-sample tests for multivariate functional data with
applications. <em>CSDA</em>, <em>157</em>, 107160. (<a
href="https://doi.org/10.1016/j.csda.2020.107160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate functional data are frequently obtained in many scientific or industrial areas where several functions for a statistical unit are observed over time. It is often interesting to check if the mean vector functions of two multivariate functional samples are equal. To address this important issue, two global tests for the above two-sample problem for multivariate functional data are proposed and studied. Their asymptotic random expressions under the null and certain local alternative hypotheses are derived and their root- n n consistencies are established. Simulation studies show that the proposed two tests generally have higher or not worse powers than some existing competitors. A real data application illustrates the proposed tests.},
  archive      = {J_CSDA},
  author       = {Zhiping Qiu and Jianwei Chen and Jin-Ting Zhang},
  doi          = {10.1016/j.csda.2020.107160},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107160},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Two-sample tests for multivariate functional data with applications},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SIMEX estimation in parametric modal regression with
measurement error. <em>CSDA</em>, <em>157</em>, 107158. (<a
href="https://doi.org/10.1016/j.csda.2020.107158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A simulation–extrapolation procedure has been developed for estimating the regression coefficients in a class of parametric modal regression models when the covariates are prone to measurement errors. Large sample properties of the proposed estimator, including the consistency and asymptotic normality , have been thoroughly investigated. Simulation studies and real data applications have been conducted to evaluate its robustness to potential outliers and its effectiveness in reducing bias caused by the measurement error.},
  archive      = {J_CSDA},
  author       = {Jianhong Shi and Yujing Zhang and Ping Yu and Weixing Song},
  doi          = {10.1016/j.csda.2020.107158},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107158},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {SIMEX estimation in parametric modal regression with measurement error},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression analysis of censored data with nonignorable
missing covariates and application to alzheimer disease. <em>CSDA</em>,
<em>157</em>, 107157. (<a
href="https://doi.org/10.1016/j.csda.2020.107157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss regression analysis of censored failure time data when there exist missing covariates and more specifically, we will consider interval-censored data, a general form of censored data , and the nonignorable missing. Although many methods have been proposed in the literature for censored data with missing covariates , they only apply to limited situations and it does not seem to exist an established procedure for the situation discussed here. For the analysis, we employ the semiparametric linear transformation model and develop a two-step estimation procedure. In addition, the asymptotic properties of the resulting estimators are established and a Poisson variable-based EM algorithm is provided for the implementation of the proposed estimation procedure. Finally the proposed approach is applied to an Alzheimer Disease study that motivated this investigation.},
  archive      = {J_CSDA},
  author       = {Mingyue Du and Huiqiong Li and Jianguo Sun},
  doi          = {10.1016/j.csda.2020.107157},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107157},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Regression analysis of censored data with nonignorable missing covariates and application to alzheimer disease},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian bayesian network comparisons with graph ordering
unknown. <em>CSDA</em>, <em>157</em>, 107156. (<a
href="https://doi.org/10.1016/j.csda.2020.107156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian approach is proposed that unifies Gaussian Bayesian network constructions and comparisons between two networks (identical or differential) for data with graph ordering unknown. When sampling graph ordering, to escape from local maximums, an adjusted single queue equi-energy algorithm is applied. The conditional posterior probability mass function for network differentiation is derived and its asymptotic proposition is theoretically assessed. Simulations are used to demonstrate the approach and compare with existing methods. Based on epigenetic data at a set of DNA methylation sites (CpG sites), the proposed approach is further examined on its ability to detect network differentiations. Findings from theoretical assessment, simulations, and real data applications support the efficacy and efficiency of the proposed method for network comparisons.},
  archive      = {J_CSDA},
  author       = {Hongmei Zhang and Xianzheng Huang and Shengtong Han and Faisal I. Rezwan and Wilfried Karmaus and Hasan Arshad and John W. Holloway},
  doi          = {10.1016/j.csda.2020.107156},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107156},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Gaussian bayesian network comparisons with graph ordering unknown},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mapping-based universal kriging model for
order-of-addition experiments in drug combination studies.
<em>CSDA</em>, <em>157</em>, 107155. (<a
href="https://doi.org/10.1016/j.csda.2020.107155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern pharmaceutical studies, treatments may include several drugs added sequentially, and the drugs’ order-of-addition can have significant impacts on their efficacy. In practice, experiments enumerating all possible drug sequences are often not affordable, and appropriate statistical models which can accurately predict all cases using only a small number of experimental trials are required. A novel mapping-based universal Kriging (MUK) model and its simplified variant are proposed for analyzing such order-of-addition experiments with blocking. They can provide accurate predictions and have robust performances under various experimental designs . The MUK model can also incorporate available domain knowledge to enhance its interpretation. The superiority of the proposed methods is illustrated via a real five-drug experiment on lymphoma and two simulation examples.},
  archive      = {J_CSDA},
  author       = {Qian Xiao and Hongquan Xu},
  doi          = {10.1016/j.csda.2020.107155},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107155},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A mapping-based universal kriging model for order-of-addition experiments in drug combination studies},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Communication-efficient distributed estimator for
generalized linear models with a diverging number of covariates.
<em>CSDA</em>, <em>157</em>, 107154. (<a
href="https://doi.org/10.1016/j.csda.2020.107154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, it has become increasingly common to store large-scale data sets distributedly across a great number of clients. The aim of the study is to develop a distributed estimator for generalized linear models (GLMs) in the “large n n , diverging p n pn ” framework with a weak assumption on the number of clients. When the dimension diverges at the rate of o ( n ) o(n) , the asymptotic efficiency of the global maximum likelihood estimator (MLE), the one-step MLE, and the aggregated estimating equation (AEE) estimator for GLMs are established. A novel distributed estimator is then proposed with two rounds of communication. It has the same asymptotic efficiency as the global MLE under p n = o ( n ) pn=o(n) . The assumption on the number of clients is more relaxed than that of the AEE estimator and the proposed method is thus more practical for real-world applications. Simulations and a case study demonstrate the satisfactory finite-sample performance of the proposed estimator.},
  archive      = {J_CSDA},
  author       = {Ping Zhou and Zhen Yu and Jingyi Ma and Maozai Tian and Ye Fan},
  doi          = {10.1016/j.csda.2020.107154},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107154},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Communication-efficient distributed estimator for generalized linear models with a diverging number of covariates},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of high dimensional factor model with multiple
threshold-type regime shifts. <em>CSDA</em>, <em>157</em>, 107153. (<a
href="https://doi.org/10.1016/j.csda.2020.107153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the estimation of high dimensional factor model with multiple threshold-type regime shifts in factor loadings. Firstly, the number of thresholds is determined by comparing the number of factors in the adjacent subintervals . Secondly, the thresholds are estimated one by one by concentrated least squares , and then the factors and loadings are obtained by the principal component method in the augmented subgroups with a single threshold. Under some regularity conditions , the consistency of these estimators can be obtained. Monte Carlo simulation results demonstrate that the proposed method has desirable performance in finite samples. A real data analysis is carried out for illustration.},
  archive      = {J_CSDA},
  author       = {Jianhong Wu},
  doi          = {10.1016/j.csda.2020.107153},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107153},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of high dimensional factor model with multiple threshold-type regime shifts},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast bayesian estimation of spatial count data models.
<em>CSDA</em>, <em>157</em>, 107152. (<a
href="https://doi.org/10.1016/j.csda.2020.107152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial count data models are used to explain and predict the frequency of phenomena such as traffic accidents in geographically distinct entities such as census tracts or road segments. These models are typically estimated using Bayesian Markov chain Monte Carlo (MCMC) simulation methods, which, however, are computationally expensive and do not scale well to large datasets. Variational Bayes (VB), a method from machine learning , addresses the shortcomings of MCMC by casting Bayesian estimation as an optimisation problem instead of a simulation problem. Considering all these advantages of VB, a VB method is derived for posterior inference in negative binomial models with unobserved parameter heterogeneity and spatial dependence. Pólya-Gamma augmentation is used to deal with the non-conjugacy of the negative binomial likelihood and an integrated non-factorised specification of the variational distribution is adopted to capture posterior dependencies. The benefits of the proposed approach are demonstrated in a Monte Carlo study and an empirical application on estimating youth pedestrian injury counts in census tracts of New York City. The VB approach is around 45 to 50 times faster than MCMC on a regular eight-core processor in a simulation and an empirical study, while offering similar estimation and predictive accuracy . Conditional on the availability of computational resources, the embarrassingly parallel architecture of the proposed VB method can be exploited to further accelerate its estimation by up to 20 times.},
  archive      = {J_CSDA},
  author       = {Prateek Bansal and Rico Krueger and Daniel J. Graham},
  doi          = {10.1016/j.csda.2020.107152},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107152},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast bayesian estimation of spatial count data models},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient inference for stochastic differential equation
mixed-effects models using correlated particle pseudo-marginal
algorithms. <em>CSDA</em>, <em>157</em>, 107151. (<a
href="https://doi.org/10.1016/j.csda.2020.107151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic differential equation mixed-effects models (SDEMEMs) are flexible hierarchical models that are able to account for random variability inherent in the underlying time-dynamics, as well as the variability between experimental units and, optionally, account for measurement error. Fully Bayesian inference for state-space SDEMEMs is performed, using data at discrete times that may be incomplete and subject to measurement error. However, the inference problem is complicated by the typical intractability of the observed data likelihood which motivates the use of sampling-based approaches such as Markov chain Monte Carlo. A Gibbs sampler is proposed to target the marginal posterior of all parameter values of interest. The algorithm is made computationally efficient through careful use of blocking strategies and correlated pseudo-marginal Metropolis–Hastings steps within the Gibbs scheme. The resulting methodology is flexible and is able to deal with a large class of SDEMEMs. The methodology is demonstrated on three case studies, including tumor growth dynamics and neuronal data. The gains in terms of increased computational efficiency are model and data dependent, but unless bespoke sampling strategies requiring analytical derivations are possible for a given model, we generally observe an efficiency increase of one order of magnitude when using correlated particle methods together with our blocked-Gibbs strategy.},
  archive      = {J_CSDA},
  author       = {Samuel Wiqvist and Andrew Golightly and Ashleigh T. McLean and Umberto Picchini},
  doi          = {10.1016/j.csda.2020.107151},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107151},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient inference for stochastic differential equation mixed-effects models using correlated particle pseudo-marginal algorithms},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compromise design for combination experiment of two drugs.
<em>CSDA</em>, <em>157</em>, 107150. (<a
href="https://doi.org/10.1016/j.csda.2020.107150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preclinical experiment on two-drug combination is a stepping stone to multi-drug combination studies. Experimental designs have been proposed in the literature to test the presence of synergism between the combined drugs. However, a design that is efficient for synergy testing is not necessarily desirable for dose–response modeling and the latter is important for future development on drug interaction analysis. This work proposes an experimental design, called a compromise design to meet the dual requirements on synergy testing and dose–response modeling. The key idea of the design is to spread the design points uniformly on a pair of design regions where synergy testing and dose–response modeling are respectively carried out. Simulations and two illustrative examples are given to demonstrate the usefulness of the compromise design. In the illustrative examples, the good balance of the proposed design is visualized by 2-D projections of the design points. The simulation results indicate that the compromise design performs satisfactorily in terms of both testing power and model prediction accuracy.},
  archive      = {J_CSDA},
  author       = {Hengzhen Huang and Xueping Chen},
  doi          = {10.1016/j.csda.2020.107150},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107150},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Compromise design for combination experiment of two drugs},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Support vector subset scan for spatial pattern detection.
<em>CSDA</em>, <em>157</em>, 107149. (<a
href="https://doi.org/10.1016/j.csda.2020.107149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovery of localized and irregularly shaped anomalous patterns in spatial data provides useful context for operational decisions across many policy domains. The support vector subset scan (SVSS) integrates the penalized fast subset scan with a kernel support vector machine classifier to accurately detect spatial clusters without imposing hard constraints on the shape or size of the pattern. The method iterates between (1) efficiently maximizing a penalized log-likelihood ratio over subsets of locations to obtain an anomalous pattern, and (2) learning a high-dimensional decision boundary between locations included in and excluded from the anomalous subset. On each iteration, location-specific penalties to the log-likelihood ratio are assigned according to distance to the decision boundary, encouraging patterns which are spatially compact but potentially highly irregular in shape. SVSS outperforms competing methods for spatial cluster detection at the task of detecting randomly generated patterns in simulated experiments. SVSS enables discovery of practically-useful anomalous patterns for disease surveillance in Chicago, IL, crime hotspot detection in Portland, OR, and pothole cluster detection in Pittsburgh, PA, as demonstrated by experiments using publicly available data sets from these domains.},
  archive      = {J_CSDA},
  author       = {Dylan Fitzpatrick and Yun Ni and Daniel B. Neill},
  doi          = {10.1016/j.csda.2020.107149},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107149},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Support vector subset scan for spatial pattern detection},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedding and learning with signatures. <em>CSDA</em>,
<em>157</em>, 107148. (<a
href="https://doi.org/10.1016/j.csda.2020.107148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential and temporal data arise in many fields of research, such as quantitative finance, medicine, or computer vision . A novel approach for sequential learning , called the signature method and rooted in rough path theory, is considered. Its basic principle is to represent multidimensional paths by a graded feature set of their iterated integrals, called the signature. This approach relies critically on an embedding principle, which consists in representing discretely sampled data as paths, i.e., functions from [ 0 , 1 ] to R d . After a survey of machine learning methodologies for signatures, the influence of embeddings on prediction accuracy is investigated with an in-depth study of three recent and challenging datasets. It is shown that a specific embedding, called lead–lag, is systematically the strongest performer across all datasets and algorithms considered. Moreover, an empirical study reveals that computing signatures over the whole path domain does not lead to a loss of local information. It is concluded that, with a good embedding, combining signatures with other simple algorithms achieves results competitive with state-of-the-art, domain-specific approaches.},
  archive      = {J_CSDA},
  author       = {Adeline Fermanian},
  doi          = {10.1016/j.csda.2020.107148},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107148},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Embedding and learning with signatures},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel method of marginalisation using low discrepancy
sequences for integrated nested laplace approximations. <em>CSDA</em>,
<em>157</em>, 107147. (<a
href="https://doi.org/10.1016/j.csda.2020.107147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, it has been shown that the shape of a marginal distribution can be more accurately and efficiently captured using a set of low discrepancy sequence (LDS) points compared to standard grid points. This suggests that the use of LDS could improve the approximation to marginal posterior distributions produced by grid-based Bayesian methods such as the Integrated Nested Laplace Approximation (INLA). However, obtaining marginal posteriors using LDS is not straightforward. Two algorithms are proposed that can be incorporated into the INLA implementation to approximate marginal posterior distributions using LDS without sacrificing computational efficiency. Two examples are also presented to demonstrate that the proposed algorithms, when used inside INLA, can estimate marginal posteriors more accurately and efficiently than the grid approximation INLA employs. A distinct advantage is that these algorithms can also capture multimodal shapes that the current numerical integration free algorithm (NIFA) method used by INLA cannot.},
  archive      = {J_CSDA},
  author       = {Paul T. Brown and Chaitanya Joshi and Stephen Joe and Håvard Rue},
  doi          = {10.1016/j.csda.2020.107147},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107147},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A novel method of marginalisation using low discrepancy sequences for integrated nested laplace approximations},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A non-parametric test for comparing conditional ROC curves.
<em>CSDA</em>, <em>157</em>, 107146. (<a
href="https://doi.org/10.1016/j.csda.2020.107146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparing the accuracy and the behaviour of different diagnostic procedures is one of the main objectives of the Receiver Operating Characteristic (ROC) curve analysis. Along with the diagnostic variables it is usual to observe other covariates , but that extra information has been hardly ever considered for the comparison of this kind of curves. A new non-parametric test is proposed for the comparison of conditional ROC curves. This test is based on a statistic whose theoretical properties are examined, and a bootstrap mechanism is used to calibrate the test. Simulations are run to analyse the practical performance of the test in terms of level approximation and power. An application to real data is also presented to illustrate the procedure.},
  archive      = {J_CSDA},
  author       = {Arís Fanjul-Hevia and Wenceslao González-Manteiga and Juan Carlos Pardo-Fernández},
  doi          = {10.1016/j.csda.2020.107146},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107146},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A non-parametric test for comparing conditional ROC curves},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustness of cost-effectiveness analyses of cluster
randomized trials assuming bivariate normality against skewed cost data.
<em>CSDA</em>, <em>157</em>, 107143. (<a
href="https://doi.org/10.1016/j.csda.2020.107143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bivariate normal multilevel model (MLM) provides a flexible modeling framework for cost-effectiveness analyses (CEAs) alongside cluster randomized trials (CRTs) as well as for sample size calculations of these trials. The bivariate MLM assumes a joint normal distribution for effects and costs, both within (individual level) and between (cluster level) clusters. A typical problem in CEAs is that costs are often associated with right-skewed distributions (e.g., gamma or lognormal), which make it sometimes difficult to justify the modeling of the data based on normality assumptions. The robustness of CEAs of CRTs based on the bivariate normal MLM to non-normal cost distributions at both cluster and individual level are investigated. Normal, gamma, and lognormal distributions are considered using scenarios that differ in the number of clusters, the number of persons per cluster, the covariance parameters of the model, and the level of skewness in the cost data. It is shown that CEA of CRTs, and therefore sample size calculation, based on the bivariate normal MLM, is quite robust against highly skewed costs across a wide range of scenarios. This robustness holds especially with respect to the type I error rate and the power. In terms of bias in variance component estimation and standard errors of fixed effects, large bias can occur in small samples. However, these biases do not appear to translate into any serious deviation of the type I error rate or power from the nominal level.},
  archive      = {J_CSDA},
  author       = {Md Abu Manju and Math J.J.M. Candel and Gerard J.P. van Breukelen},
  doi          = {10.1016/j.csda.2020.107143},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107143},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robustness of cost-effectiveness analyses of cluster randomized trials assuming bivariate normality against skewed cost data},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kendall regression coefficient. <em>CSDA</em>, <em>157</em>,
107140. (<a href="https://doi.org/10.1016/j.csda.2020.107140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new multivariate extension of Kendall’s dependence coefficient tailored for use in regression analysis is introduced. This coefficient is called Kendall regression coefficient and indicates how well the response variable can be approximated by a strictly increasing function of the regressor (predictor) variables. The properties of this coefficient are examined. In the second part the empirical regression coefficient is considered. It is proved that this coefficient is asymptotically normally distributed.},
  archive      = {J_CSDA},
  author       = {Eckhard Liebscher},
  doi          = {10.1016/j.csda.2020.107140},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107140},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Kendall regression coefficient},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric quantile regression using family of
quantile-based asymmetric densities. <em>CSDA</em>, <em>157</em>,
107129. (<a href="https://doi.org/10.1016/j.csda.2020.107129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression is an important tool in data analysis. Linear regression, or more generally, parametric quantile regression imposes often too restrictive assumptions. Nonparametric regression avoids making distributional assumptions, but might have the disadvantage of not exploiting distributional modelling elements that might be brought in. A semiparametric approach towards estimating conditional quantile curves is proposed. It is based on a recently studied large family of asymmetric densities of which the location parameter is a quantile (and not a mean). Passing to conditional densities and exploiting local likelihood techniques in a multiparameter functional setting then leads to a semiparametric estimation procedure. For the local maximum likelihood estimators the asymptotic distributional properties are established, and it is discussed how to assess finite sample bias and variance. Due to the appealing semiparametric framework, one can discuss in detail the bandwidth selection issue, and provide several practical bandwidth selectors. The practical use of the semiparametric method is illustrated in the analysis of maximum winds speeds of hurricanes in the North Atlantic region, and of bone density data. A simulation study includes a comparison with nonparametric local linear quantile regression as well as an investigation of robustness against miss-specifying the parametric model part.},
  archive      = {J_CSDA},
  author       = {Irène Gijbels and Rezaul Karim and Anneleen Verhasselt},
  doi          = {10.1016/j.csda.2020.107129},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107129},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric quantile regression using family of quantile-based asymmetric densities},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized co-sparse factor regression. <em>CSDA</em>,
<em>157</em>, 107127. (<a
href="https://doi.org/10.1016/j.csda.2020.107127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate regression techniques are commonly applied to explore the associations between large numbers of outcomes and predictors. In real-world applications, the outcomes are often of mixed types, including continuous measurements, binary indicators, and counts, and the observations may also be incomplete. Building upon the recent advances in mixed-outcome modeling and sparse matrix factorization , generalized co-sparse factor regression (GOFAR) is proposed, which utilizes the flexible vector generalized linear model framework and encodes the outcome dependency through a sparse singular value decomposition (SSVD) of the integrated natural parameter matrix. To avoid the estimation of the notoriously difficult joint SSVD, GOFAR proposes both sequential and parallel unit-rank estimation procedures. By combining the ideas of alternating convex search and majorization–minimization, an efficient algorithm is developed to solve the sparse unit-rank problem and implemented in the R package gofar . Extensive simulation studies and two real-world applications demonstrate the effectiveness of the proposed approach.},
  archive      = {J_CSDA},
  author       = {Aditya Mishra and Dipak K. Dey and Yong Chen and Kun Chen},
  doi          = {10.1016/j.csda.2020.107127},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107127},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized co-sparse factor regression},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast inference for semi-varying coefficient models via local
averaging. <em>CSDA</em>, <em>157</em>, 107126. (<a
href="https://doi.org/10.1016/j.csda.2020.107126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semi-varying coefficient models are widely used in the application of finance, economics, medical science and many other areas. In general, the functional coefficients are estimated by local smoothing methods, e.g. local linear estimator . So the computation cost is severe because one should point-wisely estimate the value of a coefficient function . In this paper, we give an insight into the trade-off between statistical efficiency and computation simplicity and proposes a fast inference procedure, local average estimator. The proposed method is easy to implement and avoid repeat estimation since it approximates the coefficient functions with piecewise constants. Though the local average estimator is not asymptotically optimal, it is still efficient enough for further inference. Thus, three tests are derived to check whether a coefficient is constant. The experimental evidence shows that when there is limited room for improving the asymptotic efficiency , a proper trade-off between statistical efficiency and computation simplicity may improve the finite-sample performance.},
  archive      = {J_CSDA},
  author       = {Heng Peng and Chuanlong Xie and Jingxin Zhao},
  doi          = {10.1016/j.csda.2020.107126},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107126},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast inference for semi-varying coefficient models via local averaging},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative GMM for partially linear single-index models with
partly endogenous regressors. <em>CSDA</em>, <em>156</em>, 107145. (<a
href="https://doi.org/10.1016/j.csda.2020.107145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the estimation method for the partially linear single-index model with endogenous regressors in the linear part. The Generalized Method of Moments (GMM) using instrumental variables is applied to cope with the problem that the parameter estimators may be inconsistent due to endogeneity. The GMM estimation is based on an iterative procedure, which has generalized the well known Minimum Average conditional Variance Estimation (MAVE) method, in the sense that in each iteration the estimates of the nonparametric components and the parameter vectors are obtained from the generalized moments equation instead of the least squares optimization. A specific algorithm to implement the estimation procedure concerning the choice of the instruments is provided. Asymptotic properties of the estimators are also established. Simulated experiments show that the proposed estimation method performs well in finite samples. Application to the National Longitudinal Survey of Young Men data illustrates the proposed model and method in analyzing the returns to schooling.},
  archive      = {J_CSDA},
  author       = {Hong-Fan Zhang},
  doi          = {10.1016/j.csda.2020.107145},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107145},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Iterative GMM for partially linear single-index models with partly endogenous regressors},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of multivariate longitudinal data using ARMA
cholesky and hypersphere decompositions. <em>CSDA</em>, <em>156</em>,
107144. (<a href="https://doi.org/10.1016/j.csda.2020.107144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal data with many replications, the high-order autoregressive (AR) structure of covariance matrix is required to capture the serial correlations between repeated outcomes. Thus, the high-order AR structure requires many parameters underlying the dynamic data dependence . In this paper, we proposed an autoregressive moving-average (ARMA) structure of covariance matrix involving multivariate linear models instead of the high-order AR structure of covariance matrix. We decomposed the covariance matrix using autoregressive moving-average Cholesky decomposition (ARMACD) to explain the correlations between responses at each time point , the correlation within separate responses over time, and the cross-correlation between different responses at different times. The ARMACD facilitates nonstationarity and heteroscedasticity of the covariance matrix, and the estimated covariance matrix is guaranteed to be positive definite. We illustrated the proposed methods using data derived from a study of nonalcoholic fatty liver disease.},
  archive      = {J_CSDA},
  author       = {Keunbaik Lee and Chang-Hoon Lee and Min-Sun Kwak and Eun Jin Jang},
  doi          = {10.1016/j.csda.2020.107144},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107144},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Analysis of multivariate longitudinal data using ARMA cholesky and hypersphere decompositions},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dimension-reduced semiparametric estimation of distribution
functions and quantiles with nonignorable nonresponse. <em>CSDA</em>,
<em>156</em>, 107142. (<a
href="https://doi.org/10.1016/j.csda.2020.107142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To estimate distribution functions and quantiles of a response variable when the data having nonignorable nonresponse and the dimension of covariate is not low, this article assumes that the propensity follows a general semiparametric model, but the distribution of the response variable and related covariates is unspecified. To address the identifiability problem, an instrumental covariate, which is related to the response variable but unrelated to the propensity given the response variable and other covariates, is used to construct sufficient instrumental estimating equations. Three different semiparametric estimation methods are developed based on inverse probability weighting, mean imputation, and augmented inverse probability weighting. Furthermore to improve the efficiency and alleviate the curse of dimensionality , the sufficient dimension reduction technique is employed to produce efficient kernel estimation, and a class of dimension-reduced estimators for distribution functions and quantiles is proposed. Consistency and asymptotic normality of the proposed estimators are established. It can be shown that these estimators are asymptotically equivalent. The finite-sample performance of the estimators is studied through simulation, and an application to HIV-CD4 data set is also presented.},
  archive      = {J_CSDA},
  author       = {Lei Wang and Puying Zhao and Jun Shao},
  doi          = {10.1016/j.csda.2020.107142},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107142},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dimension-reduced semiparametric estimation of distribution functions and quantiles with nonignorable nonresponse},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal network learning with non-invertible functional
relationships. <em>CSDA</em>, <em>156</em>, 107141. (<a
href="https://doi.org/10.1016/j.csda.2020.107141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovery of causal relationships from observational data is an important problem in many areas. Several recent results have established the identifiability of causal directed acyclic graphs (DAGs) with non-Gaussian and/or nonlinear structural equation models (SEMs). Focusing on nonlinear SEMs defined by non-invertible functions, which exist in many data domains, a novel test is proposed for non-invertible bivariate causal models . Algorithms are further developed to incorporate this test in structure learning of DAGs that contain both linear and nonlinear causal relations . Extensive numerical comparisons show that the proposed algorithms outperform existing DAG learning methods in identifying causal graphical structures. The practical application of the methods is illustrated by learning causal networks for combinatorial binding of transcription factors from ChIP-Seq data.},
  archive      = {J_CSDA},
  author       = {Bingling Wang and Qing Zhou},
  doi          = {10.1016/j.csda.2020.107141},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107141},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Causal network learning with non-invertible functional relationships},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graphical modelling and partial characteristics for
multitype and multivariate-marked spatio-temporal point processes.
<em>CSDA</em>, <em>156</em>, 107139. (<a
href="https://doi.org/10.1016/j.csda.2020.107139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method for dealing with multivariate analysis of marked spatio-temporal point processes is presented by introducing different partial point characteristics, and by extending the spatial dependence graph model formalism. The approach yields a unified framework for different types of spatio-temporal data, including both, purely qualitatively (multivariate) cases and multivariate cases with additional quantitative marks. The proposed graphical model is defined through partial spectral density characteristics; it is highly computationally efficient and reflects the conditional similarity amongst sets of spatio-temporal sub-processes of either points or marked points with identical discrete marks. Two applications, on crime and forestry data, are presented.},
  archive      = {J_CSDA},
  author       = {Matthias Eckardt and Jonatan A. González and Jorge Mateu},
  doi          = {10.1016/j.csda.2020.107139},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107139},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Graphical modelling and partial characteristics for multitype and multivariate-marked spatio-temporal point processes},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dimension reduction in binary response regression: A joint
modeling approach. <em>CSDA</em>, <em>156</em>, 107131. (<a
href="https://doi.org/10.1016/j.csda.2020.107131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Categorical responses cause no conceptual complications for dimension reduction in regression, but the performance of some methods may suffer in this context and hence supervised dimension reduction in practice must recognize the nature of the response. Using a continuous latent variable to represent an unobserved response underlying the binary response , a joint model is proposed for dimension reduction in binary regression. The minimal sufficient linear reduction is obtained, and an efficient expectation maximization algorithm is developed for carrying out maximum likelihood estimation. Simulated examples and an application to a dataset concerning the identification of handwritten digits are presented to compare the performance of the proposed method with that of existing methods.},
  archive      = {J_CSDA},
  author       = {Junlan Li and Tao Wang},
  doi          = {10.1016/j.csda.2020.107131},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107131},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dimension reduction in binary response regression: A joint modeling approach},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A nonparametric empirical bayes approach to large-scale
multivariate regression. <em>CSDA</em>, <em>156</em>, 107130. (<a
href="https://doi.org/10.1016/j.csda.2020.107130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate regression has many applications, ranging from time series prediction to genomics. Borrowing information across the outcomes can improve prediction error, even when outcomes are statistically independent. Many methods exist to implement this strategy, for example the multiresponse lasso, but choosing the optimal method for a given dataset is difficult. These issues are addressed by establishing a connection between multivariate linear regression and compound decision problems. A nonparametric empirical Bayes procedure that can learn the optimal regression method from the data itself is proposed. Furthermore, the proposed procedure is free of tuning parameters and performs well in simulations and in a multiple stock price prediction problem.},
  archive      = {J_CSDA},
  author       = {Yihe Wang and Sihai Dave Zhao},
  doi          = {10.1016/j.csda.2020.107130},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107130},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A nonparametric empirical bayes approach to large-scale multivariate regression},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Density estimation on a network. <em>CSDA</em>,
<em>156</em>, 107128. (<a
href="https://doi.org/10.1016/j.csda.2020.107128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach is proposed for density estimation on a network. Nonparametric density estimation on a network is formulated as a nonparametric regression problem by binning . Nonparametric regression using local polynomial kernel-weighted least squares have been studied rigorously, and its asymptotic properties make it superior to kernel estimators such as the Nadaraya–Watson estimator. When applied to a network, the best estimator near a vertex depends on the amount of smoothness at the vertex. Often, there are no compelling reasons to assume that a density will be continuous or discontinuous at a vertex, hence a data driven approach is proposed. To estimate the density in a neighborhood of a vertex, a two-step procedure is proposed. The first step of this pretest estimator fits a separate local polynomial regression on each edge using data only on that edge, and then tests for equality of the estimates at the vertex. If the null hypothesis is not rejected, then the second step re-estimates the regression function in a small neighborhood of the vertex, subject to a joint equality constraint. Since the derivative of the density may be discontinuous at the vertex, a piecewise polynomial local regression estimate is used to model the change in slope. The special case of local piecewise linear regression is studied in detail and the leading bias and variance terms are derived using weighted least squares theory. The proposed approach will remove the bias near a vertex that has been noted for existing methods, which typically do not allow for discontinuity at vertices. For a fixed network, the proposed method scales sub-linearly with sample size and it can be extended to regression and varying coefficient models on a network. The working of the proposed model is demonstrated by simulation studies and applications to a dendrite network dataset.},
  archive      = {J_CSDA},
  author       = {Yang Liu and David Ruppert},
  doi          = {10.1016/j.csda.2020.107128},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107128},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Density estimation on a network},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decrement rates and a numerical method under competing
risks. <em>CSDA</em>, <em>156</em>, 107125. (<a
href="https://doi.org/10.1016/j.csda.2020.107125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the interactions of competing risks that affect the occurrence of various decrements such as death or disease is an essential issue in survival analysis and actuarial science . Popular assumptions for the construction of decrement models are uniform distributions of decrements in a multiple decrement table (mUDD) and associated single decrement tables (sUDD), respectively. Even though there are many theoretical generalizations to relaxing mUDD assumption, it is not clear how to obtain theoretical and numerical methods for modeling relationships of competing risks under a general assumption in associated single decrement tables beyond the sUDD. We fill this gap in the literature by discussing the conversion between probabilities of decrement and absolute rates under a general form of a distribution of fractional ages. In particular, we show that extracting absolute rates from the probabilities under the general competing risk assumption boils down to solving a system of non-linear equations and propose a novel numerical algorithm for its solution. Extensive numerical experiments relying on the algorithm verify that the algorithm delivers reliable results in terms of efficiency and accuracy.},
  archive      = {J_CSDA},
  author       = {Hangsuck Lee and Hongjun Ha and Taewon Lee},
  doi          = {10.1016/j.csda.2020.107125},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107125},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Decrement rates and a numerical method under competing risks},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast stable parameter estimation for linear dynamical
systems. <em>CSDA</em>, <em>156</em>, 107124. (<a
href="https://doi.org/10.1016/j.csda.2020.107124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamical systems describe changes in processes that arise naturally from their underlying physical principles, such as the laws of motion or the conservation of mass , energy or momentum. These models facilitate a causal explanation for the drivers and impediments of the processes. Extracting these governing equations from data is a central challenge in many diverse areas of science and engineering. A methodology for estimating the solution; and the parameters of linear dynamical systems from incomplete and noisy observations of the processes is introduced. Building on the parameter cascading approach, where a linear combination of basis functions approximates the implicitly defined solution of the dynamical system. The systems’ parameters are estimated so that this approximating solution adheres to the data. By taking advantage of the linearity of the system, the parameter cascading estimation procedure is simplified, and by developing a new iterative scheme , fast and stable computation is achieved. An illustrative example obtains a linear differential equation that represents real data from biomechanics . A comparison of the proposed approach with popular methods for estimating the parameters of linear dynamical systems, namely, the non-linear least-squares approach, simulated annealing, parameter cascading and smooth functional tempering reveals a considerable reduction in computation and an improved bias and sampling variance.},
  archive      = {J_CSDA},
  author       = {M. Carey and J.O. Ramsay},
  doi          = {10.1016/j.csda.2020.107124},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107124},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast stable parameter estimation for linear dynamical systems},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering for time-varying relational count data.
<em>CSDA</em>, <em>156</em>, 107123. (<a
href="https://doi.org/10.1016/j.csda.2020.107123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational count data are often obtained from sources such as simultaneous purchase in online shops and social networking service information. Clustering such relational count data reveals the latent structure of the relationship between objects such as household items or people. When relational count data observed at multiple time points are available, it is worthwhile incorporating the time structure into the clustering result to understand how objects move between the clusters over time. In this paper, we propose two clustering methods for analyzing time-varying relational count data. The first model, the dynamic Poisson infinite relational model (dPIRM), handles time-varying relational count data. In the second model, which we call the dynamic zero-inflated Poisson infinite relational model, we further extend the dPIRM so that it can handle zero-inflated data. Proposing both two models is important as zero-inflated data are often encountered, especially when the time intervals are short. In addition, by explicitly deriving the relevant full conditional distributions , we describe the features of the estimated parameters and, in turn, the relationship between the two models. We show the effectiveness of both models through a simulation study and a real data example.},
  archive      = {J_CSDA},
  author       = {Satoshi Goto and Mariko Takagishi and Hiroshi Yadohisa},
  doi          = {10.1016/j.csda.2020.107123},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107123},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Clustering for time-varying relational count data},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable selection for generalized odds rate mixture cure
models with interval-censored failure time data. <em>CSDA</em>,
<em>156</em>, 107115. (<a
href="https://doi.org/10.1016/j.csda.2020.107115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection for failure time data with a cured fraction has been discussed by many authors but most of existing methods apply only to right-censored failure time data. In this paper, we consider variable selection when one faces interval-censored failure time data arising from a general class of generalized odds rate mixture cure models, and we propose a penalized variable selection method by maximizing a derived penalized likelihood function. In the method, the sieve approach is employed to approximate the unknown function, and it is implemented using a novel penalized expectation–maximization (EM) algorithm. Also the asymptotic properties of the proposed estimators of regression parameters , including the oracle property, are obtained. Furthermore, a simulation study is conducted to assess the finite sample performance of the proposed method, and the results indicate that it works well in practice. Finally, the approach is applied to a set of real data on childhood mortality taken from the Nigeria Demographic and Health Survey.},
  archive      = {J_CSDA},
  author       = {Yang Xu and Shishun Zhao and Tao Hu and Jianguo Sun},
  doi          = {10.1016/j.csda.2020.107115},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107115},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection for generalized odds rate mixture cure models with interval-censored failure time data},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Angle-based cost-sensitive multicategory classification.
<em>CSDA</em>, <em>156</em>, 107107. (<a
href="https://doi.org/10.1016/j.csda.2020.107107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world classification problems come with costs which can vary for different types of misclassification . It is thus important to develop cost-sensitive classifiers which minimize the total misclassification cost. Although binary cost-sensitive classifiers have been well-studied, solving multicategory classification problems is still challenging. A popular approach to address this issue is to construct K K classification functions for a K K -class problem and remove the redundancy by imposing a sum-to-zero constraint. However, such method usually results in higher computational complexity and inefficient algorithms. In this article, we propose a novel angle-based cost-sensitive classification framework for multicategory classification without the sum-to-zero constraint. Loss functions that included in the angle-based cost-sensitive classification framework are further justified to be Fisher consistent. To show the usefulness of the framework, two cost-sensitive multicategory boosting algorithms are derived as concrete instances. Numerical experiments demonstrate that the proposed boosting algorithms yield competitive classification performances against other existing boosting approaches.},
  archive      = {J_CSDA},
  author       = {Yi Yang and Yuxuan Guo and Xiangyu Chang},
  doi          = {10.1016/j.csda.2020.107107},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107107},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Angle-based cost-sensitive multicategory classification},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian regularization of gaussian graphical models with
measurement error. <em>CSDA</em>, <em>156</em>, 107085. (<a
href="https://doi.org/10.1016/j.csda.2020.107085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A framework for determining and estimating the conditional pairwise relationships of variables in high dimensional settings when the observed samples are contaminated with measurement error is proposed. The framework is motivated by the task of establishing gene regulatory networks from microarray studies, in which measurements are taken for a large number of genes from a small sample size, but often measured imperfectly. When no measurement error is present, this problem is often solved by estimating the precision matrix under sparsity constraints. However, when measurement error is present, not correcting for it leads to inconsistent estimates of the precision matrix and poor identification of relationships. To this end, a recent iterative imputation technique developed in the context of missing data is utilized to correct for the biases in the estimates imposed from the contamination. This technique is showcased with a recent variant of the spike-and-slab Lasso to obtain a point estimate of the precision matrix. Simulation studies show that the new method outperforms the naïve method that ignores measurement error in both identification and estimation accuracy. The new method is applied to establish a conditional gene network from a microarray dataset.},
  archive      = {J_CSDA},
  author       = {Michael Byrd and Linh H. Nghiem and Monnie McGee},
  doi          = {10.1016/j.csda.2020.107085},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107085},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian regularization of gaussian graphical models with measurement error},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The impact of genetic diversity statistics on model
selection between coalescents. <em>CSDA</em>, <em>156</em>, 107055. (<a
href="https://doi.org/10.1016/j.csda.2020.107055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling genetic diversity needs an underlying genealogy model. To choose a fitting model based on genetic data, one can perform model selection between classes of genealogical trees, e.g. Kingman’s coalescent with exponential growth or multiple merger coalescents. Such selection can be based on many different statistics measuring genetic diversity. A random forest based Approximate Bayesian Computation is used to disentangle the effects of different statistics on distinguishing between various classes of genealogy models. For the specific question of inferring whether genealogies feature multiple mergers, a new statistic, the minimal observable clade size, is introduced. When combined with classical site frequency based statistics, it reduces classification errors considerably.},
  archive      = {J_CSDA},
  author       = {Fabian Freund and Arno Siri-Jégousse},
  doi          = {10.1016/j.csda.2020.107055},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107055},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The impact of genetic diversity statistics on model selection between coalescents},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient and robust estimation of regression and scale
parameters, with outlier detection. <em>CSDA</em>, <em>155</em>, 107114.
(<a href="https://doi.org/10.1016/j.csda.2020.107114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression with normally distributed errors – including particular cases such as ANOVA, Student’s t t -test or location–scale inference – is a widely used statistical procedure. In this case the ordinary least squares estimator possesses remarkable properties but is very sensitive to outliers. Several robust alternatives have been proposed, but there is still significant room for improvement. An original method of estimation is thus proposed, which offers high efficiency simultaneously in the absence and the presence of outliers, both for the estimation of the regression coefficients and the scale parameter. The approach first consists in broadening the normal assumption of the errors to a mixture of the normal and the filtered-log-Pareto (FLP), an original distribution designed to represent the outliers. The expectation–maximization (EM) algorithm is then adapted, which yields the N–FLP estimators of the regression coefficients , the scale parameter and the proportion of outliers, along with probabilities of each observation being an outlier. The performance of the N–FLP estimators is compared with the best alternatives in an extensive Monte Carlo simulation . It is shown that this method of estimation can also be used for a complete robust inference, including confidence intervals, hypothesis testing and model selection.},
  archive      = {J_CSDA},
  author       = {Alain Desgagné},
  doi          = {10.1016/j.csda.2020.107114},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107114},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient and robust estimation of regression and scale parameters, with outlier detection},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel cross-validation: A scalable fitting method for
gaussian process models. <em>CSDA</em>, <em>155</em>, 107113. (<a
href="https://doi.org/10.1016/j.csda.2020.107113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) models are widely used to analyze spatially referenced data and to predict values at locations without observations. They are based on a statistical framework, which enables uncertainty quantification of the model structure and predictions. Both the evaluation of the likelihood and the prediction involve solving linear systems. Hence, the computational costs are large and limit the amount of data that can be handled. While there are many approximation strategies that lower the computational cost of GP models, they often provide sub-optimal support for the parallel computing capabilities of (high-performance) computing environments. To bridge this gap a parallelizable parameter estimation and prediction method is presented. The key idea is to divide the spatial domain into overlapping subsets and to use cross-validation (CV) to estimate the covariance parameters in parallel. Although simulations show that CV is less effective for parameter estimation than the maximum likelihood method , it is amenable to parallel computing and enables the handling of large datasets. Exploiting the screen effect for spatial prediction helps to arrive at a spatial analysis that is close to a global computation despite performing parallel computations on local regions. Simulation studies assess the accuracy of the parameter estimates and predictions. The implementation shows good weak and strong parallel scaling properties. For illustration, an exponential covariance model is fitted to a scientifically relevant canopy height dataset with 5 million observations. Using 512 processor cores in parallel brings the evaluation time of one covariance parameter configuration to 1.5 minutes.},
  archive      = {J_CSDA},
  author       = {Florian Gerber and Douglas W. Nychka},
  doi          = {10.1016/j.csda.2020.107113},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107113},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Parallel cross-validation: A scalable fitting method for gaussian process models},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable selection in high-dimensional linear model with
possibly asymmetric errors. <em>CSDA</em>, <em>155</em>, 107112. (<a
href="https://doi.org/10.1016/j.csda.2020.107112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many application areas, the problem of the automatic variable selection in a linear model with asymmetric errors is encountered, when the number of explanatory variables diverges with the sample size. For this high-dimensional model, the penalized least squares method is not appropriate and the quantile framework makes the inference more difficult because of the non differentiability of the loss function. An estimation method by penalizing the expectile process with an adaptive LASSO penalty is proposed and studied. Two cases are considered: first with the number of model parameters is assumed to be much smaller than the sample size and afterwards it could be of the same order; the two cases being distinct by the adaptive penalties considered. For each case, the rate convergence is obtained and the oracle properties of the adaptive LASSO expectile estimator are established. The proposed estimators are evaluated through Monte Carlo simulations and compared with the adaptive LASSO quantile estimator. The proposed estimation method is also applied to real data in genetics.},
  archive      = {J_CSDA},
  author       = {Gabriela Ciuperca},
  doi          = {10.1016/j.csda.2020.107112},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107112},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection in high-dimensional linear model with possibly asymmetric errors},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A beyond multiple robust approach for missing response
problem. <em>CSDA</em>, <em>155</em>, 107111. (<a
href="https://doi.org/10.1016/j.csda.2020.107111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imputation and the inverse probability weighting are two commonly used approaches in missing data analysis. Parametric versions of them are not robust due to model misspecification of some unknown functions. Nonparametric ones are robust but are impractical when the number of covariates is large due to the problem of “curse of dimension”. A beyond multiple robust method is proposed in this paper. This method balances the parametric and nonparametric methods by using some model information contained in the outcome regression function and the selection probability function, and hence alleviates the model misspecification problem and “curse of dimension” problem simultaneously. To illustrate the proposed method, we focus on the estimating problem of response mean in the presence of missing responses. A beyond multiple robust estimator of the response mean is defined, which is proved to be consistent and asymptotically normal as long as one of the true models for the outcome regression or selection probability functions can be some function of its assumed models, without the requirement that one of the true models is correctly specified. Also, it is shown that the asymptotic variance of the proposed estimator is equal to the semiparametric efficiency bound established by Hahn (1998, Econometrica, pp 315–331) when both the selection probability function and the outcome regression function are the functions of their assumed models, respectively. The finite sample properties of the proposed estimator are evaluated by simulation studies and the proposed method is illustrated by a real data analysis.},
  archive      = {J_CSDA},
  author       = {Qihua Wang and Miaomiao Su and Ruoyu Wang},
  doi          = {10.1016/j.csda.2020.107111},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107111},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A beyond multiple robust approach for missing response problem},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint generalized estimating equations for longitudinal
binary data. <em>CSDA</em>, <em>155</em>, 107110. (<a
href="https://doi.org/10.1016/j.csda.2020.107110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling longitudinal binary data is challenging but common in practice. Existing methods on modeling of binary responses take no account of the fact that the correlation coefficient of binary responses must have an upper bound which is smaller than one. Ignoring this fact can lead to incorrect statistical inferences for longitudinal binary data. A novel method is proposed to model the mean and within-subject correlation coefficients for longitudinal binary data, simultaneously, by taking into account the constraints of the upper bounds. By introducing latent normally distributed random variables , the correlation coefficients of binary responses are connected to those for the latent variables, of which the correlation coefficients are modeled accordingly. A joint generalized estimating equation (GEE) method is developed for this purpose and the resulting correlation coefficients are shown to satisfy the constraints. Asymptotic normality of the parameter estimators is derived and simulation studies are made under various scenarios, showing that the proposed joint GEE method works very well even if the working covariance structures are misspecified. For illustration, the proposed method is applied to two real data practices to assess the effects of covariates on the mean and within-subject correlation coefficients.},
  archive      = {J_CSDA},
  author       = {Youjun Huang and Jianxin Pan},
  doi          = {10.1016/j.csda.2020.107110},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107110},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Joint generalized estimating equations for longitudinal binary data},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Outer power transformations of hierarchical archimedean
copulas: Construction, sampling and estimation. <em>CSDA</em>,
<em>155</em>, 107109. (<a
href="https://doi.org/10.1016/j.csda.2020.107109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outer power (OP) transformations of Archimedean generators are suggested to increase the modeling flexibility and statistical fitting capabilities of classical Archimedean copulas restricted to a single parameter. For OP-transformed Archimedean copulas, a formula for computing tail dependence coefficients is obtained, as well as two feasible OP Archimedean copula estimators are proposed and their properties studied by simulation. For hierarchical extensions of OP-transformed Archimedean copulas under the sufficient nesting condition, a new construction principle, efficient sampling and parameter estimation for models based on a single one-parameter Archimedean family are addressed. Special attention is paid to the case where the sufficient nesting condition simplifies to two types of restrictions on the corresponding parameters. By simulation, the convergence rate and standard errors of the proposed estimator are studied. Excellent tail fitting capabilities of OP-transformed hierarchical Archimedean copula models are demonstrated in a risk management application. The results show that the OP transformation is able to improve the statistical fit of exchangeable Archimedean copulas, particularly of those that cannot capture upper tail dependence or strong concordance, as well as the statistical fit of hierarchical Archimedean copulas, especially in terms of tail dependence and higher dimensions. Given how comparably simple it is to include OP transformations into existing exchangeable and hierarchical Archimedean copula models, OP transformations provide an attractive trade-off between computational effort and statistical improvement.},
  archive      = {J_CSDA},
  author       = {Jan Górecki and Marius Hofert and Ostap Okhrin},
  doi          = {10.1016/j.csda.2020.107109},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107109},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Outer power transformations of hierarchical archimedean copulas: Construction, sampling and estimation},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Functional time series model identification and diagnosis by
means of auto- and partial autocorrelation analysis. <em>CSDA</em>,
<em>155</em>, 107108. (<a
href="https://doi.org/10.1016/j.csda.2020.107108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying the serial correlation across time lags is a crucial step in the identification and diagnosis of a time series model . Simple and partial autocorrelation functions of the time series are the most widely used tools for this purpose with scalar time series. Nevertheless, there is a lack of an established method for the identification of functional time series (FTS) models. Functional versions of the autocorrelation and partial autocorrelation functions for FTS based on the L 2 L2 norm of the lagged autocovariance operators of the series are proposed. Diagnostic plots of these functions coupled with prediction bounds derived from large sample results for the autocorrelation and partial autocorrelation functions estimated from a strong functional white noise series are proposed as fast and efficient tools for selecting the order and assessing the adequacy of functional SARMAX models. These methods are studied in numerical simulations with both white noise and serially correlated functional processes, which show that the structure of the processes can be well identified using the proposed techniques. The applicability of the method is illustrated with two real-world datasets: Eurodollar futures contracts and electricity price profiles.},
  archive      = {J_CSDA},
  author       = {Guillermo Mestre and José Portela and Gregory Rice and Antonio Muñoz San Roque and Estrella Alonso},
  doi          = {10.1016/j.csda.2020.107108},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107108},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Functional time series model identification and diagnosis by means of auto- and partial autocorrelation analysis},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smooth simultaneous confidence band for the error
distribution function in nonparametric regression. <em>CSDA</em>,
<em>155</em>, 107106. (<a
href="https://doi.org/10.1016/j.csda.2020.107106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A smooth simultaneous confidence band (SCB) is constructed for the distribution of unobserved errors in a nonparametric regression model based on a plug-in kernel distribution estimator. The normalized estimation error process is shown to converge to a Gaussian process . Simulation experiments indicate that the proposed SCB not only strikes an intelligent balance between coverage probability and precision, but also achieves surprisingly as much as double efficiency of the classical infeasible SCB. Furthermore, extensive empirical studies are carried out to compare the proposed method with the smooth residual bootstrap method in order to demonstrate the usefulness of each of these methods. As an illustration, the proposed SCB is applied to the Old Faithful geyser data for testing the error distribution.},
  archive      = {J_CSDA},
  author       = {Lijie Gu and Suojin Wang and Lijian Yang},
  doi          = {10.1016/j.csda.2020.107106},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107106},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Smooth simultaneous confidence band for the error distribution function in nonparametric regression},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A self-calibrated direct approach to precision matrix
estimation and linear discriminant analysis in high dimensions.
<em>CSDA</em>, <em>155</em>, 107105. (<a
href="https://doi.org/10.1016/j.csda.2020.107105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A self-calibrated direct estimation algorithm based on ℓ 1 ℓ1 -regularized quadratic programming is proposed. The self-calibration is achieved by an iterative algorithm for finding the regularization parameter simultaneously with the estimation target. The proposed algorithm is free of cross-validation. Two applications of this algorithm are proposed, namely precision matrix estimation and linear discriminant analysis . It is proven that the proposed estimators are consistent under different matrix norm errors and misclassification rate . Moreover, extensive simulation and empirical studies are conducted to evaluate the finite-sample performance and examine the support recovery ability of the proposed estimators. With the theoretical and empirical evidence, it is shown that the proposed estimator is better than its competitors in statistical accuracy and has clear computational advantages.},
  archive      = {J_CSDA},
  author       = {Chi Seng Pun and Matthew Zakharia Hadimaja},
  doi          = {10.1016/j.csda.2020.107105},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107105},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A self-calibrated direct approach to precision matrix estimation and linear discriminant analysis in high dimensions},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian goodness-of-fit test for regression.
<em>CSDA</em>, <em>155</em>, 107104. (<a
href="https://doi.org/10.1016/j.csda.2020.107104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression models are widely used statistical procedures, and the validation of their assumptions plays a crucial role in the data analysis process. Unfortunately, validating assumptions usually depends on the availability of tests tailored to the specific model of interest. A novel Bayesian goodness-of-fit hypothesis testing approach is presented for a broad class of regression models the response variable of which is univariate and continuous. The proposed approach relies on a suitable transformation of the response variable and a Bayesian prior induced by a predictor-dependent mixture model. Hypothesis testing is performed via Bayes factor , the asymptotic properties of which are discussed. The method is implemented by means of a Markov chain Monte Carlo algorithm, and its performance is illustrated using simulated and real data sets .},
  archive      = {J_CSDA},
  author       = {Andrés F. Barrientos and Antonio Canale},
  doi          = {10.1016/j.csda.2020.107104},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107104},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A bayesian goodness-of-fit test for regression},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inference for inter-arrival times of extreme
events in bursty time series. <em>CSDA</em>, <em>155</em>, 107096. (<a
href="https://doi.org/10.1016/j.csda.2020.107096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many complex systems studied in statistical physics, inter-arrival times between events such as solar flares, trades and neuron voltages follow a heavy-tailed distribution. The set of event times is fractal-like, being dense in some time windows and empty in others, a phenomenon which has been dubbed “bursty”. A new model for the inter-exceedance times of such events above high thresholds is proposed. For high thresholds and infinite-mean waiting times, it is shown that the times between threshold crossings are Mittag-Leffler distributed, and thus form a “fractional Poisson Process” which generalizes the standard Poisson Process of threshold exceedances. Graphical means of estimating model parameters and assessing model fit are provided. The inference method is applied to an empirical bursty time series, and it is shown how the memory of the Mittag-Leffler distribution affects prediction of the time until the next extreme event.},
  archive      = {J_CSDA},
  author       = {Katharina Hees and Smarak Nayak and Peter Straka},
  doi          = {10.1016/j.csda.2020.107096},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107096},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Statistical inference for inter-arrival times of extreme events in bursty time series},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust variable selection with exponential squared loss for
the spatial autoregressive model. <em>CSDA</em>, <em>155</em>, 107094.
(<a href="https://doi.org/10.1016/j.csda.2020.107094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial dependent data frequently occur in spatial econometrics and endemiology. In this work, we propose a class of penalized robust regression estimators based on exponential squared loss with independent and identical distributed errors for general spatial autoregressive models . A penalized exponential squared loss with the adaptive lasso penalty is employed for simultaneous model selection and parameter estimation. Under mild conditions, we establish the asymptotic and oracle property of the proposed estimators The induced nonconvex nondifferentiable mathematical programming offer challenges for solving algorithms. We specially design a block coordinate descent (BCD) algorithm equipped with CCCP procedure for efficiently solving the subproblem . Moreover, we provide a convergence guarantee of the BCD algorithm. Every limit point of the iterated solutions is proved a stationary point. We also present a convergence speed of spatial weight ρ k ρk . Numerical studies illustrate that the proposed method is particularly robust and applicable when the outliers or intensive noise exist in the observations or the estimated spatial weight matrix is inaccurate. All the source code could be freely downloaded from https://github.com/Isaac-QiXing/SAR .},
  archive      = {J_CSDA},
  author       = {Yunquan Song and Xijun Liang and Yanji Zhu and Lu Lin},
  doi          = {10.1016/j.csda.2020.107094},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107094},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust variable selection with exponential squared loss for the spatial autoregressive model},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Link-based survival additive models under mixed censoring to
assess risks of hospital-acquired infections. <em>CSDA</em>,
<em>155</em>, 107092. (<a
href="https://doi.org/10.1016/j.csda.2020.107092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of methods available to model survival data only deal with right censoring. However, there are many applications where left, right and/or interval censoring simultaneously occur. A methodology that is capable of handling all types of censoring as well as flexibly estimating several types of covariate effects is presented. The baseline hazard is modelled through monotonic P-splines. The model’s parameters are estimated using an efficient and stable penalised likelihood algorithm. The proposed framework is evaluated in simulation, and illustrated using an original data example on time to first hospital infection or in-hospital death in cirrhotic patients. A peak of risk in the first week since hospitalisation is identified, together with a non-linear effect of Model for End-Stage Liver Disease (MELD) score. The GJRM R package, with an implementation of our approach, is freely available on CRAN.},
  archive      = {J_CSDA},
  author       = {Giampiero Marra and Alessio Farcomeni and Rosalba Radice},
  doi          = {10.1016/j.csda.2020.107092},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107092},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Link-based survival additive models under mixed censoring to assess risks of hospital-acquired infections},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simplified r-vine based forward regression. <em>CSDA</em>,
<em>155</em>, 107091. (<a
href="https://doi.org/10.1016/j.csda.2020.107091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extension of the D-vine based forward regression procedure to a R-vine forward regression is proposed. In this extension any R-vine structure can be taken into account. Moreover, a new heuristic is proposed to determine which R-vine structure is the most appropriate to model the conditional distribution of the response variable given the covariates . It is shown in the simulation that the performance of the heuristic is comparable to the D-vine based approach. Furthermore, it is explained how to extend the heuristic into a situation when more than one response variable are of interest. Finally, the proposed R-vine regression is applied to perform a stress analysis on the manufacturing sector which shows its impact on the whole economy.},
  archive      = {J_CSDA},
  author       = {Kailun Zhu and Dorota Kurowicka and Gabriela F. Nane},
  doi          = {10.1016/j.csda.2020.107091},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107091},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Simplified R-vine based forward regression},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MM algorithms for distance covariance based sufficient
dimension reduction and sufficient variable selection. <em>CSDA</em>,
<em>155</em>, 107089. (<a
href="https://doi.org/10.1016/j.csda.2020.107089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction (SDR) using distance covariance (DCOV) was recently proposed as an approach to dimension-reduction problems. Compared with other SDR methods , it is model-free without estimating link function and does not require any particular distributions on predictors. However, the DCOV-based SDR method involves optimizing a nonsmooth and nonconvex objective function over the Stiefel manifold . To tackle the numerical challenge, the original objective function is equivalently formulated into a DC (Difference of Convex functions) program and an iterative algorithm based on the majorization–minimization (MM) principle is constructed. At each step of the MM algorithm, one iteration of Riemannian Newton’s method is taken to solve the quadratic subproblem on the Stiefel manifold inexactly. In addition, the algorithm can also be readily extended to sufficient variable selection (SVS) using distance covariance. Finally, the convergence property of the proposed algorithm under some regularity conditions is established. Simulation and real data analysis show our algorithm drastically improves the computation efficiency and is robust across various settings compared with the existing method. Matlab codes implementing our methods and scripts for regenerating the numerical results are available at https://github.com/runxiong-wu/MMRN .},
  archive      = {J_CSDA},
  author       = {Runxiong Wu and Xin Chen},
  doi          = {10.1016/j.csda.2020.107089},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107089},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {MM algorithms for distance covariance based sufficient dimension reduction and sufficient variable selection},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linearly preconditioned nonlinear conjugate gradient
acceleration of the PX-EM algorithm. <em>CSDA</em>, <em>155</em>,
107056. (<a href="https://doi.org/10.1016/j.csda.2020.107056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The EM algorithm is a widely applicable algorithm for modal estimation but often criticized for its slow convergence. A new hybrid accelerator named APX-EM is proposed for speeding up the convergence of EM algorithm , which is based on both Linearly Preconditioned Nonlinear Conjugate Gradient (PNCG) and PX-EM algorithm. The intuitive idea is that, each step of the PX-EM algorithm can be viewed approximately as a generalized gradient just like the EM algorithm, then the linearly PNCG method can be used to accelerate the EM algorithm. Essentially, this method is an adjustment of the AEM algorithm, and it usually achieves a faster convergence rate than the AEM algorithm by sacrificing a little simplicity. The convergence of the APX-EM algorithm, includes a global convergence result for this method under suitable conditions, is discussed. This method is illustrated for factor analysis and a random-effects model.},
  archive      = {J_CSDA},
  author       = {Lin Zhou and Yayong Tang},
  doi          = {10.1016/j.csda.2020.107056},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107056},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Linearly preconditioned nonlinear conjugate gradient acceleration of the PX-EM algorithm},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of incident dynamic AUC in practice.
<em>CSDA</em>, <em>154</em>, 107095. (<a
href="https://doi.org/10.1016/j.csda.2020.107095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incident/dynamic time-dependent AUC (Area Under the ROC Curve) is an appealing measure to express the discriminative value of a dynamic survival model over time. However, estimation of this measure is not straightforward. Four recently proposed estimation approaches are studied. In an extensive simulation study, a head-to-head comparison between these four estimation methods is made. The estimation algorithms of some of the methods are extended. Results are illustrated with a motivating dynamic survival model from Reproductive Medicine.},
  archive      = {J_CSDA},
  author       = {N. van Geloven and Y. He and A.H. Zwinderman and H. Putter},
  doi          = {10.1016/j.csda.2020.107095},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107095},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of incident dynamic AUC in practice},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general robust t-process regression model. <em>CSDA</em>,
<em>154</em>, 107093. (<a
href="https://doi.org/10.1016/j.csda.2020.107093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gaussian process regression (GPR) model is well-known to be susceptible to outliers. Robust process regression models based on t-process or other heavy-tailed processes have been developed to address the problem. However, due to the current definitions of heavy-tailed processes, the unknown process regression function and the random errors are always defined jointly. This definition, mainly owing to mix-up of the regression function modeling and the distribution of the random errors, is not justified in many practical problems and thus limits the application of those robust approaches. It also results in a limitation of the statistical properties and robust analysis. A general robust process regression model is proposed by separating the nonparametric regression model from the distribution assumption of the random error. An efficient estimation procedure is developed. It shows that the estimated random-effects are useful in detecting outlying curves. Statistical properties, such as unbiasedness and information consistency, are provided. Numerical studies show that the proposed method is robust against outliers and outlying curves, and has a better performance in prediction compared with the existing models.},
  archive      = {J_CSDA},
  author       = {Zhanfeng Wang and Maengseok Noh and Youngjo Lee and Jian Qing Shi},
  doi          = {10.1016/j.csda.2020.107093},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107093},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A general robust t-process regression model},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Goodness-of-fit test for latent block models. <em>CSDA</em>,
<em>154</em>, 107090. (<a
href="https://doi.org/10.1016/j.csda.2020.107090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent block models are used for probabilistic biclustering, which is shown to be an effective method for analyzing various relational data sets. However, there has been no statistical test method for determining the row and column cluster numbers of latent block models. Recent studies have constructed statistical-test-based methods for stochastic block models, which assume that the observed matrix is a square symmetric matrix and that the cluster assignments are the same for rows and columns. In this study, we developed a new goodness-of-fit test for latent block models to test whether an observed data matrix fits a given set of row and column cluster numbers, or it consists of more clusters in at least one direction of the row and the column. To construct the test method, we used a result from the random matrix theory for a sample covariance matrix . We experimentally demonstrated the effectiveness of the proposed method by showing the asymptotic behavior of the test statistic and measuring the test accuracy.},
  archive      = {J_CSDA},
  author       = {Chihiro Watanabe and Taiji Suzuki},
  doi          = {10.1016/j.csda.2020.107090},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107090},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Goodness-of-fit test for latent block models},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Laplace approximations for fast bayesian inference in
generalized additive models based on p-splines. <em>CSDA</em>,
<em>154</em>, 107088. (<a
href="https://doi.org/10.1016/j.csda.2020.107088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized additive models (GAMs) are a well-established statistical tool for modeling complex nonlinear relationships between covariates and a response assumed to have a conditional distribution in the exponential family . To make inference in this model class, a fast and flexible approach is considered based on Bayesian P-splines and the Laplace approximation . The proposed Laplace-P-spline model contributes to the development of a new methodology to explore the posterior penalty space by considering a deterministic grid-based strategy or a Markov chain sampler, depending on the number of smooth additive terms in the predictor. The approach has the merit of relying on a simple Gaussian approximation to the conditional posterior of latent variables with closed form analytical expressions available for the gradient and Hessian of the approximate posterior penalty vector. This enables to construct accurate posterior pointwise and credible set estimators for (functions of) regression and spline parameters at a relatively low computational budget even for a large number of smooth additive components . The performance of the Laplace-P-spline model is confirmed through different simulation scenarios and the method is illustrated on two real datasets.},
  archive      = {J_CSDA},
  author       = {Oswaldo Gressani and Philippe Lambert},
  doi          = {10.1016/j.csda.2020.107088},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107088},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Laplace approximations for fast bayesian inference in generalized additive models based on P-splines},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On data-driven choice of λ in nonparametric gaussian
regression via propagation–separation approach. <em>CSDA</em>,
<em>154</em>, 107087. (<a
href="https://doi.org/10.1016/j.csda.2020.107087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new procedure is proposed for selecting the input value for the adaptation bandwidth λ λ in nonparametric Gaussian regression via Propagation–Separation approach. Since λ λ stands at the bias–variance trade-off of this estimation technique, it is its key tuning parameter. So far, however, apart from theoretical concepts, there exists no approach for a data-driven choice of its input value. This complicates the practical application of the Propagation–Separation method despite its highly desirable statistical properties — it is dimension-free and very well suited for estimation problems where the underlying regression function has sharp discontinuities and large homogeneous regions . The proposed selection method is based on the idea of cross-validation. Therefore, it allows to choose λ λ in a data-driven way. Its performance is evaluated via simulations. The results are very convincing: Cross-validation is a very transparent selection procedure that provides for each sample a “tailor-made” input value for λ λ and allows a successful identification of the underlying regression function . As the sample size increases, the accuracy of estimation improves in the sense that the estimates approach the true regression function. In addition, cross-validation accounts for the weighting functions and other parameter values used within the Propagation–Separation method by adjusting λ λ accordingly, which results in very robust estimates.},
  archive      = {J_CSDA},
  author       = {Ewelina Marta Fiebig},
  doi          = {10.1016/j.csda.2020.107087},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107087},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On data-driven choice of λ in nonparametric gaussian regression via Propagation–Separation approach},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning high-dimensional gaussian linear structural
equation models with heterogeneous error variances. <em>CSDA</em>,
<em>154</em>, 107084. (<a
href="https://doi.org/10.1016/j.csda.2020.107084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new approach is presented for learning high-dimensional Gaussian linear structural equation models from only observational data when unknown error variances are heterogeneous. The proposed method consists of three steps: inferring (1) the moralized graph using the inverse covariance matrix , (2) the ordering using conditional variances , and (3) the directed edges using conditional independence relationships. These three problems can be efficiently addressed using inversion of parts of the covariance matrix . It is proved that a sample size of n = Ω ( d m 2 log p ) n=Ω(dm2logp) is sufficient for the proposed algorithm to recover the true directed graph , where p p is the number of nodes and d m dm is the maximum degree . It is also shown that the proposed algorithm requires O ( p 3 + p d m 4 ) O(p3+pdm4) operations in the worst-case, and hence, it is computationally feasible for recovering large-scale graphs. It is verified through simulations that the proposed algorithm is statistically consistent and computationally feasible in high-dimensional and large-scale graph settings, and performs well compared to the state-of-the-art structural learning algorithms. It is also demonstrated through protein signaling data that our algorithm is well-suited to the estimation of directed acyclic graphical models for multivariate data in comparison to other methods used for normally distributed data.},
  archive      = {J_CSDA},
  author       = {Gunwoong Park and Yesool Kim},
  doi          = {10.1016/j.csda.2020.107084},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107084},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Learning high-dimensional gaussian linear structural equation models with heterogeneous error variances},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of non-stationary response functions using a
bayesian composite gaussian process. <em>CSDA</em>, <em>154</em>,
107083. (<a href="https://doi.org/10.1016/j.csda.2020.107083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modeling and prediction of functions that can exhibit non-stationarity characteristics is important in many applications; for example, this is often the case for simulator output. One approach to predict a function with unknown stationarity properties is to model it as a draw from a flexible stochastic process that can produce stationary or non-stationary realizations. One such model is the composite Gaussian process (CGP) which expresses the large-scale (global) trends of the output and the small-scale (local) adjustments to the global trend as independent Gaussian processes; an extension of the CGP model can produce realizations with non-constant variance by allowing the variance of the local process to vary over the input space. A new, Bayesian extension of a global-trend plus local-trend model is proposed that also allows measurement errors. In contrast to the original CGP model, the new Bayesian CGP model introduces a weight function to allow the total process variability to be apportioned between the large- and small-scale processes. The proposed prior distributions ensure that the fitted global mean is smoother than the local deviations, a feature built into the CGP model. The log of the process variance for the Bayesian CGP is modeled as a Gaussian process to provide a flexible mechanism for handling variance functions that vary across the input space. A Markov chain Monte Carlo algorithm is proposed that provides posterior estimates of the parameters for the Bayesian CGP. It also yields predictions of the output and quantifies uncertainty about the predictions. The method is illustrated using both analytic and real-data examples.},
  archive      = {J_CSDA},
  author       = {Casey B. Davis and Christopher M. Hans and Thomas J. Santner},
  doi          = {10.1016/j.csda.2020.107083},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107083},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Prediction of non-stationary response functions using a bayesian composite gaussian process},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical copulas with archimedean blocks and asymmetric
between-block pairs. <em>CSDA</em>, <em>154</em>, 107071. (<a
href="https://doi.org/10.1016/j.csda.2020.107071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new class of hierarchical copulas is introduced based on joint survival functions of multivariate exponential mixture distributions. The key element of this construction is the mixing random vector defined by convolutions associated to a Lévy subordinator, and leading to hierarchical copulas with Archimedean within-block copulas and asymmetric between-block pair-copulas. For the specific case of two-level trees, dependence properties of pairs are investigated, and a full estimation procedure is proposed for the tree structure and parameters of the hierarchical copulas. The efficiency of the procedure is illustrated through three simulation examples and a study with two real datasets.},
  archive      = {J_CSDA},
  author       = {Ihsan Chaoubi and Hélène Cossette and Etienne Marceau and Christian Y. Robert},
  doi          = {10.1016/j.csda.2020.107071},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107071},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hierarchical copulas with archimedean blocks and asymmetric between-block pairs},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semi-parametric estimation method for the quantile
spectrum with an application to earthquake classification using
convolutional neural network. <em>CSDA</em>, <em>154</em>, 107069. (<a
href="https://doi.org/10.1016/j.csda.2020.107069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new estimation method is introduced for the quantile spectrum, which uses a parametric form of the autoregressive (AR) spectrum coupled with nonparametric smoothing. The method begins with quantile periodograms which are constructed by trigonometric quantile regression at different quantile levels, to represent the serial dependence of time series at various quantiles. At each quantile level, we approximate the quantile spectrum by a function in the form of an ordinary AR spectrum. In this model, we first compute what we call the quantile autocovariance function (QACF) by the inverse Fourier transformation of the quantile periodogram at each quantile level. Then, we solve the Yule–Walker equations formed by the QACF to obtain the quantile partial autocorrelation function (QPACF) and the scale parameter. Finally, we smooth QPACF and the scale parameter across the quantile levels using a nonparametric smoother, convert the smoothed QPACF to AR coefficients , and obtain the AR spectral density function . Numerical results show that the proposed method outperforms other conventional smoothing techniques. We take advantage of the two-dimensional property of the estimators and train a convolutional neural network (CNN) to classify smoothed quantile periodogram of earthquake data and achieve a higher accuracy than a similar classifier using ordinary periodograms .},
  archive      = {J_CSDA},
  author       = {Tianbo Chen and Ying Sun and Ta-Hsin Li},
  doi          = {10.1016/j.csda.2020.107069},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107069},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A semi-parametric estimation method for the quantile spectrum with an application to earthquake classification using convolutional neural network},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of regional transition probabilities for spatial
dynamic microsimulations from survey data lacking in regional detail.
<em>CSDA</em>, <em>154</em>, 107048. (<a
href="https://doi.org/10.1016/j.csda.2020.107048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial dynamic microsimulations allow for the multivariate analysis of complex systems with geographic segmentation. A synthetic replica of the system is stochastically projected into future periods using micro-level transition probabilities . These should accurately represent the dynamics of the system to allow for reliable simulation outcomes . In practice, transition probabilities are unknown and must be estimated from suitable survey data. This can be challenging when the dynamics vary locally. Survey data often lacks in regional detail due to confidentiality restrictions and limited sampling resources. In that case, transition probability estimates may misrepresent regional dynamics due to insufficient local observations and coverage problems. The simulation process subsequently fails to provide an authentic evolution of the system. A constrained maximum likelihood approach for probability alignment to solve these issues is proposed. It accounts for regional heterogeneity in transition dynamics through the consideration of external benchmarks from administrative records. It is proven that the method is consistent. A parametric bootstrap for uncertainty estimation is presented. Simulation experiments are conducted to compare the approach with an existing method for probability alignment. Furthermore, an empirical application to labor force estimation based on the German Microcensus is provided.},
  archive      = {J_CSDA},
  author       = {Jan Pablo Burgard and Joscha Krause and Simon Schmaus},
  doi          = {10.1016/j.csda.2020.107048},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107048},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of regional transition probabilities for spatial dynamic microsimulations from survey data lacking in regional detail},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating multiplicative error models: A residual-based
approach. <em>CSDA</em>, <em>153</em>, 107086. (<a
href="https://doi.org/10.1016/j.csda.2020.107086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a residual-based approach to diagnose the adequacy of both the univariate and vector multiplicative error model (MEM). Two residual-based statistics are constructed based on the parameter estimates of the linear autoregressions with the standardized residuals as dependent variables. Since the autoregressions involve estimated standardized residuals, the correct asymptotic distributions of test statistics are obtained by taking into account the impact of parameter estimation uncertainty. Monte Carlo simulations indicate that the proposed test statistics perform well against their competitors in terms of empirical size and power. An empirical application further shows the usefulness of the proposed test in evaluating MEMs.},
  archive      = {J_CSDA},
  author       = {Rui Ke and Wanbo Lu and Jing Jia},
  doi          = {10.1016/j.csda.2020.107086},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107086},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Evaluating multiplicative error models: A residual-based approach},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive decorrelation procedure for signal detection.
<em>CSDA</em>, <em>153</em>, 107082. (<a
href="https://doi.org/10.1016/j.csda.2020.107082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In global testing, where a large number of pointwise test statistics are aggregated to simultaneously test for a collection of null hypotheses, the handling of dependence is a crucial issue. In various fields, more particularly in genetic epidemiology and functional data analysis , many testing methods for detecting an association signal between a response and explanatory variables have been proposed. Some aggregation procedures ignore dependence across pointwise test statistics whereas others introduce a model for decorrelation , with unclear conclusions on their relative performance. Indeed, the benefit that can be expected from decorrelation highly depends on the interplay between the structure of dependence across pointwise test statistics and the pattern of the association signal. Within a large class of test statistics covering a continuum of decorrelation approaches, an optimal procedure is introduced. This procedure is based on the maximization of an ad-hoc cumulant generating function-based distance between the null and nonnull distributions of a global test statistic, in order to adapt the aggregation of the pointwise statistics to the pattern of the association signal. A comparative study including simulations and applications to genetic association studies demonstrates that the ability of this test to detect a signal is more robust to the dependence structure than existing methods.},
  archive      = {J_CSDA},
  author       = {Florian Hébert and David Causeur and Mathieu Emily},
  doi          = {10.1016/j.csda.2020.107082},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107082},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An adaptive decorrelation procedure for signal detection},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vecchia–laplace approximations of generalized gaussian
processes for big non-gaussian spatial data. <em>CSDA</em>,
<em>153</em>, 107081. (<a
href="https://doi.org/10.1016/j.csda.2020.107081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Gaussian processes (GGPs) are highly flexible models that combine latent GPs with potentially non-Gaussian likelihoods from the exponential family . GGPs can be used in a variety of settings, including GP classification, nonparametric count regression, modeling non-Gaussian spatial data, and analyzing point patterns. However, inference for GGPs can be analytically intractable, and large datasets pose computational challenges due to the inversion of the GP covariance matrix . A Vecchia–Laplace approximation for GGPs is proposed, which combines a Laplace approximation to the non-Gaussian likelihood with a computationally efficient Vecchia approximation to the GP, resulting in simple, general, scalable, and accurate methodology. Numerical studies and comparisons on simulated and real spatial data are provided. The methods are implemented in a freely available R package.},
  archive      = {J_CSDA},
  author       = {Daniel Zilber and Matthias Katzfuss},
  doi          = {10.1016/j.csda.2020.107081},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107081},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Vecchia–Laplace approximations of generalized gaussian processes for big non-gaussian spatial data},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Distributed subdata selection for big data via
sampling-based approach. <em>CSDA</em>, <em>153</em>, 107072. (<a
href="https://doi.org/10.1016/j.csda.2020.107072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of modern technologies, it is possible to gather an extraordinarily large number of observations. Due to the storage or transmission burden, big data are usually scattered at multiple locations. It is difficult to transfer all of data to the central server for analysis. A distributed subdata selection method for big data linear regression model is proposed. Particularly, a two-step subsampling strategy with optimal subsampling probabilities and optimal allocation sizes is developed. The subsample-based estimator effectively approximates the ordinary least squares estimator from the full data. The convergence rate and asymptotic normality of the proposed estimator are established. Simulation studies and an illustrative example about airline data are provided to assess the performance of the proposed method.},
  archive      = {J_CSDA},
  author       = {Haixiang Zhang and HaiYing Wang},
  doi          = {10.1016/j.csda.2020.107072},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107072},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Distributed subdata selection for big data via sampling-based approach},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rank-based tests of cross-sectional dependence in panel data
models. <em>CSDA</em>, <em>153</em>, 107070. (<a
href="https://doi.org/10.1016/j.csda.2020.107070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of panel regression, current existing cross-sectional dependence tests are mainly based on the normal assumption. However, in practical applications, the normal assumption is usually not valid, which weakens the usability of the tests. To develop more testing tools suitable for nonnormal panel data, we extend the rank-based framework of U-statistics to panel regressions, and derive their asymptotic null distributions respectively as ( N , T ) → ∞ (N,T)→∞ . The results of some simulation results and a real data analysis demonstrate the superiority of the proposed tests, especially their robustness to deviation from normality.},
  archive      = {J_CSDA},
  author       = {Long Feng and Ping Zhao and Yanling Ding and Binghui Liu},
  doi          = {10.1016/j.csda.2020.107070},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107070},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Rank-based tests of cross-sectional dependence in panel data models},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved confidence regions in meta-analysis of diagnostic
test accuracy. <em>CSDA</em>, <em>153</em>, 107068. (<a
href="https://doi.org/10.1016/j.csda.2020.107068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analyses of diagnostic test accuracy (DTA) studies have been gathering attention in research in clinical epidemiology and health technology development, and bivariate random-effects model is becoming a standard tool. However, standard inference methods usually underestimate statistical errors and possibly provide highly overconfident results under realistic situations since they ignore the variability in the estimation of variance parameters. To overcome the difficulty, a new improved inference method, namely, an accurate confidence region for the meta-analysis of DTA, by asymptotically expanding the coverage probability of the standard confidence region. The advantage of the proposed confidence region is that it holds a relatively simple expression and does not require any repeated calculations such as Bootstrap or Monte Carlo methods to compute the region, thereby the proposed method can be easily carried out in practical applications. The effectiveness of the proposed method is demonstrated through simulation studies and an application to meta-analysis of screening test accuracy for alcohol problems.},
  archive      = {J_CSDA},
  author       = {Tsubasa Ito and Shonosuke Sugasawa},
  doi          = {10.1016/j.csda.2020.107068},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107068},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Improved confidence regions in meta-analysis of diagnostic test accuracy},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two sample tests for high-dimensional autocovariances.
<em>CSDA</em>, <em>153</em>, 107067. (<a
href="https://doi.org/10.1016/j.csda.2020.107067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of testing for the equality of autocovariances of two independent high-dimensional time series is studied. Tests based on the suprema or sums of suitable averages across the dimensions are adapted from the available literature. Another test based on principal component analysis (PCA) is introduced and studied in theory. An extension is also considered to the setting of testing for the equality of autocovariances of two populations, having multiple individual high-dimensional series from the two populations. The proposed methodologies are assessed on simulated data, with the performance of the introduced PCA testing being superior overall. An application using fMRI data from individuals experiencing two different emotional states is provided.},
  archive      = {J_CSDA},
  author       = {Changryong Baek and Katheleen M. Gates and Benjamin Leinwand and Vladas Pipiras},
  doi          = {10.1016/j.csda.2020.107067},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107067},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Two sample tests for high-dimensional autocovariances},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional score matching for high-dimensional partial
graphical models. <em>CSDA</em>, <em>153</em>, 107066. (<a
href="https://doi.org/10.1016/j.csda.2020.107066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network construction has been heavily exploited in multivariate data analysis . In many cases, connections between a large portion of variables are of minimal importance. As such, partial graphs have played an important role in network construction. Due to the existence of a multiplicative normalization constant, the existing construction approaches may bear high computational cost. To reduce the computational complexity , the conditional score matching for high-dimensional partial graphical models is proposed. This approach is uniquely advantageous by being not influenced by the multiplicative normalization constant. An effective computational algorithm is developed, and it is shown that the computational complexity of the proposed method is less than that of those in the literature. Statistical properties are established, and two extensions are explored to incorporate more information and accommodate more general distributions. A wide spectrum of simulations and the analysis of a breast cancer gene expression dataset demonstrate competitive performance of the proposed methods.},
  archive      = {J_CSDA},
  author       = {Xinyan Fan and Qingzhao Zhang and Shuangge Ma and Kuangnan Fang},
  doi          = {10.1016/j.csda.2020.107066},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107066},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Conditional score matching for high-dimensional partial graphical models},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust boosting for regression problems. <em>CSDA</em>,
<em>153</em>, 107065. (<a
href="https://doi.org/10.1016/j.csda.2020.107065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient boosting algorithms construct a regression predictor using a linear combination of “base learners”. Boosting also offers an approach to obtaining robust non-parametric regression estimators that are scalable to applications with many explanatory variables . The robust boosting algorithm is based on a two-stage approach, similar to what is done for robust linear regression: it first minimizes a robust residual scale estimator , and then improves it by optimizing a bounded loss function. Unlike previous robust boosting proposals this approach does not require computing an ad hoc residual scale estimator in each boosting iteration . Since the loss functions involved in this robust boosting algorithm are typically non-convex, a reliable initialization step is required, such as an L 1 L1 regression tree , which is also fast to compute. A robust variable importance measure can also be calculated via a permutation procedure. Thorough simulation studies and several data analyses show that, when no atypical observations are present, the robust boosting approach works as well as the standard gradient boosting with a squared loss. Furthermore, when the data contain outliers, the robust boosting estimator outperforms the alternatives in terms of prediction error and variable selection accuracy.},
  archive      = {J_CSDA},
  author       = {Xiaomeng Ju and Matías Salibián-Barrera},
  doi          = {10.1016/j.csda.2020.107065},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107065},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust boosting for regression problems},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fixed design local polynomial smoothing and bandwidth
selection for right censored data. <em>CSDA</em>, <em>153</em>, 107064.
(<a href="https://doi.org/10.1016/j.csda.2020.107064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The local polynomial smoothing of the Kaplan–Meier estimate for fixed designs is explored and analyzed. The first benefit, in comparison to classical convolution kernel smoothing, is the development of boundary aware estimates of the distribution function, its derivatives and integrated derivative products of any arbitrary order. The advancements proceed by developing asymptotic mean integrated square error optimal solve-the-equation plug-in bandwidth selectors for the estimates of the distribution function and its derivatives, and as a byproduct, a mean square error optimal bandwidth rule for integrated derivative products. The asymptotic properties of all methodological contributions are quantified analytically and discussed in detail. Three real data analyses illustrate the benefits of the proposed methodology in practice. Finally, numerical evidence is provided on the finite sample performance of the proposed technique with reference to benchmark estimates.},
  archive      = {J_CSDA},
  author       = {Dimitrios Bagkavos and Dimitrios Ioannides},
  doi          = {10.1016/j.csda.2020.107064},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107064},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fixed design local polynomial smoothing and bandwidth selection for right censored data},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid safe–strong rules for efficient optimization in
lasso-type problems. <em>CSDA</em>, <em>153</em>, 107063. (<a
href="https://doi.org/10.1016/j.csda.2020.107063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lasso model has been widely used for model selection in data mining , machine learning , and high-dimensional statistical analysis. However, with the ultrahigh-dimensional, large-scale data sets now collected in many real-world applications, it is important to develop algorithms to solve the lasso that efficiently scale up to problems of this size. Discarding features from certain steps of the algorithm is a powerful technique for increasing efficiency and addressing the Big Data challenge. This paper proposes a family of hybrid safe–strong rules (HSSR) which incorporate safe screening rules into the sequential strong rule (SSR) to remove unnecessary computational burden. Two instances of HSSR are presented, SSR-Dome and SSR-BEDPP, for the standard lasso problem. SSR-BEDPP is further extended to the elastic net and group lasso problems to demonstrate the generalizability of the hybrid screening idea. Extensive numerical experiments with synthetic and real data sets are conducted for both the standard lasso and the group lasso problems. Results show that the proposed hybrid rules can substantially outperform existing state-of-the-art rules.},
  archive      = {J_CSDA},
  author       = {Yaohui Zeng and Tianbao Yang and Patrick Breheny},
  doi          = {10.1016/j.csda.2020.107063},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107063},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hybrid safe–strong rules for efficient optimization in lasso-type problems},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An algorithm for non-parametric estimation in state–space
models. <em>CSDA</em>, <em>153</em>, 107062. (<a
href="https://doi.org/10.1016/j.csda.2020.107062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State–space models are ubiquitous in the statistical literature since they provide a flexible and interpretable framework for analyzing many time series. In most practical applications, the state–space model is specified through a parametric model . However, the specification of such a parametric model may require an important modeling effort or may lead to models which are not flexible enough to reproduce all the complexity of the phenomenon of interest. In such situations, an appealing alternative consists in inferring the state–space model directly from the data using a non-parametric framework. The recent developments of powerful simulation techniques have permitted to improve the statistical inference for parametric state–space models. It is proposed to combine two of these techniques, namely the Stochastic Expectation–Maximization (SEM) algorithm and Sequential Monte Carlo (SMC) approaches, for non-parametric estimation in state–space models. The performance of the proposed algorithm is assessed though simulations on toy models and an application to environmental data is discussed.},
  archive      = {J_CSDA},
  author       = {Thi Tuyet Trang Chau and Pierre Ailliot and Valérie Monbet},
  doi          = {10.1016/j.csda.2020.107062},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107062},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An algorithm for non-parametric estimation in state–space models},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted rank estimation for nonparametric transformation
models with nonignorable missing data. <em>CSDA</em>, <em>153</em>,
107061. (<a href="https://doi.org/10.1016/j.csda.2020.107061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data occur in almost every field and a great deal of literature has been established for the analysis of missing data with different types of missing mechanisms and under various models. Nonignorable missing data can be analyzed using nonparametric transformation models, which has not been discussed in the literature. In particular, assume that the conditional response probability can be written as the product of separate unknown functions of the response variable and covariates , respectively. For estimation of regression parameters , a weighted rank (WR) estimation procedure is proposed and the asymptotic properties of the resulting WR estimator are established. For the determination of the proposed estimator, a simple coordinate-wise optimization algorithm is developed, and a numerical study is conducted for assessing the performance of the proposed approach and suggests that it works well in practice. An illustration is also provided.},
  archive      = {J_CSDA},
  author       = {Tianqing Liu and Xiaohui Yuan and Jianguo Sun},
  doi          = {10.1016/j.csda.2020.107061},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107061},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Weighted rank estimation for nonparametric transformation models with nonignorable missing data},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized logspline density estimation using total variation
penalty. <em>CSDA</em>, <em>153</em>, 107060. (<a
href="https://doi.org/10.1016/j.csda.2020.107060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a penalized logspline density estimation method using a total variation penalty. The B-spline basis is adopted to approximate the logarithm of density functions. Total variation of derivatives of splines is penalized to impart a data-driven knot selection. The proposed estimator is a bona fide density function in the sense that it is positive and integrates to one. We devise an efficient coordinate descent algorithm for implementation and study its convergence property . An oracle inequality of the proposed estimator is established when the quality of fit is measured by the Kullback–Leibler divergence. Based on the oracle inequality, it is proved that the estimator achieves an optimal rate of convergence in the minimax sense. We also propose a logspline method for the bivariate case by adopting the tensor-product B-spline basis and a two-dimensional total variation type penalty. Numerical studies show that the proposed method captures local features without compromising the global smoothness.},
  archive      = {J_CSDA},
  author       = {Kwan-Young Bak and Jae-Hwan Jhong and JungJun Lee and Jae-Kyung Shin and Ja-Yong Koo},
  doi          = {10.1016/j.csda.2020.107060},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107060},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Penalized logspline density estimation using total variation penalty},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An algorithm for blocking regular fractional factorial
2-level designs with clear two-factor interactions. <em>CSDA</em>,
<em>153</em>, 107059. (<a
href="https://doi.org/10.1016/j.csda.2020.107059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular fractional factorial designs with 2-level factors are among the most frequently used experimental plans. In many cases, designs should be blocked for dealing with inhomogeneity of experimental units. At the same time, the research question at hand may imply a focus on specified sets of two-factor interactions, while it is not justified to assume negligibility of other low order effects. An algorithm is provided for blocking a regular fraction into – possibly small – blocks while keeping specified two-factor interactions clear from confounding with main effects or other two-factor interactions. The proposed algorithm is implemented in the R package FrF2 and combines an estimability algorithm by the author with an automated implementation of a recent proposal for blocking fractions by hand.},
  archive      = {J_CSDA},
  author       = {Ulrike Grömping},
  doi          = {10.1016/j.csda.2020.107059},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107059},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An algorithm for blocking regular fractional factorial 2-level designs with clear two-factor interactions},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating scale-invariant directed dependence of bivariate
distributions. <em>CSDA</em>, <em>153</em>, 107058. (<a
href="https://doi.org/10.1016/j.csda.2020.107058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetry of dependence is an inherent property of bivariate probability distributions. Being symmetric, commonly used dependence measures such as Pearson’s r r or Spearman’s ρ ρ mask asymmetry and implicitly assume that a random variable Y Y is equally dependent on a random variable X X as vice versa. A copula-based, hence scale-invariant dependence measure called ζ 1 ζ1 overcoming the just mentioned problem was introduced in 2011. ζ 1 ζ1 attains values in [ 0 , 1 ] [0,1] , it is 0 if, and only if X X and Y Y are independent, and 1 if, and only if Y Y is a measurable function of X X . Working with so-called empirical checkerboard copulas allows to construct an estimator ζ 1 n ζ1n for ζ 1 ζ1 which is strongly consistent in full generality, i.e., without any smoothness assumptions on the underlying copula . The R-package qad (short for quantification of asymmetric dependence) containing the estimator ζ 1 n ζ1n is used both, to perform a simulation study illustrating the small sample performance of the estimator as well as to estimate the directed dependence between some global climate variables as well as between world development indicators.},
  archive      = {J_CSDA},
  author       = {Robert R. Junker and Florian Griessenberger and Wolfgang Trutschnig},
  doi          = {10.1016/j.csda.2020.107058},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107058},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimating scale-invariant directed dependence of bivariate distributions},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalised maximum likelihood estimation in multi-state
models for interval-censored data. <em>CSDA</em>, <em>153</em>, 107057.
(<a href="https://doi.org/10.1016/j.csda.2020.107057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous-time multi-state Markov models can be used to describe transitions over time across health states. Given longitudinal interval-censored data on transitions between states, statistical inference on changing health is possible by specifying models for transition hazards. Parametric time-dependent hazards can be restrictive, and nonparametric hazard specifications using splines are presented as an alternative. The smoothing of the splines is controlled by using penalised maximum likelihood estimation. With multiple time-dependent hazards in a multi-state model, there are multiple penalty parameters and selecting the optimal amount of smoothing is a challenge. A grid search to estimate the penalty parameters is computational intensive especially when combined with methods to deal with interval-censored transition times. A new and efficient method is proposed to estimate multi-state models with splines where the estimation of the penalty parameters is automatic. A simulation study is undertaken to validate the method and to illustrate the effect of interval censoring. The feasibility of the method is illustrated with two applications.},
  archive      = {J_CSDA},
  author       = {Robson J.M. Machado and Ardo van den Hout and Giampiero Marra},
  doi          = {10.1016/j.csda.2020.107057},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107057},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Penalised maximum likelihood estimation in multi-state models for interval-censored data},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
