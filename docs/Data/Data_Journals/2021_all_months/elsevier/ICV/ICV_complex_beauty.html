<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv---150">ICV - 150</h2>
<ul>
<li><details>
<summary>
(2021). MetaPix: Domain transfer for semantic segmentation by meta
pixel weighting. <em>ICV</em>, <em>116</em>, 104334. (<a
href="https://doi.org/10.1016/j.imavis.2021.104334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a deep neural model for semantic segmentation requires collecting a large amount of pixel-level labeled data. To alleviate the data scarcity problem presented in the real world, one could utilize synthetic data whose label is easy to obtain. Previous work has shown that the performance of a semantic segmentation model can be improved by training jointly with real and synthetic examples with a proper weighting on the synthetic data. Such weighting was learned by a heuristic to maximize the similarity between synthetic and real examples. In our work, we instead learn a pixel-level weighting of the synthetic data by meta-learning, i.e. , the learning of weighting should only be minimizing the loss on the target task. We achieve this by gradient-on-gradient technique to propagate the target loss back into the parameters of the weighting model. The experiments show that our method with only one single meta module can outperform a complicated combination of an adversarial feature alignment, a reconstruction loss, plus a hierarchical heuristic weighting at pixel, region and image levels.},
  archive      = {J_ICV},
  author       = {Yiren Jian and Chongyang Gao},
  doi          = {10.1016/j.imavis.2021.104334},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104334},
  shortjournal = {Image Vis. Comput.},
  title        = {MetaPix: Domain transfer for semantic segmentation by meta pixel weighting},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MLRMV: Multi-layer representation for multi-view action
recognition. <em>ICV</em>, <em>116</em>, 104333. (<a
href="https://doi.org/10.1016/j.imavis.2021.104333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Daily action recognition has gained much interest in computer vision. However, viewpoint changes will lead to sizable intra-class differences in the same action. To deal with this problem, we propose a novel multi-view daily action recognition approach based on the multi-layer representation. In use of motion atoms and motion phrases, we construct the middle-level feature representations in multi-view daily actions. A multi-view unsupervised discriminative clustering method is proposed for constructing motion atoms, and the classification accuracy of motion atoms is improved by jointly learning atom dictionaries and the classifier. Moreover, we present discontinuous temporal scale motion phrases and a grading mechanism of motion phrases to strengthen the representative ability of motion phrases and the final recognition accuracy. Finally, the experimental results based on the WVU dataset, the NTU RGB-D dataset, and N-UCLA dataset show that the proposed methods have the state-of-the-art performance, compared with the classic methods such as IDT, MoFAP, JLMF, and so on.},
  archive      = {J_ICV},
  author       = {Zhigang Liu and Ziyang Yin and Yin Wu},
  doi          = {10.1016/j.imavis.2021.104333},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104333},
  shortjournal = {Image Vis. Comput.},
  title        = {MLRMV: Multi-layer representation for multi-view action recognition},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-scale global attention feature pyramid network for
person search. <em>ICV</em>, <em>116</em>, 104332. (<a
href="https://doi.org/10.1016/j.imavis.2021.104332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search aims to locate the target person in real unconstrained scene images. It faces many challenges such as multi-scale and fine-grained. To address the challenges, a novel cross-scale global attention feature pyramid network (CSGAFPN) is proposed. Firstly, we design a novel multi-head global attention module (MHGAM), which adopts cosine similarity and sparse query location methods to effectively capture cross-scale long-distance dependence. Then, we design the CSGAFPN, which extends top-down feature pyramid network with bottom-up connections and embeds MHGAMs to the connections. CSGAFPN can capture cross-scale long-distance global correlation from multi-scale feature maps, selectively strengthen important features and restrain less important features. CSGAFPN is applied for both person detection and person re-identification (reID) subtasks of person search, it can well handle the multi-scale and fine-grained challenges, and significantly improve person search performance. Furthermore, the output multi-scale feature maps of CSGAFPN are processed by an adaptive feature aggregation with attention (AFAA) layer to further improve the performance. Numerous experiments with two public person search datasets, CUHK-SYSU and PRW, show our CSGAFPN based approach acquires better performance than other state-of-the-art (SOTA) person search approaches.},
  archive      = {J_ICV},
  author       = {Yang Li and Huahu Xu and Minjie Bian and Junsheng Xiao},
  doi          = {10.1016/j.imavis.2021.104332},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104332},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-scale global attention feature pyramid network for person search},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monozygotic twin face recognition: An in-depth analysis and
plausible improvements. <em>ICV</em>, <em>116</em>, 104331. (<a
href="https://doi.org/10.1016/j.imavis.2021.104331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monozygotic twins , commonly known as identical twins, share extreme facial resemblance that they deceive the majority of the prevailing face recognition systems . Hence, the facial recognition systems, which are turning out to be exploited in almost all real-life biometric authentication-needing applications, require significant upgradation to cope with this great challenge. This paper pays special attention towards categorizing and investigating the existing research on the facial recognition of monozygotic twins in an exhaustive sense. Subsequently, the research gap is analyzed. It is then followed by the description of the possible refinements, like the choice of feature and other methodological advancements, which may aid in augmenting the twin recognition accuracy of the face recognition systems. It is believed that the outcomes of the review can well-support and pave the way for further research in the future. Consequently, the “Double Trouble” posed on the commercial face recognition systems can be effectively evaded.},
  archive      = {J_ICV},
  author       = {Vinusha Sundaresan and S. Amala Shanthi},
  doi          = {10.1016/j.imavis.2021.104331},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104331},
  shortjournal = {Image Vis. Comput.},
  title        = {Monozygotic twin face recognition: An in-depth analysis and plausible improvements},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to disentangle scenes for person re-identification.
<em>ICV</em>, <em>116</em>, 104330. (<a
href="https://doi.org/10.1016/j.imavis.2021.104330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many challenging problems in the person re-identification (ReID) task, such as the occlusion and scale variation. Existing works usually tried to solve them by employing a one-branch network. This one-branch network needs to be robust to various challenging problems, which makes this network overburdened. This paper proposes to divide-and-conquer the ReID task. For this purpose, we employ several self-supervision operations to simulate different challenging problems and handle each challenging problem using different networks. Concretely, we use the random erasing operation and propose a novel random scaling operation to generate new images with controllable characteristics. A general multi-branch network, including one master branch and two servant branches, is introduced to handle different scenes. These branches learn collaboratively and achieve different perceptive abilities. In this way, the complex scenes in the ReID task are effectively disentangled, and the burden of each branch is relieved. The results from extensive experiments demonstrate that the proposed method achieves state-of-the-art performances on three ReID benchmarks and two occluded ReID benchmarks. Ablation study also shows that the proposed scheme and operations significantly improve the performance in various scenes.},
  archive      = {J_ICV},
  author       = {Xianghao Zang and Ge Li and Wei Gao and Xiujun Shu},
  doi          = {10.1016/j.imavis.2021.104330},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104330},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning to disentangle scenes for person re-identification},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EAR: Efficient action recognition with local-global temporal
aggregation. <em>ICV</em>, <em>116</em>, 104329. (<a
href="https://doi.org/10.1016/j.imavis.2021.104329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal modeling in videos is crucial for action recognition. Traditionally, it involves feature aggregation for both local motion and global semantic. In this paper, we propose an Efficient Action Recognition network (EAR), which includes a Persistence of Appearance (PA) module and a Various-timescale Aggregation (VA) module for local and global temporal aggregations respectively. For local motion aggregation, instead of using the previous time-consuming optical flow, our PA calculates pixel-wise differences in feature space as the motion representation, which is much more efficient (8196 fps vs. 8 fps in optical flow). Besides, to capture global semantic hints, we propose VA module which adaptively emphasizes expressive features and suppresses less informative ones across various timescales. Empowered by the local-global temporal aggregation, our EAR achieves competitive results on six challenging action recognition benchmarks at low FLOPs.},
  archive      = {J_ICV},
  author       = {Can Zhang and Yuexian Zou and Guang Chen and Lei Gan},
  doi          = {10.1016/j.imavis.2021.104329},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104329},
  shortjournal = {Image Vis. Comput.},
  title        = {EAR: Efficient action recognition with local-global temporal aggregation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VQA as a factoid question answering problem: A novel
approach for knowledge-aware and explainable visual question answering.
<em>ICV</em>, <em>116</em>, 104328. (<a
href="https://doi.org/10.1016/j.imavis.2021.104328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advancements in machine perception and scene understanding, Visual Question Answering (VQA) has garnered much attraction from researchers in the direction of training neural models for jointly analyzing, grounding and reasoning over the multi-modal space of image visual context and natural language in order to answer natural language questions pertaining to the image contents. However, though recent works have achieved significant improvement over state-of-art models for answering questions that are answerable by solely referring to the visual context of the image, such models are often limited, being incapable of tackling questions involving external world knowledge beyond the visible contents. Though recently, research has been driven towards tackling external knowledge based VQA as well, there is significant room for improvement as limited studies exist in this area. Inspired by the aforementioned challenges involved, this paper is aimed at answering free form and open ended natural language questions, not limited to visual context of an image, but external world knowledge as well. With this motive, inspired by human cognitive abilities of comprehending and reasoning answers when given a set of facts, this paper proposes a novel model architecture to model VQA as a factoid question answering problem, leveraging state-of-the-art deep learning techniques for reasoning and inferring answers to free form questions, in an attempt of improving the state-of-art in open ended visual question answering.},
  archive      = {J_ICV},
  author       = {Abhishek Narayanan and Abijna Rao and Abhishek Prasad and Natarajan S},
  doi          = {10.1016/j.imavis.2021.104328},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104328},
  shortjournal = {Image Vis. Comput.},
  title        = {VQA as a factoid question answering problem: A novel approach for knowledge-aware and explainable visual question answering},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A survey of methods, datasets and evaluation metrics for
visual question answering. <em>ICV</em>, <em>116</em>, 104327. (<a
href="https://doi.org/10.1016/j.imavis.2021.104327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a multi-disciplinary research problem that has captured the attention of both computer vision as well as natural language processing researchers. In Visual Question Answering, a system is given an image; a question in a natural language related to that image as an input, and the VQA system is required to give an answer in natural language as an output. A VQA algorithm may require common sense reasoning over the information contained in the image and world knowledge to produce the right answer. In this paper, we have discussed some of the core concepts used in VQA systems and present a comprehensive survey of efforts in the past to address this problem. Apart from traditional VQA models, we have also discussed visual question answering models that require reading texts present in images and evaluated on recently developed datasets like TextVQA, ST-VQA, and OCR-VQA. Apart from standard datasets discussed in previous surveys, we have also discussed some new datasets developed in 2019 and 2020 such as GQA, OK-VQA, TextVQA, ST-VQA, and OCR-VQA. The new evaluation metrics such as BLEU, MPT, METEOR, Average Normalized Levenshtein Similarity (ANLS), Validity, Plausibility, Distribution, Consistency, Grounding, F1-Score are explained together with the evaluation metrics discussed by previous surveys. We conclude our survey with a discussion on open issues in each phase of the VQA task and present some promising future directions.},
  archive      = {J_ICV},
  author       = {Himanshu Sharma and Anand Singh Jalal},
  doi          = {10.1016/j.imavis.2021.104327},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104327},
  shortjournal = {Image Vis. Comput.},
  title        = {A survey of methods, datasets and evaluation metrics for visual question answering},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salient object detection network with multi-scale feature
refinement and boundary feedback. <em>ICV</em>, <em>116</em>, 104326.
(<a href="https://doi.org/10.1016/j.imavis.2021.104326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the development of convolutional neural networks, salient object detection has achieved prominent progress in recent years. However, there are still two limitations when handling challenging scenarios. On the one hand, upsampling and pooling operations might cause blurry boundaries. On the other hand, multi-level features having different characteristics might incur fusion problems. To address these issues, we propose a novel pixel-wise salient object detection method equipped with multi-scale feature refinement and boundary feedback in this paper. Firstly, a feature interaction scheme is designed to depict the multi-scale representation of side-output features and the interactions between intra-layer features. Secondly, we design a context-aware feature refinement module to adaptively select useful side-output information for generating coarse saliency features. Finally, we obtain the predicted saliency maps with the help of extracted boundary information in a coarse-to-fine manner. Extensive experimental results on four benchmark datasets show that our proposed model performs favorably against state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Qing Zhang and Xiang Li},
  doi          = {10.1016/j.imavis.2021.104326},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104326},
  shortjournal = {Image Vis. Comput.},
  title        = {Salient object detection network with multi-scale feature refinement and boundary feedback},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating facial expression adversarial examples based on
saliency map. <em>ICV</em>, <em>116</em>, 104318. (<a
href="https://doi.org/10.1016/j.imavis.2021.104318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security demands in humanization should be considered as important, because artificial intelligence is developing rapidly in this area. Recent studies have shown the vulnerability of many deep learning models to adversarial examples; however, only a few studies on facial expression adversarial examples have been conducted. Thus, in this paper, we propose a novel method for generating facial expression adversarial examples using facial saliency maps and facial masking maps. Extensive numerical experiments demonstrate the outstanding performance of our method in terms of attack accuracy, structural similarity index measure score, and computational time, compared with other leading methods, such as the fast gradient sign method, projected gradient descent method, and Carlini–Wagner attacks.},
  archive      = {J_ICV},
  author       = {Yudao Sun and Juan Yin and Chunhua Wu and KangFeng Zheng and XinXin Niu},
  doi          = {10.1016/j.imavis.2021.104318},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104318},
  shortjournal = {Image Vis. Comput.},
  title        = {Generating facial expression adversarial examples based on saliency map},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ERF-YOLO: A YOLO algorithm compatible with fewer parameters
and higher accuracy. <em>ICV</em>, <em>116</em>, 104317. (<a
href="https://doi.org/10.1016/j.imavis.2021.104317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research shows that theoretical receptive field and effective receptive field are very important to target detection results. The effective receptive field determines the contribution of different positions in the theoretical receptive field. Therefore, the main purpose of this work is to increase the effective receptive field area and reduce the number of parameters. This idea obtains a high-precision and high-speed target detector. First, the algorithm needs to optimize the activation function to improve the efficiency of feature extraction. Second, the model structure needs to select the backbone network and improve the convolutional layer structure. Then, the enhanced network requires increasing the number of the residual structures and the “Concat” to improve feature extraction performance. Finally, the network needs to combine the optimized convolutional layer and the anchor box loss function to improve the performance of the anchor box. The project designed a YOLO algorithm (ERF-YOLO) with a larger effective receptive field. The training and testing of the experiment use PASCAL VOC data set and MS COCO data set respectively. Experimental results show that the parameter of ERF-YOLO is close to half of YOLO v4. In terms of detection accuracy, ERF-YOLO is superior to many current algorithms.},
  archive      = {J_ICV},
  author       = {Enhui Chai and Lin Ta and Zhanfei Ma PhD and Min Zhi},
  doi          = {10.1016/j.imavis.2021.104317},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104317},
  shortjournal = {Image Vis. Comput.},
  title        = {ERF-YOLO: A YOLO algorithm compatible with fewer parameters and higher accuracy},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aligning vision-language for graph inference in visual
dialog. <em>ICV</em>, <em>116</em>, 104316. (<a
href="https://doi.org/10.1016/j.imavis.2021.104316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a cross-media intelligence task, visual dialog calls for answering a sequence of questions based on an image, using the dialog history as context. To acquire correct answers, the exploration of the semantic dependencies among potential visual and textual contents becomes vital. Prior works usually ignored the underlying knowledge hidden in internal and external textual-visual relationships, which resulted in unreasonable inferring. In this paper, we propose an Aligning Vision-Language for Graph Inference (AVLGI) in visual dialog by combining the internal context-aware information and the external scene graph knowledge. Compared with other approaches, it makes up the lack of structural inference in visual dialog. So the whole system consists of three modules, Inter-Modalities Alignment (IMA), Visual Graph Attended by Text (VGAT) and Combining Scene Graph and Textual Contents(CSGTC). Specifically, the IMA module aims at representing an image with a set of integrated visual regions and corresponding textual concepts, reflecting certain semantics. And the VGAT module views the visual features with semantic information as observed nodes and measures the weight of importance between each two nodes in visual graph. The CSGTC supplements various relationships between visual objects by introducing additional information of the scene graph. We also qualitatively and quantitatively evaluate the model on VisDial v1.0 dataset, showing our AVLGI outperforms previous state-of-the-art models.},
  archive      = {J_ICV},
  author       = {Tianling Jiang and Hailin Shao and Xin Tian and Yi Ji and Chunping Liu},
  doi          = {10.1016/j.imavis.2021.104316},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104316},
  shortjournal = {Image Vis. Comput.},
  title        = {Aligning vision-language for graph inference in visual dialog},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive and fast image superpixel segmentation approach.
<em>ICV</em>, <em>116</em>, 104315. (<a
href="https://doi.org/10.1016/j.imavis.2021.104315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixel is one of the most popular image over-segmentations with broad applications in the computer vision field to reduce their computations by replacing pixels as primitives. The main concerns of one superpixel generation algorithm are its accuracy and efficiency. One of the most important things in superpixel accuracy is to fit the image boundaries tightly with a few pixels as possible (namely minimal contour density, which is measured by the percent of superpixel contour pixels in the whole image). In this paper, we propose a new fast algorithm based on the clustering method to produce superpixels accurately with low contour density. First, we adopt the linear path from a pixel to one superpixel seed to define a regular term and propose a new distance measurement between them. In addition, we introduce the gradient and Local Binary Pattern (LBP) features and propose formulas of parameters in the proposed method adaptively. In this way, we can use the new distance measurement to group pixels as initial regions adaptively and produce the final superpixels by merging those small ones. Finally, we test the new algorithm on two public datasets and compare it with the state-of-the-art. Our method can generate superpixels with lower contour density while being competitive in accuracy and computational time .},
  archive      = {J_ICV},
  author       = {Nannan Wang and Yongxia Zhang},
  doi          = {10.1016/j.imavis.2021.104315},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104315},
  shortjournal = {Image Vis. Comput.},
  title        = {Adaptive and fast image superpixel segmentation approach},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial likelihood voting with self-knowledge distillation
for weakly supervised object detection. <em>ICV</em>, <em>116</em>,
104314. (<a href="https://doi.org/10.1016/j.imavis.2021.104314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD), which is an effective way to train an object detection model using only image-level annotations, has attracted considerable attention from researchers. However, most of the existing methods, which are based on multiple instance learning (MIL), tend to localize instances to the discriminative parts of salient objects instead of the entire content of all objects. In this paper, we propose a WSOD framework called the Spatial Likelihood Voting with Self-knowledge Distillation Network (SLV-SD Net). In this framework, we introduce a spatial likelihood voting (SLV) module to converge region proposal localization without bounding box annotations. Specifically, in every iteration during training, all the region proposals in a given image act as voters voting for the likelihood of each category in the spatial dimensions. After dilating the alignment on the area with large likelihood values , the voting results are regularized as bounding boxes, which are then used for the final classification and localization. Based on SLV, we further propose a self-knowledge distillation (SD) module to refine the feature representations of the given image. The likelihood maps generated by the SLV module are used to supervise the feature learning of the backbone network , encouraging the network to attend to wider and more diverse areas of the image. Extensive experiments on the PASCAL VOC 2007/2012 and MS-COCO datasets demonstrate the excellent performance of SLV-SD Net. In addition, SLV-SD Net produces new state-of-the-art results on these benchmarks.},
  archive      = {J_ICV},
  author       = {Ze Chen and Zhihang Fu and Jianqiang Huang and Mingyuan Tao and Rongxin Jiang and Xiang Tian and Yaowu Chen and Xian-Sheng Hua},
  doi          = {10.1016/j.imavis.2021.104314},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104314},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatial likelihood voting with self-knowledge distillation for weakly supervised object detection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental human action recognition with dual memory.
<em>ICV</em>, <em>116</em>, 104313. (<a
href="https://doi.org/10.1016/j.imavis.2021.104313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental learning is a topic of great interest in the current state of machine learning research. Real-world problems often require a classifier to incorporate new knowledge while preserving what was learned before. One of the most challenging problems in computer vision is Human Action Recognition (HAR) in videos. However, most of the existing works approach HAR from a non-incremental point of view. This work proposes a framework for performing HAR in the incremental learning scenario called Incremental Human Action Recognition with Dual Memory (IHAR-DM). IHAR-DM contains three main components: a 3D convolutional neural network for capturing Spatio-temporal features; a Triplet Network to perform metric learning; and the dual-memory Extreme Value Machine, which is introduced in this work. The proposed method is compared with 10 other state-of-the-art incremental learning models. We propose five experimental settings containing different numbers of tasks and classes using two widely known HAR datasets: UCF-101 and HMDB51. Our results show superior performance in terms of Normalized Mutual Information (NMI) and Inter-task Intransigence (ITI), which is a new metric proposed in this work. Overall results show the feasibility of the proposal for real HAR problems, which mostly present the requirements imposed by incremental learning.},
  archive      = {J_ICV},
  author       = {Matheus Gutoski and André Eugenio Lazzaretti and Heitor Silvério Lopes},
  doi          = {10.1016/j.imavis.2021.104313},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104313},
  shortjournal = {Image Vis. Comput.},
  title        = {Incremental human action recognition with dual memory},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-based image explanations for deep neural networks.
<em>ICV</em>, <em>116</em>, 104310. (<a
href="https://doi.org/10.1016/j.imavis.2021.104310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increased use of machine learning in decision-making scenarios, there has been a growing interest in explaining and understanding the outcomes of machine learning models. Despite this growing interest, existing works on interpretability and explanations have been mostly intended for expert users. Explanations for general users have been neglected in many usable and practical applications (e.g., image tagging, caption generation). It is important for non-technical users to understand features and how they affect an instance-specific prediction to satisfy the need for justification. In this paper, we propose a model-agnostic method for generating context-based explanations aiming for general users. We implement partial masking on segmented components to identify the contextual importance of each segment in scene classification tasks. We then generate explanations based on feature importance. We present visual and text-based explanations: (i) saliency map presents the pertinent components with a descriptive textual justification, (ii) visual map with a color bar graph showing the relative importance of each feature for a prediction. Evaluating the explanations using a user study ( N = 50), we observed that our proposed explanation method visually outperformed existing gradient and occlusion based methods. Hence, our proposed explanation method could be deployed to explain models’ decisions to non-expert users in real-world applications.},
  archive      = {J_ICV},
  author       = {Sule Anjomshoae and Daniel Omeiza and Lili Jiang},
  doi          = {10.1016/j.imavis.2021.104310},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104310},
  shortjournal = {Image Vis. Comput.},
  title        = {Context-based image explanations for deep neural networks},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An analytical proof on suitability of cauchy-schwarz
divergence as the aggregation criterion in region growing algorithm.
<em>ICV</em>, <em>115</em>, 104312. (<a
href="https://doi.org/10.1016/j.imavis.2021.104312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region Growing Algorithm (RGA) is a popular, fast and strongly formed object segmentation method . In RGA, the region is grown from the seed points to adjacent points depending on an aggregation criterion. Despite the huge literature on RGA, none of the proposed aggregation criteria have been analytically proved to be suitable for an ideal segmentation. In this paper, Cauchy-Schwarz Divergence (CSD) is proved to be suitable as an aggregation criterion in RGA for object segmentation. First, RGA is formulated in this context. The Cauchy-Schwarz-based criterion is proposed here in the continuous case for a bimodal image that contains one object in the background while both regions are normally distributed with different parameters (while the assumption of normal distribution of object and background has been used by many researchers in minimum error thresholding method). Then, a proof is given that in the mentioned formulated case, the proposed RGA will lead to an ideal segmentation. The case is also investigated while object and background have heavy-tail distributions like generalized Gaussian function when β &lt; 2. While all formulations and proofs are given in the continuous case, the segmentation results in the discrete case are shown to be good. Comparison of these results with the outcomes of RGA with traditional aggregation criteria, shows how analytical justifications can suggest a better criterion.},
  archive      = {J_ICV},
  author       = {Yasser Baleghi and David Rousseau},
  doi          = {10.1016/j.imavis.2021.104312},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104312},
  shortjournal = {Image Vis. Comput.},
  title        = {An analytical proof on suitability of cauchy-schwarz divergence as the aggregation criterion in region growing algorithm},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view self-supervised learning for 3D facial texture
reconstruction from single image. <em>ICV</em>, <em>115</em>, 104311.
(<a href="https://doi.org/10.1016/j.imavis.2021.104311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years witnessed that deep learning based methods have achieved significant progresses in recovering 3D face shape from single image. However, reconstructing realistic 3D facial texture from single image is still a challenging task due to the unavailability of large-scale training datasets and the low expression ability of previous statistical texture models (e.g. 3DMM). In this paper, we introduce a novel deep architecture trained by self-supervision with multi-view setup, to reconstruct 3D facial texture. Specifically, we first obtain incomplete UV texture map from input facial image , and then introduce a Texture Completion Network (TC-Net) to inpaint missing areas. To train TC-Net, firstly, we collect 50,000 triplets of facial images from in-the-wild videos, each triplet consists of a nearly frontal, a left-side, and a right-side facial images. With this dataset, we propose a novel multi-view consistency loss that ensures consistent photometric, face identity, 3DMM identity, and UV texture among multi-view facial images. This loss allows to optimize TC-Net in a self-supervision way without using ground-truth texture map as supervision. In addition, multi-view input images are only required in training to provide self-supervision, and our method only needs single input image in inference. Extensive experiments show that our method achieves state-of-the-art performance in both qualitative and quantitative comparisons .},
  archive      = {J_ICV},
  author       = {Xiaoxing Zeng and Ruyun Hu and Wu Shi and Yu Qiao},
  doi          = {10.1016/j.imavis.2021.104311},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104311},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-view self-supervised learning for 3D facial texture reconstruction from single image},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-guided chained context aggregation for semantic
segmentation. <em>ICV</em>, <em>115</em>, 104309. (<a
href="https://doi.org/10.1016/j.imavis.2021.104309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The way features propagate in Fully Convolutional Networks is of momentous importance to capture multi-scale contexts for obtaining precise segmentation masks. This paper proposes a novel series-parallel hybrid paradigm called the Chained Context Aggregation Module (CAM) to enrich feature representation. CAM gains features of various spatial scales through chain-connected ladder-style information flows and fuses them in a two-stage process, namely pre-fusion and re-fusion. The serial flow continuously increases receptive fields of output neurons and those in parallel encode different region-based contexts. Each information flow is a shallow encoder-decoder with appropriate down-sampling scales to sufficiently capture contextual information. We further adopt an attention model in CAM to guide feature re-fusion. Based on these developments, we construct the Chained Context Aggregation Network (CANet), which employs an asymmetric decoder to recover precise spatial details of prediction maps. We conduct extensive experiments on six challenging datasets, including Pascal VOC 2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. Results evidence that CANet achieves state-of-the-art or competitive performance.},
  archive      = {J_ICV},
  author       = {Quan Tang and Fagui Liu and Tong Zhang and Jun Jiang and Yu Zhang},
  doi          = {10.1016/j.imavis.2021.104309},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104309},
  shortjournal = {Image Vis. Comput.},
  title        = {Attention-guided chained context aggregation for semantic segmentation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attribute saliency network for person re-identification.
<em>ICV</em>, <em>115</em>, 104298. (<a
href="https://doi.org/10.1016/j.imavis.2021.104298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the Attribute Saliency Network (ASNet), a deep learning model that utilizes attribute and saliency map learning for person re-identification (re-ID) task. Many re-ID methods used human pose or local body parts, either fixed position or auto-learn, to guide the learning. Person attributes, though can describe a person in greater details, are seldom used in retrieving the person&#39;s images. We therefore propose to integrate the person attributes learning into the re-ID model, and let it learns together with the person identity networks. With this arrangement, there is a synergistic effect and thus better representations are encoded. In addition, both visual and text retrievals, such as query by clothing colors, hair length, etc., are possible. We also propose to improve the granularity of the heatmap, by generating two global person attributes and body part saliency maps to capture fine-grained details of the person and thus enhance the discriminative power of the encoded vectors. As a result, we are able to achieve state-of-the-art performances. On the Market1501 dataset, we achieve 90.5% mAP and 96.3% Rank 1 accuracy. On DukeMTMC-reID, we obtained 82.7% mAP and 90.6% Rank 1 accuracy.},
  archive      = {J_ICV},
  author       = {Chiat-Pin Tay and Kim-Hui Yap},
  doi          = {10.1016/j.imavis.2021.104298},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104298},
  shortjournal = {Image Vis. Comput.},
  title        = {Attribute saliency network for person re-identification},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-tier attention network using term-weighted question
features for visual question answering. <em>ICV</em>, <em>115</em>,
104291. (<a href="https://doi.org/10.1016/j.imavis.2021.104291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a multi-modal challenging task that accepts an image and a natural language question about that image as inputs and desires to find the correct answer. This AI-complete task necessitates the fine-grained joint understanding of the two input modalities. Inspired by the success of attention mechanism in the task of efficient comprehension of visual-language features for VQA, this paper proposes a Multi-Tier Attention Network (MTAN) with the major component being term-weighted question-guided visual attention. Additionally, we introduce a novel Supervised Term Weighting (STW) scheme named ‘qf.obj.cos’ to semantically weight words utilizing the notion of visual object detection. This can be generalized to other vision-language comprehension tasks like image captioning , text-to-image-retrieval, multi-modal summarization etc. In effect, the proposed system allows the generation of more discriminative visual features from the progressive steps of question guided visual attention where question embedding is indeed guided by semantic term weighting. MTAN is quantitatively and qualitatively evaluated on the benchmark DAQUAR dataset and an extensive set of ablations are studied to demonstrate the individual significance of each of the components of the system. Experimental results certify that MTAN performs better than the previous works using the same dataset.},
  archive      = {J_ICV},
  author       = {Sruthy Manmadhan and Binsu C. Kovoor},
  doi          = {10.1016/j.imavis.2021.104291},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104291},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-tier attention network using term-weighted question features for visual question answering},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale parallel deep CNN (mpdCNN) architecture for the
real low-resolution face recognition for surveillance. <em>ICV</em>,
<em>115</em>, 104290. (<a
href="https://doi.org/10.1016/j.imavis.2021.104290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images from the surveillance networks are being extensively used for the purpose of monitoring and criminal investigations. However, it is often difficult to recognize faces using the data from the surveillance networks because the resolution of the face in the images captured is too low. Further, the low-resolution images have varying magnitude of facial feature content because of wide variations in illumination, pose, resolution and the distance from which the image is captured. Also, a single face recognition solution is not able to recognize faces efficiently in both high and low-resolution images. Wide variations in facial feature content in high and low-resolution images causes difficulty in classification of the features by a single model for the purpose of face recognition. We present a Deep-CNN based architecture called mpdCNN to solve the problem of face recognition in low as well as high-resolution images with high accuracy and robustness. Our proposed architecture mpdCNN gives 88.6% accuracy on the SCface database which is an impressive improvement over the state-of-the-art algorithms. We also achieved an accuracy of above 99% on normal to high-resolution databases for face recognition.},
  archive      = {J_ICV},
  author       = {Nayaneesh Kumar Mishra and Mainak Dutta and Satish Kumar Singh},
  doi          = {10.1016/j.imavis.2021.104290},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104290},
  shortjournal = {Image Vis. Comput.},
  title        = {Multiscale parallel deep CNN (mpdCNN) architecture for the real low-resolution face recognition for surveillance},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based reasoning attention pooling with curriculum
design for content-based image retrieval. <em>ICV</em>, <em>115</em>,
104289. (<a href="https://doi.org/10.1016/j.imavis.2021.104289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global single-pass methods have shown superior efficiency over local aggregation methods on content-based image retrieval. However, they tend to fail under challenging environments since the structural relations among regions are not exploited. To address this issue, we propose a novel Graph-based Reasoning Attention Pooling with Curriculum Design (GRAP-CD) to improve the network capability through training modification and trainable pooling. GRAP-CD can not only explore relations among salient regions but also gradually train the network to achieve better local minima. The graph-based reasoning layers regard the feature map from the last convolution layer as a graph and construct the structural relations. Then the graph-based attention layer enhances the key information guided by the relations. Besides, a front-end curriculum design is introduced to split the training dataset from simple to complex and train the model step by step, which further helps the GRAP firstly learn the basic feature information from simple samples and then learn to dig the more representative features with hard positive samples. Experimental results on popular benchmarks R R Oxford and R R Paris datasets achieve improvement over state-of-the-art global single-pass methods and competitive results with local aggregation methods.},
  archive      = {J_ICV},
  author       = {Xiaoguang Zhu and Haoyu Wang and Peilin Liu and Zhantao Yang and Jiuchao Qian},
  doi          = {10.1016/j.imavis.2021.104289},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104289},
  shortjournal = {Image Vis. Comput.},
  title        = {Graph-based reasoning attention pooling with curriculum design for content-based image retrieval},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). You look so different! Haven’t i seen you a long time ago?
<em>ICV</em>, <em>115</em>, 104288. (<a
href="https://doi.org/10.1016/j.imavis.2021.104288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-id) aims to match a query identity (ID) to an element in a gallery set, composed of elements collected from multiple cameras. Most of the existing re-id methods assume the short-term setting, where the query/gallery samples share the clothing style. In this setting, the optimal feature representations are based in the visual appearance of clothes, which considerably drops the identification performance for long-term settings. Having this problem in mind, we propose a model that learns long-term representations of persons by ignoring any features previously learned by a short-term re-id model, which naturally makes it invariant to clothing styles. We start by synthesizing a data set in which we distort the most relevant biometric information (based in face, body shape, height, and weight cues), keeping the short-term cues (color and texture of clothes) unchanged. This way, while the original data contains both ID-related and other varying features, the synthesized representations are composed mostly of short-term attributes. Then, the key to obtaining stable long-term representations is to learn embeddings of the original data that maximize the dissimilarity with the previously inferred short-term embeddings. In practice, we use the synthetic data to learn a model that embeds the ID-unrelated features and then learn a second model from the original data, where long-term embeddings are obtained, keeping their independence with respect to the previously obtained ID-unrelated features. Our experiments were performed on three challenging cloth-changing sets (LTCC, PRCC, and NKUP) and the results support the effectiveness of the proposed method, for both short and long-term re-id settings. The source code is available at https://github.com/Ehsan-Yaghoubi/You-Look-So-Different-Haven-t-I-Seen-You-a-Long-Time-Ago?},
  archive      = {J_ICV},
  author       = {Ehsan Yaghoubi and Diana Borza and Bruno Degardin and Hugo Proença},
  doi          = {10.1016/j.imavis.2021.104288},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104288},
  shortjournal = {Image Vis. Comput.},
  title        = {You look so different! haven’t i seen you a long time ago?},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level refinement enriched feature pyramid network for
object detection. <em>ICV</em>, <em>115</em>, 104287. (<a
href="https://doi.org/10.1016/j.imavis.2021.104287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class Imbalance and scales imbalance are common in object detection. A class imbalance occurs due to insufficient inequality between the number of instances with respect to different classes, while an imbalance in scale occurs when object have different scales and a different number of examples of different scales. In order to solve the problem of scale variance (scale imbalance) and class imbalance together, we propose a simple and effective feature enhancement scheme that explicitly uses all information of a multi-level structure to generate a multilevel contextual features pyramid with multiple scales. We also introduce a cascaded refinement scheme that incorporates multi-scale contextual features into the Single Shot Detector (SSD) predictive layers to improve their distinctiveness for multi-scale detection. A stack of multi-scale contextual feature modules is used in a feature enhancement scheme to merge the multi-level and multi-scale features. Then we collect the equivalent scale features over the Multi-layer Feature Fusion (MLFF) unit to construct a feature pyramid in which each feature map is made up of layers from multiple levels. More robustness and contextual information are integrated into the pyramid through chain parallel pooling operation. To improve classification and regression, a cascaded refinement scheme is proposed that effectively captures a large amount of contextual information and refines the anchors to solve the class imbalance problem . The experiments are carried out on two benchmarks datasets: MS COCO and PASCAL VOC 07/12. Our proposed approach achieves state-of-the-art accuracy with an AP of 40.6 in the case of multi-scale inference on MS COCO Test-dev (input size 320 × 320). For 512 × 512 input on the MS COCO Test-dev, our approach leads in an absolute gain in precision of 1.8% compared to the best reported results of single-stage detector (AP: 45.7).},
  archive      = {J_ICV},
  author       = {Lubna Aziz and Md. Sah Bin Haji Salam FC and Sara Ayub},
  doi          = {10.1016/j.imavis.2021.104287},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104287},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-level refinement enriched feature pyramid network for object detection},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-granularity for knowledge distillation. <em>ICV</em>,
<em>115</em>, 104286. (<a
href="https://doi.org/10.1016/j.imavis.2021.104286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the fact that students have different abilities to understand the knowledge imparted by teachers, a multi-granularity distillation mechanism is proposed for transferring more understandable knowledge for student networks. A multi-granularity self-analyzing module of the teacher network is designed, which enables the student network to learn knowledge from different teaching patterns. Furthermore, a stable excitation scheme is proposed to train the student under robust supervision. The proposed distillation mechanism can be embedded into different distillation frameworks, which are taken as baselines. Experiments show the mechanism improves the accuracy by 0.58% on average and by 1.08% in the best over the baselines, which makes its performance superior to the state-of-the-arts. It is also exploited that the student&#39;s ability of fine-tuning and robustness to noisy inputs can be improved via the proposed mechanism. The code is available at https://github.com/shaoeric/multi-granularity-distillation .},
  archive      = {J_ICV},
  author       = {Baitan Shao and Ying Chen},
  doi          = {10.1016/j.imavis.2021.104286},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104286},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-granularity for knowledge distillation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HPRNet: Hierarchical point regression for whole-body human
pose estimation. <em>ICV</em>, <em>115</em>, 104285. (<a
href="https://doi.org/10.1016/j.imavis.2021.104285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new bottom-up one-stage method for whole-body pose estimation, which we call “hierarchical point regression,” or HPRNet for short. In standard body pose estimation, the locations of ~17 major joints on the human body are estimated. Differently, in whole-body pose estimation, the locations of fine-grained keypoints (68 on face, 21 on each hand and 3 on each foot) are estimated as well, which creates a scale variance problem that needs to be addressed. To handle the scale variance among different body parts, we build a hierarchical point representation of body parts and jointly regress them. The relative locations of fine-grained keypoints in each part (e.g. face) are regressed in reference to the center of that part, whose location itself is estimated relative to the person center. In addition, unlike the existing two-stage methods, our method predicts whole-body pose in a constant time independent of the number of people in an image. On the COCO WholeBody dataset, HPRNet significantly outperforms all previous bottom-up methods on the keypoint detection of all whole-body parts (i.e. body, foot, face and hand); it also achieves state-of-the-art results on face (75.4 AP) and hand (50.4 AP) keypoint detection. Code and models are available at https://github.com/nerminsamet/HPRNet.git .},
  archive      = {J_ICV},
  author       = {Nermin Samet and Emre Akbas},
  doi          = {10.1016/j.imavis.2021.104285},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104285},
  shortjournal = {Image Vis. Comput.},
  title        = {HPRNet: Hierarchical point regression for whole-body human pose estimation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformer models for enhancing AttnGAN based text to image
generation. <em>ICV</em>, <em>115</em>, 104284. (<a
href="https://doi.org/10.1016/j.imavis.2021.104284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are capable of producing photographic images that depict given natural language text descriptions. Such models have huge potential in applications such as interior designing, video games, editing and facial sketching for digital forensics. However, only a limited number of methods in the literature have been developed for text to image (TTI) generation. Most of them use Generative Adversarial Networks (GAN) based deep learning methods. Attentional GAN (AttnGAN) is a popular GAN based TTI method that extracts meaningful information from the given text descriptions using attention mechanism. In this paper, we investigate the use of different Transformer models such as BERT, GPT2, XLNet with AttnGAN to solve the challenge of extracting semantic information from the text descriptions. Hence, the proposed AttnGAN TRANS architecture has three variants AttnGAN BERT , AttnGAN XL and AttnGAN GPT . The proposed method is successful over the conventional AttnGAN and gives a boosted inception score by 27.23% and a decline of Frechet inception distance by 49.9%. The results in our experiments indicate that the proposed method has the potential to outperform the contemporary state-of-the-art methods and validate the use of Transformer models in improving the performance of TTI generation. The code is made publicly available at https://github.com/sairamkiran9/AttnGAN-trans .},
  archive      = {J_ICV},
  author       = {S. Naveen and M. S. S Ram Kiran and M. Indupriya and T.V. Manikanta and P.V. Sudeep},
  doi          = {10.1016/j.imavis.2021.104284},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104284},
  shortjournal = {Image Vis. Comput.},
  title        = {Transformer models for enhancing AttnGAN based text to image generation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-type decision fusion network for visual q&amp;a.
<em>ICV</em>, <em>115</em>, 104281. (<a
href="https://doi.org/10.1016/j.imavis.2021.104281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing VQA task contains questions of various complexity, which can be divided into the object-level and relation-level categories. The former requires the model to focus on individual objects of images to answer the question, such as “Is a man in the image?”, denoted by object-QA, while the latter depends on a better understanding in the relation of pairwise objects to infer the answer, such as “What is the man doing now?”, denoted by relation-QA. However, existing methods indiscriminately rely on the single network to deal with multi-type question categories, which cannot meet the comprehensive nature of VQA task. To this end, we introduce the novel multi-type decision fusion model, which can adaptively leverage object- and relation-level decision networks to infer the answer. Our approach consists of three key components: 1) the object-QA network designs the attention layer to search for object regions in an image that are associated with the answer. 2) the relation-QA network captures the question aware relation information from the visual scene graph, for the answer inference; 3) the proposed balance gate involves the outputs of object- and relation-QA networks to adaptively predict the final output. Extensive experiments on two benchmarks demonstrate that the proposed method can achieve competing performance against the state-of-the-art methods. Additional ablation studies further validate its effectiveness.},
  archive      = {J_ICV},
  author       = {An-An Liu and Zimu Lu and Ning Xu and Weizhi Nie and Wenhui Li},
  doi          = {10.1016/j.imavis.2021.104281},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104281},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-type decision fusion network for visual Q&amp;A},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MUMC: Minimizing uncertainty of mixture of cues.
<em>ICV</em>, <em>115</em>, 104280. (<a
href="https://doi.org/10.1016/j.imavis.2021.104280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating natural questions from an image is a semantic task that requires using vision and language modalities to learn multimodal representations. Images can have multiple visual and language cues such as places, captions, and tags. In this paper, we propose a principled deep Bayesian learning framework that combines these cues to produce natural questions. We observe that with the addition of more cues and by minimizing uncertainty in the among cues, the Bayesian network becomes more confident. We propose a Minimizing Uncertainty of Mixture of Cues (MUMC), that minimizes uncertainty present in a mixture of cues experts for generating probabilistic questions. This is a Bayesian framework and the results show a remarkable similarity to natural questions as validated by a human study. Ablation studies of our model indicate that a subset of cues is inferior at this task and hence the principled fusion of cues is preferred. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE, and CIDEr). Here, we provide project link for Deep Bayesian VQG: https://delta-lab-iitk.github.io/BVQG/ .},
  archive      = {J_ICV},
  author       = {Badri N. Patro and Vinod K. Kurmi and Sandeep Kumar and Vinay P. Namboodiri},
  doi          = {10.1016/j.imavis.2021.104280},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104280},
  shortjournal = {Image Vis. Comput.},
  title        = {MUMC: Minimizing uncertainty of mixture of cues},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boundary guidance network for camouflage object detection.
<em>ICV</em>, <em>114</em>, 104283. (<a
href="https://doi.org/10.1016/j.imavis.2021.104283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflage object detection (COD) aims to detect camouflaged objects hidden in the background region in an image. The difficulty of COD lies in the fact that camouflaged objects are often accompanied with weak boundaries, low contrast, and similar patterns to the background. Although various methods have been proposed to address these challenges, they still suffer from coarse object boundaries. In this work, we design a novel boundary guidance network for COD, which follows a two-step framework: localization and refinement. Firstly, an Initial Localization Decoder is proposed to capture multi-scale cues by embedding a Hierarchical-Split Convolution block. After obtained the coarse localization of the camouflaged object, we further propose a Residual Refinement Decoder to fix the missing object parts and boundary details progressively. Each of the proposed decoder consists of a region branch and a boundary branch for object detection and boundary detection respectively. To sufficiently leverage their complementary features, we design a novel Boundary-Guide-Region module. Benefiting from the guidance of the boundary feature, the region branch can focus on the inside parts of the boundary for residual learning, thus leads to more accurate detection. Extensive experimental results on four benchmark datasets demonstrate that our method outperforms existing state-of-the-art algorithms in both object accuracy and boundary accuracy with real-time speed.},
  archive      = {J_ICV},
  author       = {Xiuqi Xu and Mingyu Zhu and Jinhao Yu and Shuhan Chen and Xuelong Hu and Yuequan Yang},
  doi          = {10.1016/j.imavis.2021.104283},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104283},
  shortjournal = {Image Vis. Comput.},
  title        = {Boundary guidance network for camouflage object detection},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of deep learning techniques for 2D and 3D human
pose estimation. <em>ICV</em>, <em>114</em>, 104282. (<a
href="https://doi.org/10.1016/j.imavis.2021.104282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring human pose from a monocular RGB image remains an interesting field of research in computer vision . It serves as a fundamental key for many real-world applications, including human-computer interaction, animation, and detecting abnormal or illegal human behavior. Despite the considerable progress made in this area during the last decade, the proposed methods face serious problems due to the huge variations in human appearance, occlusions, noisy backgrounds, viewpoints, and other factors that can change the context of the captured information. In this paper, we introduce a survey of state-of-the-art methods to highlight various research that have been proposed to tackle the 2D and 3D pose estimation tasks. Based on the number of persons in the image, two main pipelines are identified: single-person and multi-person methods. Each of these categories is divided into two groups according to the proposed architectures. Also, we provide a brief description of current datasets and the different metrics applied to evaluate the methods performances. Finally, we include a discussion about the advantages and disadvantages of the mentioned strategies.},
  archive      = {J_ICV},
  author       = {Miniar Ben Gamra and Moulay A. Akhloufi},
  doi          = {10.1016/j.imavis.2021.104282},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104282},
  shortjournal = {Image Vis. Comput.},
  title        = {A review of deep learning techniques for 2D and 3D human pose estimation},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time semantic segmentation with weighted
factorized-depthwise convolution. <em>ICV</em>, <em>114</em>, 104269.
(<a href="https://doi.org/10.1016/j.imavis.2021.104269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has achieved great success with the popularity of convolutional neural networks (CNNs). However, the huge computational burden restricts the application of most existing networks on edge devices with strict inference time constraints. To solve this problem, a weighted factorized-depthwise convolution network (WFDCNet) is presented in this paper, which contains full- dimensional continuous separation convolution (FCS) modules and a lateral asymmetric pyramid fusion (LAPF) module, aiming to obtain high accuracy without damaging inference speed. Specifically, the FCS module enables the calculation of each dimension to be completed independently in a continuous separation process and uses simplified SE (SSE) attention layer to adjust the channels, achieving the extensive extraction of feature information. The LAPF module is able to eliminate semantic divergence and fuse feature maps of three different scales to realize the combination of multiple information from the front-end and the back-end network. WFDCNet shows superior performance on Cityscapes, Camvid, Mapillary Vistas and COCO-Stuff datasets. Especially, the experimental results demonstrate that our network achieves 73.7% mIoU on Cityscapes dataset, with the inference speed of 102.6FPS on a single RTX 2080 Ti GPU , and 17.2FPS on Jetson TX2.},
  archive      = {J_ICV},
  author       = {Xiaochen Hao and Xingjun Hao and Yaru Zhang and Yuanyuan Li and Chao Wu},
  doi          = {10.1016/j.imavis.2021.104269},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104269},
  shortjournal = {Image Vis. Comput.},
  title        = {Real-time semantic segmentation with weighted factorized-depthwise convolution},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advances in domain adaptation for computer vision.
<em>ICV</em>, <em>114</em>, 104268. (<a
href="https://doi.org/10.1016/j.imavis.2021.104268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Pourya Shamsolmoali and Salvador García and Huiyu Zhou and M. Emre Celebi},
  doi          = {10.1016/j.imavis.2021.104268},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104268},
  shortjournal = {Image Vis. Comput.},
  title        = {Advances in domain adaptation for computer vision},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Activity guided multi-scales collaboration based on
scaled-CNN for saliency prediction. <em>ICV</em>, <em>114</em>, 104267.
(<a href="https://doi.org/10.1016/j.imavis.2021.104267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual saliency prediction has achieved significant improvements with the advent of convolutional neural networks , but the breakthrough in saliency prediction accuracy comes at the high computational cost. In this paper, we present a lightweight saliency prediction model based on scaled up convolutional neural networks (CNN), utilizing image activity guided collaboration learning of global and local information at multiple scales. we use a pseudo-siamese network with a scaled up network (EfficientNet) as the backbone, and the two branches of the network respectively capture the global saliency feature and high-level local feature . Concretely, we first utilize the image complexity-related activity features (Image Activity Measure) as our low-level local salience prior, and then feed the input images and the activity maps to scaled up CNN modules to further learn high-level features in a multi-scale collaboration manner. Through extensive evaluation, we show that the proposed method exhibits competitive and consistent results on the challenging benchmark datasets, and our method has better prediction performance, fewer trainable parameters and faster inference speed. Moreover, the proposed model has low requirements for platform computing capabilities, which improves the universality of saliency application scenarios.},
  archive      = {J_ICV},
  author       = {Deqiang Cheng and Ruihang Liu and Jiahan Li and Song Liang and Qiqi Kou and Kai Zhao},
  doi          = {10.1016/j.imavis.2021.104267},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104267},
  shortjournal = {Image Vis. Comput.},
  title        = {Activity guided multi-scales collaboration based on scaled-CNN for saliency prediction},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bioinspired retinal neural network for accurately
extracting small-target motion information in cluttered backgrounds.
<em>ICV</em>, <em>114</em>, 104266. (<a
href="https://doi.org/10.1016/j.imavis.2021.104266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust and accurate detection of small moving targets in cluttered moving backgrounds is a significant and challenging problem for robotic visual systems to perform search and tracking tasks. Inspired by the neural circuitry of elementary motion vision in the mammalian retina, this paper proposes a bioinspired retinal neural network based on a new neurodynamics-based temporal filtering and multiform 2-D spatial Gabor filtering. This model can estimate motion direction accurately via only two perpendicular spatiotemporal filtering signals, and respond to small targets of different sizes and velocities through adjusting the dendrite field size of spatial filter . Meanwhile, an algorithm of directionally selective inhibition is proposed to suppress the target-like features in the moving background, which can reduce the influence of background motion effectively. Extensive synthetic and real-data experiments show that the proposed model works stably for small targets of a wider size and velocity range, and has better detection performance than other bioinspired models. Additionally, it can also extract the information of motion direction and motion energy accurately and rapidly.},
  archive      = {J_ICV},
  author       = {Xiao Huang and Hong Qiao and Hui Li and Zhihong Jiang},
  doi          = {10.1016/j.imavis.2021.104266},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104266},
  shortjournal = {Image Vis. Comput.},
  title        = {A bioinspired retinal neural network for accurately extracting small-target motion information in cluttered backgrounds},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense graph convolutional neural networks on 3D meshes for
3D object segmentation and classification. <em>ICV</em>, <em>114</em>,
104265. (<a href="https://doi.org/10.1016/j.imavis.2021.104265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents new designs of graph convolutional neural networks (GCNs) on 3D meshes for 3D object segmentation and classification. We use the faces of the mesh as basic processing units and represent a 3D mesh as a graph where each node corresponds to a face. To enhance the descriptive power of the graph, we introduce a 1-ring face neighborhood structure to derive novel multi-dimensional spatial and structure features to represent the graph nodes . Based on this new graph representation , we then design a densely connected graph convolutional block which aggregates local and regional features as the key construction component to build effective and efficient practical GCN models for 3D object classification and segmentation. We present experimental results to show that our new technique performs comparably to state of the art across a number of benchmark datasets where our models are also shown to have smaller number of parameters. We also present ablation studies to demonstrate the soundness of our design principles and the effectiveness of our practical models.},
  archive      = {J_ICV},
  author       = {Wenming Tang and Guoping Qiu},
  doi          = {10.1016/j.imavis.2021.104265},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104265},
  shortjournal = {Image Vis. Comput.},
  title        = {Dense graph convolutional neural networks on 3D meshes for 3D object segmentation and classification},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Omnidirectional stereo depth estimation based on spherical
deep network. <em>ICV</em>, <em>114</em>, 104264. (<a
href="https://doi.org/10.1016/j.imavis.2021.104264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional depth estimation is an emerging research topic and has received significant attention in recent years. However, the existing methods were developed based on the theory of planar stereo matching ; and introduce the nonlinear epipolar constraint and significant distortions of re-projections. In this paper, we propose a novel approach that use spherical CNNs and the epipolar constraint on sphere for omnidirectional depth estimation. We discuss the epipolar constraint for spherical stereo imaging and convert the nonlinear constraint on a planar projection to the linear constraint on a sphere. We then propose a Spherical Convolution Residual Network (SCRN) for omnidirectional depth estimation via the spherical linear epipolar constraint. The input equirectangular projection (ERP) images are sampled to spherical meshes and fed into SCRN to calculate spherical depth maps. For 2D visualization, we design a Planar Refinement Network (PRN) and adopt the cascade learning scheme to improve the accuracy of depth maps. This scheme reduces the errors caused by projection, interpolation, and the limitation of spherical representation. The experiment shows that our full scheme Cascade Spherical Depth Network (CSDNet) results in more accurate and detailed depth maps with lower errors, as compared to recent seminal works. Our approach yields the comparable performance to the other state-of-the-art works on the omnidirectional stereo datasets with less number of parameters. The effectiveness of the spherical network and the cascade learning scheme is validated, and the influence of spherical sampling density is also discussed.},
  archive      = {J_ICV},
  author       = {Ming Li and Xuejiao Hu and Jingzhao Dai and Yang Li and Sidan Du},
  doi          = {10.1016/j.imavis.2021.104264},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104264},
  shortjournal = {Image Vis. Comput.},
  title        = {Omnidirectional stereo depth estimation based on spherical deep network},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepFH segmentations for superpixel-based object proposal
refinement. <em>ICV</em>, <em>114</em>, 104263. (<a
href="https://doi.org/10.1016/j.imavis.2021.104263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-agnostic object proposal generation is an important first step in many object detection pipelines. However, object proposals of modern systems are rather inaccurate in terms of segmentation and only roughly adhere to object boundaries. Since typical refinement steps are usually not applicable to thousands of proposals, we propose a superpixel-based refinement system for object proposal generation systems. Utilizing precise superpixels and superpixel pooling on deep features, we refine initial coarse proposals in an end-to-end learned system. Furthermore, we propose a novel DeepFH segmentation, which enriches the classic Felzenszwalb and Huttenlocher (FH) segmentation with deep features leading to improved segmentation results and better object proposal refinements. On the COCO dataset with LVIS annotations, we show that our refinement based on DeepFH superpixels outperforms state-of-the-art methods and leads to more precise object proposals.},
  archive      = {J_ICV},
  author       = {Christian Wilms and Simone Frintrop},
  doi          = {10.1016/j.imavis.2021.104263},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104263},
  shortjournal = {Image Vis. Comput.},
  title        = {DeepFH segmentations for superpixel-based object proposal refinement},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic information enhancement for video classification.
<em>ICV</em>, <em>114</em>, 104244. (<a
href="https://doi.org/10.1016/j.imavis.2021.104244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to extract and integrate spatiotemporal information for video classification is a major challenge. Advanced approaches adopt 2D, and 3D convolution kernels, or their variants as a basis of a spatiotemporal modeling process. However, 2D convolution kernels perform poorly along the temporal dimension, while 3D convolution kernels tend to create confusion between the spatial and temporal sources of information, with an increased risk of explosion of the number of model parameters. In this paper, we develop a more explicit way to improve the spatiotemporal modeling capacity of a 2D convolution network, which integrates two components: (1) Using Motion Intensification Block (MIB) to mandate a specific subset of channels to explicitly encode temporal clues to complement the spatial patterns extracted by other channels, achieving controlled diversity in the convolution calculations. (2) Using Spatial-temporal Squeeze-and-excitation (ST-SE) block to intensify the fused features reflecting the importance of different channels. In this manner, we improve the spatiotemporal dynamic information within the 2D backbone network, without performing complex temporal convolutions. To verify the effectiveness of the proposed approach, we conduct extensive experiments on challenging benchmarks. Our model achieves a competitive result on Something-Something V1, Something-Something V2, and a state-of-the-art performance on the Diving48 dataset, providing supporting evidence for the merits of the proposed methodology of spatiotemporal information encoding and fusion for video classification.},
  archive      = {J_ICV},
  author       = {Rong-Chang Li and Xiao-Jun Wu and Cong Wu and Tian-Yang Xu and Josef Kittler},
  doi          = {10.1016/j.imavis.2021.104244},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104244},
  shortjournal = {Image Vis. Comput.},
  title        = {Dynamic information enhancement for video classification},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human–object interaction detection with missing objects.
<em>ICV</em>, <em>113</em>, 104262. (<a
href="https://doi.org/10.1016/j.imavis.2021.104262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–object interaction (HOI) detection is an important vision task that requires the detection of individual object instances and reasoning of their relations. Despite encouraging advancement in recent years, past methods are still limited to relatively simple images where the human and object instances can be detected without difficulties. HOI in the wild should work even when the objects that a person is interacting with are not visible or hard to detect in the image. In this paper, we formulate HOI with missing objects (HOI-MO) as a research problem, and show that it is indeed critical as many such instances can be found even in the commonly-used public HOI detection datasets. We label these to compose new test sets for the proposed method. To our knowledge, we introduce the first method for such challenging HOI detection that incorporates global scene information. The effectiveness and superiority of the proposed method are demonstrated through extensive experiments and comparisons.},
  archive      = {J_ICV},
  author       = {Kaen Kogashi and Yang Wu and Shohei Nobuhara and Ko Nishino},
  doi          = {10.1016/j.imavis.2021.104262},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104262},
  shortjournal = {Image Vis. Comput.},
  title        = {Human–object interaction detection with missing objects},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A lightweight network for monocular depth estimation with
decoupled body and edge supervision. <em>ICV</em>, <em>113</em>, 104261.
(<a href="https://doi.org/10.1016/j.imavis.2021.104261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning depth from a single image is a challenging task in computer vision . Many recent works on monocular depth estimation explore increasingly large convolutional neural networks to learn monocular cues implicitly. Such methods may fail to generalize well around object boundaries as large networks tend to distort the fine details (such as edges and corners) in low-resolution layers, leading to a poor depth prediction near object edges. To reduce depth loss near object boundaries, this paper proposes to explicitly decouple depth features for the body and edges of objects corresponding to low and high-frequency regions of an image, respectively. To this end, we learn a flow field to warp depth features into consistent body features and residual edge features. Afterward, decoupled supervision is employed on both sets of features to learn body and edge depth maps explicitly. Moreover, we also propose a lightweight encoder-decoder network that efficiently combines features at multiple scales to alleviate the loss of fine details in the final feature map. Extensive experiments on NYUD-v2 and KITTI datasets demonstrate that our proposed lightweight network with depth decoupling performs comparably to state-of-the-art methods while drastically reducing the number of parameters.},
  archive      = {J_ICV},
  author       = {Usman Ali and Bayram Bayramli and Tamam Alsarhan and Hongtao Lu},
  doi          = {10.1016/j.imavis.2021.104261},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104261},
  shortjournal = {Image Vis. Comput.},
  title        = {A lightweight network for monocular depth estimation with decoupled body and edge supervision},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introducing the structural bases of typicality effects in
deep learning. <em>ICV</em>, <em>113</em>, 104249. (<a
href="https://doi.org/10.1016/j.imavis.2021.104249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we hypothesize that the effects of the degree of typicality in natural semantic categories can be generated based on the structure of artificial categories learned with deep learning models . Motivated by the human approach to representing natural semantic categories and based on the Prototype Theory foundations, we propose a novel Computational Prototype Model (CPM) to represent the internal structure of semantic categories. Unlike other prototype learning approaches , our mathematical framework proposes a first approach to provide deep neural networks with the ability to model abstract semantic concepts such as category central semantic meaning, typicality degree of an object&#39;s image, and family resemblance relationship. We proposed several methodologies based on the typicality&#39;s concept to evaluate our CPM-model in image semantic processing tasks such as image classification , a global semantic description , and transfer learning . Our experiments on different image datasets, such as ImageNet and Coco, showed that our approach might be an admissible proposition in the effort to endow machines with greater power of abstraction for the semantic representation of objects&#39; categories.},
  archive      = {J_ICV},
  author       = {Omar Vidal Pino and Erickson R. Nascimento and Mario F.M. Campos},
  doi          = {10.1016/j.imavis.2021.104249},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104249},
  shortjournal = {Image Vis. Comput.},
  title        = {Introducing the structural bases of typicality effects in deep learning},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-based parameter isolation for foreground segmentation
without catastrophic forgetting using multi-scale region and edges
fusion network. <em>ICV</em>, <em>113</em>, 104248. (<a
href="https://doi.org/10.1016/j.imavis.2021.104248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreground segmentation of moving objects is widely used in different computer vision applications ; however, existing deep learning-based methods generally suffer from overall degraded F-measure performance. The two main sources that degrade the F-measure are under-segmentation and catastrophic forgetting. Under-segmentation is the problem of misdetecting objects&#39; fine details. The catastrophic forgetting problem occurs when training on a large number of video sequences that leads to forgetting information learned from early video sequences. This paper proposes a novel multi-scale region and edges fusion network with task-based parameter isolation (REFNet-TBPI) to overcome these two problems. The proposed method consists of a novel multi-scale region and edges fusion network (REFNet) to capture the moving objects&#39; boundary details by extracting regions and boundary edges of each object at different feature scales and fusing them to produce high-detailed segmented objects. REFNet is trained using a novel continual learning technique called task-based parameter isolation (TBPI) to overcome the catastrophic forgetting problem. The proposed method (REFNet-TBPI) is extensively evaluated on three benchmarks, namely CDnet2014, DAVIS2016, and SegTrack. By comparing REFNet-TBPI with current state-of-the-art methods, the proposed method outperforms the best-reported state-of-the-art by 4.4% on average.},
  archive      = {J_ICV},
  author       = {Islam Osman and Agwad Eltantawy and Mohamed S. Shehata},
  doi          = {10.1016/j.imavis.2021.104248},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104248},
  shortjournal = {Image Vis. Comput.},
  title        = {Task-based parameter isolation for foreground segmentation without catastrophic forgetting using multi-scale region and edges fusion network},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synergic learning for noise-insensitive webly-supervised
temporal action localization. <em>ICV</em>, <em>113</em>, 104247. (<a
href="https://doi.org/10.1016/j.imavis.2021.104247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Webly-supervised temporal action localization (WebTAL) leverages web videos to train localization models without requiring manual temporal annotations. WebTAL is extremely challenging since video-level labels on the web are always noisy, seriously damaging the overall performance. Most state-of-the-art methods filter out noise before training, which will inevitably reduce the training samples . In contrast, we propose a preprocessing-free WebTAL framework along with a new synergic learning paradigm to alleviate the noise interference. Specifically, we introduce a synergic task called Spatio-Temporal Order Prediction (STOP) for spatio-temporal representation learning . This task requires a network to arrange permuted spatial crops and temporal clips, thereby learning the inherent spatial semantics and temporal interactions in videos. Instead of pre-extracting features with the well-trained STOP, we design a novel synergic learning paradigm called Warm-up Synergic Training (WST) to iteratively generate better spatio-temporal representations and improve action localization results. In this synergic fashion, experimental results show that the interference caused by label noise will be largely mitigated. We demonstrate that our method outperforms all other WebTAL methods on two public benchmarks, THUMOS&#39;14 and ActivityNet v1.2.},
  archive      = {J_ICV},
  author       = {Can Zhang and Meng Cao and Dongming Yang and Ji Jiang and Yuexian Zou},
  doi          = {10.1016/j.imavis.2021.104247},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104247},
  shortjournal = {Image Vis. Comput.},
  title        = {Synergic learning for noise-insensitive webly-supervised temporal action localization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flow guided mutual attention for person re-identification.
<em>ICV</em>, <em>113</em>, 104246. (<a
href="https://doi.org/10.1016/j.imavis.2021.104246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (ReID) is a challenging problem in many video analytics and surveillance applications, where a person&#39;s identity must be associated across a distributed non-overlapping network of cameras. Video-based person ReID has recently gained much interest given the potential for capturing discriminant spatio-temporal information from video clips that is unavailable for image-based ReID. Despite recent advances, deep learning (DL) models for video ReID often fail to leverage this information to improve the robustness of feature representations. In this paper, the motion pattern of a person is explored as an additional cue for ReID. In particular, a flow-guided Mutual Attention network is proposed for fusion of bounding box and optical flow sequences over tracklets using any 2D-CNN backbone, allowing to encode temporal information along with spatial appearance information. Our Mutual Attention network relies on the joint spatial attention between image and optical flow feature maps to activate a common set of salient features. In addition to flow-guided attention, we introduce a method to aggregate features from longer input streams for better video sequence-level representation. Our extensive experiments on three challenging video ReID datasets indicate that using the proposed approach allows to improve recognition accuracy considerably with respect to conventional gated-attention networks, and state-of-the-art methods for video-based person ReID.},
  archive      = {J_ICV},
  author       = {Madhu Kiran and Amran Bhuiyan and Le Thanh Nguyen-Meidine and Louis-Antoine Blais-Morin and Ismail Ben Ayed and Eric Granger},
  doi          = {10.1016/j.imavis.2021.104246},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104246},
  shortjournal = {Image Vis. Comput.},
  title        = {Flow guided mutual attention for person re-identification},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new perceptual hashing method for verification and
identity classification of occluded faces. <em>ICV</em>, <em>113</em>,
104245. (<a href="https://doi.org/10.1016/j.imavis.2021.104245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, research communities on Computer Vision and biometrics have shown a lot of interest in face verification and classification methods. Fighting against Child Sexual Exploitation Material (CSEM) is one of the applications that might benefit most from these advances. In CSEM, discriminative parts of the face, i.e. mostly the eyes, are often occluded to make the victim identification more difficult. Most of the current face recognition methods are not able to handle such kind of occlusions. To overcome this problem, we present One-Shot Frequency Dominant Neighborhood Structure (OSF-DNS), a new perceptual hashing method that shows advantages on two scenarios: (a) occluded face verification, i.e., matching occluded faces with their non-occluded versions, and (b) face classification, i.e., getting the identity of an occluded face by means of a classifier trained with the non-occluded faces using the perceptual hash codes as feature vectors. We have compared the face verification performance of OSF-DNS with three perceptual hashing methods and with the features obtained from five deep learning techniques, using the occluded versions of six different facial datasets. The proposed method achieves accuracies between 69.24% and 99.46% depending on the dataset, and always higher than the compared methods. For the face classification task , we compared the performance of OSF-DNS with the features obtained by four deep learning techniques . Experimental results on LFW and CFPW datasets showed that the proposed hashing method outperformed the results obtained with deep features with an accuracy up to 89.53%.},
  archive      = {J_ICV},
  author       = {Rubel Biswas and Víctor González-Castro and Eduardo Fidalgo and Enrique Alegre},
  doi          = {10.1016/j.imavis.2021.104245},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104245},
  shortjournal = {Image Vis. Comput.},
  title        = {A new perceptual hashing method for verification and identity classification of occluded faces},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MFC-net: Multi-feature fusion cross neural network for
salient object detection. <em>ICV</em>, <em>113</em>, 104243. (<a
href="https://doi.org/10.1016/j.imavis.2021.104243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although methods based on the fully convolutional neural networks (FCNs) have shown strong advantages in the field of salient object detection , the existing methods still have two challenging issues: insufficient multi-level feature fusion ability and boundary blur. To overcome these issues, we propose a novel salient object detection method based on a multi-feature fusion cross network (denoted MFC-Net). Firstly, to overcome the issue of insufficient multi-level feature fusion ability, inspired by the connection mode of human brain neurons, we propose a novel cross network framework, combined with contextual feature transfer modules (CFTMs) to integrate, enhance and transmit multi-level feature information in an iterative manner. Secondly, to address the issue of blurred boundaries, we effectively enhance the edge features of saliency map by a simple edge enhancement strategy. Thirdly, to reduce the loss of information caused by the saliency map generated by multi-level feature fusion, we use feature fusion modules (FFMs) to learn contextual feature information from multiple angles and then output the resulting saliency map. Finally, a hybrid loss function fully supervises the network at the pixel and object level, optimizing the network performance. The proposed MFC-Net has been evaluated using five benchmark datasets. The performance evaluation demonstrates that the proposed method outperforms other state-of-the-art methods, which proves the superiority of MFC-Net approach.},
  archive      = {J_ICV},
  author       = {Zhenyu Wang and Yunzhou Zhang and Yan Liu and Shichang Liu and Sonya Coleman and Dermot Kerr},
  doi          = {10.1016/j.imavis.2021.104243},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104243},
  shortjournal = {Image Vis. Comput.},
  title        = {MFC-net: Multi-feature fusion cross neural network for salient object detection},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic and edge-based visual odometry by joint minimizing
semantic and edge distance error. <em>ICV</em>, <em>113</em>, 104240.
(<a href="https://doi.org/10.1016/j.imavis.2021.104240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the progress made in deep learning for semantic segmentation has advanced development of semantic visual odometry (VO). Along with point-based and direct methods, VO has recently used edge features. However, mismatches are common in scenes in which the distribution of edges is complex owing to the lack of appropriate descriptors for edges at the present. In this paper, we propose a semantic-segmentation-aided edge-based VO (DSEVO). It is intended to improve the localization accuracy by decreasing mismatches in the edge alignment. In the reprojection process, the semantic and edge distance residual are considered to reduce the mismatches of edges between different frames. Then, camera motion estimation is accomplished by jointly minimizing the semantic and edge cost function . Our proposed method was evaluated on the public VKITTI and TUM RGB-D datasets. It was compared with state-of-the-art methods, including the respective feature-point-based, direct, and edge-based methods. We implemented a semantic-edge-based VO system. The experimental results showed that our method achieved the highest accuracy on most of the testing sequences.},
  archive      = {J_ICV},
  author       = {Jingquan Peng and Yanqing Liu and Haochen Jiang},
  doi          = {10.1016/j.imavis.2021.104240},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104240},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic and edge-based visual odometry by joint minimizing semantic and edge distance error},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label image recognition with two-stream dynamic graph
convolution networks. <em>ICV</em>, <em>113</em>, 104238. (<a
href="https://doi.org/10.1016/j.imavis.2021.104238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies use Graph Convolution Networks (GCN) to model label correlation for multi-label images because of the outstanding performance of GCN in relational modeling tasks. However, the traditional GCN has low generalization, and the current state-of-the-arts&#39; accuracy is poor. Therefore, we propose a Two-Stream Dynamic Graph Convolution Network (2S-DGCN) to improve the performance of multi-label image recognition. In 2S-DGCN, we first obtain the Up Confidence Score of prediction categories (UCS), the content-aware category and the label discriminant vector by a Semantic Attention Module (SAM) and a Dynamic Graph Convolution Network (DGCN) in upstream. Then fed the new graph feature nodes reconstructed by lateral embedding the content-aware category and the label discriminant vector into a DGCN to produce the Down Confidence Score of prediction categories (DCS) in downstream. Finally, the Final Confidence Score of prediction categories (FCS) for multi-label image recognition is synthesized by fusing the UCS and DCS. Extensive experiments on the public multi-label benchmarks achieve mAPs of 85.6% on MS-COCO and 95.4% on VOC 2007. The results of compared experiment and visualization demonstrate that our method has better performance than the current state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Pingping Cao and Pengpeng Chen and Qiang Niu},
  doi          = {10.1016/j.imavis.2021.104238},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104238},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-label image recognition with two-stream dynamic graph convolution networks},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CFFNet: Coordinated feature fusion network for crowd
counting. <em>ICV</em>, <em>112</em>, 104242. (<a
href="https://doi.org/10.1016/j.imavis.2021.104242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long skip connection or encoder-decoder networks for crowd counting have proven to be effective methods to generate high-resolution density maps. However, the simple and coarse feature fusion ignores the disharmony between features, that is, spatial misalignment and semantic inconsistency, which will weaken feature representation and degrade network performance. In this paper, we propose an end-to-end trainable architecture called Coordinated Feature Fusion Network (CFFNet) to tackle the aforementioned problems. The proposed model contains a powerful baseline network and embeds two primary modules: Spatial Alignment Module (SAM) and Semantic Consistency Module (SCM). Specifically, the SAM can learn the transformation offset of pixels to alleviate the spatial misalignment caused by the feature resolution difference; the SCM based on the multi-scale attention mechanism can capture pixel-wise weight to alleviate the semantic inconsistency due to the feature level gap. Extensive experiments on four benchmark crowd datasets (the ShanghaiTech, the UCF-QNRF, the JHU-CRWORD ++ and the NWPU-Crowd), indicate that CFFNet can achieve state-of-the-art counting performance and high robustness.},
  archive      = {J_ICV},
  author       = {Yinfeng Xia and Yuqiang He and Sifan Peng and Qianqian Yang and Baoqun Yin},
  doi          = {10.1016/j.imavis.2021.104242},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104242},
  shortjournal = {Image Vis. Comput.},
  title        = {CFFNet: Coordinated feature fusion network for crowd counting},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning auto-scale representations for person
re-identification. <em>ICV</em>, <em>112</em>, 104241. (<a
href="https://doi.org/10.1016/j.imavis.2021.104241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) is a hot topic in computer vision . The data in the ReID is often collected from the cameras with different views and is affected by other environmental factors, which poses a significant challenge to ReID. The omni-scales proposed by OSNet can extract discriminative feature representations, which shows that omni-scales are adequate for the task of the ReID. However, the OSNet is mainly based on a manually designed network architecture . In the OSnet, each block uses the same architecture and has only four scale feature representations. Inspired by neural network architecture search (NAS), we propose a method of auto-scale representations for ReID. Specifically, we first design the auto-scale block, mainly composed of the Lite 3 × 3 operations and the RCB 1 × 1 operations. The connection status among the Lite 3 × 3 operations is just our search space . Then we give our entire macro network architecture, the auto-scale network, which is mainly composed of 6 auto-scale blocks. Unlike other NAS-related work, each block in our search space does not need to share the same architecture but can maintain a different architecture. In the search process, we propose the entropy regularization , the validity regularization and the consistent regularization to alleviate the discretized gap, no valid path, and meaningless edges, respectively. Finally, we verify the effectiveness of the model we searched on four commonly used datasets. Our model can maintain the same 2.2 M parameters as OSNet but can achieve the performance of SOTA. The mAP on the Market1501 dataset can reach 88.7%.},
  archive      = {J_ICV},
  author       = {Hongyang Gu and Guangyuan Fu and Xu Wang and Jun Zhu},
  doi          = {10.1016/j.imavis.2021.104241},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104241},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning auto-scale representations for person re-identification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Editorial: IMAVIS special issue on deep cross-media neural
model for generating image descriptions. <em>ICV</em>, <em>112</em>,
104239. (<a href="https://doi.org/10.1016/j.imavis.2021.104239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Zhao Zhang and Sheng Li and Meng Wang and Shuicheng Yan},
  doi          = {10.1016/j.imavis.2021.104239},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104239},
  shortjournal = {Image Vis. Comput.},
  title        = {Editorial: IMAVIS special issue on deep cross-media neural model for generating image descriptions},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent video anomaly detection and classification using
faster RCNN with deep reinforcement learning model. <em>ICV</em>,
<em>112</em>, 104229. (<a
href="https://doi.org/10.1016/j.imavis.2021.104229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, intelligent video surveillance applications have become essential in public security by the use of computer vision technologies to investigate and understand long video streams. Anomaly detection and classification are considered a major element of intelligent video surveillance. The aim of anomaly detection is to automatically determine the existence of abnormalities in a short time period. Deep reinforcement learning (DRL) techniques can be employed for anomaly detection, which integrates the concepts of reinforcement learning and deep learning enabling the artificial agents in learning the knowledge and experience from actual data directly. With this motivation, this paper presents an Intelligent Video Anomaly Detection and Classification using Faster RCNN with Deep Reinforcement Learning Model, called IVADC-FDRL model. The presented IVADC-FDRL model operates on two major stages namely anomaly detection and classification. Firstly, Faster RCNN model is applied as an object detector with Residual Network as a baseline model , which detects the anomalies as objects. Besides, deep Q-learning (DQL) based DRL model is employed for the classification of detected anomalies . In order to validate the effective anomaly detection and classification performance of the IVADC-FDRL model, an extensive set of experimentations were carried out on the benchmark UCSD anomaly dataset. The experimental results showcased the better performance of the IVADC-FDRL model over the other compared methods with the maximum accuracy of 98.50% and 94.80% on the applied Test004 and Test007 dataset respectively.},
  archive      = {J_ICV},
  author       = {Romany F. Mansour and José Escorcia-Gutierrez and Margarita Gamarra and Jair A. Villanueva and Nallig Leal},
  doi          = {10.1016/j.imavis.2021.104229},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104229},
  shortjournal = {Image Vis. Comput.},
  title        = {Intelligent video anomaly detection and classification using faster RCNN with deep reinforcement learning model},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial to special issue on novel insights on ocular
biometrics. <em>ICV</em>, <em>112</em>, 104227. (<a
href="https://doi.org/10.1016/j.imavis.2021.104227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Maria De Marsico and Hugo Proença and Sambit Bakshi and Abhijit Das},
  doi          = {10.1016/j.imavis.2021.104227},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104227},
  shortjournal = {Image Vis. Comput.},
  title        = {Editorial to special issue on novel insights on ocular biometrics},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial on “frontiers in computer vision for human
computer interaction.” <em>ICV</em>, <em>112</em>, 104226. (<a
href="https://doi.org/10.1016/j.imavis.2021.104226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Oscar Sanjuán Martínez and Giuseppe Fenza and Ruben Gonzalez Crespo},
  doi          = {10.1016/j.imavis.2021.104226},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104226},
  shortjournal = {Image Vis. Comput.},
  title        = {Editorial on “Frontiers in computer vision for human computer interaction”},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatiotemporal module for video saliency prediction based on
self-attention. <em>ICV</em>, <em>112</em>, 104216. (<a
href="https://doi.org/10.1016/j.imavis.2021.104216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering that the existing video saliency prediction methods still have limitations in spatiotemporal correlation learning between features and saliency regions, this paper proposes a spatiotemporal module for video saliency prediction based on self-attention. The proposed model emphasizes three essential problems as follows. First, we raise a multi-scale feature-fusion network (MFN) for effective feature integration. The framework can extract and fuse features from four scales at low memory cost. Second, we view the task as a global evaluation of the correlation on pixel level to predict human visual attention in task-driven scenes more accurately. An adapted transformer encoder is designed for spatiotemporal correlation learning. Finally, we introduce DConvLSTM to learn the context in videos. Experimental results show that the proposed model achieves state-of-the-art performance on both driving scenes and natural scenes with multi-motion information. And our model also achieves very comparable performance especially in natural scenes with multi-category objects. It proves our method is practicable in both data-driven and task-driven conditions.},
  archive      = {J_ICV},
  author       = {Yuhao Wang and Zhuoran Liu and Yibo Xia and Chunbo Zhu and Danpei Zhao},
  doi          = {10.1016/j.imavis.2021.104216},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104216},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatiotemporal module for video saliency prediction based on self-attention},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DA-SACOT: Domain adaptive-segmentation guided attention for
correlation based object tracking. <em>ICV</em>, <em>112</em>, 104215.
(<a href="https://doi.org/10.1016/j.imavis.2021.104215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking relies on a recursive search technique around the previous target location, concurrently learning the target appearance in each frame. A failure in any frame causes a drift from its optimal target path. Thus, obtaining highly confident search regions is essential in each frame. Motivated by the strong localization property of segmented object masks, the proposed method introduces instance segmentation as an attention mechanism in object tracking framework. The core contribution of this paper is threefold: (i) a region proposal module (RPM) based on instance segmentation to focus on search proposals having a high probability of being the target, (ii) a target localization module (TLM) to localize the final target using a correlation filter and (iii) a domain adaptation technique in both RPM and TLM modules to incorporate target specific knowledge and strong discrimination ability. Extensive experimental evaluation on three benchmark datasets demonstrate a significant average gain of 2.47% in precision, 2.55% in AUC score and 2.15% in overlap score in comparison with recent competing trackers.},
  archive      = {J_ICV},
  author       = {Priya Mariam Raju and Deepak Mishra and Prerana Mukherjee},
  doi          = {10.1016/j.imavis.2021.104215},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104215},
  shortjournal = {Image Vis. Comput.},
  title        = {DA-SACOT: Domain adaptive-segmentation guided attention for correlation based object tracking},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LSTM with bio inspired algorithm for action recognition in
sports videos. <em>ICV</em>, <em>112</em>, 104214. (<a
href="https://doi.org/10.1016/j.imavis.2021.104214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Sport-related movement recognition plays an essential part in the wellbeing of people&#39;s lives. The mention of human movements and gestures is often studied in sports to help analyze, guide, and evaluate activity. The automatic detection of sports-related signals helps find the injuries or indirect physical issues in the human body. Action recognition patterns with complicated motion status and periodicity in sports games can help to more accurately estimate the duration of successful action states. Actions are recognized by identifying the activity in a clip. Quality evaluation of action assigns a quantitative score based on the performance of the action. Based on the score, the action states are analyzed. The main issue of sports game identification correctly tracks the behavior of sportspeople. In this paper, Long Short Term Memory networks (LSTM) with a Bio-inspired Algorithm (BIA) framework have been proposed to recognize the action of a sportsperson and motivate a person to improve sports skills. Action recognition and classification can also be used to produce matching or practice output statistics automatically. The proposed LSTM-BIA utilizes predefined actions by modeling the monitoring effects with discriminative temporal signals . It uses the Spatial pyramid pooling SPP-net to obtain the robust characteristic of each frame&#39;s tracked area. The new SPP-net network structure will produce an adjusted description irrespective of the scale and resolution of the object. It could be used for identification and entity recognition and enables variable-length image input into CNN . The experimental results show that the proposed method can evaluate the actual action of sportspersons with high accuracy when compared to other methods.},
  archive      = {J_ICV},
  author       = {Jun Chen and R. Dinesh Jackson Samuel and Parthasarathy Poovendran},
  doi          = {10.1016/j.imavis.2021.104214},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104214},
  shortjournal = {Image Vis. Comput.},
  title        = {LSTM with bio inspired algorithm for action recognition in sports videos},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discrepant collaborative training by sinkhorn divergences.
<em>ICV</em>, <em>112</em>, 104213. (<a
href="https://doi.org/10.1016/j.imavis.2021.104213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Co-Training algorithms are typically comprised of two distinct and diverse feature extractors that simultaneously attempt to learn task-specific features from the same inputs. Achieving such an objective is, however, not trivial, despite its innocent look. This is because homogeneous networks tend to mimic each other under the collaborative training setup. Keeping this difficulty in mind, we make use of the newly proposed S ∈ divergence to encourage diversity between homogeneous networks. The S ∈ divergence encapsulates popular measures such as maximum mean discrepancy and the Wasserstein distance under the same umbrella and provides us with a principled, yet simple and straightforward mechanism. Our empirical results in two domains, classification in the presence of noisy labels and semi-supervised image classification , clearly demonstrate the benefits of the proposed framework in learning distinct and diverse features. We show that in these respective settings, we achieve impressive results by a notable margin.},
  archive      = {J_ICV},
  author       = {Yan Han and Soumava Kumar Roy and Lars Petersson and Mehrtash Harandi},
  doi          = {10.1016/j.imavis.2021.104213},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104213},
  shortjournal = {Image Vis. Comput.},
  title        = {Discrepant collaborative training by sinkhorn divergences},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust image representation method against illumination
and occlusion variations. <em>ICV</em>, <em>112</em>, 104212. (<a
href="https://doi.org/10.1016/j.imavis.2021.104212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix data has arised in many field, especially in the field of image processing and computer vision . In traditional approaches, the original images need to be vectorized to one-dimension vectors, which may destroy the inherent structure of images. A novel geometrical sparse representation (GSR) model with single image is introduced in this paper that solves a model to measure the similarity between the input image and the single dictionary image. Unlike the traditional sparse representation model, the proposed model does not need to vectorize the image, so as to preserve the inherent geometrical structure of the image. We further introduce a binary coding method to preserve the local patterns of the image and enhance the sparsity of the GSR coefficients. Our method is used for face images with variations of structural noise (occlusion, illumination, etc.), extensive experiments show that our method can be competitive with or even superior to the baseline methods .},
  archive      = {J_ICV},
  author       = {Jin Tan and Taiping Zhang and Linchang Zhao and Xiaoliu Luo and Yuan Yan Tang},
  doi          = {10.1016/j.imavis.2021.104212},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104212},
  shortjournal = {Image Vis. Comput.},
  title        = {A robust image representation method against illumination and occlusion variations},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved multi-source domain adaptation by preservation of
factors. <em>ICV</em>, <em>112</em>, 104209. (<a
href="https://doi.org/10.1016/j.imavis.2021.104209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Adaptation (DA) is a highly relevant research topic when it comes to image classification with deep neural networks . Combining multiple source domains in a sophisticated way to optimize a classification model can improve the generalization to a target domain. Here, the difference in data distributions of source and target image datasets plays a major role. In this paper, we describe based on a theory of visual factors how real-world scenes appear in images in general and how recent DA datasets are composed of such. We show that different domains can be described by a set of so called domain factors, whose values are consistent within a domain, but can change across domains. Many DA approaches try to remove all domain factors from the feature representation to be domain invariant . In this paper we show that this can lead to negative transfer since task-informative factors can get lost as well. To address this, we propose Factor-Preserving DA (FP-DA), a method to train a deep adversarial unsupervised DA model, which is able to preserve specific task relevant factors in a multi-domain scenario. We demonstrate on CORe50 how such factors can be identified by standard one-to-one transfer experiments between single domains combined with PCA . By applying FP-DA, we show that the highest average and minimum performance can be achieved. We also report improved performance for an adapted version of the OpenLORIS object dataset .},
  archive      = {J_ICV},
  author       = {Sebastian Schrom and Stephan Hasler and Jürgen Adamy},
  doi          = {10.1016/j.imavis.2021.104209},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104209},
  shortjournal = {Image Vis. Comput.},
  title        = {Improved multi-source domain adaptation by preservation of factors},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable visual reasoning: A survey. <em>ICV</em>,
<em>112</em>, 104194. (<a
href="https://doi.org/10.1016/j.imavis.2021.104194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual reasoning refers to the process of solving questions about visual information. At present, most visual reasoning models are mainly based on deep learning and end-to-end architecture. Although these models have achieved good performance , they are usually black boxes for users, and it is difficult to understand the basic rationales of the reasoning process. In recent years, the academic community has realized the importance of interpretability in visual reasoning and has developed a series of Interpretable Visual Reasoning (IVR) models. In this paper, we review these models. First, we have established a taxonomy based on four explanation forms of vision, text, graph and symbol used in current visual reasoning. Secondly, we explore the typical IVR models of each category and analyze their pros and cons. Thirdly, we elaborate on the current mainstream datasets about visual reasoning and VQA , and analyze how these datasets promote IVR research from different perspectives. Finally, we summarize the challenges for IVR and point out potential research directions.},
  archive      = {J_ICV},
  author       = {Feijuan He and Yaxian Wang and Xianglin Miao and Xia Sun},
  doi          = {10.1016/j.imavis.2021.104194},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104194},
  shortjournal = {Image Vis. Comput.},
  title        = {Interpretable visual reasoning: A survey},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RoI tanh-polar transformer network for face parsing in the
wild. <em>ICV</em>, <em>112</em>, 104190. (<a
href="https://doi.org/10.1016/j.imavis.2021.104190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face parsing aims to predict pixel-wise labels for facial components of a target face in an image. Existing approaches usually crop the target face from the input image with respect to a bounding box calculated during pre-processing, and thus can only parse inner facial Regions of Interest (RoIs). Peripheral regions like hair are ignored and nearby faces that are partially included in the bounding box can cause distractions. Moreover, these methods are only trained and evaluated on near-frontal portrait images and thus their performance for in-the-wild cases has been unexplored. To address these issues, this paper makes three contributions. First, we introduce iBugMask dataset for face parsing in the wild, which consists of 21,866 training images and 1000 testing images. The training images are obtained by augmenting an existing dataset with large face poses. The testing images are manually annotated with 11 facial regions and there are large variations in sizes, poses, expressions and background. Second, we propose RoI Tanh-polar transform that warps the whole image to a Tanh-polar representation with a fixed ratio between the face area and the context, guided by the target bounding box. The new representation contains all information in the original image, and allows for rotation equivariance in the convolutional neural networks (CNNs). Third, we propose a hybrid residual representation learning block, coined HybridBlock, that contains convolutional layers in both the Tanh-polar space and the Tanh-Cartesian space, allowing for receptive fields of different shapes in CNNs. Through extensive experiments, we show that the proposed method improves the state-of-the-art for face parsing in the wild and does not require facial landmarks for alignment.},
  archive      = {J_ICV},
  author       = {Yiming Lin and Jie Shen and Yujiang Wang and Maja Pantic},
  doi          = {10.1016/j.imavis.2021.104190},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104190},
  shortjournal = {Image Vis. Comput.},
  title        = {RoI tanh-polar transformer network for face parsing in the wild},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online-adaptive classification and regression network with
sample-efficient meta learning for long-term tracking. <em>ICV</em>,
<em>112</em>, 104181. (<a
href="https://doi.org/10.1016/j.imavis.2021.104181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification and regression-based trackers (CAR) are widely adopted to tackle the short-term visual tracking task. However, the existing CAR tackers either employ offline-trained regression models based on predefined anchor-boxes, or online update their models in a rough and inflexible way, which leads to the lack of long-term adaptability for target deformations and appearance variations. To overcome this limitation, we propose a novel long-term tracking framework LT-CAR utilizing sample-efficient meta learning to online optimize both the classification and regression model. Specifically, we first introduce the ridge regression to a fully convolutional network as our regression branch, and then implement a vertically stacked GRU module termed as Meta-Sample-Filter to keep historical information about the target as well as help our model learn what to learn. Moreover, we extend our framework for long-term tracking by introducing a carefully designed spatial–temporal verification network to identify tracking failures, and a query-guided detector to conduct global re-detection. Experimental results on LaSOT, VOT-LT2018, VOT-LT2019, and TLP benchmarks show that our LT-CAR achieves comparable performance to the state-of-the-art long-term algorithms.},
  archive      = {J_ICV},
  author       = {Lang Yu and Huanlong Zhang and Junyang Yu and Baojun Qiao},
  doi          = {10.1016/j.imavis.2021.104181},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104181},
  shortjournal = {Image Vis. Comput.},
  title        = {Online-adaptive classification and regression network with sample-efficient meta learning for long-term tracking},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Double anchor embedding for accurate multi-person 2D pose
estimation. <em>ICV</em>, <em>111</em>, 104198. (<a
href="https://doi.org/10.1016/j.imavis.2021.104198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person pose estimation is an important field in computer vision . Due to the lower time complexity, the bottom-up approaches have recently received more attention in multi-person 2D pose estimation, however, they are more sensitive to challenges in real-world scenarios. In this paper, we propose a multi-person pose estimation algorithm based on the Double Anchor Embedding (DAE), which shows that bottom-up algorithms are still competitive in precision. Firstly, for reducing the modeling difficulty of the detection task we divide the human joints into upper and lower half groups which are internally continuous and highly correlated. Accordingly, a novel joint affinity cue, called Double Anchor Embedding is designed, which can help the network effectively extract the information of both local contexts and global contexts, so that can better cope with occluded scenes and complex postures. Secondly, the parallel greedy joint inference algorithm is proposed to alleviate the mismatching problem of distant joints in the post-processing stage, which can also accelerate the matching process to some extent. Extensive experiments on two challenging datasets demonstrate the effectiveness and potential of our proposed framework, which is comparable to the current state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Zhiqian Zhang and Yanmin Luo and Jin Gou},
  doi          = {10.1016/j.imavis.2021.104198},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104198},
  shortjournal = {Image Vis. Comput.},
  title        = {Double anchor embedding for accurate multi-person 2D pose estimation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feedback-driven loss function for small object detection.
<em>ICV</em>, <em>111</em>, 104197. (<a
href="https://doi.org/10.1016/j.imavis.2021.104197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent years, Convolutional Neural Network-based object detection has experienced impressive progress. Despite these improvements, the performance on the small object detection leaves much to be desired, small objects are often missed or wrongly detected. The reasonable explanation is that the supervisory signals on the small objects are insufficient, and through the analysis of loss distribution over different scales in the iterations, there is a significant gap between the loss provided by the small objects and large objects. In order to balance the loss distribution and alleviate the insufficient supervisory on the small objects, a Feedback-driven loss function is presented in this paper. The Feedback-driven loss function uses the loss distribution information as the feedback signal, compared with the original loss function, the Feedback-driven loss function can supervise small objects more effectively, and train the detectors in a more balanced way. Experiments have been conducted on various detectors, backbones, training periods and datasets. Compared to the current state-of-the-art method, the novel Feedback-driven loss function can achieve 2.3% relative improvement on the Mean Average Precision , and especially 3.5% improvement on the detection of small objects, with nearly no additional computation both in training and testing stages.},
  archive      = {J_ICV},
  author       = {Gen Liu and Jin Han and Wenzhong Rong},
  doi          = {10.1016/j.imavis.2021.104197},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104197},
  shortjournal = {Image Vis. Comput.},
  title        = {Feedback-driven loss function for small object detection},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ASPset: An outdoor sports pose video dataset with 3D
keypoint annotations. <em>ICV</em>, <em>111</em>, 104196. (<a
href="https://doi.org/10.1016/j.imavis.2021.104196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning approaches to computer vision problems have led to renewed interest in the task of predicting 3D human joint locations from raw image data , with application areas including sports analysis, human-computer interaction, and physical rehabilitation. Although supervised learning of deep neural networks has proven to be effective for pose estimation, it requires a wealth of varied data to generalise well to previously unseen examples. Consequently, progress in 3D human pose estimation has been slowed by the fact that 3D keypoint annotations are notoriously difficult to obtain, traditionally requiring a large array of cameras and/or the use of wearable markers/sensors. In this paper we describe a methodology for obtaining 3D human pose annotations using only three video cameras and without any wearables. We apply this methodology to construct ASPset-510 (Australian Sports Pose Dataset), a large collection of natural sports-related video with 3D pose annotations. Using ASPset-510 as an additional source of training examples, we found that we could improve pose model generalisation on the established MPI-INF-3DHP benchmark. We make ASPset-510 publicly available, and provide strong baseline results for future work to compare against.},
  archive      = {J_ICV},
  author       = {Aiden Nibali and Joshua Millward and Zhen He and Stuart Morgan},
  doi          = {10.1016/j.imavis.2021.104196},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104196},
  shortjournal = {Image Vis. Comput.},
  title        = {ASPset: An outdoor sports pose video dataset with 3D keypoint annotations},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient pyramid context encoding and feature embedding for
semantic segmentation. <em>ICV</em>, <em>111</em>, 104195. (<a
href="https://doi.org/10.1016/j.imavis.2021.104195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For reality applications of semantic segmentation , inference speed and memory usage are two important factors. To address these challenges, we propose a lightweight feature pyramid encoding network (FPENet) for semantic segmentation with a good trade-off between accuracy and speed. We use a series of feature pyramid encoding (FPE) blocks to encode context at multiple scales in the encoder. Each FPE block consists of different depthwise dilated convolutions that perform as a spatial pyramid to extract features and reduce computational costs. During training, a one-shot neural architecture search algorithm is adopted to find the optimal structure for each FPE block from a large search space with a small search cost. After the search for the encoder, a mutual embedding upsample module is introduced in the decoder, consisting of two attention blocks. The encoder-decoder attention mechanism is used to help aggregate efficiently high-level semantic features and low-level spatial details . The proposed network outperforms the existing real-time methods with fewer parameters and improved inference speed on the Cityscapes and CamVid benchmark datasets. Specifically, it achieved 72.3% mean IoU on the Cityscapes test set with only 0.4 M parameters and 192.6 FPS speed on an Nvidia Titan V100 GPU , and 73.4% mean IoU with 116.2 FPS when running on higher resolution images.},
  archive      = {J_ICV},
  author       = {Mengyu Liu and Hujun Yin},
  doi          = {10.1016/j.imavis.2021.104195},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104195},
  shortjournal = {Image Vis. Comput.},
  title        = {Efficient pyramid context encoding and feature embedding for semantic segmentation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point cloud completion using multiscale feature fusion and
cross-regional attention. <em>ICV</em>, <em>111</em>, 104193. (<a
href="https://doi.org/10.1016/j.imavis.2021.104193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raw point clouds obtained from real-world scanning are always incomplete and ununiformly distributed, which would result in structural losses of object shapes and bring about difficulties in further high-level 3D vision tasks. Therefore, a learning-based method called CRA-Net is proposed in this paper to repair partial point clouds and predict complete object shapes. Compared with most existing networks that only leverage global features, CRA-Net successfully utilizes local features to restore clearer details of object shapes with low instability. First, we propose an adaptive neighborhood query method that is able to adjust query centers and radiuses to cover different object shapes and acquire balanced local regions. Second, we build a parallel encoder to extract multiscale features from the input. Third, we design a cross-regional attention module based on graph attention network. It quantifies underlying relationships among all the local features under certain conditions interpreted by global features. Based on such relationships, each conditional local feature vector is able to search across the regions and selectively absorb other local features. Fourth, we design a coarse decoder to collect these cross region features and generate the skeleton of complete point cloud. Finally, we refine the coarse point cloud by comparing it with the input, and up sample it using folding-based layers. Our network is first trained and tested on manually made partial-complete point clouds pairs generated by the scanning process of a virtual LiDAR on eight categories of objects. Then it is tested on real-world point clouds of indoor and outdoor scenes. Compared with existing representative methods, our CRA-Net always restores the most accurate point clouds with the clearest details.},
  archive      = {J_ICV},
  author       = {Hang Wu and Yubin Miao and Ruochong Fu},
  doi          = {10.1016/j.imavis.2021.104193},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104193},
  shortjournal = {Image Vis. Comput.},
  title        = {Point cloud completion using multiscale feature fusion and cross-regional attention},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). ARank: Toward specific model pruning via advantage rank for
multiple salient objects detection. <em>ICV</em>, <em>111</em>, 104192.
(<a href="https://doi.org/10.1016/j.imavis.2021.104192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning unnecessary regions from weight kernels and filters used in convolutional nerual networks has proven to be effective for model compression . However, prior studies have often over-optimized for generic application areas, which are not ideal for specific problems in salient object detection . In this study, a novel pruning strategy denoted advantage rank (ARank) is developed for multiple salient object detection (Mul-SOD), in which salient priority criterion (SPC) is used to evaluate parameter contributions. The proposed SPC is a measurable standard for object priority that can be quantified and calculated using ARank. Simulation experiments demonstrate that Mul-SOD can be optimized by removing interference from low-SPC parameters. ARank effectively decreased interference and distinguished high-priority salient objects from multiple background objects.},
  archive      = {J_ICV},
  author       = {Fengwei Jia and Xuan Wang and Jian Guan and Huale Li and Chen Qiu and Shuhan Qi},
  doi          = {10.1016/j.imavis.2021.104192},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104192},
  shortjournal = {Image Vis. Comput.},
  title        = {ARank: Toward specific model pruning via advantage rank for multiple salient objects detection},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised cross-domain person re-identification with
self-attention and joint-flexible optimization. <em>ICV</em>,
<em>111</em>, 104191. (<a
href="https://doi.org/10.1016/j.imavis.2021.104191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) for person re-identication (ReID) remains a challenging task, as the trained ReID system often fails to adapting to a new dataset. Due to the lack of supervision of real labels, the performance of the UDA models suffers from inefficient feature learning and inevitable pseudo label noise. In this work, we tackle the problems by designing an effective dual-path mutual-learning framework which can capture effective information for better feature learning and mitigate the impact of label noise. Firstly, to reduce the impact of occlusion and viewpoints, we introduce the self-attention mechanism in a two-stage strategy making the models focus on the key areas of identifying people. Secondly, considering that UDA is an open-set task, we leverage density-based spatial clustering of applications with noise (DBSCAN) to avoid manually setting the number of classes of the target domain. Thirdly, for realizing joint and flexible optimization under the supervision of soft pseudo labels and hard pseudo labels, a joint and flexible loss (JFL) is proposed to train the network. Experiments on three large-scale datasets show that our model outperforms the state-of-the-art UDA methods in both mAP and top-1 evaluation protocols by large margins. Especially on task of Duke-to-Market, our method outperforms the state-of-the-art by 6.9% mAP.},
  archive      = {J_ICV},
  author       = {Haopeng Hou and Yong Zhou and Jiaqi Zhao and Rui Yao and Ying Chen and Yi Zheng and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.imavis.2021.104191},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104191},
  shortjournal = {Image Vis. Comput.},
  title        = {Unsupervised cross-domain person re-identification with self-attention and joint-flexible optimization},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Composite recurrent network with internal denoising for
facial alignment in still and video images in the wild. <em>ICV</em>,
<em>111</em>, 104189. (<a
href="https://doi.org/10.1016/j.imavis.2021.104189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial alignment is an essential task for many higher level facial analysis applications, such as animation, human activity recognition and human - computer interaction. Although the recent availability of big datasets and powerful deep-learning approaches have enabled major improvements on the state of the art accuracy, the performance of current approaches can severely deteriorate when dealing with images in highly unconstrained conditions, which limits the real-life applicability of such models. In this paper, we propose a composite recurrent tracker with internal denoising that jointly address both single image facial alignment and deformable facial tracking in the wild. Specifically, we incorporate multilayer LSTMs to model temporal dependencies with variable length and introduce an internal denoiser which selectively enhances the input images to improve the robustness of our overall model. We achieve this by combining 4 different sub-networks that specialize in each of the key tasks that are required, namely face detection, bounding-box tracking, facial region validation and facial alignment with internal denoising. These blocks are endowed with novel algorithms resulting in a facial tracker that is both accurate, robust to in-the-wild settings and resilient against drifting. We demonstrate this by testing our model on 300-W and Menpo datasets for single image facial alignment, and 300-VW dataset for deformable facial tracking. Comparison against 20 other state of the art methods demonstrates the excellent performance of the proposed approach.},
  archive      = {J_ICV},
  author       = {Decky Aspandi and Oriol Martinez and Federico Sukno and Xavier Binefa},
  doi          = {10.1016/j.imavis.2021.104189},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104189},
  shortjournal = {Image Vis. Comput.},
  title        = {Composite recurrent network with internal denoising for facial alignment in still and video images in the wild},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ATCC: Accurate tracking by criss-cross location attention.
<em>ICV</em>, <em>111</em>, 104188. (<a
href="https://doi.org/10.1016/j.imavis.2021.104188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, discriminative correlation filters (DCF) and Siamese networks based trackers have significantly advanced the performance in tracking. However, the problem of accurate target state estimation is not fully solved yet. Therefore, in this paper, we propose a Criss-Cross Location Attention (CCLA) module, which pays more concerns to global and local contextual information and is used for the adaptation of IoU-Net based trackers. Besides, our CCLA module has capability of high computational efficiency with a slight increase of network parameters. Then, we present our tracker called ATCC, a Siamese architecture with CCLA. Finally, we evaluate our tracker on OTB100, VOT-2018, LaSOT, and TrackingNet benchmark datasets. Experimental results show that our tracker performs favorably against other state-of-the-art trackers, while operating at 30 FPS on single GPU . We will release the code and models at https://github.com/yongwuSHU/atcc .},
  archive      = {J_ICV},
  author       = {Yong Wu and Zhi Liu and Xiaofei Zhou and Linwei Ye and Yang Wang},
  doi          = {10.1016/j.imavis.2021.104188},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104188},
  shortjournal = {Image Vis. Comput.},
  title        = {ATCC: Accurate tracking by criss-cross location attention},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using synthetic data for person tracking under adverse
weather conditions. <em>ICV</em>, <em>111</em>, 104187. (<a
href="https://doi.org/10.1016/j.imavis.2021.104187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust visual tracking plays a vital role in many areas such as autonomous cars, surveillance and robotics. Recent trackers were shown to achieve adequate results under normal tracking scenarios with clear weather conditions, standard camera setups and lighting conditions . Yet, the performance of these trackers, whether they are correlation filter-based or learning-based, degrade under adverse weather conditions. The lack of videos with such weather conditions, in the available visual object tracking datasets, is the prime issue behind the low performance of the learning-based tracking algorithms. In this work, we provide a new person tracking dataset of real-world sequences (PTAW172Real) captured under foggy, rainy and snowy weather conditions to assess the performance of the current trackers. We also introduce a novel person tracking dataset of synthetic sequences (PTAW217Synth) procedurally generated by our NOVA framework spanning the same weather conditions in varying severity to mitigate the problem of data scarcity. Our experimental results demonstrate that the performances of the state-of-the-art deep trackers under adverse weather conditions can be boosted when the available real training sequences are complemented with our synthetically generated dataset during training.},
  archive      = {J_ICV},
  author       = {Abdulrahman Kerim and Ufuk Celikcan and Erkut Erdem and Aykut Erdem},
  doi          = {10.1016/j.imavis.2021.104187},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104187},
  shortjournal = {Image Vis. Comput.},
  title        = {Using synthetic data for person tracking under adverse weather conditions},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose-guided part matching network via shrinking and
reweighting for occluded person re-identification. <em>ICV</em>,
<em>111</em>, 104186. (<a
href="https://doi.org/10.1016/j.imavis.2021.104186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded person re-identification (ReID) is a challenging task, which aims at retrieving an occluded person across multiple non-overlapping cameras. To address this issue, we propose a novel framework named Shrinking and Reweighting Network (SRNet) that jointly learns global features by shrinking and reweights part features for matching in an end-to-end framework. Specifically, we use a strong backbone that combines some effective designs and training tricks to learn the robust and discriminative global features. Even so, there exist noise-related features due to the occlusion, so we utilize the Deep Residual Shrinkage Module (DRS Module) to eliminate unimportant features by automatically determining the soft thresholds. When aligning two groups of part features from two images, we view it as a graph matching problem and design an effectively Reweight Module for Part Matching (RMPM) to learn self-adaptive weights for part features before the part matching stage, the proposed RMPM can alleviate the influence of meaningless part features in the part matching stage. Eventually, extensive experimental results on occluded, partial, and holistic re-id datasets clearly demonstrate that the proposed method achieves competitive performance to the state-of-the-art methods. Specifically, our framework remarkably outperforms state-of-the-art by 8.9% mAP scores on Occluded-Duke dataset. Code is available at https://github.com/chenxiangzZ/SRNet.},
  archive      = {J_ICV},
  author       = {HongXia Wang and Xiang Chen and Chun Liu},
  doi          = {10.1016/j.imavis.2021.104186},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104186},
  shortjournal = {Image Vis. Comput.},
  title        = {Pose-guided part matching network via shrinking and reweighting for occluded person re-identification},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Informative discriminator for domain adaptation.
<em>ICV</em>, <em>111</em>, 104180. (<a
href="https://doi.org/10.1016/j.imavis.2021.104180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of domain adaptation for multi-class classification, where we are provided a labeled set of examples in a source dataset and target dataset with no supervision. We tackle the mode collapse problem in adapting the classifier across domains. In this setting, we propose an adversarial learning-based approach using an informative discriminator . Our observation relies on the analysis that shows if the discriminator has access to all the information available, including the class structure present in the source dataset, then it can guide the transformation of features of the target set of classes to a more structured adapted space. Further, by training the informative discriminator using the more robust source samples, we are able to obtain better domain invariant features. Using this formulation, we achieve state-of-the-art results for the standard evaluation on benchmark datasets. We also provide detailed analysis, which shows that using all the labeled information results in an improved domain adaptation.},
  archive      = {J_ICV},
  author       = {Vinod K. Kurmi and Venkatesh K. Subramanian and Vinay P. Namboodiri},
  doi          = {10.1016/j.imavis.2021.104180},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104180},
  shortjournal = {Image Vis. Comput.},
  title        = {Informative discriminator for domain adaptation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-path CNN with max gated block for text-based person
re-identification. <em>ICV</em>, <em>111</em>, 104168. (<a
href="https://doi.org/10.1016/j.imavis.2021.104168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person re-identification (Re-id) is an important task in video surveillance, which consists of retrieving the corresponding person&#39;s image given a textual description from a large gallery of images. It is difficult to directly match visual contents with the textual descriptions due to the modality heterogeneity. On the one hand, the textual embedding are not discriminative enough, which originates from the high abstraction of the textual descriptions. One the other hand, Global average pooling (GAP) is commonly utilized to extract more general or smoothed features implicitly but ignores salient local features , which are more important for the cross-modal matching problem. With that in mind, a novel Dual-path CNN with Max Gated block (DCMG) is proposed to extract discriminative word embedding and make visual-textual association concern more on remarkable features of both modalities. The proposed framework is based on two deep residual CNNs jointly optimized with cross-modal projection matching (CMPM) loss and cross-modal projection classification (CMPC) loss to embed the two modalities into a joint feature space. First, the pre-trained language model, BERT , is combined with the convolutional neural network (CNN) to learn better word embedding in the text-to-image matching domain. Second, the global Max pooling (GMP) layer is applied to make the visual textual features focus more on the salient part. To further alleviate the noise of the maxed-pooled features, the gated block (GB) is proposed to produce an attention map that focuses on meaningful features of both modalities. Finally, extensive experiments are conducted on the benchmark dataset, CUHK-PEDES, in which our approach achieves the rank-1 score of 55.81% and outperforms the state-of-the-art method by 1.3%. We also evaluate our method on two generic retrieval datasets (Flickr30K, Oxford-102 Flowers) and obtain the competitive performance. Code is available at https://github.com/voriarty/Dual-path-CNN-with-Max-Gated-block-for-Text-Based-Person-Re-identification},
  archive      = {J_ICV},
  author       = {Tinghuai Ma and Mingming Yang and Huan Rong and Yurong Qian and Yuan Tian and Najla Al-Nabhan},
  doi          = {10.1016/j.imavis.2021.104168},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104168},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual-path CNN with max gated block for text-based person re-identification},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial for the special issue of IMAVIS on automatic face
analytics for human behavior understanding. <em>ICV</em>, <em>110</em>,
104185. (<a href="https://doi.org/10.1016/j.imavis.2021.104185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Xiaohua Huang and Abhinav Dhall and Guoying Zhao and Wenming Zheng and Matti Peitikänen},
  doi          = {10.1016/j.imavis.2021.104185},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104185},
  shortjournal = {Image Vis. Comput.},
  title        = {Editorial for the special issue of IMAVIS on automatic face analytics for human behavior understanding},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic video segmentation with dynamic keyframe selection
and distortion-aware feature rectification. <em>ICV</em>, <em>110</em>,
104184. (<a href="https://doi.org/10.1016/j.imavis.2021.104184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The per-frame segmentation methods have a high computational cost, thereby, these methods are insufficient to cope with the fast inference need of semantic video segmentation. To efficaciously reuse the extracted features by feature propagation , in this paper, we present distortion-aware feature rectification and online selection of keyframes for fast and accurate video segmentation. The proposed dynamic keyframe scheduling scheme is based on the extent of temporal variations using reinforcement learning . We employ policy gradient reinforcement strategy to learn policy function for maximizing the expected reward. The policy network has two actions (key and non-key) in the action space. State information is derived from the element-wise difference frame of the current frame and the warped current frame generated by the propagated previous frame. Afterward, an adaptive partial feature rectification with distortion-aware corrections is performed for the warped features of the non-key frames. Precise feature propagation is a critical task to uphold the temporal updates in the video sequence since it enormously affects the accuracy as well as the throughput of the whole video analysis framework. The distorted feature maps are revised with the light-weight feature extractor by the guidance of the distortion map while the correctly propagated features are not influenced. Deep feature flow approach is adopted for feature propagation. We evaluate our scheme on the Cityscapes and CamVid datasets with DeepLabv3 as segmentation network and LiteFlowNet for computing flow fields. Experimental results show that the proposed method outperforms the previous state-of-the-art methods significantly both in terms of accuracy and throughput.},
  archive      = {J_ICV},
  author       = {Mehwish Awan and Jitae Shin},
  doi          = {10.1016/j.imavis.2021.104184},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104184},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic video segmentation with dynamic keyframe selection and distortion-aware feature rectification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DCT-net: A deep co-interactive transformer network for video
temporal grounding. <em>ICV</em>, <em>110</em>, 104183. (<a
href="https://doi.org/10.1016/j.imavis.2021.104183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language-guided video temporal grounding is to temporally localize the best matched video segment in an untrimmed long video according to a given natural language query (sentence). The main challenge in this task lies in how to fuse visual and linguistic information effectively. Recent works have shown that the attention mechanism is beneficial to the multi-modal feature fusion process. In this paper, we present a concise yet valid Deep Co-Interactive Transformer Network (DCT-Net) which repurposes a Transformer-style architecture to sufficiently model cross modality interactions. It consists of Co-Interactive Transformer (CIT) layers cascaded in depth for multi-step interactions between a video-sentence pair. With the help of the proposed CIT layer, both visual and language features can share the mutually improved benefits from each other. Extensive experiments on two public datasets, i.e. ActivityNet-Caption and TACOS, demonstrate the effectiveness of our proposed model compared to state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Wen Wang and Jian Cheng and Siyu Liu},
  doi          = {10.1016/j.imavis.2021.104183},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104183},
  shortjournal = {Image Vis. Comput.},
  title        = {DCT-net: A deep co-interactive transformer network for video temporal grounding},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Camera pose estimation in multi-view environments: From
virtual scenarios to the real world. <em>ICV</em>, <em>110</em>, 104182.
(<a href="https://doi.org/10.1016/j.imavis.2021.104182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a domain adaptation strategy to efficiently train network architectures for estimating the relative camera pose in multi-view scenarios. The network architectures are fed by a pair of simultaneously acquired images, hence in order to improve the accuracy of the solutions, and due to the lack of large datasets with pairs of overlapped images, a domain adaptation strategy is proposed. The domain adaptation strategy consists on transferring the knowledge learned from synthetic images to real-world scenarios. For this, the networks are firstly trained using pairs of synthetic images , which are captured at the same time by a pair of cameras in a virtual environment; and then, the learned weights of the networks are transferred to the real-world case, where the networks are retrained with a few real images. Different virtual 3D scenarios are generated to evaluate the relationship between the accuracy on the result and the similarity between virtual and real scenarios—similarity on both geometry of the objects contained in the scene as well as relative pose between camera and objects in the scene. Experimental results and comparisons are provided showing that the accuracy of all the evaluated networks for estimating the camera pose improves when the proposed domain adaptation strategy is used, highlighting the importance on the similarity between virtual-real scenarios.},
  archive      = {J_ICV},
  author       = {Jorge L. Charco and Angel D. Sappa and Boris X. Vintimilla and Henry O. Velesaca},
  doi          = {10.1016/j.imavis.2021.104182},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104182},
  shortjournal = {Image Vis. Comput.},
  title        = {Camera pose estimation in multi-view environments: From virtual scenarios to the real world},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Short-term anchor linking and long-term self-guided
attention for video object detection. <em>ICV</em>, <em>110</em>,
104179. (<a href="https://doi.org/10.1016/j.imavis.2021.104179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new network architecture able to take advantage of spatio-temporal information available in videos to boost object detection precision. First, box features are associated and aggregated by linking proposals that come from the same anchor box in the nearby frames. Then, we design a new attention module that aggregates short-term enhanced box features to exploit long-term spatio-temporal information. This module takes advantage of geometrical features in the long-term for the first time in the video object detection domain. Finally, a spatio-temporal double head is fed with both spatial information from the reference frame and the aggregated information that takes into account the short- and long-term temporal context. We have tested our proposal in five video object detection datasets with very different characteristics, in order to prove its robustness in a wide number of scenarios. Non-parametric statistical tests show that our approach outperforms the state-of-the-art. Our code is available at https://github.com/daniel-cores/SLTnet .},
  archive      = {J_ICV},
  author       = {Daniel Cores and Víctor M. Brea and Manuel Mucientes},
  doi          = {10.1016/j.imavis.2021.104179},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104179},
  shortjournal = {Image Vis. Comput.},
  title        = {Short-term anchor linking and long-term self-guided attention for video object detection},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightweight boundary refinement module based on point
supervision for semantic segmentation. <em>ICV</em>, <em>110</em>,
104169. (<a href="https://doi.org/10.1016/j.imavis.2021.104169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective semantic segmentation approach should contain fine object boundaries and continuous regions. However, recent mask-based segmentation cannot extract boundary features well on the coarse prediction, which causes obvious problems of blurry edges. Although several segmentation methods embed boundary detection branch to calculate contours directly, this type of architecture will lead to the increased computational complexity and miss the edge detailed information such as inter-class distinction. In order to obtain fine boundaries, we present a lightweight boundary refinement module with point supervision named BRPS to improve the boundary quality for the segmentation result generated by various existing segmentation models . Firstly, a direction field is learned to complete the initial feature rectification , which is defined as pointing away from the nearest object boundary to each pixel, where the weighted Euclidean and Cosine distance function is used as the loss between predicted boundary pixels and Ground-truth labels. Then, point-based supervised learning is performed at uncertain and certain locations including random distribution and key feature points based on a new point convolutional operation to output final crisp object boundaries. Finally, we verify that our BRPS module can effectively reduce the prediction errors for segmentation results generated from various state-of-the-art models such as DeepLabv3 and HRNet on the Pascal VOC2012, NYUD v2 datasets, Cityscapes and BDD100K datasets.},
  archive      = {J_ICV},
  author       = {Zihao Dong and Jinping Li and Tiyu Fang and Xiuli Shao},
  doi          = {10.1016/j.imavis.2021.104169},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104169},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight boundary refinement module based on point supervision for semantic segmentation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep domain adaptation with ordinal regression for pain
assessment using weakly-labeled videos. <em>ICV</em>, <em>110</em>,
104167. (<a href="https://doi.org/10.1016/j.imavis.2021.104167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of pain intensity from facial expressions captured in videos has an immense potential for health care applications. Given the challenges related to subjective variations of facial expressions, and to operational capture conditions, the accuracy of state-of-the-art deep learning (DL) models for recognizing facial expressions may decline. Domain adaptation (DA) has been widely explored to alleviate the problem of domain shifts that typically occur between video data captured across various source (laboratory) and target (operational) domains. Moreover, given the laborious task of collecting and annotating videos, and the subjective bias due to ambiguity among adjacent intensity levels, weakly-supervised learning (WSL) is gaining attention in such applications. State-of-the-art WSL models are typically formulated as regression problems , and do not leverage the ordinal relationship among pain intensity levels, nor the temporal coherence of multiple consecutive frames. This paper introduces a new DL model for weakly-supervised DA with ordinal regression (WSDA-OR) that can be adapted using target domain videos with coarse labels provided on a periodic basis. The WSDA-OR model enforces ordinal relationships among the intensity levels assigned to target sequences, and associates multiple relevant frames to sequence-level labels (instead of a single frame). In particular, it learns discriminant and domain-invariant feature representations by integrating multiple instance learning with deep adversarial DA, where soft Gaussian labels are used to efficiently represent the weak ordinal sequence-level labels from the target domain. The proposed approach was validated using the RECOLA video dataset as fully-labeled source domain data , and UNBC-McMaster shoulder pain video dataset as weakly-labeled target domain data . We have also validated WSDA-OR on BIOVID and Fatigue (private) datasets for sequence level estimation. Experimental results indicate that our proposed approach can significantly improve performance over the state-of-the-art models, allowing to achieve a greater pain localization accuracy. Code is available on GitHub link: https://github.com/praveena2j/WSDAOR .},
  archive      = {J_ICV},
  author       = {Gnana Praveen Rajasekhar and Eric Granger and Patrick Cardinal},
  doi          = {10.1016/j.imavis.2021.104167},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104167},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep domain adaptation with ordinal regression for pain assessment using weakly-labeled videos},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Edge-aware salient object detection network via context
guidance. <em>ICV</em>, <em>110</em>, 104166. (<a
href="https://doi.org/10.1016/j.imavis.2021.104166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully convolutional network (FCN) based salient object detection methods have shown their advantages in highlighting salient regions because they can obtain global semantic information. And the high-level semantics are usually passed in a top-down pathway. However, the semantic information would be diluted progressively among different level features. To alleviate this issue, we propose a novel edge-aware salient object detection network. Our network utilizes high-level semantic information to assist the feature selection of shallower layers. Specifically, we extract refined features from different levels of the backbone. Then, we obtain global contextual information to locate the salient objects by extracting multi-scale features and emphasizing the important feature channels. In order to assist the shallower layers to pay attention to the learning of meaningful local information , we adopt a context guidance strategy to fuse the high-level and low-level information. Finally, we supervise the generation of low-level edge information to preserve the salient object boundaries. Extensive experiments demonstrate that the proposed mode performs favorably against most state-of-the-art methods under different evaluation metrics on six popular benchmarks.},
  archive      = {J_ICV},
  author       = {Xiaowei Chen and Qing Zhang and Liqian Zhang},
  doi          = {10.1016/j.imavis.2021.104166},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104166},
  shortjournal = {Image Vis. Comput.},
  title        = {Edge-aware salient object detection network via context guidance},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Visual question answering model based on graph neural
network and contextual attention. <em>ICV</em>, <em>110</em>, 104165.
(<a href="https://doi.org/10.1016/j.imavis.2021.104165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) has recently appeared as a hot research area in the field of computer vision and natural language processing . A VQA model uses both image and question features and fuses them to predict an answer for a given natural question related to an image. However, most VQA approaches using attention mechanism mainly concentrate on extraction of visual information from regions of interests for answer prediction and ignore the relation between the regions of interests together with the reasoning among these regions. Apart from this limitation, VQA approaches also ignore the regions which are previously attended for answer generation. These regions which are attended in past can guide the selection of the subsequent regions of attention. In this paper, a novel VQA model is presented and formulated that utilizes this relationship between the regions and employs visual context based attention that takes into account the previously attended visual content. Experimental results demonstrate that the proposed VQA model boosts the accuracy of answer prediction on publically available datasets VQA 1.0 and VQA 2.0.},
  archive      = {J_ICV},
  author       = {Himanshu Sharma and Anand Singh Jalal},
  doi          = {10.1016/j.imavis.2021.104165},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104165},
  shortjournal = {Image Vis. Comput.},
  title        = {Visual question answering model based on graph neural network and contextual attention},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PDA: Proxy-based domain adaptation for few-shot image
recognition. <em>ICV</em>, <em>110</em>, 104164. (<a
href="https://doi.org/10.1016/j.imavis.2021.104164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from limited supervision is a challenging problem that has recently attracted wide attention in the machine learning community. With scarce annotated samples available in target categories, so-called few-shot image recognition aims to transfer basic knowledge from a large-scale image set to recognize unseen classes. Many existing approaches tend to learn a general source data representation and apply it to address the few-shot task by building a target classifier on scare support features, which performs favorably only if source and target data distributions are similar. We argue that ignoring the distribution gap and directly leveraging frozen representations lead to a sub-optimal solution. Taking domain shift into consideration, we explore an efficient task adaptation strategy that can jointly achieve task and domain transfer. Accordingly, we propose a simple yet effective method, called proxy-based domain adaptation ( PDA ), to optimize the pre-trained representation and a target classifier simultaneously. PDA can be characterized as: (1) a source-data-independent approach that only leverages few support data from the target domain (2) a non-parametric adaptation method that performs model adaptation by minimizing a designed loss without involving any parametric modules additionally. We extensively conduct experiments on multiple few-shot image recognition benchmarks, highlighting the superiority of PDA over many SOTA methods . Besides, careful ablation studies verify each component&#39;s effectiveness in our method and demonstrate the significance of domain adaptation in few-shot image recognition.},
  archive      = {J_ICV},
  author       = {Ge Liu and Linglan Zhao and Xiangzhong Fang},
  doi          = {10.1016/j.imavis.2021.104164},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104164},
  shortjournal = {Image Vis. Comput.},
  title        = {PDA: Proxy-based domain adaptation for few-shot image recognition},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal assessment of apparent personality using feature
attention and error consistency constraint. <em>ICV</em>, <em>110</em>,
104163. (<a href="https://doi.org/10.1016/j.imavis.2021.104163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personality computing and affective computing , where the recognition of personality traits is essential, have gained increasing interest and attention in many research areas recently. We propose a novel approach to recognize the Big Five personality traits of people from videos. To this end, we use four different modalities, namely, ambient appearance (scene), facial appearance , voice, and transcribed speech. Through a specialized subnetwork for each of these modalities, our model learns reliable modality-specific representations and fuse them using an attention mechanism that re-weights each dimension of these representations to obtain an optimal combination of multimodal information . A novel loss function is employed to enforce the proposed model to give an equivalent importance for each of the personality traits to be estimated through a consistency constraint that keeps the trait-specific errors as close as possible. To further enhance the reliability of our model, we employ (pre-trained) state-of-the-art architectures (i.e., ResNet , VGGish, ELMo) as the backbones of the modality-specific subnetworks, which are complemented by multilayered Long Short-Term Memory networks to capture temporal dynamics. To minimize the computational complexity of multimodal optimization, we use two-stage modeling, where the modality-specific subnetworks are first trained individually, and the whole network is then fine-tuned to jointly model multimodal data. On the large scale ChaLearn First Impressions V2 challenge dataset, we evaluate the reliability of our model as well as investigating the informativeness of the considered modalities. Experimental results show the effectiveness of the proposed attention mechanism and the error consistency constraint. While the best performance is obtained using facial information among individual modalities , with the use of all four modalities, our model achieves a mean accuracy of 91.8%, improving the state of the art in automatic personality analysis.},
  archive      = {J_ICV},
  author       = {Süleyman Aslan and Uğur Güdükbay and Hamdi Dibeklioğlu},
  doi          = {10.1016/j.imavis.2021.104163},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104163},
  shortjournal = {Image Vis. Comput.},
  title        = {Multimodal assessment of apparent personality using feature attention and error consistency constraint},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-shot cuboids: Geodesics-based end-to-end manhattan
aligned layout estimation from spherical panoramas. <em>ICV</em>,
<em>110</em>, 104160. (<a
href="https://doi.org/10.1016/j.imavis.2021.104160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been shown that global scene understanding tasks like layout estimation can benefit from wider field of views , and specifically spherical panoramas. While much progress has been made recently, all previous approaches rely on intermediate representations and postprocessing to produce Manhattan-aligned estimates. In this work we show how to estimate full room layouts in a single-shot, eliminating the need for postprocessing. Our work is the first to directly infer Manhattan-aligned outputs. To achieve this, our data-driven model exploits direct coordinate regression and is supervised end-to-end. As a result, we can explicitly add quasi-Manhattan constraints, which set the necessary conditions for a homography-based Manhattan alignment module. Finally, we introduce the geodesic heatmaps and loss and a boundary-aware center of mass calculation that facilitate higher quality keypoint estimation in the spherical domain. Our models and code are publicly available at https://github.com/VCL3D/SingleShotCuboids.},
  archive      = {J_ICV},
  author       = {Nikolaos Zioulis and Federico Alvarez and Dimitrios Zarpalas and Petros Daras},
  doi          = {10.1016/j.imavis.2021.104160},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104160},
  shortjournal = {Image Vis. Comput.},
  title        = {Single-shot cuboids: Geodesics-based end-to-end manhattan aligned layout estimation from spherical panoramas},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient foreign objects detection network for power
substation. <em>ICV</em>, <em>109</em>, 104159. (<a
href="https://doi.org/10.1016/j.imavis.2021.104159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A power substation is susceptible to intrusions of foreign objects. The intrusions can likely result in failures of power supplies. Therefore, recognizing foreign objects becomes important to ensure constant and stable power supplies. However, existing object recognition methods fail to achieve acceptable accuracy and performance. In this paper, we propose an efficient Foreign Objects Detection Network for Power Substation (FODN4PS) to improve the recognition accuracy with less time. FODN4PS consists of a Moving Object Region Extraction Network (MORE Net) and a classification network, where the MORE Net can get the position of foreign objects, and the classification network can recognize the category of foreign objects. Experimental results show that FODN4PS is faster and more accurate in object recognition than the Fast R-CNN and Mask R-CNN.},
  archive      = {J_ICV},
  author       = {Liang Xu and Yongkang Song and Weishan Zhang and Yunyun An and Ye Wang and Huansheng Ning},
  doi          = {10.1016/j.imavis.2021.104159},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104159},
  shortjournal = {Image Vis. Comput.},
  title        = {An efficient foreign objects detection network for power substation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SalED: Saliency prediction with a pithy encoder-decoder
architecture sensing local and global information. <em>ICV</em>,
<em>109</em>, 104149. (<a
href="https://doi.org/10.1016/j.imavis.2021.104149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a deep convolutional neural network with a concise and effective encoder-decoder architecture for saliency prediction. Local and global contextual features make a considerable contribution to saliency prediction. In order to integrate and exploit these features more thoroughly, in the proposed pithy architecture, we deploy a dense and global context connection structure between the encoder and decoder, after that, a multi-scale readout module is designed to process various information from the previous portion of the decoder with different parallel mapping relationships for full-scale accurate results. Our model ranks first in light of multiple metrics on two famous saliency benchmarks and performs good generalization on other datasets. Besides, we evaluate the precision and the speed of our model with different backbones. The saliency prediction performance of VGGNet-Based, ResNet-based, and DenseNet-based model gradually increases while the speed also drops off. And the experiments illustrate that our model performs better than other models even if replacing the backbone of our model with the same backbone of the compared model. Therefore, we can provide optional versions of our model for different requirements of performance and efficiency.},
  archive      = {J_ICV},
  author       = {Ziqiang Wang and Zhi Liu and Weijie Wei and Huizhan Duan},
  doi          = {10.1016/j.imavis.2021.104149},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104149},
  shortjournal = {Image Vis. Comput.},
  title        = {SalED: Saliency prediction with a pithy encoder-decoder architecture sensing local and global information},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NeuralPlan: Neural floorplan radiance fields for accelerated
view synthesis. <em>ICV</em>, <em>109</em>, 104148. (<a
href="https://doi.org/10.1016/j.imavis.2021.104148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach for quickly building a visual representation of a full indoor building. Our goal is to enable intelligent systems which frequently and regularly monitor buildings to assist personnel operating remotely, a need of special importance in these days. Prior work in neural scene representations for view synthesis focuses on single objects and small scenes and does not scale to full buildings in short timeframes. We propose introducing the floorplan and learning a neural floorplan radiance field , mapping floorplan 3D points and view directions to emitted radiance , and rendering via a sinusoidal multi-layer perceptron (MLP) neural renderer. To incorporate local priors and further accelerate the overall learning, we use a hypernetwork which maps a floorplan surface normal to the parameters of the neural renderer, thus defining the scene by a space of local neural rendering functions across the building. This allows shared knowledge, reasoned in function space, of performing the neural rendering from various vantage points in the scene based on similar building structure represented in the floorplan surface normal, and facilitates meta-knowledge pre-training across multiple buildings. The meta-knowledge is used to initialize the parameters of the hypernetwork at test time for the target building. Our approach performs significantly accelerated learning of neural floorplan radiance fields in around 15 min for full buildings on a single commodity GPU , and renders in real-time at 64 Hz, allowing for immersive visual experiences.},
  archive      = {J_ICV},
  author       = {John Noonan and Ehud Rivlin and Hector Rotstein},
  doi          = {10.1016/j.imavis.2021.104148},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104148},
  shortjournal = {Image Vis. Comput.},
  title        = {NeuralPlan: Neural floorplan radiance fields for accelerated view synthesis},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepSegment: Segmentation of motion capture data using deep
convolutional neural network. <em>ICV</em>, <em>109</em>, 104147. (<a
href="https://doi.org/10.1016/j.imavis.2021.104147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework to segment 3D human motion capture data into distinct behaviors. First, in preprocessing, we build a normalized pose space by eliminating translation and orientation from the 3D poses. We then transform these normalized 3D poses into 2D RGB images , and as a result, we simplify the task of motion segmentation as image classification and recognition. Furthermore, we identify the most significant joints of the skeleton that contribute substantially to executing a motion and get benefits from them by assigning them more weights. The weight allocation to the specific joint has been done purely based on its deviation capability. Finally, each motion is encoded into compact visual representation by exploiting RGB images with weighted joints. We adopt a transfer learning approach to extract a fixed-size feature vector using off-the-shelf deep Convolutional Neural Network (CNN), Alexnet, after fine-tuning. We develop a Kd-tree on these highly descriptive feature vectors to retrieve the nearest neighbors. Based on a similarity measure, we classify the motion segments and ultimately place the cuts on the ongoing motion sequences. We perform extensive experiments to evaluate our proposed approach on popular Motion Capture (MoCap) datasets, CMU and HDM05. Our approach almost outperforms all other state-of-the-art methods, and the results highlight the capabilities of our proposed scheme for effective segmentation.},
  archive      = {J_ICV},
  author       = {Hashim Yasin and Saqib Hayat},
  doi          = {10.1016/j.imavis.2021.104147},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104147},
  shortjournal = {Image Vis. Comput.},
  title        = {DeepSegment: Segmentation of motion capture data using deep convolutional neural network},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring region relationships implicitly: Image captioning
with visual relationship attention. <em>ICV</em>, <em>109</em>, 104146.
(<a href="https://doi.org/10.1016/j.imavis.2021.104146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual attention mechanism has been widely used by image captioning model in order to dynamically attend to the related visual region based on given language information. Such capability allows a trained model to carry out fine-grained level image understanding and reasoning. However, existing visual attention models only focus on the individual visual region in the image and the alignment between the language representation and related individual visual regions. It does not fully explore the relationships/interactions between visual regions. Furthermore, it does not analyze or explore alignment for related words/phrases (e.g. verb or phrasal verb), which may best describe the relationships/interactions between these visual regions. Thus, it causes the inaccurate or impropriate description to the current image captioning model. Instead of visual region attention commonly addressed by existing visual attention mechanism, this paper proposes the novel visual relationship attention via contextualized embedding for individual regions. It can dynamically explore a related visual relationship existing between multiple regions when generating interaction words. Such relationship exploring process is constrained by spatial relationships and driven by the linguistic context of language decoder. In this work, such new visual relationship attention is designed through a parallel attention mechanism under the learned spatial constraint in order to more precisely map visual relationship information to the semantic description of such relationship in language. Different from existing methods for exploring the visual relationship, it is trained implicitly through an unsupervised approach without using any explicit visual relationship annotations. By integrating the newly proposed visual relationship attention with existing visual region attention, our image captioning model can generate high-quality captions. Solid experiments on the MSCOCO dataset demonstrate the proposed visual relationship attention can effectively boost the captioning performances by capturing related visual relationships for generating accurate interaction descriptions.},
  archive      = {J_ICV},
  author       = {Zongjian Zhang and Qiang Wu and Yang Wang and Fang Chen},
  doi          = {10.1016/j.imavis.2021.104146},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104146},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring region relationships implicitly: Image captioning with visual relationship attention},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FEANet: Foreground-edge-aware network with DenseASPOC for
human parsing. <em>ICV</em>, <em>109</em>, 104145. (<a
href="https://doi.org/10.1016/j.imavis.2021.104145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human parsing has drawn a lot of attention from the public due to its critical role in high-level computer vision applications . Recent works demonstrated the effectiveness of utilizing context module and additional information in improving the performance of human parsing. However, ambiguous objects, small scaling and occlusion problems are still the bottlenecks. In this paper, we propose a novel framework called - Foreground-Edge-Aware Network (FEANet) with DenseASPOC context module to further enhance the segmentation performance for human parsing. We claim that the fusion of foreground and edge information can effectively segment occluded regions by reducing the impact of pixels occupied by non-human object parts while persevering boundaries between each class. Moreover, we introduce the Dense Atrous Spatial Pyramid Object Context (DenseASPOC) module to address the problem of small and ambiguous objects by empowering feature extraction ability with solid spatial perception and semantic context information. We conducted comprehensive experiments on various human parsing benchmarks including both single-human and multi-human parsing. Both quantitative and qualitative results show that the proposed FEANet has superiority over the current methods. Moreover, detailed ablation studies report the effectiveness of the employment on each contribution.},
  archive      = {J_ICV},
  author       = {Wing-Yin Yu and Lai-Man Po and Yuzhi Zhao and Yujia Zhang and Kin-Wai Lau},
  doi          = {10.1016/j.imavis.2021.104145},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104145},
  shortjournal = {Image Vis. Comput.},
  title        = {FEANet: Foreground-edge-aware network with DenseASPOC for human parsing},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boundary graph convolutional network for temporal action
detection. <em>ICV</em>, <em>109</em>, 104144. (<a
href="https://doi.org/10.1016/j.imavis.2021.104144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action proposal generation is a fundamental yet challenging to locate the temporal action in untrimmed videos. Although current proposal generation methods can generate the precise boundary of actions, few focus on considering the relation of proposals. In this paper, we propose a unified framework to generate the temporal boundary proposals with a graph convolution network based on the boundary proposals&#39; feature named Boundary Graph Convolutional Network (BGCN). BGCN draws inspiration from boundary methods and uses edge graph convolution relay on the boundary proposals&#39; feature. First, we use a base layer to fusion the two-stream video features to get two-branches of base features. Then the two-branches of base features enter into the same structure of Proposal Features Graph Convolutional Network (PFGCN): Action PFGCN to extract the action classification score and Boundary PFGCN to extract the ending score and staring score. In proposal features graph convolutional network, we first densely sampled the proposals&#39; feature from the video features. We construct a proposal feature graph, where each proposal feature as a node and their relations between proposals&#39; features as an edge with edge convolution for graph convolution. After that, map the relations into a 2D map score. Experiments on popular benchmarks THUMOS14 demonstrate the superiority of BGCN over (44.8% versus 42.8% at tIoU 0.5) the state-of-the-art proposal generator (e.g., G-TAD, TAL-Net, and BMN) at any of tIoU thresholds from 0.3 to 0.7. On ActivityNet1.3, BGCN also got better results. Moreover, BGCN has high efficiency for action detection with less than 2 MB model size and fast inference time. GCN based on boundary generation for densely produce the action proposals Efficient and novel BGCN model has a great capability to learn the proposal features Has a lower model size for temporal action proposals generation Has fast inference time for temporal action proposals generation.},
  archive      = {J_ICV},
  author       = {Yaosen Chen and Bing Guo and Yan Shen and Wei Wang and Weichen Lu and Xinhua Suo},
  doi          = {10.1016/j.imavis.2021.104144},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104144},
  shortjournal = {Image Vis. Comput.},
  title        = {Boundary graph convolutional network for temporal action detection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). WRGPruner: A new model pruning solution for tiny salient
object detection. <em>ICV</em>, <em>109</em>, 104143. (<a
href="https://doi.org/10.1016/j.imavis.2021.104143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The model pruning is one of the predominant model compression tasks to decrease the demands in computing power and memory footprint . However, most existing pruning methods have overly broad application areas , which defects in a sub-optimal solution specifically to solve certain specified difficult problems in the tasks of salient object detection . In this paper, we propose a novel solution, dubbed as WRGPruner , based on the concept of salient energy level (SEL) for tiny salient object detection. The concept of SEL defines the level of assessing the distinguishing ability of parameters in the trained model between background and salient objects. To exploit the SEL, the WRGPruner is proposed, which considers three factors for model compression including the weight in the filter, the mathematical rank of the feature map matrix, and the gradient in the backward propagation. We mathematically prove the effectiveness of the WRGPruner for tiny salient objects. Besides, a tiny salient object dataset (TSOD) is constructed for evaluation. Extensive experiments show that WRGPruner reduces 60% of parameters with slight enhancement in terms of six accuracy metrics for VGG16 on TSOD. This demonstrates that the SEL is suitable for measure parameters and the effectiveness of WRGPruner.},
  archive      = {J_ICV},
  author       = {Fengwei Jia and Xuan Wang and Jian Guan and Huale Li and Chen Qiu and Shuhan Qi},
  doi          = {10.1016/j.imavis.2021.104143},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104143},
  shortjournal = {Image Vis. Comput.},
  title        = {WRGPruner: A new model pruning solution for tiny salient object detection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Certifiable relative pose estimation. <em>ICV</em>,
<em>109</em>, 104142. (<a
href="https://doi.org/10.1016/j.imavis.2021.104142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present the first fast optimality certifier for the non-minimal version of the Relative Pose problem for calibrated cameras from epipolar constraints . The proposed certifier is based on Lagrangian duality and relies on a novel closed-form expression for dual points. We also leverage an efficient solver that performs local optimization on the manifold of the original problem&#39;s non-convex domain . The optimality of the solution is then checked via our novel fast certifier. The extensive conducted experiments demonstrate that, despite its simplicity, this certifiable solver performs excellently on synthetic data , repeatedly attaining the (certified a posteriori ) optimal solution and shows a satisfactory performance on real data .},
  archive      = {J_ICV},
  author       = {Mercedes Garcia-Salguero and Jesus Briales and Javier Gonzalez-Jimenez},
  doi          = {10.1016/j.imavis.2021.104142},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104142},
  shortjournal = {Image Vis. Comput.},
  title        = {Certifiable relative pose estimation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-stream slowFast graph convolutional networks for
skeleton-based action recognition. <em>ICV</em>, <em>109</em>, 104141.
(<a href="https://doi.org/10.1016/j.imavis.2021.104141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many efforts have been made to model spatial–temporal features from human skeleton for action recognition by using graph convolutional networks (GCN). Skeleton sequence can precisely represent human pose with a small number of joints while there is still a lot of redundancies across the skeleton sequence in the term of temporal dependency. In order to improve the effectiveness of spatial–temporal feature extraction from skeleton sequence, a SlowFast graph convolution network (SF-GCN) is proposed by implementing the architecture of SlowFast network, which is consisted of the Fast and Slow pathway, in the GCN model. The Fast pathway is a temporal attention embedded lightweight GCN for extracting the feature of fast temporal changes from the skeleton sequence with a high frame rate and fast refreshing speed. The Slow pathway is a spatial attention embedded GCN for extracting the feature of slow temporal changes from the skeleton sequence with a low frame rate and slow refreshing speed. The features of two pathways are fused by using lateral connection and weighted by using channel attention. Based on the aforementioned design, SF-GCN can achieve superior ability of feature extraction while the computational cost significantly drops. In addition to the coordinate information of joints, five high order sequences including edge, the spatial difference and temporal difference of joints and edges are induced to enhance the representation of human action. Six SF-GCNs are implemented for extracting spatial–temporal feature from six kinds of sequences and fused for skeleton-based action recognition, which is called multi-stream SlowFast graph convolutional networks (MSSF-GCN). Extensive experiments are conducted to evaluate the proposed method on three skeleton-based action recognition databases including NTU RGB + D, NTU RGB + D 120, and Skeleton-Kinetics. The results show that the proposed method is effective for skeleton-based action recognition and can achieve the recognition accuracy with an obvious advantage in comparison with the state-of-the-art.},
  archive      = {J_ICV},
  author       = {Ning Sun and Ling Leng and Jixin Liu and Guang Han},
  doi          = {10.1016/j.imavis.2021.104141},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104141},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-stream slowFast graph convolutional networks for skeleton-based action recognition},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HCFS3D: Hierarchical coupled feature selection network for
3D semantic and instance segmentation. <em>ICV</em>, <em>109</em>,
104129. (<a href="https://doi.org/10.1016/j.imavis.2021.104129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation and instance segmentation based on 3D point clouds involve significant challenges, specifically in the task of joint semantic and instance segmentation. The efficient and effective mutual assistance between semantic and instance segmentation is rarely considered and still remains an unaddressed research problem. To address this, herein, a novel and robust 3D point cloud segmentation framework employing hierarchical coupled feature selection, named HCFS3D, is proposed; this framework can jointly and reciprocally perform semantic and instance segmentation. The framework is designed to promote these two tasks to exploit beneficial information from each other, on a shallow as well as a deep level. Moreover, to prevent the network from overfitting and to improve performance, we designed a loss function called the Adaptive Smooth Loss, which can adaptively assign different weights to samples that are difficult to segment. Furthermore, joint semantic and instance conditional random fields are included in the proposed framework to further improve its performance. Extensive experiments based on different datasets and various backbone networks demonstrate that HCFS3D outperforms other state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Jingang Tan and Kangru Wang and Lili Chen and Guanghui Zhang and Jiamao Li and Xiaolin Zhang},
  doi          = {10.1016/j.imavis.2021.104129},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104129},
  shortjournal = {Image Vis. Comput.},
  title        = {HCFS3D: Hierarchical coupled feature selection network for 3D semantic and instance segmentation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 2D progressive fusion module for action recognition.
<em>ICV</em>, <em>109</em>, 104122. (<a
href="https://doi.org/10.1016/j.imavis.2021.104122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network convergence as well as recognition accuracy are essential issues when applying Convolutional Neural Networks (CNN) to human action recognition . Most deep learning methods neglect model convergence when striving to improve the abstraction capability, thus degrading the performances sharply when computing resources are limited. To mitigate this problem, we propose a structure named 2D Progressive Fusion (2DPF) Module which is inserted after the 2D backbone CNN layers. 2DPF fuses features through a novel 2D convolution on the spatial and temporal dimensions called variation attenuating convolution and applies fusion techniques to improve the recognition accuracy and the convergency . Our experiments performed on several benchmarks ( e.g. , Something-Something V1&amp;V2, Kinetics400 &amp; 600, AViD, UCF101) demonstrate the effectiveness of the proposed method. ARTICLE INFO.},
  archive      = {J_ICV},
  author       = {Zhongwei Shen and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1016/j.imavis.2021.104122},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104122},
  shortjournal = {Image Vis. Comput.},
  title        = {2D progressive fusion module for action recognition},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ThickSeg: Efficient semantic segmentation of large-scale 3D
point clouds using multi-layer projection. <em>ICV</em>, <em>108</em>,
104161. (<a href="https://doi.org/10.1016/j.imavis.2021.104161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient semantic segmentation of large-scale three-dimensional (3D) point clouds is an essential approach for intelligent robots to perceive the surrounding environment. However, due to the expensive sampling process or time-consuming pre/post-processing steps, most of the current solutions are inefficient or limited in scale. In this paper, we propose a novel framework, ThickSeg, to efficiently assign semantic labels for large-scale point clouds. ThickSeg contains three main steps: Firstly, it projects raw point clouds onto a multi-layer image with a random-hit strategy to efficiently preserve more local geometric features . Secondly, the projected multi-layer image is fed into a Self-Sorting 3D Convolutional Neural Network (SS-3DCNN) to predict grid-wise semantics and subsequently project them back to their corresponding 3D points. Finally, the labels of occluded points are determined by an iterative and accumulative post-processing mechanism, avoiding time-consuming explicit 3D neighborhood searching. We validate our approach on two well-known public benchmarks (SemanticKITTI and KITTI), where ThickSeg gets state-of-the-art results and more efficient than previous methods. Our detailed ablation study shows how each component contributes to the final performance.},
  archive      = {J_ICV},
  author       = {Qian Gao and Xukun Shen},
  doi          = {10.1016/j.imavis.2021.104161},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104161},
  shortjournal = {Image Vis. Comput.},
  title        = {ThickSeg: Efficient semantic segmentation of large-scale 3D point clouds using multi-layer projection},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cluster adaptation networks for unsupervised domain
adaptation. <em>ICV</em>, <em>108</em>, 104137. (<a
href="https://doi.org/10.1016/j.imavis.2021.104137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is an important technology for transferring source domain knowledge to new, unseen target domains. Recently, domain adaptation models are applied to learn domain invariant representations by minimizing distribution distance or adversarial training in the feature space . However, existing adversarial domain adaptation methods fail to preserve the data structure in the feature space. In this paper, we propose a novel domain adaptation method called Cluster adaptation Networks (CAN). CAN decreases the domain shift by aligning the category centers of source representations and the cluster centers of target representations in the feature space, which preserves the class-level structure and facilitates the classification of the target domain. Experiments on Digits, Office-Home and ImageCLEF-DA datasets validate the effectiveness of the structure preservation in our model.},
  archive      = {J_ICV},
  author       = {Qiang Zhou and Wen’an Zhou and Shirui Wang},
  doi          = {10.1016/j.imavis.2021.104137},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104137},
  shortjournal = {Image Vis. Comput.},
  title        = {Cluster adaptation networks for unsupervised domain adaptation},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive multi-scale feature representation enhancement
for small object detection. <em>ICV</em>, <em>108</em>, 104128. (<a
href="https://doi.org/10.1016/j.imavis.2021.104128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of detection, there is a wide gap between the performance of small objects and that of medium, large objects. Some studies show that this gap is due to the contradiction between the classification-based backbone and localization. Although the reduction in the feature map size is beneficial for the extraction of abstract features, it will cause the loss of detailed features in the localization as traversing the backbone. Therefore, an interactive multi-scale feature representation enhancement strategy is proposed. This strategy includes two modules: first a multi-scale auxiliary enhancement network is proposed for feature interaction under multiple inputs. We scale the input to multiple scales corresponding to the prediction layers, and only passes through the lightweight extraction module to extract more detailed features for enhancing the original futures. Moreover, an adaptive interaction module is designed to aggregate the features of adjacent layers . This approach provides flexibility in achieving the improvement of small objects detection ability without changing the original network structure. Comprehensive experimental results based on PASCAL VOC and MS COCO datasets show the effectiveness of the proposed method.},
  archive      = {J_ICV},
  author       = {Qiyuan Zheng and Ying Chen},
  doi          = {10.1016/j.imavis.2021.104128},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104128},
  shortjournal = {Image Vis. Comput.},
  title        = {Interactive multi-scale feature representation enhancement for small object detection},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Triangulate geometric constraint combined with visual-flow
fusion network for accurate 6DoF pose estimation. <em>ICV</em>,
<em>108</em>, 104127. (<a
href="https://doi.org/10.1016/j.imavis.2021.104127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the 6D object pose based on a monocular RGB image is a challenging task in computer vision , which produces false positives under the influence of occlusion or cluttered environments. In addition, the prediction of translation is affected by changes of the image size. In this work, we present a novel two-stage method TGCPose6D for robust 6DoF object pose estimation which is composed of 2D keypoint detection and translation refinement. In the first stage, the 2D keypoint regression space is constrained by triangulate geometric feature vectors, and the low-quality prediction is suppressed by the center-heatmap weighted loss function, thereby the performance of keypoint detection is significantly improved. In the second stage, the Visual-Flow Fusion network (VFFNet) is used to extract the visual feature and optical flow feature of the rendered image and the observed image, and to predict the relative translation based on the difference of features. Specifically, the VFFNet is trained iteratively to gain the ability to predict the relative translation deviation. Extensive experiments are conducted to demonstrate the effectiveness of the proposed TGCPose6D method. Our overall pose estimation pipeline outperforms state-of-the-art object pose estimation methods on several benchmarks.},
  archive      = {J_ICV},
  author       = {Zhihong Jiang and Xin Wang and Xiao Huang and Hui Li},
  doi          = {10.1016/j.imavis.2021.104127},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104127},
  shortjournal = {Image Vis. Comput.},
  title        = {Triangulate geometric constraint combined with visual-flow fusion network for accurate 6DoF pose estimation},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image captioning via proximal policy optimization.
<em>ICV</em>, <em>108</em>, 104126. (<a
href="https://doi.org/10.1016/j.imavis.2021.104126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is the task of generating captions of images in natural language. The training typically consists of two phases, first minimizing the XE (cross-entropy) loss, and then with RL (reinforcement learning) over CIDEr scores. Although there are many innovations in neural architectures, fewer works are proposed for the RL phase. Motivated by one recent state-of-the-art architecture X-Transformer [Pan et al., CVPR 2020], we apply PPO (Proximal Policy Optimization) to it to establish a further improvement. However, trivially applying a vanilla policy gradient objective function with the clipping form of PPO would not improve the result. Therefore, we introduce certain modifications. We show that PPO is capable of enforcing trust-region constraints effectively. Also, experimentally performance decreases when PPO is combined with the regularization technique dropout. We analyze the possible reason in terms of KL-divergence of RL policies. As to the baseline adopted in the policy gradient estimator of RL, it is generally sentence-level. So all words in the same sentence use the same baseline in the gradient estimator. We instead use a word-level baseline via Monte-Carlo estimation. Thus, different words can have different baseline values. With all these, by fine-tuning a pre-trained X-Transformer, we train a single model achieving a competitive result of 133.3% on the MSCOCO Karpathy test set. Source code is available at https://github.com/lezhang-thu/xtransformer-ppo .},
  archive      = {J_ICV},
  author       = {Le Zhang and Yanshuo Zhang and Xin Zhao and Zexiao Zou},
  doi          = {10.1016/j.imavis.2021.104126},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104126},
  shortjournal = {Image Vis. Comput.},
  title        = {Image captioning via proximal policy optimization},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tibetan thangka data set and relative tasks. <em>ICV</em>,
<em>108</em>, 104125. (<a
href="https://doi.org/10.1016/j.imavis.2021.104125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data set of high quality is the cornerstone of the current data-driven machine learning models, and plays an important role in promoting the development of various application areas . At present, image analysis and processing techniques have intensively involved into the tasks of inheriting and protecting culture resources. However, currently there are few effective image data sets about the traditional Chinese Tibetan culture. In this work, we provided a small data set referred as CYTKv1(Chomo Yarlung Tibet version 1) which includes 1700 + Thangka images (an important and representative carrier of Chinese Tibetan culture), and the main objects in each image are manually labeled and bounding-boxed with semantic words. In addition, we shared a list of tasks of processing and analyzing the Thangkas to enlighten researchers about the challenges and potential applications on this data set. At last, we tested several famous deep learning models for the purpose of validating the annotation task on the new data set and presented the results of them, and finally selected the best one as the baseline for the annotation task.},
  archive      = {J_ICV},
  author       = {Yanchun Ma and Yongjian Liu and Qing Xie and Shengwu Xiong and Lihua Bai and Anshu Hu},
  doi          = {10.1016/j.imavis.2021.104125},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104125},
  shortjournal = {Image Vis. Comput.},
  title        = {A tibetan thangka data set and relative tasks},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving eye movement biometrics in low frame rate
eye-tracking devices using periocular and eye blinking features.
<em>ICV</em>, <em>108</em>, 104124. (<a
href="https://doi.org/10.1016/j.imavis.2021.104124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the biometric potential of eye movement patterns extracted from low frame rate eye-tracking devices is evaluated. Also, possible improvement in recognition rates is investigated using other static and dynamic features extracted from the eyes including eye blinking patterns and periocular shape features. These modalities can be applicable for specific biometric applications like continuous driver authentication for law enforcement. For this purpose, two databases are collected with two low frame rate eye-tracking systems that capture the eye movements. Data were recorded from 55 participants while watching real driving sessions. For eye gaze, features from fixations and saccades are extracted separately including duration, amplitude, and statistical features. For eye blinking, features from the blinking pattern, its speed, acceleration, and power per unit mass profiles are extracted. Periocular features include the eye-opening height, width, axial ratio , etc. Each modality is evaluated first, then, these modalities are combined in a multi-modal setup for performance improvement . While each trait achieved a moderate performance in a single-modality setup, the fusion of the static and the dynamic features from the eye provides a great performance improvement up to 98.5% recognition rate and 0% error rate in both modes of authentication. Although the single-modality setup might not be secure enough, the fusion of these traits achieves high levels of identification making these traits effective for continuous driver authentication application.},
  archive      = {J_ICV},
  author       = {Sherif Nagib Abbas Seha and Dimitrios Hatzinakos and Ali Shahidi Zandi and Felix J.E. Comeau},
  doi          = {10.1016/j.imavis.2021.104124},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104124},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving eye movement biometrics in low frame rate eye-tracking devices using periocular and eye blinking features},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel features for art movement classification of portrait
paintings. <em>ICV</em>, <em>108</em>, 104121. (<a
href="https://doi.org/10.1016/j.imavis.2021.104121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing availability of extensive digitized fine art collections opens up new research directions. In particular, correctly identifying the artistic style or art movement of paintings is crucial for large artistic database indexing, painter authentication, and mobile recognition of painters. Even though the implementation of CNN on artwork classification improved the performance dramatically compared to tradition classifier, the feature extraction methods are still valuable to help establishing better image representation for both common classifiers and neural networks. The main goal of this article is to present three novel features and a mature model structure for artistic movement recognition of portrait paintings. The proposed features include two unique color features and one texture feature: (a) Modified Color Distance (MCD), (b) ColorRatio Feature and (c) Weber&#39;s law Based Texture Feature. We demonstrate the superiority of our proposed method over the state-of-the-art approaches, and how successful our features are to support features from various neural networks. Another contribution of our work is a new portrait database that consists of 927 paintings from 6 different art movements. Extensive computer evaluations on this database show that we achieved an average accuracy of 98% for classifying two categories and 82.6% for classifying all 6 categories. Besides, our novel features improved the performance of pre-trained CNN significantly.},
  archive      = {J_ICV},
  author       = {Shao Liu and Jiaqi Yang and Sos S. Agaian and Changhe Yuan},
  doi          = {10.1016/j.imavis.2021.104121},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104121},
  shortjournal = {Image Vis. Comput.},
  title        = {Novel features for art movement classification of portrait paintings},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A study on attention-based LSTM for abnormal behavior
recognition with variable pooling. <em>ICV</em>, <em>108</em>, 104120.
(<a href="https://doi.org/10.1016/j.imavis.2021.104120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Behavior recognition is a well-known computer vision mobile technology. It has been used in many applications such as video surveillance, motion detection on devices, human-computer interaction and sports video, etc. However, most of the existing works ignored the depth and spatio-temporal information so that they resulted in over-fitting and inferior performance. Consequently, a novel framework for behavior recognition is proposed in this paper. In this framework, we propose a target depth estimation algorithm to calculate the 3D spatial position information of the target, and take this information as the input of the behavior recognition model. Simultaneously, in order to obtain more Spatio-temporal information and better handle long-term video, combining with the idea of attention mechanism , we propose a skeleton behavior recognition model which is based on spatio-temporal convolution and attention-based LSTM (ST-CNN &amp; ATT-LSTM). The deep spatial information is merged into each segment, and the model focuses on the key information extraction, which is essential for improving behavior recognition performance. Meanwhile, we use a feature compression method based on variable pooling to solve the problem of inconsistent input sizes caused by multi-person behavior recognition, so that the network can flexibly recognize multi-person skeleton sequences. Finally, the proposed framework is evaluated with real-world surveillance video data, and the results indicate that our framework is superior to existing methods.},
  archive      = {J_ICV},
  author       = {Kai Zhou and Bei Hui and Junfeng Wang and Chunyu Wang and Tingting Wu},
  doi          = {10.1016/j.imavis.2021.104120},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104120},
  shortjournal = {Image Vis. Comput.},
  title        = {A study on attention-based LSTM for abnormal behavior recognition with variable pooling},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative adversarial networks and their application to 3D
face generation: A survey. <em>ICV</em>, <em>108</em>, 104119. (<a
href="https://doi.org/10.1016/j.imavis.2021.104119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks ( GANs ) have been extensively studied in recent years and have been used to address several problems in the fields of image generation and computer vision . Despite significant advancements in computer vision, applying GANs to real-world problems such as 3 D face generation remains a challenge. Owing to the proliferation of fake images generated by GANs , it is important to analyze and build a taxonomy for providing an overall view of GANs . This, in turn, would facilitate many interesting applications, including virtual reality, augmented reality , computer games, teleconferencing , virtual try-on, special effects in movies, and 3 D avatars. This paper reviews and discusses GANs and their application to 3 D face generation. We aim to compare existing GANs methods in terms of their application to 3 D face generation, investigate the related theoretical issues, and highlight the open research problems. Authors provided both qualitative and quantitative evaluations of the proposed approach. They claimed their results show the higher quality of the synthesized data compared to state-of-the-art ones.},
  archive      = {J_ICV},
  author       = {Mukhiddin Toshpulatov and Wookey Lee and Suan Lee Ph.D.},
  doi          = {10.1016/j.imavis.2021.104119},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104119},
  shortjournal = {Image Vis. Comput.},
  title        = {Generative adversarial networks and their application to 3D face generation: A survey},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond modality alignment: Learning part-level
representation for visible-infrared person re-identification.
<em>ICV</em>, <em>108</em>, 104118. (<a
href="https://doi.org/10.1016/j.imavis.2021.104118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared person re-IDentification (VI-reID) aims to automatically retrieve the pedestrian of interest exposed to sensors in different modalities, such as visible camera v.s. infrared sensor. It struggles to learn both modality-invariant and discriminant representations. Unfortunately, existing VI-reID work mainly focuses on tackling the modality difference, which fine-grained level discriminant information has not been well investigated. This causes inferior identification performance. To address the problem, we propose a Dual-Alignment Part-aware Representation (DAPR) framework to simultaneously alleviate the modality bias and mine different level of discriminant representations. Particularly, our DAPR reduces modality discrepancy of high-level features hierarchically by back-propagating reversal gradients from a modality classifier, in order to learn a modality-invariant feature space . And meanwhile, multiple heads of classifiers with the improved part-aware BNNeck are integrated to supervise the network producing identity-discriminant representations w.r.t. both local details and global structures in the learned modality-invariant space. By training in an end-to-end manner, the proposed DAPR produces camera-modality-invariant yet discriminant features 1 for the purpose of person matching across modalities. Extensive experiments are conducted on two benchmarks, i.e., SYSU MM01 and RegDB, and the results demonstrate the effectiveness of our proposed method.},
  archive      = {J_ICV},
  author       = {Peng Zhang and Qiang Wu and Xunxiang Yao and Jingsong Xu},
  doi          = {10.1016/j.imavis.2021.104118},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104118},
  shortjournal = {Image Vis. Comput.},
  title        = {Beyond modality alignment: Learning part-level representation for visible-infrared person re-identification},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of iris datasets. <em>ICV</em>, <em>108</em>,
104109. (<a href="https://doi.org/10.1016/j.imavis.2021.104109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on human eye image processing and iris recognition has grown steadily over the last few decades. It is important for researchers interested in this discipline to know the relevant datasets in this area to (i) be able to compare their results and (ii) speed up their research using existing datasets rather than creating custom datasets. In this paper, we provide a comprehensive overview of the existing publicly available datasets and their popularity in the research community using a bibliometric approach. We reviewed 158 different iris datasets referenced from the 689 most relevant research articles indexed by the Web of Science online library. We categorized the datasets and described the properties important for performing relevant research. We provide an overview of the databases per category to help investigators conducting research in the domain of iris recognition to identify relevant datasets.},
  archive      = {J_ICV},
  author       = {Lubos Omelina and Jozef Goga and Jarmila Pavlovicova and Milos Oravec and Bart Jansen},
  doi          = {10.1016/j.imavis.2021.104109},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104109},
  shortjournal = {Image Vis. Comput.},
  title        = {A survey of iris datasets},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge distillation methods for efficient unsupervised
adaptation across multiple domains. <em>ICV</em>, <em>108</em>, 104096.
(<a href="https://doi.org/10.1016/j.imavis.2021.104096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beyond the complexity of CNNs that require training on large annotated datasets, the domain shift between design and operational data has limited the adoption of CNNs in many real-world applications. For instance, in person re-identification, videos are captured over a distributed set of cameras with non-overlapping viewpoints. The shift between the source (e.g. lab setting) and target (e.g. cameras) domains may lead to a significant decline in recognition accuracy . Additionally, state-of-the-art CNNs may not be suitable for such real-time applications given their computational requirements. Although several techniques have recently been proposed to address domain shift problems through unsupervised domain adaptation (UDA), or to accelerate/compress CNNs through knowledge distillation (KD), we seek to simultaneously adapt and compress CNNs to generalize well across multiple target domains. In this paper, we propose a progressive KD approach for unsupervised single-target DA (STDA) and multi-target DA (MTDA) of CNNs. Our method for KD-STDA adapts a CNN to a single target domain by distilling from a larger teacher CNN, trained on both target and source domain data in order to maintain its consistency with a common representation. This method is extended to address MTDA problems, where multiple teachers are used to distill multiple target domain knowledge to a common student CNN. A different target domain is assigned to each teacher model for UDA, and they alternatively distill their knowledge to the student model to preserve specificity of each target, instead of directly combining the knowledge from each teacher using fusion methods. Our proposed approach is compared against state-of-the-art methods for compression and STDA of CNNs on the Office31 and ImageClef-DA image classification datasets. It is also compared against state-of-the-art methods for MTDA on Digits, Office31, and OfficeHome. In both settings – KD-STDA and KD-MTDA – results indicate that our approach can achieve the highest level of accuracy across target domains, while requiring a comparable or lower CNN complexity.},
  archive      = {J_ICV},
  author       = {Le Thanh Nguyen-Meidine and Atif Belal and Madhu Kiran and Jose Dolz and Louis-Antoine Blais-Morin and Eric Granger},
  doi          = {10.1016/j.imavis.2021.104096},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104096},
  shortjournal = {Image Vis. Comput.},
  title        = {Knowledge distillation methods for efficient unsupervised adaptation across multiple domains},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-source material image optimized selection based
multi-option composition. <em>ICV</em>, <em>107</em>, 104123. (<a
href="https://doi.org/10.1016/j.imavis.2021.104123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image composition aims to composite a material region into a target image. Using this technique, more images could be interactive, as there are millions of images created daily in modern life. However, it is difficult for the majority of traditional composition methods to retrieve relatively realistic semantically valid material images. Also, even if the minority of traditional composition methods yield superior results, it remains difficult to retrieve adequate material images due to the limitation of existing semantically valid images. Artificial traces also significantly reduce the visual esthetic of composited results. Based on these problems, we propose a multi-source material image optimized selection based multi-option composition method. Firstly, a robust sparse coding based retrieval model is used to retrieve material images effectively. On this basis, the attention mechanism based circular SA-GAN model could generate many semantically valid material images that guarantee the adequacy of material images. Secondly, the pixel-level optimization based multi-scale composition model minimizes artificial traces and reduces computational burden. Finally, an experimental database using many images was built. Based on this model, adequate comparative experiments using multiple evaluation criteria fully show the proposed method&#39;s effectiveness and robustness.},
  archive      = {J_ICV},
  author       = {Hao Wu and Ding An and Xiaoyu Zhu and Zhiyi Zhang and Guodong Fan and Zhen Hua},
  doi          = {10.1016/j.imavis.2021.104123},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104123},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-source material image optimized selection based multi-option composition},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted boxes fusion: Ensembling boxes from different
object detection models. <em>ICV</em>, <em>107</em>, 104117. (<a
href="https://doi.org/10.1016/j.imavis.2021.104117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a crucial task in computer vision systems with a wide range of applications in autonomous driving, medical imaging, retail, security, face recognition , robotics, and others. Nowadays, neural networks-based models are used to localize and classify instances of objects of particular classes. When real-time inference is not required, ensembles of models help to achieve better results. In this work, we present a novel method for fusing predictions from different object detection models: weighted boxes fusion. Our algorithm utilizes confidence scores of all proposed bounding boxes to construct averaged boxes. We tested the method on several datasets and evaluated it in the context of Open Images and COCO Object Detection challenges, achieving top results in these challenges. The 3D version of boxes fusion was successfully applied by the winning teams of Waymo Open Dataset and Lyft 3D Object Detection for Autonomous Vehicles challenges. The source code is publicly available at GitHub (Solovyev, 2019 [31] ). We present a novel method for combining predictions in ensembles of different object detection models: weighted boxes fusion. This method significantly improves the quality of the fused predicted rectangles for an ensemble. We tested the method on several datasets and evaluated it in the context of the Open Images and COCO Object Detection challenges. It helped to achieve top results in these challenges. The source code is publicly available at GitHub.},
  archive      = {J_ICV},
  author       = {Roman Solovyev and Weimin Wang and Tatiana Gabruseva},
  doi          = {10.1016/j.imavis.2021.104117},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104117},
  shortjournal = {Image Vis. Comput.},
  title        = {Weighted boxes fusion: Ensembling boxes from different object detection models},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on role of computer vision in smart cities.
<em>ICV</em>, <em>107</em>, 104113. (<a
href="https://doi.org/10.1016/j.imavis.2021.104113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Wei Wei and Jinsong Wu and Chunsheng Zhu},
  doi          = {10.1016/j.imavis.2021.104113},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104113},
  shortjournal = {Image Vis. Comput.},
  title        = {Special issue on role of computer vision in smart cities},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Whether normalized or not? Towards more robust iris
recognition using dynamic programming. <em>ICV</em>, <em>107</em>,
104112. (<a href="https://doi.org/10.1016/j.imavis.2021.104112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris recognition is one of the most promising fields in biometrics due to more accurate, convenient and low-cost. However, it is still a challenging task for application in practical complex scenarios. More attention have been paid on non-ideal iris segmentation and cross-system feature extraction in recent years. In order to solve the issues, this paper investigates a novel non-normalized preprocessing method based on dynamic path search for iris segmentation. Meanwhile, we employ a deep convolution network (DCNN) based on partial convolution operators to extract iris features. Through benchmark experiments on two public iris datasets CASIA-Iris-Thousand (CASIA) and IIT Delhi Iris Dataset (IITD), we achieve the significant and encouraging results, which demonstrate the effectiveness of the proposed methods. More importantly, we prove that using iris segmentation images without normalization may be a better choice when exploring iris recognition solutions based on deep learning .},
  archive      = {J_ICV},
  author       = {Yifeng Chen and Cheng Wu and Yiming Wang},
  doi          = {10.1016/j.imavis.2021.104112},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104112},
  shortjournal = {Image Vis. Comput.},
  title        = {Whether normalized or not? towards more robust iris recognition using dynamic programming},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative knowledge distillation for incomplete
multi-view action prediction. <em>ICV</em>, <em>107</em>, 104111. (<a
href="https://doi.org/10.1016/j.imavis.2021.104111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting future actions is a key in visual understanding, surveillance, and human behavior analysis. Current methods for video-based prediction are primarily using single-view data, while in the real world multiple cameras and produced videos are readily available, which may potentially benefit the action prediction tasks. However, it may bring up a new challenge: subjects in the videos are more likely to be occluded by objects when captured from different angles, or suffer from signal jittering in transmission. To that end, in this paper we propose a novel student network called Collaborative Knowledge Distillation (CKD) to predict human actions with missing information under a multi-view setting, i.e., incomplete multi-view action prediction. First, we create a graph attention based teacher model capable of fusing multi-view video features for prediction task. Second, we construct a corruption pattern bank (CPB) to simulate various missing segments in multi-view video, and each student model will manage one pattern through privileged information and knowledge distillation. Third, to account for arbitrary missing video segments in real-world, the ensemble of student models will be developed to make a joint prediction. The proposed framework has been extensively evaluated on popular multi-view visual action datasets, including PKU-MMD and NTU-RGB to validate the effectiveness of our approach and to the best of our knowledge action prediction has not yet been explored in the multi-view setting.},
  archive      = {J_ICV},
  author       = {Deepak Kumar and Chetan Kumar and Ming Shao},
  doi          = {10.1016/j.imavis.2021.104111},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104111},
  shortjournal = {Image Vis. Comput.},
  title        = {Collaborative knowledge distillation for incomplete multi-view action prediction},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-information-based convolutional neural network with
attention mechanism for pedestrian trajectory prediction. <em>ICV</em>,
<em>107</em>, 104110. (<a
href="https://doi.org/10.1016/j.imavis.2021.104110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting pedestrian trajectory is useful in many applications, such as autonomous driving and unmanned vehicles . However, it is a challenging task because of the complexity of the interactions among pedestrians and the environment. Most existing works employ long short-term memory networks to learn pedestrian behaviors, but their prediction accuracy is not good, and their computing speed is relatively slow. To tackle this problem, we propose a multi-information-based convolutional neural network (MI-CNN) to incorporate the historical trajectory, depth map, pose, and 2D-3D size information to predict the future trajectory of the pedestrian subject. After training, we evaluate our model on crowded videos in the public datasets MOT16 and MOT20. Experiments demonstrate that the proposed method outperforms state-of-the-art approaches both in prediction accuracy and computing speed.},
  archive      = {J_ICV},
  author       = {Ruiping Wang and Yong Cui and Xiao Song and Kai Chen and Hong Fang},
  doi          = {10.1016/j.imavis.2021.104110},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104110},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-information-based convolutional neural network with attention mechanism for pedestrian trajectory prediction},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion saliency based multi-stream multiplier ResNets for
action recognition. <em>ICV</em>, <em>107</em>, 104108. (<a
href="https://doi.org/10.1016/j.imavis.2021.104108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Motion Saliency based multi-stream Multiplier ResNets (MSM-ResNets) for action recognition. The proposed MSM-ResNets model consists of three interactive streams: the appearance stream, motion stream and motion saliency stream. Similar to conventional two-stream CNNs models, the appearance stream and motion stream are responsible for capturing the appearance information and motion information, respectively, while the motion saliency stream is responsible for capturing the salient motion information. In particular, to effectively utilize the spatiotemporal interactive information between different streams, the proposed MSM-ResNets model establishes interactive connections between different streams instead of fusing three streams at the final output layer. Two kinds of different multiplicative connections are injected, the first one is to inject multiplicative connections from the motion stream to the appearance stream, while the second one is to inject multiplicative connections from the motion saliency stream to the motion stream. Experimental results verify the effectiveness of the proposed MSM-ResNets on two standard action recognition datasets: UCF101 and HMDB51.},
  archive      = {J_ICV},
  author       = {Ming Zong and Ruili Wang and Xiubo Chen and Zhe Chen and Yuanhao Gong},
  doi          = {10.1016/j.imavis.2021.104108},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104108},
  shortjournal = {Image Vis. Comput.},
  title        = {Motion saliency based multi-stream multiplier ResNets for action recognition},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optokinetic response for mobile device biometric liveness
assessment. <em>ICV</em>, <em>107</em>, 104107. (<a
href="https://doi.org/10.1016/j.imavis.2021.104107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a practical pursuit of quantified uniqueness, biometrics explores the parameters that make us who we are and provides the tools we need to secure the integrity of that identity. In our culture of constant connectivity, an increasing reliance on biometrically secured mobile devices is transforming them into a target for bad actors. While no system will ever prevent all forms of intrusion, even state of the art biometric methods remain vulnerable to spoof attacks. As these attacks become more sophisticated, liveness based attack detection methods provide a potential deterrent. We present a novel optokinetc nystagmus (OKN) based liveness assessment system for mobile applications which leverages phase-locked temporal features of a unique reflexive behavioral response. In this paper we provide proof of concept for eliciting, collecting and extracting the OKN response motion signature on a mobile device. Results of our most successful experimental machine learning classifier are reported for a multi-layer LSTM based model demonstrating a 98.4% single stimulus detection performance for simulated video based attacks.},
  archive      = {J_ICV},
  author       = {Jesse Lowe and Reza Derakhshani},
  doi          = {10.1016/j.imavis.2021.104107},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104107},
  shortjournal = {Image Vis. Comput.},
  title        = {Optokinetic response for mobile device biometric liveness assessment},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pooling-based feature pyramid network for salient object
detection. <em>ICV</em>, <em>107</em>, 104099. (<a
href="https://doi.org/10.1016/j.imavis.2021.104099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively utilize and fuse deep features has become a critical point for salient object detection. Most existing methods usually adopt the convolutional features based on U-shape structures and fuse multi-scale convolutional features without fully considering the different characteristics between high-level features and low-level features. Furthermore, existing salient object detection methods rarely consider the role of pooling in convolutional neural networks . Moreover, there is still much room to improve the detection performance for objects in complex scenes. To address the problems mentioned above, we propose a pooling-based feature pyramid (PFP) network to boost salient object detection performance in this paper. First, we design two U-shaped feature pyramid modules to capture rich semantic information from high-level features and to obtain clear saliency boundaries from low-level features respectively. Second, a pyramid pooling refinement module is designed to utilize the pooling to capture more semantic information. Third, a universal channel-wise attention (UCA) module is designed to select effective high-level features of multi-scale and multi-receptive-field for rich semantic information, even in complex scenes. Finally, we fuse the selected high-level features and low-level features together, followed by an edge preservation loss to obtain accurate boundary location. Extensive experiments are conducted on five datasets and the experimental results indicate that our proposed method has the ability to get better salient object detection performance compared to the state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Caijuan Shi and Weiming Zhang and Changyu Duan and Houru Chen},
  doi          = {10.1016/j.imavis.2021.104099},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104099},
  shortjournal = {Image Vis. Comput.},
  title        = {A pooling-based feature pyramid network for salient object detection},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unsupervised domain adaptation scheme for single-stage
artwork recognition in cultural sites. <em>ICV</em>, <em>107</em>,
104098. (<a href="https://doi.org/10.1016/j.imavis.2021.104098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing artworks in a cultural site using images acquired from the user&#39;s point of view (First Person Vision) allows to build interesting applications for both the visitors and the site managers. However, current object detection algorithms working in fully supervised settings need to be trained with large quantities of labeled data, whose collection requires a lot of times and high costs in order to achieve good performance . Using synthetic data generated from the 3D model of the cultural site to train the algorithms can reduce these costs. On the other hand, when these models are tested with real images, a significant drop in performance is observed due to the differences between real and synthetic images . In this study we consider the problem of Unsupervised Domain Adaptation for object detection in cultural sites. To address this problem, we created a new dataset containing both synthetic and real images of 16 different artworks. We hence investigated different domain adaptation techniques based on one-stage and two-stage object detector, image-to-image translation and feature alignment. Based on the observation that single-stage detectors are more robust to the domain shift in the considered settings, we proposed a new method which builds on RetinaNet and feature alignment that we called DA-RetinaNet. The proposed approach achieves better results than compared methods on the proposed dataset and on Cityscapes. To support research in this field we release the dataset at the following link https://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/ and the code of the proposed architecture at https://github.com/fpv-iplab/DA-RetinaNet .},
  archive      = {J_ICV},
  author       = {Giovanni Pasqualino and Antonino Furnari and Giovanni Signorello and Giovanni Maria Farinella},
  doi          = {10.1016/j.imavis.2021.104098},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104098},
  shortjournal = {Image Vis. Comput.},
  title        = {An unsupervised domain adaptation scheme for single-stage artwork recognition in cultural sites},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clothing generation by multi-modal embedding: A
compatibility matrix-regularized GAN model. <em>ICV</em>, <em>107</em>,
104097. (<a href="https://doi.org/10.1016/j.imavis.2021.104097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clothing compatibility learning has gained increasing research attention due to the fact that a properly coordinated outfit can represent personality and improve an individual&#39;s appearance greatly. In this paper, we propose a C ompatibility M atrix- R egularized G enerative A dversarial N etwork (CMRGAN) for compatible item generation. In particular, we utilize a multi-modal embedding to transform the image and text information of an input clothing item into a latent feature code. Sequentially, compatibility learning among latent features is performed to obtain a compatibility style space. The feature of the input image is then regularized by the style space. Finally, a compatible clothing image is generated by a decoder which is fed by the regularized features. To verify the proposed model, we train an Inception-v3 classification model to evaluate the authenticity of synthesized images, a regression scoring VGG model to measure the compatibility degree of the generated image pairs and a deep attentional multimodal similarity model to evaluate the semantic similarity between generated images and ground truth text descriptions. In order to give an objective evaluation, these models are trained based on datasets consisting of fashion data only. The results demonstrate the effectiveness of the proposed method on image-to-image translation based on compatibility space.},
  archive      = {J_ICV},
  author       = {Linlin Liu and Haijun Zhang and Dongliang Zhou},
  doi          = {10.1016/j.imavis.2021.104097},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104097},
  shortjournal = {Image Vis. Comput.},
  title        = {Clothing generation by multi-modal embedding: A compatibility matrix-regularized GAN model},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracking fiducial markers with discriminative correlation
filters. <em>ICV</em>, <em>107</em>, 104094. (<a
href="https://doi.org/10.1016/j.imavis.2020.104094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, squared fiducial markers have become a popular and efficient tool to solve monocular localization and tracking problems at a very low cost. Nevertheless, marker detection is affected by noise and blur: small camera movements may cause image blurriness that prevents marker detection. The contribution of this paper is two-fold. First, it proposes a novel approach for estimating the location of markers in images using a set of Discriminative Correlation Filters (DCF). The proposed method outperforms state-of-the-art methods for marker detection and standard DCFs in terms of speed, precision, and sensitivity. Our method is robust to blur and scales very well with image resolution, obtaining more than 200fps in HD images using a single CPU thread. As a second contribution, this paper proposes a method for camera localization with marker maps employing a predictive approach to detect visible markers with high precision, speed, and robustness to blurriness. The method has been compared to the state-of-the-art SLAM methods obtaining, better accuracy, sensitivity, and speed. The proposed approach is publicly available as part of the ArUco library.},
  archive      = {J_ICV},
  author       = {Francisco J. Romero-Ramirez and Rafael Muñoz-Salinas and Rafael Medina-Carnicer},
  doi          = {10.1016/j.imavis.2020.104094},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104094},
  shortjournal = {Image Vis. Comput.},
  title        = {Tracking fiducial markers with discriminative correlation filters},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on object detection for the internet of multimedia
things (IoMT) using deep learning and event-based middleware:
Approaches, challenges, and future directions. <em>ICV</em>,
<em>106</em>, 104095. (<a
href="https://doi.org/10.1016/j.imavis.2020.104095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An enormous amount of sensing devices (scalar or multimedia) collect and generate information (in the form of events) over the Internet of Things (IoT). Present research on IoT mainly focus on the processing of scalar sensor data events and barely considers the challenges posed by multimedia based events. In this paper, we systematically review the existing solutions available for the Internet of Multimedia Things (IoMT) by analyzing sensing , networking , service , and application-level services provided by IoT. We present state-of-the-art event-based middleware methods and their suitability for multimedia event processing methods . We observe that existing IoT event-based middleware solutions focus on structured (scalar) events and possess only domain-specific characteristics for unstructured (multimedia) events. A case study for object detection is also presented to demonstrate the requirements associated with the processing of multimedia events within smart cities, even with common image recognition based applications. In order to validate the existing issues in the detection of objects, we also presented an evaluation of object detection models using existing datasets. At the end of each section, we shed light on trends, gaps, and possible solutions based on our analysis, experiments, and review of the existing research. Finally, we summarize the challenges and future research directions for the generalized multimedia event processing (by taking detection of each and every object as an example) based on applications using IoMT. Our experiments demonstrate that existing models are very slow to respond to any unseen class, and existing rich datasets do not have a sufficient number of classes to meet the requirements of real-time applications of smart cities. We show that although there is a significantly large technical literature on IoT, and research on IoMT is also quite actively growing, there have not been much research efforts directed towards the processing of multimedia events. As an example, although deep learning techniques have been shown to achieve impressive performance in applications like image recognition, the methods are deficient in detecting new (previously unseen) objects for multimedia based applications in smart cities. In light of these facts, it becomes imperative to conduct research on bringing together the abilities of event-based middleware for IoMT, and low response-time based online training and adaptation techniques.},
  archive      = {J_ICV},
  author       = {Asra Aslam and Edward Curry},
  doi          = {10.1016/j.imavis.2020.104095},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104095},
  shortjournal = {Image Vis. Comput.},
  title        = {A survey on object detection for the internet of multimedia things (IoMT) using deep learning and event-based middleware: Approaches, challenges, and future directions},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised face frontalization for pose-invariant face
recognition. <em>ICV</em>, <em>106</em>, 104093. (<a
href="https://doi.org/10.1016/j.imavis.2020.104093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face frontalization aims to normalize profile faces to frontal ones for pose-invariant face recognition . Current works have achieved promising results in face frontalization by using deep learning techniques. However, training deep models of face frontalization usually needs paired training data which is undoubtedly costly and time-consuming to acquire. To address this issue, we propose a Pose Conditional CycleGAN (PCCycleGAN) to generate authentic and identity-preserving frontal face images for pose-invariant face recognition. First, through coupling with a pair of inverse mappings, constraining with cycle consistent loss and using conditional pose label to control specific face pose generation, PCCycleGAN can be trained with unpaired samples. Second, pixel-level loss, feature space perception loss, and identity preserving loss are introduced in PCCycleGAN to help synthesize realistic and identity-preserving frontal face images. Extensive experiments on both constrained Multi-PIE dataset and unconstrained LFW and IJB-A datasets are conducted on face synthesis and pose-invariant face recognition. Results demonstrate that the proposed face frontalization model can synthesize frontal faces with high image quality as well as maintaining the identity information in both the constrained and unconstrained environments. In addition, our method enhances the performance of face recognition on the Multi-PIE, LFW and IJB-A datasets and achieves competitive face recognition performance on LFW and IJB-A datasets.},
  archive      = {J_ICV},
  author       = {Yanfei Liu and Junhua Chen},
  doi          = {10.1016/j.imavis.2020.104093},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104093},
  shortjournal = {Image Vis. Comput.},
  title        = {Unsupervised face frontalization for pose-invariant face recognition},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point cloud classification with deep normalized reeb graph
convolution. <em>ICV</em>, <em>106</em>, 104092. (<a
href="https://doi.org/10.1016/j.imavis.2020.104092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, plenty of deep learning methods have been proposed to handle point clouds. Almost all of them input the entire point cloud and ignore the information redundancy lying in point clouds. This paper addresses this problem by extracting the Reeb graph from point clouds, which is a much more informative and compact representation of point clouds, and then filter the graph with deep graph convolution . To be able to classify or segment point clouds well, we propose (1) Graph Normalization to transform various graphs into a canonical graph space; (2) Normalized Similarity Distance to better identify the graph structure;(3) Reeb Graph Guided Node Pooling in order to aggregate high-level features from kNN graphs . Besides, our method naturally fits into the problem of classifying point clouds with unknown orientations. In the results, we show that our method gives a competitive performance to the state-of-the-art methods and outperforms previous methods by a large margin on handling point clouds with unknown orientations.},
  archive      = {J_ICV},
  author       = {Weiming Wang and Yang You and Wenhai Liu and Cewu Lu},
  doi          = {10.1016/j.imavis.2020.104092},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104092},
  shortjournal = {Image Vis. Comput.},
  title        = {Point cloud classification with deep normalized reeb graph convolution},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ReMOT: A model-agnostic refinement for multiple object
tracking. <em>ICV</em>, <em>106</em>, 104091. (<a
href="https://doi.org/10.1016/j.imavis.2020.104091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although refinement is commonly used in visual tasks to improve pre-obtained results, it has not been studied for Multiple Object Tracking (MOT) tasks. This could be attributed to two reasons: i) it has not been explored what kinds of errors should — and could — be reduced in MOT refinement; ii) the refinement target, namely, the tracklets, are intertwined and interactive in a 3D spatio-temporal space, and therefore changing one tracklet may affect the others. To tackle these issues, i) we define two types of errors in imperfect tracklets, as Mix-up Error and Cut-off Error, to clarify the refinement goal; ii) we propose a Refining MOT Framework (ReMOT), which first splits imperfect tracklets and then merges the split tracklets with appearance features improved by self-supervised learning. Experiments demonstrate that ReMOT can make significant improvements to state-of-the-art MOT results as powerful post-processing. As a new application, we demonstrate that ReMOT has the potential of being used to assist semi-automatic MOT data annotation and partially release humans from the tedious work.},
  archive      = {J_ICV},
  author       = {Fan Yang and Xin Chang and Sakriani Sakti and Yang Wu and Satoshi Nakamura},
  doi          = {10.1016/j.imavis.2020.104091},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104091},
  shortjournal = {Image Vis. Comput.},
  title        = {ReMOT: A model-agnostic refinement for multiple object tracking},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework of human action recognition using length control
features fusion and weighted entropy-variances based feature selection.
<em>ICV</em>, <em>106</em>, 104090. (<a
href="https://doi.org/10.1016/j.imavis.2020.104090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we implement an action recognition technique based on features fusion and best feature selection. In the proposed method, HSI color transformation is performed in the first step to improve the contrast of video frames and then extract their motion features by optical flow algorithm. The frames fusion approach extracts the moving regions that find out by optical flow. After that, extract shape and texture features fused by a new parallel approach name length control features. A new Weighted Entropy-Variances approach is applied to a combined vector and selects the best of them for classification. Finally, features are passed in M-SVM for final features classification into relevant human actions. The experimental process is conducted in four famous action datasets- Weizmann, KTH, UCF Sports, and UCF YouTube, with recognition rate 97.9%, 100%, 99.3%, and 94.5%, respectively. Experimental results show that the proposed scheme performed significantly sound output concerning listed methods.},
  archive      = {J_ICV},
  author       = {Farhat Afza and Muhammad Attique Khan and Muhammad Sharif and Seifedine Kadry and Gunasekaran Manogaran and Tanzila Saba and Imran Ashraf and Robertas Damaševičius},
  doi          = {10.1016/j.imavis.2020.104090},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104090},
  shortjournal = {Image Vis. Comput.},
  title        = {A framework of human action recognition using length control features fusion and weighted entropy-variances based feature selection},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Projection-dependent input processing for 3D object
recognition in human robot interaction systems. <em>ICV</em>,
<em>106</em>, 104089. (<a
href="https://doi.org/10.1016/j.imavis.2020.104089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Robot Interaction (HRI) provides assisted services in different real-time applications. The robotic systems identify objects through digital visualization wherein a three-dimensional (3D) image is converged to a plane-based projection. The projection is analyzed using the co-ordinates and identification points for recognizing the object. In such a converging process, the misidentification of projections in different planes results in recognition errors. This article proposes projection-dependent input processing (PDIP) method to reduce the misidentifications in object recognition. In this method, the input is the visualizing image projected in all the possible dimensions to identify the conjoining indices. The conjoined indices without intersection are segregated using labeled analysis. The non-correlating indices are identified in the possible dimension projections to prevent errors. The deviations in planes and indices matching are prevented by correlating the input with similar stored inputs with labels. The proposed method is verified using the metrics recognition ratio (96.4%), time (630.36 ms), complexity (5.93), and error (0.605).},
  archive      = {J_ICV},
  author       = {P.S. Febin Sheron and K.P. Sridhar and S. Baskar and P. Mohamed Shakeel},
  doi          = {10.1016/j.imavis.2020.104089},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104089},
  shortjournal = {Image Vis. Comput.},
  title        = {Projection-dependent input processing for 3D object recognition in human robot interaction systems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Attention-guided aggregation stereo matching network.
<em>ICV</em>, <em>106</em>, 104088. (<a
href="https://doi.org/10.1016/j.imavis.2020.104088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing stereo matching networks based on deep learning lack multi-level and multi-module attention and integration for feature information. Therefore, we propose an attention-guided aggregation stereo matching network to encode and integrate information multiple times. Specifically, we design a residual network based on the 2D channel attention block to adaptively calibrate weight response, improving the robustness of the feature representation. We also construct a 3D stacked hourglass structure based on the 3D channel attention block to calibrate the weight response of the 4D cost volume in the channel dimension, further enhancing the network guidance and aggregation capabilities . In addition, we introduce a 4D guided cost volume, which pre-groups the extracted image features and exploits the similarity measures in each group to guide the concatenation features, further realizing interactive learning of cost volume. The experimental results on the Scene Flow and KITTI benchmark datasets showed that the proposed network significantly improves the prediction disparity accuracy with a small increase in calculation time.},
  archive      = {J_ICV},
  author       = {Yaru Zhang and Yaqian Li and Chao Wu and Bin Liu},
  doi          = {10.1016/j.imavis.2020.104088},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104088},
  shortjournal = {Image Vis. Comput.},
  title        = {Attention-guided aggregation stereo matching network},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel-wise ordinal classification for salient object
grading. <em>ICV</em>, <em>106</em>, 104086. (<a
href="https://doi.org/10.1016/j.imavis.2020.104086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by business intelligence applications for rating attraction of products in shops, a new problem — salient object grading is studied in this paper. In computer vision , plenty of salient object detection approaches have been proposed, while most existing studies detect objects in a binary manner: salient or not. This paper focuses on a new problem setting that requires detecting all salient objects and categorizing them into different salient levels. Based on that, a pixel-wise ordinal classification method is proposed. It consists of a multi-resolution saliency detector which detects and segments objects, an ordinal classifier which grades pixels into different salient levels, and a binary saliency enhancer which sharpens the difference between non-saliency and all other salient levels. Two new image datasets with salient level labels are constructed. Experimental results demonstrate that, on the one hand, the proposed method provides effective salient level predictions and on the other hand, offers very comparable performance with state-of-the-art salient object detection methods in the traditional problem setting.},
  archive      = {J_ICV},
  author       = {Yanzhu Liu and Yanan Wang and Adams Wai Kin Kong},
  doi          = {10.1016/j.imavis.2020.104086},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104086},
  shortjournal = {Image Vis. Comput.},
  title        = {Pixel-wise ordinal classification for salient object grading},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ScPnP: A non-iterative scale compensation solution for PnP
problems. <em>ICV</em>, <em>106</em>, 104085. (<a
href="https://doi.org/10.1016/j.imavis.2020.104085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an accurate non-iterative method for the Perspective-n-Point problem(PnP). Our main idea is to mitigate scale bias by multiplying an independent inverse average depth variable onto the object space error. The introduced variable is of order 2 in the objective function and the optimality conditions constitute a polynomial system with three third-order and one first-order unknowns. Subsequently, we employ the Dixon resultant method to compute explicit expressions of the action matrix, the eigenvalue decomposition of which determines all the roots of the problem. We further extend this scale compensation technology to sphere cameras and contribute a uniform solver to PnP problems for both camera types. Experimental results confirm that our method is reliable and more accurate than state-of-the-art PnP algorithms.},
  archive      = {J_ICV},
  author       = {Chengzhe Meng and Weiwei Xu},
  doi          = {10.1016/j.imavis.2020.104085},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104085},
  shortjournal = {Image Vis. Comput.},
  title        = {ScPnP: A non-iterative scale compensation solution for PnP problems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cuepervision: Self-supervised learning for continuous domain
adaptation without catastrophic forgetting. <em>ICV</em>, <em>106</em>,
104079. (<a href="https://doi.org/10.1016/j.imavis.2020.104079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perception systems, to a large extent, rely on neural networks . Commonly, the training of neural networks uses a finite amount of data. The usual assumption is that an appropriate training dataset is available, which covers all relevant domains. This abstract will follow the example of different lighting conditions in autonomous driving scenarios. In real-world datasets, a single source domain, such as day images, often dominates the dataset composition. This poses a risk to overfit on specific source domain features within the dataset, and implicitly breaches the assumption of full or relevant domain coverage. While applying the model to data outside of the source domain, the performance drops, posing a significant challenge for data-driven methods. A common approach is supervised retraining of the model on additional data. Supervised training requires the laborious acquisition and labeling of an adequate amount of data and often becomes infeasible when data augmentation strategies are not applicable. Furthermore, retraining on additional data often causes a performance drop in the source domain, so-called catastrophic forgetting. In this paper, we present a self-supervised continuous domain adaptation method. A model trained supervised on the source domain (day) is used to generate pseudo labels on the samples of an adjacent target domain (dawn). The pseudo labels and samples enable to fine-tune the existing model, which, as a result, is adapted into the intermediate domain. By iteratively repeating these steps, the model reaches the target domain (night). The results, of the novel method, on the MNIST dataset and its modification, the continuous rotatedMNIST dataset demonstrate a domain adaptation of 86.2%, and a catastrophic forgetting of only 1.6% in the target domain. The work contributes a hyperparameter ablation study, analysis, and discussion of the new learning strategy.},
  archive      = {J_ICV},
  author       = {Mark Schutera and Frank M. Hafner and Jochen Abhau and Veit Hagenmeyer and Ralf Mikut and Markus Reischl},
  doi          = {10.1016/j.imavis.2020.104079},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104079},
  shortjournal = {Image Vis. Comput.},
  title        = {Cuepervision: Self-supervised learning for continuous domain adaptation without catastrophic forgetting},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive review on deep learning-based methods for
video anomaly detection. <em>ICV</em>, <em>106</em>, 104078. (<a
href="https://doi.org/10.1016/j.imavis.2020.104078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video surveillance systems are popular and used in public places such as market places, shopping malls, hospitals, banks, streets, education institutions, city administrative offices, and smart cities to enhance the safety of public lives and assets. Most of the time, the timely and accurate detection of video anomalies is the main objective of security applications. The video anomalies such as anomalous activities and anomalous entities are defined as the abnormal or irregular patterns present in the video that do not conform to the normal trained patterns. Anomalous activities such as fighting, riots, traffic rule violations, and stampede as well as anomalous entities such as weapons at the sensitive place and abandoned luggage should be detected automatically in time. However, the detection of video anomalies is challenging due to the ambiguous nature of the anomaly, various environmental conditions, the complex nature of human behaviors, and the lack of proper datasets. There are only a few dedicated surveys related to deep learning-based video anomaly detection as the research domain is in its early stages. However, state of the art lacks a review that provides a comprehensive study covering all the aspects such as definitions, classifications, modelings, performance evaluation methodologies, open and trending research challenges of video anomaly detection. Hence, in this survey, we present a comprehensive study of the deep learning-based methods reported in state of the art to detect the video anomalies. Further, we discuss the comparative analysis of the state of the art methods in terms of datasets, computational infrastructure, and performance metrics for both quantitative and qualitative analyses. Finally, we outline the challenges and promising directions for further research.},
  archive      = {J_ICV},
  author       = {Rashmiranjan Nayak and Umesh Chandra Pati and Santos Kumar Das},
  doi          = {10.1016/j.imavis.2020.104078},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104078},
  shortjournal = {Image Vis. Comput.},
  title        = {A comprehensive review on deep learning-based methods for video anomaly detection},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video-based person re-identification by intra-frame and
inter-frame graph neural network. <em>ICV</em>, <em>106</em>, 104068.
(<a href="https://doi.org/10.1016/j.imavis.2020.104068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, video-based person re-identification (Re-ID) have attracted growing research attention. The crucial problem for this task is how to learn robust video feature representation, which can weaken the influence of factors such as occlusion, illumination, and background etc. A great deal of previous works utilize spatio-temporal information to represent pedestrian video, but the correlations between parts of human body are ignored. In order to take advantage of the relationship among different parts, we propose a novel Intra-frame and Inter-frame Graph Neural Network (I2GNN) to solve the video-based person Re-ID task. Specifically, (1) the features from each part are treated as graph nodes from each frame; (2) the intra-frame edges are established by the correlation between different parts; (3) the inter-frame edges are constructed between the same parts across adjacent frames. I2GNN learns video representations by employing the adjacent matrix of the graph and input features to conduct graph convolution , and then adopts projection metric learning on Grassman manifold to measure the similarities between learned pedestrian features. Moreover, this paper proposes a novel occlusion-invariant term to make the part features close to their center, which can relive several uncontrolled complicated factors, such as occlusion and pose invariance. Besides, we have carried out extensive experiments on four widely used datasets: MARS, DukeMTMC-VideoReID, PRID2011, and iLIDS-VID. The experimental results demonstrate that our proposed I2GNN model is more competitive than other state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Guiqing Liu and Jinzhao Wu},
  doi          = {10.1016/j.imavis.2020.104068},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104068},
  shortjournal = {Image Vis. Comput.},
  title        = {Video-based person re-identification by intra-frame and inter-frame graph neural network},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved generative adversarial network and its application
in image oil painting style transfer. <em>ICV</em>, <em>105</em>,
104087. (<a href="https://doi.org/10.1016/j.imavis.2020.104087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the difficulty in training the algorithm of image oil painting style migration and reconstruction based on the generative adversarial network, and the loss gradient of generator and discriminator disappears, this paper proposes an improved generative adversarial network based on gradient penalty, and constructs the total variance loss function to carry out the research of image oil painting style migration and reconstruction. Firstly, the Wasserstein distance (WGAN) is added to the loss function of the generative adversarial network to improve the stability of the alternative iterative training; secondly, the gradient penalty (WGAN-GP) is added to the loss function to deal with the problem of gradient disappearance in the training; finally, the LBP texture feature and total variation of the prototype are introduced based on the CycleGAN Loss noise constraint is used to improve the edge and texture strength of the image after migration of oil painting style. The experimental results show that the WGAN-GP algorithm constructed in this study has the ability of stable gradient and alternating iterative convergence, and the total variation loss noise constraint can provide good edge and texture details for the migration process of image oil painting style. Compared with the existing mainstream algorithm, the algorithm proposed in this study has better performance of image oil painting style migration and reconstruction, and better effect of image oil painting style migration and reconstruction.},
  archive      = {J_ICV},
  author       = {Yuan Liu},
  doi          = {10.1016/j.imavis.2020.104087},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104087},
  shortjournal = {Image Vis. Comput.},
  title        = {Improved generative adversarial network and its application in image oil painting style transfer},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd density detection method based on crowd gathering mode
and multi-column convolutional neural network. <em>ICV</em>,
<em>105</em>, 104084. (<a
href="https://doi.org/10.1016/j.imavis.2020.104084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowds and stampedes often occur in crowd gathering places, resulting in a large number of casualties and causing great negative social impacts. Traditional research on the dynamic assessment of crowd gathering safety mainly relies on real-time video monitoring, but lacks reliable methods for processing a large amount of video data from different sources, different perspectives and different granularities . Based on Edward Hall&#39;s personal space theory, this article considers crowd psychology and other factors, and establishes static basic model of crowd gathering patterns. In order to fuse real-time multi-granularity surveillance videos with different perspectives, a multi-column convolutional neural network (M-CNN) was used to extract the local density characteristics of the crowd in a low-altitude perspective, thereby establishing a holographic model of the temporal and spatial evolution of the crowd situation, and a new crowd gathering safety assessment method. This method was actually applied to the safety assessment of crowd gathering in Suzhou landmark-Urban Living Fountain Square, and achieved good results, providing theoretical support for the safety management of crowd gathering places.},
  archive      = {J_ICV},
  author       = {Liu Bai and Cheng Wu and Feng Xie and Yiming Wang},
  doi          = {10.1016/j.imavis.2020.104084},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104084},
  shortjournal = {Image Vis. Comput.},
  title        = {Crowd density detection method based on crowd gathering mode and multi-column convolutional neural network},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bias alleviating generative adversarial network for
generalized zero-shot classification. <em>ICV</em>, <em>105</em>,
104077. (<a href="https://doi.org/10.1016/j.imavis.2020.104077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot classification is predicting the labels of the test images coming from seen or unseen classes. The task is difficult because of the bias problem, that is, unseen samples are easily to be misclassified to seen classes. Many methods have handled the problem by training a generative adversarial network (GAN) to generate fake samples. However, the GAN model trained with seen samples might not be appropriate for generating unseen samples. For dealing with this problem, we learn a bias alleviating generative adversarial network for generalized zero-shot classification by generating seen and unseen samples, simultaneously. We train the generator to generate more realistic unseen samples by adding semantic similarity and cluster center regularizations to alleviate the bias problem. The semantic similarity regularization is to restrict the relationships of the generated unseen visual prototypes and seen visual prototypes by their class prototypes to avoid the generated unseen samples similar to the seen samples. The cluster center regularization is to utilize the cluster property of target data to make the generated unseen visual prototypes near to the most similar cluster centers, generating realistic unseen samples. From the experiments, we can see the proposed method achieves promising results.},
  archive      = {J_ICV},
  author       = {Xiao Li and Min Fang and Haikun Li},
  doi          = {10.1016/j.imavis.2020.104077},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104077},
  shortjournal = {Image Vis. Comput.},
  title        = {Bias alleviating generative adversarial network for generalized zero-shot classification},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Industrial visual perception technology in smart city.
<em>ICV</em>, <em>105</em>, 104070. (<a
href="https://doi.org/10.1016/j.imavis.2020.104070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to study the application effect and function of industrial visual perception technology in smart city, the image processing and quality evaluation system was constructed by using convolutional neural network (CNN) and Internet of things (IoT) technology. The system was simulated, and then the quality performance of image and video obtained by using industrial visual perception technology was processed and analyzed. The results show that in the analysis of image local optimization effect, it is found that the classification performance of all algorithms decreases with the increase of noise, and the performance of local anisotropic mode (LAP) is superior, which has strong robustness to rotation, illumination, and noise. In the analysis of image feature similarity effect, it is found that the chi square distance between Log Gabor features is positively correlated with the degree of image distortion, and the validity of the measurement method is verified. Further analysis of the video processing effect of industrial visual perception technology shows that the video processing effect of test algorithm is significantly better than that of HM16.8 by comparing the distortion performance of the two algorithms with different sequences, with low distortion and significantly improved performance. Therefore, through the research, it is found that the improved CNN algorithm is superior to other algorithms in image and video processing.},
  archive      = {J_ICV},
  author       = {Zhihan Lv and Dongliang Chen},
  doi          = {10.1016/j.imavis.2020.104070},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104070},
  shortjournal = {Image Vis. Comput.},
  title        = {Industrial visual perception technology in smart city},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient pedestrian detection in top-view fisheye images
using compositions of perspective view patches. <em>ICV</em>,
<em>105</em>, 104069. (<a
href="https://doi.org/10.1016/j.imavis.2020.104069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection in images is a topic that has been studied extensively, but existing detectors designed for perspective images do not perform as successfully on images taken with top-view fisheye cameras, mainly due to the orientation variation of people in such images. In our proposed approach, several perspective views are generated from a fisheye image and then concatenated to form a composite image. As pedestrians in this composite image are more likely to be upright, existing detectors designed and trained for perspective images can be applied directly without additional training. We also describe a new method of mapping detection bounding boxes from the perspective views to the fisheye frame. The detection performance on several public datasets compare favorably with state-of-the-art results.},
  archive      = {J_ICV},
  author       = {Sheng-Ho Chiang and Tsaipei Wang and Yi-Fu Chen},
  doi          = {10.1016/j.imavis.2020.104069},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104069},
  shortjournal = {Image Vis. Comput.},
  title        = {Efficient pedestrian detection in top-view fisheye images using compositions of perspective view patches},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). I-SOCIAL-DB: A labeled database of images collected from
websites and social media for iris recognition. <em>ICV</em>,
<em>105</em>, 104058. (<a
href="https://doi.org/10.1016/j.imavis.2020.104058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People upload daily a huge number of portrait face pictures on websites and social media, which can be processed using biometric systems based on the face characteristics to perform an automatic recognition of the individuals. However, the performance of face recognition approaches can be limited by negative factors as aging, occlusions, rotations, and uncontrolled expressions. Nevertheless, the constantly increasing quality and resolution of the portrait pictures uploaded on websites and social media could permit to overcome these problems and improve the robustness of biometric recognition methods by enabling the analysis of additional traits, like the iris. To point the attention of the research community to the possible use of iris-based recognition techniques for images uploaded on websites and social media, we present a public image dataset called I-SOCIAL-DB (Iris Social Database). This dataset is composed of 3,286 ocular regions, extracted from 1,643 high-resolution face images of 400 individuals, collected from public websites. For each ocular region, a human expert extracted the coordinates of the circles approximating the inner and outer iris boundaries and performed a pixelwise segmentation of the iris contours, occlusions, and reflections. This dataset is the first collection of ocular images from public websites and social media, and one of the biggest collections of manually segmented ocular images in the literature. In this paper, we also present a qualitative analysis of the samples, a set of testing protocols and figures of merit, and benchmark results achieved using publicly available iris segmentation and recognition algorithms . We hope that this initiative can give a new test tool to the biometric research community, aiming to stimulate new studies in this challenging research field.},
  archive      = {J_ICV},
  author       = {R. Donida Labati and Angelo Genovese and Vincenzo Piuri and Fabio Scotti and Sarvesh Vishwakarma},
  doi          = {10.1016/j.imavis.2020.104058},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104058},
  shortjournal = {Image Vis. Comput.},
  title        = {I-SOCIAL-DB: A labeled database of images collected from websites and social media for iris recognition},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-database and cross-attack iris presentation attack
detection using micro stripes analyses. <em>ICV</em>, <em>105</em>,
104057. (<a href="https://doi.org/10.1016/j.imavis.2020.104057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread use of mobile devices, iris recognition systems encounter more challenges, such as the vulnerability of Presentation Attack Detection (PAD). Recent works pointed out the contact lens attacks, especially images captured under the uncontrolled environment, as a hard task for iris PAD. In this paper, we propose a novel framework for detecting iris presentation attacks that especially for detecting contact lenses based on the normalized multiple micro stripes. The classification decision is made by the majority vote of those micro-stripes. An in-depth experimental evaluation of this framework reveals a superior performance in three databases compared with state-of-the-art (SoTA) algorithms and baselines. Moreover, our solution minimizes the confusion between textured (attack) and transparent (bona fide) presentations in comparison to SoTA methods . We support the rationalization of our proposed method by studying the significance of different pupil-centered eye areas in iris PAD decisions under different experimental settings. In addition, extensive cross-database and cross-attack (unknown attack) detection evaluation experiments are demonstrated to explore the generalizability of our proposed method, texture-based method, and neural network based methods in three different databases. The results indicate that our Micro Stripes Analyses (MSA) method has, in most experiments, better generalizability compared to other baselines.},
  archive      = {J_ICV},
  author       = {Meiling Fang and Naser Damer and Fadi Boutros and Florian Kirchbuchner and Arjan Kuijper},
  doi          = {10.1016/j.imavis.2020.104057},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104057},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-database and cross-attack iris presentation attack detection using micro stripes analyses},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal image fusion based on point-wise mutual
information. <em>ICV</em>, <em>105</em>, 104047. (<a
href="https://doi.org/10.1016/j.imavis.2020.104047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal image fusion aims to generate a fused image from different signals that captured by multimodal sensors. Although the images obtained by multimodal sensors have different appearances, the information included in these images might be redundant and noisy. In the previous studies, the fusion rule and their properties that guiding how to merge the features from multiple images is relatively simple functions such as choose-max or weighted average. However, merging the features with redundant information based on these fusion rules may lead to brightness distortion or extra noises, since these fusion rules ignore the spatial consistency of feature selections. In this paper, we propose a novel multimodal image fusion algorithm that focuses on both the transferring the salient structures and maintaining spatial consistency that we consider as fundamental to build our proposed architecture. The proposed algorithm selects features to be transferred into fusion results by a graph cut algorithm, in which the spatial varying smoothness cost is formulated based on the independence between local features measured by point-wise mutual information (PMI). Experiment results demonstrate that, with the straightforward gradient features, the proposed method can obtain state-of-the-art performance on several publicly available multimodal image databases.},
  archive      = {J_ICV},
  author       = {Donghao Shen and Masoumeh Zareapoor and Jie Yang},
  doi          = {10.1016/j.imavis.2020.104047},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104047},
  shortjournal = {Image Vis. Comput.},
  title        = {Multimodal image fusion based on point-wise mutual information},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of micro-expression recognition. <em>ICV</em>,
<em>105</em>, 104043. (<a
href="https://doi.org/10.1016/j.imavis.2020.104043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited capacity to recognize micro-expressions with subtle and rapid motion changes is a long-standing problem that presents a unique challenge for expression recognition systems and even for humans. The problem regarding micro-expression is less covered by research when compared to macro-expression. Nevertheless, micro-expression recognition (MER) is imperative to exploit the full potential of expression recognition for real-world applications. Recent MER systems generally focus on three important issues: overfitting caused by a lack of sufficient training data , the imbalanced distribution of samples, and robust features for improving the accuracy of recognition. In this paper, we provide a comprehensive survey on MER, including datasets and algorithms that provide insights into these intrinsic problems. First, we introduce the available datasets that are widely used in the literature. We then describe the pre-processing in the standard pipeline of an MER system. For the state of the art in MER, we divide the existing novel algorithms into 6 different tasks according to the type of classes and evaluation protocols. Detailed experiment settings and competitive performances for those 6 tasks are summarized in this section. Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust MER systems.},
  archive      = {J_ICV},
  author       = {Ling Zhou and Xiuyan Shao and Qirong Mao},
  doi          = {10.1016/j.imavis.2020.104043},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104043},
  shortjournal = {Image Vis. Comput.},
  title        = {A survey of micro-expression recognition},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multimodal fusion for semantic image segmentation: A
survey. <em>ICV</em>, <em>105</em>, 104042. (<a
href="https://doi.org/10.1016/j.imavis.2020.104042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning have shown excellent performance in various scene understanding tasks. However, in some complex environments or under challenging conditions , it is necessary to employ multiple modalities that provide complementary information on the same scene. A variety of studies have demonstrated that deep multimodal fusion for semantic image segmentation achieves significant performance improvement . These fusion approaches take the benefits of multiple information sources and generate an optimal joint prediction automatically. This paper describes the essential background concepts of deep multimodal fusion and the relevant applications in computer vision . In particular, we provide a systematic survey of multimodal fusion methodologies, multimodal segmentation datasets, and quantitative evaluations on the benchmark datasets. Existing fusion methods are summarized according to a common taxonomy: early fusion, late fusion, and hybrid fusion. Based on their performance, we analyze the strengths and weaknesses of different fusion strategies. Current challenges and design choices are discussed, aiming to provide the reader with a comprehensive and heuristic view of deep multimodal image segmentation .},
  archive      = {J_ICV},
  author       = {Yifei Zhang and Désiré Sidibé and Olivier Morel and Fabrice Mériaudeau},
  doi          = {10.1016/j.imavis.2020.104042},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104042},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep multimodal fusion for semantic image segmentation: A survey},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Expression recognition with deep features extracted from
holistic and part-based models. <em>ICV</em>, <em>105</em>, 104038. (<a
href="https://doi.org/10.1016/j.imavis.2020.104038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition aims to accurately interpret facial muscle movements in affective states (emotions). Previous studies have proposed holistic analysis of the face, as well as the extraction of features pertained only to specific facial regions towards expression recognition. While classically the latter have shown better performances, we here explore this in the context of deep learning . In particular, this work provides a performance comparison of holistic and part-based deep learning models for expression recognition. In addition, we showcase the effectiveness of skip connections, which allow a network to infer from both low and high-level feature maps. Our results suggest that holistic models outperform part-based models, in the absence of skip connections. Finally, based on our findings, we propose a data augmentation scheme, which we incorporate in a part-based model. The proposed multi-face multi-part (MFMP) model leverages the wide information from part-based data augmentation, where we train the network using the facial parts extracted from different face samples of the same expression class. Extensive experiments on publicly available datasets show a significant improvement of facial expression classification with the proposed MFMP framework.},
  archive      = {J_ICV},
  author       = {S.L. Happy and Antitza Dantcheva and François Bremond},
  doi          = {10.1016/j.imavis.2020.104038},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104038},
  shortjournal = {Image Vis. Comput.},
  title        = {Expression recognition with deep features extracted from holistic and part-based models},
  volume       = {105},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
