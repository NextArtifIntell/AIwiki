<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dke---33">DKE - 33</h2>
<ul>
<li><details>
<summary>
(2021). Evolution management in multi-model databases. <em>DKE</em>,
<em>136</em>, 101932. (<a
href="https://doi.org/10.1016/j.datak.2021.101932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the Gartner predictions, most of the DBMSs , both traditional relational and NoSQL, have become multi-model. However, this functionality brought on plenty of related issues. One of the most complex ones is evolution management and respective propagation of changes to all affected parts of the system. In this paper we introduce our prototype implementation called MM-evolver which enables to carry out user-triggered schema modification operations over a multi-model database, and propagates them across all models. As a novelty, MM-evolver supports both inter- and intra-model schema modification operators. To the best of our knowledge, ours is the first tool addressing evolution management in the world of multi-model databases.},
  archive      = {J_DKE},
  author       = {Irena Holubová and Michal Vavrek and Stefanie Scherzinger},
  doi          = {10.1016/j.datak.2021.101932},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101932},
  shortjournal = {Data Knowl. Eng.},
  title        = {Evolution management in multi-model databases},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling metadata in data lakes—a generic model.
<em>DKE</em>, <em>136</em>, 101931. (<a
href="https://doi.org/10.1016/j.datak.2021.101931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data contains important knowledge and has the potential to provide new insights. Due to new technological developments such as the Internet of Things , data is generated in increasing volumes. In order to deal with these data volumes and extract the data’s value new concepts such as the data lake were created. The data lake is a data management platform designed to handle data at scale for analytical purposes. To prevent a data lake from becoming inoperable and turning into a data swamp, metadata management is needed. To store and handle metadata, a generic metadata model is required that can reflect metadata of any potential metadata management use case, e.g., data versioning or data lineage . However, an evaluation of existent metadata models yields that none so far are sufficiently generic as their design basis is not suited. In this work, we use a different design approach to build HANDLE, a generic metadata model for data lakes. The new metadata model supports the acquisition of metadata on varying granular levels , any metadata categorization, including the acquisition of both metadata that belongs to a specific data element as well as metadata that applies to a broader range of data. HANDLE supports the flexible integration of metadata and can reflect the same metadata in various ways according to the intended utilization. Furthermore, it is created for data lakes and therefore also supports data lake characteristics like data lake zones. With these capabilities HANDLE enables comprehensive metadata management in data lakes. HANDLE’s feasibility is shown through the application to an exemplary access-use-case and a prototypical implementation. By comparing HANDLE with existing models we demonstrate that it can provide the same information as the other models as well as adding further capabilities needed for metadata management in data lakes.},
  archive      = {J_DKE},
  author       = {Rebecca Eichler and Corinna Giebler and Christoph Gröger and Holger Schwarz and Bernhard Mitschang},
  doi          = {10.1016/j.datak.2021.101931},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101931},
  shortjournal = {Data Knowl. Eng.},
  title        = {Modeling metadata in data lakes—A generic model},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient machine learning on data science languages with
parallel data summarization. <em>DKE</em>, <em>136</em>, 101930. (<a
href="https://doi.org/10.1016/j.datak.2021.101930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, data science analysts prefer “easy” high-level languages for machine learning computation like R and Python, but they present memory and speed limitations. Also, scalability is another issue when the data set size grows. On the other hand, acceleration of machine learning algorithms can be achieved with data summarization which has been a fundamental technique in data mining. With these motivations in mind, we present an efficient way to compute the statistical and machine learning models with parallel data summarization that can work with popular data science languages. Our summarization produces one or multiple summaries, accelerates a broader class of statistical and machine learning models, and requires a small amount of RAM . We present an algorithm that works in three phases and is capable to handle data sets bigger than the main memory. Our solution evaluates a vector–vector outer product with C++ code to escape the bottleneck of the high-level programming languages. We present an experimental evaluation section with a prototype in the R language where the summarization is programmed in C++. Our experiments prove that our solution can work on both data subsets and full data set without any performance penalty. Also, we compare our solution (R combined with C++) with other parallel big data systems : Spark (Spark-MLlib library), and a parallel DBMS (similar approach implemented with UDFs and SQL queries). We show our solution is simpler and mostly faster than Spark based on the storage of the data set, and it is much faster than a parallel DBMS regardless of the storage of the data set.},
  archive      = {J_DKE},
  author       = {Sikder Tahsin Al-Amin and Carlos Ordonez},
  doi          = {10.1016/j.datak.2021.101930},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101930},
  shortjournal = {Data Knowl. Eng.},
  title        = {Efficient machine learning on data science languages with parallel data summarization},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-adaptation in smartphone applications: Current
state-of-the-art techniques, challenges, and future directions.
<em>DKE</em>, <em>136</em>, 101929. (<a
href="https://doi.org/10.1016/j.datak.2021.101929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, smartphones have become an essential part of our lives. Several mobile applications have been uploaded to their respective app stores on daily basis to facilitate the end-users. To ensure the availability of the application and its services, the application needs to be able to detect and deal with various types of changes. Mobile application based self-adaptation is regarded as a solution to handle the above-mentioned chnages. The objectives of this study are: (i) identification of current state-of-the-art techniques, frameworks, models, and algorithms for self-adaptation in mobile applications context, (ii) exploring the core challenges faced in the development of self-adaptive mobile applications, and (iii) conducting a SWOT (Strength Weakness Opportunity Threat) analysis to figure out the research gaps in the targeted research field. To accomplish the mentioned objective, we conducted a systematic literature review to enlighten the need for adaptation in this domain, the targeted areas, their beneficial aspects, state-of-the-art proposed solutions, and their challenges. We performed the keyword-based search on 11 well-known databases to determine the potential studies published between years 2015 to 2020. In total, 31 studies (from 933 potential studies) are selected grounded on the defined inclusion and exclusion criteria. The main findings of this work are: (i) MAPE-K (Monitor Analyze Plan Execute-Knowledge) model is frequently used in several studies due to its resilience, (ii) current work focuses on the android operating system due to its popularity and flexibility; however, it lacks in considering other mobile’s operating systems, and (iii) reported work also lacks in mentioning any benchmark (self-adaptive framework). The current study is helpful for the researchers intended to work in the smartphones domain. From developers viewpoint, this study reports the faced challenges during the development of smartphones applications . Moreover, the performed study is beneficial in filling the identified research gaps by providing a foundation useful to plan future research regarding self-adaptation in smartphone applications context.},
  archive      = {J_DKE},
  author       = {Mughees Ali and Saif Ur Rehman Khan and Shahid Hussain},
  doi          = {10.1016/j.datak.2021.101929},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101929},
  shortjournal = {Data Knowl. Eng.},
  title        = {Self-adaptation in smartphone applications: Current state-of-the-art techniques, challenges, and future directions},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining closed high utility itemsets based on propositional
satisfiability. <em>DKE</em>, <em>136</em>, 101927. (<a
href="https://doi.org/10.1016/j.datak.2021.101927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A high utility itemset mining problem is the question of recognizing a set of items that have utility values greater than a given user utility threshold. This generalization of the classical problem of frequent itemset mining is a useful and well-known task in data analysis and data mining, since it is used in a wide range of real applications. In this paper, we first propose to use symbolic Artificial Intelligence for computing the set of all closed high utility itemsets from transaction databases. Our approach is based on reduction to enumeration problems of propositional satisfiability. Then, we enhance the efficiency of our SAT-based approach using the weighted clique cover problem. After that, in order to improve scalability, a decomposition technique is applied to derive smaller and independent sub-problems in order to capture all the closed high utility itemsets. Clearly, our SAT-based encoding can be constantly enhanced by integrating the last improvements in powerful SAT solvers and models enumeration algorithms. Finally, through empirical evaluations on different real-world datasets, we demonstrate that the proposed approach is very competitive with state-of-the-art specialized algorithms for high utility itemsets mining, while being sufficiently flexible to take into account additional constraints to finding closed high utility itemsets.},
  archive      = {J_DKE},
  author       = {Amel Hidouri and Said Jabbour and Badran Raddaoui and Boutheina Ben Yaghlane},
  doi          = {10.1016/j.datak.2021.101927},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101927},
  shortjournal = {Data Knowl. Eng.},
  title        = {Mining closed high utility itemsets based on propositional satisfiability},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the research landscape of data warehousing and
mining based on DaWaK conference full-text articles. <em>DKE</em>,
<em>135</em>, 101926. (<a
href="https://doi.org/10.1016/j.datak.2021.101926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The international conference on Data Warehousing and Knowledge Discovery (DaWaK) has become a pivotal place to exchange experiences and knowledge among researchers and practitioners in big data analytics . The conference has been essential to data warehousing and data analytics for the last 21 years (1999–2019). This study explored the knowledge structure embedded in the DaWaK Conference papers and examined the research trends over time. It also analyzed the performance of published papers, authors, and their affiliations and countries and visualized a collaboration network in DaWaK. We applied several text mining techniques, including co-word analysis, topic modeling, co-author network analysis , and network visualization. The study’s findings indicate that the core topics are data mining techniques , algorithm performance, and information systems. The popular topic trends are associated with database encryption , whereas the topics related to online analytical processing (OLAP) technology are in decline. The research metrics results demonstrate that the DaWaK papers were cited 6,262 times, with an h -index of 34 for the 722 DaWaK papers. The article titled “Outlier Detection Using Replicator Neural Networks” reached the most citations (177), and the most productive author was Bellatreche, Ladjel (15 papers). Nanyang Technological University is the most frequently mentioned as the author’s affiliation, the United States is the country with the largest number of authors, and the National Science Foundation was the largest funding agency that supported the DaWaK researchers. Moreover, the authorship network of Bellatreche, Ladjel is the largest collaboration network in the DaWaK scholar community. The outcomes of this study would be beneficial for comprehending the knowledge in data warehousing and the relevant cross-disciplinary areas of research and collaboration networks in this field.},
  archive      = {J_DKE},
  author       = {Tatsawan Timakum and Soobin Lee and Min Song},
  doi          = {10.1016/j.datak.2021.101926},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101926},
  shortjournal = {Data Knowl. Eng.},
  title        = {Exploring the research landscape of data warehousing and mining based on DaWaK conference full-text articles},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preface. <em>DKE</em>, <em>135</em>, 101925. (<a
href="https://doi.org/10.1016/j.datak.2021.101925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Il-Yeol Song ( Guest Editor )},
  doi          = {10.1016/j.datak.2021.101925},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101925},
  shortjournal = {Data Knowl. Eng.},
  title        = {Preface},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining high utility patterns in interval-based event
sequences. <em>DKE</em>, <em>135</em>, 101924. (<a
href="https://doi.org/10.1016/j.datak.2021.101924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential pattern mining is an interesting research area with broad range of applications. Most prior research on sequential pattern mining has considered point-based data where events occur instantaneously. However, in many application domains, events persist over intervals of time of varying lengths. Furthermore, traditional frameworks for sequential pattern mining assume all events have the same weight or utility. This simplifying assumption neglects the opportunity to find informative patterns in terms of utilities, such as profits. To address these issues, we incorporate the concept of utility into interval-based sequences and define a framework to mine high utility patterns in interval-based sequences i.e., patterns whose utility meets or exceeds a minimum threshold. In the proposed framework, the utility of events is considered while assuming multiple events can occur coincidentally and persist over varying periods of time. An algorithm named High Utility Interval-based Pattern Miner ( HUIPMiner ) is proposed and applied to real datasets. To achieve an efficient solution, HUIPMiner is augmented with two effective pruning strategies. Experimental results show that HUIPMiner is an effective solution to the problem of mining high utility interval-based sequences. Moreover, it is shown that the execution time of the algorithm is reduced when the proposed pruning strategies are applied.},
  archive      = {J_DKE},
  author       = {S. Mohammad Mirbagheri and Howard J. Hamilton},
  doi          = {10.1016/j.datak.2021.101924},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101924},
  shortjournal = {Data Knowl. Eng.},
  title        = {Mining high utility patterns in interval-based event sequences},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge extraction using semantic similarity of concepts
from web of things knowledge bases. <em>DKE</em>, <em>135</em>, 101923.
(<a href="https://doi.org/10.1016/j.datak.2021.101923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is one of the rapidly growing technologies with the aim of establishing communication among objects, people, and processes. This rapidly growing technology faces a lot of challenges that hinder its wider adoption, specifically in developing applications that involve heterogeneous domains. Currently, developing such interoperable applications require substantial efforts by the developers to hard code the requirements to ensure the correctness of transferring knowledge. The efforts can be significantly reduced by developing an interoperable platform that ensures seamless communication between heterogeneous IoT devices. W3C Web of Things (WoT) is a significant step towards enabling interoperability between IoT devices by integrating the existing Web ecosystem with “Things”. WoT provides a unified interface over a suitable network protocol facilitating interactions between different IoT protocols. WoT Thing Descriptions (TD) enrich interoperability providing both human and machine readable metadata about a Thing. However, the WoT still falls short in providing semantic interoperability due to insufficient standard vocabularies which can describe different IoT application domains. In this paper, we propose a semantic similarity-based approach to automatically identify and extract the most common concepts from sixteen popular ontologies belonging to smart home and smart building domains. The proposed method helps the developers and researchers to develop a domain ontology with reduced effort. The extracted concepts are evaluated by the domain experts and are found to be sufficient in describing the smart home and smart building domains.},
  archive      = {J_DKE},
  author       = {Vamsee Muppavarapu and Gowtham Ramesh and Amelie Gyrard and Mahda Noura},
  doi          = {10.1016/j.datak.2021.101923},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101923},
  shortjournal = {Data Knowl. Eng.},
  title        = {Knowledge extraction using semantic similarity of concepts from web of things knowledge bases},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DBHC: A DBSCAN-based hierarchical clustering algorithm.
<em>DKE</em>, <em>135</em>, 101922. (<a
href="https://doi.org/10.1016/j.datak.2021.101922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is the process of partitioning objects of a dataset into some groups according to similarities and dissimilarities between its objects. DBSCAN is one of the most important clustering algorithms in the density based approach of clustering. In spite of the numerous advantages of the DBSCAN algorithm, it has two important input parameters, MinPts and Eps, which determining their values is still a great challenge. This problem arises because values of these parameters are heavily dependent on data distribution. To overcome this challenge, firstly features of these parameters are investigated and the data distribution are analyzed. Then a DBSCAN-based hierarchical clustering (DBHC) method is proposed in this paper in order to fix this challenge. For this purpose, DBHC first determines values of these parameters using the notion of k nearest neighbor and k-dist plot. Because most of the real world data is not distributed uniformly, it is needed to be produced several values for the Eps parameter. Then, DBHC executes the DBSCAN algorithm several times based on the number of Eps produced earlier. Finally, DBHC method merges obtained clusters if the number of produced clusters is larger than the number which has estimated by the user. To evaluate the performance of the DBHC method, several experiments were performed on some of benchmark datasets of UCI database. Obtained results were compared with other previous works. The obtained results consistently showed that the DBHC method led to better results in comparison to the other works.},
  archive      = {J_DKE},
  author       = {Alireza Latifi-Pakdehi and Negin Daneshpour},
  doi          = {10.1016/j.datak.2021.101922},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101922},
  shortjournal = {Data Knowl. Eng.},
  title        = {DBHC: A DBSCAN-based hierarchical clustering algorithm},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TopicBank: Collection of coherent topics using multiple
model training with their further use for topic model validation.
<em>DKE</em>, <em>135</em>, 101921. (<a
href="https://doi.org/10.1016/j.datak.2021.101921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic topic modeling of a text collection is a tool for unsupervised learning of the inherent thematic structure of the collection. Given only the text of documents as input, the topic model aims to reveal latent topics as probability distributions over words. The shortcomings of topic models are that they are unstable in the sense that topics may depend on the random initialization, and incomplete in the sense that each new run of the model on the same collection may discover some new topics. This means that data exploration using topic modeling usually requires too many experiments for looking over many topic models and tuning their parameters in search of a model that describes the data best. To deal with the instability and incompleteness of topic models, we propose to gradually accumulate interpretable topics in a “topic bank” using multiple model training. To add topics into the bank, we learn a child level in a hierarchical topic model, then we analyze the coherence of child subtopics and their relationships with parent bank topics in order to exclude irrelevant and duplicate subtopics instead of adding them to the bank. Then we introduce a new way to topic model evaluation by comparing the topics found by the model with the ones that were collected beforehand in a bank. Our experiments with several datasets and topic models show that the proposed method does help in finding a model with more interpretable topics.},
  archive      = {J_DKE},
  author       = {Vasiliy Alekseev and Evgeny Egorov and Konstantin Vorontsov and Alexey Goncharov and Kaidar Nurumov and Timur Buldybayev},
  doi          = {10.1016/j.datak.2021.101921},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101921},
  shortjournal = {Data Knowl. Eng.},
  title        = {TopicBank: Collection of coherent topics using multiple model training with their further use for topic model validation},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel domain and event adaptive tweet augmentation
approach for enhancing the classification of crisis related tweets.
<em>DKE</em>, <em>135</em>, 101913. (<a
href="https://doi.org/10.1016/j.datak.2021.101913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the purposes of detecting the crisis related tweets is the ability to single out the tweets that provide information about the helps needed and offered. Classification of such tweets is difficult because of the unavailability of sufficient annotated tweets in those categories. To facilitate such classifications, a domain and event adaptive augmentation approach is proposed. The main objective of the research is to enhance the classification of crisis related tweets that have less training samples. The proposed algorithms are designed to integrate the innate domain and event information during the selection of words for augmentation. Components such as CrisisLex lexicon, Word2Vec embeddings and WordNet are utilized for the proposed augmentation. Experimentation is carried out to substantiate the benefits of augmentation. Results indicate increased performance of the classifier when provided with the expanded dataset including the augmented and original tweets. To combat the problem of overfitting and class imbalance arising due to the lesser training samples, a novel tweets augmentation algorithm can be utilized. The advantage in the proposed algorithms is the ability to retain the structure and inherent nature of the tweets during the augmentation.},
  archive      = {J_DKE},
  author       = {Dharini Ramachandran and Parvathi R.},
  doi          = {10.1016/j.datak.2021.101913},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101913},
  shortjournal = {Data Knowl. Eng.},
  title        = {A novel domain and event adaptive tweet augmentation approach for enhancing the classification of crisis related tweets},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning in the COVID-19 epidemic: A deep model for
urban traffic revitalization index. <em>DKE</em>, <em>135</em>, 101912.
(<a href="https://doi.org/10.1016/j.datak.2021.101912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research of traffic revitalization index can provide support for the formulation and adjustment of policies related to urban management, epidemic prevention and resumption of work and production. This paper proposes a deep model for the prediction of urban Traffic Revitalization Index (DeepTRI). The DeepTRI builds model for the data of COVID-19 epidemic and traffic revitalization index for major cities in China. The location information of 29 cities forms the topological structure of graph. The Spatial Convolution Layer proposed in this paper captures the spatial correlation features of the graph structure. The special Graph Data Fusion module distributes and fuses the two kinds of data according to different proportions to increase the trend of spatial correlation of the data. In order to reduce the complexity of the computational process, the Temporal Convolution Layer replaces the gated recursive mechanism of the traditional recurrent neural network with a multi-level residual structure. It uses the dilated convolution whose dilation factor changes according to convex function to control the dynamic change of the receptive field and uses causal convolution to fully mine the historical information of the data to optimize the ability of long-term prediction. The comparative experiments among DeepTRI and three baselines (traditional recurrent neural network, ordinary spatial–temporal model and graph spatial–temporal model) show the advantages of DeepTRI in the evaluation index and resolving two under-fitting problems (under-fitting of edge values and under-fitting of local peaks).},
  archive      = {J_DKE},
  author       = {Zhiqiang Lv and Jianbo Li and Chuanhao Dong and Haoran Li and Zhihao Xu},
  doi          = {10.1016/j.datak.2021.101912},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101912},
  shortjournal = {Data Knowl. Eng.},
  title        = {Deep learning in the COVID-19 epidemic: A deep model for urban traffic revitalization index},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conceptual modeling in the era of big data and artificial
intelligence: Research topics and introduction to the special issue.
<em>DKE</em>, <em>135</em>, 101911. (<a
href="https://doi.org/10.1016/j.datak.2021.101911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.},
  archive      = {J_DKE},
  author       = {Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey},
  doi          = {10.1016/j.datak.2021.101911},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101911},
  shortjournal = {Data Knowl. Eng.},
  title        = {Conceptual modeling in the era of big data and artificial intelligence: Research topics and introduction to the special issue},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on research challenges in information science
— RCIS 2020. <em>DKE</em>, <em>135</em>, 101910. (<a
href="https://doi.org/10.1016/j.datak.2021.101910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Fabiano Dalpiaz and Jelena Zdravkovic},
  doi          = {10.1016/j.datak.2021.101910},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101910},
  shortjournal = {Data Knowl. Eng.},
  title        = {Special issue on research challenges in information science — RCIS 2020},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pairing conceptual modeling with machine learning.
<em>DKE</em>, <em>134</em>, 101909. (<a
href="https://doi.org/10.1016/j.datak.2021.101909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.},
  archive      = {J_DKE},
  author       = {Wolfgang Maass and Veda C. Storey},
  doi          = {10.1016/j.datak.2021.101909},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101909},
  shortjournal = {Data Knowl. Eng.},
  title        = {Pairing conceptual modeling with machine learning},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group-based privacy preservation techniques for process
mining. <em>DKE</em>, <em>134</em>, 101908. (<a
href="https://doi.org/10.1016/j.datak.2021.101908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining techniques help to improve processes using event data. Such data are widely available in information systems. However, they often contain highly sensitive information. For example, healthcare information systems record event data that can be utilized by process mining techniques to improve the treatment process, reduce patient’s waiting times, improve resource productivity, etc. However, the recorded event data include highly sensitive information related to treatment activities. Responsible process mining should provide insights about the underlying processes, yet, at the same time, it should not reveal sensitive information. In this paper, we discuss the challenges regarding directly applying existing well-known group-based privacy preservation techniques, e.g., k -anonymity, l -diversity, etc, to event data. We provide formal definitions of attack models and introduce an effective group-based privacy preservation technique for process mining. Our technique covers the main perspectives of process mining including control-flow , time , case , and organizational perspectives. The proposed technique provides interpretable and adjustable parameters to handle different privacy aspects. We employ real-life event data and evaluate both data utility and result utility to show the effectiveness of the privacy preservation technique. We also compare this approach with other group-based approaches for privacy-preserving event data publishing.},
  archive      = {J_DKE},
  author       = {Majid Rafiei and Wil M.P. van der Aalst},
  doi          = {10.1016/j.datak.2021.101908},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101908},
  shortjournal = {Data Knowl. Eng.},
  title        = {Group-based privacy preservation techniques for process mining},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ontology-driven evolution of software security.
<em>DKE</em>, <em>134</em>, 101907. (<a
href="https://doi.org/10.1016/j.datak.2021.101907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontologies as a means to formally specify the knowledge of a domain of interest have made their way into information and communication technology . Most often, such knowledge is subject to continuous change, which demands for consistent evolution of ontologies and dependent artifacts. In this article, we study ontology evolution in the context of software security, where ontologies may be used to formalize the security context knowledge which is needed to properly implement security requirements. In this application scenario, techniques for detecting ontology changes and determining their semantic impact are required to maintain the security of a software-intensive system in response to changing security context knowledge. Our solution is capable of detecting semantic editing patterns, which may be customly defined using graph transformation rules, but it does not depend on information about editing processes such as persistently managed changelogs. We leverage semantic editing patterns for (i) generating system co-evolution proposals, (ii) adapting the configuration of standard security checks, and (iii) performing incremental security compliance analyses between co-evolved system models and the implementation. We demonstrate the feasibility of the approach using a realistic medical information system known as iTrust.},
  archive      = {J_DKE},
  author       = {Sven Peldszus and Jens Bürger and Timo Kehrer and Jan Jürjens},
  doi          = {10.1016/j.datak.2021.101907},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101907},
  shortjournal = {Data Knowl. Eng.},
  title        = {Ontology-driven evolution of software security},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering business process simulation models in the
presence of multitasking and availability constraints. <em>DKE</em>,
<em>134</em>, 101897. (<a
href="https://doi.org/10.1016/j.datak.2021.101897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business process simulation is a versatile technique for quantitative analysis of business processes. A well-known limitation of process simulation is that the accuracy of the simulation results is limited by the faithfulness of the process model and simulation parameters given as input to the simulator. To tackle this limitation, various authors have proposed to discover simulation models from process execution logs, so that the resulting simulation models more closely match reality. However, existing techniques in this field make certain assumptions about resource behavior that do not typically hold in practice, including: (i) that each resource performs one task at a time; and (ii) that resources are continuously available (24/7). In reality, resources may engage in multitasking behavior and they work only during certain periods of the day or the week. This article proposes an approach to discover process simulation models from execution logs in the presence of multitasking and availability constraints. To account for multitasking, we adjust the processing times of tasks in such a way that executing the multitasked tasks sequentially with the adjusted times is equivalent to executing them concurrently with the original times. Meanwhile, to account for availability constraints, we use an algorithm for discovering calendar expressions from collections of time-points to infer resource timetables from an execution log. We then adjust the parameters of this algorithm to maximize the similarity between the simulated log and the original one. We evaluate the approach using real-life and synthetic datasets . The results show that the approach improves the accuracy of simulation models discovered from execution logs both in the presence of multitasking and availability constraints.},
  archive      = {J_DKE},
  author       = {Bedilia Estrada-Torres and Manuel Camargo and Marlon Dumas and Luciano García-Bañuelos and Ibrahim Mahdy and Maksym Yerokhin},
  doi          = {10.1016/j.datak.2021.101897},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101897},
  shortjournal = {Data Knowl. Eng.},
  title        = {Discovering business process simulation models in the presence of multitasking and availability constraints},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Managing polyglot systems metadata with hypergraphs.
<em>DKE</em>, <em>134</em>, 101896. (<a
href="https://doi.org/10.1016/j.datak.2021.101896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single type of data store can hardly fulfill every end-user requirements in the NoSQL world. Therefore, polyglot systems use different types of NoSQL datastores in combination. However, the heterogeneity of the data storage models makes managing the metadata a complex task in such systems, with only a handful of research carried out to address this. In this paper, we propose a hypergraph-based approach for representing the catalog of metadata in a polyglot system. Taking an existing common programming interface to NoSQL systems, we extend and formalize it as hypergraphs. Then, we define design constraints and query transformation rules for three representative data store types. Next, we propose a simple query rewriting algorithm from the metadata of the catalog to underlying data store specific ones and provide a prototype implementation. Furthermore, we introduce a storage statistics estimator on the underlying data stores. Finally, we show the feasibility of our approach on a use case of an existing polyglot system, and its usefulness in metadata and physical query path calculations.},
  archive      = {J_DKE},
  author       = {Moditha Hewasinghage and Alberto Abelló and Jovan Varga and Esteban Zimányi},
  doi          = {10.1016/j.datak.2021.101896},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101896},
  shortjournal = {Data Knowl. Eng.},
  title        = {Managing polyglot systems metadata with hypergraphs},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Seamless conceptual modeling of processes with transactional
and analytical data. <em>DKE</em>, <em>134</em>, 101895. (<a
href="https://doi.org/10.1016/j.datak.2021.101895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of Business Process Management (BPM), modeling business processes and related data is a critical issue since process activities need to manage data stored in databases. The connection between processes and data is usually handled at the implementation level, even if modeling both processes and data at the conceptual level should help designers in improving business process models and identifying requirements for implementation. Especially in data- and decision-intensive contexts, business process activities need to access data stored both in databases and data warehouses. In this paper, we complete our approach for defining a novel conceptual view that bridges process activities and data. The proposed approach allows the designer to model the connection between business processes and database models and define the operations to perform, providing interesting insights on the overall connected perspective and hints for identifying activities that are crucial for decision support.},
  archive      = {J_DKE},
  author       = {Carlo Combi and Barbara Oliboni and Mathias Weske and Francesca Zerbato},
  doi          = {10.1016/j.datak.2021.101895},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101895},
  shortjournal = {Data Knowl. Eng.},
  title        = {Seamless conceptual modeling of processes with transactional and analytical data},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level conceptual modeling: Theory, language and
application. <em>DKE</em>, <em>134</em>, 101894. (<a
href="https://doi.org/10.1016/j.datak.2021.101894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many important subject domains, there are central real-world phenomena that span across multiple classification levels. In these subject domains, besides having the traditional type-level domain regularities (classes) that classify multiple concrete instances, we also have higher-order type-level regularities (metaclasses) that classify multiple instances that are themselves types. Multi-Level Modeling aims to address this technical challenge. Despite the advances in this area in the last decade, a number of requirements arising from representation needs in subject domains have not yet been addressed in current modeling approaches. In this paper, we address this issue by proposing an expressive multi-level conceptual modeling language (dubbed ML2). We follow a principled language engineering approach in the design of ML2, constructing its abstract syntax as to reflect a fully axiomatized theory for multi-level modeling (termed MLT*). We show that ML2 enables the expression of a number of multi-level modeling scenarios that cannot be currently expressed in the existing multi-level modeling languages. A textual syntax for ML2 is provided with an implementation in Xtext. We discuss how the formal theory influences the language in two aspects: (i) by providing rigorous justification for the language’s syntactic rules, which follow MLT* theorems and (ii) by forming the basis for model simulation and verification. We show that the language can reveal problems in multi-level taxonomic structures, using Wikidata fragments to demonstrate the language’s practical relevance.},
  archive      = {J_DKE},
  author       = {Claudenir M. Fonseca and João Paulo A. Almeida and Giancarlo Guizzardi and Victorio A. Carvalho},
  doi          = {10.1016/j.datak.2021.101894},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101894},
  shortjournal = {Data Knowl. Eng.},
  title        = {Multi-level conceptual modeling: Theory, language and application},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis and evaluation of document-oriented structures.
<em>DKE</em>, <em>134</em>, 101893. (<a
href="https://doi.org/10.1016/j.datak.2021.101893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document-oriented bases allow high flexibility in data representation which facilitates a rapid development of applications and enables many possibilities for data structuring. Unfortunately, in many cases, due to this flexibility and the absence of data modelling , the choice of a data representation is neglected by developers leading to many issues on several aspects of the document base and application quality; e.g., memory print, data redundancy , readability and maintainability . We aim at facilitating the study of data structuring alternatives and providing objective metrics to better reveal the advantages and disadvantages of a structure with respect to user needs. The main contributions of our approach are twofold. First of all, the semi-automatic generation of many suitable alternatives for data structuring given an initial UML model. Second, the automatic computation of structural metrics, allowing a comparison of the alternatives for JSON-compatible schema abstraction. These metrics reflect the complexity of the structure and are intended to be used in decision criteria for schema analysis and design process. This work capitalises on experiences with MongoDB , XML and software complexity metrics. The paper presents the schema generation and the metrics together with a validation scenario where we discuss how to use the results in a schema recommendation perspective.},
  archive      = {J_DKE},
  author       = {Paola Gómez and Claudia Roncancio and Rubby Casallas},
  doi          = {10.1016/j.datak.2021.101893},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101893},
  shortjournal = {Data Knowl. Eng.},
  title        = {Analysis and evaluation of document-oriented structures},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ontological analysis of software system anomalies and
their associated risks. <em>DKE</em>, <em>134</em>, 101892. (<a
href="https://doi.org/10.1016/j.datak.2021.101892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software systems have an increasing value in our lives, as our society relies on them for the numerous services they provide. However, as our need for larger and more complex software systems grows, the risks involved in their operation also grows, with possible consequences in terms of significant material and social losses. The rational management of software defects and possible failures is a fundamental requirement for a mature software industry. Standards, professional guides and capability models directly emphasize how important it is for an organization to know and to have a well-established history of failures, errors and defects as they occur in software activities. The problem is that each of these reference models employs its own vocabulary to deal with these phenomena, which can lead to a deficiency in the understanding of these notions by software engineers, causing potential interoperability problems between supporting tools, and, consequently, a poorer adoption of these standards and tools in practice. In this paper, we address this problem of the lack of a consensual conceptualization in this area by proposing two reference conceptual models: an Ontology of Software Defects, Errors and Failures (OSDEF), which takes into account an ecosystem of software artifacts, and a Reference Ontology of Software Systems (ROSS), which characterizes software systems and related artifacts at different levels of abstraction. Moreover, we use OSDEF and ROSS to perform an ontological analysis of the impact of defects, errors and failures of software systems from a risk analysis perspective. To do that, we employee an existing core ontology, namely, the Common Ontology of Value and Risk (COVR). The ontologies presented here are grounded on the Unified Foundational Ontology (UFO) and based on well-known and widely-accepted standards, professional and scientific guides and capability models. We demonstrate how this approach can suitably promote conceptual clarification and terminological harmonization in this area.},
  archive      = {J_DKE},
  author       = {Bruno Borlini Duarte and Ricardo de Almeida Falbo and Giancarlo Guizzardi and Renata Guizzardi and Vítor E. Silva Souza},
  doi          = {10.1016/j.datak.2021.101892},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101892},
  shortjournal = {Data Knowl. Eng.},
  title        = {An ontological analysis of software system anomalies and their associated risks},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Types and taxonomic structures in conceptual modeling: A
novel ontological theory and engineering support. <em>DKE</em>,
<em>134</em>, 101891. (<a
href="https://doi.org/10.1016/j.datak.2021.101891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Types are fundamental for conceptual modeling and knowledge representation, being an essential construct in all major modeling languages in these fields. Despite that, from an ontological and cognitive point of view, there has been a lack of theoretical support for precisely defining a consensual view on types. As a consequence, there has been a lack of precise methodological support for users when choosing the best way to model general terms representing types that appear in a domain, and for building sound taxonomic structures involving them. For over a decade now, a community of researchers has contributed to the development of the Unified Foundational Ontology (UFO) - aimed at providing foundations for all major conceptual modeling constructs. At the core of this enterprise, there has been a theory of types specially designed to address these issues. This theory is ontologically well-founded, psychologically informed, and formally characterized. These results have led to the development of a Conceptual Modelling language dubbed OntoUML, reflecting the ontological micro-theories comprising UFO. Over the years, UFO and OntoUML have been successfully employed on conceptual model design in a variety of domains including academic, industrial, and governmental settings. These experiences exposed improvement opportunities for both the OntoUML language and its underlying theory, UFO. In this paper, we revise the theory of types in UFO in response to empirical evidence. The new version of this theory shows that many of OntoUML’s meta-types (e.g. kind, role, phase, mixin) should be considered not as restricted to substantial types but instead should be applied to model endurant types in general, including relator types, quality types, and mode types. We also contribute with a formal characterization of this fragment of the theory, which is then used to advance a new metamodel for OntoUML (termed OntoUML 2). To demonstrate that the benefits of this approach are extended beyond OntoUML, the proposed formal theory is then employed to support the definition of UFO-based lightweight Semantic Web ontologies with ontological constraint checking in OWL. Additionally, we report on empirical evidence from the literature, mainly from cognitive psychology but also from linguistics, supporting some of the key claims made by this theory. Finally, we propose a computational support for this updated metamodel.},
  archive      = {J_DKE},
  author       = {Giancarlo Guizzardi and Claudenir M. Fonseca and João Paulo A. Almeida and Tiago Prince Sales and Alessander Botti Benevides and Daniele Porello},
  doi          = {10.1016/j.datak.2021.101891},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101891},
  shortjournal = {Data Knowl. Eng.},
  title        = {Types and taxonomic structures in conceptual modeling: A novel ontological theory and engineering support},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering closed and maximal embedded patterns from large
tree data. <em>DKE</em>, <em>133</em>, 101890. (<a
href="https://doi.org/10.1016/j.datak.2021.101890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many current applications and systems produce large tree datasets and export, exchange, and represent data in tree-structured form. Extracting informative patterns from large data trees is an important research direction with multiple applications in practice. Pattern mining research initially focused on mining induced patterns and gradually evolved into mining embedded patterns. A well-known problem of frequent pattern mining is the huge number of patterns it produces. This affects not only the efficiency but also the effectiveness of mining. A typical solution to this problem is to summarize frequent patterns through closed and maximal patterns. No previous work addresses the problem of mining closed and/or maximal embedded tree patterns, not even in the framework of mining multiple small trees. We address the problem of summarizing embedded tree patterns extracted from large data trees, by defining and mining closed and maximal embedded unordered tree patterns. We design an embedded frequent pattern mining algorithm extended with a local closedness checking technique. This algorithm is called closedEmbTM-eager as it eagerly eliminates non-closed patterns. To mitigate the generation of intermediate patterns, we devise pattern search space pruning rules to proactively detect and prune branches in the pattern search space which do not correspond to closed patterns. The pruning rules are accommodated into the extended embedded pattern miner to produce a new algorithm, called closedEmbTM-prune , for mining all the closed and maximal embedded frequent patterns. Our extensive experiments on synthetic and real large-tree datasets demonstrate that, on dense datasets, closedEmbTM-prune not only generates a complete closed and maximal pattern set which is substantially smaller than that generated by the embedded pattern miner, but also runs much faster with negligible overhead on pattern pruning.},
  archive      = {J_DKE},
  author       = {Xiaoying Wu and Dimitri Theodoratos and Nikos Mamoulis},
  doi          = {10.1016/j.datak.2021.101890},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101890},
  shortjournal = {Data Knowl. Eng.},
  title        = {Discovering closed and maximal embedded patterns from large tree data},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ontology knowledge inspection methodology for quality
assessment and continuous improvement. <em>DKE</em>, <em>133</em>,
101889. (<a href="https://doi.org/10.1016/j.datak.2021.101889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology-learning methods were introduced in the knowledge engineering area to automatically build ontologies from natural language texts related to a domain. Despite the initial appeal of these methods, automatically generated ontologies may have errors, inconsistencies, and a poor design quality, all of which must be manually fixed, in order to maintain the validity and usefulness of automated output. In this work, we propose a methodology to assess ontologies quality (quantitatively and graphically) and to fix ontology inconsistencies minimizing design defects. The proposed methodology is based on the Deming cycle and is grounded on quality standards that proved effective in the software engineering domain and present high potential to be extended to knowledge engineering quality management . This paper demonstrates that software engineering quality assessment approaches and techniques can be successfully extended and applied to the ontology-fixing and quality improvement problem. The proposed methodology was validated in a testing ontology, by ontology design quality comparison between a manually created and automatically generated ontology.},
  archive      = {J_DKE},
  author       = {Gabriela R. Roldán-Molina and David Ruano-Ordás and Vitor Basto-Fernandes and José R. Méndez},
  doi          = {10.1016/j.datak.2021.101889},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101889},
  shortjournal = {Data Knowl. Eng.},
  title        = {An ontology knowledge inspection methodology for quality assessment and continuous improvement},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COPri v.2 — a core ontology for privacy requirements.
<em>DKE</em>, <em>133</em>, 101888. (<a
href="https://doi.org/10.1016/j.datak.2021.101888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, most enterprises collect, store, and manage personal information of customers to deliver their services. In such a setting, privacy has emerged as a key concern since companies often neglect or even misuse personal data. In response to multiple massive breaches of personal data, governments around the world have enacted laws and regulations for privacy protection. These laws dictate privacy requirements for any system that acquires and manages personal data. Unfortunately, these requirements are often incomplete and/or inaccurate as many RE practitioners are insufficiently versed with privacy requirements and how are they different from other requirements, such as security. To tackle this problem, we developed a comprehensive ontology for privacy requirements. In particular, the contributions of this work include the derivation of an ontology from a previously conducted systematic literature review, an implementation using an ontology definition tool (Protégé), a demonstration of its coverage through an extensive example on Ambient Assisted Living, and a validation through competency questions . Also, we evaluate the ontology against the common pitfalls for ontologies with the help of some software tools , lexical semantics experts, and privacy and security researchers. The ontology presented herein (COPri v.2) has been enhanced with extensions motivated by the feedback received from privacy and security experts.},
  archive      = {J_DKE},
  author       = {Mohamad Gharib and Paolo Giorgini and John Mylopoulos},
  doi          = {10.1016/j.datak.2021.101888},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101888},
  shortjournal = {Data Knowl. Eng.},
  title        = {COPri v.2 — a core ontology for privacy requirements},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An artifact ontology for design science research.
<em>DKE</em>, <em>133</em>, 101878. (<a
href="https://doi.org/10.1016/j.datak.2021.101878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From a design science perspective, information systems and their components are viewed as artifacts. However, not much has been written yet on the ontological status of artifacts or their structure. After March &amp; Smith’s (1995) initial classification of artifacts in terms of models, constructs, methods and instantiations , there have been only a few attempts to come up with a more systematic approach. This conceptual paper provides an ontological analysis of the notion of artifact grounded in the foundational ontology UFO. Its core is an ontological characterization of artifacts, and technical objects in general from a Design Science Research perspective, developed in conversation with other approaches. This general artifact ontology is applied in a systematic classification of IS artifacts. We include practical implications for Design Science Research.},
  archive      = {J_DKE},
  author       = {Hans Weigand and Paul Johannesson and Birger Andersson},
  doi          = {10.1016/j.datak.2021.101878},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101878},
  shortjournal = {Data Knowl. Eng.},
  title        = {An artifact ontology for design science research},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilingual verbalization and summarization for explainable
link discovery. <em>DKE</em>, <em>133</em>, 101874. (<a
href="https://doi.org/10.1016/j.datak.2021.101874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number and size of datasets abiding by the Linked Data paradigm increase every day. Discovering links between these datasets is thus central to achieving the vision behind the Data Web. Declarative Link Discovery (LD) frameworks rely on complex Link Specification (LS) to express the conditions under which two resources should be linked. Understanding such LS is not a trivial task for non-expert users. Particularly when such users are interested in generating LS to match their needs. Even if the user applies a machine learning algorithm for the automatic generation of the required LS, the challenge of explaining the resultant LS persists. Hence, providing explainable LS is the key challenge to enable users who are unfamiliar with underlying LS technologies to use them effectively and efficiently. In this paper, we extend our previous work (Ahmed et al., 2019) by proposing a generic multilingual approach that allows verbalization of LS in many languages, i.e., converts LS into understandable natural language text. In this work, we ported our LS verbalization framework into German and Spanish, in addition to English language. Our adequacy and fluency evaluations show that our approach can generate complete and easily understandable natural language descriptions even by lay users. Moreover, we devised an experimental neural approach for improving the quality of our generated texts. Our neural approach achieves promising results in terms of BLEU, METEOR and chrF++.},
  archive      = {J_DKE},
  author       = {Abdullah Fathi Ahmed and Mohamed Ahmed Sherif and Diego Moussallem and Axel-Cyrille Ngonga Ngomo},
  doi          = {10.1016/j.datak.2021.101874},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101874},
  shortjournal = {Data Knowl. Eng.},
  title        = {Multilingual verbalization and summarization for explainable link discovery},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A provenance model for control-flow driven scientific
workflows. <em>DKE</em>, <em>131-132</em>, 101877. (<a
href="https://doi.org/10.1016/j.datak.2021.101877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provenance in the context of workflows, both for their specification and for the data they derive, is essential for result reproducibility, sharing, and knowledge reuse in the scientific community. The information models to capture the provenance of scientific workflows, known as provenance models, have been of wide interest and studied in many fields, including the semantic Web. However, most existing provenance models rely on commonly perceived data-driven execution paradigm of scientific workflows and overlook control constructs that may appear in scientific workflows. The provenance models proposed by the semantic Web community for data-driven scientific workflows underspecify the structure of the workflows (i.e., workflow provenance). Such an underspecified workflow structure can result in the misinterpretation of a scientific experiment and precludes its conformance checking, thus restricting provenance gains. This paper shows that the design of a provenance model for control-flow constructs and the means to integrate it with the existing provenance models can maximise the provenance gains for the users of scientific workflows. In this regard, first, we specify the need for a control-flow driven scientific workflow provenance model and detail the minimal characteristics which such a model should address. Secondly, we present a formal model to specify the control-flows that may appear in scientific workflows and describe how the existing provenance models can be complemented by using the proposed model. Finally, we show that the proposed model can capture accurate provenance information for the scientific workflows, which makes it possible to understand, reproduce and validate workflows and their output.},
  archive      = {J_DKE},
  author       = {Anila Sahar Butt and Peter Fitch},
  doi          = {10.1016/j.datak.2021.101877},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101877},
  shortjournal = {Data Knowl. Eng.},
  title        = {A provenance model for control-flow driven scientific workflows},
  volume       = {131-132},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generation of gaussian sets for clustering methods
assessment. <em>DKE</em>, <em>131-132</em>, 101876. (<a
href="https://doi.org/10.1016/j.datak.2021.101876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering methods are generally used to study the homogeneity in a set of observations. The results obtained from the clustering process differ from one method to another, to the extent that the same method or validity index gives different outcomes depending on the initial parameters. Analytical evaluation appears to be insufficient for studying the behavior of clustering methods due to its ad hoc nature. Even if the real data set is used in evaluating clustering methods, artificial data is fundamental for assessing the performance since it allows creating different scenarios of test with known structures. The main drawback of existing methods of artificial data is that they do not take into consideration the problem of sensitivity to the size of clusters. In this paper, we propose an automatic method: the high-dimensional artificial Gaussian mixture generator. By formally quantifying the overlap, the generator preserves the notion of the overlap rate between the mixture components. The advantages of this generator are its use of the notion of overlap rate, the unlimited number of mixture components, high-dimensionality of the observations, and the non-utilization of visual inspection as a criterion to quantify the overlap. In addition, we evaluate the k-means, fuzzy c-means (FCM), FCM-based splitting algorithm (FBSA), and expectation maximization (EM) in different dimensions. The results obtained confirm previous work and reveal new findings that are not pointed out when using 1D and 2D artificial data. 1},
  archive      = {J_DKE},
  author       = {Radhwane Gherbaoui and Mohammed Ouali and Nacéra Benamrane},
  doi          = {10.1016/j.datak.2021.101876},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101876},
  shortjournal = {Data Knowl. Eng.},
  title        = {Generation of gaussian sets for clustering methods assessment},
  volume       = {131-132},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A profile-aware methodological framework for collaborative
multidimensional modeling. <em>DKE</em>, <em>131-132</em>, 101875. (<a
href="https://doi.org/10.1016/j.datak.2021.101875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multidimensional modeling, i.e., the design of cube schemata, has a key role in data warehouse (DW) projects, in self-service business intelligence, and in general to let users analyze data via the OLAP paradigm. Though an effective involvement of users in multidimensional modeling is crucial in these projects, not much has been said about how to establish a fruitful collaboration in projects involving numerous users with different skills, reputations, and degrees of authority. This issue is especially relevant in citizen science projects, where several volunteers can contribute their requirements despite not being formally-trained experts in the application domain. To fill this gap, we propose a framework for collaborative multidimensional modeling that can adapt itself to the profiles and skills of the actors involved. We first classify users depending on their authoritativeness, skills, and engagement in the project. Then, following this classification, we identify four possible methodological scenarios and propose a profile-aware methodology supported by two sets of quality attributes. Finally, we describe a Group Decision Support System that implements our methodological framework and present some experiments carried out on a real case study.},
  archive      = {J_DKE},
  author       = {Amir Sakka and Sandro Bimonte and Stefano Rizzi and Lucile Sautot and François Pinet and Michela Bertolotto and Aurélien Besnard and Noura Rouillier},
  doi          = {10.1016/j.datak.2021.101875},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101875},
  shortjournal = {Data Knowl. Eng.},
  title        = {A profile-aware methodological framework for collaborative multidimensional modeling},
  volume       = {131-132},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
