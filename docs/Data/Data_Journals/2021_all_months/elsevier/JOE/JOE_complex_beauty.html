<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joe---163">JOE - 163</h2>
<ul>
<li><details>
<summary>
(2021). Matching estimators with few treated and many control
observations. <em>JOE</em>, <em>225</em>(2), 295–307. (<a
href="https://doi.org/10.1016/j.jeconom.2021.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the properties of matching estimators when there are few treated, but many control observations. We show that, under standard assumptions, the nearest neighbor matching estimator for the average treatment effect on the treated is asymptotically unbiased in this framework. However, when the number of treated observations is fixed, the estimator is not consistent, and it is generally not asymptotically normal. Since standard inference methods are inadequate, we propose alternative inference methods, based on the theory of randomization tests under approximate symmetry, that are asymptotically valid in this framework. We show that these tests are valid under relatively strong assumptions when the number of treated observations is fixed, and under weaker assumptions when the number of treated observations increases, but at a lower rate relative to the number of control observations.},
  archive      = {J_JOE},
  author       = {Bruno Ferman},
  doi          = {10.1016/j.jeconom.2021.07.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {295-307},
  shortjournal = {J. Econ.},
  title        = {Matching estimators with few treated and many control observations},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covariate-adjusted fisher randomization tests for the
average treatment effect. <em>JOE</em>, <em>225</em>(2), 278–294. (<a
href="https://doi.org/10.1016/j.jeconom.2021.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher’s randomization test ( frt ) delivers exact p p -values under the strong null hypothesis of no treatment effect on any units whatsoever and allows for flexible covariate adjustment to improve the power. Of interest is whether the resulting covariate-adjusted procedure could also be valid for testing the weak null hypothesis of zero average treatment effect . To this end, we evaluate two general strategies for conducting covariate adjustment in frt s: the pseudo-outcome strategy that uses the residuals from an outcome model with only the covariates as the pseudo, covariate-adjusted outcomes to form the test statistic, and the model-output strategy that directly uses the output from an outcome model with both the treatment and covariates as the covariate-adjusted test statistic. Based on theory and simulation, we recommend using the ordinary least squares ( ols ) fit of the observed outcome on the treatment, centered covariates, and their interactions for covariate adjustment, and conducting frt with the robust t t -value of the treatment as the test statistic. The resulting frt is finite-sample exact for testing the strong null hypothesis, asymptotically valid for testing the weak null hypothesis, and more powerful than the unadjusted counterpart under alternatives, all irrespective of whether the linear model is correctly specified or not. We start with complete randomization, and then extend the theory to cluster randomization, stratified randomization, and rerandomization, respectively, giving a recommendation for the test procedure and test statistic under each design. Our theory is design-based, also known as randomization-based, in which we condition on the potential outcomes but average over the random treatment assignment.},
  archive      = {J_JOE},
  author       = {Anqi Zhao and Peng Ding},
  doi          = {10.1016/j.jeconom.2021.04.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {278-294},
  shortjournal = {J. Econ.},
  title        = {Covariate-adjusted fisher randomization tests for the average treatment effect},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Difference-in-differences with variation in treatment
timing. <em>JOE</em>, <em>225</em>(2), 254–277. (<a
href="https://doi.org/10.1016/j.jeconom.2021.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The canonical difference-in-differences (DD) estimator contains two time periods, ”pre” and ”post”, and two groups, ”treatment” and ”control”. Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of two-way fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.},
  archive      = {J_JOE},
  author       = {Andrew Goodman-Bacon},
  doi          = {10.1016/j.jeconom.2021.03.014},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {254-277},
  shortjournal = {J. Econ.},
  title        = {Difference-in-differences with variation in treatment timing},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The identification region of the potential outcome
distributions under instrument independence. <em>JOE</em>,
<em>225</em>(2), 231–253. (<a
href="https://doi.org/10.1016/j.jeconom.2021.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the identifying power of instrument exogeneity in the treatment effect model. We derive the identification region of the potential outcome distributions, which are the collection of distributions that are compatible with data and with the restrictions of the model. We consider identification when (i) the instrument is independent of each of the potential outcomes ( marginal independence ), (ii) the instrument is independent of the potential outcomes and selection heterogeneity jointly ( joint independence ), and (iii.) the instrument satisfies joint independence and monotonicity (the LATE restriction ). By comparing the size of the identification region under each restriction, we show that joint independence provides more identifying information for the potential outcome distributions than marginal independence, but that the LATE restriction provides no identification gain beyond joint independence. We also, under each restriction, derive sharp bounds for the Average Treatment Effect and sharp testable implication to falsify the restriction. Our analysis covers discrete or continuous outcomes, and extends the Average Treatment Effect bounds of Balke and Pearl (1997) developed for the dichotomous outcome case to a more general setting.},
  archive      = {J_JOE},
  author       = {Toru Kitagawa},
  doi          = {10.1016/j.jeconom.2021.03.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {231-253},
  shortjournal = {J. Econ.},
  title        = {The identification region of the potential outcome distributions under instrument independence},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Difference-in-differences with multiple time periods.
<em>JOE</em>, <em>225</em>(2), 200–230. (<a
href="https://doi.org/10.1016/j.jeconom.2020.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates . We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.},
  archive      = {J_JOE},
  author       = {Brantly Callaway and Pedro H.C. Sant’Anna},
  doi          = {10.1016/j.jeconom.2020.12.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {200-230},
  shortjournal = {J. Econ.},
  title        = {Difference-in-differences with multiple time periods},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating dynamic treatment effects in event studies with
heterogeneous treatment effects. <em>JOE</em>, <em>225</em>(2), 175–199.
(<a href="https://doi.org/10.1016/j.jeconom.2020.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To estimate the dynamic effects of an absorbing treatment, researchers often use two-way fixed effects regressions that include leads and lags of the treatment. We show that in settings with variation in treatment timing across units, the coefficient on a given lead or lag can be contaminated by effects from other periods, and apparent pretrends can arise solely from treatment effects heterogeneity. We propose an alternative estimator that is free of contamination, and illustrate the relative shortcomings of two-way fixed effects regressions with leads and lags through an empirical application.},
  archive      = {J_JOE},
  author       = {Liyang Sun and Sarah Abraham},
  doi          = {10.1016/j.jeconom.2020.09.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {175-199},
  shortjournal = {J. Econ.},
  title        = {Estimating dynamic treatment effects in event studies with heterogeneous treatment effects},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Permutation test for heterogeneous treatment effects with a
nuisance parameter. <em>JOE</em>, <em>225</em>(2), 148–174. (<a
href="https://doi.org/10.1016/j.jeconom.2020.09.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an asymptotically valid permutation test for heterogeneous treatment effects in the presence of an estimated nuisance parameter . Not accounting for the estimation error of the nuisance parameter results in statistics that depend on the particulars of the data generating process, and the resulting permutation test fails to control the Type 1 error, even asymptotically. In this paper we consider a permutation test based on a martingale transformation of the empirical process to render an asymptotically pivotal statistic, effectively nullifying the effect associated with the estimation error on the limiting distribution of the statistic. Under weak conditions, we show that the permutation test based on the martingale-transformed statistic results in the asymptotic rejection probability of α α in general while retaining the exact control of the test level when testing for the more restrictive sharp null . We also show how our martingale-based permutation test extends to testing whether there exists treatment effect heterogeneity within subgroups defined by observable covariates . Our approach comprises testing the joint null hypothesis that treatment effects are constant within mutually exclusive subgroups while allowing the treatment effects to vary across subgroups. Monte Carlo simulations show that the permutation test presented here performs well in finite samples, and is comparable to those existing in the literature. To gain further understanding of the test to practical problems, we investigate the gift exchange hypothesis in the context of two field experiments from Gneezy and List (2006). Lastly, we provide the companion RATest R package to facilitate and encourage the application of our test in empirical research.},
  archive      = {J_JOE},
  author       = {EunYi Chung and Mauricio Olivares},
  doi          = {10.1016/j.jeconom.2020.09.015},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {148-174},
  shortjournal = {J. Econ.},
  title        = {Permutation test for heterogeneous treatment effects with a nuisance parameter},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification in nonparametric models for dynamic treatment
effects. <em>JOE</em>, <em>225</em>(2), 132–147. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a nonparametric model that represents how sequences of outcomes and treatment choices influence one another in a dynamic manner. In this setting, we are interested in identifying the average outcome for individuals in each period, had a particular treatment sequence been assigned. The identification of this quantity allows us to identify the average treatment effects (ATE’s) and the ATE’s on transitions, as well as the optimal treatment regimes, namely, the regimes that maximize the (weighted) sum of the average potential outcomes, possibly less the cost of the treatments. The main contribution of this paper is to relax the sequential randomization assumption widely used in the biostatistics literature by introducing a flexible choice-theoretic framework for a sequence of endogenous treatments. This framework allows non-compliance of subjects in experimental studies or endogenous treatment decisions in observational settings. We show that the parameters of interest are identified under each period’s exclusion restrictions, which are motivated by, e.g., a sequence of randomized treatment assignments or encouragements and a behavioral/information assumption on agents who receive treatments.},
  archive      = {J_JOE},
  author       = {Sukjin Han},
  doi          = {10.1016/j.jeconom.2019.08.014},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {132-147},
  shortjournal = {J. Econ.},
  title        = {Identification in nonparametric models for dynamic treatment effects},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A note from the editors. <em>JOE</em>, <em>225</em>(2), 131.
(<a href="https://doi.org/10.1016/j.jeconom.2021.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {The editorial board},
  doi          = {10.1016/j.jeconom.2021.10.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {131},
  shortjournal = {J. Econ.},
  title        = {A note from the editors},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impulse response analysis for structural dynamic models with
nonlinear regressors. <em>JOE</em>, <em>225</em>(1), 107–130. (<a
href="https://doi.org/10.1016/j.jeconom.2021.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the construction of nonlinear impulse responses in linear structural dynamic models that include nonlinearly transformed regressors . We derive the closed-form solution for the population impulse responses to a given shock and propose a control function approach to estimating these responses without taking a stand on how the remainder of the model is identified. Our plug-in estimator dispenses with the need for simulations and, unlike conventional local projection (LP) estimators, is consistent. A modified LP estimator is shown to be consistent in special cases, but less accurate in finite samples than the plug-in estimator.},
  archive      = {J_JOE},
  author       = {Sílvia Gonçalves and Ana María Herrera and Lutz Kilian and Elena Pesavento},
  doi          = {10.1016/j.jeconom.2021.06.009},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {107-130},
  shortjournal = {J. Econ.},
  title        = {Impulse response analysis for structural dynamic models with nonlinear regressors},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference in bayesian proxy-SVARs. <em>JOE</em>,
<em>225</em>(1), 88–106. (<a
href="https://doi.org/10.1016/j.jeconom.2020.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the increasing use of external instruments to identify structural vector autoregressions (SVARs), we develop an algorithm for exact finite sample inference in this class of time series models , commonly known as Proxy-SVARs. Our algorithm makes independent draws from any posterior distribution over the structural parameterization of a Proxy-SVAR. Our approach allows researchers to simultaneously use proxies and traditional zero and sign restrictions to identify structural shocks. We illustrate our methods with two applications. In particular, we show how to generalize the counterfactual analysis in Mertens and Montiel-Olea (2018) to identified structural shocks.},
  archive      = {J_JOE},
  author       = {Jonas E. Arias and Juan F. Rubio-Ramírez and Daniel F. Waggoner},
  doi          = {10.1016/j.jeconom.2020.12.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {88-106},
  shortjournal = {J. Econ.},
  title        = {Inference in bayesian proxy-SVARs},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference in structural vector autoregressions identified
with an external instrument. <em>JOE</em>, <em>225</em>(1), 74–87. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies Structural Vector Autoregressions in which a structural shock of interest (e.g., an oil supply shock) is identified using an external instrument. The external instrument is taken to be correlated with the target shock (the instrument is relevant) and to be uncorrelated with other shocks of the model (the instrument is exogenous). The potential weak correlation between the external instrument and the target structural shock compromises the large-sample validity of standard inference. We suggest a confidence set for impulse response coefficients that is not affected by the instrument strength (i.e., is weak-instrument robust) and asymptotically coincides with the standard confidence set when the instrument is strong.},
  archive      = {J_JOE},
  author       = {José L. Montiel Olea and James H. Stock and Mark W. Watson},
  doi          = {10.1016/j.jeconom.2020.05.014},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {74-87},
  shortjournal = {J. Econ.},
  title        = {Inference in structural vector autoregressions identified with an external instrument},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using time-varying volatility for identification in vector
autoregressions: An application to endogenous uncertainty. <em>JOE</em>,
<em>225</em>(1), 47–73. (<a
href="https://doi.org/10.1016/j.jeconom.2021.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a structural vector autoregression with stochastic volatility in which one of the variables can impact both the mean and the variance of the other variables. We provide conditional posterior distributions for this model, develop an MCMC algorithm for estimation, and show how stochastic volatility can be used to provide useful restrictions for the identification of structural shocks. We then use the model with US data to show that some variables have a significant contemporaneous feedback effect on macroeconomic uncertainty, and overlooking this channel can lead to distortions in the estimated effects of uncertainty on the economy.},
  archive      = {J_JOE},
  author       = {Andrea Carriero and Todd E. Clark and Massimiliano Marcellino},
  doi          = {10.1016/j.jeconom.2021.07.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {47-73},
  shortjournal = {J. Econ.},
  title        = {Using time-varying volatility for identification in vector autoregressions: An application to endogenous uncertainty},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification of structural vector autoregressions through
higher unconditional moments. <em>JOE</em>, <em>225</em>(1), 27–46. (<a
href="https://doi.org/10.1016/j.jeconom.2020.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper pursues two objectives. First, we determine the sufficient condition for local, statistical identification of SVAR processes through the third and fourth unconditional moments of the reduced-form innovations. Our findings provide novel insights when the entire system is not identified, as they highlight which subset of structural parameters is identified and which is not. Second, we elaborate a tractable testing procedure to verify whether the identification condition holds, prior to the estimation of the structural parameters of the SVAR process. To do so, we design a new bootstrap procedure that improves the small-sample properties of rank tests for the symmetry and kurtosis of the structural shocks.},
  archive      = {J_JOE},
  author       = {Alain Guay},
  doi          = {10.1016/j.jeconom.2020.10.006},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {27-46},
  shortjournal = {J. Econ.},
  title        = {Identification of structural vector autoregressions through higher unconditional moments},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting groups in large vector autoregressions.
<em>JOE</em>, <em>225</em>(1), 2–26. (<a
href="https://doi.org/10.1016/j.jeconom.2021.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces the stochastic block vector autoregressive (SB-VAR) model. In this class of vector autoregressions , the time series are partitioned into latent groups such that spillover effects are stronger among series that belong to the same group than otherwise. A key question that arises in this framework is how to detect the latent groups from a sample of observations generated by the model. To this end, we propose a group detection algorithm based on the eigenvectors of a function of the estimated autoregressive matrices. We establish that the proposed algorithm consistently detects the groups when the cross-sectional and time-series dimensions are sufficiently large. The methodology is applied to study the group structure of a panel of risk measures of top financial institutions in the United States and a panel of word counts extracted from Twitter.},
  archive      = {J_JOE},
  author       = {Guðmundur Stefán Guðmundsson and Christian Brownlees},
  doi          = {10.1016/j.jeconom.2021.03.012},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {2-26},
  shortjournal = {J. Econ.},
  title        = {Detecting groups in large vector autoregressions},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial for special issue: Vector autoregressions.
<em>JOE</em>, <em>225</em>(1), 1. (<a
href="https://doi.org/10.1016/j.jeconom.2021.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Sophocles Mavroeidis},
  doi          = {10.1016/j.jeconom.2021.07.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1},
  shortjournal = {J. Econ.},
  title        = {Editorial for special issue: Vector autoregressions},
  volume       = {225},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recursive estimation in large panel data models: Theory and
practice. <em>JOE</em>, <em>224</em>(2), 439–465. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bai (2009) proposes recursive estimation for panel data models with interactive effects. We study the behaviours of this recursive estimator. The recursive formula is established that shows the behaviours of recursive estimators depend on the initial estimator, the population structure and the iterative steps. Under some general scenarios, we find that the recursive estimator becomes consistent after the first iteration from any initials. We also obtain the optimal number of iterative steps under some prescribed conditions. The central limit theorem of the recursive estimator is established when the initial estimator is OLS. Various simulations are conducted to support our theoretical findings.},
  archive      = {J_JOE},
  author       = {Bin Jiang and Yanrong Yang and Jiti Gao and Cheng Hsiao},
  doi          = {10.1016/j.jeconom.2020.07.055},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {439-465},
  shortjournal = {J. Econ.},
  title        = {Recursive estimation in large panel data models: Theory and practice},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust nonlinear regression estimation in null recurrent
time series. <em>JOE</em>, <em>224</em>(2), 416–438. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study parametric robust estimation in nonlinear regression models with regressors generated by a class of non-stationary and null recurrent Markov processes . The nonlinear regression functions can be either integrable or asymptotically homogeneous, covering many commonly-used functional forms in parametric nonlinear regression . Under regularity conditions , we derive both the consistency and limit distribution results for the developed general robust estimators (including the nonlinear least squares, least absolute deviation and Huber’s M-estimators). The convergence rates of the estimation depend on not only the functional form of the nonlinear regression , but also on the recurrence rate of the Markov process. Some Monte-Carlo simulation studies are conducted to examine the numerical performance of the proposed estimators and verify the established asymptotic properties in finite samples. Finally two empirical applications illustrate the usefulness of the proposed robust estimation method.},
  archive      = {J_JOE},
  author       = {Francesco Bravo and Degui Li and Dag Tjøstheim},
  doi          = {10.1016/j.jeconom.2020.03.028},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {416-438},
  shortjournal = {J. Econ.},
  title        = {Robust nonlinear regression estimation in null recurrent time series},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-varying instrumental variable estimation. <em>JOE</em>,
<em>224</em>(2), 394–415. (<a
href="https://doi.org/10.1016/j.jeconom.2020.08.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop non-parametric instrumental variable estimation and inferential theory for econometric models with possibly endogenous regressors whose coefficients can vary over time either deterministically or stochastically, and the time-varying and uniform versions of the standard Hausman exogeneity test. After deriving the asymptotic properties of the proposed procedures, we assess their finite sample performance by means of a set of Monte Carlo experiments, and illustrate their application by means of an empirical example on the Phillips curve.},
  archive      = {J_JOE},
  author       = {Liudas Giraitis and George Kapetanios and Massimiliano Marcellino},
  doi          = {10.1016/j.jeconom.2020.08.013},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {394-415},
  shortjournal = {J. Econ.},
  title        = {Time-varying instrumental variable estimation},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved bootstrap test for restricted stochastic
dominance. <em>JOE</em>, <em>224</em>(2), 371–393. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bootstrap Testing for restricted stochastic dominance of a pre-specified order between two distributions is of interest in many areas of economics. This paper develops a new method for improving the performance of such tests that employ a moment selection procedure: tilting the empirical distribution in the moment selection procedure. We propose that the amount of tilting be chosen to maximize the empirical likelihood subject to the restrictions of the null hypothesis, which are a continuum of unconditional moment inequality conditions. We characterize sets of population distributions on which a modified test is (i) asymptotically equivalent to its non-modified version to first-order, and (ii) superior to its non-modified version according to local power when the sample size is large enough. We report simulation results that show the modified versions of leading tests are noticeably less conservative than their non-modified counterparts and have improved power. Finally, an empirical example is discussed to illustrate the proposed method.},
  archive      = {J_JOE},
  author       = {Thomas M. Lok and Rami V. Tabri},
  doi          = {10.1016/j.jeconom.2019.08.016},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {371-393},
  shortjournal = {J. Econ.},
  title        = {An improved bootstrap test for restricted stochastic dominance},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An econometric model of network formation with an
application to board interlocks between firms. <em>JOE</em>,
<em>224</em>(2), 345–370. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study identification of the players’ preferences in a network formation game featuring complete information, nonreciprocal links, and a spillover effect . We decompose the network formation game into local games such that the network formation game is in equilibrium if and only if each local game is in equilibrium. This decomposition helps us prove equilibrium existence, reduce the number of moment inequalities characterising the identified set, and simplify the calculation of the integrals entering those moment inequalities . The developed methodology is used to investigate Italian firms’ incentives for having their executive directors sitting on competitors’ boards.},
  archive      = {J_JOE},
  author       = {Cristina Gualdani},
  doi          = {10.1016/j.jeconom.2019.08.015},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {345-370},
  shortjournal = {J. Econ.},
  title        = {An econometric model of network formation with an application to board interlocks between firms},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust estimation with exponentially tilted hellinger
distance. <em>JOE</em>, <em>224</em>(2), 330–344. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with estimation of parameters defined by moment equalities. We introduce the exponentially tilted Hellinger distance (ETHD) estimator which is efficient under correct specification, and robust to both global and local misspecification. In the spirit of Schennach (2007), ETHD combines the Hellinger distance and the Kullback–Leibler information criterion. We show that it achieves optimal minimax robust properties under local deviations from the model and remains well-behaved under global misspecification.},
  archive      = {J_JOE},
  author       = {Bertille Antoine and Prosper Dovonon},
  doi          = {10.1016/j.jeconom.2020.03.027},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {330-344},
  shortjournal = {J. Econ.},
  title        = {Robust estimation with exponentially tilted hellinger distance},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive inference for a semiparametric generalized
autoregressive conditional heteroskedasticity model. <em>JOE</em>,
<em>224</em>(2), 306–329. (<a
href="https://doi.org/10.1016/j.jeconom.2020.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a semiparametric generalized autoregressive conditional heteroskedasticity (S-GARCH) model. For this model, we first estimate the time-varying long run component for unconditional variance by the kernel estimator , and then estimate the non-time-varying parameters in GARCH-type short run component by the quasi maximum likelihood estimator (QMLE). We show that the QMLE is asymptotically normal with the parametric convergence rate. Next, we construct a Lagrange multiplier test for linear parameter constraint and a portmanteau test for model checking, and obtain their asymptotic null distributions. Our entire statistical inference procedure works for the non-stationary data with two important features: first, our QMLE and two tests are adaptive to the unknown form of the long run component; second, our QMLE and two tests share the same efficiency and testing power as those in variance targeting method when the S-GARCH model is stationary.},
  archive      = {J_JOE},
  author       = {Feiyu Jiang and Dong Li and Ke Zhu},
  doi          = {10.1016/j.jeconom.2020.10.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {306-329},
  shortjournal = {J. Econ.},
  title        = {Adaptive inference for a semiparametric generalized autoregressive conditional heteroskedasticity model},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An empirical total survey error decomposition using data
combination. <em>JOE</em>, <em>224</em>(2), 286–305. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survey error is known to be pervasive and to bias even simple, but important, estimates of means, rates, and totals, such as the poverty and the unemployment rate. In order to summarize and analyze the extent, sources, and consequences of survey error, we define empirical counterparts of key components of the Total Survey Error Framework that can be estimated using data combination. Specifically, we estimate total survey error and decompose it into three high level sources of error: generalized coverage error, item non-response error and measurement error. We further decompose these sources into lower level sources such as failure to report a positive amount and errors in amounts conditional on reporting a positive value. For errors in dollars paid by two large government transfer programs, we use administrative records on the universe of program payments in New York State linked to three major household surveys to estimate the error components previously defined. We find that total survey error is large and varies in its size and composition, but measurement error is always by far the largest source of error. Our application shows that data combination makes it possible to routinely measure total survey error and its components. Our results allow survey producers to assess error reduction strategies and survey users to mitigate the consequences of survey errors or gauge the reliability of their conclusions.},
  archive      = {J_JOE},
  author       = {Bruce D. Meyer and Nikolas Mittag},
  doi          = {10.1016/j.jeconom.2020.03.026},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {286-305},
  shortjournal = {J. Econ.},
  title        = {An empirical total survey error decomposition using data combination},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The medium-run efficiency consequences of unfair school
matching: Evidence from chinese college admissions. <em>JOE</em>,
<em>224</em>(2), 271–285. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we empirically examine how unfair (i.e. negatively assortive) matching between colleges and students affects the medium-run allocative efficiency of the college admissions system, as measured by total wage levels among college graduates. Using data from China College Student Survey, we find that unfair matching tends to increase the total wage level. The implication is that student ability and school quality tend to substitute for, rather than complement each other. We also find evidence that unfair matching leads to higher total human capital investment in English skills, leadership ability, and double majors, but not to higher GPAs. We interpret this finding as an indication that unfair matching, by increasing the inequality of ability within a college, encourages diversified human capital investment instead of monotonic competition. These findings also indicate possible channels of improvement in allocative efficiency.},
  archive      = {J_JOE},
  author       = {Xiaohan Zhong and Lin Zhu},
  doi          = {10.1016/j.jeconom.2020.07.054},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {271-285},
  shortjournal = {J. Econ.},
  title        = {The medium-run efficiency consequences of unfair school matching: Evidence from chinese college admissions},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diagnostic tests for homoskedasticity in spatial
cross-sectional or panel models. <em>JOE</em>, <em>224</em>(2), 245–270.
(<a href="https://doi.org/10.1016/j.jeconom.2020.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an Adjusted Quasi-Score (AQS) method for constructing tests for homoskedasticity in spatial econometric models . We first obtain an AQS function by adjusting the score-type function from the given model to achieve unbiasedness , and then develop an Outer-Product-of-Martingale-Difference (OPMD) estimate of its variance. In standard problems where a genuine (quasi) score vector is available, the AQS–OPMD method leads to finite sample improved tests over the usual methods. More importantly in non-standard problems where a genuine (quasi) score is not available and the usual methods fail, the proposed AQS–OPMD method provides feasible solutions. The AQS tests are formally derived and asymptotic properties examined for three representative models: spatial cross-sectional, static and dynamic panel models. Monte Carlo results show that the proposed AQS tests have good finite sample properties.},
  archive      = {J_JOE},
  author       = {Badi H. Baltagi and Alain Pirotte and Zhenlin Yang},
  doi          = {10.1016/j.jeconom.2020.10.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {245-270},
  shortjournal = {J. Econ.},
  title        = {Diagnostic tests for homoskedasticity in spatial cross-sectional or panel models},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistent inference for predictive regressions in
persistent economic systems. <em>JOE</em>, <em>224</em>(1), 215–244. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies standard predictive regressions in economic systems governed by persistent vector autoregressive dynamics for the state variables. In particular, all – or a subset – of the variables may be fractionally integrated, which induces a spurious regression problem. We propose a new inference and testing procedure – the Local speCtruM (LCM) approach – for joint significance of the regressors , that is robust against the variables having different integration orders and remains valid regardless of whether predictors are significant and, if they are, whether they induce cointegration. Specifically, the LCM procedure is based on fractional filtering and band spectrum regression using a suitably selected set of frequency ordinates. Contrary to existing procedures, we establish a uniform Gaussian limit theory and a standard χ 2 χ2 -distributed test statistic. Using the LCM inference and testing techniques, we explore predictive regressions for the realized return variation. Standard least squares inference indicates that popular financial and macroeconomic variables convey valuable information about future return volatility . In contrast, we find no significant evidence using our robust LCM procedure. If anything, our tests support a reverse chain of causality, with rising financial volatility predating adverse innovations to key macroeconomic variables . Simulations are employed to illustrate the relevance of the theoretical arguments for finite-sample inference.},
  archive      = {J_JOE},
  author       = {Torben G. Andersen and Rasmus T. Varneskov},
  doi          = {10.1016/j.jeconom.2020.04.051},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {215-244},
  shortjournal = {J. Econ.},
  title        = {Consistent inference for predictive regressions in persistent economic systems},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple tests for stock return predictability with good size
and power properties. <em>JOE</em>, <em>224</em>(1), 198–214. (<a
href="https://doi.org/10.1016/j.jeconom.2021.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop easy-to-implement tests for return predictability which, relative to extant tests in the literature, display attractive finite sample size control and power across a wide range of persistence and endogeneity levels for the predictor. Our approach is based on the standard regression t t -ratio and a variant where the predictor is quasi-GLS (rather than OLS) demeaned. In the strongly persistent near-unit root environment, the limiting null distributions of these statistics depend on the endogeneity and local-to-unity parameters characterising the predictor. Analysis of the asymptotic local power functions of feasible implementations of these two tests, based on asymptotically conservative critical values, motivates a switching procedure between the two, employing the quasi-GLS demeaned variant unless the magnitude of the estimated endogeneity correlation parameter is small. Additionally, if the data suggests the predictor is weakly persistent, our approach switches to the standard t t -ratio test with reference to standard normal critical values.},
  archive      = {J_JOE},
  author       = {David I. Harvey and Stephen J. Leybourne and A.M. Robert Taylor},
  doi          = {10.1016/j.jeconom.2021.01.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {198-214},
  shortjournal = {J. Econ.},
  title        = {Simple tests for stock return predictability with good size and power properties},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple estimators and inference for higher-order stochastic
volatility models. <em>JOE</em>, <em>224</em>(1), 181–197. (<a
href="https://doi.org/10.1016/j.jeconom.2021.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of estimating higher-order stochastic volatility [SV ( p ) (p) ] models. Due to the difficulty of evaluating the likelihood function, this remains a challenging problem, even in the relatively simple SV ( 1 ) (1) case. We propose simple moment-based winsorized ARMA-type estimators, which are computationally inexpensive and remarkably accurate. The proposed estimators do not require choosing a sampling algorithm, initial parameter values, or an auxiliary model. We show that a Durbin–Levinson-type updating algorithm can be applied to recursively estimate models of increasing order p p . The asymptotic distribution of the estimators is established. Due to their computational simplicity, the proposed estimators allow one to perform finite-sample Monte Carlo tests. Simulation results show that the proposed estimators have lower bias and mean squared error than all alternative estimators (including Bayes-type estimators). The proposed estimators are applied to S&amp;P 500 daily returns (1928–2016). We find that an SV ( 3 ) (3) model is preferable to an SV(1) model.},
  archive      = {J_JOE},
  author       = {Md. Nazmul Ahsan and Jean-Marie Dufour},
  doi          = {10.1016/j.jeconom.2021.03.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {181-197},
  shortjournal = {J. Econ.},
  title        = {Simple estimators and inference for higher-order stochastic volatility models},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bootstrapping non-stationary stochastic volatility.
<em>JOE</em>, <em>224</em>(1), 161–180. (<a
href="https://doi.org/10.1016/j.jeconom.2021.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate to what extent the bootstrap can be applied to conditional mean models, such as regression or time series models , when the volatility of the innovations is random and possibly non-stationary. In fact, the volatility of many economic and financial time series displays persistent changes and possible non-stationarity. However, the theory of the bootstrap for such models has focused on deterministic changes of the unconditional variance and little is known about the performance and the validity of the bootstrap when the volatility is driven by a non-stationary stochastic process. This includes near-integrated exogenous volatility processes as well as near-integrated GARCH processes, where the conditional variance has a diffusion limit ; a further important example is the case where volatility exhibits infrequent jumps. This paper fills this gap in the literature by developing conditions for bootstrap validity in time series and regression models with non-stationary, stochastic volatility . We show that in such cases the distribution of bootstrap statistics (conditional on the data) is random in the limit. Consequently, the conventional approaches to proofs of bootstrap consistency, based on the notion of weak convergence in probability of the bootstrap statistic, fail to deliver the required validity results. Instead, we use the concept of ‘weak convergence in distribution’ to develop and establish novel conditions for validity of the wild bootstrap, conditional on the volatility process. We apply our results to several testing problems in the presence of non-stationary stochastic volatility, including testing in a location model, testing for structural change using CUSUM-type functionals, and testing for a unit root in autoregressive models . Importantly, we work under sufficient conditions for bootstrap validity that include the absence of statistical leverage effects, i.e., correlation between the error process and its future conditional variance. The results of the paper are illustrated using Monte Carlo simulations , which indicate that a wild bootstrap approach leads to size control even in small samples.},
  archive      = {J_JOE},
  author       = {H. Peter Boswijk and Giuseppe Cavaliere and Iliyan Georgiev and Anders Rahbek},
  doi          = {10.1016/j.jeconom.2021.01.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {161-180},
  shortjournal = {J. Econ.},
  title        = {Bootstrapping non-stationary stochastic volatility},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic spatial panel data models with common shocks.
<em>JOE</em>, <em>224</em>(1), 134–160. (<a
href="https://doi.org/10.1016/j.jeconom.2020.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies dynamic spatial panel data models with common shocks to deal with both weak and strong cross-sectional correlations. Weak correlations are captured by a spatial structure and strong correlations are captured by a factor structure. The proposed quasi-maximum likelihood estimator (QMLE) is capable of handling both types of cross sectional dependence . We provide a rigorous analysis for the asymptotic theory of the QMLE, demonstrating its desirable properties . Heteroskedasticity is explicitly allowed. This is important because QML is inconsistent in the presence of heteroskedasticity while homoskedasticity is imposed. We further show that when heteroskedasticity is estimated, the limiting variance of QMLE is not a sandwich form regardless of normality. Monte Carlo simulations show that the QMLE has good finite sample properties.},
  archive      = {J_JOE},
  author       = {Jushan Bai and Kunpeng Li},
  doi          = {10.1016/j.jeconom.2020.12.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {134-160},
  shortjournal = {J. Econ.},
  title        = {Dynamic spatial panel data models with common shocks},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference in time series models using smoothed-clustered
standard errors. <em>JOE</em>, <em>224</em>(1), 113–133. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a long run variance estimator for conducting inference in time series regression models that combines the nonparametric approach with a cluster approach. The basic idea is to divide the time periods into non-overlapping clusters. The long run variance estimator is constructed by first aggregating within clusters and then kernel smoothing across clusters or applying the nonparametric series method to the clusters with Type II discrete cosine transform . We develop an asymptotic theory for test statistics based on these “smoothed-clustered” long run variance estimators. We derive asymptotic results holding the number of clusters fixed and also treating the number of clusters as increasing with the sample size. For the kernel smoothing approach, these two asymptotic limits are different whereas for the cosine series approach, the two limits are the same. When clustering before kernel smoothing, we find that the “fixed-number-of-clusters” asymptotic approximation works well whether the number of clusters is small or large. Finite sample simulations suggest that the naive i . i . d . i.i.d. bootstrap mimics the fixed-number-of-clusters critical values. The simulations also suggest that clustering before kernel smoothing can reduce over-rejections caused by strong serial correlation at a cost of power. When there is a natural way of clustering, clustering can reduce over-rejection problems and achieve small gains in power for the kernel approach. In contrast, the cosine series approach does not benefit from clustering.},
  archive      = {J_JOE},
  author       = {Seunghwa Rho and Timothy J. Vogelsang},
  doi          = {10.1016/j.jeconom.2020.07.037},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {113-133},
  shortjournal = {J. Econ.},
  title        = {Inference in time series models using smoothed-clustered standard errors},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sieve estimation of option-implied state price density.
<em>JOE</em>, <em>224</em>(1), 88–112. (<a
href="https://doi.org/10.1016/j.jeconom.2021.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a nonparametric estimator for the state price density implied by a single cross-section of European options with different strikes and the same maturity. The proposed estimator has two distinctive features. First, it extracts information from both call and put options, as opposed to only call options . Second, it does not require estimating any second-order derivative; instead, it solves a constrained and penalized linear regression. The asymptotic analysis faces two challenges because the state price density is defined by a Fredholm integral equation of the first kind with an unbounded support, and the kernel function is unbounded and non-differentiable. We address these challenges by exploiting the structure of the option pricing problem. After establishing the estimator’s consistency and convergence rate, we apply it to estimate the state price densities implied by the S&amp;P500 index options and those by the VIX options. The sample period includes the recent financial crisis and the Great Recession , during which the turbulent market conditions imposed substantial challenges on the estimation. We show that the estimator can work with both daily and high-frequency observations. We also study whether the various features of this density can predict future asset returns and obtain positive findings. Finally, we apply the method to examine the causal effects of monetary policy announcements on the financial market, using high-frequency data.},
  archive      = {J_JOE},
  author       = {Junwen Lu and Zhongjun Qu},
  doi          = {10.1016/j.jeconom.2021.03.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {88-112},
  shortjournal = {J. Econ.},
  title        = {Sieve estimation of option-implied state price density},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosting high dimensional predictive regressions with time
varying parameters. <em>JOE</em>, <em>224</em>(1), 60–87. (<a
href="https://doi.org/10.1016/j.jeconom.2020.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensional predictive regressions are useful in wide range of applications. However, the theory is mainly developed assuming that the model is stationary with time invariant parameters. This is at odds with the prevalent evidence for parameter instability in economic time series, but theories for parameter instability are mainly developed for models with a small number of covariates . In this paper, we present two L 2 L2 boosting algorithms for estimating high dimensional models in which the coefficients are modeled as functions evolving smoothly over time and the predictors are locally stationary. The first method uses componentwise local constant estimators as base learner, while the second relies on componentwise local linear estimators. We establish consistency of both methods, and address the practical issues of choosing the bandwidth for the base learners and the number of boosting iterations. In an extensive application to macroeconomic forecasting with many potential predictors, we find that the benefits to modeling time variation are substantial and they increase with the forecast horizon. Furthermore, the timing of the benefits suggests that the Great Moderation is associated with substantial instability in the conditional mean of various economic series.},
  archive      = {J_JOE},
  author       = {Kashif Yousuf and Serena Ng},
  doi          = {10.1016/j.jeconom.2020.08.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {60-87},
  shortjournal = {J. Econ.},
  title        = {Boosting high dimensional predictive regressions with time varying parameters},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference after estimation of breaks. <em>JOE</em>,
<em>224</em>(1), 39–59. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an important class of econometric problems, researchers select a target parameter by maximizing the Euclidean norm of a data-dependent vector. Examples that can be cast into this frame include threshold regression models with estimated thresholds and structural break models with estimated break dates. Estimation and inference procedures that ignore the randomness of the target parameter can be severely biased and misleading when this randomness is non-negligible. This paper studies conditional and unconditional inference in such settings, accounting for the data-dependent choice of target parameters. We detail the construction of quantile-unbiased estimators and confidence sets with correct coverage, and prove their asymptotic validity under data generating process such that the target parameter remains random in the limit. We also provide a novel sample splitting approach that improves on conventional split-sample inference.},
  archive      = {J_JOE},
  author       = {Isaiah Andrews and Toru Kitagawa and Adam McCloskey},
  doi          = {10.1016/j.jeconom.2020.07.036},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {39-59},
  shortjournal = {J. Econ.},
  title        = {Inference after estimation of breaks},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical tests of a simple energy balance equation in a
synthetic model of cotrending and cointegration. <em>JOE</em>,
<em>224</em>(1), 22–38. (<a
href="https://doi.org/10.1016/j.jeconom.2020.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop new tests for the linear relationship between temperature and forcing, which is one of the most studied implications from a simple energy balance model. We consider a bivariate system of temperature and forcing where the time path of well-mixed-greenhouse-gases forcing is included as a potential common trend function in addition to a stochastic trend and a broken linear trend. Our test statistics are first devised as the likelihood ratio and then are modified to remove nuisance parameters in the asymptotic null distribution. The asymptotic null distribution and the required modification differ as to the existence of a stochastic trend. Thus, the test statistics are modified in two different ways and then are combined using the super-efficient estimator of the sum of autoregressive coefficients . The asymptotic critical values from the two cases remain close and we use the bigger one to control size for both cases. The proposed tests are applied to four temperature series and a forcing series. The null hypothesis of the linear relationship is not rejected with conventional sizes.},
  archive      = {J_JOE},
  author       = {Josep Lluís Carrion-i-Silvestre and Dukpa Kim},
  doi          = {10.1016/j.jeconom.2020.09.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {22-38},
  shortjournal = {J. Econ.},
  title        = {Statistical tests of a simple energy balance equation in a synthetic model of cotrending and cointegration},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous record laplace-based inference about the break
date in structural change models. <em>JOE</em>, <em>224</em>(1), 3–21.
(<a href="https://doi.org/10.1016/j.jeconom.2020.05.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building upon the continuous record asymptotic framework recently introduced by Casini and Perron (2020a) for inference in structural change models, we propose a Laplace-based (Quasi-Bayes) procedure for the construction of the estimate and confidence set for the date of a structural change. It is defined by an integration rather than an optimization-based method. A transformation of the least-squares criterion function is evaluated in order to derive a proper distribution, referred to as the Quasi-posterior. For a given choice of a loss function, the Laplace-type estimator is the minimizer of the expected risk with the expectation taken under the Quasi-posterior. Besides providing an alternative estimate that is more precise—lower mean absolute error (MAE) and lower root-mean squared error (RMSE)—than the usual least-squares one, the Quasi-posterior distribution can be used to construct asymptotically valid inference using the concept of Highest Density Region. The resulting Laplace-based inferential procedure is shown to have lower MAE and RMSE, and the confidence sets strike a better balance between empirical coverage rates and average lengths of the confidence sets relative to traditional long-span methods, whether the break size is small or large.},
  archive      = {J_JOE},
  author       = {Alessandro Casini and Pierre Perron},
  doi          = {10.1016/j.jeconom.2020.05.020},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {3-21},
  shortjournal = {J. Econ.},
  title        = {Continuous record laplace-based inference about the break date in structural change models},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Annals issue: PI-day honoring pierre perron. <em>JOE</em>,
<em>224</em>(1), 1–2. (<a
href="https://doi.org/10.1016/j.jeconom.2021.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Serena Ng and Zhongjun Qu and Timothy Vogelsang},
  doi          = {10.1016/j.jeconom.2021.05.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-2},
  shortjournal = {J. Econ.},
  title        = {Annals issue: PI-day honoring pierre perron},
  volume       = {224},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Illegal drugs, education, and labor market outcomes.
<em>JOE</em>, <em>223</em>(2), 454–484. (<a
href="https://doi.org/10.1016/j.jeconom.2019.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate the causal effects of consuming illegal drugs on educational attainment, employment, and wages. To identify these effects we develop and estimate a dynamic structural model to jointly consider decisions of whether to consume drugs, attend school, participate in the labor force, and save. Using data from the National Longitudinal Survey of Youth 1997 (NLSY97), we focus our analysis on males; the period of analysis begins at age 13, when they are young enough to have had no experience with drugs. Contrary to findings in the literature, non-drug users have higher wages than marijuana and/or hard drug users. This effect is small for individuals who consume marijuana in low doses but increases with the frequency of drug use. Results from a counterfactual experiment suggest that a 30 percent increase in the price of marijuana each period would reduce the number of marijuana consumers among the 13- to 30-year-olds by 16 percent. Individuals who are dissuaded from consuming marijuana due to the higher price would increase their level of education, their annual income, and work more.},
  archive      = {J_JOE},
  author       = {Alvaro Mezza and Moshe Buchinsky},
  doi          = {10.1016/j.jeconom.2019.03.009},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {454-484},
  shortjournal = {J. Econ.},
  title        = {Illegal drugs, education, and labor market outcomes},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Labor market search, informality, and on-the-job human
capital accumulation. <em>JOE</em>, <em>223</em>(2), 433–453. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a search and matching model where firms and workers produce output that depends both on match-specific productivity and worker-specific human capital. The human capital is accumulated while working but depreciates while searching for a job. Jobs can be formal or informal. The model is estimated on labor market data for Mexico. Human capital accumulation is responsible for more than half of the overall value of production, and upgrades more quickly while working formally than informally. Policy experiments reveal that human capital accumulation magnifies the negative impact on productivity of the labor market institutions that give rise to informality.},
  archive      = {J_JOE},
  author       = {Matteo Bobba and Luca Flabbi and Santiago Levy and Mauricio Tejada},
  doi          = {10.1016/j.jeconom.2019.05.026},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {433-453},
  shortjournal = {J. Econ.},
  title        = {Labor market search, informality, and on-the-job human capital accumulation},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effects of taxes and safety net pensions on life-cycle labor
supply, savings and human capital: The case of australia. <em>JOE</em>,
<em>223</em>(2), 401–432. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We structurally estimate a life-cycle model of consumption, labor supply and retirement, using data from the Australian HILDA panel. We use the model to evaluate effects of Australia’s Age Pension system and income tax policy on labor supply, consumption and retirement. Our model accounts for human capital, savings, uninsurable wage risk and credit constraints . We account for “bunching” of hours by assuming a discrete set of hours levels, and we investigate labor supply on both the intensive and extensive margins. Our model allows us to quantify the effects of anticipated and unanticipated tax and pension policy changes at different points of the life-cycle. Our results imply that Australia’s Age Pension system as currently designed is poorly targeted. Our simulations suggest that a doubling of taper rates, combined with a 5.9\% reduction of income tax rates , would be budget neutral and Pareto improving.},
  archive      = {J_JOE},
  author       = {Fedor Iskhakov and Michael Keane},
  doi          = {10.1016/j.jeconom.2020.01.023},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {401-432},
  shortjournal = {J. Econ.},
  title        = {Effects of taxes and safety net pensions on life-cycle labor supply, savings and human capital: The case of australia},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bidding frictions in ascending auctions. <em>JOE</em>,
<em>223</em>(2), 376–400. (<a
href="https://doi.org/10.1016/j.jeconom.2019.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops an approach for identifying and estimating the distribution of valuations in ascending auctions where an indeterminate number of bidders have an unknown number of bidding opportunities. To finesse the complications for identification and estimation due to multiple equilibria, our empirical analysis is based on the fact that bidders play undominated strategies in every equilibrium. We apply the model to a monthly financial market in which local banks compete for deposit securities. This market features frequent jump bidding and winning bids well above the highest losing bid, suggesting standard empirical approaches for ascending auctions may not be suitable. We find that frictions are costly both for revenue and allocative efficiency.},
  archive      = {J_JOE},
  author       = {Aaron Barkley and Joachim R. Groeger and Robert A. Miller},
  doi          = {10.1016/j.jeconom.2019.11.009},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {376-400},
  shortjournal = {J. Econ.},
  title        = {Bidding frictions in ascending auctions},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The likelihood of mixed hitting times. <em>JOE</em>,
<em>223</em>(2), 361–375. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for computing the likelihood of a mixed hitting-time model that specifies durations as the first time a latent Lévy process crosses a heterogeneous threshold. This likelihood is not generally known in closed form, but its Laplace transform is. Our approach to its computation relies on numerical methods for inverting Laplace transforms that exploit special properties of the first passage times of Lévy processes. We use our method to implement a maximum likelihood estimator of the mixed hitting-time model in MATLAB . We illustrate the application of this estimator with an analysis of Kennan’s (1985) strike data.},
  archive      = {J_JOE},
  author       = {Jaap H. Abbring and Tim Salimans},
  doi          = {10.1016/j.jeconom.2019.08.017},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {361-375},
  shortjournal = {J. Econ.},
  title        = {The likelihood of mixed hitting times},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving dynamic discrete choice models using smoothing and
sieve methods. <em>JOE</em>, <em>223</em>(2), 328–360. (<a
href="https://doi.org/10.1016/j.jeconom.2020.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to combine smoothing, simulations and sieve approximations to solve for either the integrated or expected value function in a general class of dynamic discrete choice (DDC) models. We use importance sampling to approximate the Bellman operators defining the two functions. The random Bellman operators, and therefore also the corresponding solutions, are generally non-smooth which is undesirable. To circumvent this issue, we introduce smoothed versions of the random Bellman operators and solve for the corresponding smoothed value functions using sieve methods. We also show that one can avoid using sieves by generalizing and adapting the “self-approximating” method of Rust (1997b) to our setting. We provide an asymptotic theory for both approximate solution methods and show that they converge with N N -rate, where N N is number of Monte Carlo draws, towards Gaussian processes . We examine their performance in practice through a set of numerical experiments and find that both methods perform well with the sieve method being particularly attractive in terms of computational speed and accuracy.},
  archive      = {J_JOE},
  author       = {Dennis Kristensen and Patrick K. Mogensen and Jong Myun Moon and Bertel Schjerning},
  doi          = {10.1016/j.jeconom.2020.02.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {328-360},
  shortjournal = {J. Econ.},
  title        = {Solving dynamic discrete choice models using smoothing and sieve methods},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric estimation of dynamic discrete choice models.
<em>JOE</em>, <em>223</em>(2), 312–327. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation of dynamic binary choice models in a semiparametric setting, in which the per-period utility functions are known up to a finite number of parameters, but the distribution of utility shocks is left unspecified. This semiparametric setup differs from most of the existing identification and estimation literature for dynamic discrete choice models . To show identification we derive and exploit a new recursive representation for the unknown quantile function of the utility shocks. Our estimators are straightforward to compute, and resemble classic closed-form estimators from the literature on semiparametric regression and average derivative estimation. Monte Carlo simulations demonstrate that our estimator performs well in small samples.},
  archive      = {J_JOE},
  author       = {Nicholas Buchholz and Matthew Shum and Haiqing Xu},
  doi          = {10.1016/j.jeconom.2020.01.024},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {312-327},
  shortjournal = {J. Econ.},
  title        = {Semiparametric estimation of dynamic discrete choice models},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sufficient statistics for unobserved heterogeneity in
structural dynamic logit models. <em>JOE</em>, <em>223</em>(2), 280–311.
(<a href="https://doi.org/10.1016/j.jeconom.2019.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the identification and estimation of structural parameters in dynamic panel data logit models where decisions are forward-looking and the joint distribution of unobserved heterogeneity and observable state variables is nonparametric, i.e., fixed-effects model. We consider models with two endogenous state variables: the lagged decision variable, and the time duration in the last choice. This class of models includes as particular cases important economic applications such as models of market entry–exit, occupational choice , machine replacement, inventory and investment decisions, or dynamic demand of differentiated products. We prove the identification of the structural parameters using a conditional likelihood approach. The structure of the model implies that there is a sufficient statistic such that the likelihood function conditional on this statistic no longer depends on the unobserved heterogeneity – neither through the current utility nor through the continuation value of the forward-looking decision problem – but still depends on the structural parameters. We apply this estimator to a machine replacement model.},
  archive      = {J_JOE},
  author       = {Victor Aguirregabiria and Jiaying Gu and Yao Luo},
  doi          = {10.1016/j.jeconom.2019.07.010},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {280-311},
  shortjournal = {J. Econ.},
  title        = {Sufficient statistics for unobserved heterogeneity in structural dynamic logit models},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overview: Implementation of structural dynamic models:
Methodology and applications. <em>JOE</em>, <em>223</em>(2), 277–279.
(<a href="https://doi.org/10.1016/j.jeconom.2021.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Michael Keane ( The guest editors ) and Dennis Kristensen and Fedor Iskhakov and Bertel Schjerning},
  doi          = {10.1016/j.jeconom.2021.03.009},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {277-279},
  shortjournal = {J. Econ.},
  title        = {Overview: implementation of structural dynamic models: methodology and applications},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient estimation and filtering for multivariate
jump–diffusions. <em>JOE</em>, <em>223</em>(1), 251–275. (<a
href="https://doi.org/10.1016/j.jeconom.2020.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops estimators of the transition density, filters, and parameters of multivariate jump–diffusion models. The drift, volatility, jump intensity, and jump magnitude are allowed to be state-dependent and non-affine. It is not necessary to diagonalize the volatility matrix. Our density and filter estimators converge at the canonical rate typically associated with exact Monte Carlo estimation. Our parameter estimators have the same asymptotic distribution as maximum likelihood estimators , which are often intractable for the class of models we consider. The results of this paper enable the empirical analysis of previously intractable models of asset prices and economic time series.},
  archive      = {J_JOE},
  author       = {François Guay and Gustavo Schwenkler},
  doi          = {10.1016/j.jeconom.2020.09.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {251-275},
  shortjournal = {J. Econ.},
  title        = {Efficient estimation and filtering for multivariate jump–diffusions},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Macroeconomic uncertainty prices when beliefs are tenuous.
<em>JOE</em>, <em>223</em>(1), 222–250. (<a
href="https://doi.org/10.1016/j.jeconom.2019.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investors face uncertainty over models when they do not know which member of a set of well-defined “structured models” is best. They face uncertainty about models when they suspect that all of the structured models might be misspecified. We refer to worries about the first type of ignorance as ambiguity concerns and worries about the second type as misspecification concerns . These two types of ignorance about probability distributions of risks add what we call uncertainty components to equilibrium prices of those risks. A quantitative example highlights a representative investor’s uncertainties about the size and persistence of macroeconomic growth rates. Our model of preferences under concerns about model ambiguity and misspecification puts nonlinearities into marginal valuations that induce time variations in market prices of uncertainty. These reflect the representative investor’s fears of high persistence of low growth rate states and low persistence of high growth rate states.},
  archive      = {J_JOE},
  author       = {Lars Peter Hansen and Thomas J. Sargent},
  doi          = {10.1016/j.jeconom.2019.11.010},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {222-250},
  shortjournal = {J. Econ.},
  title        = {Macroeconomic uncertainty prices when beliefs are tenuous},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model averaging prediction for time series models with a
diverging number of parameters. <em>JOE</em>, <em>223</em>(1), 190–221.
(<a href="https://doi.org/10.1016/j.jeconom.2020.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important problem with the model averaging approach is the choice of weights. In this paper, a generalized Mallows model averaging (GMMA) criterion for choosing weights is developed in the context of an infinite order autoregressive (AR( ∞ ∞ )) process. The GMMA method adapts to the circumstances in which the dimensions of candidate models can be large and increase with the sample size. The GMMA method is shown to be asymptotically optimal in the sense of achieving the lowest out-of-sample mean squared prediction error (MSPE) for both the independent-realization and the same-realization predictions, which, as a byproduct, solves a conjecture put forward by Hansen (2008) that the well-known Mallows model averaging criterion from Hansen (2007) is asymptotically optimal for predicting the future of a time series. The rate of the GMMA-based weight estimator tending to the optimal weight vector minimizing the independent-realization MSPE is derived as well. Both simulation experiment and real data analysis illustrate the merits of the GMMA method in the prediction of an AR( ∞ ∞ ) process.},
  archive      = {J_JOE},
  author       = {Jun Liao and Guohua Zou and Yan Gao and Xinyu Zhang},
  doi          = {10.1016/j.jeconom.2020.10.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {190-221},
  shortjournal = {J. Econ.},
  title        = {Model averaging prediction for time series models with a diverging number of parameters},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shrinkage for categorical regressors. <em>JOE</em>,
<em>223</em>(1), 161–189. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a flexible regularization approach that reduces point estimation risk of group means stemming from e.g. categorical regressors , (quasi-)experimental data or panel data models . The loss function is penalized by adding weighted squared ℓ 2 ℓ2 -norm differences between group location parameters and informative first stage estimates. Under quadratic loss, the penalized estimation problem has a simple interpretable closed-form solution that nests methods established in the literature on ridge regression, discretized support smoothing kernels and model averaging methods. We derive risk-optimal penalty parameters and propose a plug-in approach for estimation. The large sample properties are analyzed in an asymptotic local to zero framework by introducing a class of sequences for close and distant systems of locations that is sufficient for describing a large range of data generating processes. We provide the asymptotic distributions of the shrinkage estimators under different penalization schemes. The proposed plug-in estimator uniformly dominates the ordinary least squares estimator in terms of asymptotic risk if the number of groups is larger than three. Monte Carlo simulations reveal robust improvements over standard methods in finite samples. Real data examples of estimating time trends in a panel and a difference-in-differences study illustrate potential applications.},
  archive      = {J_JOE},
  author       = {Phillip Heiler and Jana Mareckova},
  doi          = {10.1016/j.jeconom.2020.07.051},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {161-189},
  shortjournal = {J. Econ.},
  title        = {Shrinkage for categorical regressors},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference without smoothing for large panels with
cross-sectional and temporal dependence. <em>JOE</em>, <em>223</em>(1),
125–160. (<a
href="https://doi.org/10.1016/j.jeconom.2020.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses inference in large panel data models in the presence of both cross-sectional and temporal dependence of unknown form. We are interested in making inferences that do not rely on the choice of any smoothing parameter as is the case with the often employed “ H A C HAC ” estimator for the covariance matrix . To that end, we propose a cluster estimator for the asymptotic covariance of the estimators and valid bootstrap schemes that do not require the selection of a bandwidth or smoothing parameter and accommodate the nonparametric nature of both temporal and cross-sectional dependence. Our approach is based on the observation that the spectral representation of the fixed effect panel data model is such that the errors become approximately temporally uncorrelated. Our proposed bootstrap schemes can be viewed as wild bootstraps in the frequency domain. We present some Monte Carlo simulations to shed some light on the small sample performance of our inferential procedure.},
  archive      = {J_JOE},
  author       = {Javier Hidalgo and Marcia Schafgans},
  doi          = {10.1016/j.jeconom.2020.10.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {125-160},
  shortjournal = {J. Econ.},
  title        = {Inference without smoothing for large panels with cross-sectional and temporal dependence},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model selection in utility-maximizing binary prediction.
<em>JOE</em>, <em>223</em>(1), 96–124. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum utility estimation proposed by Elliott and Lieli (2013) can be viewed as cost-sensitive binary classification ; thus, its in-sample overfitting issue is similar to that of perceptron learning. A utility-maximizing prediction rule (UMPR) is constructed to alleviate the in-sample overfitting of the maximum utility estimation. We establish non-asymptotic upper bounds on the difference between the maximal expected utility and the generalized expected utility of the UMPR. Simulation results show that the UMPR with an appropriate data-dependent penalty achieves larger generalized expected utility than common estimators in the binary classification if the conditional probability of the binary outcome is misspecified.},
  archive      = {J_JOE},
  author       = {Jiun-Hua Su},
  doi          = {10.1016/j.jeconom.2020.07.052},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {96-124},
  shortjournal = {J. Econ.},
  title        = {Model selection in utility-maximizing binary prediction},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrated likelihood based inference for nonlinear panel
data models with unobserved effects. <em>JOE</em>, <em>223</em>(1),
73–95. (<a href="https://doi.org/10.1016/j.jeconom.2020.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new integrated likelihood based approach for estimating panel data models when the unobserved individual effects enter the model nonlinearly. Unlike existing integrated likelihoods in the literature, the one we propose is closer to a genuine likelihood. Although the statistical theory for the proposed estimator is developed in an asymptotic setting where the number of individuals and the number of time periods both approach infinity, results from a simulation study suggest that our methodology can work very well even in moderately sized panels of short duration in both static and dynamic models.},
  archive      = {J_JOE},
  author       = {Martin Schumann and Thomas A. Severini and Gautam Tripathi},
  doi          = {10.1016/j.jeconom.2020.10.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {73-95},
  shortjournal = {J. Econ.},
  title        = {Integrated likelihood based inference for nonlinear panel data models with unobserved effects},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric estimation of large covariance matrices with
conditional sparsity. <em>JOE</em>, <em>223</em>(1), 53–72. (<a
href="https://doi.org/10.1016/j.jeconom.2020.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies estimation of covariance matrices with conditional sparse structure. We overcome the challenge of estimating dense matrices using a factor structure, the challenge of estimating large-dimensional matrices by postulating sparsity on covariance of random noises, and the challenge of estimating varying matrices by allowing factor loadings to smoothly change. A kernel-weighted estimation approach combined with generalised shrinkage is proposed. Under some technical conditions, we derive uniform consistency for the developed estimation method and obtain convergence rates. Numerical studies including simulation and an empirical application are presented to examine the finite-sample performance of the developed methodology.},
  archive      = {J_JOE},
  author       = {Hanchao Wang and Bin Peng and Degui Li and Chenlei Leng},
  doi          = {10.1016/j.jeconom.2020.09.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {53-72},
  shortjournal = {J. Econ.},
  title        = {Nonparametric estimation of large covariance matrices with conditional sparsity},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric regression with selectively missing
covariates. <em>JOE</em>, <em>223</em>(1), 28–52. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of regression with selectively observed covariates in a nonparametric framework. Our approach relies on instrumental variables that explain variation in the latent covariates but have no direct effect on selection. The regression function of interest is shown to be a weighted version of observed conditional expectation where the weighting function is a fraction of selection probabilities . Nonparametric identification of the fractional probability weight (FPW) function is achieved via a partial completeness assumption. We provide primitive functional form assumptions for partial completeness to hold. The identification result is constructive for the FPW series estimator. We derive the rate of convergence and also the pointwise asymptotic distribution . In both cases, the asymptotic performance of the FPW series estimator does not suffer from the inverse problem which derives from the nonparametric instrumental variable approach. In a Monte Carlo study , we analyze the finite sample properties of our estimator and we compare our approach to inverse probability weighting, which can be used alternatively for unconditional moment estimation. In the empirical application, we focus on two different applications. We estimate the association between income and health using linked data from the SHARE survey and administrative pension information and use pension entitlements as an instrument. In the second application we revisit the question how income affects the demand for housing based on data from the German Socio–Economic Panel Study (SOEP). In this application we use regional income information on the residential block level as an instrument. In both applications we show that income is selectively missing and we demonstrate that standard methods that do not account for the nonrandom selection process lead to significantly biased estimates for individuals with low income.},
  archive      = {J_JOE},
  author       = {Christoph Breunig and Peter Haan},
  doi          = {10.1016/j.jeconom.2020.07.050},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {28-52},
  shortjournal = {J. Econ.},
  title        = {Nonparametric regression with selectively missing covariates},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Indirect inference for locally stationary models.
<em>JOE</em>, <em>223</em>(1), 1–27. (<a
href="https://doi.org/10.1016/j.jeconom.2020.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the use of indirect inference estimation to conduct inference in complex locally stationary models. We develop a local indirect inference algorithm and establish the asymptotic properties of the proposed estimator. Due to the nonparametric nature of locally stationary models, the resulting indirect inference estimator exhibits nonparametric rates of convergence. We validate our methodology with simulation studies in the confines of a locally stationary moving average model and a new locally stationary multiplicative stochastic volatility model . Using this indirect inference methodology and the new locally stationary volatility model, we obtain evidence of non-linear, time-varying volatility trends for monthly returns on several Fama–French portfolios.},
  archive      = {J_JOE},
  author       = {David T. Frazier and Bonsoo Koo},
  doi          = {10.1016/j.jeconom.2020.08.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-27},
  shortjournal = {J. Econ.},
  title        = {Indirect inference for locally stationary models},
  volume       = {223},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Valid inference for treatment effect parameters under
irregular identification and many extreme propensity scores.
<em>JOE</em>, <em>222</em>(2), 1083–1108. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a framework for conducting valid inference for causal parameters without imposing strong variance or support restrictions on the propensity score. In particular, it covers the case of irregularly identified treatment effect parameters. We provide limit theorems for inverse probability weighting and doubly robust estimation of causal or counterfactual parameters that do not rely on trimming approaches. By construction the limiting distributions of these estimators belong to the alpha-stable class which implies that standard inference methods such as the nonparametric bootstrap are inconsistent. We propose an adaptive version of the m m -out-of- n n bootstrap that is robust to all types of identification and a bootstrap aggregation method for the optimal m m choice. Monte Carlo simulations suggest that the modified resampling method compares favorably to conventional methods in finite samples. The method is applied to a re-analysis of the causal impact of right heart catheterization on survival rates.},
  archive      = {J_JOE},
  author       = {Phillip Heiler and Ekaterina Kazak},
  doi          = {10.1016/j.jeconom.2020.03.025},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {1083-1108},
  shortjournal = {J. Econ.},
  title        = {Valid inference for treatment effect parameters under irregular identification and many extreme propensity scores},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bounding the difference between true and nominal rejection
probabilities in tests of hypotheses about instrumental variables
models. <em>JOE</em>, <em>222</em>(2), 1057–1082. (<a
href="https://doi.org/10.1016/j.jeconom.2020.08.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a simple method for carrying out inference in a wide variety of possibly nonlinear IV models under weak assumptions . The method provides a finite-sample bound on the difference between the true and nominal probabilities of rejecting a correct null hypothesis. The method is a non-Studentized version of the S S test of Stock and Wright (2000) but is implemented and analyzed differently. It does not require restrictive distributional assumptions, linearity of the estimated model, simultaneous equations, or information about whether the instruments are strong or weak. It can be applied to quantile IV models that may be nonlinear and can be used to test a parametric IV model against a nonparametric alternative. It provides information about the relation between the “degree of weakness” of the instruments and the power of the test. The bound presented here holds in finite samples, regardless of the strength of the instruments.},
  archive      = {J_JOE},
  author       = {Joel L. Horowitz},
  doi          = {10.1016/j.jeconom.2020.08.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {1057-1082},
  shortjournal = {J. Econ.},
  title        = {Bounding the difference between true and nominal rejection probabilities in tests of hypotheses about instrumental variables models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving euler equations via two-stage nonparametric
penalized splines. <em>JOE</em>, <em>222</em>(2), 1024–1056. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel estimation-based approach to solving asset pricing models for both stationary and time-varying observations. Our method is robust to misspecification errors while inheriting a closed-form solution. By representing the Euler equation into a well-posed integral equation of the second kind, we propose a penalized two-stage nonparametric estimation method and establish its optimal convergence under mild conditions. With the merit of penalized splines, our estimate is less sensitive to the spline setting and we also design a fast data-driven algorithm to effectively tune the key smoother, i.e. the penalty amount. Our approach exhibits excellent finite sample performance. Using the US data from 1947 to 2017, we reinvestigate the return predictability and find that the estimated implied dividend yield significantly predicts lower future cash flows and higher interest rates at short horizons.},
  archive      = {J_JOE},
  author       = {Liyuan Cui and Yongmiao Hong and Yingxing Li},
  doi          = {10.1016/j.jeconom.2020.04.042},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {1024-1056},
  shortjournal = {J. Econ.},
  title        = {Solving euler equations via two-stage nonparametric penalized splines},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple and trustworthy cluster-robust GMM inference.
<em>JOE</em>, <em>222</em>(2), 993–1023. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new asymptotic theory for GMM estimation and inference in the presence of clustered dependence. The key feature of our alternative asymptotics is that the number of clusters G G is regarded as fixed as the sample size increases. Under the fixed- G G asymptotics, we show that the Wald and t t tests in two-step GMM are asymptotically pivotal only if we recenter the estimated moment process in the clustered covariance estimator (CCE). Also, the J J statistic , the trinity of two-step GMM statistics (QLR, LM, and Wald), and the t t statistic can be modified to have an asymptotic standard F F distribution or t t distribution. We suggest a finite-sample variance correction to further improve the accuracy of the F F and t t approximations. The proposed tests are very appealing to practitioners because the test statistics are simple modifications of conventional GMM test statistics, and critical values are readily available from F F and t t tables. No further simulations or resampling methods are needed. A Monte Carlo study shows that our proposed tests are more accurate than the conventional large- G G asymptotic inferences.},
  archive      = {J_JOE},
  author       = {Jungbin Hwang},
  doi          = {10.1016/j.jeconom.2020.07.048},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {993-1023},
  shortjournal = {J. Econ.},
  title        = {Simple and trustworthy cluster-robust GMM inference},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-varying model averaging. <em>JOE</em>, <em>222</em>(2),
974–992. (<a
href="https://doi.org/10.1016/j.jeconom.2020.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural changes often occur in economics and finance due to changes in preferences, technologies, institutional arrangements, policies, crises, etc. Improving forecast accuracy of economic time series with structural changes is a long-standing problem. Model averaging aims at providing an insurance against selecting a poor forecast model. All existing model averaging approaches in the literature are designed with constant (non-time-varying) combination weights. Little attention has been paid to time-varying model averaging, which is more realistic in economics under structural changes. This paper proposes a novel model averaging estimator which selects optimal time-varying combination weights by minimizing a local jackknife criterion. It is shown that the proposed time-varying jackknife model averaging (TVJMA) estimator is asymptotically optimal in the sense of achieving the lowest possible local squared error loss in a class of time-varying model averaging estimators. Under a set of regularity assumptions, the TVJMA estimator is T h Th -consistent. A simulation study and an empirical application highlight the merits of the proposed TVJMA estimator relative to a variety of popular estimators with constant model averaging weights and model selection.},
  archive      = {J_JOE},
  author       = {Yuying Sun and Yongmiao Hong and Tae-Hwy Lee and Shouyang Wang and Xinyu Zhang},
  doi          = {10.1016/j.jeconom.2020.02.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {974-992},
  shortjournal = {J. Econ.},
  title        = {Time-varying model averaging},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncovering heterogeneous social effects in binary choices.
<em>JOE</em>, <em>222</em>(2), 959–973. (<a
href="https://doi.org/10.1016/j.jeconom.2020.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We identify and estimate heterogeneous social effects within groups of individuals that make binary choices. These heterogeneous social effects, which include peer and contextual effects, are modeled through unobserved influence matrices that summarize how the members within each group affect each other’s outcomes. We recover parameters in social effects as well as the unknown influence matrices by exploiting how these matrices are linked to the reduced-form effects of multiple characteristics. Monte Carlo experiments show that a nested fixed-point maximum-likelihood estimator for the social effects has good finite-sample performance. Using a new dataset, we analyze how college roommates influence each other’s decisions to participate in volunteering activities. Our estimates reveal substantial heterogeneity in the social effects among these students.},
  archive      = {J_JOE},
  author       = {Zhongjian Lin and Xun Tang and Ning Neil Yu},
  doi          = {10.1016/j.jeconom.2020.08.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {959-973},
  shortjournal = {J. Econ.},
  title        = {Uncovering heterogeneous social effects in binary choices},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian robust chi-squared test for testing simple
hypotheses. <em>JOE</em>, <em>222</em>(2), 933–958. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new Bayesian chi-squared test based on an adjusted quadratic loss function for testing a simple null hypothesis. We show that the asymptotic null distribution of our suggested test is a central chi-squared distribution under some assumptions required for the Bayesian large sample theory . We refer to our test as the Bayesian robust chi-squared test, since it is robust to parametric misspecification in the alternative model. That is, the limiting null distribution of our test is a central chi-squared distribution irrespective of parametric misspecification in the alternative model. In addition to being robust to parametric misspecification, our test also shares properties of the test suggested by Li et al. (2015) based on a quadratic loss function . We provide four examples to illustrate the implementation of our suggested Bayesian test statistic.},
  archive      = {J_JOE},
  author       = {Osman Doğan and Süleyman Taşpınar and Anil K. Bera},
  doi          = {10.1016/j.jeconom.2020.07.046},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {933-958},
  shortjournal = {J. Econ.},
  title        = {A bayesian robust chi-squared test for testing simple hypotheses},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A weighted sieve estimator for nonparametric time series
models with nonstationary variables. <em>JOE</em>, <em>222</em>(2),
909–932. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of nonparametric regression models that includes deterministic time trends and both stationary and nonstationary stochastic processes (whose shocks are allowed to be mutually correlated). We propose a unified approach to estimation based on the weighted sieve method to tackle the issue of unbounded support of the covariates . This approach improves on the existing technology in terms of some key regularity conditions such as moment conditions and the α α -mixing coefficients for the stationary process . We establish self-normalized central limit theorems for the sieve estimator and other related quantities. Monte Carlo simulation confirms the theoretical results. We use our methodology to study the effect of CO 2 and solar irradiance on global sea level rise.},
  archive      = {J_JOE},
  author       = {Chaohua Dong and Oliver Linton and Bin Peng},
  doi          = {10.1016/j.jeconom.2020.03.024},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {909-932},
  shortjournal = {J. Econ.},
  title        = {A weighted sieve estimator for nonparametric time series models with nonstationary variables},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Limit theorems for network dependent random variables.
<em>JOE</em>, <em>222</em>(2), 882–908. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with cross-sectional dependence arising because observations are interconnected through an observed network. Following (Doukhan and Louhichi, 1999), we measure the strength of dependence by covariances of nonlinearly transformed variables. We provide a law of large numbers and central limit theorem for network dependent variables. We also provide a method of calculating standard errors robust to general forms of network dependence. For that purpose, we rely on a network heteroskedasticity and autocorrelation consistent (HAC) variance estimator , and show its consistency. The results rely on conditions characterized by tradeoffs between the rate of decay of dependence across a network and network’s denseness. Our approach can accommodate data generated by network formation models, random fields on graphs, conditional dependency graphs , and large functional-causal systems of equations.},
  archive      = {J_JOE},
  author       = {Denis Kojevnikov and Vadim Marmer and Kyungchul Song},
  doi          = {10.1016/j.jeconom.2020.05.019},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {882-908},
  shortjournal = {J. Econ.},
  title        = {Limit theorems for network dependent random variables},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bounds on distributional treatment effect parameters using
panel data with an application on job displacement. <em>JOE</em>,
<em>222</em>(2), 861–881. (<a
href="https://doi.org/10.1016/j.jeconom.2020.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops new techniques to bound distributional treatment effect parameters that depend on the joint distribution of potential outcomes — an object not identified by standard identifying assumptions such as selection on observables or even when treatment is randomly assigned. I show that panel data and an additional assumption on the dependence between untreated potential outcomes for the treated group over time (i) provide more identifying power for distributional treatment effect parameters than existing bounds and (ii) provide a more plausible set of conditions than existing methods that obtain point identification. I apply these bounds to study heterogeneity in the effect of job displacement during the Great Recession . Using standard techniques, I find that workers who were displaced during the Great Recession lost on average 34\% of their earnings relative to their counterfactual earnings had they not been displaced. Using the methods developed in the current paper, I also show that the average effect masks substantial heterogeneity across workers.},
  archive      = {J_JOE},
  author       = {Brantly Callaway},
  doi          = {10.1016/j.jeconom.2020.02.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {861-881},
  shortjournal = {J. Econ.},
  title        = {Bounds on distributional treatment effect parameters using panel data with an application on job displacement},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian MIDAS penalized regressions: Estimation, selection,
and prediction. <em>JOE</em>, <em>222</em>(1), 833–860. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian estimation and inference. In particular, to improve the prediction properties of the model and its sparse recovery ability, we consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. We establish good frequentist asymptotic properties of the posterior prediction error, we recover the optimal posterior contraction rate, and we show optimality of the posterior predictive density. Simulations show that the proposed models have good selection and forecasting performance in small samples, even when the design matrix presents cross-correlation. When applied to forecasting U.S. GDP, our penalized regressions can outperform many strong competitors. Results suggest that financial variables may have some, although very limited, short-term predictive content.},
  archive      = {J_JOE},
  author       = {Matteo Mogliani and Anna Simoni},
  doi          = {10.1016/j.jeconom.2020.07.022},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {833-860},
  shortjournal = {J. Econ.},
  title        = {Bayesian MIDAS penalized regressions: Estimation, selection, and prediction},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical asset pricing with multi-period disaster risk: A
simulation-based approach. <em>JOE</em>, <em>222</em>(1), 805–832. (<a
href="https://doi.org/10.1016/j.jeconom.2020.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simulation-based strategy to estimate and empirically assess a class of asset pricing models that account for rare but severe consumption contractions that can extend over multiple periods. Our approach expands the scope of prevalent calibration studies and tackles the inherent sample selection problem associated with measuring the effect of rare disaster risk on asset prices. An analysis based on postwar U.S. and historical multi-country panel data yields estimates of investor preference parameters that are economically plausible and robust with respect to alternative specifications. The estimated model withstands tests of validity; the model-implied key financial indicators and timing premium all have reasonable magnitudes. These findings suggest that the rare disaster hypothesis can help restore the nexus between the real economy and financial markets when allowing for multi-period disaster events. Our methodological contribution is a new econometric framework for empirical asset pricing with rare disaster risk.},
  archive      = {J_JOE},
  author       = {Jantje Sönksen and Joachim Grammig},
  doi          = {10.1016/j.jeconom.2020.08.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {805-832},
  shortjournal = {J. Econ.},
  title        = {Empirical asset pricing with multi-period disaster risk: A simulation-based approach},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linear IV regression estimators for structural dynamic
discrete choice models. <em>JOE</em>, <em>222</em>(1), 778–804. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In structural dynamic discrete choice models , unobserved or mis-measured state variables may lead to biased parameter estimates and misleading inference. In this paper, we show that instrumental variables can address such measurement problems when they relate to state variables that evolve exogenously from the perspective of individual agents (i.e., market-level states). We define a class of linear instrumental variables estimators that rely on Euler equations expressed in terms of conditional choice probabilities (ECCP estimators). These estimators do not require observing or modeling the agent’s entire information set, nor solving or simulating a dynamic program. As such, they are simple to implement and computationally light. We provide constructive arguments for the identification of model primitives , and establish the estimator’s consistency and asymptotic normality . Four applied examples serve to illustrate the ECCP approach’s implementation, advantages, and limitations: dynamic demand for durable goods , agricultural land use change, technology adoption, and dynamic labor supply. We illustrate the estimator’s good finite-sample performance in a Monte Carlo study , and we estimate a labor supply model empirically for taxi drivers in New York City.},
  archive      = {J_JOE},
  author       = {Myrto Kalouptsidi and Paul T. Scott and Eduardo Souza-Rodrigues},
  doi          = {10.1016/j.jeconom.2020.03.016},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {778-804},
  shortjournal = {J. Econ.},
  title        = {Linear IV regression estimators for structural dynamic discrete choice models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On factor models with random missing: EM estimation,
inference, and cross validation. <em>JOE</em>, <em>222</em>(1), 745–777.
(<a href="https://doi.org/10.1016/j.jeconom.2020.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation and inference in approximate factor models with random missing values. We show that with the low rank structure of the common component, we can estimate the factors and factor loadings consistently with the missing values replaced by zeros. We establish the asymptotic distributions of the resulting estimators and those based on the EM algorithm . We also propose a cross-validation-based method to determine the number of factors in factor models with or without missing values and justify its consistency. Simulations demonstrate that our cross validation method is robust to fat tails in the error distribution and significantly outperforms some existing popular methods in terms of correct percentage in determining the number of factors. An application to the factor-augmented regression models shows that a proper treatment of the missing values can improve the out-of-sample forecast of some macroeconomic variables .},
  archive      = {J_JOE},
  author       = {Sainan Jin and Ke Miao and Liangjun Su},
  doi          = {10.1016/j.jeconom.2020.08.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {745-777},
  shortjournal = {J. Econ.},
  title        = {On factor models with random missing: EM estimation, inference, and cross validation},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). (Machine) learning parameter regions. <em>JOE</em>,
<em>222</em>(1), 716–744. (<a
href="https://doi.org/10.1016/j.jeconom.2020.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How many random points from an identified set, a confidence set, or a highest posterior density set suffice to describe them? This paper argues that taking random draws from a parameter region in order to approximate its shape is a supervised learning problem (analogous to sampling pixels of an image to recognize it). Misclassification error – a common criterion in machine learning – provides an off-the-shelf tool to assess the quality of a given approximation. We say a parameter region can be learned if there is an algorithm that yields a misclassification error of at most ϵ with probability at least 1−δ , regardless of the sampling distribution. We show that learning a parameter region is possible if and only if its potential shapes are not too complex. Moreover, the tightest band that contains a d -dimensional parameter region is always learnable from the inside (in a sense we make precise), with at least max(1−ϵ)ln1∕δ,(3∕16)d∕ϵ draws, but at most min{2dln(2d∕δ),exp(1)(2d+ln(1∕δ))}∕ϵ . These bounds grow linearly in the dimension of the parameter region, and are uniform with respect to its true shape. We illustrate the usefulness of our results using structural vector autoregressions . We show how many orthogonal matrices are necessary/sufficient to evaluate the impulse responses’ identified set and how many ‘shotgun plots’ to report when conducting joint inference on impulse responses.},
  archive      = {J_JOE},
  author       = {José Luis Montiel Olea and James Nesbit},
  doi          = {10.1016/j.jeconom.2020.06.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {716-744},
  shortjournal = {J. Econ.},
  title        = {(Machine) learning parameter regions},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric estimation of jump diffusion models.
<em>JOE</em>, <em>222</em>(1), 688–715. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops the asymptotics for nonparametric kernel estimators of local time, drift and volatilities, and Lévy measure in jump diffusion models . Our asymptotics are developed in a very general set-up, allowing the sample span to increase as the sampling interval decreases, and without assuming stationarity . For drift and volatilities, we analyze both local constant and local linear estimators. We consider not only estimators for instantaneous conditional second moment, but also threshold estimators to disentangle diffusive and jump volatilities. The optimal bandwidths are provided for all these estimators.},
  archive      = {J_JOE},
  author       = {Joon Y. Park and Bin Wang},
  doi          = {10.1016/j.jeconom.2020.07.020},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {688-715},
  shortjournal = {J. Econ.},
  title        = {Nonparametric estimation of jump diffusion models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the validity of akaike’s identity for random fields.
<em>JOE</em>, <em>222</em>(1), 676–687. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For univariate stationary and centered time series ( X t ) t ∈ Z (Xt)t∈Z , Akaike’s identity links the inverse of the Yule–Walker matrix Γ ( p ) = E ( X X ′ ) Γ(p)=E(XX′) , where X = ( X t − 1 , … , X t − p ) ′ X=(Xt−1,…,Xt−p)′ , to the corresponding finite predictor coefficients. It reads as a Cholesky-type factorization Γ ( p ) − 1 = L ( p ) ′ Σ ( p ) − 1 L ( p ) Γ(p)−1=L(p)′Σ(p)−1L(p) , where L ( p ) L(p) is lower-triangular and Σ ( p ) − 1 Σ(p)−1 is diagonal. Whereas this Cholesky-type factorization exists whenever Γ ( p ) Γ(p) is positive definite, Akaike derived a meaningful interpretation of L ( p ) L(p) and Σ ( p ) − 1 Σ(p)−1 in terms of finite predictor coefficients. It is useful in many applications and is particularly crucial to derive asymptotic theory for Berk’s spectral density estimator. We investigate the validity of a bona fide extension of Akaike’s identity to univariate stationary random fields ( X t ̲ ) t ̲ ∈ Z d (Xt̲)t̲∈Zd . An analogue of Akaike’s result is shown to hold true if and only if the corresponding Yule–Walker matrix Γ ( p ) Γ(p) is Toeplitz. This condition turns out to be very restrictive and rules out commonly used unilateral autoregressive models in Z d Zd (such as half-plane or quarter-plane fits in Z 2 Z2 ). Instead, we prove that the corresponding Cholesky-type factorization Γ ( p ) − 1 = L ( p ) ′ Σ ( p ) − 1 L ( p ) Γ(p)−1=L(p)′Σ(p)−1L(p) for random fields does establish a link to a different, but practically less relevant sequence of so-called reshaping past projection coefficients.},
  archive      = {J_JOE},
  author       = {Carsten Jentsch and Marco Meyer},
  doi          = {10.1016/j.jeconom.2020.04.044},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {676-687},
  shortjournal = {J. Econ.},
  title        = {On the validity of akaike’s identity for random fields},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic decisions under subjective expectations: A
structural analysis. <em>JOE</em>, <em>222</em>(1), 645–675. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study dynamic discrete choice models without assuming rational expectations. Agents’ beliefs about state transitions are subjective, unknown, and may differ from their objective counterparts. We show that agents’ preferences and subjective beliefs are identified in both finite and infinite horizon models. We estimate the model primitives via maximum likelihood estimation and demonstrate the good performance of the estimator by Monte Carlo experiments. Using the Panel Study of Income Dynamics (PSID) data, we illustrate our method in an analysis of women’s labor participation. We find that workers do not hold rational expectations about income transitions.},
  archive      = {J_JOE},
  author       = {Yonghong An and Yingyao Hu and Ruli Xiao},
  doi          = {10.1016/j.jeconom.2020.04.046},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {645-675},
  shortjournal = {J. Econ.},
  title        = {Dynamic decisions under subjective expectations: A structural analysis},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing constancy in varying coefficient models.
<em>JOE</em>, <em>222</em>(1), 625–644. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a coefficients constancy test in semi-varying coefficient models that only needs to estimate the restricted coefficients under the null hypothesis. The test statistic resembles the union-intersection test after ordering the data according to the varying coefficients’ explanatory variable . This statistic depends on a trimming parameter that can be chosen by a data-driven calibration method we propose. A bootstrap test is justified under fairly general regularity conditions . Under more restrictive assumptions, the critical values can be tabulated, and trimming is unnecessary. The finite sample performance is studied by means of Monte Carlo experiments, and a real data application for modeling education returns.},
  archive      = {J_JOE},
  author       = {Miguel A. Delgado and Luis A. Arteaga-Molina},
  doi          = {10.1016/j.jeconom.2020.07.041},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {625-644},
  shortjournal = {J. Econ.},
  title        = {Testing constancy in varying coefficient models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing for observation-dependent regime switching in
mixture autoregressive models. <em>JOE</em>, <em>222</em>(1), 601–624.
(<a href="https://doi.org/10.1016/j.jeconom.2020.04.048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing for regime switching when the regime switching probabilities are specified either as constants (‘mixture models’) or are governed by a finite-state Markov chain (‘Markov switching models’) are long-standing problems that have also attracted recent interest. This paper considers testing for regime switching when the regime switching probabilities are time-varying and depend on observed data (‘observation-dependent regime switching’). Specifically, we consider the likelihood ratio test for observation-dependent regime switching in mixture autoregressive models . The testing problem is highly nonstandard, involving unidentified nuisance parameters under the null , parameters on the boundary, singular information matrices, and higher-order approximations of the log-likelihood. We derive the asymptotic null distribution of the likelihood ratio test statistic in a general mixture autoregressive setting using high-level conditions that allow for various forms of dependence of the regime switching probabilities on past observations, and we illustrate the theory using two particular mixture autoregressive models. The likelihood ratio test has a nonstandard asymptotic distribution that can easily be simulated, and Monte Carlo studies show the test to have good finite sample size and power properties.},
  archive      = {J_JOE},
  author       = {Mika Meitz and Pentti Saikkonen},
  doi          = {10.1016/j.jeconom.2020.04.048},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {601-624},
  shortjournal = {J. Econ.},
  title        = {Testing for observation-dependent regime switching in mixture autoregressive models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Max-linear regression models with regularization.
<em>JOE</em>, <em>222</em>(1), 579–600. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the newly developed max-linear competing copula factor models and max-stable nonlinear time series models , we propose a new class of max-linear regression models to take advantages of easy interpretable features embedded in linear regression models. It can be seen that linear relation is a special case of max-linear relation. We develop an EM algorithm based maximum likelihood estimation procedure. The consistency and asymptotics of the estimators for parameters are proved. To advance max-linear models to deal with high dimensional predictors, we adopt the common strategy of regularization in the high dimensional regression literature. We demonstrate the broad applicability of max-linear models using simulation examples and real applications in econometric and business modeling. The results, in terms of predictability, show a significant improvement compared with solely using regular regression models and other existing machine learning methods. The results enhance our understanding of the relationship between the response variable and the predictors, and among the predictors as well.},
  archive      = {J_JOE},
  author       = {Qiurong Cui and Yuqing Xu and Zhengjun Zhang and Vincent Chan},
  doi          = {10.1016/j.jeconom.2020.07.017},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {579-600},
  shortjournal = {J. Econ.},
  title        = {Max-linear regression models with regularization},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The wisdom of the crowd and prediction markets.
<em>JOE</em>, <em>222</em>(1), 561–578. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to digital innovation , the wisdom of the crowd, which aims at gathering information (e.g. Wikipedia) and making a prediction (e.g. using prediction markets) from a group’s aggregated inputs, has been widely appreciated. An innovative survey design, based on a Bayesian learning framework, called the Bayesian truth serum (BTS), was proposed previously to reduce the bias in the simple majority rule by asking additional survey questions. A natural question is whether we can extend the BTS framework to prediction markets (not just polls). To do so, this paper proposes two estimators, one based on a prediction market alone and the other based on both the market and a poll question. We show that both estimators are consistent within the BTS framework, under different sets of regularity conditions . Simulations are conducted to examine the convergence of different estimators. A real data set of sports betting is used to demonstrate the effectiveness of one estimator.},
  archive      = {J_JOE},
  author       = {Min Dai and Yanwei Jia and Steven Kou},
  doi          = {10.1016/j.jeconom.2020.07.016},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {561-578},
  shortjournal = {J. Econ.},
  title        = {The wisdom of the crowd and prediction markets},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autoregressive models for matrix-valued time series.
<em>JOE</em>, <em>222</em>(1), 539–560. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In finance , economics and many other fields, observations in a matrix form are often generated over time. For example, a set of key economic indicators are regularly reported in different countries every quarter. The observations at each quarter neatly form a matrix and are observed over consecutive quarters. Dynamic transport networks with observations generated on the edges can be formed as a matrix observed over time. Although it is natural to turn the matrix observations into long vectors, then use the standard vector time series 2 models for analysis, it is often the case that the columns and rows of the matrix represent different types of structures that are closely interplayed. In this paper we follow the autoregression for modeling time series and propose a novel matrix autoregressive model in a bilinear form that maintains and utilizes the matrix structure to achieve a substantial dimensional reduction, as well as more interpretability . Probabilistic properties of the models are investigated. Estimation procedures with their theoretical properties are presented and demonstrated with simulated and real examples.},
  archive      = {J_JOE},
  author       = {Rong Chen and Han Xiao and Dan Yang},
  doi          = {10.1016/j.jeconom.2020.07.015},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {539-560},
  shortjournal = {J. Econ.},
  title        = {Autoregressive models for matrix-valued time series},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New testing approaches for mean–variance predictability.
<em>JOE</em>, <em>222</em>(1), 516–538. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose parametric tests for serial correlation in levels and squares that exploit the non-normality of financial returns. Our tests are robust to distributional misspecification. Furthermore, our mean predictability tests can be robustified against time-varying volatility. Local power analyses confirm their gains over existing methods, while Monte Carlo exercises assess their finite sample reliability. We apply our tests to quarterly returns on the five Fama–French factors for international stocks, whose distributions are mostly symmetric but fat-tailed. Our results highlight noticeable differences across regions and factors and confirm the numerical sensitivity of the usual tests to influential observations.},
  archive      = {J_JOE},
  author       = {Gabriele Fiorentini and Enrique Sentana},
  doi          = {10.1016/j.jeconom.2020.07.014},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {516-538},
  shortjournal = {J. Econ.},
  title        = {New testing approaches for mean–variance predictability},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High dimensional minimum variance portfolio estimation under
statistical factor models. <em>JOE</em>, <em>222</em>(1), 502–515. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a high dimensional minimum variance portfolio estimator under statistical factor models, and show that our estimated portfolio enjoys sharp risk consistency. Our approach relies on properly integrating ℓ 1 ℓ1 constraint on portfolio weights with an appropriate covariance matrix estimator. In terms of covariance matrix estimation, we extend the theoretical results of POET (Fan et al., 2013) to a setting that is coherent with principal component analysis . Simulation and extensive empirical studies on S&amp;P 100 Index constituent stocks demonstrate favorable performance of our MVP estimator compared with benchmark portfolios.},
  archive      = {J_JOE},
  author       = {Yi Ding and Yingying Li and Xinghua Zheng},
  doi          = {10.1016/j.jeconom.2020.07.013},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {502-515},
  shortjournal = {J. Econ.},
  title        = {High dimensional minimum variance portfolio estimation under statistical factor models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient estimation of multivariate semi-nonparametric
GARCH filtered copula models. <em>JOE</em>, <em>222</em>(1), 484–501.
(<a href="https://doi.org/10.1016/j.jeconom.2020.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers estimation of semi-nonparametric GARCH filtered copula models in which the individual time series are modeled by semi-nonparametric GARCH and the joint distributions of the multivariate standardized innovations are characterized by parametric copulas with nonparametric marginal distributions. The models extend those of Chen and Fan (2006) to allow for semi-nonparametric conditional means and volatilities, which are estimated via the method of sieves. The fitted residuals are then used to estimate the copula parameters and the marginal densities of the standardized innovations jointly via the sieve maximum likelihood (SML). We show that, even using nonparametric filtered data, the copula parameters estimated via our SML and the two-step procedure of Chen and Fan (2006) are still root-n consistent and asymptotically normal, and the asymptotic variances of both estimators do not depend on the nonparametric filtering errors. Even more surprisingly, our SML copula estimator using the filtered data achieves the full semiparametric efficiency bound as if the standardized innovations were directly observed. These nice properties lead to simple and more accurate estimation of Value-at-Risk (VaR) for multivariate financial data with flexible dynamics, contemporaneous tail dependence and asymmetric distributions of innovations. Monte Carlo studies demonstrate that our SML estimators of the copula parameters and the marginal distributions of the standardized innovations have smaller variances and smaller mean squared errors compared to those of the two-step estimators in finite samples. A real data application is presented.},
  archive      = {J_JOE},
  author       = {Xiaohong Chen and Zhuo Huang and Yanping Yi},
  doi          = {10.1016/j.jeconom.2020.07.012},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {484-501},
  shortjournal = {J. Econ.},
  title        = {Efficient estimation of multivariate semi-nonparametric GARCH filtered copula models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The implied arbitrage mechanism in financial markets.
<em>JOE</em>, <em>222</em>(1), 468–483. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The no-arbitrage condition is a cornerstone concept in financial market research. However, the arbitrage mechanism that is inherent in the trading process for related securities, is not readily observable. We develop a generalized smooth-transition vector error-correction model, or GST-VECM, to estimate the arbitrage mechanism from financial market data. The GST-VECM can (i) back out the implied no-arbitrage band, (ii) estimate arbitrage intensity for upper and lower bound violations, and (iii) accommodate convergence risk for statistical arbitrage. Using the introduction of CSI300 ETF trading in China as a natural experiment, we estimate the GST-VECM to reveal some insight into how a microstructural policy, by altering the index arbitrage mechanism, affects the pricing link between spot and futures markets.},
  archive      = {J_JOE},
  author       = {Shiyi Chen and Michael T. Chng and Qingfu Liu},
  doi          = {10.1016/j.jeconom.2020.07.011},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {468-483},
  shortjournal = {J. Econ.},
  title        = {The implied arbitrage mechanism in financial markets},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized aggregation of misspecified models: With an
application to asset pricing. <em>JOE</em>, <em>222</em>(1), 451–467.
(<a href="https://doi.org/10.1016/j.jeconom.2020.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generalized aggregation approach for model averaging. The entropy-based optimal criterion is a natural choice for aggregating information from many “globally” misspecified models as it adapts better to the underlying model uncertainty and obtains more robust approximations. Unlike almost all other approaches in the existing literature, we do not require a “reference model,” or a true data generation process contained in the set of models — neither implicitly nor in otherwise popular limiting forms. This shift in paradigm prioritizes stochastic optimization and aggregation of information about outcomes over parameter estimation of an optimally selected model. Stochastic optimization is based on a risk function of aggregators across models that satisfies oracle inequalities . Our generalized aggregators relax the common perfect substitutability of the candidate models, implicit in linear averaging and pooling. The aggregation weights are data-driven and obtained from a proper (Hellinger) distance measure. The empirical results illustrate the performance and economic significance of the aggregation approach in the context of stochastic discount factor models and inflation forecasting.},
  archive      = {J_JOE},
  author       = {Nikolay Gospodinov and Esfandiar Maasoumi},
  doi          = {10.1016/j.jeconom.2020.07.010},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {451-467},
  shortjournal = {J. Econ.},
  title        = {Generalized aggregation of misspecified models: With an application to asset pricing},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autoencoder asset pricing models. <em>JOE</em>,
<em>222</em>(1), 429–450. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new latent factor conditional asset pricing model. Like Kelly, Pruitt, and Su (KPS, 2019), our model allows for latent factors and factor exposures that depend on covariates such as asset characteristics. But, unlike the linearity assumption of KPS, we model factor exposures as a flexible nonlinear function of covariates . Our model retrofits the workhorse unsupervised dimension reduction device from the machine learning literature – autoencoder neural networks – to incorporate information from covariates along with returns themselves. This delivers estimates of nonlinear conditional exposures and the associated latent factors. Furthermore, our machine learning framework imposes the economic restriction of no-arbitrage. Our autoencoder asset pricing model delivers out-of-sample pricing errors that are far smaller (and generally insignificant) compared to other leading factor models.},
  archive      = {J_JOE},
  author       = {Shihao Gu and Bryan Kelly and Dacheng Xiu},
  doi          = {10.1016/j.jeconom.2020.07.009},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {429-450},
  shortjournal = {J. Econ.},
  title        = {Autoencoder asset pricing models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The observed asymptotic variance: Hard edges, and a
regression approach. <em>JOE</em>, <em>222</em>(1), 411–428. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High frequency financial data has become an essential component of the digital economy, yielding an increasing number of estimators. However, it is hard to reliably assess the uncertainty of such estimators. The Observed Asymptotic Variance (observed AVAR) is a non-parametric estimator for (squared) standard error in high frequency data. The device is related to observed information in likelihood theory , but in this case it is non-parametric and uses the high-frequency data structure. An earlier paper has developed the estimator in the case where edge effects are small to moderate. In practical data, it is often more realistic to assume that edge effects can be large, and this is the problem that we tackle in the current paper. We here find a regression approach to observed AVAR which is highly robust to large edges. This approach covers most high frequency estimators.},
  archive      = {J_JOE},
  author       = {Per A. Mykland and Lan Zhang},
  doi          = {10.1016/j.jeconom.2020.07.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {411-428},
  shortjournal = {J. Econ.},
  title        = {The observed asymptotic variance: Hard edges, and a regression approach},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Volatility analysis with realized GARCH-itô models.
<em>JOE</em>, <em>222</em>(1), 393–410. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a unified approach for modeling high-frequency financial data that can accommodate both the continuous-time jump–diffusion and discrete-time realized GARCH model by embedding the discrete realized GARCH structure in the continuous instantaneous volatility process. The key feature of the proposed model is that the corresponding conditional daily integrated volatility adopts an autoregressive structure, where both integrated volatility and jump variation serve as innovations. We name it as the realized GARCH-Itô model. Given the autoregressive structure in the conditional daily integrated volatility, we propose a quasi-likelihood function for parameter estimation and establish its asymptotic properties . To improve the parameter estimation, we propose a joint quasi-likelihood function that is built on the marriage of daily integrated volatility estimated by high-frequency data and nonparametric volatility estimator obtained from option data. We conduct a simulation study to check the finite sample performance of the proposed methodologies and an empirical study with the S&amp;P500 stock index and option data.},
  archive      = {J_JOE},
  author       = {Xinyu Song and Donggyu Kim and Huiling Yuan and Xiangyu Cui and Zhiping Lu and Yong Zhou and Yazhen Wang},
  doi          = {10.1016/j.jeconom.2020.07.007},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {393-410},
  shortjournal = {J. Econ.},
  title        = {Volatility analysis with realized GARCH-itô models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Closed-form implied volatility surfaces for stochastic
volatility models with jumps. <em>JOE</em>, <em>222</em>(1), 364–392.
(<a href="https://doi.org/10.1016/j.jeconom.2020.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a closed-form bivariate expansion of the shape characteristics of the implied volatility surface generated by a stochastic volatility model with jumps in returns. We use the expansion to analyse the impact on the shape of the implied volatility surface of the various features of the stochastic volatility model and to determine which stochastic volatility models are capable of reproducing the observed characteristics of the implied volatility market data.},
  archive      = {J_JOE},
  author       = {Yacine Aït-Sahalia and Chenxu Li and Chen Xu Li},
  doi          = {10.1016/j.jeconom.2020.07.006},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {364-392},
  shortjournal = {J. Econ.},
  title        = {Closed-form implied volatility surfaces for stochastic volatility models with jumps},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tail risk and return predictability for the japanese equity
market. <em>JOE</em>, <em>222</em>(1), 344–363. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the predictability of the Japanese equity market, focusing on the forecasting power of nonparametric volatility and tail risk measures obtained from options data on the S&amp;P 500 and Nikkei 225 market indices. The Japanese market is notoriously difficult to forecast using standard predictive indicators. We confirm that country-specific regressions for Japan – contrary to existing evidence for other national equity indices – produce insignificant predictability patterns. However, we also find that the U.S. option-implied tail risk measure provides significant forecast power both for the dollar–yen exchange rate and the Japanese excess returns, especially when measured in U.S. dollars. Thus, the dollar-denominated Japanese returns are, in fact, predictable through the identical mechanism as for other equity market indices, suggesting a high degree of global integration for the Japanese financial market.},
  archive      = {J_JOE},
  author       = {Torben G. Andersen and Viktor Todorov and Masato Ubukata},
  doi          = {10.1016/j.jeconom.2020.07.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {344-363},
  shortjournal = {J. Econ.},
  title        = {Tail risk and return predictability for the japanese equity market},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-varying general dynamic factor models and the
measurement of financial connectedness. <em>JOE</em>, <em>222</em>(1),
324–343. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new time-varying Generalized Dynamic Factor Model for high-dimensional, locally stationary time series . Estimation is based on dynamic principal component analysis jointly with singular VAR estimation, and extends to the locally stationary case the one-sided estimation method proposed by Forni et al. (2017) for stationary data. We prove consistency of our estimators of time-varying impulse response functions as both the sample size T T and the dimension n n of the time series grow to infinity. This approach is used in an empirical application in order to construct a time-varying measure of financial connectedness for a large panel of adjusted intra-day log ranges of stocks. We show that large increases in long-run connectedness are associated with the main financial turmoils. Moreover, we provide evidence of a significant heterogeneity in the dynamic responses to common shocks in time and over different scales, as well as across industrial sectors.},
  archive      = {J_JOE},
  author       = {Matteo Barigozzi and Marc Hallin and Stefano Soccorsi and Rainer von Sachs},
  doi          = {10.1016/j.jeconom.2020.07.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {324-343},
  shortjournal = {J. Econ.},
  title        = {Time-varying general dynamic factor models and the measurement of financial connectedness},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and inference in semiparametric quantile factor
models. <em>JOE</em>, <em>222</em>(1), 295–323. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a semiparametric quantile factor panel model that allows observed stock-specific characteristics to affect stock returns in a nonlinear time-varying way, extending Connor, Hagmann, and Linton (2012) to the quantile restriction case. We propose a sieve-based estimation methodology that is easy to implement. We provide tools for inference that are robust to the existence of moments and to the form of weak cross-sectional dependence in the idiosyncratic error term. We apply our method to daily stock return data where we find significant evidence of nonlinearity in many of the characteristic exposure curves.},
  archive      = {J_JOE},
  author       = {Shujie Ma and Oliver Linton and Jiti Gao},
  doi          = {10.1016/j.jeconom.2020.07.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {295-323},
  shortjournal = {J. Econ.},
  title        = {Estimation and inference in semiparametric quantile factor models},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmented factor models with applications to validating
market risk factors and forecasting bond risk premia. <em>JOE</em>,
<em>222</em>(1), 269–294. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study factor models augmented by observed covariates that have explanatory powers on the unknown factors. In financial factor models, the unknown factors can be reasonably well explained by a few observable proxies, such as the Fama–French factors. In diffusion index forecasts, identified factors are strongly related to several directly measurable economic variables such as consumption-wealth variable, financial ratios, and term spread. With those covariates , both the factors and loadings are identifiable up to a rotation matrix even only with a finite dimension. To incorporate the explanatory power of these covariates, we propose a smoothed or projected principal component analysis (PCA): (i) regress the data onto the observed covariates , and (ii) take the principal components of the fitted data to estimate the loadings and factors. This allows us to more accurately estimate the percentage of both explained and unexplained components in factors and thus to assess the explanatory power of covariates. We show that both the estimated factors and loadings can be estimated with improved rates of convergence compared to the benchmark method. The degree of improvement depends on the strength of the signals, representing the explanatory power of the covariates on the factors. The proposed estimator is robust to possibly heavy-tailed distributions. We apply the model to forecast US bond risk premia, and find that the observed macroeconomic characteristics contain strong explanatory powers of the factors. The gain of forecast is more substantial when the characteristics are incorporated to estimate the common factors than directly used for forecasts.},
  archive      = {J_JOE},
  author       = {Jianqing Fan and Yuan Ke and Yuan Liao},
  doi          = {10.1016/j.jeconom.2020.07.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {269-294},
  shortjournal = {J. Econ.},
  title        = {Augmented factor models with applications to validating market risk factors and forecasting bond risk premia},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial for the special issue on financial econometrics in
the age of the digital economy. <em>JOE</em>, <em>222</em>(1), 265–268.
(<a href="https://doi.org/10.1016/j.jeconom.2020.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Oliver Linton and Viktor Todorov and Zhengjun Zhang},
  doi          = {10.1016/j.jeconom.2020.07.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {265-268},
  shortjournal = {J. Econ.},
  title        = {Editorial for the special issue on financial econometrics in the age of the digital economy},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Epilogue. <em>JOE</em>, <em>222</em>(1), 261–263. (<a
href="https://doi.org/10.1016/j.jeconom.2021.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Daniel McFadden},
  doi          = {10.1016/j.jeconom.2021.03.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {261-263},
  shortjournal = {J. Econ.},
  title        = {Epilogue},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The browser war — analysis of markov perfect equilibrium in
markets with dynamic demand effects. <em>JOE</em>, <em>222</em>(1),
244–260. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a concentrated market for differentiated products exhibits dynamic demand effects due to inertia, contagion, or network externalities, forward-looking firms consider the strategic impact of investment, pricing, and other conduct that can tip the market by grabbing market share. When the contested market provides a line of defense for a more lucrative core market, a firm can have a powerful strategic incentive to capture and control the contested market, as a “loss leader” if necessary. We use this framework to analyze the browser war between Netscape and Microsoft. Adopting a Markov Perfect Equilibrium model to capture firms’ strategic behavior, we discuss the steps needed for such analysis. We compare as-is market trajectories with but-for trajectories under a counterfactual without “anticompetitive acts” deemed in violation of anti-trust law. Our empirical analysis uses incomplete and noisy public data. Consequently, specifications and results should be viewed only as illustrating the method in a highly parametrized model with restrictions that might not be robust. Access to complete and confidential firm data would result in a less restricted model with potentially different results.},
  archive      = {J_JOE},
  author       = {Mark Jenkins and Paul Liu and Rosa L. Matzkin and Daniel L. McFadden},
  doi          = {10.1016/j.jeconom.2020.07.034},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {244-260},
  shortjournal = {J. Econ.},
  title        = {The browser war — analysis of markov perfect equilibrium in markets with dynamic demand effects},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of endogenously sampled time series: The case of
commodity price speculation in the steel market. <em>JOE</em>,
<em>222</em>(1), 219–243. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating a Markov process that is endogenously sampled. We observe a discrete-time Markov process { p t } {pt} at a set of random times { t 1 , … , t n } {t1,…,tn} that depend on the outcome of a probabilistic sampling rule that depends on the state of the process and other observed covariates x t xt . We focus on a particular example where p t pt is the daily wholesale price of a standardized steel product. The endogenous sampling problem arises from the fact that we only observe p t pt on the days the firm purchases steel. We show how to solve this problem under two different assumptions about firm behavior: (1) optimality : the timing of steel purchases is governed by an optimal purchasing strategy that maximizes expected discounted profits, and (2) potential suboptimality: we allow the firm to use any randomized, Markovian purchasing strategy. In the latter case, the estimation problem becomes semi-parametric and we use the method of sieves to estimate a flexible parametric approximation to the firm’s purchasing behavior that best fits the data without imposing optimality. We show how estimation of this model becomes tractable under either of these assumptions using the method of simulated moments (MSM). We simulate realizations of wholesale steel prices and sample them in the same way as they are sampled in the actual data, i.e. only on days where purchases occur. We use the MSM estimator to estimate a truncated lognormal AR(1) model of the wholesale price processes for particular types of steel plate and test and reject the assumption that the firm is behaving optimally.},
  archive      = {J_JOE},
  author       = {George Hall and John Rust},
  doi          = {10.1016/j.jeconom.2020.07.033},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {219-243},
  shortjournal = {J. Econ.},
  title        = {Estimation of endogenously sampled time series: The case of commodity price speculation in the steel market},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vehicle size choice and automobile externalities: A dynamic
analysis. <em>JOE</em>, <em>222</em>(1), 196–218. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effect of highway congestion on the “arms race” on American roads, which has led to larger and more powerful vehicles that reduce safety and increase fuel consumption. We develop a new methodology to estimate a dynamic vehicle size choice and replacement model and find that congestion delays affect vehicle sizes. We then show that by addressing complementary externalities – congestion and the externalities associated with larger vehicle sizes – congestion pricing could reduce the vehicle fatality rate, generating $25 billion in annual benefits, and could improve vehicle fleet fuel efficiency, generating roughly $10 billion in annual operating cost savings.},
  archive      = {J_JOE},
  author       = {Clifford Winston and Jia Yan},
  doi          = {10.1016/j.jeconom.2020.07.032},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {196-218},
  shortjournal = {J. Econ.},
  title        = {Vehicle size choice and automobile externalities: A dynamic analysis},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How well do structural demand models work? Counterfactual
predictions in school choice. <em>JOE</em>, <em>222</em>(1), 161–195.
(<a href="https://doi.org/10.1016/j.jeconom.2020.07.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the prediction accuracy of discrete choice models of school demand, using a policy reform in Boston that altered where applicants can apply under school choice. We find that the discrete choice models do not consistently outperform a much simpler heuristic, but their inconsistent performance largely arises from prediction errors in applicant characteristics, which are auxiliary inputs. Once we condition on the correct inputs, the discrete choice models consistently outperform, and their accuracy does not significantly improve upon refitting using post-reform data, suggesting that the choice models capture stable components of the preference distribution across policy regimes.},
  archive      = {J_JOE},
  author       = {Parag A. Pathak and Peng Shi},
  doi          = {10.1016/j.jeconom.2020.07.031},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {161-195},
  shortjournal = {J. Econ.},
  title        = {How well do structural demand models work? counterfactual predictions in school choice},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disentangling moral hazard and adverse selection in private
health insurance. <em>JOE</em>, <em>222</em>(1), 141–160. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moral hazard and adverse selection create inefficiencies in private health insurance markets and understanding the relative importance of each factor is critical for addressing these inefficiencies. We use claims data from a large firm which changed health insurance plan options to isolate moral hazard from plan selection, estimating a discrete choice model to predict household plan preferences and attrition. Variation in plan preferences identifies the differential causal impact of each health insurance plan on the entire distribution of medical expenditures. Our estimates imply that 53\% of the additional medical spending observed in the most generous plan in our data relative to the least generous is due to adverse selection. We find that quantifying adverse selection by using prior medical expenditures overstates the true magnitude of selection due to mean reversion . We also statistically reject that individual health care consumption responds solely to the end-of-the-year marginal price.},
  archive      = {J_JOE},
  author       = {David Powell and Dana Goldman},
  doi          = {10.1016/j.jeconom.2020.07.030},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {141-160},
  shortjournal = {J. Econ.},
  title        = {Disentangling moral hazard and adverse selection in private health insurance},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating consumers’ choices of medicare part d plans: A
study in behavioral welfare economics. <em>JOE</em>, <em>222</em>(1),
107–140. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new methods to model behavior and conduct welfare analysis in complex environments where some choices are unlikely to reveal preferences. We develop a mixture-of-experts model that incorporates heterogeneity in consumers’ preferences and in their choice processes. We also develop a method to decompose logit errors into latent preferences versus optimization errors. Applying these methods to Medicare beneficiaries’ prescription drug insurance choices suggests that: (1) average welfare losses from suboptimal choices are small, (2) beneficiaries with dementia and depression have larger losses, and (3) policies that simplify choice sets offer small average benefits, helping some people but harming others.},
  archive      = {J_JOE},
  author       = {Michael Keane and Jonathan Ketcham and Nicolai Kuminoff and Timothy Neal},
  doi          = {10.1016/j.jeconom.2020.07.029},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {107-140},
  shortjournal = {J. Econ.},
  title        = {Evaluating consumers’ choices of medicare part d plans: A study in behavioral welfare economics},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing consumer demand with noisy neural measurements.
<em>JOE</em>, <em>222</em>(1), 89–106. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have used the random utility framework to examine whether neural data can assess and predict demand for consumer products, both within and across individuals. However the effectiveness of this methodology has been limited by the large degree of measurement error in neural data. The resulting “error-in-variables” problem severely biases the estimates of the relationship between neural measurements and choice behaviour, thus limiting the role such data can play in assessing marginal contributions to utility. In this article, we propose a method for controlling for this large degree of measurement error in value regions of the brain. We propose that additional neural variables from areas of the brain that are unrelated to valuation can serve as “proxies” for the measurement error in value regions, substantially alleviating the bias in model estimates. We demonstrate the feasibility of our proposed method on an existing dataset of fMRI measurements and consumer choices. We find a substantial reduction in the bias of model estimates compared to existing baseline methods (the estimated coefficients roughly double), leading to improved inference and out-of-sample demand prediction. After controlling for measurement error, we also find a considerable reduction in the variation of model estimates across consumers.},
  archive      = {J_JOE},
  author       = {Ryan Webb and Nitin Mehta and Ifat Levy},
  doi          = {10.1016/j.jeconom.2020.07.028},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {89-106},
  shortjournal = {J. Econ.},
  title        = {Assessing consumer demand with noisy neural measurements},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Control variables, discrete instruments, and identification
of structural functions. <em>JOE</em>, <em>222</em>(1), 73–88. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. We allow for discrete instruments, giving identification results under a variety of restrictions on the way the endogenous variable and the control variables affect the outcome. We consider many structural objects of interest, such as average or quantile treatment effects . We illustrate our results with an empirical application to Engel curve estimation.},
  archive      = {J_JOE},
  author       = {Whitney Newey and Sami Stouli},
  doi          = {10.1016/j.jeconom.2020.07.027},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {73-88},
  shortjournal = {J. Econ.},
  title        = {Control variables, discrete instruments, and identification of structural functions},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BLP estimation using laplace transformation and overlapping
simulation draws. <em>JOE</em>, <em>222</em>(1), 56–72. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive the asymptotic distribution of the parameters of the Berry et al. (1995) (BLP) model in a many markets setting which takes into account simulation noise under the assumption of overlapping simulation draws. We show that as long as the number of simulation draws R R and the number of markets T T approach infinity, our estimator is m = m i n ( R , T ) m=min(R,T) consistent and asymptotically normal. We do not impose any relationship between the rates at which R R and T T go to infinity, thus allowing for the case of R ≪ T R≪T . We provide a consistent estimate of the asymptotic variance which can be used to form asymptotically valid confidence intervals. Instead of directly minimizing the BLP GMM objective function, we propose using Hamiltonian Markov Chain Monte Carlo methods to implement a Laplace-type estimator which is asymptotically equivalent to the GMM estimator.},
  archive      = {J_JOE},
  author       = {Han Hong and Huiyu Li and Jessie Li},
  doi          = {10.1016/j.jeconom.2020.07.026},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {56-72},
  shortjournal = {J. Econ.},
  title        = {BLP estimation using laplace transformation and overlapping simulation draws},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using penalized likelihood to select parameters in a random
coefficients multinomial logit model. <em>JOE</em>, <em>222</em>(1),
44–55. (<a href="https://doi.org/10.1016/j.jeconom.2019.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is about estimating a random coefficients logit model in which the distribution of each coefficient is characterized by finitely many parameters, some of which may be zero. The paper gives conditions under which, with probability approaching 1 as the sample size increases, penalized maximum likelihood (PML) estimation with the adaptive LASSO (AL) penalty distinguishes correctly between zero and non-zero parameters. The paper also gives conditions under which PML reduces the asymptotic mean-square estimation error of any continuously differentiable function of the model’s parameters. The paper describes a method for computing PML estimates and presents the results of Monte Carlo experiments that illustrate their performance. It also presents the results of PML estimation of a random coefficients logit model of choice among brands of butter and margarine in the British groceries market.},
  archive      = {J_JOE},
  author       = {Joel L. Horowitz and Lars Nesheim},
  doi          = {10.1016/j.jeconom.2019.11.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {44-55},
  shortjournal = {J. Econ.},
  title        = {Using penalized likelihood to select parameters in a random coefficients multinomial logit model},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of preference estimation with unobserved choice set
heterogeneity. <em>JOE</em>, <em>222</em>(1), 4–43. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide an introduction to the estimation of discrete choice models when choice sets are heterogeneous and unobserved to the econometrician. We survey the two most popular approaches: “integrating over” and “differencing out” unobserved choice sets. Inspired by Chamberlain (1980)’s original idea of constructing sufficient statistics from observed choices, we introduce the term “sufficient set” to refer to any combination of observed choices that lies within the true but unobserved choice set. The concept of sufficient set helps to unify notation and organize our thinking, to map econometric assumptions onto economic models, and to implement both methods in practice.},
  archive      = {J_JOE},
  author       = {Gregory S. Crawford and Rachel Griffith and Alessandro Iaria},
  doi          = {10.1016/j.jeconom.2020.07.024},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {4-43},
  shortjournal = {J. Econ.},
  title        = {A survey of preference estimation with unobserved choice set heterogeneity},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overview: Structural econometrics honoring daniel McFadden.
<em>JOE</em>, <em>222</em>(1), 1–3. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Han Hong and Michael Keane and Clifford Winston},
  doi          = {10.1016/j.jeconom.2020.07.023},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-3},
  shortjournal = {J. Econ.},
  title        = {Overview: Structural econometrics honoring daniel McFadden},
  volume       = {222},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The continuous-time limit of score-driven volatility models.
<em>JOE</em>, <em>221</em>(2), 655–675. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide general conditions under which a class of discrete-time volatility models driven by the score of the conditional density converges in distribution to a stochastic differential equation as the interval between observations goes to zero. We show that the form of the diffusion limit depends on: (i) the link function, (ii) the conditional second moment of the score, (iii) the normalization of the score. Interestingly, the properties of the stochastic differential equation are strictly entangled with those of the discrete-time counterpart. Score-driven models with fat-tailed densities lead to continuous-time processes with finite volatility of volatility, as opposed to fat-tailed models with a GARCH update, for which the volatility of volatility is explosive. We examine in simulations the implications of such results on approximate estimation and filtering of diffusion processes . An extension to models with a time-varying conditional mean and to conditional covariance models is also developed.},
  archive      = {J_JOE},
  author       = {Giuseppe Buccheri and Fulvio Corsi and Franco Flandoli and Giulia Livieri},
  doi          = {10.1016/j.jeconom.2020.07.042},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {655-675},
  shortjournal = {J. Econ.},
  title        = {The continuous-time limit of score-driven volatility models},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overlap in observational studies with high-dimensional
covariates. <em>JOE</em>, <em>221</em>(2), 644–654. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating causal effects under exogeneity hinges on two key assumptions: unconfoundedness and overlap. Researchers often argue that unconfoundedness is more plausible when more covariates are included in the analysis. Less discussed is the fact that covariate overlap is more difficult to satisfy in this setting. In this paper, we explore the implications of overlap in observational studies with high-dimensional covariates and formalize curse-of-dimensionality argument, suggesting that these assumptions are stronger than investigators likely realize. Our key innovation is to explore how strict overlap restricts global discrepancies between the covariate distributions in the treated and control populations. Exploiting results from information theory, we derive explicit bounds on the average imbalance in covariate means under strict overlap and show that these bounds become more restrictive as the dimension grows large. We discuss how these implications interact with assumptions and procedures commonly deployed in observational causal inference, including sparsity and trimming.},
  archive      = {J_JOE},
  author       = {Alexander D’Amour and Peng Ding and Avi Feller and Lihua Lei and Jasjeet Sekhon},
  doi          = {10.1016/j.jeconom.2019.10.014},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {644-654},
  shortjournal = {J. Econ.},
  title        = {Overlap in observational studies with high-dimensional covariates},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diffusion copulas: Identification and estimation.
<em>JOE</em>, <em>221</em>(2), 616–643. (<a
href="https://doi.org/10.1016/j.jeconom.2020.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new semiparametric approach for modelling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion (UPD). This modelling strategy yields a general class of semiparametric Markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. We provide primitive conditions for the identification of the UPD parameters together with the unknown transformations from discrete samples. Likelihood-based estimators of both parametric and nonparametric components are developed and we analyse their asymptotic properties . Kernel-based drift and diffusion estimators are also proposed and shown to be normally distributed in large samples. A simulation study investigates the finite sample performance of our estimators in the context of modelling US short-term interest rates . We also present a simple application of the proposed method for modelling the CBOE volatility index data.},
  archive      = {J_JOE},
  author       = {Ruijun Bu and Kaddour Hadri and Dennis Kristensen},
  doi          = {10.1016/j.jeconom.2020.06.004},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {616-643},
  shortjournal = {J. Econ.},
  title        = {Diffusion copulas: Identification and estimation},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and inference in spatial models with dominant
units. <em>JOE</em>, <em>221</em>(2), 591–615. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spatial econometrics literature estimation and inference are carried out assuming that the matrix of spatial or network connections has uniformly bounded absolute column sums in the number of units, n n , in the network. This paper relaxes this restriction and allows for one or more units to have pervasive effects in the network. The linear–quadratic central limit theorem of Kelejian and Prucha (2001) is generalized to allow for such dominant units, and the asymptotic properties of the GMM estimators are established in this more general setting. A new bias-corrected method of moments (BMM) estimator is also proposed that avoids the problem of weak instruments by self-instrumenting the spatially lagged dependent variable. Both cases of homoskedastic and heteroskedastic errors are considered and the associated estimators are shown to be consistent and asymptotically normal, depending on the rate at which the maximum column sum of the weights matrix rises with n n . The small sample properties of GMM and BMM estimators are investigated by Monte Carlo experiments and shown to be satisfactory. An empirical application to sectoral price changes in the US over the pre- and post-2008 financial crisis is also provided. It is shown that the share of capital can be estimated reasonably well from the degree of sectoral interdependence using the input–output tables, despite the evidence of dominant sectors being present in the US economy.},
  archive      = {J_JOE},
  author       = {M. Hashem Pesaran and Cynthia Fan Yang},
  doi          = {10.1016/j.jeconom.2020.04.045},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {591-615},
  shortjournal = {J. Econ.},
  title        = {Estimation and inference in spatial models with dominant units},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The factor analytical approach in near unit root interactive
effects panels. <em>JOE</em>, <em>221</em>(2), 569–590. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent study, Bai (2013) proposes a new factor analytical (FA) method for estimation of stationary dynamic panel data models with fixed effects. Our interest in this method originates with the fact it does not require explicit demeaning of the data, a practice that is known to cause problems of bias and low power in near unit root panels . The purpose is to study the properties of FA when applied to such panels when the common component admits to a interactive effects representation, which is more general than fixed effects. It is shown that the estimator of the autoregressive parameter is consistent with a well centered asymptotic normal distribution , leading to unit root tests with maximal achievable power. In fact, FA is consistent and asymptotically normal regardless of whether the data are near unit root non-stationary or stationary. It is therefore very general and hence widely applicable.},
  archive      = {J_JOE},
  author       = {Milda Norkutė and Joakim Westerlund},
  doi          = {10.1016/j.jeconom.2020.03.017},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {569-590},
  shortjournal = {J. Econ.},
  title        = {The factor analytical approach in near unit root interactive effects panels},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Missing observations in observation-driven time series
models. <em>JOE</em>, <em>221</em>(2), 542–568. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We argue that existing methods for the treatment of missing observations in time-varying parameter observation-driven models lead to inconsistent inference. We provide a formal proof of this inconsistency for a Gaussian model with time-varying mean. A Monte Carlo simulation study supports this theoretical result and illustrates how the inconsistency problem extends to score-driven and, more generally, to observation-driven models, which include well-known models for conditional volatility. To overcome the problem of inconsistent inference, we propose a novel estimation procedure based on indirect inference. This easy-to-implement method delivers consistent inference. The asymptotic properties of the new method are formally derived. Our proposed estimation procedure shows a promising performance in a Monte Carlo simulation exercise as well as in an empirical study concerning the measurement of conditional volatility from financial returns data.},
  archive      = {J_JOE},
  author       = {F. Blasques and P. Gorgi and S.J. Koopman},
  doi          = {10.1016/j.jeconom.2020.07.043},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {542-568},
  shortjournal = {J. Econ.},
  title        = {Missing observations in observation-driven time series models},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of units with pervasive effects in large panel
data models. <em>JOE</em>, <em>221</em>(2), 510–541. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of units that influence a large number of other units in a network has become increasingly recognized in the literature. In this paper we propose a new method to detect such pervasive units by basing our analysis on unit-specific residual error variances subject to suitable adjustments due to the multiple testing issues involved. Accordingly, a sequential multiple testing (SMT) procedure is proposed, which allows identification of pervasive units (if any) without a priori knowledge of the interconnections amongst cross-section units or availability of a short list of candidate units to search over. The proposed method is applicable even if the cross-section dimension exceeds the time series dimension, and most importantly it could end up with none of the units selected as pervasive when this is in fact the case. The SMT procedure exhibits satisfactory small-sample performance in Monte Carlo simulations and compares well relative to existing approaches. We apply the SMT detection method to sectoral indices of U.S. industrial production, U.S. house price changes by states, and the rates of change of real GDP and real equity prices across the world’s largest economies.},
  archive      = {J_JOE},
  author       = {G. Kapetanios and M.H. Pesaran and S. Reese},
  doi          = {10.1016/j.jeconom.2020.05.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {510-541},
  shortjournal = {J. Econ.},
  title        = {Detection of units with pervasive effects in large panel data models},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting the location of FDI in china: A panel data
approach with heterogeneous shocks. <em>JOE</em>, <em>221</em>(2),
483–509. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreign Direct Investment (FDI) is viewed as a primary driving force in shaping the global economy and receives particular attention in empirical studies. In this paper, we argue that many of the existing studies ignore endogeneities that arise from shocks in source and destination countries. To address this endogeneity issue, we take the “controlling through estimating” idea from the econometric literature and propose using panel data models with heterogeneous shocks to deal with it. We consider the quasi maximum likelihood (QML) method to estimate our proposed model. We investigate the asymptotic properties of the QML estimator , including the consistency, the asymptotic representation, and the limiting distribution. We also propose new statistics to test the validity of the use of traditional dynamic and static panel data estimation methods. Applying it to the location determinants of inward FDI in China, we find that the endogeneity issue does exist, and that controlling for heterogeneous shocks helps to improve the estimation results.},
  archive      = {J_JOE},
  author       = {Lei Hou and Kunpeng Li and Qi Li and Min Ouyang},
  doi          = {10.1016/j.jeconom.2020.04.047},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {483-509},
  shortjournal = {J. Econ.},
  title        = {Revisiting the location of FDI in china: A panel data approach with heterogeneous shocks},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large-dimensional dynamic factor models: Estimation of
impulse–response functions with i(1) cointegrated factors. <em>JOE</em>,
<em>221</em>(2), 455–482. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a large-dimensional Dynamic Factor Model where: (i) the vector of factors Ft is I(1) and driven by a number of shocks that is smaller than the dimension of Ft ; and, (ii) the idiosyncratic components are either I(1) or I(0) . Under (i), the factors Ft are cointegrated and can be modeled as a Vector Error Correction Model (VECM). Under (i) and (ii), we provide consistent estimators , as both the cross-sectional size n and the time dimension T go to infinity, for the factors, the loadings, the shocks, the coefficients of the VECM and therefore the Impulse–Response Functions (IRF) of the observed variables to the shocks. Furthermore, possible deterministic linear trends are fully accounted for, and the case of an unrestricted VAR in the levels Ft , instead of a VECM, is also studied. The finite-sample properties the proposed estimators are explored by means of a MonteCarlo exercise. Finally, we revisit two distinct and widely studied empirical applications. By correctly modeling the long-run dynamics of the factors, our results partly overturn those obtained by recent literature. Specifically, we find that: (i) oil price shocks have just a temporary effect on US real activity; and, (ii) in response to a positive news shock, the economy first experiences a significant boom, and then a milder recession.},
  archive      = {J_JOE},
  author       = {Matteo Barigozzi and Marco Lippi and Matteo Luciani},
  doi          = {10.1016/j.jeconom.2020.05.004},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {455-482},
  shortjournal = {J. Econ.},
  title        = {Large-dimensional dynamic factor models: Estimation of Impulse–Response functions with i(1) cointegrated factors},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial dynamic panel data models with correlated random
effects. <em>JOE</em>, <em>221</em>(2), 424–454. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, M -estimation and inference methods are developed for spatial dynamic panel data models with correlated random effects , based on short panels . The unobserved individual-specific effects are assumed to be correlated with the observed time-varying regressors linearly or in a linearizable way, giving the so-called correlated random effects model, which allows the estimation of effects of time-invariant regressors . The unbiased estimating functions are obtained by adjusting the conditional quasi-scores given the initial observations, leading to M -estimators that are consistent, asymptotically normal, and free from the initial conditions except the process starting time. By decomposing the estimating functions into sums of terms uncorrelated given idiosyncratic errors, a hybrid method is developed for consistently estimating the variance–covariance matrix of the M -estimators, which again depends only on the process starting time. Monte Carlo results demonstrate that the proposed methods perform well in finite sample. An empirical application on the political competition in China is presented.},
  archive      = {J_JOE},
  author       = {Liyao Li and Zhenlin Yang},
  doi          = {10.1016/j.jeconom.2020.05.016},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {424-454},
  shortjournal = {J. Econ.},
  title        = {Spatial dynamic panel data models with correlated random effects},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing high-dimensional covariance matrices under the
elliptical distribution and beyond. <em>JOE</em>, <em>221</em>(2),
409–423. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop tests for high-dimensional covariance matrices under a generalized elliptical model. Our tests are based on a central limit theorem for linear spectral statistics of the sample covariance matrix based on self-normalized observations. For testing sphericity, our tests neither assume specific parametric distributions nor involve the kurtosis of data. More generally, we can test against any non-negative definite matrix that can even be not invertible. As an interesting application, we illustrate in empirical studies that our tests can be used to test uncorrelatedness among idiosyncratic returns.},
  archive      = {J_JOE},
  author       = {Xinxin Yang and Xinghua Zheng and Jiaqi Chen},
  doi          = {10.1016/j.jeconom.2020.05.017},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {409-423},
  shortjournal = {J. Econ.},
  title        = {Testing high-dimensional covariance matrices under the elliptical distribution and beyond},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Varying random coefficient models. <em>JOE</em>,
<em>221</em>(2), 381–408. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes unobserved heterogeneity when observed characteristics are modeled nonlinearly. The proposed model builds on varying random coefficients (VRC) that are determined by nonlinear functions of observed regressors and additively separable unobservables. This paper proposes a novel estimator of the VRC density based on weighted sieve minimum distance. The main example of sieve bases are Hermite functions which yield a numerically stable estimation procedure. This paper shows inference results that go beyond what has been shown in ordinary RC models . We provide in each case rates of convergence and also establish pointwise limit theory of linear functionals , where a prominent example is the density of potential outcomes. In addition, a multiplier bootstrap procedure is proposed to construct uniform confidence bands. A Monte Carlo study examines finite sample properties of the estimator and shows that it performs well even when the regressors associated to RC are far from being heavy tailed. Finally, the methodology is applied to analyze heterogeneity in income elasticity of demand for housing.},
  archive      = {J_JOE},
  author       = {Christoph Breunig},
  doi          = {10.1016/j.jeconom.2020.04.049},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {381-408},
  shortjournal = {J. Econ.},
  title        = {Varying random coefficient models},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and optimal estimation for partially linear
instrumental variables models with partial identification. <em>JOE</em>,
<em>221</em>(2), 368–380. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies robust and optimal estimation of the slope coefficients in a partially linear instrumental variables model with nonparametric partial identification. We establish the root-n asymptotic normality of a penalized sieve minimum distance estimator of the slope coefficients . We show that the asymptotic normality holds regardless of whether the nonparametric function is point identified or only partially identified. However, in the presence of nonparametric partial identification, the slope coefficients may not be continuous in the underlying distribution and the asymptotic variance matrix may depend on the penalty, so classical efficiency analysis does not apply. We instead develop an optimally penalized estimator that minimizes the asymptotic variance of a linear functional of the slope coefficients estimator by employing an optimal penalty for a given weight, and propose a feasible two-step procedure. We also propose an iterated procedure to address how to choose both penalty and weight optimally and further improve efficiency. To conduct inference, we provide a consistent variance matrix estimator. Monte Carlo simulations examine the finite sample performance of our estimators.},
  archive      = {J_JOE},
  author       = {Qihui Chen},
  doi          = {10.1016/j.jeconom.2020.05.012},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {368-380},
  shortjournal = {J. Econ.},
  title        = {Robust and optimal estimation for partially linear instrumental variables models with partial identification},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of dynamic panel spatial vector autoregression:
Stability and spatial multivariate cointegration. <em>JOE</em>,
<em>221</em>(2), 337–367. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces dynamic panel spatial vector autoregressive models . We study features of dynamics and spatial interactions that an SVAR model can generate and classify the model into stable or unstable cases by partitioning parameter spaces. For stable, spatial cointegration, and mixed cointegration cases, we investigate identification and QML estimation of the models to take into account simultaneity and correlated relationships. Asymptotic properties and bias-corrected estimators are presented. To detect unknown cointegration relationships, we introduce a sequential likelihood ratio testing procedure. Simulations show the advantage of QMLEs on bias reduction and efficiency gains. The empirical application provides evidences on ancient China’s market integration.},
  archive      = {J_JOE},
  author       = {Kai Yang and Lung-fei Lee},
  doi          = {10.1016/j.jeconom.2020.05.010},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {337-367},
  shortjournal = {J. Econ.},
  title        = {Estimation of dynamic panel spatial vector autoregression: Stability and spatial multivariate cointegration},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequentist properties of bayesian inequality tests.
<em>JOE</em>, <em>221</em>(1), 312–336. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian and frequentist criteria fundamentally differ, but often posterior and sampling distributions agree asymptotically. For the corresponding single-draw experiment, we characterize the frequentist size of a certain Bayesian hypothesis test of (possibly nonlinear) inequalities . If the null hypothesis is that the parameter lies in a specified half-space, then the Bayesian test’s size equals α α ; if the null hypothesis is a subset of a half-space, then size is above α α ; otherwise, size may be equal to, above, or below α α . Rejection probabilities at certain points are also characterized. Two examples illustrate our results: translog cost function curvature and ordinal distribution relationships.},
  archive      = {J_JOE},
  author       = {David M. Kaplan and Longhao Zhuo},
  doi          = {10.1016/j.jeconom.2020.05.015},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {312-336},
  shortjournal = {J. Econ.},
  title        = {Frequentist properties of bayesian inequality tests},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating multiple breaks in nonstationary autoregressive
models. <em>JOE</em>, <em>221</em>(1), 277–311. (<a
href="https://doi.org/10.1016/j.jeconom.2020.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chong (1995) and Bai (1997) proposed a sample-splitting method to estimate a multiple-break model. However, their studies focused on stationary time series models , in which the identification of the first break depends on the magnitude and the duration of the break, and a testing procedure is needed to assist the estimation of the remaining breaks in subsamples split by the break points found earlier. In this paper, we focus on nonstationary multiple-break autoregressive models . Unlike the stationary case, we show that the duration of a break does not affect whether it will be identified first. Rather, it depends on the stochastic order of magnitude of signal strength of the break under the case of constant break magnitude and also the square of the magnitude of the break under the case of shrinking break magnitude. Since the subsamples usually have different stochastic orders in nonstationary autoregressive models with breaks, one can therefore determine which break will be identified first. We apply this finding to the models proposed in Phillips and Yu (2011) and Phillips et al. (2011, 2015a, 2015b). We propose an estimation procedure as well as the asymptotic theory for the model. Some extensions to more general models are provided, and the hypothesis test with the null hypothesis being the unit root model is examined. Results of numerical simulations and an empirical study are given to illustrate the finite-sample performance.},
  archive      = {J_JOE},
  author       = {Tianxiao Pang and Lingjie Du and Terence Tai-Leung Chong},
  doi          = {10.1016/j.jeconom.2020.06.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {277-311},
  shortjournal = {J. Econ.},
  title        = {Estimating multiple breaks in nonstationary autoregressive models},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automated approach towards sparse single-equation
cointegration modelling. <em>JOE</em>, <em>221</em>(1), 247–276. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose the Single-equation Penalized Error Correction Selector (SPECS) as an automated estimation procedure for dynamic single-equation models with a large number of potentially (co)integrated variables. By extending the classical single-equation error correction model, SPECS enables the researcher to model large cointegrated datasets without necessitating any form of pre-testing for the order of integration or cointegrating rank. Under an asymptotic regime in which both the number of parameters and time series observations jointly diverge to infinity, we show that SPECS is able to consistently estimate an appropriate linear combination of the cointegrating vectors that may occur in the underlying DGP. In addition, SPECS is shown to enable the correct recovery of sparsity patterns in the parameter space and to possess the same limiting distribution as the OLS oracle procedure. A simulation study shows strong selective capabilities, as well as superior predictive performance in the context of nowcasting compared to high-dimensional models that ignore cointegration. An empirical application to nowcasting Dutch unemployment rates using Google Trends confirms the strong practical performance of our procedure.},
  archive      = {J_JOE},
  author       = {Stephan Smeekes and Etienne Wijler},
  doi          = {10.1016/j.jeconom.2020.07.021},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {247-276},
  shortjournal = {J. Econ.},
  title        = {An automated approach towards sparse single-equation cointegration modelling},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal linear instrumental variables approximations.
<em>JOE</em>, <em>221</em>(1), 223–246. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the identification and estimation of the optimal linear approximation of a structural regression function . The parameter in the linear approximation is called the Optimal Linear Instrumental Variables Approximation (OLIVA). This paper shows that a necessary condition for standard inference on the OLIVA is also sufficient for the existence of an IV estimand in a linear model. The instrument in the IV estimand is unknown and may not be identified. A Two-Step IV (TSIV) estimator based on Tikhonov regularization is proposed, which can be implemented by standard regression routines. We establish the asymptotic normality of the TSIV estimator assuming neither completeness nor identification of the instrument. As an important application of our analysis, we robustify the classical Hausman test for exogeneity against misspecification of the linear structural model. We also discuss extensions to weighted least squares criteria. Monte Carlo simulations suggest an excellent finite sample performance for the proposed inferences. Finally, in an empirical application estimating the elasticity of intertemporal substitution (EIS) with US data, we obtain TSIV estimates that are much larger than their standard IV counterparts, with our robust Hausman test failing to reject the null hypothesis of exogeneity of real interest rates.},
  archive      = {J_JOE},
  author       = {Juan Carlos Escanciano and Wei Li},
  doi          = {10.1016/j.jeconom.2020.05.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {223-246},
  shortjournal = {J. Econ.},
  title        = {Optimal linear instrumental variables approximations},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonstationary panel models with latent group structures and
cross-section dependence. <em>JOE</em>, <em>221</em>(1), 198–222. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel Lasso-based approach to handle unobserved parameter heterogeneity and cross-section dependence in nonstationary panel models. In particular, a penalized principal component (PPC) method is developed to estimate group-specific long-run relationships and unobserved common factors and jointly to identify the unknown group membership. The PPC estimators are shown to be consistent under weakly dependent innovation processes. But they suffer an asymptotically non-negligible bias from correlations between the nonstationary regressors and unobserved stationary common factors and/or the equation errors . To remedy these shortcomings we provide three bias-correction procedures under which the estimators are re-centered about zero as both dimensions ( N and T ) of the panel tend to infinity. We establish a mixed normal limit theory for the estimators of the group-specific long-run coefficients, which permits inference using standard test statistics. Simulations suggest good finite sample performance. An empirical application applies the methodology to study international R&amp;D spillovers and the results offer a convincing explanation for the growth convergence puzzle through the heterogeneous impact of R&amp;D spillovers .},
  archive      = {J_JOE},
  author       = {Wenxin Huang and Sainan Jin and Peter C.B. Phillips and Liangjun Su},
  doi          = {10.1016/j.jeconom.2020.05.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {198-222},
  shortjournal = {J. Econ.},
  title        = {Nonstationary panel models with latent group structures and cross-section dependence},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of a SAR model with endogenous spatial weights
constructed by bilateral variables. <em>JOE</em>, <em>221</em>(1),
180–197. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the estimation of a cross-sectional spatial autoregressive (SAR) model with spatial weights constructed by bilateral variables like the trade or investment between regions. We model the possible endogeneity in spatial weights due to the correlation between the error term in the SAR model and unobserved interactive fixed effects in bilateral variables. Using a control function approach, we propose two-stage estimation methods and establish their consistency and asymptotic normality . Finite sample properties are investigated by a Monte Carlo study . We further apply our method to an empirical study of interactions among different US industries through production networks.},
  archive      = {J_JOE},
  author       = {Xi Qu and Lung-fei Lee and Chao Yang},
  doi          = {10.1016/j.jeconom.2020.05.011},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {180-197},
  shortjournal = {J. Econ.},
  title        = {Estimation of a SAR model with endogenous spatial weights constructed by bilateral variables},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Likelihood inference and the role of initial conditions for
the dynamic panel data model. <em>JOE</em>, <em>221</em>(1), 160–179.
(<a href="https://doi.org/10.1016/j.jeconom.2020.04.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lancaster (2002) proposes an estimator for the dynamic panel data model with homoskedastic errors and zero initial conditions. In this paper, we show this estimator is invariant to orthogonal transformations , but is inefficient because it ignores additional information available in the data. The zero initial condition is trivially satisfied by subtracting initial observations from the data. We show that differencing out the data further erodes efficiency compared to drawing inference conditional on the first observations. Finally, we compare the conditional method with standard random effects approaches for unobserved data. Standard approaches implicitly rely on normal approximations, which may not be reliable when unobserved data is very skewed with some mass at zero values. For example, panel data on firms naturally depend on the first period in which the firm enters a new state. It seems unreasonable, then, to assume that the process determining unobserved data is known or stationary. We can instead make inference on structural parameters by conditioning on the initial observations.},
  archive      = {J_JOE},
  author       = {José Diogo Barbosa and Marcelo J. Moreira},
  doi          = {10.1016/j.jeconom.2020.04.039},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {160-179},
  shortjournal = {J. Econ.},
  title        = {Likelihood inference and the role of initial conditions for the dynamic panel data model},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing continuity of a density via g-order statistics in
the regression discontinuity design. <em>JOE</em>, <em>221</em>(1),
138–159. (<a
href="https://doi.org/10.1016/j.jeconom.2020.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the regression discontinuity design (RDD), it is common practice to assess the credibility of the design by testing the continuity of the density of the running variable at the cut-off, e.g., McCrary (2008). In this paper we propose an approximate sign test for continuity of a density at a point based on the so-called g g -order statistics, and study its properties under two complementary asymptotic frameworks. In the first asymptotic framework, the number q q of observations local to the cut-off is fixed as the sample size n n diverges to infinity, while in the second framework q q diverges to infinity slowly as n n diverges to infinity. Under both of these frameworks, we show that the test we propose is asymptotically valid in the sense that it has limiting rejection probability under the null hypothesis not exceeding the nominal level. More importantly, the test is easy to implement, asymptotically valid under weaker conditions than those used by competing methods, and exhibits finite sample validity under stronger conditions than those needed for its asymptotic validity. In a simulation study, we find that the approximate sign test provides good control of the rejection probability under the null hypothesis while remaining competitive under the alternative hypothesis. We finally apply our test to the design in Lee (2008), a well-known application of the RDD to study incumbency advantage.},
  archive      = {J_JOE},
  author       = {Federico A. Bugni and Ivan A. Canay},
  doi          = {10.1016/j.jeconom.2020.02.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {138-159},
  shortjournal = {J. Econ.},
  title        = {Testing continuity of a density via g-order statistics in the regression discontinuity design},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical markov-switching models for multivariate
integer-valued time-series. <em>JOE</em>, <em>221</em>(1), 118–137. (<a
href="https://doi.org/10.1016/j.jeconom.2020.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new flexible dynamic model for multivariate nonnegative integer-valued time-series. Observations are assumed to depend on the realization of two unobserved integer-valued stochastic variables which control for the time- and cross-dependence of the data. We provide conditional and unconditional (cross)-moments implied by the model, as well as the limiting distribution of the series. An Expectation–Maximization algorithm for maximum likelihood estimation of the model parameters is derived, and an extensive Monte Carlo experiment investigates the finite sample properties of the resulting maximum likelihood estimator . Constrained specifications of the model are also formulated by modifying the assumptions about the dependence structure of the latent variables, and model identification is discussed accordingly. An application by means of a crime data set from the New South Wales (NSW) Bureau Of Crime Statistics And Research with observations spanning beyond 20 years is reported to illustrate the methodology. Results indicate that the proposed approach provides a good description of the conditional distribution of crime records, outperforming the standard hidden Markov model.},
  archive      = {J_JOE},
  author       = {Leopoldo Catania and Roberto Di Mari},
  doi          = {10.1016/j.jeconom.2020.02.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {118-137},
  shortjournal = {J. Econ.},
  title        = {Hierarchical markov-switching models for multivariate integer-valued time-series},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ExpectHill estimation, extreme risk and heavy tails.
<em>JOE</em>, <em>221</em>(1), 97–117. (<a
href="https://doi.org/10.1016/j.jeconom.2020.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk measures of a financial position are, from an empirical point of view, mainly based on quantiles . Replacing quantiles with their least squares analogues, called expectiles, has recently received increasing attention. The novel expectile-based risk measures satisfy all coherence requirements. We revisit their extreme value estimation for heavy-tailed distributions. First, we estimate the underlying tail index via weighted combinations of top order statistics and asymmetric least squares estimates . The resulting expectHill estimators are then used as the basis for estimating tail expectiles and Expected Shortfall. The asymptotic theory of the proposed estimators is provided, along with numerical simulations and applications to actuarial and financial data.},
  archive      = {J_JOE},
  author       = {Abdelaati Daouia and Stéphane Girard and Gilles Stupfler},
  doi          = {10.1016/j.jeconom.2020.02.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {97-117},
  shortjournal = {J. Econ.},
  title        = {ExpectHill estimation, extreme risk and heavy tails},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient size correct subset inference in homoskedastic
linear instrumental variables regression. <em>JOE</em>, <em>221</em>(1),
78–96. (<a href="https://doi.org/10.1016/j.jeconom.2019.10.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that Moreira’s (2003) conditional critical value function for likelihood ratio (LR) tests on the structural parameter in homoskedastic linear instrumental variables (IV) regression provides a bounding critical value function for subset LR tests on one structural parameter of several for general homoskedastic linear IV regression. The resulting subset LR test is size correct under weak identification and efficient under strong identification. A power study shows that it outperforms the subset Anderson–Rubin test with conditional critical values from Guggenberger et al. (2019a) when the structural parameters are reasonably identified and has slightly less power when identification is weak.},
  archive      = {J_JOE},
  author       = {Frank Kleibergen},
  doi          = {10.1016/j.jeconom.2019.10.013},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {78-96},
  shortjournal = {J. Econ.},
  title        = {Efficient size correct subset inference in homoskedastic linear instrumental variables regression},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Jackknife empirical likelihood for inequality constraints on
regular functionals. <em>JOE</em>, <em>221</em>(1), 68–77. (<a
href="https://doi.org/10.1016/j.jeconom.2019.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical likelihood is effective in many different practical situations involving moment equality and/or inequality restrictions. However, in applications with nonlinear functionals of the underlying distribution, it becomes computationally more difficult to implement. We propose the use of jackknife empirical likelihood (Jing et al., 2009) to circumvent the computational difficulties with nonlinear inequality constraints and establish the chi-bar-square distribution as the limiting null distribution of the resulting empirical likelihood-ratio statistic, where a finite number of inequalities on functionals that are regular in the sense of Hoeffding (1948), defines the null hypothesis. The class of regular functionals includes many nonlinear functionals that arise in practice and has moments as a special case. To overcome the implementation challenges with this non-pivotal asymptotic null distribution, we propose an empirical likelihood bootstrap procedure that is valid with uniformity. Finally, we investigate the finite-sample properties of the bootstrap procedure using Monte Carlo simulations and find that the results are promising.},
  archive      = {J_JOE},
  author       = {Ruxin Chen and Rami V. Tabri},
  doi          = {10.1016/j.jeconom.2019.11.007},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {68-77},
  shortjournal = {J. Econ.},
  title        = {Jackknife empirical likelihood for inequality constraints on regular functionals},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general semiparametric approach to inference with
marker-dependent hazard rate models. <em>JOE</em>, <em>221</em>(1),
43–67. (<a href="https://doi.org/10.1016/j.jeconom.2019.05.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine a new general class of hazard rate models for duration data, containing a parametric and a nonparametric component. Both can be a mix of a time effect and possibly time-dependent covariate effects. A number of well-known models are special cases. In a counting process framework, a general profile likelihood estimator is developed and the parametric component of the model is shown to be asymptotically normal and efficient. Finite sample properties are investigated in simulations. The estimator is applied to investigate the long-run relationship between birth weight and later-life mortality.},
  archive      = {J_JOE},
  author       = {Gerard. J. van den Berg and Lena Janys and Enno Mammen and Jens Perch Nielsen},
  doi          = {10.1016/j.jeconom.2019.05.025},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {43-67},
  shortjournal = {J. Econ.},
  title        = {A general semiparametric approach to inference with marker-dependent hazard rate models},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing semiparametric efficiency bounds in discrete
choice models with strategic-interactions and rational expectations.
<em>JOE</em>, <em>221</em>(1), 25–42. (<a
href="https://doi.org/10.1016/j.jeconom.2020.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper computes semiparametric efficiency bounds for finite-dimensional parameters in discrete choice models with nonparametric regressors in the form of conditional expectations . These can include expectations about exogenous events as well as expectations about the choices of other agents. Thus, the models studied here include incomplete-information games, social-interactions models as well as single-agent discrete choice models with uncertainty as special cases. Our bounds rely on the assumption of rational expectations and on regularity conditions of equilibrium beliefs. The paper focuses on binary-choice models but the derivation of the bounds illustrates how our approach can be extended to multinomial choice cases. Explicit efficiency bound expressions for the models examined here had not been derived before. Furthermore, since we also characterize the efficient influence functions, our results can also potentially be used to construct semiparametrically efficient estimators for these models.},
  archive      = {J_JOE},
  author       = {Andrés Aradillas-López},
  doi          = {10.1016/j.jeconom.2020.02.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {25-42},
  shortjournal = {J. Econ.},
  title        = {Computing semiparametric efficiency bounds in discrete choice models with strategic-interactions and rational expectations},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bootstrap based probability forecasting in multiplicative
error models. <em>JOE</em>, <em>221</em>(1), 1–24. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As evidenced by an extensive empirical literature, multiplicative error models (MEM) show good performance in capturing the stylized facts of nonnegative time series; examples include, trading volume, financial durations, and volatility. This paper develops a bootstrap based method for producing multi-step-ahead probability forecasts for a nonnegative valued time-series obeying a parametric MEM. In order to test the adequacy of the underlying parametric model , a class of bootstrap specification tests is also developed. Rigorous proofs are provided for establishing the validity of the proposed bootstrap methods . The paper also establishes the validity of a bootstrap based method for producing probability forecasts in a class of semiparametric MEMs. Monte Carlo simulations suggest that our methods perform well in finite samples. A real data example illustrates the methods.},
  archive      = {J_JOE},
  author       = {Indeewara Perera and Mervyn J. Silvapulle},
  doi          = {10.1016/j.jeconom.2020.01.022},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-24},
  shortjournal = {J. Econ.},
  title        = {Bootstrap based probability forecasting in multiplicative error models},
  volume       = {221},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic panels with MIDAS covariates: Nonlinearity,
estimation and fit. <em>JOE</em>, <em>220</em>(2), 589–605. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Mixed Data Sampling (MIDAS) into the panel data context. To address the unidentified nuisance parameter problem, we propose to invert model specification tests for inference on the MIDAS parameter along with bounds tests for model coefficients. Illustrative identification, simulation and empirical analyses are conducted in the dynamic GMM framework. Our framework allows for departures from i.i.d errors such as clustering and dynamic specifications. A simulation study and an application to a model of reserve holdings illustrate the usefulness of the proposed methods, and more broadly set a promising template for shrinkage approaches.},
  archive      = {J_JOE},
  author       = {Lynda Khalaf and Maral Kichian and Charles J. Saunders and Marcel Voia},
  doi          = {10.1016/j.jeconom.2020.04.015},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {589-605},
  shortjournal = {J. Econ.},
  title        = {Dynamic panels with MIDAS covariates: Nonlinearity, estimation and fit},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of a nonparametric model for bond prices from
cross-section and time series information. <em>JOE</em>,
<em>220</em>(2), 562–588. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel estimation methodology for an additive nonparametric panel model that is suitable for capturing the pricing of coupon-paying government bonds followed over many time periods. We use our model to estimate the discount function and yield curve of nominally riskless government bonds. The novelty of our approach is the combination of two different techniques: cross-sectional nonparametric methods and kernel estimation for time varying dynamics in the time series context. The resulting estimator is used for predicting individual bond prices given the full schedule of their future payments. In addition, it is able to capture the yield curve shapes and dynamics commonly observed in the fixed income markets . We establish the consistency, the rate of convergence, and the asymptotic normality of the proposed estimator. A Monte Carlo exercise illustrates the good performance of the method under different scenarios. We apply our methodology to the daily CRSP bond market dataset, and compare ours with the popular Diebold and Li (2006) method.},
  archive      = {J_JOE},
  author       = {Bonsoo Koo and Davide La Vecchia and Oliver Linton},
  doi          = {10.1016/j.jeconom.2020.04.014},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {562-588},
  shortjournal = {J. Econ.},
  title        = {Estimation of a nonparametric model for bond prices from cross-section and time series information},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting granular time series in large panels.
<em>JOE</em>, <em>220</em>(2), 544–561. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large economic and financial panels can include time series that influence the entire cross-section. We name such series granular . In this paper we introduce a panel data model that allows to formalize the notion of granular time series. We then propose a methodology, which is inspired by the network literature in statistics and econometrics , to detect the set of granulars when such set is unknown. The influence of the i i th series in the panel is measured by the norm of the i i th column of the inverse covariance matrix. We show that a detection procedure based on the column norms allows to consistently select granular series when the cross-section and time series dimensions are large. Importantly, the methodology allows to consistently detect granulars also when the series in the panel are influenced by common factors. A simulation study shows that the proposed procedures perform satisfactorily in finite samples. Our empirical study shows the granular influence of the automobile sector in US industrial production.},
  archive      = {J_JOE},
  author       = {Christian Brownlees and Geert Mesters},
  doi          = {10.1016/j.jeconom.2020.04.013},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {544-561},
  shortjournal = {J. Econ.},
  title        = {Detecting granular time series in large panels},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An econometric approach to the estimation of multi-level
models. <em>JOE</em>, <em>220</em>(2), 532–543. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider “multidimensional” or “hierarchical” or “multilevel” models that are popular in the educational and economics literatures. Instead of two levels (individuals over time in the standard panel data model), we now have multiple levels (e.g. students in classrooms in schools in districts). We apply standard methods of analysis for econometric panel data to multilevel models. Specifically, we generalize the results of Hausman and Taylor and the subsequent literature to these models. This is a non-trivial extension because we now have more than one kind of time-invariant effect and more than one kind of “between” regression. We discuss estimation by GMM both with and without the assumption of no conditional heteroskedasticity . We also discuss endogeneity and dynamic models, and we generalize the concept of testing the exogeneity assumptions using a variable addition test.},
  archive      = {J_JOE},
  author       = {Yimin Yang and Peter Schmidt},
  doi          = {10.1016/j.jeconom.2020.04.012},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {532-543},
  shortjournal = {J. Econ.},
  title        = {An econometric approach to the estimation of multi-level models},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and inference for multi-dimensional heterogeneous
panel datasets with hierarchical multi-factor error structure.
<em>JOE</em>, <em>220</em>(2), 504–531. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the growing availability of large datasets and following recent research trends on multi-dimensional modelling, we develop three dimensional (3D) panel data models with hierarchical error components that allow for strong cross-section dependence through unobserved heterogeneous global and local factors. We propose consistent estimation procedures by extending the common correlated effects (CCE) estimation approach proposed by Pesaran (2006). The standard CCE approach needs to be modified in order to account for the hierarchical factor structure in 3D panels. Further, we provide asymptotic theory , including new nonparametric variance estimators . The validity of the proposed approach is confirmed by Monte Carlo simulation studies. We demonstrate the empirical usefulness of the proposed framework through an application to a 3D panel gravity model of bilateral export flows.},
  archive      = {J_JOE},
  author       = {George Kapetanios and Laura Serlenga and Yongcheol Shin},
  doi          = {10.1016/j.jeconom.2020.04.011},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {504-531},
  shortjournal = {J. Econ.},
  title        = {Estimation and inference for multi-dimensional heterogeneous panel datasets with hierarchical multi-factor error structure},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferential theory for heterogeneity and cointegration in
large panels. <em>JOE</em>, <em>220</em>(2), 474–503. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides an estimation and testing framework to assess the presence and the extent of slope heterogeneity and cointegration when the units are a mixture of spurious and/or cointegrating regressions. We propose two moment estimators for the degree of heterogeneity (measured by the dispersion of the slope coefficients around their average) and for the fraction of spurious regressions, which are found to be consistent in the whole parameter space. Based on these estimators, two tests for the null hypotheses of slope homogeneity and for cointegration are proposed. Monte Carlo simulations show that both tests have the correct size and satisfactory power.},
  archive      = {J_JOE},
  author       = {Lorenzo Trapani},
  doi          = {10.1016/j.jeconom.2020.04.010},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {474-503},
  shortjournal = {J. Econ.},
  title        = {Inferential theory for heterogeneity and cointegration in large panels},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heterogeneous structural breaks in panel data models.
<em>JOE</em>, <em>220</em>(2), 447–473. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new model and estimation procedure for panel data that allows us to identify heterogeneous structural breaks. We model individual heterogeneity using a grouped pattern. For each group, we allow common structural breaks in the coefficients. However, the number, timing, and size of these breaks can differ across groups. We develop a hybrid estimation procedure of the grouped fixed effects approach and adaptive group fused Lasso. We show that our method can consistently identify the latent group structure, detect structural breaks, and estimate the regression parameters . Monte Carlo results demonstrate the good performance of the proposed method in finite samples. An empirical application to the relationship between income and democracy illustrates the importance of considering heterogeneous structural breaks.},
  archive      = {J_JOE},
  author       = {Ryo Okui and Wendun Wang},
  doi          = {10.1016/j.jeconom.2020.04.009},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {447-473},
  shortjournal = {J. Econ.},
  title        = {Heterogeneous structural breaks in panel data models},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instrumental variable estimation of dynamic linear panel
data models with defactored regressors and a multifactor error
structure. <em>JOE</em>, <em>220</em>(2), 416–446. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops two instrumental variable (IV) estimators for dynamic panel data models with exogenous covariates and a multifactor error structure when both the cross-sectional and time series dimensions, N N and T T respectively, are large. The main idea is to project out the common factors from the exogenous covariates of the model, and to construct instruments based on defactored covariates. For models with homogeneous slope coefficients , we propose a two-step IV estimator. In the first step, the model is estimated consistently by employing defactored covariates as instruments. In the second step, the entire model is defactored based on estimated factors extracted from the residuals of the first-step estimation, after which an IV regression is implemented using the same instruments as in step one. For models with heterogeneous slope coefficients , we propose a mean-group-type estimator, which involves the averaging of first-step IV estimates of cross-section-specific slopes. The proposed estimators do not need to seek for instrumental variables outside the model. Furthermore, these estimators are linear, and therefore computationally robust and inexpensive. Notably, they require no bias correction. We investigate the finite sample performances of the proposed estimators and associated statistical tests, and the results show that the estimators and the tests perform well even for small N N and T T .},
  archive      = {J_JOE},
  author       = {Milda Norkutė and Vasilis Sarafidis and Takashi Yamagata and Guowei Cui},
  doi          = {10.1016/j.jeconom.2020.04.008},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {416-446},
  shortjournal = {J. Econ.},
  title        = {Instrumental variable estimation of dynamic linear panel data models with defactored regressors and a multifactor error structure},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of heterogeneous panels with systematic slope
variations. <em>JOE</em>, <em>220</em>(2), 399–415. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse estimation procedures for the panel data models with heterogeneous slopes. Specifically we take into account a possible dependence between regressors and heterogeneous slope coefficients , which is referred to as systematic variation. It is shown that under relevant forms of systematic slope variations (i) the pooled OLS estimator is severely biased, (ii) Swamy’s GLS estimator is inconsistent if the number of time periods T T is fixed, whereas (iii) the mean-group estimator always provides consistent estimators at the risk of high variances. Following Mundlak (1978) we propose an augmentated regression which results in a simple and robust version of the pooled estimator. The latter approach avoids the risk of large standard errors of the mean-group estimator, whenever T T is small. We also propose two test statistics for systematic slope variation using the Lagrange multiplier and Hausman principles. We derive their asymptotic properties and provide a local power analysis of both test statistics. Monte Carlo experiments corroborate our theoretical findings and show that for all combinations of N N and T T the Mundlak-type GLS estimator outperform all other estimators.},
  archive      = {J_JOE},
  author       = {Jörg Breitung and Nazarii Salish},
  doi          = {10.1016/j.jeconom.2020.04.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {399-415},
  shortjournal = {J. Econ.},
  title        = {Estimation of heterogeneous panels with systematic slope variations},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting the VIX and the volatility risk premium: The role
of short-run funding spreads volatility factors. <em>JOE</em>,
<em>220</em>(2), 366–398. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an innovative approach to extract Volatility Factors which predict the VIX, the S&amp;P500 Realized Volatility (RV) and the Variance Risk Premium (VRP). The approach is innovative along two different dimensions, namely: (1) we extract Volatility Factors from panels of filtered volatilities — in particular large panels of univariate ARCH-type models and propose methods to estimate common Volatility Factors in the presence of estimation error and (2) we price equity volatility risk using factors which go beyond the equity class namely Volatility Factors extracted from panels of volatilities of short-run funding spreads. The role of these Volatility Factors is compared with the corresponding factors extracted from the panels of the above spreads as well as related factors proposed in the literature. Our monthly short-run funding spreads Volatility Factors provide both in- and out-of-sample predictive gains for forecasting the monthly VIX, RV as well as the equity premium, while the corresponding daily volatility factors via Mixed Data Sampling (MIDAS) models provide further improvements.},
  archive      = {J_JOE},
  author       = {Elena Andreou and Eric Ghysels},
  doi          = {10.1016/j.jeconom.2020.04.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {366-398},
  shortjournal = {J. Econ.},
  title        = {Predicting the VIX and the volatility risk premium: The role of short-run funding spreads volatility factors},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating and testing high dimensional factor models with
multiple structural changes. <em>JOE</em>, <em>220</em>(2), 349–365. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers multiple changes in the factor loadings of a high dimensional factor model occurring at dates that are unknown but common to all subjects. Since the factors are unobservable, the problem is converted to estimating and testing structural changes in the second moments of the pseudo factors. We consider both joint and sequential estimation of the change points and show that the distance between the estimated and the true change points is O p ( 1 ) Op(1) . We find that the estimation error contained in the estimated pseudo factors has no effect on the asymptotic properties of the estimated change points as the cross-sectional dimension N N and the time dimension T T go to infinity jointly. No N N - T T ratio condition is needed. We also propose (i) tests for no change versus l l changes (ii) tests for l l changes versus l + 1 l+1 changes, and show that using estimated factors asymptotically has no effect on their limit distributions if T ∕ N → 0 T∕N→0 . These tests allow us to make inference on the presence and number of structural changes. Simulation results show good performance of the proposed procedure. In an application to US quarterly macroeconomic data we detect two possible breaks.},
  archive      = {J_JOE},
  author       = {Badi H. Baltagi and Chihwa Kao and Fa Wang},
  doi          = {10.1016/j.jeconom.2020.04.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {349-365},
  shortjournal = {J. Econ.},
  title        = {Estimating and testing high dimensional factor models with multiple structural changes},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the robustness of the pooled CCE estimator. <em>JOE</em>,
<em>220</em>(2), 325–348. (<a
href="https://doi.org/10.1016/j.jeconom.2020.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the existing estimators of factor-augmented regressions, the CCE approach is the most popular. A major reason for this popularity is the simplicity and good small-sample performance of the approach, making it very attractive from an empirical point of view. The main drawback is that most of the available asymptotic theory is based on quite restrictive assumptions, such as that the common factor component should be independent of the regressors . The present paper can be seen as a reaction to this. The purpose is to study the asymptotic properties of the pooled CCE estimator under more realistic conditions. In particular, the common factor component may be correlated with the regressors, and the true number of common factors, r r , can be larger than the number of estimated factors, which in CCE is given by k + 1 k+1 , where k k is the number of regressors. The main conclusion is that while the estimator is generally consistent, asymptotic normality can fail when r &gt; k + 1 r&amp;gt;k+1 .},
  archive      = {J_JOE},
  author       = {Artūras Juodis and Hande Karabiyik and Joakim Westerlund},
  doi          = {10.1016/j.jeconom.2020.06.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {325-348},
  shortjournal = {J. Econ.},
  title        = {On the robustness of the pooled CCE estimator},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlinear factor models for network and panel data.
<em>JOE</em>, <em>220</em>(2), 296–324. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor structures or interactive effects are convenient devices to incorporate latent variables in panel data models . We consider fixed effect estimation of nonlinear panel single-index models with factor structures in the unobservables, which include logit, probit, ordered probit and Poisson specifications. We establish that fixed effect estimators of model parameters and average partial effects have normal distributions when the two dimensions of the panel grow large, but might suffer from incidental parameter bias. We also show how models with factor structures can be applied to capture important features of network data such as reciprocity, degree heterogeneity, homophily in latent variables, and clustering. We illustrate this applicability with an empirical example to the estimation of a gravity equation of international trade between countries using a Poisson model with multiple factors.},
  archive      = {J_JOE},
  author       = {Mingli Chen and Iván Fernández-Val and Martin Weidner},
  doi          = {10.1016/j.jeconom.2020.04.004},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {296-324},
  shortjournal = {J. Econ.},
  title        = {Nonlinear factor models for network and panel data},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying latent group structures in nonlinear panels.
<em>JOE</em>, <em>220</em>(2), 272–295. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a procedure to identify latent group structures in nonlinear panel data models where some regression coefficients are heterogeneous across groups but homogeneous within a group and the group number and membership are unknown. To identify the group structures, we consider the order statistics for the preliminary unconstrained consistent estimators of the regression coefficients and translate the problem of classification into the problem of break detection. Then we extend the sequential binary segmentation algorithm of Bai (1997) for break detection from the time series setup to the panel data framework. We demonstrate that our method is able to identify the true latent group structures with probability approaching one and the post-classification estimators are oracle-efficient. The method has the advantage of more convenient implementation compared with some alternative methods, which is a desirable feature in nonlinear panel applications. To improve the finite sample performance, we also consider an alternative version based on the spectral decomposition of certain estimated matrix and link the group identification issue to the community detection problem in the network literature. Simulations show that our method has good finite sample performance. We apply this method to explore how individuals’ portfolio choices respond to their financial status and other characteristics using the Netherlands household panel data from year 1993 to 2015, and find three latent groups.},
  archive      = {J_JOE},
  author       = {Wuyi Wang and Liangjun Su},
  doi          = {10.1016/j.jeconom.2020.04.003},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {272-295},
  shortjournal = {J. Econ.},
  title        = {Identifying latent group structures in nonlinear panels},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric identification in panel data discrete
response models. <em>JOE</em>, <em>220</em>(2), 253–271. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies semiparametric identification in linear index discrete response panel data models with fixed effects. Departing from the classic binary response static panel data model, this paper examines identification in the binary response dynamic panel data model and the ordered response static panel data model. It is shown that under mild distributional assumptions on the fixed effect and the time-varying unobservables point-identification fails, but informative bounds on the regression coefficients can still be derived. Partial identification is achieved by eliminating the fixed effect and discovering features of the distribution of the unobservable time-varying components that do not depend on the unobserved heterogeneity. Numerical analyses illustrate how the identification bounds change as the support of the explanatory variables varies.},
  archive      = {J_JOE},
  author       = {Eleni Aristodemou},
  doi          = {10.1016/j.jeconom.2020.04.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {253-271},
  shortjournal = {J. Econ.},
  title        = {Semiparametric identification in panel data discrete response models},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Second-order corrected likelihood for nonlinear panel models
with fixed effects. <em>JOE</em>, <em>220</em>(2), 227–252. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a second-order correction for nonlinear fixed-effect panel models. The correction is made via the log-likelihood function. It removes the two leading terms of the bias of the log-likelihood that arises from estimating the fixed effects. Maximizing the corrected likelihood gives a second-order bias-corrected estimator, with bias O T − 3 OT−3 , where T T is the number of time periods . The corrected likelihood also gives second-order corrected test statistics. The correction applies to general nonlinear fixed-effect models with independent observations. The bias correction properties are confirmed in simulations for binary-choice models.},
  archive      = {J_JOE},
  author       = {Geert Dhaene and Yutao Sun},
  doi          = {10.1016/j.jeconom.2020.04.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {227-252},
  shortjournal = {J. Econ.},
  title        = {Second-order corrected likelihood for nonlinear panel models with fixed effects},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Celebrating 40 years of panel data analysis: Past, present
and future. <em>JOE</em>, <em>220</em>(2), 215–226. (<a
href="https://doi.org/10.1016/j.jeconom.2020.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present special issue features a collection of papers presented at the 2017 International Panel Data Conference, hosted by the University of Macedonia in Thessaloniki, Greece. The conference marked the 40th anniversary of the inaugural International Panel Data Conference, which was held in 1977 at INSEE in Paris, under the auspices of the French National Centre for Scientific Research. As a collection, the papers appearing in this special issue of the Journal of Econometrics continue to advance the analysis of panel data, and paint a state-of-the-art picture of the field.},
  archive      = {J_JOE},
  author       = {Vasilis Sarafidis and Tom Wansbeek},
  doi          = {10.1016/j.jeconom.2020.06.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {215-226},
  shortjournal = {J. Econ.},
  title        = {Celebrating 40 years of panel data analysis: Past, present and future},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of covid-19 prevalence from serology tests: A
partial identification approach. <em>JOE</em>, <em>220</em>(1), 193–213.
(<a href="https://doi.org/10.1016/j.jeconom.2020.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a partial identification method for estimating disease prevalence from serology studies. Our data are results from antibody tests in some population sample, where the test parameters, such as the true/false positive rates, are unknown. Our method scans the entire parameter space, and rejects parameter values using the joint data density as the test statistic. The proposed method is conservative for marginal inference, in general, but its key advantage over more standard approaches is that it is valid in finite samples even when the underlying model is not point identified. Moreover, our method requires only independence of serology test results, and does not rely on asymptotic arguments, normality assumptions, or other approximations. We use recent Covid-19 serology studies in the US, and show that the parameter confidence set is generally wide, and cannot support definite conclusions. Specifically, recent serology studies from California suggest a prevalence anywhere in the range 0\%-2\% (at the time of study), and are therefore inconclusive. However, this range could be narrowed down to 0.7\%–1.5\% if the actual false positive rate of the antibody test was indeed near its empirical estimate ( ∼ ∼ 0.5\%). In another study from New York state, Covid-19 prevalence is confidently estimated in the range 13\%–17\% in mid-April of 2020, which also suggests significant geographic variation in Covid-19 exposure across the US. Combining all datasets yields a 5\%–8\% prevalence range. Our results overall suggest that serology testing on a massive scale can give crucial information for future policy design, even when such tests are imperfect and their parameters unknown.},
  archive      = {J_JOE},
  author       = {Panos Toulis},
  doi          = {10.1016/j.jeconom.2020.10.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {193-213},
  shortjournal = {J. Econ.},
  title        = {Estimation of covid-19 prevalence from serology tests: A partial identification approach},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the COVID-19 infection rate: Anatomy of an
inference problem. <em>JOE</em>, <em>220</em>(1), 181–192. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a consequence of missing data on tests for infection and imperfect accuracy of tests, reported rates of cumulative population infection by the SARS CoV-2 virus are lower than actual rates of infection. Hence, reported rates of severe illness conditional on infection are higher than actual rates. Understanding the time path of the COVID-19 pandemic has been hampered by the absence of bounds on infection rates that are credible and informative. This paper explains the logical problem of bounding these rates and reports illustrative findings, using data from Illinois, New York, and Italy. We combine the data with assumptions on the infection rate in the untested population and on the accuracy of the tests that appear credible in the current context. We find that the infection rate might be substantially higher than reported. We also find that, assuming accurate reporting of deaths, the infection fatality rates in Illinois, New York, and Italy are substantially lower than reported.},
  archive      = {J_JOE},
  author       = {Charles F. Manski and Francesca Molinari},
  doi          = {10.1016/j.jeconom.2020.04.041},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {181-192},
  shortjournal = {J. Econ.},
  title        = {Estimating the COVID-19 infection rate: Anatomy of an inference problem},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse HP filter: Finding kinks in the COVID-19 contact
rate. <em>JOE</em>, <em>220</em>(1), 158–180. (<a
href="https://doi.org/10.1016/j.jeconom.2020.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we estimate the time-varying COVID-19 contact rate of a Susceptible–Infected–Recovered (SIR) model. Our measurement of the contact rate is constructed using data on actively infected, recovered and deceased cases. We propose a new trend filtering method that is a variant of the Hodrick–Prescott (HP) filter, constrained by the number of possible kinks. We term it the sparse HP filter and apply it to daily data from five countries: Canada, China, South Korea, the UK and the US. Our new method yields the kinks that are well aligned with actual events in each country. We find that the sparse HP filter provides a fewer kinks than the ℓ 1 ℓ1 trend filter, while both methods fitting data equally well. Theoretically, we establish risk consistency of both the sparse HP and ℓ 1 ℓ1 trend filters. Ultimately, we propose to use time-varying contact growth rates to document and monitor outbreaks of COVID-19.},
  archive      = {J_JOE},
  author       = {Sokbae Lee and Yuan Liao and Myung Hwan Seo and Youngki Shin},
  doi          = {10.1016/j.jeconom.2020.08.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {158-180},
  shortjournal = {J. Econ.},
  title        = {Sparse HP filter: Finding kinks in the COVID-19 contact rate},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). When will the covid-19 pandemic peak? <em>JOE</em>,
<em>220</em>(1), 130–157. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We carry out some analysis of the daily data on the number of new cases and the number of new deaths by (191) countries as reported to the European Centre for Disease Prevention and Control (ECDC). Our benchmark model is a quadratic time trend model applied to the log of new cases for each country. We use our model to predict when the peak of the epidemic will arise in terms of new cases or new deaths in each country and the peak level. We also predict how long the number of new daily cases in each country will fall by an order of magnitude. Finally, we also forecast the total number of cases and deaths for each country. We consider two models that link the joint evolution of new cases and new deaths.},
  archive      = {J_JOE},
  author       = {Shaoran Li and Oliver Linton},
  doi          = {10.1016/j.jeconom.2020.07.049},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {130-157},
  shortjournal = {J. Econ.},
  title        = {When will the covid-19 pandemic peak?},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the fraction of unreported infections in
epidemics with a known epicenter: An application to COVID-19.
<em>JOE</em>, <em>220</em>(1), 106–129. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an analytically tractable method to estimate the fraction of unreported infections in epidemics with a known epicenter and estimate the number of unreported COVID-19 infections in the U.S. during the first half of March 2020. Our method utilizes the covariation in initial reported infections across U.S. regions and the number of travelers to these regions from the epicenter, along with the results of an early randomized testing study in Iceland. Using our estimates of the number of unreported infections, which are substantially larger than the number of reported infections, we also provide estimates for the infection fatality rate using data on reported COVID-19 fatalities from U.S. counties.},
  archive      = {J_JOE},
  author       = {Ali Hortaçsu and Jiarui Liu and Timothy Schwieg},
  doi          = {10.1016/j.jeconom.2020.07.047},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {106-129},
  shortjournal = {J. Econ.},
  title        = {Estimating the fraction of unreported infections in epidemics with a known epicenter: An application to COVID-19},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consumer panic in the COVID-19 pandemic. <em>JOE</em>,
<em>220</em>(1), 86–105. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an econometric model of consumer panic (or panic buying) during the COVID-19 pandemic. Using Google search data on relevant keywords, we construct a daily index of consumer panic for 54 countries from January 1st to April 30th 2020. We also assemble data on government policy announcements and daily COVID-19 cases for all countries. Our panic index reveals widespread consumer panic in most countries, primarily during March, but with significant variation in the timing and severity of panic between countries. Our model implies that both domestic and world virus transmission contribute significantly to consumer panic. But government policy is also important: Internal movement restrictions – whether announced by domestic or foreign governments – generate substantial short run panic that largely vanishes in a week to ten days. Internal movement restrictions announced early in the pandemic generated more panic than those announced later. Stimulus announcements had smaller impacts, and travel restrictions do not appear to generate consumer panic.},
  archive      = {J_JOE},
  author       = {Michael Keane and Timothy Neal},
  doi          = {10.1016/j.jeconom.2020.07.045},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {86-105},
  shortjournal = {J. Econ.},
  title        = {Consumer panic in the COVID-19 pandemic},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification and estimation of the SEIRD epidemic model
for COVID-19. <em>JOE</em>, <em>220</em>(1), 63–85. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the SEIRD epidemic model for COVID-19. First, I show that the model is poorly identified from the observed number of deaths and confirmed cases. There are many sets of parameters that are observationally equivalent in the short run but lead to markedly different long run forecasts. Second, I show that the basic reproduction number R 0 R0 can be identified from the data, conditional on epidemiologic parameters, and propose several nonlinear SUR approaches to estimate R 0 R0 . I examine the performance of these methods using Monte Carlo studies and demonstrate that they yield fairly accurate estimates of R 0 R0 . Next, I apply these methods to estimate R 0 R0 for the US, California, and Japan, and document heterogeneity in the value of R 0 R0 across regions. My estimation approach accounts for possible underreporting of the number of cases. I demonstrate that if one fails to take underreporting into account and estimates R 0 R0 from the reported cases data, the resulting estimate of R 0 R0 may be biased downward and the resulting forecasts may exaggerate the long run number of deaths. Finally, I discuss how auxiliary information from random tests can be used to calibrate the initial parameters of the model and narrow down the range of possible forecasts of the future number of deaths.},
  archive      = {J_JOE},
  author       = {Ivan Korolev},
  doi          = {10.1016/j.jeconom.2020.07.038},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {63-85},
  shortjournal = {J. Econ.},
  title        = {Identification and estimation of the SEIRD epidemic model for COVID-19},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal impact of masks, policies, behavior on early covid-19
pandemic in the u.s. <em>JOE</em>, <em>220</em>(1), 23–62. (<a
href="https://doi.org/10.1016/j.jeconom.2020.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper evaluates the dynamic impact of various policies adopted by US states on the growth rates of confirmed Covid-19 cases and deaths as well as social distancing behavior measured by Google Mobility Reports, where we take into consideration people’s voluntarily behavioral response to new information of transmission risks in a causal structural model framework. Our analysis finds that both policies and information on transmission risks are important determinants of Covid-19 cases and deaths and shows that a change in policies explains a large fraction of observed changes in social distancing behavior. Our main counterfactual experiments suggest that nationally mandating face masks for employees early in the pandemic could have reduced the weekly growth rate of cases and deaths by more than 10 percentage points in late April and could have led to as much as 19 to 47 percent less deaths nationally by the end of May, which roughly translates into 19 to 47 thousand saved lives. We also find that, without stay-at-home orders, cases would have been larger by 6 to 63 percent and without business closures, cases would have been larger by 17 to 78 percent. We find considerable uncertainty over the effects of school closures due to lack of cross-sectional variation; we could not robustly rule out either large or small effects. Overall, substantial declines in growth rates are attributable to private behavioral response, but policies played an important role as well. We also carry out sensitivity analyses to find neighborhoods of the models under which the results hold robustly: the results on mask policies appear to be much more robust than the results on business closures and stay-at-home orders. Finally, we stress that our study is observational and therefore should be interpreted with great caution. From a completely agnostic point of view, our findings uncover predictive effects (association) of observed policies and behavioral changes on future health outcomes, controlling for informational and other confounding variables.},
  archive      = {J_JOE},
  author       = {Victor Chernozhukov and Hiroyuki Kasahara and Paul Schrimpf},
  doi          = {10.1016/j.jeconom.2020.09.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {23-62},
  shortjournal = {J. Econ.},
  title        = {Causal impact of masks, policies, behavior on early covid-19 pandemic in the U.S.},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Panel forecasts of country-level covid-19 infections.
<em>JOE</em>, <em>220</em>(1), 2–22. (<a
href="https://doi.org/10.1016/j.jeconom.2020.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use a dynamic panel data model to generate density forecasts for daily active Covid-19 infections for a panel of countries/regions. Our specification that assumes the growth rate of active infections can be represented by autoregressive fluctuations around a downward sloping deterministic trend function with a break. Our fully Bayesian approach allows us to flexibly estimate the cross-sectional distribution of slopes and then implicitly use this distribution as prior to construct Bayes forecasts for the individual time series. We find some evidence that information from locations with an early outbreak can sharpen forecast accuracy for late locations. There is generally a lot of uncertainty about the evolution of active infection, due to parameter and shock uncertainty, in particular before and around the peak of the infection path. Over a one-week horizon, the empirical coverage frequency of our interval forecasts is close to the nominal credible level. Weekly forecasts from our model are published at https://laurayuliu.com/covid19-panel-forecast/ .},
  archive      = {J_JOE},
  author       = {Laura Liu and Hyungsik Roger Moon and Frank Schorfheide},
  doi          = {10.1016/j.jeconom.2020.08.010},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {2-22},
  shortjournal = {J. Econ.},
  title        = {Panel forecasts of country-level covid-19 infections},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to pandemic econometrics/covid-19 pandemic.
<em>JOE</em>, <em>220</em>(1), 1. (<a
href="https://doi.org/10.1016/j.jeconom.2020.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Elie Tamer ( For the Editors )},
  doi          = {10.1016/j.jeconom.2020.11.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1},
  shortjournal = {J. Econ.},
  title        = {Introduction to pandemic econometrics/Covid-19 pandemic},
  volume       = {220},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
