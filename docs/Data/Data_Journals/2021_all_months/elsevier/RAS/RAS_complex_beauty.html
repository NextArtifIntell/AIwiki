<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>RAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ras---143">RAS - 143</h2>
<ul>
<li><details>
<summary>
(2021). Robotics and artificial intelligence in healthcare during
COVID-19 pandemic: A systematic review. <em>RAS</em>, <em>146</em>,
103902. (<a href="https://doi.org/10.1016/j.robot.2021.103902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of the COVID-19 pandemic is unarguably the biggest catastrophe of the 21st century, probably the most significant global crisis after the second world war. The rapid spreading capability of the virus has compelled the world population to maintain strict preventive measures. The outrage of the virus has rampaged through the healthcare sector tremendously. This pandemic created a huge demand for necessary healthcare equipment, medicines along with the requirement for advanced robotics and artificial intelligence-based applications. The intelligent robot systems have great potential to render service in diagnosis, risk assessment, monitoring, telehealthcare, disinfection , and several other operations during this pandemic which has helped reduce the workload of the frontline workers remarkably. The long-awaited vaccine discovery of this deadly virus has also been greatly accelerated with AI-empowered tools. In addition to that, many robotics and Robotics Process Automation platforms have substantially facilitated the distribution of the vaccine in many arrangements pertaining to it. These forefront technologies have also aided in giving comfort to the people dealing with less addressed mental health complicacies. This paper investigates the use of robotics and artificial intelligence-based technologies and their applications in healthcare to fight against the COVID-19 pandemic. A systematic search following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method is conducted to accumulate such literature, and an extensive review on 147 selected records is performed.},
  archive      = {J_RAS},
  author       = {Sujan Sarker and Lafifa Jamal and Syeda Faiza Ahmed and Niloy Irtisam},
  doi          = {10.1016/j.robot.2021.103902},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103902},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robotics and artificial intelligence in healthcare during COVID-19 pandemic: A systematic review},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust biped locomotion using deep reinforcement learning on
top of an analytical control approach. <em>RAS</em>, <em>146</em>,
103900. (<a href="https://doi.org/10.1016/j.robot.2021.103900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a modular framework to generate robust biped locomotion using a tight coupling between an analytical walking approach and deep reinforcement learning . This framework is composed of six main modules which are hierarchically connected to reduce the overall complexity and increase its flexibility. The core of this framework is a specific dynamics model which abstracts a humanoid’s dynamics model into two masses for modeling upper and lower body. This dynamics model is used to design an adaptive reference trajectories planner and an optimal controller which are fully parametric. Furthermore, a learning framework is developed based on Genetic Algorithm (GA) and Proximal Policy Optimization (PPO) to find the optimum parameters and to learn how to improve the stability of the robot by moving the arms and changing its center of mass height. A set of simulations are performed to validate the performance of the framework using the official RoboCup 3D League simulation environment. The results validate the performance of the framework, not only in creating a fast and stable gait but also in learning to improve the upper body efficiency.},
  archive      = {J_RAS},
  author       = {Mohammadreza Kasaei and Miguel Abreu and Nuno Lau and Artur Pereira and Luis Paulo Reis},
  doi          = {10.1016/j.robot.2021.103900},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103900},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robust biped locomotion using deep reinforcement learning on top of an analytical control approach},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time low-computation cost human-following framework
in outdoor environment for legged robots. <em>RAS</em>, <em>146</em>,
103899. (<a href="https://doi.org/10.1016/j.robot.2021.103899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legged robots have potential advantages in mobility compared with wheeled robots. Hence, legged robots are widely utilized in outdoor unstructured environments. Human-following operation is one of important tasks for outdoor robots. However, most current human-following strategies requires large amount of computation resource hence difficult to be applied to legged robots. This paper proposes real-time low-computation cost human-following framework in outdoor environment for legged robots. Our method takes a full consideration of the differences between legged robots and wheeled robots. Firstly, an on-line extrinsic calibration method is proposed to calculate the camera coordinate and the world coordinate system. Then, a real-time low-computation cost human-following method utilizing RGBD cameras and 3D LIDAR is proposed. The robot motion considers tracking the leading person while avoiding obstacles. Furthermore, a dynamic alternating tripod trotting gait is developed to control the robot to follow the leading person. Finally, the method is implemented and tested on a hexapod robot Qingzhui with indoor and outdoor experiments. The framework proposed in this paper can be a valuable reference for other legged robots when operated in outdoor environments.},
  archive      = {J_RAS},
  author       = {Yue Zhao and Yue Gao and Qiao Sun and Yuan Tian and Liheng Mao and Feng Gao},
  doi          = {10.1016/j.robot.2021.103899},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103899},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A real-time low-computation cost human-following framework in outdoor environment for legged robots},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid autonomous controller for bipedal robot balance with
deep reinforcement learning and pattern generators. <em>RAS</em>,
<em>146</em>, 103891. (<a
href="https://doi.org/10.1016/j.robot.2021.103891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering after an abrupt push is essential for bipedal robots in real-world applications within environments where humans must collaborate closely with robots. There are several balancing algorithms for bipedal robots in the literature, however most of them either rely on hard coding or power-hungry algorithms. We propose a hybrid autonomous controller that hierarchically combines two separate, efficient systems, to address this problem. The lower-level system is a reliable, high-speed, full state controller that was hardcoded on a microcontroller to be power efficient. The higher-level system is a low-speed reinforcement learning controller implemented on a low-power onboard computer. While one controller offers speed, the other provides trainability and adaptability. An efficient control is then formed without sacrificing adaptability to new dynamic environments. Additionally, as the higher-level system is trained via deep reinforcement learning , the robot could learn after deployment, which is ideal for real-world applications. The system’s performance is validated with a real robot recovering after a random push in less than 5 s, with minimal steps from its initial positions. The training was conducted using simulated data.},
  archive      = {J_RAS},
  author       = {Christos Kouppas and Mohamad Saada and Qinggang Meng and Mark King and Dennis Majoe},
  doi          = {10.1016/j.robot.2021.103891},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103891},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Hybrid autonomous controller for bipedal robot balance with deep reinforcement learning and pattern generators},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Delay compensated state estimation for telepresence robot
navigation. <em>RAS</em>, <em>146</em>, 103890. (<a
href="https://doi.org/10.1016/j.robot.2021.103890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telepresence robots empower human operators to navigate remote environments. However, operating and navigating the robot in an unknown environment is challenging due to delay in the communication network ( e.g., distance, bandwidth, communication drop-outs etc.), processing delays and slow dynamics of the mobile robots resulting in time-lagged in the system. Also, erroneous sensor data measurement which is important to estimate the robot’s true state (positional information) in the remote environment, often create complications and make it harder for the system to control the robot. In this paper, we propose a new approach for state estimation assuming uncertain delayed sensor measurements of a Telepresence robot during navigation. A new real world experimental model, based on Augmented State Extended Kalman Filter (AS-EKF), is proposed to estimate the true position of the Telepresence robot. The uncertainty of the delayed sensor measurements have been modelled using probabilistic density functions (PDF). The proposed model was successfully verified in our proposed experimental framework which consists of a state-of-the-art differential-drive Telepresence robot and a motion tracking multi-camera system. The results show significant improvements compared to the traditional EKF that does not consider uncertain delays in sensor measurements. The proposed model will be beneficial to build a real time predictive display by reducing the effect of visual delay to navigate the robot under the operator’s control command, without waiting for delayed sensor measurements.},
  archive      = {J_RAS},
  author       = {Barnali Das and Gordon Dobie},
  doi          = {10.1016/j.robot.2021.103890},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103890},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Delay compensated state estimation for telepresence robot navigation},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed herding of multiple robots in cluttered
environments. <em>RAS</em>, <em>146</em>, 103889. (<a
href="https://doi.org/10.1016/j.robot.2021.103889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces distributed herding controllers to make multiple robots (agents) follow a leader in environments with many unknown obstacles. The proposed herding controllers are developed considering agents which can only measure the bearing of a nearby agent using bearing sensors, such as camera. In addition, an agent uses Received Signal Strength Indicator (RSSI) sensors to detect the moment when it meets another agent. Every agent, except for the leader, is not equipped with sensors to localize itself. Every agent can only detect nearby agents and moves utilizing local sensing measurements. Since every agent moves in an unknown cluttered environment, communication or sensing between agents may be lost blocked by obstacles. Therefore, this article presents distributed herding controllers so that multiple agents follow a leader while maintaining network connectivity in unknown cluttered environments. The leader changes its velocity adaptively so that it can reach the goal while maintaining network connectivity with its followers. The proposed herding controllers are developed to overcome a network faulty situation. As far as we know, this paper is novel in presenting distributed herding controls of multiple agents , such that the network connectivity is maintained while agents move in cluttered environments. Under MATLAB simulations, we verify the effectiveness of the proposed herding approach in cluttered environments.},
  archive      = {J_RAS},
  author       = {Jonghoek Kim},
  doi          = {10.1016/j.robot.2021.103889},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103889},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Distributed herding of multiple robots in cluttered environments},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A flexible and collaborative approach to robotic box-filling
and item sorting. <em>RAS</em>, <em>146</em>, 103888. (<a
href="https://doi.org/10.1016/j.robot.2021.103888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce an adaptive robotic manipulation framework to respond to the flexibility needs of common industrial tasks such as box-filling and item sorting. The proposed framework consists of a vision module and a robot control module. The vision module is responsible for the detection and tracking of the environment (e.g., box and the items), which is also capable of creating an occupancy grid in real-time, to continuously update the robot trajectory planner with the occupied portions of the detected box and their coordinates. The robot control module includes a trajectory planner and a self-tuning Cartesian impedance controller, to implement an adaptive strategy for the picking, placement, and sorting of the items in the box. The item-sorting strategy is based on our preliminary observations on human motor behavior, implementing a trade-off between the task execution accuracy and environmental perception uncertainty. The efficacy of the framework in performing a flexible box-filling task using a robot, autonomously or in collaboration with a human, is evaluated through several experiments.},
  archive      = {J_RAS},
  author       = {Pietro Balatti and Mattia Leonori and Arash Ajoudani},
  doi          = {10.1016/j.robot.2021.103888},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103888},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A flexible and collaborative approach to robotic box-filling and item sorting},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-finger robot system for adaptive landing gear and
aerial manipulation. <em>RAS</em>, <em>146</em>, 103878. (<a
href="https://doi.org/10.1016/j.robot.2021.103878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a bioinspired multi-finger robot system (MFRS) is designed based on the characteristics of eagle claws. The MFRS is attached to a rotary-wing unmanned aerial vehicle (RUAV), which can be enabled to land on uneven terrain. In addition, the robot can also grab target objects or perch on a cylinder. The finger of the robot can simultaneously rotate three revolute joints only relying on one motor, to achieve an action similar to the grip of the eagle claw. The hardware structure and control system architecture of the MFRS are established. Based on depth vision, an adaptive landing algorithm that can achieve real-time optimal landing point selection is proposed. The outdoor experiments show that the robot can effectively land the RUAV on the slopes, steps, and unstructured terrains.},
  archive      = {J_RAS},
  author       = {Jian Liu and Dan Zhang and Chenwei Wu and Hongyan Tang and Chunxu Tian},
  doi          = {10.1016/j.robot.2021.103878},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103878},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A multi-finger robot system for adaptive landing gear and aerial manipulation},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PC-SD-VIO: A constant intensity semi-direct monocular
visual-inertial odometry with online photometric calibration.
<em>RAS</em>, <em>146</em>, 103877. (<a
href="https://doi.org/10.1016/j.robot.2021.103877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brightness constancy assumption is the cornerstone of direct or semi-direct visual odometry (VO) and visual simultaneous localization and mapping (SLAM). However, due to the existence of automatic exposure time, nonlinear camera response function , and vignetting, this assumption is difficult to hold in practical applications. Therefore, the corresponding algorithm performs poorly on arbitrary video sequences and uncalibrated cameras. Hence, we propose a novel constant intensity semi-direct visual-inertial odometry (VIO) integrated with online photometric calibration, which combines the exactness of the feature-based method and the quickness of the direct method. We combine gain-adaptive direct image alignment and gain-adaptive Kanade–Lucas–Tomasi (KLT) optical flow tracking to complete the feature matching and use it as the input for the back-end optimization and online photometric calibration. Our photometric calibration module can complete the estimation of all photometric parameters without any prior knowledge and cooperate with the front-end to complete the real-time photometric calibration of the latest frame. Experiments on the TUM Mono VO dataset, EuRoC dataset, and real environments prove that the algorithm can reliably calibrate the photometric parameters of an arbitrary video sequence. The semi-direct VIO algorithm integrated with the photometric calibration algorithm achieves a good balance between speed, accuracy, and robustness.},
  archive      = {J_RAS},
  author       = {Quanpan Liu and Zhengjie Wang and Huan Wang},
  doi          = {10.1016/j.robot.2021.103877},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103877},
  shortjournal = {Robot. Auton. Syst.},
  title        = {PC-SD-VIO: A constant intensity semi-direct monocular visual-inertial odometry with online photometric calibration},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised fault detection and recovery for intelligent
robotic rollators. <em>RAS</em>, <em>146</em>, 103876. (<a
href="https://doi.org/10.1016/j.robot.2021.103876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis is a key safety component in robotic assistive technologies . Although conventional model-based methods for sensor fault diagnosis in mobile robots have been well established, they face challenges due to model parameter changes and uncertainties. On the other hand, data-driven approaches becomes more appealing in order to take advantage from available historical data in the era of Big Data . To provide a new generic unsupervised solution to the fault detection and recovery, we explicitly include kinematic relations and temporal finite differences from measured sensor signals into training a multi-task deep neural network . To evaluate the proposed fault diagnosis and recovery framework, experiments have been conducted on a robotic rollator platform. Experiments under several conditions confirm that the proposed approach, which leverages machine learning-enhanced algorithms, exhibits reliable performance. Outperforming other baselines and state-of-the-art diagnosis algorithms , the framework presents a promising solution to sensor fault recovery challenges in assistive devices .},
  archive      = {J_RAS},
  author       = {Yiwen Liao and Abdullah Yeaser and Bin Yang and James Tung and Ehsan Hashemi},
  doi          = {10.1016/j.robot.2021.103876},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103876},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Unsupervised fault detection and recovery for intelligent robotic rollators},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Engineering efficient and massively parallel 3D
self-reconfiguration using sandboxing, scaffolding and coating.
<em>RAS</em>, <em>146</em>, 103875. (<a
href="https://doi.org/10.1016/j.robot.2021.103875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programmable matter based on modular self-reconfigurable robots could stand as the ultimate form of display system, through which humans could not only see the virtual world in 3D, but manipulate it and interact with it through touch. These systems rely on self-reconfiguration processes to reshape themselves and update their representation, using methods that we argue, are currently too slow for such applications due to a lack of parallelism in the motion of the robotic modules. Therefore, we propose a novel approach to the problem, promising faster and more efficient self-reconfigurations in programmable matter display systems. We contend that this can be achieved by using a dedicated platform supporting self-reconfiguration named a sandbox , acting as a reserve of modules, and by engineering the representation of objects using an internal scaffolding covered by a coating . This paper introduces a complete view of our framework for realizing this approach on quasi-spherical modules arranged in a face-centered cubic lattice. After thoroughly discussing the model, motivations, and making a case for our method, we synthesize results from published research highlighting its benefits and engage in an honest and critical discussion of its current state of implementation and perspectives.},
  archive      = {J_RAS},
  author       = {Pierre Thalamy and Benoît Piranda and Julien Bourgeois},
  doi          = {10.1016/j.robot.2021.103875},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103875},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Engineering efficient and massively parallel 3D self-reconfiguration using sandboxing, scaffolding and coating},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal feature fusion for better understanding of human
personality traits in social human–robot interaction. <em>RAS</em>,
<em>146</em>, 103874. (<a
href="https://doi.org/10.1016/j.robot.2021.103874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the dynamic nature of human–robot interaction becomes increasingly prevalent in our daily life, there is a great demand for enabling the robot to better understand human personality traits and inspiring humans to be more engaged in the interaction with the robot. Therefore, in this work, as we design the paradigm of human–robot interaction as close to the real situation as possible, the following three main problems are addressed: (1) fusion of visual and audio features of human interaction modalities, (2) integration of variable length feature vectors, and (3) compensation of shaky camera motion caused by movements of the robot’s communicative gesture. Specifically, the three most important visual features of humans including head motion, gaze, and body motion were extracted from a camera mounted on the robot performing verbal and body gestures during the interaction. Then, our system was geared to fuse the aforementioned visual features and different types of vocal features, such as voice pitch, voice energy, and Mel-Frequency Cepstral Coefficient , dealing with variable length multiple feature vectors. Lastly, considering unknown patterns and sequential characteristics of human communicative behavior, we proposed a multi-layer Hidden Markov Model that improved the classification accuracy of personality traits and offered notable advantages of fusing the multiple features. The results were thoroughly analyzed and supported by psychological studies. The proposed multi-modal fusion approach is expected to deepen the communicative competence of social robots interacting with humans from different cultures and backgrounds.},
  archive      = {J_RAS},
  author       = {Zhihao Shen and Armagan Elibol and Nak Young Chong},
  doi          = {10.1016/j.robot.2021.103874},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103874},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Multi-modal feature fusion for better understanding of human personality traits in social human–robot interaction},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coordination approaches for multi-item pickup and delivery
in logistic scenarios. <em>RAS</em>, <em>146</em>, 103871. (<a
href="https://doi.org/10.1016/j.robot.2021.103871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the Multi-Robot pickup and delivery problem for logistic scenarios that recently received significant attention from the research community. In particular we consider an innovative variant of the pickup and delivery problem where robots can deliver, in a single travel, multiple items. We propose a decentralized coordination algorithm based on a token passing approach. Our algorithm allocates delivery tasks (i.e., an aggregation of items to be delivered by a single robot) to the Multi-Robot System avoiding conflicts among the robots. In more detail, we show that our approach generates conflict-free paths for the Multi-Robot system requiring weaker assumptions on the operational area compared to previous approaches. We empirically evaluate the proposed method on three different scenarios, including the production line of a smart factory, comparing the performance of our decentralized method against two centralized approaches. Results show that our approach finds solutions of similar quality (in terms of makespan and travel distance) reducing the associated computational effort.},
  archive      = {J_RAS},
  author       = {Antonello Contini and Alessandro Farinelli},
  doi          = {10.1016/j.robot.2021.103871},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103871},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Coordination approaches for multi-item pickup and delivery in logistic scenarios},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heuristic-based method for conflict discovery of shared
control between humans and autonomous systems - a driving automation
case study. <em>RAS</em>, <em>146</em>, 103867. (<a
href="https://doi.org/10.1016/j.robot.2021.103867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a heuristic-based prospective method to discover possible conflicts of shared control between humans and autonomous systems . This method adapts the triplet Competence-Availability-Possibility-to-act (CAP) that represents the autonomy characteristics of decision-makers such as humans or autonomous systems . The CAP-based autonomy is decomposed into several scenarios of shared control in a workspace or between workspaces. Conflicting decisions between humans and autonomous systems are conflicts of autonomy relating to competence, availability, or possibility to act and are determined by applying heuristics adapted from deductive, inductive, abductive, and counterfactual reasoning principles. This heuristic-based method comprises four main steps: verification of shared control, identification of discovery parameters, discovery of possible conflicting decisions, and validation of these conflicts. It was applied to a driving automation case study involving two autonomous systems: Lane Keeping Assist (LKA) and Automated Cruise Control (ACC). The conflicts of autonomy identified determine possible confusions between the reasoning of a driver and that of an autonomous system, sometimes resulting in dangerous situations. These were validated and analyzed in two ways by drivers with at least two years of driving experience: during an investigation to obtain qualitative feedback from 43 drivers and during a tutorial on human reliability assessment involving 17 people. The results demonstrate the advantage of the heuristic-based method to detect possible conflicting decisions or sources of conflicts between humans and machines. The approach proposed will consequently be of assistance in the design of shared control processes between humans and autonomous systems through the implementation of technical learning or pedagogical abilities, the improvement of alarm systems to control human attention or avoid confusion between the intentions of humans and machines, or the development of training programs or driving lessons to increase user awareness of such conflicts.},
  archive      = {J_RAS},
  author       = {Frédéric Vanderhaegen},
  doi          = {10.1016/j.robot.2021.103867},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103867},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Heuristic-based method for conflict discovery of shared control between humans and autonomous systems - a driving automation case study},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light deep learning models enriched with entangled features
for RGB-d semantic segmentation. <em>RAS</em>, <em>146</em>, 103862. (<a
href="https://doi.org/10.1016/j.robot.2021.103862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a crucial task in emerging robotic applications like autonomous driving and social robotics. State-of-the-art methods in this field rely on deep learning , with several works in the literature following the trend of using larger networks to achieve higher performance. However, this leads to greater model complexity and higher computational costs, which make it difficult to integrate such models on mobile robots. In this work we investigate how it is possible to obtain lighter performing deep models introducing additional data at a very low computational cost, instead of increasing the network complexity. We consider the features used in the 3D Entangled Forests algorithm, proposing different strategies to integrate such additional information into different deep networks. The new features allow to obtain lighter and performing segmentation models , either by shrinking the network size or improving existing networks proposed for real-time segmentation. Such result represents an interesting alternative in mobile robotics application, where computational power and energy are limited.},
  archive      = {J_RAS},
  author       = {Matteo Terreran and Stefano Ghidoni},
  doi          = {10.1016/j.robot.2021.103862},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103862},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Light deep learning models enriched with entangled features for RGB-D semantic segmentation},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-spectral image synthesis for crop/weed segmentation in
precision farming. <em>RAS</em>, <em>146</em>, 103861. (<a
href="https://doi.org/10.1016/j.robot.2021.103861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective perception system is a fundamental component for farming robots, as it enables them to properly perceive the surrounding environment and to carry out targeted operations. The most recent methods make use of state-of-the-art machine learning techniques to learn a valid model for the target task. However, those techniques need a large amount of labeled data for training. A recent approach to deal with this issue is data augmentation through Generative Adversarial Networks (GANs), where entire synthetic scenes are added to the training data, thus enlarging and diversifying their informative content. In this work, we propose an alternative solution with respect to the common data augmentation methods, applying it to the fundamental problem of crop/weed segmentation in precision farming. Starting from real images, we create semi-artificial samples by replacing the most relevant object classes (i.e., crop and weeds) with their synthesized counterparts. To do that, we employ a conditional GAN (cGAN), where the generative model is trained by conditioning the shape of the generated object. Moreover, in addition to RGB data, we take into account also near-infrared (NIR) information, generating four channel multi-spectral synthetic images . Quantitative experiments, carried out on three publicly available datasets, show that (i) our model is capable of generating realistic multi-spectral images of plants and (ii) the usage of such synthetic images in the training process improves the segmentation performance of state-of-the-art semantic segmentation convolutional networks .},
  archive      = {J_RAS},
  author       = {Mulham Fawakherji and Ciro Potena and Alberto Pretto and Domenico D. Bloisi and Daniele Nardi},
  doi          = {10.1016/j.robot.2021.103861},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103861},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Multi-spectral image synthesis for Crop/Weed segmentation in precision farming},
  volume       = {146},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new human-aware robot navigation framework based on
time-dependent social interaction spaces: An application to assistive
robots in caregiving centers. <em>RAS</em>, <em>145</em>, 103873. (<a
href="https://doi.org/10.1016/j.robot.2021.103873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most critical problems to be addressed by future generation socially-assistive robots working in semi-organized social environments, such as shopping centers, nursing homes, airports , hospitals, or assisted living centers, is the capability of human-aware navigation. Autonomous navigation in a complex environment with people, staff with different roles, timetables, and restrictions to access, among others, requires adapting to socially accepted rules. Consequently, the path-planner must consider concepts related to proxemics and personal spaces of interaction that include human–human, human–robot, or human–object combinations. Likewise, the speed of approaching people, both to initiate communication or to navigate nearby, must be adapted to social conventions. Some of these situations have already been studied in the literature with varying degrees of success. However, the concept of time dependency or chronemics in the robot social navigation has been poorly explored. Current algorithms do not take into account the social complexity of real environments and their relationship with the time of day or the activities performed in these scenarios. This article presents a new framework for robot social navigation in human environments, introducing the concept of time-dependent social mapping. The main novelty is that the social route planned by the robot considers variables that depend on the time and the scheduled center activities. The article describes how the areas of interaction vary over time and how they affect human-aware navigation. To this end, the proposed navigation stack defines a new function for time-dependent social interaction space that takes continuous values and is configurable by the center’s staff. The global path-planner uses this function to choose dynamically a socially accepted path to the target. Then, the framework uses an elastic band path optimizer as a local planner, adapting the robot’s navigation to possible changes during the trajectory. Several use cases in simulated caregiving centers have been explored to validate the robot’s social navigation improvements using these temporal variables.},
  archive      = {J_RAS},
  author       = {L.V. Calderita and A. Vega and P. Bustos and P. Núñez},
  doi          = {10.1016/j.robot.2021.103873},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103873},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A new human-aware robot navigation framework based on time-dependent social interaction spaces: An application to assistive robots in caregiving centers},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain adversarial transfer for cross-domain and
task-constrained grasp pose detection. <em>RAS</em>, <em>145</em>,
103872. (<a href="https://doi.org/10.1016/j.robot.2021.103872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring the grasping skills learned from simulated environments to the real world is favorable for many robotic applications , in which the collecting and labeling processes of real-world visual grasping datasets are often expensive or even impractical. However, the models purely trained on simulated data are often difficult to generalize well to the unseen real world due to the domain gap between the training and testing data. In this paper, we propose a novel domain adversarial transfer network to narrow the domain gap for cross-domain and task-constrained grasp pose detection. Generative adversarial training is exploited to constrain the generator to produce simulation-like data for extracting the shared features with the joint distribution. We also propose to improve the backbone by extracting task-constrained grasp candidates and constructing the grasp candidate evaluator with a lightweight structure and an embedded recalibration technique. To validate the effectiveness and superiority of our proposed method, grasping performance evaluation and task-oriented human–robot interaction experiments were investigated. The experiment results indicate that the proposed method achieves state-of-the-art performance in these experimental settings. An average task-constrained grasping success rate of 83.3\% without using any real-world labels for the task-oriented human–robot interaction experiment was achieved especially.},
  archive      = {J_RAS},
  author       = {Xingshuo Jing and Kun Qian and Xin Xu and Jishen Bai and Bo Zhou},
  doi          = {10.1016/j.robot.2021.103872},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103872},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Domain adversarial transfer for cross-domain and task-constrained grasp pose detection},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid force/position control in workspace of robotic
manipulator in uncertain environments based on adaptive fuzzy control.
<em>RAS</em>, <em>145</em>, 103870. (<a
href="https://doi.org/10.1016/j.robot.2021.103870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel hybrid force/position controller in workspace of robotic manipulator based on adaptive fuzzy control is developed to improve the controlling performance of force and position in contact with uncertain environments. The dynamic model of the robotic manipulator in joint space is converted to a dynamic model in workspace, which is consisted of the model of position-controlled subsystem and model of force-controlled subsystem. Furthermore, in the position-controlled subsystem, an adaptive fuzzy computed torque control is proposed by considering adaptive fuzzy control and conventional computed torque control (AFCTC), for compensating deviations caused by the presence of structured uncertainty and unstructured uncertainty. In the force-controlled subsystem, a fuzzy proportional integral control method (FPI) is proposed by fuzzy logic and conventional proportional integral method to improve the control performance. The asymptotic stability of the developed controller is proved by Lyapunov theorem. The simulation results show the preferable performance of the proposed controller (AFCTC-FPI) by comparison with adaptive fuzzy sliding mode control (AFSMC), adaptive network-based fuzzy inference system with proportion differential and integral (ANFIS-PD+I), computed torque control-proportion integral (CTC-PI), fuzzy proportional integral derivative (FPID), and proportional integral derivative (PID) in uncertain environments. Furthermore, in the experimental study, average position error with AFCTC-FPI is decreased by 87.14\% than that with PID, meanwhile, range of interaction force with AFCTC-FPI is reduced by 70.31\% than that with PID. In summary, the experimental results also show the superior control accuracy of the proposed controller in real environment.},
  archive      = {J_RAS},
  author       = {Ziling Wang and Lai Zou and Xiaojie Su and Guoyue Luo and Rui Li and Yun Huang},
  doi          = {10.1016/j.robot.2021.103870},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103870},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Hybrid force/position control in workspace of robotic manipulator in uncertain environments based on adaptive fuzzy control},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development and validation of a ROS-based mobile robotic
platform for human gait analysis applications. <em>RAS</em>,
<em>145</em>, 103869. (<a
href="https://doi.org/10.1016/j.robot.2021.103869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of mobile robotic platforms in human gait analysis offers the potential to develop multiple medical applications and achieve new discoveries. The aim of this paper is to present a first design and validation of a ROS-based mobile robotic platform for human gait analysis . During the design stage, the model identification and the configuration of the control law were performed. The design of the control law required the integration of a lead compensator and a Filtered Smith Predictor (FSP). During the validation procedure, the accuracy of the system to retrieve kinematic gait data and the main descriptors of gait disorders was calculated with respect to the ground truth of a Vicon system. For this purpose, one hundred gait recordings were processed thanks to the collaboration of twenty participants. The participants walked in a one-way straight line gait. Results showed high correlation and low error rates mainly in joint excursions from sagittal and transverse planes. This gait analysis system demonstrated several advantages compared with the current approaches. The use of a mobile robotic platform allowed gait analysis in long tracking ranges and without space limitations. Furthermore, the design of a suitable control law allowed a smooth tracking of the person. This led to optimal results when assessing joint excursions. This system represents a cost-effective and non-invasive alternative that could be used for human gait analysis applications.},
  archive      = {J_RAS},
  author       = {Diego Guffanti and Alberto Brunete and Miguel Hernando},
  doi          = {10.1016/j.robot.2021.103869},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103869},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Development and validation of a ROS-based mobile robotic platform for human gait analysis applications},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Precise hand–eye calibration method based on spatial
distance and epipolar constraints. <em>RAS</em>, <em>145</em>, 103868.
(<a href="https://doi.org/10.1016/j.robot.2021.103868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new hand–eye calibration method based on spatial distance and epipolar constraints is proposed to obtain the transformation X X between the end effector (hand) of the robotic arm and the camera (eye) fixed on the end effector. Most of the current effective hand–eye calibration methods utilize the classical identity, AX = XB AX=XB , to obtain the analytical solution of X X , and then apply various constraints to iteratively optimize the initial X X , but these constraints are often at the 2D level. However, the result of hand–eye​ calibration needs to ensure the accuracy of the vision-guided robot arm system operating in 3D space, which leads to inconsistency between optimization goals and the actual requirements. Therefore, the proposed method introduces 3D constraints into the iterative optimization of X X and takes 3D error as an evaluation indicator of the calibration quality. There are two main steps in the proposed method. Firstly, the initial value of hand–eye transformation matrix is calculated by utilizing Kronecker product, which avoids the error propagation from rotation parameters to translation parameters. Then, the inherent epipolar constraints and the spatial distance constraints between feature points are combined to optimize hand–eye calibration parameters iteratively. To evaluate the precision and robustness of the proposed method, both simulation experiment and real experiment are carried out. The experimental results show that compared with conventional methods, the proposed method has higher accuracy and stronger robustness.},
  archive      = {J_RAS},
  author       = {Zhenyu Liu and Xia Liu and Guifang Duan and Jianrong Tan},
  doi          = {10.1016/j.robot.2021.103868},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103868},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Precise hand–eye calibration method based on spatial distance and epipolar constraints},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive heterogeneous multi-robot collaboration from formal
task specifications. <em>RAS</em>, <em>145</em>, 103866. (<a
href="https://doi.org/10.1016/j.robot.2021.103866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently coordinating different types of robots is an important enabler for many commercial and industrial automation tasks. Here, we present a distributed framework that enables a team of heterogeneous robots to dynamically generate actions from a common, user-defined goal specification . In particular, we discuss the integration of various robotic capabilities into a common task allocation and planning formalism, as well as the specification of expressive, temporally-extended goals by non-expert users. Models for task allocation and execution both consider non-deterministic outcomes of actions and thus, are suitable for a wide range of real-world tasks including formally specified reactions to online observations. One main focus of our paper is to evaluate the framework and its integration of software modules through a number of experiments. These experiments comprise industry-inspired scenarios as motivated by future real-world applications. Finally, we discuss the results and learnings for motivating practically relevant, future research questions.},
  archive      = {J_RAS},
  author       = {Philipp Schillinger and Sergio García and Alexandros Makris and Konstantinos Roditakis and Michalis Logothetis and Konstantinos Alevizos and Wei Ren and Pouria Tajvar and Patrizio Pelliccione and Antonis Argyros and Kostas J. Kyriakopoulos and Dimos V. Dimarogonas},
  doi          = {10.1016/j.robot.2021.103866},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103866},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Adaptive heterogeneous multi-robot collaboration from formal task specifications},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A riemannian metric for geometry-aware singularity avoidance
by articulated robots. <em>RAS</em>, <em>145</em>, 103865. (<a
href="https://doi.org/10.1016/j.robot.2021.103865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Articulated robots such as manipulators increasingly must operate in uncertain and dynamic environments where interaction (with human coworkers, for example) is necessary. In these situations, the capacity to quickly adapt to unexpected changes in operational space constraints is essential. At certain points in a manipulator’s configuration space , termed singularities , the robot loses one or more degrees of freedom (DoF) and is unable to move in specific operational space directions. The inability to move in arbitrary directions in operational space compromises adaptivity and, potentially, safety. We introduce a geometry-aware singularity index , defined using a Riemannian metric on the manifold of symmetric positive definite matrices , to provide a measure of proximity to singular configurations . We demonstrate that our index avoids some of the failure modes and difficulties inherent to other common indices. Further, we show that our index can be differentiated easily, making it compatible with local optimization approaches used for operational space control. Our experimental results establish that, for reaching and path following tasks, optimization based on our index outperforms a common manipulability maximization technique and ensures singularity-robust motions.},
  archive      = {J_RAS},
  author       = {Filip Marić and Luka Petrović and Marko Guberina and Jonathan Kelly and Ivan Petrović},
  doi          = {10.1016/j.robot.2021.103865},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103865},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A riemannian metric for geometry-aware singularity avoidance by articulated robots},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ring gaussian mixture modelling and regression for
collaborative robots. <em>RAS</em>, <em>145</em>, 103864. (<a
href="https://doi.org/10.1016/j.robot.2021.103864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task Parametrised Gaussian Mixture Modelling and Regression (TP-GMM/R) is an eminent algorithm to enable collaborative robots (cobots) to adapt to new environments intuitively by learning robotic paths demonstrated by humans. Task parameters in the TP-GMM/R algorithm, i.e., frames associated with demonstration paths, are considered to have orientations by default. This requirement, however, limits the range of applications that TP-GMM/R can support. To address the issue, in this paper, a novel ring Gaussian ( r Gaussian) is defined to cater for orientation-less frames, and an improved TP-GMM/R algorithm based on r Gaussians is developed to improve the adaptability and robustness of the algorithm. In the improved algorithm, firstly, kernels are incorporated to enable Gaussians encoding points from all demonstrations, and criteria are devised to judge a frame to be oriented or orientation-less. Then, improved Gaussian mixture regression that caters for r Gaussians and orientation-less frames is developed to generate regression paths adaptable to complex environments. Finally, a series of case studies are used to benchmark the improved TP-GMM/R algorithm with the conventional TP-GMM/R algorithm under different conditions. Quantitative analyses are conducted in terms of smoothness, efficiency and reachability . Results show that the improved algorithm outperformed the conventional algorithm on all the cases.},
  archive      = {J_RAS},
  author       = {Shirine El Zaatari and Weidong Li and Zahid Usman},
  doi          = {10.1016/j.robot.2021.103864},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103864},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Ring gaussian mixture modelling and regression for collaborative robots},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Receding horizon task and motion planning in changing
environments. <em>RAS</em>, <em>145</em>, 103863. (<a
href="https://doi.org/10.1016/j.robot.2021.103863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex manipulation tasks require careful integration of symbolic reasoning and motion planning. This problem, commonly referred to as Task and Motion Planning (TAMP), is even more challenging if the workspace is non-static, e.g. due to human interventions and perceived with noisy non-ideal sensors. This work proposes an online approximated TAMP method that combines a geometric reasoning module and a motion planner with a standard task planner in a receding horizon fashion. Our approach iteratively solves a reduced planning problem over a receding window of a limited number of future actions during the implementation of the actions. Thus, only the first action of the horizon is actually scheduled at each iteration, then the window is moved forward, and the problem is solved again. This procedure allows to naturally take into account potential changes in the scene while ensuring good runtime performance. We validate our approach within extensive experiments in a simulated environment. We showed that our approach is able to deal with unexpected changes in the environment while ensuring comparable performance with respect to other recent TAMP approaches in solving traditional static benchmarks. We release with this paper the open-source implementation of our method.},
  archive      = {J_RAS},
  author       = {Nicola Castaman and Enrico Pagello and Emanuele Menegatti and Alberto Pretto},
  doi          = {10.1016/j.robot.2021.103863},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103863},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Receding horizon task and motion planning in changing environments},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive grasp taxonomy of continuum robots.
<em>RAS</em>, <em>145</em>, 103860. (<a
href="https://doi.org/10.1016/j.robot.2021.103860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuum robots (CRs) have been the subject of intensive research in recent years due to their wide range of potential applications. Research on grasp taxonomy plays a key role in a number of task-based problems such as grasp synthesis, motion planning, and motion control. Additionally, grasp taxonomy has been shown to reduce the complexity of the design of robotic systems and human–computer interaction operations. The main goal of this research is to present a general CR-based grasp taxonomy. For this purpose, we first overview and summarize different types of CR-based grasp tasks. Then, we compare existing CR-based grasp configurations in the CR-related literature and classify the configurations into a comprehensive taxonomy. On the basis of this survey, nine major CR-based grasp families are introduced and arranged in subgroups for more detailed research. It should be noted that we studied grasps performed by different CR types and configurations without considering the object/CR size. Finally, the work includes different analyses of CR-based grasp taxonomy, including grasp frequency, grasp adaptability, taxonomy completeness, and properties of manipulated objects and tasks enabled by the proposed taxonomy.},
  archive      = {J_RAS},
  author       = {Ali Mehrkish and Farrokh Janabi-Sharifi},
  doi          = {10.1016/j.robot.2021.103860},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103860},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A comprehensive grasp taxonomy of continuum robots},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Marine locomotion: A tethered UAV-buoy system with surge
velocity control. <em>RAS</em>, <em>145</em>, 103858. (<a
href="https://doi.org/10.1016/j.robot.2021.103858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) are reaching offshore. In this work, we formulate the novel problem of a marine locomotive quadrotor UAV, which manipulates the surge velocity of a floating buoy by means of a cable. The proposed robotic system can have a variety of novel applications for UAVs where their high speed and maneuverability , as well as their ease of deployment and wide field of vision, give them a superior advantage. In addition, the major limitation of limited flight time of quadrotor UAVs is typically addressed through an umbilical power cable, which naturally integrates with the proposed system. A detailed high-fidelity dynamic model is presented for the buoy, UAV, and water environment. In addition, a stable control system design is proposed to manipulate the surge velocity of the buoy within certain constraints that keep the buoy in contact with the water surface. Polar coordinates are used in the controller design process since they outperform traditional Cartesian-based velocity controllers when it comes to ensuring correlated effects on the tracking performance, where each control channel independently affects one control parameter. The system model and controller design are validated in numerical simulation under different settings, configurations, and wave scenarios.},
  archive      = {J_RAS},
  author       = {Ahmad Kourani and Naseem Daher},
  doi          = {10.1016/j.robot.2021.103858},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103858},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Marine locomotion: A tethered UAV-buoy system with surge velocity control},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Where is my hand? Deep hand segmentation for visual
self-recognition in humanoid robots. <em>RAS</em>, <em>145</em>, 103857.
(<a href="https://doi.org/10.1016/j.robot.2021.103857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to distinguish between the self and the background is of paramount importance for robotic tasks. The particular case of hands, as the end effectors of a robotic system that more often enter into contact with other elements of the environment, must be perceived and tracked with precision to execute the intended tasks with dexterity and without colliding with obstacles. They are fundamental for several applications, from Human–Robot Interaction tasks to object manipulation. Modern humanoid robots are characterized by high number of degrees of freedom which makes their forward kinematics models very sensitive to uncertainty. Thus, resorting to vision sensing can be the only solution to endow these robots with a good perception of the self, being able to localize their body parts with precision. In this paper, we propose the use of a Convolution Neural Network (CNN) to segment the robot hand from an image in an egocentric view . It is known that CNNs require a huge amount of data to be trained. To overcome the challenge of labeling real-world images, we propose the use of simulated datasets exploiting domain randomization techniques. We fine-tuned the Mask-RCNN network for the specific task of segmenting the hand of the humanoid robot Vizzy. We focus our attention on developing a methodology that requires low amounts of data to achieve reasonable performance while giving detailed insight on how to properly generate variability in the training dataset. Moreover, we analyze the fine-tuning process within the complex model of Mask-RCNN, understanding which weights should be transferred to the new task of segmenting robot hands. Our final model was trained solely on synthetic images and achieves an average IoU of 82\% on synthetic validation data and 56.3\% on real test data. These results were achieved with only 1000 training images and 3 h of training time using a single GPU .},
  archive      = {J_RAS},
  author       = {Alexandre Almeida and Pedro Vicente and Alexandre Bernardino},
  doi          = {10.1016/j.robot.2021.103857},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103857},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Where is my hand? deep hand segmentation for visual self-recognition in humanoid robots},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained visual predictive control of tendon-driven
continuum robots. <em>RAS</em>, <em>145</em>, 103856. (<a
href="https://doi.org/10.1016/j.robot.2021.103856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their compliance, continuum robots (CRs) hold great potential for many applications. However, despite intensive recent research, their control poses significant challenges. The nonlinear kinematic behavior, limited actuation channels, and physical and environmental constraints typically associated with CRs hinder the development of effective control strategies. In this paper, a visual predictive position control method for tendon-driven continuum robots is proposed. The developed control approach integrates the advantages of image-based visual servoing and model predictive control techniques to enable direct end-point control in the presence of constraints and improve the control robustness to system uncertainties, sensing noise, and modeling errors. Both simulation and experimental results demonstrate the effectiveness of the method.},
  archive      = {J_RAS},
  author       = {Somayeh Norouzi-Ghazbi and Ali Mehrkish and Mostafa M.H. Fallah and Farrokh Janabi-Sharifi},
  doi          = {10.1016/j.robot.2021.103856},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103856},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Constrained visual predictive control of tendon-driven continuum robots},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel search and survey technique for unmanned aerial
systems in detecting and estimating the area for wildfires.
<em>RAS</em>, <em>145</em>, 103848. (<a
href="https://doi.org/10.1016/j.robot.2021.103848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years Unmanned Aerial Vehicles (UAVs) have progressively been utilized for wildfire management, and are especially in prevalent in forest fire monitoring missions. To ensure the fast detection and accurate area estimation of forest fires, a two-step search and survey algorithm for multi-UAV system is proposed to address these fire scenarios. Initially, a grid-based partition method is applied to divide the area-of-interest into several search areas. Then, an archetype search pattern is used to provide timely UAV exploration within those sub-areas. Once the fire zones are detected, a novel survey strategy is employed for UAVs to discover the boundary points of the fire zones, so that the area of the fire zones can be estimated using the sampled boundary points. In addition, the effect of wind is accounted for improving fire zone boundary estimates. The proposed search-and-survey procedure is validated on multiple simulated scenarios using the U.S. Air Force’s mission-realistic Aerospace Multi-Agent Simulation Environment (AMASE) software. Simulation results showcase that the proposed search pattern can effectively discover the seeded fire zones within 40 min of the mission. This is relatively faster than the other two well-known search patterns. Moreover, the proposed survey technique provides a coverage estimate with at least 85\% accuracy for the area of interest within 90 min of the mission.},
  archive      = {J_RAS},
  author       = {Mrinmoy Sarkar and Xuyang Yan and Berat A. Erol and Ioannis Raptis and Abdollah Homaifar},
  doi          = {10.1016/j.robot.2021.103848},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103848},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A novel search and survey technique for unmanned aerial systems in detecting and estimating the area for wildfires},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncalibrated stereo vision with deep learning for 6-DOF pose
estimation for a robot arm system. <em>RAS</em>, <em>145</em>, 103847.
(<a href="https://doi.org/10.1016/j.robot.2021.103847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel method for six degrees of freedom pose estimation of objects for the application of robot arm pick and place. It is based on the use of a stereo vision system, which does not require calibration. Using both cameras, four corner points of the object are detected. A deep-neural-network (DNN) is trained for the prediction of the 6 DOF pose of the object from the four detected corner points’ coordinates in each image of both cameras. The stereo vision used is a low-end vision system placed in a custom-made setup. Before the training phase of the DNN, the robot is set to auto collect data in a predefined workspace. This workspace is defined dependently on the spatial feasibility of the robot arm and the shared field of view of the stereo vision system. The collected data represent images of a 2D marker attached to the robot arm gripper. The 2D marker is used for data collection to ease the detection of the four corner points. The proposed method succeeds in estimating the six degrees of freedom pose of the object, without the need for the determination of neither the intrinsic nor the extrinsic parameters of the stereo vision system. The optimum design of the proposed DNN is obtained after comparing different activation functions and optimizers associated with the DNN. The proposed uncalibrated DNN-based method performance is compared to that of the traditional calibration-based method. In the calibration-based method, the rotational matrix relating the robot coordinates to the stereo vision coordinates is computed using two approaches. The first approach uses Singular Value Decomposition (SVD) while the second approach uses a novel proposed modification of particle swarm optimization (PSO) called Hyper particle Scouts optimization (HPSO). HPSO outperforms other metaheuristic optimization algorithms such as PSO and genetic algorithm (GA). Exhaustive tests are performed, and the proposed DNN-based method is shown to outperform all tested alternatives.},
  archive      = {J_RAS},
  author       = {Mahmoud Abdelaal and Ramy M.A. Farag and Mohamed S. Saad and Ahmed Bahgat and Hassan M. Emara and Ayman El-Dessouki},
  doi          = {10.1016/j.robot.2021.103847},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103847},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Uncalibrated stereo vision with deep learning for 6-DOF pose estimation for a robot arm system},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing user-centric programming aids for kinesthetic
teaching of collaborative robots. <em>RAS</em>, <em>145</em>, 103845.
(<a href="https://doi.org/10.1016/j.robot.2021.103845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just as end-user programming has helped make computer programming accessible for a variety of users and settings, end-user robot programming has helped empower end-users without specialized knowledge or technical skills to customize robotic assistance that meets diverse environmental constraints and task requirements. While end-user robot programming methods such as kinesthetic teaching have introduced direct approaches to task demonstration that allow users to avoid working with traditional programming constructs, our formative study revealed that everyday people still have difficulties in specifying effective robot programs using these methods due to challenges in understanding robot kinematics and programming without situated context and assistive system feedback. These findings informed our development of Demoshop , an interactive robot programming tool that includes user-centric programming aids to help end-users author and edit task demonstrations. To evaluate the effectiveness of Demoshop, we conducted a user study comparing task performance and user experience associated with using Demoshop relative to a widely used commercial baseline interface. Results of our study indicate that users have greater task efficiency while authoring robot programs and maintain stronger mental models of the system when using Demoshop compared to the baseline interface. Our system implementation and study have implications for the further development of assistance in end-user robot programming.},
  archive      = {J_RAS},
  author       = {Gopika Ajaykumar and Maia Stiber and Chien-Ming Huang},
  doi          = {10.1016/j.robot.2021.103845},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103845},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Designing user-centric programming aids for kinesthetic teaching of collaborative robots},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards autonomous ergonomic upper-limb exoskeletons: A
computational approach for planning a human-like path. <em>RAS</em>,
<em>145</em>, 103843. (<a
href="https://doi.org/10.1016/j.robot.2021.103843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational path planning approaches can enable development of autonomous rehabilitation and assistive exoskeletons. Using a human-like reference behavior for such wearable systems can ensure safe, effective, and intuitive human–robot interaction. This is of significant importance since the quality of interaction and ergonomic considerations have a substantial effect on technology usability and acceptance by the users. This paper proposes a novel framework for generating human-like paths for wearable exoskeletons in the shoulder-elbow level. The introduced method is a two-stage process where a human-like reference path is planned in the configuration space of the human arm, followed by an analytical transformation that directly maps the derived path to the configuration space of the exoskeleton. The analytical mapping presented is a function of the kinematic parameters of the system and can be adapted for other upper-limb exoskeletons. As a case study, the proposed method is used for generating human-like reference motions for a six-degree-of-freedom exoskeleton supporting scapulohumeral rhythm, glenohumeral rotations, and elbow flexion/extension. Firstly, it is shown that reaching motions associated with activities of daily living can be predicted with high accuracy in the human joint space. This is demonstrated by analyzing the experimental data collected from healthy subjects. Subsequently, it is verified through kinematic analysis that the transformation of generated paths to the exoskeleton configuration space does not alter their spatial profile in the task space.},
  archive      = {J_RAS},
  author       = {Rana Soltani Zarrin and Amin Zeiaee and Reza Langari and John J. Buchanan and Nina Robson},
  doi          = {10.1016/j.robot.2021.103843},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103843},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Towards autonomous ergonomic upper-limb exoskeletons: A computational approach for planning a human-like path},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on human-aware robot navigation. <em>RAS</em>,
<em>145</em>, 103837. (<a
href="https://doi.org/10.1016/j.robot.2021.103837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent systems are increasingly part of our everyday lives and have been integrated seamlessly to the point where it is difficult to imagine a world without them. Physical manifestations of those systems on the other hand, in the form of embodied agents or robots, have so far been used only for specific applications and are often limited to functional roles (e.g. in the industry, entertainment and military fields). Given the current growth and innovation in the research communities concerned with the topics of robot navigation , human–robot-interaction and human activity recognition, it seems like this might soon change. Robots are increasingly easy to obtain and use and the acceptance of them in general is growing. However, the design of a socially compliant robot that can function as a companion needs to take various areas of research into account. This paper is concerned with the navigation aspect of a socially-compliant robot and provides a survey of existing solutions for the relevant areas of research as well as an outlook on possible future directions.},
  archive      = {J_RAS},
  author       = {Ronja Möller and Antonino Furnari and Sebastiano Battiato and Aki Härmä and Giovanni Maria Farinella},
  doi          = {10.1016/j.robot.2021.103837},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103837},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A survey on human-aware robot navigation},
  volume       = {145},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a systematic computational framework for modeling
multi-agent decision-making at micro level for smart vehicles in a smart
world. <em>RAS</em>, <em>144</em>, 103859. (<a
href="https://doi.org/10.1016/j.robot.2021.103859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multi-agent based computational framework for modeling decision-making and strategic interaction at micro level for smart vehicles in a smart world. The concepts of Markov game and best response dynamics are heavily leveraged. Our aim is to make the framework conceptually sound and computationally practical for a range of realistic applications, including micro path planning for autonomous vehicles. To this end, we first convert the would-be stochastic game problem into a closely related deterministic one by introducing risk premium in the utility function for each individual agent. We show how the sub-game perfect Nash equilibrium of the simplified deterministic game can be solved by an algorithm based on best response dynamics. In order to better model human driving behaviors with bounded rationality, we seek to further simplify the solution concept by replacing the Nash equilibrium condition with a heuristic and adaptive optimization with finite look-ahead anticipation. In addition, the algorithm corresponding to the new solution concept drastically improves the computational efficiency. To demonstrate how our approach can be applied to realistic traffic settings, we conduct a simulation experiment: to derive merging and yielding behaviors on a double-lane highway with an unexpected barrier. Despite assumption differences involved in the two solution concepts, the derived numerical solutions show that the endogenized driving behaviors are very similar. We also briefly comment on how the proposed framework can be further extended in a number of directions in our forthcoming work, such as behavioral calibration using real traffic video data, and computational mechanism design for traffic policy optimization .},
  archive      = {J_RAS},
  author       = {Qi Dai and Xunnong Xu and Wen Guo and Suzhou Huang and Dimitar Filev},
  doi          = {10.1016/j.robot.2021.103859},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103859},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Towards a systematic computational framework for modeling multi-agent decision-making at micro level for smart vehicles in a smart world},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A modular functional framework for the design and evaluation
of multi-robot navigation. <em>RAS</em>, <em>144</em>, 103849. (<a
href="https://doi.org/10.1016/j.robot.2021.103849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the design of tightly integrated control, estimation, and allocation algorithms allowing a group of robots to move collectively. For doing so, we leverage a modular framework that allows us to define precisely the needed functional components and thus consider and compare multiple algorithmic solutions for the same module. We demonstrate the effectiveness of such a framework through multiple spatial coordination challenges carried out both in simulation and reality and leveraging different distributed control laws (graph-based and behavior-based controllers). Moreover, we investigate the impact of different localization and communication constraints as well as that of real-time switching of control laws on selected coordination metrics. Finally, we also introduce additional algorithmic components for demonstrating further the modularity of the framework. We find that defining the modularity based on functionality is a very effective way to enable algorithm benchmarking and discover possible improvements of the overall software stack while at the same time being agnostic to the underlying hardware and middleware resources. This is an especially welcome feature in case of severely resource-constrained multi-robot systems. Moreover, an important benefit of such design process is that the resulting distributed control algorithms are very robust to the considered noise sources and amplitudes as well as to the diverse types of challenges considered.},
  archive      = {J_RAS},
  author       = {Cyrill Baumann and Alcherio Martinoli},
  doi          = {10.1016/j.robot.2021.103849},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103849},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A modular functional framework for the design and evaluation of multi-robot navigation},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on soft lower limb cable-driven wearable robots
without rigid links and joints. <em>RAS</em>, <em>144</em>, 103846. (<a
href="https://doi.org/10.1016/j.robot.2021.103846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional wearable robots mostly consist of rigid structures connected to the human body. Due to the inherent characteristics of human joints , it is unavoidable that the robot joints have some misalignment with the wearer’s biological joints. One solution to the joint misalignment problem in wearable robots is to use soft structures instead of traditional rigid structures as the interface between the robot and the wearer. This survey paper aims to provide an overview of the designs of wearable robots for the lower limbs that do not contain any rigid structures or joints. This study is mainly focused on robots with an electrical cable-driven actuator. The lower limb joint-less robots introduced in this paper were categorized into three main groups, namely exoskeleton-based robots, end-effector-based robots, and​ exosuits. Application of these devices can be categorized as rehabilitation of patients with gait impairments and power augmentation of healthy users. After a detailed review of the notable designs in each group, a discussion about the advantages and disadvantages indicated that the main drawback of current designs is the limitation on the amount of provided assistive loads. Because the forces are mainly applied in parallel to the human body, the amount of these forces is limited by the wearer’s comfort level. In the end, a possible research direction for future researchers is presented in an attempt to address the limitations of current designs.},
  archive      = {J_RAS},
  author       = {Asghar Mahmoudi Khomami and Farid Najafi},
  doi          = {10.1016/j.robot.2021.103846},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103846},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A survey on soft lower limb cable-driven wearable robots without rigid links and joints},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overcoming some drawbacks of dynamic movement primitives.
<em>RAS</em>, <em>144</em>, 103844. (<a
href="https://doi.org/10.1016/j.robot.2021.103844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Movement Primitives (DMPs) is a framework for learning a point-to-point trajectory from a demonstration. Despite being widely used, DMPs still present some shortcomings that may limit their usage in real robotic applications . Firstly, at the state of the art, mainly Gaussian basis functions have been used to perform function approximation. Secondly, the adaptation of the trajectory generated by the DMP heavily depends on the choice of hyperparameters and the new desired goal position. Lastly, DMPs are a framework for ‘one-shot learning’, meaning that they are constrained to learn from a unique demonstration. In this work, we present and motivate a new set of basis functions to be used in the learning process, showing their ability to accurately approximate functions while having both analytical and numerical advantages w.r.t. Gaussian basis functions. Then, we show how to use the invariance of DMPs w.r.t. affine transformations to make the generalization of the trajectory robust against both the choice of hyperparameters and new goal position, performing both synthetic tests and experiments with real robots to show this increased robustness. Finally, we propose an algorithm to extract a common behavior from multiple observations, validating it both on a synthetic dataset and on a dataset obtained by performing a task on a real robot.},
  archive      = {J_RAS},
  author       = {Michele Ginesi and Nicola Sansonetto and Paolo Fiorini},
  doi          = {10.1016/j.robot.2021.103844},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103844},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Overcoming some drawbacks of dynamic movement primitives},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework for dynamic modeling of legged modular miniature
robots with soft backbones. <em>RAS</em>, <em>144</em>, 103841. (<a
href="https://doi.org/10.1016/j.robot.2021.103841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the dynamics of ”n” legged modular miniature robots with a soft body is modeled. The dynamic formulation is obtained using Newton–Euler formulation that depends on the contact parameters and the feet closed-chain kinematic analysis . The dynamic model determines the locomotion parameters of each module as an individual system as well as the dynamics of the whole robot in a 3D space; i.e., the robot is modeled as one system, and modules are considered to be sets of flexible links connected within this system. Kinematic constraints among these modules are obtained by considering the type of backbone integrated into the modular robot. Various types of backbones are used that are classified into three groups: rigid, only torsional, and soft. The model is verified using SMoLBot, an origami-inspired miniature robot made of multiple modules and soft/rigid backbones. Additional to the dynamic model, the effect of different sets of design parameters on the locomotion of the legged soft-bodied modular miniature robots is studied. Analyses comparing the velocity of SMoLBot with a different number of modules and various types of backbones are presented using the proposed dynamic model. Our results show the existence of an optimum backbone torsional stiffness for legged miniature modular robots and an optimum number of legs for a given backbone stiffness that maximizes the robot’s velocity. In this research, presented results and locomotion study show that the robot’s design should be iteratively improved based on specific optimum goals for exclusively defined task to satisfy the operational needs.},
  archive      = {J_RAS},
  author       = {Nima Mahkam and Onur Özcan},
  doi          = {10.1016/j.robot.2021.103841},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103841},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A framework for dynamic modeling of legged modular miniature robots with soft backbones},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary tabu inverted ant cellular automata with
elitist inertia for swarm robotics as surrogate method in surveillance
task using e-puck architecture. <em>RAS</em>, <em>144</em>, 103840. (<a
href="https://doi.org/10.1016/j.robot.2021.103840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area of swarm robotics has grown widely in recent years, precisely because its formulation is based on the use of various techniques, ranging from computer networks to controllers. We can employ different types of techniques to carry out the control of a robots team. In this work we will focus on creating techniques based on bio-inspired computing. Within this theme, we will be focused on using cellular automata with synchronous and asynchronous rules and ant colonies optimization . Additionally, we will consider greedy approaches to select the next robot’s state cell and a local Tabu search with a queue of robot movement restrictions. Thus, we have a surrogate model capable of providing the team robot navigation in the surveillance task. We developed two different controllers, a simpler first, based on a precursor model and a second optimized model, based on the previous controller refinement. At the end, we used a genetic algorithm , which received the surrogate model as input for the improvement of our proposed models parameters. In addition, a survey with the evolution of surveillance models using cellular automata in a systematic review of literature will be shown. Experiments were performed to demonstrate the degree of robot team coverage by different environments. We accomplished statistical analysis with the intention of presenting different sizes of robot teams and amounts of pheromone deposited into the environment. In the end, we fulfilled experiments using the empirical simulation methodology of a robots team using the Webots simulator with e-Puck architecture. The results were promising, the robot team performed this task efficiently and the system is highly scalable.},
  archive      = {J_RAS},
  author       = {Hamilton J.M. Lopes and Danielli A. Lima},
  doi          = {10.1016/j.robot.2021.103840},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103840},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Evolutionary tabu inverted ant cellular automata with elitist inertia for swarm robotics as surrogate method in surveillance task using e-puck architecture},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An arrovian analysis on the multi-robot task allocation
problem: Analyzing a behavior-based architecture. <em>RAS</em>,
<em>144</em>, 103839. (<a
href="https://doi.org/10.1016/j.robot.2021.103839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in multi-robot systems is a rich field that has attracted much attention in recent decades. However, robot coordination and task allocation to a correct mission accomplishment are still challenging even with technological advances. Despite many proposals presented in the literature, the applications and theories about the task allocation problem are not yet exhausted. Thus, this work proposes an axiomatic framework based on Social Choice Theory to analyze the task allocation problem in intentional cooperation multi-robot systems. It uses Kenneth J. Arrow’s framework of his famous Impossibility Theorem. The conditions imposed by Arrow aim to create an ideal for preference aggregation mechanisms through axiomatic analysis. This paper aims to transport this analysis to the multi-robot domain. A behavior-based Multi-robot Task Allocation architecture is used to present simulation results and discuss two cases in the ordinal preference domain. The analysis results show that using the proposed framework to analyze, under the Arrovian perspective, implemented MRTA architectures is feasible.},
  archive      = {J_RAS},
  author       = {Wallace Pereira Neves dos Reis and Gustavo Leite Lopes and Guilherme Sousa Bastos},
  doi          = {10.1016/j.robot.2021.103839},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103839},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An arrovian analysis on the multi-robot task allocation problem: Analyzing a behavior-based architecture},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot passes the mirror test by inner speech. <em>RAS</em>,
<em>144</em>, 103838. (<a
href="https://doi.org/10.1016/j.robot.2021.103838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mirror test is a well-known task in Robotics. The existing strategies are based on kinesthetic-visual matching techniques and manipulate perceptual and motion data. The proposed work attempts to demonstrate that it is possible to implement a robust robotic self-recognition method by the inner speech, i.e. the self-dialogue that enables reasoning on symbolic information. The robot self-talks and conceptually reasons on the symbolic forms of signals, and infers if the robot it sees in the mirror is itself or not. The idea is supported by the existing literature in psychology, where the importance of inner speech in self-reflection and self-concept emergence for solving the mirror test was empirically demonstrated.},
  archive      = {J_RAS},
  author       = {Arianna Pipitone and Antonio Chella},
  doi          = {10.1016/j.robot.2021.103838},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103838},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robot passes the mirror test by inner speech},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite-time disturbance reconstruction and robust
fractional-order controller design for hybrid port-hamiltonian dynamics
of biped robots. <em>RAS</em>, <em>144</em>, 103836. (<a
href="https://doi.org/10.1016/j.robot.2021.103836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, disturbance reconstruction and robust trajectory tracking control of biped robots with hybrid dynamics in the port-Hamiltonian form is investigated. A new type of Hamiltonian function is introduced, which ensures the finite-time stability of the closed-loop system. The proposed control system consists of two loops: an inner and an outer loop. A fractional proportional–integral–derivative filter is used to achieve finite-time convergence for position tracking errors at the outer loop. A fractional-order sliding mode controller acts as a centralized controller at the inner-loop, ensuring the finite-time stability of the velocity tracking error. In this loop, the undesired effects of unknown external disturbance and parameter uncertainties are compensated using estimators. Two disturbance estimators are envisioned. The former is designed using fractional calculus. The latter is an adaptive estimator, and it is constructed using the general dynamic of biped robots. Stability analysis shows that the closed-loop system is finite-time stable in both contact-less and impact phases. Simulation studies on three types of biped robots (i.e., two-link walker, RABBIT biped robot, and flat-feet biped robot) demonstrate the proposed controller’s tracking performance and disturbance rejection capability.},
  archive      = {J_RAS},
  author       = {Yousef Farid and Fabio Ruggiero},
  doi          = {10.1016/j.robot.2021.103836},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103836},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Finite-time disturbance reconstruction and robust fractional-order controller design for hybrid port-hamiltonian dynamics of biped robots},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised skill transfer learning for autonomous robots
using distributed growing self organizing maps. <em>RAS</em>,
<em>144</em>, 103835. (<a
href="https://doi.org/10.1016/j.robot.2021.103835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A persistent challenge in the cognitive development of autonomous robotics is the unsupervised and unstructured nature of skill transfer learning where the Self Organizing Map (SOM) has been used as the enabling technology. The Growing Self-Organizing Map (GSOM) algorithm is an unsupervised, structure-adapting machine learning algorithm conventionally used for data exploration, clustering, visualization, outlier detection and dimensionality reduction. In this paper, we present the design and development of a new distributed algorithm based on the GSOM for unsupervised skill transfer learning in autonomous robotics settings which overcomes the key limitations of the SOM in real-life scenarios. We posit this new algorithm will be directly applicable to skill transfer learning scenarios that require unsupervised, incremental and on-going self-learning of multi-tasks and knowledge transfer. The distributed and scalable properties of the proposed algorithm handle large volumes of data required for unsupervised skill transfer learning, based on data parallelization . It generates multiple maps representing diverse skill knowledge, which are then projected together to a single embedding. The new algorithm is positioned within an autonomous developmental robotics framework for knowledge acquisition and skill transfer learning. This framework was further adapted to three contemporary distributed computing platforms, Hadoop , Spark and Hama. Empirical evaluation of these three adaptations using several benchmark and real-life datasets demonstrates its practical value and computational efficiency for unsupervised skill transfer learning in autonomous robots.},
  archive      = {J_RAS},
  author       = {Madhura Jayaratne and Damminda Alahakoon and Daswin de Silva},
  doi          = {10.1016/j.robot.2021.103835},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103835},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Unsupervised skill transfer learning for autonomous robots using distributed growing self organizing maps},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A systematic mapping study of robotics in human care.
<em>RAS</em>, <em>144</em>, 103833. (<a
href="https://doi.org/10.1016/j.robot.2021.103833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The World Health Organization (WHO) reported that more than 1 billion people live with some form of disability. Moreover, the number of elderly is increasing in recent years. According to the United Nations (UN), in 2050, there will be 2.1 billion people above 60 years of age worldwide. Many of these people live alone in their homes or clinics and rely on some kind of help to fulfill their specific needs. In this context, emerging opportunities for the application of robotics to support ubiquitous healthcare may reflect in reducing medical costs and increasing the convenience of patients and people in general. This paper presents a systematic mapping study to identify the application of service robots in the assistance of human care, focusing on the employment of computational technologies and unexplored research gaps in the literature. The study conducted searches in eight scientific repositories in the area of service robots through a systematic filtering process to remove bias. Afterward, the filtering process allowed to reduce from an initial sample of 9372 to 69 studies. As a result, these studies were reviewed entirely, analyzed, and categorized to answer six research questions. In addition, the study proposed four taxonomies illustrating the state-of-the-art of robotics in human care. The results highlight therapy and entertainment as the most common categories of the usage of robotics in human care. The most widely-used technologies to integrate with smart environments are smartphone sensors, smart device integration, wearables, and cloud services. The most frequently used mean of human–robot interaction is verbal communication, which is useful to help the elderly, children, and people with a mental health disorder. The most commonly cited diseases were cognition impairment, autism spectrum disorder, and motor impairment . Finally, we observed a trend in the growth of the use of service robots to improve the intelligence of the environment supporting human care. The scientific contribution of this article are four taxonomies that classify and group caregiver robots according to the application, integration with a smart environment, human–robot interaction, and target audience. This study also allowed the learning of 11 lessons on methodological and technological aspects based on the profound research performed.},
  archive      = {J_RAS},
  author       = {Nícolas B. Santos and Rodrigo S. Bavaresco and João E.R. Tavares and Gabriel de O. Ramos and Jorge L.V. Barbosa},
  doi          = {10.1016/j.robot.2021.103833},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103833},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A systematic mapping study of robotics in human care},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Special issue on autonomous driving and driver
assistance systems — some main trends. <em>RAS</em>, <em>144</em>,
103832. (<a href="https://doi.org/10.1016/j.robot.2021.103832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_RAS},
  author       = {Vitor Santos and Angel D. Sappa and Miguel Oliveira and Arturo de la Escalera},
  doi          = {10.1016/j.robot.2021.103832},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103832},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Editorial: Special issue on autonomous driving and driver assistance systems — some main trends},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive framework of real-time continuous gait phase
variable estimation for lower-limb wearable robots. <em>RAS</em>,
<em>143</em>, 103842. (<a
href="https://doi.org/10.1016/j.robot.2021.103842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase-variable-based approaches are emerging in the control of lower-limb wearable robots , such as exoskeletons and prosthetic legs. However, real-time smooth estimation of the gait phase within each gait cycle remains an open problem. This paper presents a novel method for real-time continuous gait phase estimation during walking. The proposed framework consists of three subsystems: real-time kinematic data collection, gait phase variable estimation, and online adaptation of individual kinematics through backward data segmentation of completed gait strides. It is worth noting that we introduce an online learning mechanism for extracting and learning gait features from previous strides, in contrast with offline parameter tuning. The proposed basic gait model is initialized by human average data and is incrementally refined as a function of the individual gait features over different walking speeds. This provides a framework for long-term personalized control. Furthermore, the phase variable is constructed through the thigh angle measured by an inertial measurement unit . The resulting simple sensor system improves the usability of the proposed technique in wearable robotics. Validation experiments with seven healthy subjects, including treadmill walking and free level-ground walking, were conducted to evaluate the performance of the proposed method. In treadmill validation, the root-mean-square error (RMSE) of the phase estimator was 4.14 ± ± 1.68\% for steady speeds, while it was 6.77 ± ± 2.29\% for unsteady-speed walking. In level-ground validation, the average RMSE of the phase estimator was 4.59 ± ± 1.76\%. Preliminary experiments were also conducted using a single-joint hip exoskeleton to demonstrate the usability of our method in lower-limb wearable robots .},
  archive      = {J_RAS},
  author       = {Binquan Zhang and Sun’an Wang and Min Zhou and Wanlu Xu},
  doi          = {10.1016/j.robot.2021.103842},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103842},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An adaptive framework of real-time continuous gait phase variable estimation for lower-limb wearable robots},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A literature review of sensor heads for humanoid robots.
<em>RAS</em>, <em>143</em>, 103834. (<a
href="https://doi.org/10.1016/j.robot.2021.103834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conducted a literature review on sensor heads for humanoid robots . A strong case is made on topics involved in human–robot interaction. Having found that vision is the most abundant perception system among sensor heads for humanoid robots, we included a review of control techniques for humanoid active vision. We provide historical insight and inform on current robotic head design and applications. Information is chronologically organized whenever possible and exposes trends in control techniques, mechanical design, periodical advances and overall philosophy. We found that there are two main types of humanoid robot heads which we propose to classify as either non-expressive face robot heads or expressive face robot heads. We expose their respective characteristics and provide some ideas on design and vision control considerations for humanoid robot heads involved in human–robot interactions.},
  archive      = {J_RAS},
  author       = {J.A. Rojas-Quintero and M.C. Rodríguez-Liñán},
  doi          = {10.1016/j.robot.2021.103834},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103834},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A literature review of sensor heads for humanoid robots},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Three-dimensional aperiodic biped walking including the
double support phase using LIPM and LPM. <em>RAS</em>, <em>143</em>,
103831. (<a href="https://doi.org/10.1016/j.robot.2021.103831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a trajectory generation algorithm with which a three-dimensional (3D) biped robot can perform aperiodic gaits by modifying only a small set of gait parameters. In addition to the double support phase (DSP), the gait can transit smoothly from one single support phase (SSP) to another. We decouple the sagittal and coronal dynamics, firstly. In the sagittal plane , the linear inverted pendulum model (LIPM) is used to generate the walking reference trajectory during the SSP. An extra template model, linear pendulum model (LPM), is added to describe the motion in the DSP. For the coronal plane, we only utilize the LPM for trajectory generation . Thanks to linearity properties, the proposed method can obtain the biped locomotion computationally fast without the need for numerical time-integration. Herein, we also introduce the trajectory generation algorithm for different aperiodic gaits including from standing to walking, stopping walking, as well as speed switch. A full-dynamics 3D humanoid robot is used for the tests of the developed reference trajectory through simulation. The results are promising for implementations.},
  archive      = {J_RAS},
  author       = {Zhongqu Xie and Long Li and Xiang Luo},
  doi          = {10.1016/j.robot.2021.103831},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103831},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Three-dimensional aperiodic biped walking including the double support phase using LIPM and LPM},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual recognition of gymnastic exercise sequences.
Application to supervision and robot learning by demonstration.
<em>RAS</em>, <em>143</em>, 103830. (<a
href="https://doi.org/10.1016/j.robot.2021.103830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a novel software architecture to autonomously identify and evaluate the gymnastic activity that people are carrying out. It is composed of three different interconnected layers. The first corresponds to a Multilayer Perceptron (MLP) trained from a set of angular magnitudes derived from the information provided by the OpenPose library. This library works frame by frame, so some postures may be incorrectly detected due to eventual occlusions. The MLP layer makes it possible to accurately identify the posture a person is performing. A second layer, based on a Hidden Markov Model (HMM) and the Viterbi algorithm , filters the incorrect spurious postures. Thus, the accuracy of the algorithm is improved, leading to a precise sequence of postures. A third layer identifies the current exercise and evaluates whether the person is doing it at a correct speed. This layer uses an innovative Modified Levenshtein Distance (MLD), which considers not only the number of operations to transform a given sequence, but also the nature of the elements participating in the comparison. The system works in real time with little delay, thus recognizing sequences of arbitrary length and providing continuous feedback on the exercises being performed. An experiment carried out consisted in reproducing the output of the second layer on an autonomous Pepper robot that can be used in environments where physical exercise is performed, such as a residence for the elderly or others. It has reproduced different exercises previously executed by an instructor so that people can copy the robot. The article analyzes the current situation of the automated gymnastic activities recognition, presents the architecture, the different experiments carried out and the results obtained. The integration of the three components (MLP, HMM and MLD) results in a robust system that has allowed us to improve the results of previous works.},
  archive      = {J_RAS},
  author       = {Jaime Duque Domingo and Jaime Gómez-García-Bermejo and Eduardo Zalama},
  doi          = {10.1016/j.robot.2021.103830},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103830},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Visual recognition of gymnastic exercise sequences. application to supervision and robot learning by demonstration},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring relationships between design features and system
usability of intelligent car human–machine interface. <em>RAS</em>,
<em>143</em>, 103829. (<a
href="https://doi.org/10.1016/j.robot.2021.103829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-vehicle human–machine interface (HMI) mainly refers to the T-shaped panel system with instruments, centre console, gear lever and other components installed. For intelligent vehicles , the high level of intelligent interconnection may to some extent make drivers lack situational safety awareness and reduce the usability of the system. Thus, this study attempted to establish a relationship between design features and system usability of the in-vehicle panels. From the perspective of visual ergonomics , the panels were deconstructed into design features to determine 36 samples to be studied. After dividing each sample into four areas of interest (AOI), eye movement and subjective preference data were collected to quantify the user experience . Artificial neural network (ANN) and support vector machine (SVM) were used in the study. Nevertheless, conventional learning algorithms often underwent deficiencies in accuracy and robustness in the detection of multifarious kinds of panels. Therefore, the parameters of the two models were tuned to deal with the noise common in user experience data. The determinant coefficients, mean-square errors and mean relative errors of the two models showed that the SVM model had a higher accuracy, smaller error and was more stable in the learning of user experience of HMI design features, which could provide a method for the layout design and evaluation of T-shaped instrument panel.},
  archive      = {J_RAS},
  author       = {Hao Yang and Jitao Zhang and Yueran Wang and Ruoyu Jia},
  doi          = {10.1016/j.robot.2021.103829},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103829},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Exploring relationships between design features and system usability of intelligent car human–machine interface},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and characterization of a lightweight underactuated
RACA hand exoskeleton for neurorehabilitation. <em>RAS</em>,
<em>143</em>, 103828. (<a
href="https://doi.org/10.1016/j.robot.2021.103828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of the use of robotic devices in neuro-rehabilitation therapies requires the availability of lightweight, easy-to-use, cost-effective and versatile systems. RobHand has been designed with these goals in mind. It is a hand exoskeleton especially suitable for patients suffering from spasticity in the fingers since it is easy to place in the hand and, from an underactuated design, allows both flexion and extension of the fingers. In this work, the structural characteristics, the mechanical design and the development and validation of the kinematic model of the device are presented, all of which has been carried out taking into account the recommendations of the new IEC 80601-2-78 standard, which formalizes the concept of RACA (Rehabilitation, Assessment, Compensation, Alleviation) robot and addresses aspects of efficiency and safety, essential in this type of equipment.},
  archive      = {J_RAS},
  author       = {Victor Moreno-SanJuan and Ana Cisnal and Juan-Carlos Fraile and Javier Pérez-Turiel and Eusebio de-la-Fuente},
  doi          = {10.1016/j.robot.2021.103828},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103828},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Design and characterization of a lightweight underactuated RACA hand exoskeleton for neurorehabilitation},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End control damping algorithm for a stabilized gun turret
system for the satisfaction of the collision avoidance requirement.
<em>RAS</em>, <em>143</em>, 103827. (<a
href="https://doi.org/10.1016/j.robot.2021.103827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a collision avoidance algorithm for stabilized gun turrets and its real-time implementation. With the help of new collision avoidance algorithm, all types of turrets can be driven more efficiently and safely according to the specified speed, acceleration and jerk limits. Even in situations such as avoiding obstacles, deceleration/acceleration, if the user issues new commands which does not cause a collision, the algorithm starts to apply the new commands providing flexibility to the user. Since all possible worst scenarios are examined one by one, it is guaranteed that the algorithm provides collision free motion in both simulations and real-time tests. A configuration space where worst scenarios can occur is created for the performance measurement of the algorithm, and the same space is used in all tests. By giving different speed commands in the specified configuration space , the performance of the algorithm at different speeds is observed on the stabilized gun turret. For the measurement of the performance under the noisy speed commands, a custom noisy speed command of about 1000 s is created and both simulation and real-time tests are performed. As a result of these tests, it is shown that there is no collision. Finally, by adding cascade position control loop, the departure from the starting point to the desired target point is achieved without any collision. The most important feature that distinguishes this algorithm from others is both speed and position can be controlled and during transition phase, the target point can be changed instantly. In addition, no target position is required for the system to move collision-free, only axis speed commands are sufficient. Since the algorithm does not intervene in the speed and torque loops in contrast to potential field-based methods, it can be added to ready-to-use systems by manipulating only the speed references.},
  archive      = {J_RAS},
  author       = {Ümit Yerlikaya and R. Tuna Balkan},
  doi          = {10.1016/j.robot.2021.103827},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103827},
  shortjournal = {Robot. Auton. Syst.},
  title        = {End control damping algorithm for a stabilized gun turret system for the satisfaction of the collision avoidance requirement},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A chaotic path planning generator enhanced by a memory
technique. <em>RAS</em>, <em>143</em>, 103826. (<a
href="https://doi.org/10.1016/j.robot.2021.103826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the problem of chaotic path planning , using an improved memory technique to boost performance. In this application, the dynamics of two simple chaotic maps are first used to generate a pseudo-random bit generator. Using this as a source, a series of navigation commands are generated and used by an autonomous robot to explore an area, while maintaining a random and unpredictable motion. This navigation strategy can bring overall area coverage, but also yields numerous revisits to previous cells. Here, a memory technique is applied to limit the chaotic motion of the robot to adjacent cells with the least number of visits, leading to overall improvement in performance. Numerical simulations are performed to evaluate the path planning strategy. The simulation results showcase a major improvement in coverage performance compared to the memory-free technique and also compared to an inverse pheromone technique previously developed by the authors. Also, the number of multiple visits to previous cells is significantly reduced with the proposed technique.},
  archive      = {J_RAS},
  author       = {Eleftherios Petavratzis and Lazaros Moysis and Christos Volos and Ioannis Stouboulos and Hector Nistazakis and Kimon Valavanis},
  doi          = {10.1016/j.robot.2021.103826},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103826},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A chaotic path planning generator enhanced by a memory technique},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal sampling annealing schemes for receding horizon
multi-agent planning. <em>RAS</em>, <em>143</em>, 103823. (<a
href="https://doi.org/10.1016/j.robot.2021.103823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with multi-agent scenarios where individual agents must coordinate their plans in order to efficiently complete a set of tasks. Our strategy formulates the task planning problem as a potential game and uses decentralized stochastic sampling policies to reach a consensus on which sequences of actions agents should take. We execute this over a receding finite time horizon and take special care to discourage agents from breaking promises in the near future, which may cause other agents to unsuccessfully attempt a joint action. At the same time, we allow agents to change plans in the distant future, as this gives time for other agents to adapt their plans, allowing the team to escape locally optimal solutions. To do this we introduce two sampling schemes for new actions: a geometric-based scheme, where the probability of sampling a new action increases geometrically in time, and an inference-based sampling scheme, where a convolutional neural network provides recommendations for joint actions. We test the proposed schemes in a cooperative orienteering environment to illustrate their performance and validate the intuition behind their design.},
  archive      = {J_RAS},
  author       = {Aaron Ma and Mike Ouimet and Jorge Cortés},
  doi          = {10.1016/j.robot.2021.103823},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103823},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Temporal sampling annealing schemes for receding horizon multi-agent planning},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An assistive upper-limb exoskeleton controlled by
multi-modal interfaces for severely impaired patients: Development and
experimental assessment. <em>RAS</em>, <em>143</em>, 103822. (<a
href="https://doi.org/10.1016/j.robot.2021.103822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active exoskeletons can help adults with muscular dystrophy regain independence and self-esteem, which have been limited due to their severe and progressive muscular weakness. A four degrees-of-freedom fully actuated upper limb exoskeleton, equipped with a spring-based anti-gravity system, has been designed, prototyped, and tested on end-users. While wearing the exoskeleton, the user directly controls the system by actively driving the end-effector position (i.e., the hand) using a joystick or vocal control. The exoskeleton’s kinematic model has been determined so that, given a desired user’s position in the task-space, a differential inverse kinematics algorithm computes the desired joint-space motion trajectories. The dynamic model was investigated in the vertical plane, demonstrating that gravity torques were considerably higher than velocity-induced and inertia torques, which have been therefore neglected. A pilot study on 14 Muscular Dystrophy patients was conducted. Outcome measures included: (i) externally-assessed functional benefit evaluated through the Performance of Upper Limbs module, (ii) self-perceived functional benefit assessed through the ABILHAND questionnaire, and (iii) usability of the system assessed through the System Usability Scale . All participants strongly increased their range of motion, and they were able to perform activities that were not possible without the exoskeleton, such as feeding. The externally-assessed and self-perceived functional improvements were statistically improved when wearing the exoskeleton (PUL p-value = 0 . 001 =0.001 , ABILHAND p-value = 0 . 005 =0.005 ). System usability was evaluated to be excellent. Patients’ feedbacks were encouraging and outlined future development steps.},
  archive      = {J_RAS},
  author       = {Marta Gandolla and Stefano Dalla Gasperina and Valeria Longatelli and Alessandro Manti and Lorenzo Aquilante and Maria Grazia D’Angelo and Emilia Biffi and Eleonora Diella and Franco Molteni and Mauro Rossini and Margit Gföhler and Markus Puchinger and Marco Bocciolone and Francesco Braghin and Alessandra Pedrocchi},
  doi          = {10.1016/j.robot.2021.103822},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103822},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An assistive upper-limb exoskeleton controlled by multi-modal interfaces for severely impaired patients: Development and experimental assessment},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptation in a variable parallel elastic actuator for
rotary mechanisms towards energy efficiency. <em>RAS</em>, <em>143</em>,
103815. (<a href="https://doi.org/10.1016/j.robot.2021.103815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the presentation of a parallel compliance adaptation method for systems equipped with rotary motion mechanisms towards obtaining energy efficiency in cyclic tasks over a reasonable range of task frequency variations. In this work, we first introduce a variable parallel elastic actuator (VPEA) design for implementation on uni-directional joints that can respond in line with the torque requirements caused by frequency variations in rotary mechanisms. Then, in the next step, we propose two design approaches namely “ general method ” and “ frequency-based method ” for the VPEA along with the stiffness adjustment approaches both in offline and online manners. The optimality and convergence of the adaptation method for the proposed rotary VPEA are also analytically proved in general to be globally exponentially stable in the sense of Lyapunov. Finally, to demonstrate the applicability and efficiency of our VPEA, we deployed it in a robotic leg model as the case study. The simulation results demonstrate the stability and convergence of our adaptation rule and highlight the performance of the proposed VPEA in increasing energy efficiency over a wide range of task frequency variations.},
  archive      = {J_RAS},
  author       = {Omid Mohseni and Majid Abedinzadeh Shahri and Ayoob Davoodi and Majid Nili Ahmadabadi},
  doi          = {10.1016/j.robot.2021.103815},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103815},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Adaptation in a variable parallel elastic actuator for rotary mechanisms towards energy efficiency},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning whole-image descriptors for real-time loop
detection and kidnap recovery under large viewpoint difference.
<em>RAS</em>, <em>143</em>, 103813. (<a
href="https://doi.org/10.1016/j.robot.2021.103813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a real-time stereo visual-inertial-SLAM system which is able to recover from complicated kidnap scenarios and failures online in realtime. We propose to learn the whole-image-descriptor in a weakly supervised manner based on NetVLAD and decoupled convolutions. We analyze the training difficulties in using standard loss formulations and propose an allpairloss and show its effect through extensive experiments. Compared to standard NetVLAD, our network takes an order of magnitude fewer computations and model parameters, as a result runs about three times faster. We evaluate the representation power of our descriptor on standard datasets with precision–recall. Unlike previous loop detection methods which have been evaluated only on fronto-parallel revisits, we evaluate the performance of our method with competing methods on scenarios involving large viewpoint difference. Finally, we present the fully functional system with relative computation and handling of multiple world co-ordinate system which is able to reduce odometry drift, recover from complicated kidnap scenarios and random odometry failures. We open source our fully functional system as an add-on for the popular VINS-Fusion.},
  archive      = {J_RAS},
  author       = {Manohar Kuse and Shaojie Shen},
  doi          = {10.1016/j.robot.2021.103813},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103813},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Learning whole-image descriptors for real-time loop detection and kidnap recovery under large viewpoint difference},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmarking pose estimation for robot manipulation.
<em>RAS</em>, <em>143</em>, 103810. (<a
href="https://doi.org/10.1016/j.robot.2021.103810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot grasping and manipulation require estimation of 3D object poses. Recently, a number of methods and datasets for vision-based pose estimation have been proposed. However, it is unclear how well the performance measures developed for visual pose estimation predict success in robot manipulation. In this work, we introduce an approach that connects error in pose and success in robot manipulation, and propose a probabilistic performance measure of the task success rate . A physical setup is needed to estimate the probability densities from real world samples, but evaluation of pose estimation methods is offline using captured test images, ground truth poses and the estimated densities. We validate the approach with four industrial manipulation tasks and evaluate a number of publicly available pose estimation methods. The popular pose estimation performance measure, Average Distance of Corresponding model points (ADC), does not offer any quantitatively meaningful indication of the frequency of success in robot manipulation. Our measure is instead quantitatively informative: e.g., a score of 0.24 corresponds to average success probability of 24\%.},
  archive      = {J_RAS},
  author       = {Antti Hietanen and Jyrki Latokartano and Alessandro Foi and Roel Pieters and Ville Kyrki and Minna Lanz and Joni-Kristian Kämäräinen},
  doi          = {10.1016/j.robot.2021.103810},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103810},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Benchmarking pose estimation for robot manipulation},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Peg-in-hole assembly in live-line maintenance based on
generative mapping and searching network. <em>RAS</em>, <em>143</em>,
103797. (<a href="https://doi.org/10.1016/j.robot.2021.103797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replacement of lightning arrester is one of the common tasks in live-line maintenance, and peg-in-hole assembly is a very difficult operation for a robot, because there are visual inaccuracy and force model uncertainty in the process of assembly. This paper presents a new implementation approach fusing signals of vision detection and fuzzy force to realize the high efficiency peg-in-hole assembly by a manipulator autonomously. YOLOv3 is applied as the visual detection network for rough alignment. In the phase of precise hole-searching, we establish a two-dimensional hole-searching model by fusing signal of vision detection and fuzzy force as the condition of state transitions, and propose a new semi-supervised learning network to optimize the hole-searching routine. The performance of the approach is verified by experiments in the simulation environment and the laboratory environment.},
  archive      = {J_RAS},
  author       = {Wei Wu and Hui Zhou and Yu Guo and Yifei Wu and Jian Guo},
  doi          = {10.1016/j.robot.2021.103797},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103797},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Peg-in-hole assembly in live-line maintenance based on generative mapping and searching network},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor-variate mixture of experts for proportional
myographic control of a robotic hand. <em>RAS</em>, <em>142</em>,
103812. (<a href="https://doi.org/10.1016/j.robot.2021.103812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When data are organized in matrices or arrays of higher dimensions (tensors), classical regression methods first transform these data into vectors, therefore ignoring the underlying structure of the data and increasing the dimensionality of the problem. This flattening operation typically leads to overfitting when only few training data is available. In this paper, we present a mixture-of-experts model that exploits tensorial representations for regression of tensor-valued data. The proposed formulation takes into account the underlying structure of the data and remains efficient when few training data are available. Evaluation on artificially generated data, as well as offline and real-time experiments recognizing hand movements from tactile myography prove the effectiveness of the proposed approach.},
  archive      = {J_RAS},
  author       = {Noémie Jaquier and Robert Haschke and Sylvain Calinon},
  doi          = {10.1016/j.robot.2021.103812},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103812},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Tensor-variate mixture of experts for proportional myographic control of a robotic hand},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-based learning of underwater acoustic communication
performance for marine robots. <em>RAS</em>, <em>142</em>, 103811. (<a
href="https://doi.org/10.1016/j.robot.2021.103811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of acoustic communication performance is an important capability for marine robots. In this paper, we propose a model-based learning methodology for the prediction of underwater acoustic communication performance. The learning algorithm consists of two steps: (i) estimation of the covariance matrix by evaluating candidate functions with estimated parameters using detrended measurements;and (ii) prediction of communication performance. Covariance estimation is addressed with a multi-stage iterative training method that produces unbiased and robust results with nested models. The efficiency of the framework is validated with simulations and experimental data from field trials. The field trials involved a manned surface vehicle and an autonomous underwater vehicle.},
  archive      = {J_RAS},
  author       = {George P. Kontoudis and Stephen Krauss and Daniel J. Stilwell},
  doi          = {10.1016/j.robot.2021.103811},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103811},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Model-based learning of underwater acoustic communication performance for marine robots},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlation filter of 2D laser scans for indoor environment.
<em>RAS</em>, <em>142</em>, 103809. (<a
href="https://doi.org/10.1016/j.robot.2021.103809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern laser SLAM (simultaneous localization and mapping) and structure from motion algorithms face the problem of processing redundant data. Even if a sensor does not move, it still continues to capture scans that should be processed. This paper presents the novel filter that allows dropping 2D scans that bring no new information to the system. Experiments on MIT and TUM datasets show that it is possible to drop more than half of the scans. Moreover the paper describes the formulas that enable filter adaptation to a particular robot with known speed and characteristics of lidar . In addition, the indoor corridor detector is introduced that also can be applied to any specific shape of a corridor and sensor.},
  archive      = {J_RAS},
  author       = {Kirill Krinkin and Anton Filatov},
  doi          = {10.1016/j.robot.2021.103809},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103809},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Correlation filter of 2D laser scans for indoor environment},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed safe planning for satisfying minimal temporal
relaxations of TWTL specifications. <em>RAS</em>, <em>142</em>, 103801.
(<a href="https://doi.org/10.1016/j.robot.2021.103801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a multi-agent planning problem, where each agent aims to achieve an individual task while avoiding collisions with other agents. Each agent’s task is expressed as a Time-Window Temporal Logic (TWTL) specification defined over a discretized environment. We propose a distributed receding horizon algorithm for online planning of agent trajectories. We show that under mild assumptions on the environment, the resulting trajectories are always safe (collision-free) and lead to the satisfaction of the TWTL specifications or a finite temporal relaxation. Accordingly, each agent is guaranteed to safely achieve its task, possibly with some minimal finite delay. Performance of the proposed algorithm is demonstrated via numerical simulations and experiments with quadrotors.},
  archive      = {J_RAS},
  author       = {Ryan Peterson and Ali Tevfik Buyukkocak and Derya Aksaray and Yasin Yazıcıoğlu},
  doi          = {10.1016/j.robot.2021.103801},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103801},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Distributed safe planning for satisfying minimal temporal relaxations of TWTL specifications},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe path planning for UAV urban operation under GNSS signal
occlusion risk. <em>RAS</em>, <em>142</em>, 103800. (<a
href="https://doi.org/10.1016/j.robot.2021.103800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a concept of safe path planning for UAV’s autonomous operation in an urban environment where GNSS-positioning may become unreliable or even unavailable. If the operation environment is a priori known and geo-localized, it is possible to predict a GNSS satellite constellation and hence to anticipate its signal occlusions at a given point and time. Motivated from this, our main idea is to utilize such sensor availability map in path planning task for ensuring UAV navigation safety. The proposed concept is implemented by a Partially Observable Markov Decision Process (POMDP) model. It incorporates a low-level navigation and guidance module for propagating the UAV state uncertainty in function of the probabilistic sensor availability. A new definition of cost function is introduced in this model such that the resulting optimal policy respects a user-defined safety requirement. A goal-oriented version of Monte-Carlo Tree Search algorithm, called POMCP-GO, is proposed for POMDP solving. The developed safe path planner is evaluated on two simple obstacle benchmark maps as well as on a real elevation map of San Diego downtown, along with GPS availability maps.},
  archive      = {J_RAS},
  author       = {Jean-Alexis Delamer and Yoko Watanabe and Caroline P.C. Chanel},
  doi          = {10.1016/j.robot.2021.103800},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103800},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Safe path planning for UAV urban operation under GNSS signal occlusion risk},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing continuous control of mobile robots for end-to-end
visual active tracking. <em>RAS</em>, <em>142</em>, 103799. (<a
href="https://doi.org/10.1016/j.robot.2021.103799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, visual target tracking has been one of the primary research interests of the Robotics research community. The recent advances in Deep Learning technologies have made the exploitation of visual tracking approaches effective and possible in a wide variety of applications, ranging from automotive to surveillance and human assistance. However, the majority of the existing works focus exclusively on passive visual tracking, i.e. , tracking elements in sequences of images by assuming that no actions can be taken to adapt the camera position to the motion of the tracked entity. On the contrary, in this work, we address visual active tracking, in which the tracker has to actively search for and track a specified target. Current State-of-the-Art approaches use Deep Reinforcement Learning (DRL) techniques to address the problem in an end-to-end manner. However, two main problems arise: (i) most of the contributions focus only on discrete action spaces, and the ones that consider continuous control do not achieve the same level of performance; and (ii) if not properly tuned, DRL models can be challenging to train, resulting in considerably slow learning progress and poor final performance. To address these challenges, we propose a novel DRL-based visual active tracking system that provides continuous action policies. To accelerate training and improve the overall performance, we introduce additional objective functions and a Heuristic Trajectory Generator (HTG) to facilitate learning. Through extensive experimentation, we show that our method can reach and surpass other State-of-the-Art approaches performances, and demonstrate that, even if trained exclusively in simulation, it can successfully perform visual active tracking even in real scenarios.},
  archive      = {J_RAS},
  author       = {Alessandro Devo and Alberto Dionigi and Gabriele Costante},
  doi          = {10.1016/j.robot.2021.103799},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103799},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Enhancing continuous control of mobile robots for end-to-end visual active tracking},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision-based manipulation of deformable and rigid objects
using subspace projections of 2D contours. <em>RAS</em>, <em>142</em>,
103798. (<a href="https://doi.org/10.1016/j.robot.2021.103798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a unified vision-based manipulation framework using image contours of deformable/rigid objects. Instead of explicitly defining the features by geometries or functions, the robot automatically learns the visual features from processed vision data. Our method simultaneously generates – from the same data – both visual features and the interaction matrix that relates them to the robot control inputs. Extraction of the feature vector and control commands is done online and adaptively, and requires little data for initialization. Our method allows the robot to manipulate an object without knowing whether it is rigid or deformable. To validate our approach, we conduct numerical simulations and experiments with both deformable and rigid objects.},
  archive      = {J_RAS},
  author       = {Jihong Zhu and David Navarro-Alarcon and Robin Passama and Andrea Cherubini},
  doi          = {10.1016/j.robot.2021.103798},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103798},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Vision-based manipulation of deformable and rigid objects using subspace projections of 2D contours},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CHAD: Compact hand-assistive device for enhancement of
function in hand impairments. <em>RAS</em>, <em>142</em>, 103784. (<a
href="https://doi.org/10.1016/j.robot.2021.103784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand-assistive devices are used to help post-stroke victims encumbered with hand impairments perform activities of daily living (ADL). Unlike robotic rehabilitation devices used in restricted medical conditions for designated periods, hand-assistive devices are designed to be portable and to be used for extended periods by individuals engaging in ADL. Several hand-assistive device designs have been proposed. With these, designers have focused on key elements, such as size, weight, motion profile of the fingers, and generated grip/pinch force. In this paper, we propose a unique compact hand-assistive device (CHAD) that incorporates most of these design parameters, but with less trade-offs. CHAD consists of a single unit worn on the patient’s forearm, which includes all necessary components. It is compact and does not compromise functionality. The novelty of this design can be found in the use of a unique cable-driven mechanism. This mechanism uses dual linear actuators to achieve the flexion of both the index and the middle fingers via the pull of tendon-like structures originating in two selected interphalangeal joints . This permits the numerous necessary sequences in the motion profiles of the digits. The thumb is also made able to flex with a single linear actuator. Finger extensions, in contrast, are achieved passively via adjustable flexible rubber cords joined to the dorsal side of the glove. Experimental results demonstrate that CHAD generates sufficient force and motion profiles for the comfortable execution of ADL. Additionally, CHAD produces a grip and pinch motion profile similar to that of a natural hand and does not force unwanted muscle activities.},
  archive      = {J_RAS},
  author       = {Fady Alnajjar and Hassan Umari and Waleed K. Ahmed and Munkhjargal Gochoo and Alistair A. Vogan and Adel Aljumaily and Peer Mohamad and Shingo Shimoda},
  doi          = {10.1016/j.robot.2021.103784},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103784},
  shortjournal = {Robot. Auton. Syst.},
  title        = {CHAD: Compact hand-assistive device for enhancement of function in hand impairments},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Force analysis of the redundantly actuated parallel
mechanism 2RP̲r+p considering different control methodologies.
<em>RAS</em>, <em>142</em>, 103783. (<a
href="https://doi.org/10.1016/j.robot.2021.103783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the problem of driving forces/torques distribution of the redundantly actuated parallel mechanisms (PMs), although numerous optimization analysis methods, including the minimum input torque method and minimum energy consumption method have been proposed so far, however, the actual control modes of the actuators were not taken into account among the existing methods for the above problem. Therefore, the present study comprehensively considers both the elastic deformation and actuator’s displacement of each limb, proposes the idea of ”displacement coordination” and establishes the overall displacement coordination equations of the mechanisms. Three different control methodologies of the redundantly actuated PMs, including the full-position methodology, hybrid position–force control methodology and full-force methodology, are studied. For each control methodology, the correlation among the driving forces/torques, actuators’ displacements, external loads and limbs’ stiffness are discussed. An experimental platform of a redundantly actuated PM is built, and the corresponding test investigations for three control methodologies are conducted. In the present study, different control methodologies of the redundantly actuated PMs are considered for the first time, the principle of the dynamic coordination distribution is revealed in different methodologies, which have important reference values for design of coordinated motion control strategy of such kind of mechanisms.},
  archive      = {J_RAS},
  author       = {Yundou Xu and Ze Jiang and Zhongjin Ju and Zengzhao Wang and Wenlan Liu and Yongsheng Zhao},
  doi          = {10.1016/j.robot.2021.103783},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103783},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Force analysis of the redundantly actuated parallel mechanism 2RP̲R+P considering different control methodologies},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A controlled investigation of behaviorally-cloned deep
neural network behaviors in an autonomous steering task. <em>RAS</em>,
<em>142</em>, 103780. (<a
href="https://doi.org/10.1016/j.robot.2021.103780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imitation learning (IL) is a popular method used to train machine learning models that are capable of acting on their environment based on expert examples. Two types of IL models are inverse reinforcement learning (IRL) and behavioral cloning (BC). Models trained under IRL traditionally perform better than those trained under BC due to compounding covariate shift associated with the latter, which typically requires algorithms such as DAGGer to help compensate for this. More recently, however, deep learning architectures with increased generalization performance have been developed, which may help to alleviate the problem of compounding covariate shift and allow researchers to take advantage of the simplicity of BC. Despite these developments, recent studies on BC in sub-scale autonomous robots employ relatively primitive convolutional networks without such tools as batch normalization and skip connections, and it is difficult to judge their networks’ performance relative to others due to drastically different training and testing conditions. Here, we examine how an array of artificial neural networks , chosen to reflect more recent architectural choices available, behave in a highly controlled IL task – navigating around a small, indoor racetrack – upon being embedded in a sub-scale RC vehicle as an end-to-end steering system. For our main findings, we report the lap completion rate and path smoothness of each network under the exact same conditions as it controls the vehicle on the track. To supplement these findings, we also measure each network’s bias toward the distribution of the training actions and develop a method to highlight regions of a given input image that are deemed ‘important’ to a given network. We observe that most of the more recent neural networks perform reasonably well during testing, as opposed to the more primitive networks which did not perform as well. For these reasons and others, we identify VGG-16 and AlexNet – out of the networks tested here – as attractive candidate architectures for such tasks.},
  archive      = {J_RAS},
  author       = {Michael Teti and William Edward Hahn and Shawn Martin and Christopher Teti and Elan Barenholtz},
  doi          = {10.1016/j.robot.2021.103780},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103780},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A controlled investigation of behaviorally-cloned deep neural network behaviors in an autonomous steering task},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ColMap: A memory-efficient occupancy grid mapping framework.
<em>RAS</em>, <em>142</em>, 103755. (<a
href="https://doi.org/10.1016/j.robot.2021.103755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to possess a significant degree of autonomy, a robot must be able to perceive its environment and store a representation of that environment for use in tasks such as localisation , navigation, collision avoidance , and higher decision making. It must do this subject to constraints on memory and processing power typical of the embedded computer systems commonly found on small robotic devices. These constraints are particularly important for flying robots (i.e. unmanned aerial vehicles), for which weight must be minimised. The challenge of storing a detailed map of a large area on a small embedded computer has led to the development of many algorithms that exploit the sparsity of typical maps to create a more memory-efficient representation. In this paper, we demonstrate that the verticality of both natural and man-made structures can be exploited to create a framework that can store occupancy grid maps efficiently, without causing additional computational burden. The new framework achieves an order-of-magnitude reduction in memory footprint relative to widely-used occupancy grid mapping software, while also achieving a slight speed-up in map insertion and access times. We also make available LIDAR scans taken from a hexacopter of an indoor flight arena that can be used to assist in evaluating future mapping and SLAM developments.},
  archive      = {J_RAS},
  author       = {Alex Fisher and Ricardo Cannizzaro and Madeleine Cochrane and Chatura Nagahawatte and Jennifer L. Palmer},
  doi          = {10.1016/j.robot.2021.103755},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103755},
  shortjournal = {Robot. Auton. Syst.},
  title        = {ColMap: A memory-efficient occupancy grid mapping framework},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intrinsically compliant parallel robot for fractured femur
reduction: Mechanism optimization and control. <em>RAS</em>,
<em>141</em>, 103787. (<a
href="https://doi.org/10.1016/j.robot.2021.103787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robotic system for the reduction of fractured femur bone is proposed in this research to help orthopedics during the labor intensive bone reduction procedures and also save them from radiation stimulated environment. Fractured femur reduction is a good candidate for robotics application owing to its elongated anatomy and strong counteracting forces from surrounding muscles. However, the robot forces should be compliant, and motions need to be accurate. Aiming to achieve these two conflicting objective, a parallel robot actuated by six intrinsically compliant actuators is being proposed here. After an initial design analysis, three performance metrics, namely, the conditioning index, actuator force index and interaction compliance index were identified and formulated. An evolutionary algorithm SPEA2 was employed to simultaneously optimize these objectives by varying the key robot design variables. Subsequent to the optimization, an optimal robot design is obtained which provides the best trade-off between the performance measures . Initial proof of concept experiments were carried out whereby the robot was tested for trajectory following accuracies while maneuvering the moving platform about the three axes. A fuzzy based closed loop feedback controller was implemented on the robot. Excellent trajectory tracking results were observed in response to the sinusoidal inputs .},
  archive      = {J_RAS},
  author       = {Prashant K. Jamwal and Shahid Hussain and Mergen H. Ghayesh},
  doi          = {10.1016/j.robot.2021.103787},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103787},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Intrinsically compliant parallel robot for fractured femur reduction: Mechanism optimization and control},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MPTP: Motion-planning-aware task planning for navigation in
belief space. <em>RAS</em>, <em>141</em>, 103786. (<a
href="https://doi.org/10.1016/j.robot.2021.103786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an integrated Task-Motion Planning (TMP) framework for navigation in large-scale environments. Of late, TMP for manipulation has attracted significant interest resulting in a proliferation of different approaches. In contrast, TMP for navigation has received considerably less attention. Autonomous robots operating in real-world complex scenarios require planning in the discrete (task) space and the continuous (motion) space. In knowledge-intensive domains, on the one hand, a robot has to reason at the highest-level, for example, the objects to procure, the regions to navigate to in order to acquire them; on the other hand, the feasibility of the respective navigation tasks have to be checked at the execution level. This presents a need for motion-planning-aware task planners. In this paper, we discuss a probabilistically complete approach that leverages this task-motion interaction for navigating in large knowledge-intensive domains, returning a plan that is optimal at the task-level. The framework is intended for motion planning under motion and sensing uncertainty, which is formally known as belief space planning. The underlying methodology is validated in simulation, in an office environment and its scalability is tested in the larger Willow Garage world. A reasonable comparison with a work that is closest to our approach is also provided. We also demonstrate the adaptability of our approach by considering a building floor navigation domain. Finally, we also discuss the limitations of our approach and put forward suggestions for improvements and future work.},
  archive      = {J_RAS},
  author       = {Antony Thomas and Fulvio Mastrogiovanni and Marco Baglietto},
  doi          = {10.1016/j.robot.2021.103786},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103786},
  shortjournal = {Robot. Auton. Syst.},
  title        = {MPTP: Motion-planning-aware task planning for navigation in belief space},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Review of snake robots in constrained environments.
<em>RAS</em>, <em>141</em>, 103785. (<a
href="https://doi.org/10.1016/j.robot.2021.103785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snake robots have advantages of terrain adaptability over wheeled mobile robots and traditional articulated robot arms because of their limbless thin body structure and high flexibility. They have extensive applications in tasks such as rescue, disaster recovery, inspection and minimally invasive surgery . Current research on snake robots is mainly focused on snake-like locomotion and the embodiment of these motion gaits for different applications. Modular structure and real-time control algorithms are two key aspects for snake robots operating in constrained environments. This review will attempt to address both. First, a review on the snake motion and the body structure is provided, which outlines the biological foundation of all snake robots. This is followed by the mechanical structure of snake robots, especially the structure of elemental snake modules. Finally, control algorithms for variant terrain contours and obstacle avoidance are discussed. The review also outlines emerging application areas and potential future directions of snake robots.},
  archive      = {J_RAS},
  author       = {Jindong Liu and Yuchuang Tong and Jinguo Liu},
  doi          = {10.1016/j.robot.2021.103785},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103785},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Review of snake robots in constrained environments},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modest-vocabulary loop-closure detection with incremental
bag of tracked words. <em>RAS</em>, <em>141</em>, 103782. (<a
href="https://doi.org/10.1016/j.robot.2021.103782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key feature in the context of simultaneous localization and mapping is loop-closure detection, a process determining whether the current robot’s environment perception coincides with previous observation. However, in long-term operations, both computational efficiency and memory requirements involved in an autonomous robot operation in uncontrolled environments, are of particular importance. The majority of approaches scale linearly with the environment’s size in terms of storage and query time. The article at hand presents an efficient appearance-based loop-closure detection pipeline , which encodes the traversed trajectory by a low amount of unique visual words generated on-line through feature tracking. The incrementally constructed visual vocabulary is referred to as the “Bag of Tracked Words.” A nearest-neighbor voting scheme is utilized to query the database and assign probabilistic scores to all visited locations. Exploiting the inherent temporal coherency in the loop-closure task, the produced scores are processed through a Bayesian filter to estimate the belief state about the robot’s location on the map. Also, a geometrical verification step ensures consistency between image matches. Management is also applied to the resulting vocabulary to reduce its growth rate and constraint the system’s computational complexity while improving its voting distinctiveness. The proposed approach’s performance is experimentally evaluated on several publicly available and challenging datasets, including hand-held, car-mounted, aerial, and ground trajectories. Results demonstrate the method’s adaptability, which retains high operational frequency in environments of up to 13 km and high recall rates for perfect precision, outperforming other state-of-the-art techniques. The system’s effectiveness is owed to the reduced vocabulary size, which is at least one order of magnitude smaller than other contemporary approaches. An open research-oriented source code has been made publicly available, which is dubbed as “BoTW-LCD.”},
  archive      = {J_RAS},
  author       = {Konstantinos A. Tsintotas and Loukas Bampis and Antonios Gasteratos},
  doi          = {10.1016/j.robot.2021.103782},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103782},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Modest-vocabulary loop-closure detection with incremental bag of tracked words},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid position–rate teleoperation system. <em>RAS</em>,
<em>141</em>, 103781. (<a
href="https://doi.org/10.1016/j.robot.2021.103781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Position resolution is a major problem in teleoperation applications with significant disparity between master and slave workspace. While rate mode control is suitable for slave free motion operations, it poses significant stability and performance challenges for task manipulation. In this paper, we propose a hybrid control scheme that offers seamless transition between position and rate control modes based on the environment location information collected from a range sensor. The system incorporates the strengths of position and rate control modes while masking their shortcomings. Experiments to determine the viability of this method are carried out on a single degree-of-freedom teleoperation test-bed.},
  archive      = {J_RAS},
  author       = {Chiedu N. Mokogwu and Keyvan Hashtrudi-Zaad},
  doi          = {10.1016/j.robot.2021.103781},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103781},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A hybrid position–rate teleoperation system},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning context-adaptive task constraints for robotic
manipulation. <em>RAS</em>, <em>141</em>, 103779. (<a
href="https://doi.org/10.1016/j.robot.2021.103779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constraint-based control approaches offer a flexible way to specify robotic manipulation tasks and execute them on robots with many degrees of freedom. However, the specification of task constraints and their associated priorities usually requires a human-expert and often leads to tailor-made solutions for specific situations. This paper presents our recent efforts to automatically derive task constraints for a constraint-based robot controller from data and adapt them with respect to previously unseen situations (contexts). We use a programming-by-demonstration approach to generate training data in multiple variations (context changes) of a given task. From this data we learn a probabilistic model that maps context variables to task constraints and their respective soft task priorities. We evaluate our approach with 3 different dual-arm manipulation tasks on an industrial robot and show that it performs better than comparable approaches with respect to reproduction accuracy in previously unseen contexts.},
  archive      = {J_RAS},
  author       = {Dennis Mronga and Frank Kirchner},
  doi          = {10.1016/j.robot.2021.103779},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103779},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Learning context-adaptive task constraints for robotic manipulation},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LiDAR–camera calibration method based on ranging statistical
characteristics and improved RANSAC algorithm. <em>RAS</em>,
<em>141</em>, 103776. (<a
href="https://doi.org/10.1016/j.robot.2021.103776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For sensory data fusion, a calibration method between 3D light detection and ranging (LiDAR) and color camera based on ranging statistical characteristics and improved RANSAC algorithm is proposed. The multi-frame LiDAR point cloud data of the calibration triangular board are recorded. The scanned points with close angles are defined a cluster within same degrees. Furthermore, accurate points are preserved using statistical filtering based on Gaussian distribution. Afterwards, the plane and edge parameters of the triangular board are estimated by the reserved point cloud using improved the random sample consensus (RANSAC) algorithm to obtain the 3D locations of the vertices. Meanwhile, corner points in the image can be extracted manually. Finally, the projection matrix between the camera and the LiDAR is estimated by using the 2D–3D​ correspondences in different positions. The projection errors of different frames and corresponding points are calculated. The results demonstrate that the average error with 300 frames is reduced by 23.05\% compared to 1 frame. Moreover, the standard deviation diminishes with the increasing of corresponding points. The reliability and advantage of the method are verified compared with other state-of-art methods. It provides theoretical and technical support for low resolution LiDAR.},
  archive      = {J_RAS},
  author       = {Xiaobin Xu and Lei Zhang and Jian Yang and Cong Liu and Yiyang Xiong and Minzhou Luo and Zhiying Tan and Bo Liu},
  doi          = {10.1016/j.robot.2021.103776},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103776},
  shortjournal = {Robot. Auton. Syst.},
  title        = {LiDAR–camera calibration method based on ranging statistical characteristics and improved RANSAC algorithm},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 6D pose estimation with combined deep learning and 3D vision
techniques for a fast and accurate object grasping. <em>RAS</em>,
<em>141</em>, 103775. (<a
href="https://doi.org/10.1016/j.robot.2021.103775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time robotic grasping, supporting a subsequent precise object-in-hand operation task, is a priority target towards highly advanced autonomous systems . However, such an algorithm which can perform sufficiently-accurate grasping with time efficiency is yet to be found. This paper proposes a novel method with a 2-stage approach that combines a fast 2D object recognition using a deep neural network and a subsequent accurate and fast 6D pose estimation based on Point Pair Feature framework to form a real-time 3D object recognition and grasping solution capable of multi-object class scenes. The proposed solution has a potential to perform robustly on real-time applications, requiring both efficiency and accuracy. In order to validate our method, we conducted extensive and thorough experiments involving laborious preparation of our own dataset. The experiment results show that the proposed method scores 97.37\% accuracy in 5cm5deg metric and 99.37\% in Average Distance metric. Experiment results have shown an overall 62\% relative improvement (5cm5deg metric) and 52.48\% (Average Distance metric) by using the proposed method. Moreover, the pose estimation execution also showed an average improvement of 47.6\% in running time. Finally, to illustrate the overall efficiency of the system in real-time operations, a pick-and-place robotic experiment is conducted and has shown a convincing success rate with 90\% of accuracy. This experiment video is available at https://sites.google.com/view/dl-ppf6dpose/ .},
  archive      = {J_RAS},
  author       = {Tuan-Tang Le and Trung-Son Le and Yu-Ru Chen and Joel Vidal and Chyi-Yeu Lin},
  doi          = {10.1016/j.robot.2021.103775},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103775},
  shortjournal = {Robot. Auton. Syst.},
  title        = {6D pose estimation with combined deep learning and 3D vision techniques for a fast and accurate object grasping},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlinear MPC for collision-free and deadlock-free
navigation of multiple nonholonomic mobile robots. <em>RAS</em>,
<em>141</em>, 103774. (<a
href="https://doi.org/10.1016/j.robot.2021.103774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an online nonlinear Model Predictive Control (MPC) method for collision-free, deadlock-free navigation by multiple autonomous nonholonomic Wheeled Mobile Robots (WMRs). Our proposed method solves a nonlinear constrained optimization problem at each time step over a specified horizon to compute a sequence of optimal control inputs that drive the robots to target poses along collision-free trajectories, where the robots’ future states are predicted according to a unicycle kinematic model . To reduce the computational complexity of the optimization problem , we formulate it without stabilizing terminal constraints or terminal costs. We describe a computationally efficient approach to programming and solving the optimization problem, using open-source software tools for fast nonlinear optimization and applying the multiple-shooting method. We also provide rigorous proofs of the feasibility of the optimization problem and the stability of the proposed method. To validate the performance of our MPC method, we implement it in both 3D robot simulations and experiments with real nonholonomic WMRs for different multi-robot navigation scenarios with up to six robots. In all scenarios, the robots successfully navigate to their goal poses without colliding with one another or becoming trapped in a deadlock .},
  archive      = {J_RAS},
  author       = {Amir Salimi Lafmejani and Spring Berman},
  doi          = {10.1016/j.robot.2021.103774},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103774},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Nonlinear MPC for collision-free and deadlock-free navigation of multiple nonholonomic mobile robots},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive visual servo control law for finite-time tracking
to land quadrotor on moving platform using virtual reticle algorithm.
<em>RAS</em>, <em>141</em>, 103764. (<a
href="https://doi.org/10.1016/j.robot.2021.103764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed an image-based visual servoing (IBVS) control law for a quadrotor that is equipped with a single monocular camera attached to its bottom. For control purposes virtual reticle plane (VRP) algorithm is used to track the relative 3D position of the quadrotor to the tilting and moving target landing platform within the range of the camera’s field of view (FOV). In this article, the landing platform’s tilting motion is considered sinusoidal type oscillatory motion for the overhead camera. For control purposes, an adaptive finite-time control (AFTC) is proposed, based on the finite-time control (FTC) and adaptive approximation of uncertainties. A constructive combination of FTC and adaptive approximation inherits benefits of both to overcome each other’s limitations. The task of the controller is to regulate the position error to zero in time calculated by VRP. The experimental results confirmed the effectiveness of the VRP algorithm to track the desired parameters of the moving target. Finally, simulations are performed to illustrate the effectiveness and improved performance of the proposed AFTC in response time, robustness, and tracking accuracy.},
  archive      = {J_RAS},
  author       = {Adeel Arif and Hesheng Wang and Zhe Liu and Herman Castañeda and Yong Wang},
  doi          = {10.1016/j.robot.2021.103764},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103764},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Adaptive visual servo control law for finite-time tracking to land quadrotor on moving platform using virtual reticle algorithm},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A probabilistic framework for learning geometry-based robot
manipulation skills. <em>RAS</em>, <em>141</em>, 103761. (<a
href="https://doi.org/10.1016/j.robot.2021.103761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming robots to perform complex manipulation tasks is difficult because many tasks require sophisticated controllers that may rely on data such as manipulability ellipsoids, stiffness/damping and inertia matrices. Such data are naturally represented as Symmetric Positive Definite (SPD) matrices to capture specific geometric characteristics of the data, which increases the complexity of hard-coding them. To alleviate this difficulty, the Learning from Demonstration (LfD) paradigm can be used in order to learn robot manipulation skills with specific geometric constraints encapsulated in SPD matrices. Learned skills often need to be adapted when they are applied to new situations. While existing techniques can adapt Cartesian and joint space trajectories described by various desired points, the adaptation of motion skills encapsulated in SPD matrices remains an open problem . In this paper, we introduce a new LfD framework that can learn robot manipulation skills encapsulated in SPD matrices from expert demonstrations and adapt them to new situations defined by new start-, via- and end-matrices. The proposed approach leverages Kernelized Movement Primitives (KMPs) to generate SPD-based robot manipulation skills that smoothly adapt the demonstrations to conform to new constraints. We validate the proposed framework using a couple of simulations in addition to a real experiment scenario.},
  archive      = {J_RAS},
  author       = {Fares J. Abu-Dakka and Yanlong Huang and João Silvério and Ville Kyrki},
  doi          = {10.1016/j.robot.2021.103761},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103761},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A probabilistic framework for learning geometry-based robot manipulation skills},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal trajectory planning for cinematography with multiple
unmanned aerial vehicles. <em>RAS</em>, <em>140</em>, 103778. (<a
href="https://doi.org/10.1016/j.robot.2021.103778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a method for planning optimal trajectories with a team of Unmanned Aerial Vehicles (UAVs) performing autonomous cinematography. The method is able to plan trajectories online and in a distributed manner, providing coordination between the UAVs. We propose a novel non-linear formulation for this challenging problem of computing multi-UAV optimal trajectories for cinematography; integrating UAVs dynamics and collision avoidance constraints, together with cinematographic aspects like smoothness, gimbal mechanical limits and mutual camera visibility. We integrate our method within a hardware and software architecture for UAV cinematography that was previously developed within the framework of the MultiDrone project; and demonstrate its use with different types of shots filming a moving target outdoors. We provide extensive experimental results both in simulation and field experiments. We analyze the performance of the method and prove that it is able to compute online smooth trajectories, reducing jerky movements and complying with cinematography constraints.},
  archive      = {J_RAS},
  author       = {Alfonso Alcántara and Jesús Capitán and Rita Cunha and Aníbal Ollero},
  doi          = {10.1016/j.robot.2021.103778},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103778},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Optimal trajectory planning for cinematography with multiple unmanned aerial vehicles},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ontology-based knowledge management with verbal interaction
for command interpretation and execution by home service robots.
<em>RAS</em>, <em>140</em>, 103763. (<a
href="https://doi.org/10.1016/j.robot.2021.103763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a system for service robots that combines ontological knowledge reasoning and human–robot interaction to interpret natural language commands and successfully perform household chores, such as finding and delivering objects. Knowledge and context reasoning is essential for providing more efficient service robots , given their diverse and continuously changing environments. Moreover, since they are in contact with humans, robots require such skills as interaction and language. Therefore, we developed a system with specific modules to manage robots’ knowledge and reasoning, command analysis, decision-making, and talking interaction. The system relies on inference methods and verbal interaction to understand commands and clarify uncertain information. We tested our system inside a simulated environment where the robot receives commands with missing or unclear information. The system’s performance was compared with the average performance of human subjects who completed the same commands in the simulation.},
  archive      = {J_RAS},
  author       = {L. Villamar Gómez and J. Miura},
  doi          = {10.1016/j.robot.2021.103763},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103763},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Ontology-based knowledge management with verbal interaction for command interpretation and execution by home service robots},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust gait design for a compass gait biped on slippery
surfaces. <em>RAS</em>, <em>140</em>, 103762. (<a
href="https://doi.org/10.1016/j.robot.2021.103762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current bipedal robots were modeled with an assumption that there is no slip between the stance foot and ground. This paper relaxes that assumption and undertakes a comprehensive study of the compass gait biped on slippery ground. It presents in detail the control of a biped that allows for foot slipping, and shows that feasible gaits fail on slippery ground for two causes: falling backward or requiring negative contact force which cannot be provided by the ground. To characterize a robust gait on slippery ground, three safety factors are proposed to measure the robustness: slip friction, falling friction and tolerance ability of slipping without falling. This study thus uses these factors to investigate independent influence of gait speed and step length on the robustness of the gait, and shows that gaits with small step length and moderate speed are robust and preferable on slippery surfaces. In contrast, gaits with large step length generally require large friction to maintain stable walking on slippery surfaces. Moreover, gaits with a backward swing foot velocity relative to the ground just before touch down are generally more robust than ones with a forward velocity . It is further shown that only one parameter in gait design determines the swing-backward feature, which can help design robust gaits. Models with varying physical parameters such as mass, leg length and position of center of mass (CoM) in each leg, are also studied to validate the universality of this result.},
  archive      = {J_RAS},
  author       = {Tan Chen and Bill Goodwine},
  doi          = {10.1016/j.robot.2021.103762},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103762},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robust gait design for a compass gait biped on slippery surfaces},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GR-LOAM: LiDAR-based sensor fusion SLAM for ground robots on
complex terrain. <em>RAS</em>, <em>140</em>, 103759. (<a
href="https://doi.org/10.1016/j.robot.2021.103759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping is a fundamental process in robot navigation . We focus on LiDAR to complete this process in ground robots traveling on complex terrain by proposing GR-LOAM, a method to estimate robot ego-motion by fusing LiDAR, inertial measurement unit (IMU), and encoder measurements in a tightly coupled scheme. First, we derive a odometer increment model that fuses the IMU and encoder measurements to estimate the robot pose variation on a manifold. Then, we apply point cloud segmentation and feature extraction to obtain distinctive edge and planar features. Moreover, we propose an evaluation algorithm for the sensor measurements to detect abnormal data and reduce their corresponding weight during optimization. By jointly optimizing the cost derived from the LiDAR, IMU, and encoder measurements in a local window, we obtain low-drift odometry even on complex terrain. We use the estimated relative pose in the local window to reevaluate the matching distance across features and remove dynamic objects and outliers, thus refining the features before being fed to a mapping thread and increasing the mapping efficiency. In the back end, GR-LOAM uses the refined point cloud and tightly couples the IMU and encoder measurements with ground constraints to further refine the estimated pose by aligning the features on a global map. Results from extensive experiments performed in indoor and outdoor environments using real ground robot demonstrate the high accuracy and robustness of the proposed GR-LOAM for state estimation of ground robots.},
  archive      = {J_RAS},
  author       = {Yun Su and Ting Wang and Shiliang Shao and Chen Yao and Zhidong Wang},
  doi          = {10.1016/j.robot.2021.103759},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103759},
  shortjournal = {Robot. Auton. Syst.},
  title        = {GR-LOAM: LiDAR-based sensor fusion SLAM for ground robots on complex terrain},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing the effectiveness of rescheduling and flexible
execution methods to address uncertainty in execution duration for a
planetary rover. <em>RAS</em>, <em>140</em>, 103758. (<a
href="https://doi.org/10.1016/j.robot.2021.103758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During execution, activity durations may vary from those predicted in the generated schedule. In this article we study (re) scheduling invocation, execution during rescheduling, and flexible execution to enable a high level of responsiveness to uncertainty in activity execution duration. We discuss these methods theoretically in the context of an embedded scheduler and practically in the context of a limited CPU embedded scheduler with a nonzero scheduler runtime intended for a planetary rover. We use the concept of a commit window to enable execution of the previously generated schedule while (re) scheduling. We define Fixed Cadence and Event Driven scheduling as methods to decide when to reinvoke the scheduler. We define and analyze Flexible Execution (FE) as an approach to execute the generated schedule while adapting it to variations in execution. Specifically, FE focuses on (1) how to take advantage of activities ending earlier than expected and (2) how to maintain a consistent schedule if activities take more time than expected. We present a theoretical model and empirical results documenting how these various methods interact and perform on both synthetic data and best available data for NASA’s next planetary rover, the Mars 2020 rover. We then describe how these analyses influenced the onboard software for the Mars 2020 rover.},
  archive      = {J_RAS},
  author       = {Jagriti Agrawal and Wayne Chi and Steve Chien and Gregg Rabideau and Daniel Gaines and Stephen Kuhn},
  doi          = {10.1016/j.robot.2021.103758},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103758},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Analyzing the effectiveness of rescheduling and flexible execution methods to address uncertainty in execution duration for a planetary rover},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fractional sliding mode control for an autonomous
two-wheeled vehicle equipped with an innovative gyroscopic actuator.
<em>RAS</em>, <em>140</em>, 103756. (<a
href="https://doi.org/10.1016/j.robot.2021.103756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balancing two-wheeled autonomous vehicles at low forward speeds is one of the primary challenges in the development of such vehicles. Gyrostabilizers can be used as actuators to make the balance; however, conventional gyros are not typically able to maintain constant moments and directions to stabilize against constant ‘heel’. In this paper, we present an innovative gyrostabilizer including a twin-flywheel arrangement that can provide any desired gyroscopic roll moment. The dynamical model of a bicycle together with the gyrostabilizer is derived using Newton–Euler method. The actuator dynamics is included when designing the control system. A robust non-integer sliding mode controller is then developed to guarantee perfect trajectory tracking in the presence of roll disturbance. Extensive comparative simulations (based on the experimentally measured parameters of a typical bike) are conducted to evaluate the method and to show the impact of introducing the novel actuator. Results demonstrate that the proposed system offers superior performance while the control effort also remains within the capacity of normal actuators.},
  archive      = {J_RAS},
  author       = {M.A. Tofigh and M.J. Mahjoob and M.R. Hanachi and M. Ayati},
  doi          = {10.1016/j.robot.2021.103756},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103756},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Fractional sliding mode control for an autonomous two-wheeled vehicle equipped with an innovative gyroscopic actuator},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The impact of catastrophic collisions and collision
avoidance on a swarming behavior. <em>RAS</em>, <em>140</em>, 103754.
(<a href="https://doi.org/10.1016/j.robot.2021.103754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarms of autonomous agents are useful in many applications due to their ability to accomplish tasks in a decentralized manner, making them more robust to failures. Due to the difficulty in running experiments with large numbers of hardware agents, researchers often make simplifying assumptions and remove constraints that might be present in a real swarm deployment. While simplifying away some constraints is tolerable, we feel that two in particular have been overlooked: one, that agents in a swarm take up physical space, and two, that agents might be damaged in collisions. Many existing works assume agents have negligible size or pass through each other with no added penalty. It seems possible to ignore these constraints using collision avoidance , but we show using an illustrative example that this is easier said than done. In particular, we show that collision avoidance can interfere with the intended swarming behavior and significant parameter tuning is necessary to ensure the behavior emerges as best as possible while collisions are avoided. We compare four different collision avoidance algorithms, two of which we consider to be the best decentralized collision avoidance algorithms available. Despite putting significant effort into tuning each algorithm to perform at its best, we believe our results show that further research is necessary to develop swarming behaviors that can achieve their goal while avoiding collisions with agents of non-negligible volume.},
  archive      = {J_RAS},
  author       = {Chris Taylor and Cameron Nowzari},
  doi          = {10.1016/j.robot.2021.103754},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103754},
  shortjournal = {Robot. Auton. Syst.},
  title        = {The impact of catastrophic collisions and collision avoidance on a swarming behavior},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative human-autonomy semantic sensing through
structured POMDP planning. <em>RAS</em>, <em>140</em>, 103753. (<a
href="https://doi.org/10.1016/j.robot.2021.103753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous unmanned systems and robots must be able to actively leverage all available information sources — including imprecise but readily available semantic observations provided by human collaborators . This work develops and validates a novel active collaborative human–machine sensing solution for robotic information gathering and optimal decision making problems, with an example implementation of a dynamic target search scenario. Our approach uses continuous partially observable Markov decision process (CPOMDP) planning to generate vehicle trajectories that optimally exploit imperfect detection data from onboard sensors, as well as semantic natural language observations that can be specifically requested from human sensors. The key innovations are a method for the inclusion of a human querying/sensing model in a CPOMDP based autonomous decision making process, as well as a scalable hierarchical Gaussian mixture model formulation for efficiently solving CPOMDPs with semantic observations in continuous dynamic state spaces. Unlike previous state-of-the-art approaches this allows planning in large, complex, highly segmented environments. Our solution is demonstrated and validated with a real human–robot team engaged in dynamic indoor target search and capture scenarios on a custom testbed .},
  archive      = {J_RAS},
  author       = {Luke Burks and Nisar Ahmed and Ian Loefgren and Luke Barbier and Jeremy Muesing and Jamison McGinley and Sousheel Vunnam},
  doi          = {10.1016/j.robot.2021.103753},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103753},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Collaborative human-autonomy semantic sensing through structured POMDP planning},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new approach to time-optimal trajectory planning with
torque and jerk limits for robot. <em>RAS</em>, <em>140</em>, 103744.
(<a href="https://doi.org/10.1016/j.robot.2021.103744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a new convex optimization (CO) approach to time-optimal trajectory planning (TOTP) is described, which considers both torque and jerk limits. The key insight of the approach is that the non-convex jerk limits are transformed to linear acceleration constraints and indirectly introduced into CO as the linear acceleration constraints . In this way, the convexity of CO will not be destroyed and the number of optimization variables will not increase, which give the approach a fast computation speed. The proposed approach is implemented on random geometric path of a 6-DOF manipulator. Compared with a similar method, the results show that the torque and jerk limits are addressed by a reasonable increase in the computation time. In addition, the maximum value of joint jerk reduces by over 80\% and the joint torque curves are smoother in the comparison, which demonstrates that this approach has the ability to effectively restrain acceleration mutation.},
  archive      = {J_RAS},
  author       = {Jian-wei Ma and Song Gao and Hui-teng Yan and Qi Lv and Guo-qing Hu},
  doi          = {10.1016/j.robot.2021.103744},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103744},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A new approach to time-optimal trajectory planning with torque and jerk limits for robot},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A benchmark for point clouds registration algorithms.
<em>RAS</em>, <em>140</em>, 103734. (<a
href="https://doi.org/10.1016/j.robot.2021.103734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds registration is a fundamental step of many point clouds processing pipelines; however, most algorithms are tested on data that are collected ad-hoc and not shared with the research community. These data often cover only a very limited set of use cases; therefore, the results cannot be generalized. Public datasets proposed until now, taken individually, cover only a few kinds of environment and mostly a single sensor. For these reasons, we developed a benchmark, for localization and mapping applications, using multiple publicly available datasets. In this way, we are able to cover many kinds of environment and many kinds of sensor that can produce point clouds. Furthermore, the ground truth has been thoroughly inspected and evaluated to ensure its quality. For some of the datasets, the accuracy of the ground truth measuring system was not reported by the original authors, therefore we estimated it with our own novel method, based on an iterative registration algorithm. Along with the data, we provide a broad set of registration problems, chosen to cover different types of initial misalignment, various degrees of overlap, and different kinds of registration problems. Lastly, we propose a metric to measure the performances of registration algorithms: it combines the commonly used rotation and translation errors together, to allow an objective comparison of the alignments. This work aims at encouraging authors to use a public and shared benchmark, instead of data collected ad-hoc, to ensure objectivity and repeatability, two fundamental characteristics in any scientific field.},
  archive      = {J_RAS},
  author       = {Simone Fontana and Daniele Cattaneo and Augusto L. Ballardini and Matteo Vaghi and Domenico G. Sorrenti},
  doi          = {10.1016/j.robot.2021.103734},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103734},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A benchmark for point clouds registration algorithms},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast-moving piezoelectric micro-robotic fish with double
caudal fins. <em>RAS</em>, <em>140</em>, 103733. (<a
href="https://doi.org/10.1016/j.robot.2021.103733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-robotic fish (length ≤ ≤ 10 cm) driven by smart materials have remarkable advantages over conventional motors and piston-based robotic fish . In particular, they are highly efficient and compact. One of the key challenges is attaining high mobility with high energy density , low driving voltage and power loss. In this work, a double caudal fin micro-robotic fish actuated by two piezoelectric bimorph cantilevers is proposed and fabricated from rigid carbon fiber/resin composites and flexible polyimide hinges. Its weight is about 1.93 g and the maximum uniform swimming velocity is as high as about 0.75 BL/s (4.5 cm/s), which is much faster than previously reported micro-robotic fish actuated by ionic polymer–metal composites, shape memory alloys and dielectric elastomers . A theoretical model is validated by the experimental results and can be used to design and analyze a variety of piezoelectric robotic fish propelled by caudal fins.},
  archive      = {J_RAS},
  author       = {Quanliang Zhao and Shiqi Liu and Jinghao Chen and Guangping He and Jiejian Di and Lei Zhao and Tingting Su and Mengying Zhang and Zhiling Hou},
  doi          = {10.1016/j.robot.2021.103733},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103733},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Fast-moving piezoelectric micro-robotic fish with double caudal fins},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive planning framework for dexterous robotic
grasping with grasp type detection. <em>RAS</em>, <em>140</em>, 103727.
(<a href="https://doi.org/10.1016/j.robot.2021.103727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dexterous grasping is one of the most fundamental abilities of robots to implement various manipulation tasks. Robots should have the same ability as humans to plan various grasp types for dexterous grasping. This paper addresses the problem of the adaptability of grasp planning. A novel adaptive grasp planning framework is designed to adapt to various grasp types rather than a single one. In this framework, six commonly used grasp types are considered. The information of grasp type is extracted from visual data. Then, inspired by the opposition concept, a novel concept of pregrasping opposition is introduced as the pregrasping configuration to encode the information of the grasp type. After that, a two-stage adaptive grasp planning method is proposed, which determines the pregrasping opposition in Stage One and finds a feasible grasp configuration for object grasping in Stage Two. The pregrasping opposition is used as a waypoint for the formation of complex grasps. The effectiveness of the proposed framework was evaluated in simulation and real-world experiments. The experimental results demonstrated that the proposed framework can plan various grasp types for dexterous robotic grasping. Additionally, the use of grasp types helps to reduce the complexity of grasp planning and improve the grasp dexterity of robotic hands .},
  archive      = {J_RAS},
  author       = {Zhen Deng and Bin Fang and Bingwei He and Jianwei Zhang},
  doi          = {10.1016/j.robot.2021.103727},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103727},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An adaptive planning framework for dexterous robotic grasping with grasp type detection},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online plan modification in uncertain resource-constrained
environments. <em>RAS</em>, <em>140</em>, 103726. (<a
href="https://doi.org/10.1016/j.robot.2021.103726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an approach to planning under uncertainty in resource-constrained environments. We describe our novel method for online plan modification and execution monitoring, which augments an existing plan with pre-computed plan fragments in response to observed resource availability. Our plan merging algorithm uses causal structure to interleave actions, creating solutions online using observations of the true state without introducing significant computational cost. Our system monitors resource availability, reasoning about the probability of successfully completing the goals. We show that when the probability of completing a plan decreases, by removing low-priority goals our system reduces the risk of plan failure, increasing mission success rate. Conversely, when resource availability allows, by including additional goals our system increases reward without adversely affecting success rate. We evaluate our approach using the example domain of long-range autonomous underwater vehicle (AUV) missions, in which a vehicle spends months at sea with little or no opportunity for intervention. We compare the performance to a state-of-the-art oversubscription planner. Planning within such domains is challenging because significant resource usage uncertainty means it is computationally infeasible to calculate the optimal strategy in advance. We also evaluate the applicability of our plan merging algorithm to existing IPC domains, presenting a discussion of the domain characteristics which favour the use of our approach.},
  archive      = {J_RAS},
  author       = {Catherine A. Harris and Nick Hawes and Richard Dearden},
  doi          = {10.1016/j.robot.2021.103726},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103726},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Online plan modification in uncertain resource-constrained environments},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ourobot—a sensorized closed-kinematic-chain robot for
shape-adaptive rolling in rough terrain. <em>RAS</em>, <em>140</em>,
103715. (<a href="https://doi.org/10.1016/j.robot.2020.103715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the abilities of amoeba to alter their shape, a continuous-track robot called O urobot has been developed that is able to adapt its shape to the environment. Using tactile sensors at the outer hull of the robot, the outline of the terrain and collisions with obstacles can be detected. Thus, the robot is able to locomote in uneven terrain and climb steep slopes. Since the shape adaption is based on run-time optimization, the quality function can be easily expanded to consider additional side conditions. The functionality of the proposed approach is demonstrated both in simulation and hardware.},
  archive      = {J_RAS},
  author       = {Jan Paskarbeit and Simon Beyer and Matthäus Engel and Adrian Gucze and Johann Schröder and Axel Schneider},
  doi          = {10.1016/j.robot.2020.103715},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103715},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Ourobot—A sensorized closed-kinematic-chain robot for shape-adaptive rolling in rough terrain},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing satellite semantic maps with ground-level imagery.
<em>RAS</em>, <em>139</em>, 103760. (<a
href="https://doi.org/10.1016/j.robot.2021.103760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper at hand introduces a novel system for producing an enhanced semantic map that leverages a reconstruction approach of street-view scenes using computer vision and machine learning techniques . Focusing on the recognition and localization of objects/entities, the composed map combines semantic information from publicly available, yet of lower accuracy, satellite images, with more detailed data from ground-level camera measurements. This merging is achieved by utilizing odometry information from a street-moving vehicle and the 3D reconstruction of its recorded view. Then, the 3D semantic segmentation results are georeferenced and superimposed on the semantic map from the satellite images. In such a way, areas that require fine semantic accuracy can be improved, while the rest are left with the segmentation results of the satellite information. Every part of the proposed system is individually evaluated. We additionally test the overall approach on a case-study of georeferencing new labels of traffic signs, which are detected through a specifically designed classification network over a publicly available dataset collected around the city of Berlin.},
  archive      = {J_RAS},
  author       = {Vasiliki Balaska and Loukas Bampis and Ioannis Kansizoglou and Antonios Gasteratos},
  doi          = {10.1016/j.robot.2021.103760},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103760},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Enhancing satellite semantic maps with ground-level imagery},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time deep learning approach to visual servo control and
grasp detection for autonomous robotic manipulation. <em>RAS</em>,
<em>139</em>, 103757. (<a
href="https://doi.org/10.1016/j.robot.2021.103757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots still cannot perform everyday manipulation tasks, such as grasping, with the same dexterity as humans do. In order to explore the potential of supervised deep learning for robotic grasping in unstructured and dynamic environments, this work addresses the visual perception phase involved in the task. This phase involves the processing of visual data to obtain the location of the object to be grasped, its pose and the points at which the robot’s grippers must make contact to ensure a stable grasp. For this, the Cornell Grasping Dataset (CGD) is used to train a Convolutional Neural Network (CNN) that is able to consider these three stages simultaneously. In other words, having an image of the robot’s workspace, containing a certain object, the network predicts a grasp rectangle that symbolizes the position, orientation and opening of the robot’s parallel grippers the instant before its closing. In addition to this network, which runs in real-time, another network is designed, so that it is possible to deal with situations in which the object moves in the environment. Therefore, the second convolutional network is trained to perform a visual servo control , ensuring that the object remains in the robot’s field of view. This network predicts the proportional values of the linear and angular velocities that the camera must have to ensure the object is in the image processed by the grasp network. The dataset used for training was automatically generated by a Kinova Gen3 robotic manipulator with seven Degrees of Freedom (DoF). The robot is also used to evaluate the applicability in real-time and obtain practical results from the designed algorithms. Moreover, the offline results obtained through test sets are also analyzed and discussed regarding their efficiency and processing speed. The developed controller is able to achieve a millimeter accuracy in the final position considering a target object seen for the first time. To the best of our knowledge, we have not found in the literature other works that achieve such precision with a controller learned from scratch. Thus, this work presents a new system for autonomous robotic manipulation, with the ability to generalize to different objects and with high processing speed, which allows its application in real robotic systems .},
  archive      = {J_RAS},
  author       = {Eduardo Godinho Ribeiro and Raul de Queiroz Mendes and Valdir Grassi Jr.},
  doi          = {10.1016/j.robot.2021.103757},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103757},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Real-time deep learning approach to visual servo control and grasp detection for autonomous robotic manipulation},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical POMDP planning for object manipulation in
clutter. <em>RAS</em>, <em>139</em>, 103736. (<a
href="https://doi.org/10.1016/j.robot.2021.103736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object manipulation planning in clutter suffers from perception uncertainties due to occlusion, as well as action constraints required by collision avoidance . Partially observable Markov decision process (POMDP) provides a general model for planning under uncertainties. But a manipulation task usually have a large action space, which not only makes task planning intractable but also brings significant motion planning effort to check action feasibility. In this work, a new kind of hierarchical POMDP is presented for object manipulation tasks, in which a brief abstract POMDP is extracted and utilized together with the original POMDP. And a hierarchical belief tree search algorithm is proposed for efficient online planning, which constructs fewer belief nodes by building part of the tree with the abstract POMDP and invokes motion planning fewer times by determining action feasibility with observation function of the abstract POMDP. A learning mechanism is also designed in case there are unknown probabilities in transition and observation functions. This planning framework is demonstrated with an object fetching task and the performance is empirically validated by simulations and experiments.},
  archive      = {J_RAS},
  author       = {Wenrui Zhao and Weidong Chen},
  doi          = {10.1016/j.robot.2021.103736},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103736},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Hierarchical POMDP planning for object manipulation in clutter},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate and real-time human-joint-position estimation for a
patient-transfer robot using a two-level convolutional neutral network.
<em>RAS</em>, <em>139</em>, 103735. (<a
href="https://doi.org/10.1016/j.robot.2021.103735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-joint-position estimation is crucial for patient-transfer robots. However, high accuracy and real-time property are difficult to achieve simultaneously. To tackle the problem, we develop a new convolutional neural network (CNN), containing two levels of subnetworks , to fuse the information in color and depth images. The first-level subnetwork generates two-dimensional (2D) human joint positions from a color image by the part-affinity-fields method. The second-level subnetwork estimates 3D human-joint positions from 2D ones and corresponding depth images. Here, strong feature-extraction function of the CNN may suppress the negative effect caused by invalid information in depth images. Meanwhile, all the estimations are implemented with the 2D CNNs, which may cause higher time-efficiency than 3D ones (mostly used in previous studies). To assess the validity, first we employed the CNN to estimate human joint positions, and obtained the accuracy and speed of respectively 90.3\% and 210 ms (implemented with an affordable processing unit). Then we applied the CNN to a dual-arm nursing-care robot and found that the accuracy and processing speed satisfied the requirements in practical usage; these validated the effectiveness of our proposal and provided a new approach to generate 3D-human-joint positions through information fusion of color and depth images.},
  archive      = {J_RAS},
  author       = {Mengqian Chen and Jiang Wu and Shunda Li and Jinyue Liu and Hideo Yokota and Shijie Guo},
  doi          = {10.1016/j.robot.2021.103735},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103735},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Accurate and real-time human-joint-position estimation for a patient-transfer robot using a two-level convolutional neutral network},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamics compensation of impedance-based motion control for
LHDS of legged robot. <em>RAS</em>, <em>139</em>, 103704. (<a
href="https://doi.org/10.1016/j.robot.2020.103704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aimed at the negative effect of dynamics characteristics of leg hydraulic drive system (LHDS) on the accuracy of motion control of the hydraulic drive legged robot, a dynamics compensation control method is proposed. First, according to the mechanical structure of LHDS, the kinematics and statics models of LHDS are analyzed and obtained respectively. Based on the principle of force-based impedance control of LHDS, an impedance based motion control simulation model of LHDS is built and analyzed. The simulation results show that the dynamics characteristics have a great influence on the accuracy of impedance based motion control. Then, a dynamics compensation method considering gravity and inertia force is proposed to solve this problem. Finally, the effect of the dynamics compensation method is verified on the robot single leg test platform. The experimental results show that the compensation method reduces the negative effect of the dynamics characteristics of LHDS on impedance based motion control accuracy, and the position tracking accuracy of the robot’s foot end can be improved by more than 65\%. The theory proposed in this paper provides a theoretical basis for the motion control of the whole robot prototype.},
  archive      = {J_RAS},
  author       = {Kaixian Ba and Yanhe Song and Bin Yu and Xiaolong He and Zhipeng Huang and Chunhe Li and Lipeng Yuan and Xiangdong Kong},
  doi          = {10.1016/j.robot.2020.103704},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103704},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Dynamics compensation of impedance-based motion control for LHDS of legged robot},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated algorithm for ego-vehicle and obstacles state
estimation for autonomous driving. <em>RAS</em>, <em>139</em>, 103662.
(<a href="https://doi.org/10.1016/j.robot.2020.103662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding of the driving scenario represents a necessary condition for autonomous driving . Within the control routine of an autonomous vehicle, it represents the preliminary step for the motion planning system. Estimation algorithms hence need to handle a considerable number of information coming from multiple sensors, to provide estimates regarding the motion of ego-vehicle and surrounding obstacles. Furthermore, tracking is crucial in obstacles state estimation, because it ensures obstacles recognition during time. This paper presents an integrated algorithm for the estimation of ego-vehicle and obstacles’ positioning and motion along a given road, modeled in curvilinear coordinates . Sensor fusion deals with information coming from two Radars and a Lidar to identify and track obstacles. The algorithm has been validated through experimental tests carried on a prototype of an autonomous vehicle.},
  archive      = {J_RAS},
  author       = {Mattia Bersani and Simone Mentasti and Pragyan Dahal and Stefano Arrigoni and Michele Vignati and Federico Cheli and Matteo Matteucci},
  doi          = {10.1016/j.robot.2020.103662},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103662},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An integrated algorithm for ego-vehicle and obstacles state estimation for autonomous driving},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A wearable sensor vest for social humanoid robots with
GPGPU, IoT, and modular software architecture. <em>RAS</em>,
<em>139</em>, 103536. (<a
href="https://doi.org/10.1016/j.robot.2020.103536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, most social robots interact with their surroundings and humans through sensors that are integral parts of the robots, which limits the usability of the sensors, human–robot interaction, and interchangeability. A wearable sensor garment that fits many robots is needed in many applications. This article presents an affordable wearable sensor vest, and an open-source software architecture with the Internet of Things (IoT) for social humanoid robots . The vest consists of touch, temperature, gesture, distance, vision sensors, and a wireless communication module. The IoT feature allows the robot to interact with humans locally and over the Internet. The designed architecture works for any social robot that has a general-purpose graphics processing unit (GPGPU), I 2 C/SPI buses, Internet connection, and the Robotics Operating System (ROS). The modular design of this architecture enables developers to easily add/remove/update complex behaviors. The proposed software architecture provides IoT technology, GPGPU nodes, I 2 C and SPI bus mangers, audio-visual interaction nodes (speech to text, text to speech, and image understanding), and isolation between behavior nodes and other nodes. The proposed IoT solution consists of related nodes in the robot, a RESTful web service, and user interfaces. We used the HTTP protocol as a means of two-way communication with the social robot over the Internet. Developers can easily edit or add nodes in C, C++, and Python programming languages . Our architecture can be used for designing more sophisticated behaviors for social humanoid robots .},
  archive      = {J_RAS},
  author       = {Mohsen Jafarzadeh Ph.D. and Stephen Brooks B.S. and Shimeng Yu M.S. and Balakrishnan Prabhakaran Ph.D. and Yonas Tadesse Ph.D.},
  doi          = {10.1016/j.robot.2020.103536},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103536},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A wearable sensor vest for social humanoid robots with GPGPU, IoT, and modular software architecture},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic prioritized motion coordination of multi-AGV
systems. <em>RAS</em>, <em>139</em>, 103534. (<a
href="https://doi.org/10.1016/j.robot.2020.103534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motion coordination problem for a fleet of Autonomous Guided Vehicles (AGVs) in a confined industrial facility is addressed. The working scenario involves a group of AGVs that is tasked to transport without collisions to predefined locations within the industrial facility. We introduce a centralized motion coordination controller that utilizes a dynamic priority logic to resolve motion conflicts between AGVs as they appear. The controller relies on the implementation of a predefined, virtual transportation network that is comparable to a conventional right-handed bidirectional traffic system. The construction of the transportation network considers the physical and motion characteristics of the AGVs (dimensions and maximum speed). The high-level function of the controller is to detect imminent collisions and determine the right-of-way of conflicting AGVs in same-directional routes and intersection junctions of the transportation network. The priority update logic is inspired by the traffic control of conventional four-way stop-controlled intersections. Based on the updated priorities, the motion coordinator adjusts the advancement of the AGVs to eliminate collisions. The proposed formulation combines a high-level event-driven logic for collision avoidance with low-level feedback control laws for guidance and navigation. As a result, the controller relies only on real-time measurements, removing the need for computationally demanding look-ahead predictions (heuristics) of the AGVs’ motion. It is shown that the proposed method ensures collision- and blockage-free motion of a large number of AGVs. Extensive numerical simulations validate the performance of the motion coordination algorithm.},
  archive      = {J_RAS},
  author       = {Mehmet Ali Guney and Ioannis A. Raptis},
  doi          = {10.1016/j.robot.2020.103534},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103534},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Dynamic prioritized motion coordination of multi-AGV systems},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-stage visual navigation by deep neural networks and
multi-goal reinforcement learning. <em>RAS</em>, <em>138</em>, 103731.
(<a href="https://doi.org/10.1016/j.robot.2021.103731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a two-stage learning framework for visual navigation in which the experience of the agent during exploration of one goal is shared to learn to navigate to other goals. We train a deep neural network for estimating the robot’s position in the environment using ground truth information provided by a classical localization and mapping approach. The second simpler multi-goal Q-function learns to traverse the environment by using the provided discretized map. Transfer learning is applied to the multi-goal Q-function from a maze structure to a 2D simulator and is finally deployed in a 3D simulator where the robot uses the estimated locations from the position estimator deep network. In the experiments, we first compare different architectures to select the best deep network for location estimation, and then compare the effects of the multi-goal reinforcement learning method to traditional reinforcement learning . The results show a significant improvement when multi-goal reinforcement learning is used. Furthermore, the results of the location estimator show that a deep network can learn and generalize in different environments using camera images with high accuracy in both position and orientation.},
  archive      = {J_RAS},
  author       = {Amirhossein Shantia and Rik Timmers and Yiebo Chong and Cornel Kuiper and Francesco Bidoia and Lambert Schomaker and Marco Wiering},
  doi          = {10.1016/j.robot.2021.103731},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103731},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Two-stage visual navigation by deep neural networks and multi-goal reinforcement learning},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning image-based receding horizon planning for
manipulation in clutter. <em>RAS</em>, <em>138</em>, 103730. (<a
href="https://doi.org/10.1016/j.robot.2021.103730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The manipulation of an object into a desired location in a cluttered and restricted environment requires reasoning over the long-term consequences of an action while reacting locally to the multiple physics-based interactions. We present Visual Receding Horizon Planning (VisualRHP) in a framework which interleaves real-world execution with look-ahead planning to efficiently solve a short-horizon approximation to a multi-step sequential decision making problem. VisualRHP is guided by a learned heuristic that acts on an abstract colour-labelled image-based representation of the state. With this representation, the robot can generalize its behaviours to different environment setups, that is, different number and shape of objects, while also having transferable manipulation skills that can be applied to a multitude of real-world objects. We train the heuristic with imitation and reinforcement learning in discrete and continuous actions spaces. We detail our heuristic learning process for environments with sparse rewards, and non-linear, non-continuous, dynamics. In particular, we introduce necessary changes for improving the stability of existing reinforcement learning algorithms that use neural networks with shared parameters. In a series of simulation and real-world experiments, we show the robot performing prehensile and non-prehensile actions in synergy to successfully manipulate a variety of real-world objects in real-time.},
  archive      = {J_RAS},
  author       = {Wissam Bejjani and Matteo Leonetti and Mehmet R. Dogar},
  doi          = {10.1016/j.robot.2021.103730},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103730},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Learning image-based receding horizon planning for manipulation in clutter},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LIDAR based detection of road boundaries using the density
of accumulated point clouds and their gradients. <em>RAS</em>,
<em>138</em>, 103714. (<a
href="https://doi.org/10.1016/j.robot.2020.103714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving and driver assistance require a continuous and reliable perception of the road boundaries, namely curbs and berms, including also other minor, or not so minor, obstacles in the neighborhood of the car. This paper proposes to use a 4-layer LIDAR placed close to the ground to capture measurements of the road ahead of the car and allow the detection of the boundaries. This setup provides a special point of view that allows the accumulation of points on vertical surfaces on the road as the car moves, which increases the point density in vertical surfaces but keeps it limited in horizontal surfaces. This technique allows to successfully distinguish curbs from the flat parts of the road. However, this approach has some limitations, namely to detect berms, and another approach had to be developed using the gradient of point density, which extends the detection capabilities to berms and negative obstacles. This is achieved by flattening the point clouds to 2D and use traditional computer vision gradient and edge detection techniques, which also improves the processing speed. Results are obtained on the ATLASCAR real system, at different velocities, and a good performance is reached when comparing to a manually created ground truth.},
  archive      = {J_RAS},
  author       = {Daniela Rato and Vítor Santos},
  doi          = {10.1016/j.robot.2020.103714},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103714},
  shortjournal = {Robot. Auton. Syst.},
  title        = {LIDAR based detection of road boundaries using the density of accumulated point clouds and their gradients},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-robot goal conflict resolution under communication
constraints using spatial approximation and strategic caching.
<em>RAS</em>, <em>138</em>, 103713. (<a
href="https://doi.org/10.1016/j.robot.2020.103713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of distributed goal conflict resolution in multi-robot systems while remaining resilient to intermittent communication losses between robots. Our proposed approach uses a spatial approximation technique called α α -shape to represent the regions that have been explored by robots followed by a O ( log n ) O(logn) algorithm that incrementally combines and shares the α α -shape information between robots along the robots’ communication tree and rapidly checks for conflicts of a robot’s selected location. We provide theoretical guarantees of the time complexity of our proposed algorithm along with experimental results with simulated and physical robots in different environments. The results show that our approach can rapidly determine conflicts between goal locations selected by multiple robots as well as reduce message loss and re-transmissions between robots. These result in more efficient inter-robot communications as well as less extraneous distance traveled by robots, as compared to a flooding-based communications approach.},
  archive      = {J_RAS},
  author       = {Bradley Woosley and Prithviraj Dasgupta and John G. Rogers III and Jeffery Twigg},
  doi          = {10.1016/j.robot.2020.103713},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103713},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Multi-robot goal conflict resolution under communication constraints using spatial approximation and strategic caching},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The mesh tools package – introducing annotated 3D triangle
maps in ROS. <em>RAS</em>, <em>138</em>, 103688. (<a
href="https://doi.org/10.1016/j.robot.2020.103688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle mesh maps for robotic applications are becoming increasingly popular, but are not yet effectively supported in the Robot Operating System (ROS). We introduce the Mesh Tools package consisting of message definitions, RViz plugins and tools, as well as a persistence layer. These tools make annotated triangle maps available in ROS and allow to publish, edit and inspect such maps within the existing ROS software stack. The persistence layer efficiently loads and stores large mesh maps. The proposed plugins and tools enable the visualization and validation of the complete layered map and associated properties to allow fluid interaction. We demonstrate the seamless integration of our tools in two application areas as a proof-of-concept: Labeling of triangle clusters for semantic mapping and robot navigation on triangle meshes in rough terrain outdoor environments by integrating our tools into an existing navigation stack.},
  archive      = {J_RAS},
  author       = {Sebastian Pütz and Thomas Wiemann and Joachim Hertzberg},
  doi          = {10.1016/j.robot.2020.103688},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103688},
  shortjournal = {Robot. Auton. Syst.},
  title        = {The mesh tools package – introducing annotated 3D triangle maps in ROS},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Line–circle–square (LCS): A multilayered geometric filter
for edge-based detection. <em>RAS</em>, <em>137</em>, 103732. (<a
href="https://doi.org/10.1016/j.robot.2021.103732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a state-of-the-art filter that reduces the complexity in object detection, tracking and mapping applications. Existing edge detection and tracking methods are proposed to create suitable autonomy for mobile robots, however, many of them face overconfidence and large computations at the entrance to scenarios with an immense number of landmarks. The method in this work, the Line–Circle–Square (LCS) filter, claims that mobile robots without a large database for object recognition and highly advanced prediction methods can deal with incoming objects that the camera captures in real-time. The proposed filter applies detection, tracking and learning to each defined expert to extract higher level information for judging scenes without over-calculation. The interactive learning feed between each expert increases the consistency of detected landmarks that works against overwhelming detected features in crowded scenes. Our experts are dependent on trust factors’ covariance under the geometric definitions to ignore, emerge and compare detected landmarks. The experiment validates the effectiveness of the proposed filter in terms of detection precision and resource usage in both experimental and real-world scenarios.},
  archive      = {J_RAS},
  author       = {Seyed Amir Tafrishi and Xiaotian Dai and Vahid Esmaeilzadeh Kandjani},
  doi          = {10.1016/j.robot.2021.103732},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103732},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Line–Circle–Square (LCS): A multilayered geometric filter for edge-based detection},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robotic mobile fulfillment systems: A survey on recent
developments and research opportunities. <em>RAS</em>, <em>137</em>,
103729. (<a href="https://doi.org/10.1016/j.robot.2021.103729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of the autonomous mobile robots applied to Warehouses and the creation of the Robotic Mobile Fulfillment System after the market implementation of the Kiva Robots , it is necessary to carry out a deeper approach of the researches carried out to this date. The objective of this survey is to provide a unified and accessible presentation of the basic concepts of a warehouse system, such as its types, layouts, systems, and methodologies already applied to improve the activities, thus going to the latest research and methodologies focused on the development of new architectures and algorithms in Robotic Mobile Fulfillment Systems (RMFS) . The main contribution of this work is an attempt to present a comprehensive review of recent breakthroughs in the goods-to-person RMFS field, providing links to the most interesting and successful works from the state-of-the-art, but also to provide a presentation and summary of how a Warehouse systems works, in a way that allows future researchers to understand his taxonomies and principles of operation.},
  archive      = {J_RAS},
  author       = {Ítalo Renan da Costa Barros and Tiago Pereira Nascimento},
  doi          = {10.1016/j.robot.2021.103729},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103729},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robotic mobile fulfillment systems: A survey on recent developments and research opportunities},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-UAV trajectory planning using gradient-based sequence
minimal optimization. <em>RAS</em>, <em>137</em>, 103728. (<a
href="https://doi.org/10.1016/j.robot.2021.103728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-UAV system is widely used in surveillance, search and rescue, and industrial inspection. Multi-UAV trajectory planning is crucial for the multi-UAV system, but multi-UAV trajectory planning often needs to consider many constraints, such as trajectory smoothness, obstacle collisions, mutual collisions, dynamic limits, time-consuming, and trajectory length . It is a challenge to balance these constraints while considering computational performance. This paper proposes a novel multi-UAV trajectory planning method to solve the challenge. This method uses time segmentation instead of traditional waypoint segmentation to establish a trajectory optimization model based on the unified time interval, which simplifies the calculation of cost functions. At the same time, virtual segments are introduced to adapt to the trajectory length of different UAVs to reduce the total arrival time. Nonlinear constraints are cast into cost functions and a gradient-based sequential minimal optimization (GB-SMO) algorithm is proposed to minimize the cost function, which decouples the constraint of the mutual collisions in each iteration to save the planning time. Experiments are performed on a multi-UAV system to prove the effectiveness of the proposed method. Results show that this method has good performance in obstacle-rich environments and is efficient for a large number of UAVs.},
  archive      = {J_RAS},
  author       = {Qiaoyang Xia and Shuang Liu and Mingyang Guo and Hui Wang and Qigao Zhou and Xiancheng Zhang},
  doi          = {10.1016/j.robot.2021.103728},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103728},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Multi-UAV trajectory planning using gradient-based sequence minimal optimization},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Particle filter refinement based on clustering procedures
for high-dimensional localization and mapping systems. <em>RAS</em>,
<em>137</em>, 103725. (<a
href="https://doi.org/10.1016/j.robot.2021.103725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing safe autonomous robotic applications for outdoor agricultural environments is a research field that still presents many challenges. Simultaneous Localization and Mapping can be crucial to endow the robot to localize itself with accuracy and, consequently, perform tasks such as crop monitoring and harvesting autonomously. In these environments, the robotic localization and mapping systems usually benefit from the high density of visual features. When using filter-based solutions to localize the robot, such an environment usually uses a high number of particles to perform accurately. These two facts can lead to computationally expensive localization algorithms that are intended to perform in real-time. This work proposes a refinement step to a standard high-dimensional filter-based localization solution through the novelty of downsampling the filter using an online clustering algorithm and applying a scan-match procedure to each cluster. Thus, this approach allows scan-matchers without high computational cost, even in high dimensional filters. Experiments using real data in an agricultural environment show that this approach improves the Particle Filter performance estimating the robot pose. Additionally, results show that this approach can build a precise 3D reconstruction of agricultural environments using visual scans , i.e., 3D scans with RGB information.},
  archive      = {J_RAS},
  author       = {André Silva Aguiar and Filipe Neves dos Santos and Héber Sobreira and José Boaventura Cunha and Armando Jorge Sousa},
  doi          = {10.1016/j.robot.2021.103725},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103725},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Particle filter refinement based on clustering procedures for high-dimensional localization and mapping systems},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian process-based nonlinear predictive control for
visual servoing of constrained mobile robots with unknown dynamics.
<em>RAS</em>, <em>136</em>, 103712. (<a
href="https://doi.org/10.1016/j.robot.2020.103712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a Gaussian process-based nonlinear model predictive control (GP-based NMPC) algorithm is presented to deal with the visual servoing problem for constrained mobile robots. Firstly, a GP-enhanced model is established by incorporating a GP model and a visual servoing kinematic model where the GP-model is used to capture the robot dynamics with on-line updating. Then, a nonlinear model predictive control (NMPC) strategy is proposed to transform the visual servoing task into a nonlinear optimization problem with robot-physical and camera-visibility constraints. Subsequently, a variant iterative linear quadratic regulator algorithm is presented to solve the constrained NMPC problem in real time. Finally, simulations and experiments are conducted to show the effectiveness of the presented method.},
  archive      = {J_RAS},
  author       = {Zhehao Jin and Jinhui Wu and Andong Liu and Wen-An Zhang and Li Yu},
  doi          = {10.1016/j.robot.2020.103712},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103712},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Gaussian process-based nonlinear predictive control for visual servoing of constrained mobile robots with unknown dynamics},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human–robot collaboration in sensorless assembly task
learning enhanced by uncertainties adaptation via bayesian optimization.
<em>RAS</em>, <em>136</em>, 103711. (<a
href="https://doi.org/10.1016/j.robot.2020.103711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots are increasingly exploited in production plants. Within the Industry 4.0 paradigm, the robot complements the human’s capabilities, learning new tasks and adapting itself to compensate for uncertainties. With this aim, the presented paper focuses on the investigation of machine learning techniques to make a sensorless robot able to learn and optimize an industrial assembly task. Relying on sensorless Cartesian impedance control , two main contributions are defined: (1) a task-trajectory learning algorithm based on a few human’s demonstrations (exploiting Hidden Markov Model approach), and (2) an autonomous optimization procedure of the task execution (exploiting Bayesian Optimization). To validate the proposed methodology, an assembly task has been selected as a reference application. The task consists of mounting a gear into its square-section shaft on a fixed base to simulate the assembly of a gearbox. A Franka EMIKA Panda manipulator has been used as a test platform, implementing the proposed methodology. The experiments, carried out on a population of 15 subjects, show the effectiveness of the proposed strategy, making the robot able to learn and optimize its behavior to accomplish the assembly task, even in the presence of task uncertainties.},
  archive      = {J_RAS},
  author       = {Loris Roveda and Mauro Magni and Martina Cantoni and Dario Piga and Giuseppe Bucca},
  doi          = {10.1016/j.robot.2020.103711},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103711},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Human–robot collaboration in sensorless assembly task learning enhanced by uncertainties adaptation via bayesian optimization},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From exploration to control: Learning object manipulation
skills through novelty search and local adaptation. <em>RAS</em>,
<em>136</em>, 103710. (<a
href="https://doi.org/10.1016/j.robot.2020.103710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming a robot to deal with open-ended tasks remains a challenge, in particular if the robot has to manipulate objects. Launching, grasping, pushing or any other object interaction can be simulated but the corresponding models are not reversible and the robot behavior thus cannot be directly deduced. These behaviors are hard to learn without a demonstration as the search space is large and the reward sparse. We propose a method to autonomously generate a diverse repertoire of simple object interaction behaviors in simulation. Our goal is to bootstrap a robot learning and development process with limited information about what the robot has to achieve and how. This repertoire can be exploited to solve different tasks in reality thanks to a proposed adaptation method or could be used as a training set for data-hungry algorithms. The proposed approach relies on the definition of a goal space and generates a repertoire of trajectories to reach attainable goals, thus allowing the robot to control this goal space. The repertoire is built with an off-the-shelf simulation thanks to a quality–diversity algorithm. The result is a set of solutions tested in simulation only. It may result in two different problems: (1) as the repertoire is discrete and finite, it may not contain the trajectory to deal with a given situation or (2) some trajectories may lead to a behavior in reality that differs from simulation because of a reality gap. We propose an approach to deal with both issues by using a local linearization of the mapping between the motion parameters and the observed effects. Furthermore, we present an approach to update the existing solutions repertoire with the tests done on the real robot. The approach has been validated on two different experiments on the Baxter robot: a ball launching and a joystick manipulation tasks.},
  archive      = {J_RAS},
  author       = {Seungsu Kim and Alexandre Coninx and Stephane Doncieux},
  doi          = {10.1016/j.robot.2020.103710},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103710},
  shortjournal = {Robot. Auton. Syst.},
  title        = {From exploration to control: Learning object manipulation skills through novelty search and local adaptation},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Long-term vehicle localization in urban environments based
on pole landmarks extracted from 3-d lidar scans. <em>RAS</em>,
<em>136</em>, 103709. (<a
href="https://doi.org/10.1016/j.robot.2020.103709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation (Schaefer and Büscher, 0000). In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15 months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.},
  archive      = {J_RAS},
  author       = {Alexander Schaefer and Daniel Büscher and Johan Vertens and Lukas Luft and Wolfram Burgard},
  doi          = {10.1016/j.robot.2020.103709},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103709},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Long-term vehicle localization in urban environments based on pole landmarks extracted from 3-D lidar scans},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bootstrapped neuro-simulation for complex robots.
<em>RAS</em>, <em>136</em>, 103708. (<a
href="https://doi.org/10.1016/j.robot.2020.103708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic simulators are often used to speed up the Evolutionary Robotics (ER) process. Most simulation approaches are based on physics modelling. However, physics-based simulators can become complex to develop and require prior knowledge of the robotic system . Robotics simulators can be constructed using Machine Learning techniques , such as Artificial Neural Networks (ANNs). ANN-based simulator development usually requires a lengthy behavioural data collection period before the simulator can be trained and used to evaluate controllers during the ER process. The Bootstrapped Neuro-Simulation (BNS) approach can be used to simultaneously collect behavioural data, train an ANN-based simulator and evolve controllers for a particular robotic problem. This paper investigates proposed improvements to the BNS approach and demonstrates the viability of the approach by optimising gait controllers for a Hexapod and Snake robot platform.},
  archive      = {J_RAS},
  author       = {Grant W. Woodford and Mathys C. du Plessis},
  doi          = {10.1016/j.robot.2020.103708},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103708},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Bootstrapped neuro-simulation for complex robots},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Driver identification using only the CAN-bus vehicle data
through an RCN deep learning approach. <em>RAS</em>, <em>136</em>,
103707. (<a href="https://doi.org/10.1016/j.robot.2020.103707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent years, many studies claim that humans have a unique driving behavior style that could be used as a fingerprint in recognizing the identity of the driver. With the rising evolution of Machine Learning (ML), the research efforts aiming to take advantage of the human driving style identifiers have been increasing exponentially. For Advanced Driver Assistance Systems (ADAS), this attribute can be an efficient factor to ensure the security and protection of the vehicle. Additionally, it extends the ADAS capabilities by creating different profiles for the drivers, which helps every driver according to his own driving style and improve the ADAS fidelity. Nonetheless, certain problems in the unpredictability of human behavior and the effectiveness of capturing the temporal features of the signal represented an ongoing challenge to accomplish driver identification. In this paper, we propose a novel deep learning approach to driver identification based on a Residual Convolutional Network (RCN). This approach outperforms the existing state of the art methods in less than two hours of training, while simultaneously achieving 99.3\% accuracy. The used data are exclusively provided by the Controller Area Network (CAN-Bus) vehicle data that eliminates any privacy invading concerns from the user.},
  archive      = {J_RAS},
  author       = {N. Abdennour and T. Ouni and N. Ben Amor},
  doi          = {10.1016/j.robot.2020.103707},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103707},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Driver identification using only the CAN-bus vehicle data through an RCN deep learning approach},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate autonomous navigation strategy dedicated to the
storage of buses in a bus center. <em>RAS</em>, <em>136</em>, 103706.
(<a href="https://doi.org/10.1016/j.robot.2020.103706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with an innovative autonomous bus navigation and parking system in a bus depot, in order to optimize their movements in a confined area. The kinematic model of the vehicle is defined. Considering its dimensions and weight as well as the centimetric accuracy required, a predictive controller is designed, based on its model linearized around the changing path curvature value, to perform accurate curved paths tracking with a limited tracking error guaranteed by the consideration of a constraint. This controller and additional sliding observers are designed according to the distance traveled, allowing maneuvers to be performed at any forward or backward speed with constant accuracy. In addition, these observers are not affected by path tracking errors. The implementation on an industrial vehicle, operated under realistic conditions, demonstrates the performance and robustness of this navigation system .},
  archive      = {J_RAS},
  author       = {Eric Lucet and Alain Micaelli and François-Xavier Russotto},
  doi          = {10.1016/j.robot.2020.103706},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103706},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Accurate autonomous navigation strategy dedicated to the storage of buses in a bus center},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Directional optimal reciprocal collision avoidance.
<em>RAS</em>, <em>136</em>, 103705. (<a
href="https://doi.org/10.1016/j.robot.2020.103705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A great amount of effort has been devoted to the study on self-separation assurance approach for civil aviation in the airspace with increasing density. In this article, the Optimal Reciprocal Collision Avoidance (ORCA) algorithm is modified to make it work for autonomous and decentralized collision avoidance for civil aircraft. Without considering the direction selectivity of collision-free maneuver, aircraft may select the relative parallel trajectories by deploying the ORCA algorithm in both decentralized and centralized way. As a result, the collision tends to be postponed to the next time horizon because civil aircraft need to return to original trajectories. Simultaneously, the unified rules can hardly be integrated into the approach due to the lack of direction selectivity for collision-free navigation. The process of separation assurance will be disorderly when multiple aircraft are involved. To solve the problem mentioned above, a new algorithm called Directional Optimal Reciprocal Collision Avoidance (DORCA) is proposed. The DORCA algorithm employs a vector rotation mode to construct the forbidden Velocity Obstacle (VO) set in order to improve the computation efficiency. In addition, the direction selectivity of maneuver is achieved through constructing the direction-constrained VO set according to the direction of relative motion in velocity space. Direction selectivity of the algorithm enables the process of collision avoidance to comply with the unified rules. A number of encounter scenarios are conducted to confirm the validity and feasibility of the proposed DORCA algorithm. In all scenarios tested, the direction selectivity of collision-free maneuver can be successfully integrated into the DORCA algorithm, and the algorithm is more efficient than the ORCA algorithm for collision avoidance in decentralized way.},
  archive      = {J_RAS},
  author       = {Haotian Niu and Cunbao Ma and Pei Han},
  doi          = {10.1016/j.robot.2020.103705},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103705},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Directional optimal reciprocal collision avoidance},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Jumping over obstacles with MIT cheetah 2. <em>RAS</em>,
<em>136</em>, 103703. (<a
href="https://doi.org/10.1016/j.robot.2020.103703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a planning framework for jumping over obstacles with quadruped robots . The framework accomplishes planning via a structured predictive control strategy that combines the use of heterogeneous simplified models over different prediction time scales. A receding multi-horizon predictive controller coordinates the approach before the jump using a kinematic point-mass model. Consideration of the optimal value function over different planning horizons enables the system to select an appropriate number of steps to take before jumping. The jumping motion is then tailored to the sensed obstacle by solving a nonlinear trajectory optimization problem . The solution of this problem online is enabled by exploiting the analyticity of the flow map for a planar bounding template model under polynomial inputs. By planning with this combination of models, MIT Cheetah 2 is shown to autonomously jump over obstacles up to 40 cm in height during high-speed bounding. Untethered results showcase the ability of the method to automatically adapt to obstacles of different heights and placements in a single trial.},
  archive      = {J_RAS},
  author       = {Hae-Won Park and Patrick M. Wensing and Sangbae Kim},
  doi          = {10.1016/j.robot.2020.103703},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103703},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Jumping over obstacles with MIT cheetah 2},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An alternative approach for robot localization inside pipes
using RF spatial fadings. <em>RAS</em>, <em>136</em>, 103702. (<a
href="https://doi.org/10.1016/j.robot.2020.103702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate robot localization represents a challenge inside pipes due to the particular conditions that characterize this type of environment. Outdoor techniques (GPS in particular) do not work at all inside metal pipes, while traditional indoor localization methods based on camera or laser sensors do not perform well mainly due to a lack of external illumination and distinctive features along pipes. Moreover, humidity and slippery surfaces make wheel odometry unreliable. In this paper, we estimate the localization of a robot along a pipe with an alternative Radio Frequency (RF) approach. We first analyze wireless propagation in metallic pipes and propose a series of setups that allow us to obtain periodic RF spatial fadings (a sort of standing wave periodic pattern), together with the influence of the antenna position and orientation over these fadings. Subsequently, we propose a discrete RF odometry-like method, by means of counting the fadings while traversing them. The transversal fading analysis (number of antennas and cross-section position) makes it possible to increase the resolution of this method. Lastly, the model of the signal is used in a continuous approach serving as an RF map. The proposed localization methods outperform our previous contributions in terms of resolution, accuracy, reliability and robustness. Experimental results demonstrate the effectiveness of the RF-based strategy without the need for a previously known map of the scenario or any substantial modification of the existing infrastructure.},
  archive      = {J_RAS},
  author       = {Carlos Rizzo and Teresa Seco and Jesús Espelosín and Francisco Lera and José Luis Villarroel},
  doi          = {10.1016/j.robot.2020.103702},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103702},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An alternative approach for robot localization inside pipes using RF spatial fadings},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On deep learning techniques to boost monocular depth
estimation for autonomous navigation. <em>RAS</em>, <em>136</em>,
103701. (<a href="https://doi.org/10.1016/j.robot.2020.103701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation . We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.},
  archive      = {J_RAS},
  author       = {Raul de Queiroz Mendes and Eduardo Godinho Ribeiro and Nicolas dos Santos Rosa and Valdir Grassi Jr.},
  doi          = {10.1016/j.robot.2020.103701},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103701},
  shortjournal = {Robot. Auton. Syst.},
  title        = {On deep learning techniques to boost monocular depth estimation for autonomous navigation},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning dynamical systems with bifurcations. <em>RAS</em>,
<em>136</em>, 103700. (<a
href="https://doi.org/10.1016/j.robot.2020.103700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory planning through dynamical systems (DS) provides robust control for robots and has found numerous applications from locomotion to manipulation. However, to date, DS for controlling rhythmic patterns are distinct from DS used to control point to point motion and current approaches switch at run time across these to enable multiple behaviors. This switching can be brittle and subject to instabilities. We present an approach to embed cyclic and point to point dynamics in a single DS. We offer a method to learn the parameters of complete DS through a two-step optimization. By exploiting Hopf bifurcations , we can explicitly and smoothly transit across periodic and non-periodic phases, linear and nonlinear limit cycles, and non-periodic phases, in addition to changing the equilibrium’s location and the limit cycle’s amplitude. We use diffeomorphism and learn a mapping to modify the learned limit cycle to generate nonlinear limit cycles. The approach is validated with a real 7 DOF KUKA LWR 4+ manipulator to control wiping and with a humanoid robot in simulation.},
  archive      = {J_RAS},
  author       = {Farshad Khadivar and Ilaria Lauzana and Aude Billard},
  doi          = {10.1016/j.robot.2020.103700},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103700},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Learning dynamical systems with bifurcations},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LoOP: Iterative learning for optimistic planning on robots.
<em>RAS</em>, <em>136</em>, 103693. (<a
href="https://doi.org/10.1016/j.robot.2020.103693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient robotic behaviors require robustness and adaptation to dynamic changes of the environment, whose characteristics rapidly vary during robot operation. To generate effective robot action policies, planning and learning techniques have shown the most promising results. However, if considered individually, they present different limitations. Planning techniques lack generalization among similar states and require experts to define behavioral routines at different levels of abstraction. Conversely, learning methods usually require a considerable number of training samples and iterations of the algorithm. To overcome these issues, and to efficiently generate robot behaviors, we introduce LoOP , an iterative learning algorithm for optimistic planning that combines state-of-the-art planning and learning techniques to generate action policies. The main contribution of LoOP is the combination of Monte-Carlo Search Planning and Q-learning, which enables focused exploration during policy refinement in different robotic applications . We demonstrate the robustness and flexibility of LoOP in various domains and multiple robotic platforms , by validating the proposed approach with an extensive experimental evaluation.},
  archive      = {J_RAS},
  author       = {Francesco Riccio and Roberto Capobianco and Daniele Nardi},
  doi          = {10.1016/j.robot.2020.103693},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103693},
  shortjournal = {Robot. Auton. Syst.},
  title        = {LoOP: Iterative learning for optimistic planning on robots},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot gaining accurate pouring skills through
self-supervised learning and generalization. <em>RAS</em>, <em>136</em>,
103692. (<a href="https://doi.org/10.1016/j.robot.2020.103692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pouring is one of the most commonly executed tasks in humans’ daily lives, whose accuracy is affected by multiple factors, including the type of material to be poured and the geometry of the source and receiving containers. In this work, we propose a self-supervised learning approach that learns the pouring dynamics, pouring motion, and outcomes from unsupervised demonstrations for accurate pouring. The learned pouring model is then generalized by self-supervised practicing to different conditions such as using unaccustomed pouring cups. We have evaluated the proposed approach first with one container from the training set and four new but similar containers. The proposed approach achieved better pouring accuracy than a regular human with a similar pouring speed for all five cups. Both the accuracy and pouring speed outperform state-of-the-art works. We have also evaluated the proposed self-supervised generalization approach using unaccustomed containers that are far different from the ones in the training set. The self-supervised generalization reduces the pouring error of the unaccustomed containers to the desired accuracy level.},
  archive      = {J_RAS},
  author       = {Yongqiang Huang and Juan Wilches and Yu Sun},
  doi          = {10.1016/j.robot.2020.103692},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103692},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robot gaining accurate pouring skills through self-supervised learning and generalization},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Informative path planner with exploration–exploitation
trade-off for radiological surveys in non-convex scenarios.
<em>RAS</em>, <em>136</em>, 103691. (<a
href="https://doi.org/10.1016/j.robot.2020.103691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The risk toward human lives in situations involving chemical, biological, radiological, and nuclear (CBRN) threats can be mitigated or even neutralized by deploying carrying a suite of suitable sensors. Furthermore, mobile robots open up the possibility for automated radiological field surveys and monitoring operations, which have important applications in scenarios with CBRN threats. A path planner is one of the essential tools required for these robots to perform their tasks autonomously. Moreover, sophisticated path planners can greatly increase the efficiency of monitoring tasks by maximizing the information gathered in the minimum amount of time. This work proposes an informative path planner as an instrument to efficiently estimate maps of scalar quantities (e.g., radiation intensity, chemical concentration), motivated by applications in radiological inspection. The proposed path planner models the path with B-splines, enabling planning in continuous space. A Gaussian Process with a squared exponential kernel is used to model the underlying field. A modified form of mutual information, estimated from the Gaussian Process , is maximized to determine the most informative path, additionally rewarding observations made in regions where the field magnitude is large (e.g., near a radioactive source). A maximum likelihood estimator for source parameters is used to demonstrate that the proposed solution increases the accuracy of the estimated source positions. Simulation results show that the informative path planner adapts to non-convex environments and increases the number of observations made close to radioactive sources while avoiding obstacles.},
  archive      = {J_RAS},
  author       = {Yoeri Brouwer and Alberto Vale and Rodrigo Ventura},
  doi          = {10.1016/j.robot.2020.103691},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103691},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Informative path planner with exploration–exploitation trade-off for radiological surveys in non-convex scenarios},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed output feedback nonlinear h∞ formation control
algorithm for heterogeneous aerial robotic teams. <em>RAS</em>,
<em>136</em>, 103689. (<a
href="https://doi.org/10.1016/j.robot.2020.103689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the formation flying control problem for a team of nonlinear uncertain quadrotors in presence of noisy measurements and environmental disturbances . A novel distributed output-feedback nonlinear robust algorithm is proposed to solve the problem. The algorithm leads to a series of combined estimation-control local policies with minimum communicated information by decomposing the global network to local star networks. An analytical study establishes the stability of the closed-loop system and the Monte-Carlo simulation demonstrates the robust performance and boundedness of the outputs numerically. The Software In the Loop (SIL) testing is performed utilizing Pixhawk open source flight management unit, Raspberry-pi 3 and GAZEBO simulation environment to reveal the effectiveness of the proposed distributed control-estimation algorithm for practical implementation.},
  archive      = {J_RAS},
  author       = {Fatemeh Rekabi and Farzad A. Shirazi and Mohammad Jafar Sadigh and Mahmood Saadat},
  doi          = {10.1016/j.robot.2020.103689},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103689},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Distributed output feedback nonlinear h∞ formation control algorithm for heterogeneous aerial robotic teams},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to see through the haze: Multi-sensor
learning-fusion system for vulnerable traffic participant detection in
fog. <em>RAS</em>, <em>136</em>, 103687. (<a
href="https://doi.org/10.1016/j.robot.2020.103687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an experimental investigation of a multi-sensor fusion-learning system for detecting pedestrians in foggy weather conditions. The method combines two pipelines for people detection running on two different sensors commonly found on moving vehicles: lidar and radar. The two pipelines are not only combined by sensor fusion, but information from one pipeline is used to train the other. We build upon our previous work, where we showed that a lidar pipeline can be used to train a Support Vector Machine (SVM)-based pipeline to interpret radar data, which is useful when conditions then become unfavourable to the original lidar pipeline. In this paper, we test the method on a wider range of conditions, such as from a moving vehicle, and with multiple people present. Additionally, we also compare how the traditional SVM performs interpreting the radar data versus a modern deep neural network on these experiments. Our experiments indicate that either of the approaches results in progressive improvement in the performance during normal operation. Further, our experiments indicate that in the event of the loss of information from a sensor, pedestrian detection and position estimation is still effective.},
  archive      = {J_RAS},
  author       = {George Broughton and Filip Majer and Tomáš Rouček and Yassine Ruichek and Zhi Yan and Tomáš Krajník},
  doi          = {10.1016/j.robot.2020.103687},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103687},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Learning to see through the haze: Multi-sensor learning-fusion system for vulnerable traffic participant detection in fog},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparative look at two formation control approaches based
on optimization and algebraic graph theory. <em>RAS</em>, <em>136</em>,
103686. (<a href="https://doi.org/10.1016/j.robot.2020.103686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper takes a novel look at formation control by comparing control setups based on two very different frameworks. These are applied to the distributed control of communicating omnidirectional mobile robots. One framework, which is possibly the most common approach to formation control , is based on algebraic graph theory, whereas the other, namely distributed model predictive control (DMPC), is based on distributed optimization , representing a rather uncommon view on the task. In this study, formation control is understood as the task of attaining and maintaining a specific relative positioning between robotic agents while moving the formation through the environment. While interesting on its own, formation control can serve as the basis for superordinate tasks like cooperative transportation. For an encompassing treatment of the task, two different control goals are considered, resulting in different setups for each control framework. One goal consists of moving the formation’s geometric center to a specific position, whereas the other aims at letting the whole formation move with the desired velocity. In both cases, the involved robots are subject to input constraints. Already during control design, some qualitative differences between the two frameworks become apparent, with the DMPC controller exhibiting characteristic beneficial qualities in exchange for its higher computational demand. Results from various simulation scenarios confirm these observations. Considerations on the practical implementation of the two schemes, as well as hardware experiments with tailor-made mobile robots, provide valuable insight for robotics practitioners, and highlight the applicability of the two frameworks.},
  archive      = {J_RAS},
  author       = {Henrik Ebel and Peter Eberhard},
  doi          = {10.1016/j.robot.2020.103686},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103686},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A comparative look at two formation control approaches based on optimization and algebraic graph theory},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot skill learning in latent space of a deep autoencoder
neural network. <em>RAS</em>, <em>135</em>, 103690. (<a
href="https://doi.org/10.1016/j.robot.2020.103690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just like humans, robots can improve their performance by practicing, i. e. by performing the desired behavior many times and updating the underlying skill representation using the newly gathered data. In this paper, we propose to implement robot practicing by applying statistical and reinforcement learning (RL) in a latent space of the selected skill representation. The latent space is computed by a deep autoencoder neural network , with the data to train the network generated in simulation. However, we show that the resulting latent space representation is useful also for learning on a real robot. Our simulation and real-world results demonstrate that by exploiting the latent space of the underlying motor skill representation, a significant reduction of the amount of data needed for effective learning by Gaussian Process Regression (GPR) can be achieved. Similarly, the number of RL epochs can be significantly reduced. Finally, it is evident from our results that an autoencoder-based latent space is more effective for these purposes than a latent space computed by principal component analysis.},
  archive      = {J_RAS},
  author       = {Rok Pahič and Zvezdan Lončarević and Andrej Gams and Aleš Ude},
  doi          = {10.1016/j.robot.2020.103690},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103690},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robot skill learning in latent space of a deep autoencoder neural network},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Change detection using weighted features for image-based
localization. <em>RAS</em>, <em>135</em>, 103676. (<a
href="https://doi.org/10.1016/j.robot.2020.103676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous mobile robots are becoming increasingly important in many industrial and domestic environments. Dealing with unforeseen situations is a difficult problem that must be tackled to achieve long-term robot autonomy. In vision-based localization and navigation methods, one of the major issues is the scene dynamics. The autonomous operation of the robot may become unreliable if the changes occurring in dynamic environments are not detected and managed. Moving chairs, opening and closing doors or windows, replacing objects and other changes make many conventional methods fail. To deal with these challenges, we present a novel method for change detection based on weighted local visual features. The core idea of the algorithm is to distinguish the valuable information in stable regions of the scene from the potentially misleading information in the regions that are changing. We evaluate the change detection algorithm in a visual localization framework based on feature matching by performing a series of long-term localization experiments in various real-world environments. The results show that the change detection method yields an improvement in the localization accuracy , compared to the baseline method without change detection. In addition, an experimental evaluation on a public long-term localization data set with more than 10 000 images reveals that the proposed method outperforms two alternative localization methods on images recorded several months after the initial mapping.},
  archive      = {J_RAS},
  author       = {Erik Derner and Clara Gomez and Alejandra C. Hernandez and Ramon Barber and Robert Babuška},
  doi          = {10.1016/j.robot.2020.103676},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103676},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Change detection using weighted features for image-based localization},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamics evaluation of 2UPU/SP parallel mechanism for a
5-DOF hybrid robot considering gravity. <em>RAS</em>, <em>135</em>,
103675. (<a href="https://doi.org/10.1016/j.robot.2020.103675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamics performance is very important for a manipulator used for high-speed machining. In this paper, the dynamic performance evaluation method of the 2UPU/SP parallel mechanism in a hybrid robot for aerospace composite machining is studied. The dynamic model is obtained by the virtual work principle, and a dynamic performance index considering gravity is proposed. Based on the given performance index, the effect of placement direction on dynamic performance of 2UPU/SP mechanism is studied, and the comparison between the dynamic performance of 2UPU/SP and the traditional Tricept mechanism is carried out. The results show that the 2UPU/SP mechanism has better dynamic performance in the vertical placement than the horizontal placement, and 2UPU/SP mechanism has better dynamic performance than Tricept mechanism.},
  archive      = {J_RAS},
  author       = {Xiaojian Wang and Jun Wu and Yutian Wang},
  doi          = {10.1016/j.robot.2020.103675},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103675},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Dynamics evaluation of 2UPU/SP parallel mechanism for a 5-DOF hybrid robot considering gravity},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mathematical approach for the design configuration of
magnetic system with multiple electromagnets. <em>RAS</em>,
<em>135</em>, 103674. (<a
href="https://doi.org/10.1016/j.robot.2020.103674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic actuation techniques and microrobots have attracted great interest since they have potential in biomedicine applications. Interventional techniques have emerged as a tool to handle a wide range of minimally invasive surgeries (MIS). However, current MIS procedures are constrained by the limitation of manual operation by surgeon. Thus, various microrobotic solutions including magnetic navigation systems have been proposed for MIS, which carries many potential benefits such as reduced incision, less intraoperative hemorrhaging and postoperative pain, and faster recovery time. In recent decades, many electromagnetic actuation (EMA) systems have been reported and involved to general surgery. The EMA system allows to generate efficiently magnetic source for microrobot control when its specifications are further investigated and satisfied for the desired application. To precisely manipulate the biomedical microrobot, a key issue still relies on the design of a suitable EMA platform. In this paper, we demonstrate a mathematical approach for the design configuration of magnetic system with multiple electromagnets. Especially, the required magnetic coil number has been investigated where the heading motion control, magnetic force control and their combination control are discussed respectively. The singular cases of control are pre-evaluated by a mathematical analysis of the simulated electromagnetic field. In addition, the placed positions and tilted orientations of the applied electromagnets are investigated for the optimization regarding the six typical configurations of EMA platform with 4, 6 and 8 coils. The various configurations of EMA systems have been comprehensively analyzed. Therefore, with the number of electromagnets and their optimal configuration obtained by the proposed approach, the EMA system can be initially established.},
  archive      = {J_RAS},
  author       = {Ruipeng Chen and David Folio and Antoine Ferreira},
  doi          = {10.1016/j.robot.2020.103674},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103674},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Mathematical approach for the design configuration of magnetic system with multiple electromagnets},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel UAV path planning algorithm to search for floating
objects on the ocean surface based on object’s trajectory prediction by
regression. <em>RAS</em>, <em>135</em>, 103673. (<a
href="https://doi.org/10.1016/j.robot.2020.103673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Search and find mission in ocean environment is a none trivial operation given the amount of random parameters associated with it. The uncertain and dynamic aspects related to ocean current movement make the trajectory prediction of drifting lost object onto sea water a very complicated task. In this work we present a novel lost target searching algorithm based on Recursive Area Clustering and target trajectory predication in ocean environment. Based on the widely known GlobCurrent v2 dataset which model the drifting of ocean surface current using satellite sensory data combined with mathematical and simulation modeling, we propose a regression algorithm based on our Recursive Area Clustering algorithm that we have developed previously to determine the strategic zones (weight centers) characterizing the high density areas extracted from drifting target history. Given those weight centers, we predict the object trajectory through refined regression. The predicted lost object trajectory is used to plan the path of UAV search mission. The model developed has a significant impact as we have tested our strategy in a scenario for searching an area covering 68517 km 2 2 , we have shown that 78\% of the time, the lost object can be found within 32 km distance of the predicted trajectories limiting the significant search area to be about 5\% of the whole searched area.},
  archive      = {J_RAS},
  author       = {Mehrez Boulares and Ahmed Barnawi},
  doi          = {10.1016/j.robot.2020.103673},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103673},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A novel UAV path planning algorithm to search for floating objects on the ocean surface based on object’s trajectory prediction by regression},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-fingered grasping force optimization based on
generalized penalty-function concepts. <em>RAS</em>, <em>135</em>,
103672. (<a href="https://doi.org/10.1016/j.robot.2020.103672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an efficient multi-fingered grasping force optimization (GFO) method based on generalized penalty-function concepts. In view of the fact that the mainstream multi-fingered GFO method often treats the second-order cone programming (SOCP) problem as a semi-definite programming (SDP) problem, whose computational complexity is high, we hereby use the barrier function to construct the regularized optimization problem . The trade-off representation of different dimension objective functions is given, and the penalty factor is introduced to form the augmented optimization objective function. For specific operational tasks, by adjusting the penalty factor, a more compact, stable or slack, flexible grasping scheme could be obtained. Monte Carlo simulation is used to determine the probability of successful grasping when variability is introduced, and the robustness of the proposed method in the change of contact position and the friction coefficient between hand and object is verified. Experimental results and dynamic simulation are given, which show that the proposed algorithm outperforms the mainstream SDP method in execution time and iteration number , and the obtained force distribution has both continuity and distribution. Operational flexibility is instructive for practical applications.},
  archive      = {J_RAS},
  author       = {Zhong Chen and Qisen Wu and Cao Hong and Xianmin Zhang},
  doi          = {10.1016/j.robot.2020.103672},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103672},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Multi-fingered grasping force optimization based on generalized penalty-function concepts},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An innovative bio-inspired flight controller for quad-rotor
drones: Quad-rotor drone learning to fly using reinforcement learning.
<em>RAS</em>, <em>135</em>, 103671. (<a
href="https://doi.org/10.1016/j.robot.2020.103671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animals learn to master their capabilities by trial and error, and with out having any knowledge about their dynamics model and mathematical or physical rules. They use their maximum capabilities in an optimized way. This is the result of millions of years of evolution where the best of different possibilities are kept, and makes us rethink How does the nature perform things? , particularly when natural systems outperform our rigid systems. In this study, inspired by the nature, we developed an innovative algorithm by enhancing an existing reinforcement learning algorithm (proximal policy optimization (PPO)). Our algorithm is capable of learning to control a quad-rotor drone in order to fly. This new algorithm called Bio-inspired Flight Controller (BFC) does not use any conventional controller such as PID or MPC to control the quad-rotor drone. The goal of BFC is to completely replace the conventional controller with a controller that acts in a similar way to the animals where they learn to control their movements. It is capable of stabilizing a quad-copter in a desired point, and following way points. We implemented our algorithm in an AscTec Hummingbird quad-copter simulated in Gazebo, and tested it using different scenarios to fully measure its capabilities.},
  archive      = {J_RAS},
  author       = {Amir Ramezani Dooraki and Deok-Jin Lee},
  doi          = {10.1016/j.robot.2020.103671},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103671},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An innovative bio-inspired flight controller for quad-rotor drones: Quad-rotor drone learning to fly using reinforcement learning},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative step-climbing strategy using an autonomous
wheelchair and a robot. <em>RAS</em>, <em>135</em>, 103670. (<a
href="https://doi.org/10.1016/j.robot.2020.103670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This report describes an automatic control system that allows an assistive robot pushing a wheelchair to climb steps. The robot is equipped with a wheeled mechanism and dual manipulators. The wheelchair is a commercially available model that has been equipped with sensors, circuits, and batteries . The robot and wheelchair are connected when the vehicles climb a step. In that operation, the front wheels of the wheelchair are lifted and placed on the step using the velocity differences between the wheelchair and the robot. Next, when the rear wheels of the wheelchair ascend the step, the robot imitates the upper arm motions of a human pushing against his/her chest that commonly occurs when maneuvering a wheelchair up a step. Similarly, the front wheels of the robot are lifted and placed on the step using the velocity differences between the vehicles and the robot’s front wheels. After that, with the assistance of the wheelchair, the other wheels of the robot climb onto the step. In an effort to ensure safety, we also performed a theoretical analysis to determine the most suitable distance for lifting the front wheels of the robot when approaching and climbing a step. Our newly developed cooperative step-climbing system makes it possible to eliminate the complicated operations that were required by previous methods and can also prevent collisions between the wheelchair’s front wheels and the step, thus drastically improving the convenience of the operation. The test subject riding the wheelchair was an able-bodied male, and the experiment conducted to evaluate our system was performed on a 120 mm step height that had a friction coefficient of 0.72. This setup was sufficient for demonstrating the overall effectiveness of our system.},
  archive      = {J_RAS},
  author       = {Hidetoshi Ikeda and Takafumi Toyama and Daisuke Maki and Keisuke Sato and Eiji Nakano},
  doi          = {10.1016/j.robot.2020.103670},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103670},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Cooperative step-climbing strategy using an autonomous wheelchair and a robot},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning compliant robotic movements based on biomimetic
motor adaptation. <em>RAS</em>, <em>135</em>, 103668. (<a
href="https://doi.org/10.1016/j.robot.2020.103668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is one of the great challenges for a robot to learn compliant movements in interaction tasks. The robot can easily acquire motion skills from a human tutor by kinematics demonstration, however, this becomes much more difficult when it comes to the compliant skills. This paper aims to provide a possible solution to address this problem by proposing a two-stage approach. In the first stage, the human tutor demonstrates the robot how to perform a task, during which only motion trajectories are recorded without the involvement of force sensing. A dynamical movement primitives (DMPs) model which can generate human-like motion is then used to encode the kinematics data. In the second stage, a biomimetic controller, which is inspired by the neuroscience findings in human motor learning, is employed to obtain the desired robotic compliant behaviors by online adapting the impedance profiles and the feedforward torques simultaneously. Several tests are conducted to validate the effectiveness of the proposed approach.},
  archive      = {J_RAS},
  author       = {Chao Zeng and Xiongjun Chen and Ning Wang and Chenguang Yang},
  doi          = {10.1016/j.robot.2020.103668},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103668},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Learning compliant robotic movements based on biomimetic motor adaptation},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review on absolute visual localization for UAV.
<em>RAS</em>, <em>135</em>, 103666. (<a
href="https://doi.org/10.1016/j.robot.2020.103666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on unmanned aerial vehicles is growing as they are becoming less expensive and more available than before. The applications span a large number of areas and include border security, search and rescue, wildlife surveying, firefighting, precision agriculture, structure inspection, surveying and mapping, aerial photography, and recreative applications. These applications can require autonomous behavior which is only possible with a precise and robust self-localization. Until recently, the favored approach to localization was based on inertial sensors and global navigation satellite systems. However, global navigation satellite systems have multiple shortcomings related to long-distance radio communications (e.g. non-line-of-sight reception, multipath, spoofing). This motivated the development of new approaches to supplement or supplant satellite navigation. Absolute visual localization is one of the two main approaches to vision-based localization. The goal is to locate the current view of the UAV in a reference satellite map or georeferenced imagery from previous flights. Various approaches were proposed in this area and this paper review most of the literature in this field since 2015. The problematic at hand is analyzed and defined. Existing approaches are reviewed in 4 categories: template matching , feature points matching, deep learning and visual odometry .},
  archive      = {J_RAS},
  author       = {Andy Couturier and Moulay A. Akhloufi},
  doi          = {10.1016/j.robot.2020.103666},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103666},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A review on absolute visual localization for UAV},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time person orientation estimation and tracking using
colored point clouds. <em>RAS</em>, <em>135</em>, 103665. (<a
href="https://doi.org/10.1016/j.robot.2020.103665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustly estimating the orientations of people is a crucial precondition for a wide range of applications. Especially for autonomous systems operating in populated environments, the orientation of a person can give valuable information to increase their acceptance. Given people’s orientations, mobile systems can apply navigation strategies which take people’s proxemics into account or approach them in a human like manner to perform human robot interaction (HRI) tasks. In this paper, we present an approach for person orientation estimation based on computationally efficient features extracted from colored point clouds, formerly used for a two-class person attribute classification. The classification approach has been extended to the continuous domain while treating the problem of orientation estimation in real time. Furthermore, we present an approach for tracking estimated orientations over time using a Bayesian filter. We will show that tracking can increase the accuracy of orientations by up to 3 . 69 ° 3.69° on a dataset recorded with a mobile robot. Best results on this highly challenging dataset are achieved with a regression approach for orientation estimation in combination with tracking. The mean angular error of just 16 . 49 ° 16.49° proofs the applicability in real-world scenarios.},
  archive      = {J_RAS},
  author       = {Tim Wengefeld and Benjamin Lewandowski and Daniel Seichter and Lennard Pfennig and Steffen Müller and Horst-Michael Gross},
  doi          = {10.1016/j.robot.2020.103665},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103665},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Real-time person orientation estimation and tracking using colored point clouds},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skill learning for robotic assembly based on visual
perspectives and force sensing. <em>RAS</em>, <em>135</em>, 103651. (<a
href="https://doi.org/10.1016/j.robot.2020.103651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An environment cannot be effectively described with a single perception form in skill learning for robotic assembly . The visual perception may provide the object’s apparent characteristics and the softness or stiffness of the object could be detected using the contact force/torque information during the assembly process. In the process of inserting assembly strategy learning, most of the work takes the contact force information as the current observation state of the assembly process, ignoring the influence of visual information on the assembly state. This paper proposes robotic assembly skill learning with deep Q-learning using visual perspectives and force sensing to learn an assembly policy. The reward system is designed with an image template matching for assembly state, which is used to judge whether the process is completed successfully. The observations of assembly state are described by force/torque information and the pose of the end effector . To evaluate the performance of the proposed skill learning method, experiments with a KUKA iiwa robot are performed for a plastic fasten assembly in a low-voltage apparatus. The results indicate that the robot can complete the plastic fasten assembly using the learned inserting assembly strategy with visual perspectives and force sensing.},
  archive      = {J_RAS},
  author       = {Rui Song and Fengming Li and Wei Quan and Xuting Yang and Jie Zhao},
  doi          = {10.1016/j.robot.2020.103651},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103651},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Skill learning for robotic assembly based on visual perspectives and force sensing},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
