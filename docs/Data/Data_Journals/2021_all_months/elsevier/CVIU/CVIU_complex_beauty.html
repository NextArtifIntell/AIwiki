<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CVIU_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cviu---98">CVIU - 98</h2>
<ul>
<li><details>
<summary>
(2021). Self-attentive 3D human pose and shape estimation from
videos. <em>CVIU</em>, <em>213</em>, 103305. (<a
href="https://doi.org/10.1016/j.cviu.2021.103305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of estimating 3D human pose and shape from videos. While existing frame-based approaches have made significant progress, these methods are independently applied to each image, thereby often leading to inconsistent predictions. In this work, we present a video-based learning algorithm for 3D human pose and shape estimation. The key insights of our method are two-fold. First, to address the inconsistent temporal prediction issue, we exploit temporal information in videos and propose a self-attention module that jointly considers short-range and long-range dependencies across frames, resulting in temporally coherent estimations. Second, we model human motion with a forecasting module that allows the transition between adjacent frames to be smooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M datasets. Extensive experimental results show that our algorithm performs favorably against the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Yun-Chun Chen and Marco Piccirilli and Robinson Piramuthu and Ming-Hsuan Yang},
  doi          = {10.1016/j.cviu.2021.103305},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103305},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-attentive 3D human pose and shape estimation from videos},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TMF: Temporal motion and fusion for action recognition.
<em>CVIU</em>, <em>213</em>, 103304. (<a
href="https://doi.org/10.1016/j.cviu.2021.103304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal motion information plays an important role in video understanding , human action recognition and other fields. Optical flow, which contains rich temporal motion information, has been widely used in many visual tasks and has achieved superior performance. However, the extraction of optical flow is time-consuming and laborious. In this paper, we propose a Temporal Motion and Fusion (TMF) module, including a motion extraction (ME) module and a temporal crossing fusion (TCF) module. The ME module can replace the traditional optical flow, establish the matching relationship between adjacent frames on the convoluted feature maps. And then extract simple and effective short-term motion information. TCF module crosses adjacent frames and fuse the information of nonadjacent video frames to realize long-term motion information modeling. Finally, the extracted motion information is fused with the appearance information captured by 2D convolution for final recognition. The experiment proved that with only a few additional parameters and calculation costs increased, our proposed lightweight model achieves state-of-the-art results on Something-Something-V1&amp;V2 and Diving-48, and obtains competitive results on HMDB-51 and UCF-101 among the single models.},
  archive      = {J_CVIU},
  author       = {Yanze Wang and Junyong Ye},
  doi          = {10.1016/j.cviu.2021.103304},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103304},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {TMF: Temporal motion and fusion for action recognition},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransRPN: Towards the transferable adversarial perturbations
using region proposal networks and beyond. <em>CVIU</em>, <em>213</em>,
103302. (<a href="https://doi.org/10.1016/j.cviu.2021.103302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adversarial perturbation for object detectors has drawn increasing attention due to the application in video surveillance and autonomous driving . However, few works have explored the transferability of adversarial perturbations across different object detectors. In this paper, we describe a simple but effective method, namely TransRPN , to generate adversarial perturbations that can reliably transfer among different object detectors—different categories ( e.g. , SSD, Faster-RCNN, YOLO) and different base networks ( e.g. , VGG16, ResNet , MobileNet), and even other tasks such as instance segmentation methods. Our method targets the Region Proposal Network (RPN) as the common bottleneck of the existing object detectors and disrupts the RPN by attacking the intermediate features. Moreover, as RPNs have no constraint on the size of the input image, our method can generate the adversarial images directly fitting into object detectors with arbitrary input size, which thereby improves the feasibility of our method in practical applications. We study four types of RPNs and validate our method on each type of RPN on MSCOCO dataset with nine object detectors and two instance segmentation methods, as well as the real-world API, which demonstrates the effectiveness of our method regarding transferability.},
  archive      = {J_CVIU},
  author       = {Yuezun Li and Ming-Ching Chang and Pu Sun and Honggang Qi and Junyu Dong and Siwei Lyu},
  doi          = {10.1016/j.cviu.2021.103302},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103302},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {TransRPN: Towards the transferable adversarial perturbations using region proposal networks and beyond},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliable shot identification for complex event detection via
visual-semantic embedding. <em>CVIU</em>, <em>213</em>, 103300. (<a
href="https://doi.org/10.1016/j.cviu.2021.103300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia event detection is the task of detecting a specific event of interest in an user-generated video on websites. The most fundamental challenge facing this task lies in the enormously varying quality of the video as well as the high-level semantic abstraction of event inherently. In this paper, we decompose the video into several segments and intuitively model the task of complex event detection as a multiple instance learning problem by representing each video as a “bag” of segments in which each segment is referred to as an instance. Instead of treating the instances equally, we associate each instance with a reliability variable to indicate its importance and then select reliable instances for training. To measure the reliability of the varying instances precisely, we propose a visual-semantic guided loss by exploiting low-level feature from visual information together with instance-event similarity based high-level semantic feature. Motivated by curriculum learning, we introduce a negative elastic-net regularization term to start training the classifier with instances of high reliability and gradually taking the instances with relatively low reliability into consideration. An alternative optimization algorithm is developed to solve the proposed challenging non-convex non-smooth problem. Experimental results on standard datasets, i.e ., TRECVID MEDTest 2013 and TRECVID MEDTest 2014, demonstrate the effectiveness and superiority of the proposed method to the baseline algorithms.},
  archive      = {J_CVIU},
  author       = {Minnan Luo and Xiaojun Chang and Chen Gong},
  doi          = {10.1016/j.cviu.2021.103300},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103300},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Reliable shot identification for complex event detection via visual-semantic embedding},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised action segmentation with effective use of
attention and self-attention. <em>CVIU</em>, <em>213</em>, 103298. (<a
href="https://doi.org/10.1016/j.cviu.2021.103298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper generates human action sequences using a novel hybrid sequence-to-sequence model that outputs a sequence of actions in the chronological order of the actions being performed in the longer activity of a given video. At test time, our models are able to generate action for each frame using weak supervision. We evaluate several sequence-to-sequence models to solve this task and demonstrate that they are able to solve action segment generation tasks on three challenging action recognition datasets. We present how to use self-attention and standard attention mechanisms with known sequence-to-sequence models for weakly supervised video action segmentation. Our new architecture is effective for weakly supervised action segmentation that uses a combination of recurrent and transformer-based sequence-to-sequence models. Our architecture consists of Transformers and GRU encoders to encode temporal information and we use self-attention and standard attention during the decoding process. We introduce an effective positional weight prior to further improve action segmentation performance . Using this architecture and two types of attention along with positional weight priors, we obtain state-of-the-art results on Breakfast and 50Salads datasets for weakly supervised action segmentation.},
  archive      = {J_CVIU},
  author       = {Yan Bin Ng and Basura Fernando},
  doi          = {10.1016/j.cviu.2021.103298},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103298},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Weakly supervised action segmentation with effective use of attention and self-attention},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Report on UG2+ challenge track 1: Assessing algorithms to
improve video object detection and classification from unconstrained
mobility platforms. <em>CVIU</em>, <em>213</em>, 103297. (<a
href="https://doi.org/10.1016/j.cviu.2021.103297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we effectively engineer a computer vision system that is able to interpret videos from unconstrained mobility platforms like UAVs ? One promising option is to make use of image restoration and enhancement algorithms from the area of computational photography to improve the quality of the underlying frames in a way that also improves automatic visual recognition. Along these lines, exploratory work is needed to find out which image pre-processing algorithms, in combination with the strongest features and supervised machine learning approaches , are good candidates for difficult scenarios like motion blur , weather, and mis-focus — all common artifacts in UAV acquired images. This paper summarizes the protocols and results of Track 1 of the UG 2 + 2+ Challenge held in conjunction with IEEE/CVF CVPR 2019. The challenge looked at two separate problems: (1) object detection improvement in video, and (2) object classification improvement in video. The challenge made use of new protocols for the UG 2 2 (UAV, Glider, Ground) dataset, which is an established benchmark for assessing the interplay between image restoration and enhancement and visual recognition. In total, 16 algorithms were submitted by academic and corporate teams, and a detailed analysis of them is reported here.},
  archive      = {J_CVIU},
  author       = {Sreya Banerjee and Rosaura G. VidalMata and Zhangyang Wang and Walter J. Scheirer},
  doi          = {10.1016/j.cviu.2021.103297},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103297},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Report on UG2+ challenge track 1: Assessing algorithms to improve video object detection and classification from unconstrained mobility platforms},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rolling-shutter-stereo-aware motion estimation and image
correction. <em>CVIU</em>, <em>213</em>, 103296. (<a
href="https://doi.org/10.1016/j.cviu.2021.103296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast majority of modern consumer-grade cameras employ the Rolling Shutter (RS) mechanism, which often produces geometrically distorted images due to inter-row delay during imaging, that needs to be properly handled. While several models and solutions have been proposed for the motion estimation problem of the monocular RS camera, the corresponding motion estimation model and algorithm have not been developed for the standard RS stereo setup. This is primarily due to the temporal-dynamic nature of the RS camera, which results in scanline-varying camera poses. This paper fills in this gap by developing two tractable RS stereo models under constant velocity motion and constant acceleration motion respectively, which only need to simply scale the optical flow vector . We further propose two corresponding RS stereo motion estimation algorithms to accurately compute the relative pose of the RS stereo camera between two consecutive frames. As a result of utilizing these RS stereo models associated with the parallel visual rays, our proposed methods offer better disambiguation of the translation and rotation, hence leading to significantly improved performance compared with the Global Shutter (GS) stereo model. Furthermore, we develop an RS stereo image correction method to remove RS distortions and recover high-quality GS images by conducting motion compensation. Experimental results on both simulated data and real RS stereo images show the superiority of our proposed models and methods.},
  archive      = {J_CVIU},
  author       = {Bin Fan and Yuchao Dai and Ke Wang},
  doi          = {10.1016/j.cviu.2021.103296},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103296},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Rolling-shutter-stereo-aware motion estimation and image correction},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semantically driven self-supervised algorithm for
detecting anomalies in image sets. <em>CVIU</em>, <em>213</em>, 103279.
(<a href="https://doi.org/10.1016/j.cviu.2021.103279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can readily detect when an image does not belong in a set by comparing semantic information between images to derive meaning and relationships from the colors, shapes, sizes, locations, and textures within them. Current self-supervised anomaly detection algorithms in computer vision do not possess this ability. Most algorithms learn to detect anomalies through a training objective that only optimizes for machine-level features and do not evaluate semantic features . To fill this gap, we propose a novel self-supervised algorithm that detects anomalies by learning and modeling semantic information within a set of images. This is accomplished by first training our algorithm to be sensitive or invariant to targeted semantic information, and then modeling the semantic relationships learned so that we can detect anomalies by measuring how far images deviate from the model. Experimenting with our algorithm, we show that different datasets can have different semantic sensitivities, those sensitivities can fluctuate between and within sets depending on different aspects of the images, and directly targeting those sensitivities can improve anomaly detection performance. Using our algorithm, we were able to achieve a AUROC of 0.7146 on the CIFAR-10 dataset, which is an improvement of 0.0741 over another current leading self-supervised anomaly detection algorithm.},
  archive      = {J_CVIU},
  author       = {Bradley J. Wheeler and Hassan A. Karimi},
  doi          = {10.1016/j.cviu.2021.103279},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103279},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A semantically driven self-supervised algorithm for detecting anomalies in image sets},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monocular 3D multi-person pose estimation via predicting
factorized correction factors. <em>CVIU</em>, <em>213</em>, 103278. (<a
href="https://doi.org/10.1016/j.cviu.2021.103278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great achievement of 3D human pose estimation, recovering the 3D poses of multiple persons in a single image is still a challenging problem. In this paper, we focus on one specific problem in 3D multi-person pose estimation (3D-MPPE): estimating the absolute 3D human poses. We proposed a pipeline consists of human detection, absolute 3D human root localization , and root-relative 3D single-person pose estimation modules. For the absolute 3D human root localization task, we propose a decoupling dual-branch structure to reconstruct the height of the human body, and further output the depth and localization of the 3D human root in the camera coordinate system. Furthermore, a data augmentation strategy is presented to tackle occlusions, such that our model can effectively estimate the root localization with the incomplete bounding boxes . For the 3D human relative pose estimation task, we use the attention mechanism to capture the correlation between human joint coordinates and further improve the accuracy of relative pose estimation. Finally, we merge the absolute depth of human and the relative 3D pose to output the absolute 3D human pose.},
  archive      = {J_CVIU},
  author       = {Yu Guo and Lichen Ma and Zhi Li and Xuan Wang and Fei Wang},
  doi          = {10.1016/j.cviu.2021.103278},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103278},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Monocular 3D multi-person pose estimation via predicting factorized correction factors},
  volume       = {213},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mutual calibration training: Training deep neural networks
with noisy labels using dual-models. <em>CVIU</em>, <em>212</em>,
103277. (<a href="https://doi.org/10.1016/j.cviu.2021.103277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A precise large-scale dataset is crucial for supervising the training of deep neural networks (DNNs) in image classification . However, manually annotating large-scale dataset is time-consuming, which limits the scalability of supervised training. On the other hand, it is relatively easier to obtain a small clean dataset as well as a vast amount of data with noisy labels, but training on a noisy dataset causes the performance of deep networks dropping dramatically. To overcome this problem, this work studies how to effectively and efficiently train deep works on the noisy large-scale dataset in conjunction with a small clean dataset. One problem with transfer learning from a small clean dataset is that the transfer learning technique risks over-fitting on the clean dataset due to more parameters than the training examples. Hence, we propose a new approach, called online easy example mining (OEEM) to train deep network on the entire noisy dataset. OEEM aims to select clean samples to guide the training without human annotation by estimating the confidence of the observed labels with the model prediction. However, the sample-selection bias in the OEEM can trap the model into a locally optimal value. Consequently, we propose a general framework called Mutual Calibration Training (MCT) against different noise levels and noise types using dual-models, which combines the idea of transfer learning and OEEM. Finally, we conduct experiments on synthetic as well as real-world dataset with different noise types and noise rates, respectively. And the results demonstrate effectiveness of our approach.},
  archive      = {J_CVIU},
  author       = {Rui Liu and Yi Liu and Rui Wang and Yucong Zhou},
  doi          = {10.1016/j.cviu.2021.103277},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103277},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Mutual calibration training: Training deep neural networks with noisy labels using dual-models},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MetaVD: A meta video dataset for enhancing human action
recognition datasets. <em>CVIU</em>, <em>212</em>, 103276. (<a
href="https://doi.org/10.1016/j.cviu.2021.103276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous practical datasets have been developed to recognize human actions from videos. However, many of them were constructed by collecting videos within a limited domain; thus, a model trained using one of the existing datasets often fails to classify videos in a different domain accurately. A possible solution for this drawback is to enhance the domain of each action label, i.e., to import videos associated with a given action label from the other datasets, and then, to train a model using the enhanced dataset. To realize this solution, we constructed a meta video dataset from the existing datasets for human action recognition , referred to as MetaVD . MetaVD comprises six popular human action recognition datasets, which we integrated by annotating 568,015 relation labels in total. These relation labels reflect equality, similarity, and hierarchy between action labels of the original datasets. We further present simple yet effective dataset enhancement methods using MetaVD, which are useful for training models with higher generalization performance , as established by experiments on human action classification . As a further contribution of MetaVD, we show that its analysis can provide useful insight into the datasets.},
  archive      = {J_CVIU},
  author       = {Yuya Yoshikawa and Yutaro Shigeto and Akikazu Takeuchi},
  doi          = {10.1016/j.cviu.2021.103276},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103276},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MetaVD: A meta video dataset for enhancing human action recognition datasets},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of 3D human pose estimation algorithms for
markerless motion capture. <em>CVIU</em>, <em>212</em>, 103275. (<a
href="https://doi.org/10.1016/j.cviu.2021.103275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation is a very active research field, stimulated by its important applications in robotics, entertainment or health and sports sciences, among others. Advances in convolutional networks triggered noticeable improvements in 2D pose estimation, leading modern 3D markerless motion capture techniques to an average error per joint of 20 mm. However, with the proliferation of methods, it is becoming increasingly difficult to make an informed choice. Here, we review the leading human pose estimation methods of the past five years, focusing on metrics, benchmarks and method structures. We propose a taxonomy based on accuracy, speed and robustness that we use to classify de methods and derive directions for future research.},
  archive      = {J_CVIU},
  author       = {Yann Desmarais and Denis Mottet and Pierre Slangen and Philippe Montesinos},
  doi          = {10.1016/j.cviu.2021.103275},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103275},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A review of 3D human pose estimation algorithms for markerless motion capture},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic generation of dense non-rigid optical flow.
<em>CVIU</em>, <em>212</em>, 103274. (<a
href="https://doi.org/10.1016/j.cviu.2021.103274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There hardly exists any large-scale datasets with dense optical flow of non-rigid motion from real-world imagery as of today. The reason lies mainly in the required setup to derive ground truth optical flows: a series of images with known camera poses along its trajectory, and an accurate 3D model from a textured scene. Human annotation is not only too tedious for large databases, it can simply hardly contribute to accurate optical flow. To circumvent the need for manual annotation, we propose a framework to automatically generate optical flow from real-world videos. The method extracts and matches objects from video frames to compute initial constraints, and applies a deformation over the objects of interest to obtain dense optical flow fields . We propose several ways to augment the optical flow variations. Extensive experimental results show that training on our automatically generated optical flow outperforms methods that are trained on rigid synthetic data using FlowNet-S, LiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow generation framework are released at https://github.com/lhoangan/arap_flow .},
  archive      = {J_CVIU},
  author       = {Hoàng-Ân Lê and Tushar Nimbhorkar and Thomas Mensink and Anil S. Baslamisli and Sezer Karaoglu and Theo Gevers},
  doi          = {10.1016/j.cviu.2021.103274},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103274},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Automatic generation of dense non-rigid optical flow},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Margin-based discriminant embedding guided sparse matrix
regression for image supervised feature selection. <em>CVIU</em>,
<em>212</em>, 103273. (<a
href="https://doi.org/10.1016/j.cviu.2021.103273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix regression uses matrix data as input and directly selects the features from matrix data by employing several couples of left and right regression matrices. However, the existing matrix regression methods do not consider the relationship between different classes of data and cannot get discriminant left/right regression matrix, which results in poor classifications. In this paper, a margin-based discriminant embedding sparse matrix regression (MDESMR) model for image supervised feature selection is proposed. For each matrix data, a margin is first defined as the difference between two types of distances determined by the left/right regression matrix. Maximizing the average margin for all training matrix data can get the nonlinear discriminant embedding. Thus, a nonlinear embedding and its linear approximation can be obtained simultaneously. An alternative iterative optimization algorithm for solving the proposed model is also designed and the corresponding closed-form solutions in each iteration are found. Some experiments on several datasets demonstrate the superiority of our method.},
  archive      = {J_CVIU},
  author       = {Xiuhong Chen and Yun Lu and Jun Zhang and Xingyu Zhu},
  doi          = {10.1016/j.cviu.2021.103273},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103273},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Margin-based discriminant embedding guided sparse matrix regression for image supervised feature selection},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subspace reconstruction based correlation filter for object
tracking. <em>CVIU</em>, <em>212</em>, 103272. (<a
href="https://doi.org/10.1016/j.cviu.2021.103272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correlation filter (CF) achieves excellent performance, showing high robustness to motion blur or illumination change by learning filters. However, tracking in challenging scenarios with occlusion or out-of-view is still not well resolved. In the scenario of occlusion, the background information is mixed into the image patch to learn the filter, which causes the filter to learn the background. To alleviate this problem, we improve CF trackers by proposing the subspace reconstruction based CF (SRBCF) tracker. In our method, the original image patch for learning filters is replaced by a reconstructed patch when the appearance of the object dramatically changes, such as occlusion or disappearance, so that the filter can learn from the object instead of the background. We construct the subspace with image patches of the searching window in previous frames. To track the changes in the subspace and mitigate the adverse effects of outliers on the subspace during the tracking process, we improve a dynamic L1-PCA method to construct and update the subspace with a slightly extra computational cost. Our method can be embedded in various correlation filter trackers, such as STAPLE and KCF. Extensive experiments on the OTB-100 dataset, UAV123, DTB70, and Temple Color Pure dataset (we removed 49 sequences repeated in OTB-100) validate the effectiveness of our method. The maximum AUC increase reaches 11.2\% for the baseline method on DTB70.},
  archive      = {J_CVIU},
  author       = {Yuan Tai and Yihua Tan and Shengzhou Xiong and Jinwen Tian},
  doi          = {10.1016/j.cviu.2021.103272},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103272},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Subspace reconstruction based correlation filter for object tracking},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lookahead adversarial learning for near real-time semantic
segmentation. <em>CVIU</em>, <em>212</em>, 103271. (<a
href="https://doi.org/10.1016/j.cviu.2021.103271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is one of the most fundamental problems in computer vision with significant impact on a wide variety of applications. Adversarial learning is shown to be an effective approach for improving semantic segmentation quality by enforcing higher-level pixel correlations and structural information. However, state-of-the-art semantic segmentation models cannot be easily plugged into an adversarial setting because they are not designed to accommodate convergence and stability issues in adversarial networks. We bridge this gap by building a conditional adversarial network with a state-of-the-art segmentation model (DeepLabv3+) at its core. To battle the stability issues, we introduce a novel lookahead adversarial learning (LoAd) approach with an embedded label map aggregation module. We focus on semantic segmentation models that run fast at inference for near real-time field applications. Through extensive experimentation, we demonstrate that the proposed solution can alleviate divergence issues in an adversarial semantic segmentation setting and results in considerable performance improvements ( + 5\% +5\% in some classes) on the baseline for three standard datasets.},
  archive      = {J_CVIU},
  author       = {Hadi Jamali-Rad and Attila Szabó},
  doi          = {10.1016/j.cviu.2021.103271},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103271},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lookahead adversarial learning for near real-time semantic segmentation},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to teach and learn for semi-supervised few-shot
image classification. <em>CVIU</em>, <em>212</em>, 103270. (<a
href="https://doi.org/10.1016/j.cviu.2021.103270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel semi-supervised few-shot image classification method named Learning to Teach and Learn (LTTL) to effectively leverage unlabeled samples in small-data regimes. Our method is based on self-training, which assigns pseudo labels to unlabeled data . However, the conventional pseudo-labeling operation heavily relies on the initial model trained by using a handful of labeled data and may produce many noisy labeled samples. We propose to solve the problem with three steps: firstly, cherry-picking searches valuable samples from pseudo-labeled data by using a soft weighting network; and then, cross-teaching allows the classifiers to teach mutually for rejecting more noisy labels. A feature synthesizing strategy is introduced for cross-teaching to avoid clean samples being rejected by mistake; finally, the classifiers are fine-tuned with a few labeled data to avoid gradient drifts. We use the meta-learning paradigm to optimize the parameters in the whole framework. The proposed LTTL combines the power of meta-learning and self-training, achieving superior performance compared with the baseline methods on two public benchmarks.},
  archive      = {J_CVIU},
  author       = {Xinzhe Li and Jianqiang Huang and Yaoyao Liu and Qin Zhou and Shibao Zheng and Bernt Schiele and Qianru Sun},
  doi          = {10.1016/j.cviu.2021.103270},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103270},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning to teach and learn for semi-supervised few-shot image classification},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handling new target classes in semantic segmentation with
domain adaptation. <em>CVIU</em>, <em>212</em>, 103258. (<a
href="https://doi.org/10.1016/j.cviu.2021.103258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we define and address a novel domain adaptation (DA) problem in semantic scene segmentation, where the target domain not only exhibits a data distribution shift w.r.t. the source domain, but also includes novel classes that do not exist in the latter. Different to “open-set” (Panareda Busto and Gall, 2017) and “universal domain adaptation” (You et al. 2019), which both regard all objects from new classes as “unknown”, we aim at explicit test-time prediction for these new classes. To reach this goal, we propose a framework that leverages domain adaptation and zero-shot learning techniques to enable “boundless” adaptation in the target domain. It relies on a novel architecture, along with a dedicated learning scheme, to bridge the source–target domain gap while learning how to map new classes’ labels to relevant visual representations. The performance is further improved using self-training on target-domain pseudo-labels. For validation, we consider different domain adaptation set-ups, namely synthetic-2-real, country-2-country and dataset-2-dataset. Our framework outperforms the baselines by significant margins, setting competitive standards on all benchmarks for the new task. Code and models are available at: https://github.com/valeoai/buda .},
  archive      = {J_CVIU},
  author       = {Maxime Bucher and Tuan-Hung Vu and Matthieu Cord and Patrick Pérez},
  doi          = {10.1016/j.cviu.2021.103258},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103258},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Handling new target classes in semantic segmentation with domain adaptation},
  volume       = {212},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised instance segmentation using multi-prior
fusion. <em>CVIU</em>, <em>211</em>, 103261. (<a
href="https://doi.org/10.1016/j.cviu.2021.103261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation usually requires mask annotation, which has a much higher labeling cost than bounding box annotation. In recent years, weakly supervised instance segmentation (WSIS) has drawn great attention with only bounding box annotation needed during training. However, bounding box annotation can only provide limited information for the segmentation task . Without pixel-wise fine-grained supervision, predicted segmentation masks usually have blurred boundaries with poor performance. To address the issue caused by weak bounding box annotation, in this paper, two types of prior knowledge, i.e., bounding box tightness prior and contour prior, are employed to guide the mask prediction. For the bounding box tightness prior, we assume that the objects are annotated tightly with bounding boxes. A multi-instance learning (MIL) approach is adopted with the tightness constraint. For the contour prior, we assume that the mask boundaries should align with strong image gradients. We maximize the inner product of the gradients between the mask proposal and the corresponding image region, which is treated as the contour prior constraint. Finally, both prior constraints are combined for separating targets and background. The experiment results on the PASCAL VOC dataset and the augmented dataset demonstrate that our method outperforms most state-of-the-art WSIS methods.},
  archive      = {J_CVIU},
  author       = {Shengyu Hao and Gaoang Wang and Renshu Gu},
  doi          = {10.1016/j.cviu.2021.103261},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103261},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Weakly supervised instance segmentation using multi-prior fusion},
  volume       = {211},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image editing with varying intensities of processing.
<em>CVIU</em>, <em>211</em>, 103260. (<a
href="https://doi.org/10.1016/j.cviu.2021.103260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current image editing algorithms prevailingly involve specific processing intensities. After the network training phase, the definite mapping is carried on testing images, which leads to under- or over-editing in certain cases. However, for many real problems, having access to diverse intensities of processing of the output is preferable. The hypothesis is that the part of parameters that correspond to the intensity of processing of the network are implicitly embedded in the whole parameter set and can be explicitly extracted and utilized, even with limited supervision. In this paper, we introduce the concept of Intensity of Processing, termed as IP, and propose image editing with Varying Intensities of Processing, termed as VIP. Given the input and corresponding fully-edited output, we would like to not only figure out the particular mapping from input to output, but also be able to obtain multiple continuous intermediate states with in-between intensities of processing. Refer to the network that deals with the original image editing operation as Base Network . By adding an extra module, which we call Condition Encoder , to condition and regulate the Base Network , we make the whole image editing process controllable. Moreover, we develop a triplet loss and construct the sequentiality of generated intermediate states with limited supervision. A significant advantage of our method is that it does not require groundtruth images of the middle states during training, which substantially relaxes the range of applicable problems. Extensive experiments demonstrate the effectiveness and potentiality of the proposed framework; discussion and future work are also provided.},
  archive      = {J_CVIU},
  author       = {Yasi Wang and Yuankai Qi and Hongxun Yao and Dong Gong and Qi Wu},
  doi          = {10.1016/j.cviu.2021.103260},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103260},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Image editing with varying intensities of processing},
  volume       = {211},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diversified text-to-image generation via deep mutual
information estimation. <em>CVIU</em>, <em>211</em>, 103259. (<a
href="https://doi.org/10.1016/j.cviu.2021.103259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating photo-realistic, text-matched, and diverse images simultaneously from given text descriptions is a challenging task in computer vision . Previous works mostly focus on visual realism and semantic relevance , but neglect variations of the generated results which is also an important target in text-to-image generation (T2I). In this paper, we design a new module that improves the training of generative adversarial nets (GANs) to generate diverse images from a text input. Our T2I method is based on conditional GANs and has three components: contextual text embedding module (CTEM), deep Mutual Information (MI) estimation module for stacked image generation (DMIEM), and text-image semantic relevance module (TISRM). CTEM attempts to learn text embeddings via leveraging the fine-tuning capabilities of BERT . DMIEM has a stack of attention embedded generators, integrating with global/local MI estimation and maximization between input and output of the generator to make the T2I mapping more relevant, and then generating diverse and photo-realistic images progressively. TISRM is introduced to enhance the semantic consistency of the text-image pairs by regenerating the text descriptions from the generated images. Extensive experiments on three datasets indicate that our method can generate text-matched and more diverse images without quality degradation compared to the state-of-the-art approaches.},
  archive      = {J_CVIU},
  author       = {Ailin Li and Lei Zhao and Zhiwen Zuo and Zhizhong Wang and Haibo Chen and Dongming Lu and Wei Xing},
  doi          = {10.1016/j.cviu.2021.103259},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103259},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Diversified text-to-image generation via deep mutual information estimation},
  volume       = {211},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pick-object-attack: Type-specific adversarial attack for
object detection. <em>CVIU</em>, <em>211</em>, 103257. (<a
href="https://doi.org/10.1016/j.cviu.2021.103257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many recent studies have shown that deep neural models are vulnerable to adversarial samples: images with imperceptible perturbations, for example, can fool image classifiers. In this paper, we present the first type-specific approach to generating adversarial examples for object detection, which entails detecting bounding boxes around multiple objects present in the image and classifying them at the same time, making it a harder task than against image classification . We specifically aim to attack the widely used Faster R-CNN by changing the predicted label for a particular object in an image: where prior work has targeted one specific object (a stop sign), we generalize to arbitrary objects, with the key challenge being the need to change the labels of all bounding boxes for all instances of that object type. To do so, we propose a novel method, named Pick-Object-Attack . Pick-Object-Attack successfully adds perturbations only to bounding boxes for the targeted object, preserving the labels of other detected objects in the image. In terms of perceptibility, the perturbations induced by the method are very small. Furthermore, for the first time, we examine the effect of adversarial attacks on object detection in terms of a downstream task, image captioning; we show that where a method that can modify all object types leads to very obvious changes in captions, the changes from our constrained attack are much less apparent.},
  archive      = {J_CVIU},
  author       = {Omid Mohamad Nezami and Akshay Chaturvedi and Mark Dras and Utpal Garain},
  doi          = {10.1016/j.cviu.2021.103257},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103257},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pick-object-attack: Type-specific adversarial attack for object detection},
  volume       = {211},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightweight adaptive weighted network for single image
super-resolution. <em>CVIU</em>, <em>211</em>, 103254. (<a
href="https://doi.org/10.1016/j.cviu.2021.103254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been successfully applied to the single-image super-resolution (SISR) task with superior performance in recent years. However, most convolutional neural network (CNN) based SR models have a large number of parameters to be optimized, which requires heavy computation and thereby limits their real-world applications. In this work, a novel lightweight SR network, named Adaptive Weighted Super-Resolution Network (LW-AWSRN), is proposed to address this issue. A novel local fusion block (LFB) is developed in LW-AWSRN for efficient residual learning, which consists of several stacked adaptive weighted residual units (AWRU) and a local residual fusion unit (LRFU). Moreover, an adaptive weighted multi-scale (AWMS) module is proposed to make full use of features for the reconstruction of HR images. The AWMS module includes several convolutions with multiple scales, and the redundancy scale branch can be removed according to the contribution of adaptive weights for the lightweight network. The experimental results on the commonly used datasets show that the proposed LW-AWSRN achieves superior performance on × × 2, × × 3, × × 4, and × × 8 scale factors compared to state-of-the-art methods with similar parameters and computational overhead. It suggests that LW-AWSRN has a better trade-off between reconstruction quality and model size.},
  archive      = {J_CVIU},
  author       = {Zheng Li and Chaofeng Wang and Jun Wang and Shihui Ying and Jun Shi},
  doi          = {10.1016/j.cviu.2021.103254},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103254},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lightweight adaptive weighted network for single image super-resolution},
  volume       = {211},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dehazing cost volume for deep multi-view stereo in
scattering media with airlight and scattering coefficient estimation.
<em>CVIU</em>, <em>211</em>, 103253. (<a
href="https://doi.org/10.1016/j.cviu.2021.103253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a learning-based multi-view stereo (MVS) method in scattering media, such as fog or smoke, with a novel cost volume, called the dehazing cost volume. Images captured in scattering media are degraded due to light scattering and attenuation caused by suspended particles. This degradation depends on scene depth; thus, it is difficult for traditional MVS methods to evaluate photometric consistency because the depth is unknown before three-dimensional (3D) reconstruction. The dehazing cost volume can solve this chicken-and-egg problem of depth estimation and image restoration by computing the scattering effect using swept planes in the cost volume. We also propose a method of estimating scattering parameters, such as airlight, and a scattering coefficient , which are required for our dehazing cost volume. The output depth of a network with our dehazing cost volume can be regarded as a function of these parameters; thus, they are geometrically optimized with a sparse 3D point cloud obtained at a structure-from-motion step. Experimental results on synthesized hazy images indicate the effectiveness of our dehazing cost volume against the ordinary cost volume regarding scattering media. We also demonstrated the applicability of our dehazing cost volume to real foggy scenes.},
  archive      = {J_CVIU},
  author       = {Yuki Fujimura and Motoharu Sonogashira and Masaaki Iiyama},
  doi          = {10.1016/j.cviu.2021.103253},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103253},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dehazing cost volume for deep multi-view stereo in scattering media with airlight and scattering coefficient estimation},
  volume       = {211},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting the future from first person (egocentric) vision:
A survey. <em>CVIU</em>, <em>211</em>, 103252. (<a
href="https://doi.org/10.1016/j.cviu.2021.103252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric videos can bring a lot of information about how humans perceive the world and interact with the environment, which can be beneficial for the analysis of human behaviour. The research in egocentric video analysis is developing rapidly thanks to the increasing availability of wearable devices and the opportunities offered by new large-scale egocentric datasets. As computer vision techniques continue to develop at an increasing pace, the tasks related to the prediction of future are starting to evolve from the need of understanding the present. Predicting future human activities , trajectories and interactions with objects is crucial in applications such as human–robot interaction, assistive wearable technologies for both industrial and daily living scenarios, entertainment and virtual or augmented reality . This survey summarizes the evolution of studies in the context of future prediction from egocentric vision making an overview of applications, devices, existing problems, commonly used datasets, models and input modalities. Our analysis highlights that methods for future prediction from egocentric vision can have a significant impact in a range of applications and that further research efforts should be devoted to the standardization of tasks and the proposal of datasets considering real-world scenarios such as the ones with an industrial vocation.},
  archive      = {J_CVIU},
  author       = {Ivan Rodin and Antonino Furnari and Dimitrios Mavroeidis and Giovanni Maria Farinella},
  doi          = {10.1016/j.cviu.2021.103252},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103252},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Predicting the future from first person (egocentric) vision: A survey},
  volume       = {211},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAPS: Self-attentive pathway search for weakly-supervised
action localization with background-action augmentation. <em>CVIU</em>,
<em>210</em>, 103256. (<a
href="https://doi.org/10.1016/j.cviu.2021.103256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal action localization is a challenging computer vision task , which aims to derive frame-level action identifier based on video-level supervision. Attention mechanism is a widely used paradigm for action recognition and localization in most recent methods. However, existing attention-based methods mostly focus on capturing the global dependency of the frame sequence regardless of the local inter-frame distances. Moreover, during background modeling , different background contents are typically classified into one category, which inevitably jeopardizes the discriminative ability of classifiers and brings about irrelevant noise. In this paper, we present a novel self-attentive pathway search framework, namely SAPS, to address the above challenges. To achieve comprehensive representation with discriminative attention weights, we design a NAS-based attentive module with a path-level searching process, and construct a competitive attention structure revealing both local and global dependency. Furthermore, we propose the action-related background modeling for robust background-action augmentation, where knowledge derived from background can provide informative clues for action recognition. An ensemble T-CAM operation is subsequently designed to incorporate background information to further refine the temporal action localization results. Extensive experiments on two benchmark datasets (i.e., THUMOS14 and ActivityNet1.2) have clearly corroborated the efficacy of our method.},
  archive      = {J_CVIU},
  author       = {Xiao-Yu Zhang and Yaru Zhang and Haichao Shi and Jing Dong},
  doi          = {10.1016/j.cviu.2021.103256},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103256},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SAPS: Self-attentive pathway search for weakly-supervised action localization with background-action augmentation},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal attention networks for low-level
vision-and-language navigation. <em>CVIU</em>, <em>210</em>, 103255. (<a
href="https://doi.org/10.1016/j.cviu.2021.103255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-and-Language Navigation (VLN) is a challenging task in which an agent needs to follow a language-specified path to reach a target destination. The goal gets even harder as the actions available to the agent get simpler and move towards low-level, atomic interactions with the environment. This setting takes the name of low-level VLN. In this paper, we strive for the creation of an agent able to tackle three key issues: multi-modality, long-term dependencies, and adaptability towards different locomotive settings. To that end, we devise “Perceive, Transform, and Act” (PTA): a fully-attentive VLN architecture that leaves the recurrent approach behind and the first Transformer-like architecture incorporating three different modalities — natural language, images, and low-level actions for the agent control. In particular, we adopt an early fusion strategy to merge lingual and visual information efficiently in our encoder. We then propose to refine the decoding phase with a late fusion extension between the agent’s history of actions and the perceptual modalities. We experimentally validate our model on two datasets: PTA achieves promising results in low-level VLN on R2R and achieves good performance in the recently proposed R4R benchmark.},
  archive      = {J_CVIU},
  author       = {Federico Landi and Lorenzo Baraldi and Marcella Cornia and Massimiliano Corsini and Rita Cucchiara},
  doi          = {10.1016/j.cviu.2021.103255},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103255},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multimodal attention networks for low-level vision-and-language navigation},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic segmentation from remote sensor data and the
exploitation of latent learning for classification of auxiliary tasks.
<em>CVIU</em>, <em>210</em>, 103251. (<a
href="https://doi.org/10.1016/j.cviu.2021.103251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we address three different aspects of semantic segmentation from remote sensor data using deep neural networks . Firstly, we focus on the semantic segmentation of buildings from remote sensor data and propose ICT-Net: a novel network with the underlying architecture of a fully convolutional network , infused with feature re-calibrated Dense blocks at each layer. Uniquely, the proposed network combines the localization accuracy and use of context of the U-Net network architecture , the compact internal representations and reduced feature redundancy of the Dense blocks, and the dynamic channel-wise feature re-weighting of the Squeeze-and-Excitation(SE) blocks. The proposed network has been tested on the INRIA and AIRS benchmark datasets and is shown to outperform other state of the art. Secondly, as the building classification is typically the first step of the reconstruction process, we investigate the relationship of the classification accuracy to the reconstruction accuracy . Due to the lack of (1) scene depth information, and (2) ground-truth (blueprints) for large urban-areas, the evaluation of the 3D reconstructions is not possible. Thus, we use boundary localization as a proxy to reconstruction accuracy and perform the evaluation in 2D. A comparative quantitative analysis of reconstruction accuracies corresponding to different classification accuracies confirms the strong correlation between the two. We present the results which show a consistent and considerable reduction in the reconstruction accuracy. Finally, we present the simple yet compelling concept of latent learning and the implications it carries within the context of deep learning . We posit that a network trained on a primary task (i.e. building classification) is unintentionally learning about auxiliary tasks (e.g. the classification of road, tree, etc.) which are complementary to the primary task. Although embedded in a trained network, this latent knowledge relating to the auxiliary tasks is never externalized or immediately expressed but instead only knowledge relating to the primary task is ever output by the network. We experimentally prove this occurrence of incidental learning on the pre-trained ICT-Net and show how sub-classification of the negative label is possible without further training/fine-tuning . We present the results of our experiments and explain how knowledge about auxiliary and complementary tasks – for which the network was never trained – can be retrieved and utilized for further classification. We extensively tested the proposed technique on the ISPRS benchmark dataset which contains multi-label ground truth, and report an average classification accuracy (F1 score) of 54.29\% (SD=17.03) for roads, 10.15\% (SD=2.54) for cars, 24.11\% (SD=5.25) for trees, 42.74\% (SD=6.62) for low vegetation, and 18.30\% (SD=16.08) for clutter. The source code and supplemental material is publicly available at http://www.theICTlab.org/lp/2020ICTNet/ .},
  archive      = {J_CVIU},
  author       = {Bodhiswatta Chatterjee and Charalambos Poullis},
  doi          = {10.1016/j.cviu.2021.103251},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103251},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Semantic segmentation from remote sensor data and the exploitation of latent learning for classification of auxiliary tasks},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot action recognition with implicit temporal alignment
and pair similarity optimization. <em>CVIU</em>, <em>210</em>, 103250.
(<a href="https://doi.org/10.1016/j.cviu.2021.103250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to recognize instances from novel classes with few labeled samples, which has great value in research and application. Although there has been a lot of work in this area recently, most of the existing work is based on image classification tasks . Video-based few-shot action recognition has not been explored well and remains challenging: (1) the differences of implementation details among different papers make a fair comparison difficult; (2) the wide variations and misalignment of temporal sequences make the video-level similarity comparison difficult; (3) the scarcity of labeled data makes the optimization difficult. To solve these problems, this paper presents (1) a specific setting to evaluate the performance of few-shot action recognition algorithms ; (2) an implicit sequence-alignment algorithm for better video-level similarity comparison; (3) an advanced loss for few-shot learning to optimize pair similarity with limited data. Specifically, we propose a novel few-shot action recognition framework that uses long short-term memory following 3D convolutional layers for sequence modeling and alignment. Circle loss is introduced to maximize the within-class similarity and minimize the between-class similarity flexibly towards a more definite convergence target. Instead of using random or ambiguous experimental settings, we set a concrete criterion analogous to the standard image-based few-shot learning setting for few-shot action recognition evaluation. Extensive experiments on two datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_CVIU},
  author       = {Congqi Cao and Yajuan Li and Qinyi Lv and Peng Wang and Yanning Zhang},
  doi          = {10.1016/j.cviu.2021.103250},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103250},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Few-shot action recognition with implicit temporal alignment and pair similarity optimization},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decoupled appearance and motion learning for efficient
anomaly detection in surveillance video. <em>CVIU</em>, <em>210</em>,
103249. (<a href="https://doi.org/10.1016/j.cviu.2021.103249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating the analysis of surveillance video footage is of great interest when urban environments or industrial sites are monitored by a large number of cameras. As anomalies are often context-specific, it is hard to predefine events of interest and collect labeled training data. A purely unsupervised approach for automated anomaly detection is much more suitable. For every camera, a separate algorithm could then be deployed that learns over time a baseline model of appearance and motion related features of the objects within the camera viewport. Anything that deviates from this baseline is flagged as an anomaly for further analysis downstream. We propose a new neural network architecture that learns the normal behavior in a purely unsupervised fashion. In contrast to previous work, we use latent code predictions as our anomaly metric. We show that this outperforms frame reconstruction-based and prediction-based methods on different benchmark datasets both in terms of accuracy and robustness against changing lighting and weather conditions. By decoupling an appearance and a motion model, our model can also process 16 to 45 times more frames per second than related approaches which makes our model suitable for deploying on the camera itself or on other edge devices.},
  archive      = {J_CVIU},
  author       = {Bo Li and Sam Leroux and Pieter Simoens},
  doi          = {10.1016/j.cviu.2021.103249},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103249},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Decoupled appearance and motion learning for efficient anomaly detection in surveillance video},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-level prior-based loss functions for medical image
segmentation: A survey. <em>CVIU</em>, <em>210</em>, 103248. (<a
href="https://doi.org/10.1016/j.cviu.2021.103248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, deep convolutional neural networks (CNNs) have demonstrated state of the art performance for supervised medical image segmentation , across various imaging modalities and tasks. Despite early success, segmentation networks may still generate anatomically aberrant segmentations, with holes or inaccuracies near the object boundaries. To mitigate this effect, recent research works have focused on incorporating spatial information or prior knowledge to enforce anatomically plausible segmentation. If the integration of prior knowledge in image segmentation is not a new topic in classical optimization approaches, it is today an increasing trend in CNN based image segmentation, as shown by the growing literature on the topic. In this survey, we focus on high level prior, embedded at the loss function level. We categorize the articles according to the nature of the prior: the object shape, size, topology, and the inter-regions constraints. We highlight strengths and limitations of current approaches, discuss the challenge related to the design and the integration of prior-based losses, and the optimization strategies , and draw future research directions.},
  archive      = {J_CVIU},
  author       = {Rosana El Jurdi and Caroline Petitjean and Paul Honeine and Veronika Cheplygina and Fahed Abdallah},
  doi          = {10.1016/j.cviu.2021.103248},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103248},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {High-level prior-based loss functions for medical image segmentation: A survey},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning-based single image face depth data
enhancement. <em>CVIU</em>, <em>210</em>, 103247. (<a
href="https://doi.org/10.1016/j.cviu.2021.103247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition can benefit from the utilization of depth data captured using low-cost cameras, in particular for presentation attack detection purposes. Depth video output from these capture devices can however contain defects such as holes or general depth inaccuracies. This work proposes a deep learning face depth enhancement method in this context of facial biometrics , which adds a security aspect to the topic. U-Net-like architectures are utilized, and the networks are compared against hand-crafted enhancer types, as well as a similar depth enhancer network from related work trained for an adjacent application scenario. All tested enhancer types exclusively use depth data as input, which differs from methods that enhance depth based on additional input data such as visible light color images. Synthetic face depth ground truth images and degraded forms thereof are created with help of PRNet, to train multiple deep learning enhancer models with different network sizes and training configurations. Evaluations are carried out on the synthetic data, on Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435 images. These evaluations include an assessment of the falsification for occluded face depth input, which is relevant to biometric security. The proposed deep learning enhancers yield noticeably better results than the tested preexisting enhancers, without overly falsifying depth data when non-face input is provided, and are shown to reduce the error of a simple landmark-based PAD method.},
  archive      = {J_CVIU},
  author       = {Torsten Schlett and Christian Rathgeb and Christoph Busch},
  doi          = {10.1016/j.cviu.2021.103247},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103247},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep learning-based single image face depth data enhancement},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unifying frame rate and temporal dilations for improved
remote pulse detection. <em>CVIU</em>, <em>210</em>, 103246. (<a
href="https://doi.org/10.1016/j.cviu.2021.103246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) is the monitoring of blood volume pulse from a camera at a distance. 3-Dimensional Convolutional Neural Networks (3DCNNs) have shown promising performance on the rPPG task, although it is critical that we understand the impact of both video and model parameters. In this paper, we explore the effect of frame rate, temporal kernel width, and – more generally – temporal receptive field on the reliability of heart rate and waveform estimation carried out by 3DCNNs. We train and evaluate 32 3DCNNs with different temporal parameters on a new large-scale database for physiological monitoring in an interview scenario. We show that previous studies reporting null effects of frame rate changes on pulse estimators may no longer be valid when using CNNs, and decreasing the frame rate may actually improve performance. In particular, we found that models trained on videos with frame rates as low as 12.9 frames per second (fps) perform better than those trained on videos recorded at a full 90 fps, perhaps due to the temporal receptive fields becoming larger in time dimension when the fps decreases. Using this insight, we propose RemotePulseNet, a novel 3DCNN architecture that exploits temporally dilated convolutions with increasing dilation rate to drastically increase the receptive field. We compare its performance with that of recent state-of-the-art pulse estimation methods, and show that both RemotePulseNet and the low frame rate 3DCNNs produce high-quality pulse signals from faces captured under a challenging interview scenario. The source code and instructions for obtaining a copy of the test data are made available with this paper.},
  archive      = {J_CVIU},
  author       = {Jeremy Speth and Nathan Vance and Patrick Flynn and Kevin Bowyer and Adam Czajka},
  doi          = {10.1016/j.cviu.2021.103246},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103246},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unifying frame rate and temporal dilations for improved remote pulse detection},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AC-VRNN: Attentive conditional-VRNN for multi-future
trajectory prediction. <em>CVIU</em>, <em>210</em>, 103245. (<a
href="https://doi.org/10.1016/j.cviu.2021.103245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anticipating human motion in crowded scenarios is essential for developing intelligent transportation systems , social-aware robots and advanced video surveillance applications. A key component of this task is represented by the inherently multi-modal nature of human paths which makes socially acceptable multiple futures when human interactions are involved. To this end, we propose a generative architecture for multi-future trajectory predictions based on Conditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning mainly relies on prior belief maps, representing most likely moving directions and forcing the model to consider past observed dynamics in generating future positions. Human interactions are modelled with a graph-based attention mechanism enabling an online attentive hidden state refinement of the recurrent estimation. To corroborate our model, we perform extensive experiments on publicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS SportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its effectiveness in crowded scenes compared to several state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Alessia Bertugli and Simone Calderara and Pasquale Coscia and Lamberto Ballan and Rita Cucchiara},
  doi          = {10.1016/j.cviu.2021.103245},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103245},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {AC-VRNN: Attentive conditional-VRNN for multi-future trajectory prediction},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A distribution independence based method for 3D face shape
decomposition. <em>CVIU</em>, <em>210</em>, 103244. (<a
href="https://doi.org/10.1016/j.cviu.2021.103244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposing a 3D face shape into different attribute components is usually beneficial to many applications, such as 3D face generation and attribute transfer. In this paper, we propose a novel method to learn independent latent representations of 3D face shapes to decompose a given 3D face shape into identity and expression components. We assume that the identity describes the intrinsic geometry of a face while the expression captures the extrinsic one, and thus they are independent of each other. Based on this assumption, we encode a 3D face shape into its identity and expression representations by a variational inference framework, that is equipped with Graph Convolutional Networks (GCN). Furthermore, we introduce a binary discriminator to enforce the latent representations of identity and expression to be distribution independent by adversarial learning. Both qualitative and quantitative experimental results show that the proposed approach can achieve state-of-the-art results in 3D face shape decomposition. Extensive applications on 3D facial expression transfer, 3D face recognition , and 3D face generation further demonstrate that the proposed method can achieve visually better transferred expressions, purer identity representations, and more diverse 3D face shapes, compared with existing state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Cuican Yu and Zihui Zhang and Huibin Li and Jian Sun and Zongben Xu},
  doi          = {10.1016/j.cviu.2021.103244},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103244},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A distribution independence based method for 3D face shape decomposition},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Near-convex decomposition of 2D shape using visibility
range. <em>CVIU</em>, <em>210</em>, 103243. (<a
href="https://doi.org/10.1016/j.cviu.2021.103243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part-based representation plays an important role in many shape related applications, including segmentation, recognition, editing and animation. An issue of particular interest in recent research is decomposing shapes into near-convex parts. However, it is usually challenging for existing methods to handle such heterogeneous real world shapes, especially when they possess long curved branches such as a lizard with a long curved tail. In this study, we propose a novel shape signature named visibility range, and a concavity measure based on this signature to describe the long curved branches. The visibility range reaches low values for points in concave regions and high values in convex regions, acting as the electrical charge distribution on the shape. Using these techniques, we present a coarse-to-fine approximate convex shape decomposition method, which separates the salient parts from the shape first and then refines the decomposition of the remaining main body of the shape by a visibility graph cut process. Qualitative and quantitative experiments have been conducted on shapes with various kinds of near-convex parts, demonstrating that our method captures the long curved branches as contiguous segments and outperforms the state-of-the-art methods that are based on other concave–convex features.},
  archive      = {J_CVIU},
  author       = {Zhiyang Li and Wenyu Qu and Heng Qi and Milos Stojmenovic},
  doi          = {10.1016/j.cviu.2021.103243},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103243},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Near-convex decomposition of 2D shape using visibility range},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Long term spatio-temporal modeling for action detection.
<em>CVIU</em>, <em>210</em>, 103242. (<a
href="https://doi.org/10.1016/j.cviu.2021.103242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling person interactions with their surroundings has proven to be effective for recognizing and localizing human actions in videos. While most recent works focus on learning short term interactions, in this work, we consider long-term person interactions and jointly localize actions of multiple actors over an entire video shot. We construct a graph with nodes that correspond to keyframe actor instances and connect them with two edge types. Spatial edges connect actors within a keyframe, and temporal edges connect multiple instances of the same actor over a video shot. We propose a Graph Neural Network that explicitly models spatial and temporal states for each person instance and learns to effectively combine information from both modalities to make predictions at the same time. We conduct experiments on the AVA dataset and show that our graph-based model provides consistent improvements over several video descriptors, achieving state-of-the-art performance without any fine-tuning.},
  archive      = {J_CVIU},
  author       = {Makarand Tapaswi and Vijay Kumar and Ivan Laptev},
  doi          = {10.1016/j.cviu.2021.103242},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103242},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Long term spatio-temporal modeling for action detection},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel shape matching descriptor for real-time static hand
gesture recognition. <em>CVIU</em>, <em>210</em>, 103241. (<a
href="https://doi.org/10.1016/j.cviu.2021.103241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current state-of-the-art hand gesture recognition methodologies heavily rely in the use of machine learning . However there are scenarios that machine learning cannot be applied successfully, for example in situations where data is scarce. This is the case when one-to-one matching is required between a query and a dataset of hand gestures where each gesture represents a unique class. In situations where learning algorithms cannot be trained, classic computer vision techniques such as feature extraction can be used to identify similarities between objects. Shape is one of the most important features that can be extracted from images, however the most accurate shape matching algorithms tend to be computationally inefficient for real-time applications. In this work we present a novel shape matching methodology for real-time hand gesture recognition. Extensive experiments were carried out comparing our method with other shape matching methods with respect to accuracy and computational complexity . Our method outperforms the other methods and provides a good combination of accuracy and computational efficiency for real-time applications.},
  archive      = {J_CVIU},
  author       = {Michalis Lazarou and Bo Li and Tania Stathaki},
  doi          = {10.1016/j.cviu.2021.103241},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103241},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A novel shape matching descriptor for real-time static hand gesture recognition},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MAIN: Multi-attention instance network for video
segmentation. <em>CVIU</em>, <em>210</em>, 103240. (<a
href="https://doi.org/10.1016/j.cviu.2021.103240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance-level video segmentation requires a solid integration of spatial and temporal information. However, current methods rely mostly on domain-specific information (online learning) to produce accurate instance-level segmentations. We propose a novel approach that relies exclusively on the integration of generic spatio-temporal attention cues. Our strategy, named Multi-Attention Instance Network (MAIN) , overcomes challenging segmentation scenarios over arbitrary videos without modeling sequence- or instance-specific knowledge. We design MAIN to segment multiple instances in a single forward pass, and optimize it with a novel loss function that favors class agnostic predictions and assigns instance-specific penalties. We achieve state-of-the-art performance on the challenging Youtube-VOS dataset and benchmark, improving the unseen Jaccard and F-Metric by 6.8\% and 12.7\% respectively, while operating at real-time (30.3 FPS).},
  archive      = {J_CVIU},
  author       = {Juan León Alcázar and María A. Bravo and Guillaume Jeanneret and Ali K. Thabet and Thomas Brox and Pablo Arbeláez and Bernard Ghanem},
  doi          = {10.1016/j.cviu.2021.103240},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103240},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MAIN: Multi-attention instance network for video segmentation},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction and description of near-future activities in
video. <em>CVIU</em>, <em>210</em>, 103230. (<a
href="https://doi.org/10.1016/j.cviu.2021.103230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing works on human activity analysis focus on recognition or early recognition of the activity labels from complete or partial observations. Similarly, almost all of the existing video captioning approaches focus on the observed events in videos. Predicting the labels and the captions of future activities where no frames of the predicted activities have been observed is a challenging problem, with important applications that require anticipatory response. In this work, we propose a system that can infer the labels and the captions of a sequence of future activities. Our proposed network for label prediction of a future activity sequence has three branches where the first branch takes visual features from the objects present in the scene, the second branch takes observed sequential activity features, and the third branch captures the last observed activity features. The predicted labels and the observed scene context are then mapped to meaningful captions using a sequence-to-sequence learning-based method. Experiments on four challenging activity analysis datasets and a video description dataset demonstrate that our label prediction approach achieves comparable performance with the state-of-the-arts and our captioning framework outperform the state-of-the-arts.},
  archive      = {J_CVIU},
  author       = {Tahmida Mahmud and Mohammad Billah and Mahmudul Hasan and Amit K. Roy-Chowdhury},
  doi          = {10.1016/j.cviu.2021.103230},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103230},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Prediction and description of near-future activities in video},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SID: Incremental learning for anchor-free object detection
via selective and inter-related distillation. <em>CVIU</em>,
<em>210</em>, 103229. (<a
href="https://doi.org/10.1016/j.cviu.2021.103229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental learning requires a model to continually learn new tasks from streaming data. However, traditional fine-tuning of a well-trained deep neural network on a new task will dramatically degrade performance on the old task — a problem known as catastrophic forgetting. In this paper, we address this issue in the context of anchor-free object detection, which is a new trend in computer vision as it is simple, fast, and flexible. Simply adapting current incremental learning strategies fails on these anchor-free detectors due to lack of consideration of their specific model structures. To deal with the challenges of incremental learning on anchor-free object detectors, we propose a novel incremental learning paradigm called Selective and Inter-related Distillation (SID). In addition, a novel evaluation metric is proposed to better assess the performance of detectors under incremental learning conditions. By selective distilling at the proper locations and further transferring additional instance relation knowledge, our method demonstrates significant advantages on the benchmark datasets PASCAL VOC and COCO.},
  archive      = {J_CVIU},
  author       = {Can Peng and Kun Zhao and Sam Maksoud and Meng Li and Brian C. Lovell},
  doi          = {10.1016/j.cviu.2021.103229},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103229},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SID: Incremental learning for anchor-free object detection via selective and inter-related distillation},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-focus image fusion approach based on CNP systems in
NSCT domain. <em>CVIU</em>, <em>210</em>, 103228. (<a
href="https://doi.org/10.1016/j.cviu.2021.103228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coupled neural P (CNP) systems are recently developed distributed and parallel computing models that are abstracted by the mechanisms of coupled and spiking neurons . CNP systems differ from spiking neural P (SNP) systems in two main ways, namely the utilization of three data units, and a coupled firing and dynamic threshold mechanism for neurons. This paper focuses on the application of CNP systems to solve multi-focus image fusion problems, and proposes a novel image fusion approach based on CNP systems. Based on two CNP systems with local topology, a multi-focus image fusion framework in the non-subsampled contourlet transform (NSCT) domain is developed, where the two CNP systems are utilized to control the fusion of low-frequency coefficients in the NSCT domain. The proposed fusion approach is evaluated on an open data set of 19 multi-focus images based on five fusion quality indices, and compared to 11 state-of-the-art fusion approaches. Quantitative and qualitative experimental results demonstrate the advantages of the proposed fusion approach in terms of visual quality and fusion performance.},
  archive      = {J_CVIU},
  author       = {Hong Peng and Bo Li and Qian Yang and Jun Wang},
  doi          = {10.1016/j.cviu.2021.103228},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103228},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-focus image fusion approach based on CNP systems in NSCT domain},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). One-class anomaly detection via novelty normalization.
<em>CVIU</em>, <em>210</em>, 103226. (<a
href="https://doi.org/10.1016/j.cviu.2021.103226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is an important task in many real-world applications, such as within cybersecurity and surveillance. As with most data these days, the size and dimensionality of the data within these fields are constantly growing, which makes it essential to develop an approach that can both accurately and efficiently identify anomalies within these datasets. In this paper, we address the problem of one-class anomaly detection, where after training on a singular class, we try to determine whether or not inputs belong to that said class. Most of the currently existing methods have limitations in which the criterion of the novel class relies solely on the reconstruction error term. We attempt to break away from this restriction by proposing the use of an autoencoder network with a normalization term. We pair this with an additive novelty scoring module during the training procedure as a way to determine the difference between a given image and our determined normal class, therefore improving the efficiency of our model. We evaluate our model on MNIST, CIFAR-10, and Fashion-MNIST, three popular datasets for image classification , and compare the results against other various state-of-the-art models to determine the efficacy of our efforts. Our model not only outperforms the existing methods, but it also gives us a narrower range of AUCs for the tested classes, suggesting a stark improvement in both accuracy and precision. Moreover, we discover that introducing this “Novelty Normalization” concept into our model allows us to expand its usage into multiclass scenarios without a steep drop in accuracy.},
  archive      = {J_CVIU},
  author       = {Jhih-Ciang Wu and Sherman Lu and Chiou-Shann Fuh and Tyng-Luh Liu},
  doi          = {10.1016/j.cviu.2021.103226},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103226},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {One-class anomaly detection via novelty normalization},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep 3D human pose estimation: A review. <em>CVIU</em>,
<em>210</em>, 103225. (<a
href="https://doi.org/10.1016/j.cviu.2021.103225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) human pose estimation involves estimating the articulated 3D joint locations of a human body from an image or video. Due to its widespread applications in a great variety of areas, such as human motion analysis, human–computer interaction, robots, 3D human pose estimation has recently attracted increasing attention in the computer vision community, however, it is a challenging task due to depth ambiguities and the lack of in-the-wild datasets. A large number of approaches, with many based on deep learning , have been developed over the past decade, largely advancing the performance on existing benchmarks. To guide future development, a comprehensive literature review is highly desired in this area. However, existing surveys on 3D human pose estimation mainly focus on traditional methods and a comprehensive review on deep learning based methods remains lacking in the literature. In this paper, we provide a thorough review of existing deep learning based works for 3D pose estimation, summarize the advantages and disadvantages of these methods and provide an in-depth understanding of this area. Furthermore, we also explore the commonly-used benchmark datasets on which we conduct a comprehensive study for comparison and analysis. Our study sheds light on the state of research development in 3D human pose estimation and provides insights that can facilitate the future design of models and algorithms.},
  archive      = {J_CVIU},
  author       = {Jinbao Wang and Shujie Tan and Xiantong Zhen and Shuo Xu and Feng Zheng and Zhenyu He and Ling Shao},
  doi          = {10.1016/j.cviu.2021.103225},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103225},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep 3D human pose estimation: A review},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A linear method for camera pair self-calibration.
<em>CVIU</em>, <em>210</em>, 103223. (<a
href="https://doi.org/10.1016/j.cviu.2021.103223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine 3D reconstruction in unordered sets of uncalibrated images. We introduce a linear method to self-calibrate and find the metric reconstruction of a camera pair. We assume unknown and varying focal lengths but otherwise known internal camera parameters and a known projective reconstruction of the camera pair. We recover two possible camera configurations in space and use the Cheirality condition, that all 3D scene points are in front of both cameras, to disambiguate the solution. Towards identifying camera configurations that would perplex solution disambiguation, we show in two Theorems, first that the two solutions are in mirror positions and then the relations between their viewing directions. We validate our approach in synthetic and real scenes. In camera pair self-calibration and metric reconstruction, our method performs on par (median rotation error Δ R = 3 . 49 ° ΔR=3.49° ) with the standard approach of Kruppa equations followed by 5P algorithm ( Δ R = 3 . 77 ° ΔR=3.77° ). We get realistic multi-view reconstructions, using numerous camera pair metric reconstructions generated by our linear method, rotation-averaging algorithms and a novel method to average focal length estimates.},
  archive      = {J_CVIU},
  author       = {Nikos Melanitis and Petros Maragos},
  doi          = {10.1016/j.cviu.2021.103223},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103223},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A linear method for camera pair self-calibration},
  volume       = {210},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Adversarial deep learning in biometrics
&amp; forensics. <em>CVIU</em>, <em>208-209</em>, 103227. (<a
href="https://doi.org/10.1016/j.cviu.2021.103227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CVIU},
  author       = {Rama Chellappa and Diego Gragnaniello and Chang-Tsun Li and Francesco Marra and Richa Singh},
  doi          = {10.1016/j.cviu.2021.103227},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103227},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Guest editorial: Adversarial deep learning in biometrics &amp; forensics},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stacked capsule graph autoencoders for geometry-aware 3D
head pose estimation. <em>CVIU</em>, <em>208-209</em>, 103224. (<a
href="https://doi.org/10.1016/j.cviu.2021.103224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of image-based 3D head pose estimation is try to estimate the facial direction with 2D images. It is an important attribute widely used in many applications related to faces. However, accurate estimation is hard due to complicated part and pose absence in facial images . Recently, some improvement has been obtained with methods based on neural networks , but most of them ignore the geometric information of facial parts. In this paper, we try to tackle this issue and propose a novel geometry-aware representation. It is based on Stacked Capsule Graph Autoencoders (SCGAE). Different from current methods, we apply Stacked Capsule Autoencoders (SCAE) to encode the parts and poses of facial images. These parts and poses are used to train templates and reconstruct the original faces in decoders. In addition, we improve SCAE with locality loss, in which the inner relationships of similar samples are utilized. To achieve it, graph regularization is applied. In this way, an improved geometry-aware representation can be computed. It is compatible with existing regression methods and experimental results on commonly-used datasets about head pose estimation validate the effectiveness of SCGAE.},
  archive      = {J_CVIU},
  author       = {Chaoqun Hong and Liang Chen and Yuxin Liang and Zhiqiang Zeng},
  doi          = {10.1016/j.cviu.2021.103224},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103224},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Stacked capsule graph autoencoders for geometry-aware 3D head pose estimation},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial for CVIU_DL for image restoration. <em>CVIU</em>,
<em>208-209</em>, 103222. (<a
href="https://doi.org/10.1016/j.cviu.2021.103222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CVIU},
  author       = {Jinshan Pan and Deqing Sun and Jian Yang and Wangmeng Zuo and Paolo Favaro and Yasuyuki Matsushita and Ming-Hsuan Yang},
  doi          = {10.1016/j.cviu.2021.103222},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103222},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Editorial for CVIU_DL for image restoration},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep-STaR: Classification of image time series based on
spatio-temporal representations. <em>CVIU</em>, <em>208-209</em>,
103221. (<a href="https://doi.org/10.1016/j.cviu.2021.103221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image time series (ITS) represent complex 3D (2D+t in practice) data that are now daily produced in various domains, from medical imaging to remote sensing. They contain rich spatio-temporal information allowing the observation of the evolution of a sensed scene over time. In this work, we focus on the classification task of ITS, as often available in remote sensing tasks. An underlying problem here is to consider jointly the spatial and the temporal dimensions of the data. We present Deep-STaR, a method to learn such features from ITS data to proceed to their classification. Instead of reasoning in the original 2D+t space, we investigate novel 2D planar data representations, containing both temporal and spatial information. Such representations are a novel way to structure the ITS, compatible with deep learning architectures. They are used to feed a convolutional neural network to learn spatio-temporal features with 2D convolutions, leading ultimately to classification decision. To enhance the explainability of the results, we also propose a post-hoc attention mechanism , enabled by this new approach, providing a semantic map giving some insights for the taken decision. Deep-STaR is evaluated on a remote sensing application, for the classification of agricultural crops from satellite ITS. The results highlight the benefice of this method, compared to the literature, and its interest to make easier the interpretation of ITS to understand spatio-temporal phenomena.},
  archive      = {J_CVIU},
  author       = {Mohamed Chelali and Camille Kurtz and Anne Puissant and Nicole Vincent},
  doi          = {10.1016/j.cviu.2021.103221},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103221},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep-STaR: Classification of image time series based on spatio-temporal representations},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pruning CNN filters via quantifying the importance of deep
visual representations. <em>CVIU</em>, <em>208-209</em>, 103220. (<a
href="https://doi.org/10.1016/j.cviu.2021.103220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The achievement of convolutional neural networks (CNNs) in a variety of applications is accompanied by a dramatic increase in computational costs and memory requirements. In this paper, we propose a novel framework to measure the importance of individual hidden units by computing a measure of relevance to identify the most critical filters and prune them to compress and accelerate CNNs. Unlike existing methods, we introduce the use of the activation of feature maps to detect valuable information and the essential semantic parts to evaluate the importance of feature maps, inspired by novel neural network interpretability . A majority voting technique based on the degree of alignment between a semantic concept and individual hidden unit representations is proposed to quantitatively evaluate the importance of feature maps. We also propose a simple yet effective method to estimate new convolution kernels based on the remaining, crucial channels to accomplish effective CNN compression. Experimental results show the effectiveness of our filter selection criteria, which outperforms the state-of-the-art baselines. Furthermore, we evaluate our pruning method on CIFAR-10, CUB-200, and ImageNet (ILSVRC 2012) datasets. The experimental results show that the proposed method efficiently achieves a 50\% FLOPs reduction on CIFAR-10, with only 0.86\% accuracy drop on the VGG-16 model. Meanwhile, ResNet pruned on CIFAR-10 achieves a 30\% reduction in FLOPs with only 0.12\% and 0.02\% drops in accuracy on ResNet-20 and ResNet-32 respectively. For ResNet-50 on ImageNet, our pruned model achieves a 50\% reduction in FLOPs with only a top-5 accuracy drop of 0.27\%, which significantly outperforms state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Ali Alqahtani and Xianghua Xie and Mark W. Jones and Ehab Essa},
  doi          = {10.1016/j.cviu.2021.103220},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103220},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pruning CNN filters via quantifying the importance of deep visual representations},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skeleton-based action recognition via spatial and temporal
transformer networks. <em>CVIU</em>, <em>208-209</em>, 103219. (<a
href="https://doi.org/10.1016/j.cviu.2021.103219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based Human Activity Recognition has achieved great interest in recent years as skeleton data has demonstrated being robust to illumination changes, body scales, dynamic camera views, and complex background. In particular, Spatial–Temporal Graph Convolutional Networks (ST-GCN) demonstrated to be effective in learning both spatial and temporal dependencies on non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem, especially when it comes to extracting effective information from joint motion patterns and their correlations. In this work, we propose a novel Spatial–Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self-attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network, whose performance is evaluated on three large-scale datasets, NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving backbone results. Compared with methods that use the same input data, the proposed ST-TR achieves state-of-the-art performance on all datasets when using joints’ coordinates as input, and results on-par with state-of-the-art when adding bones information.},
  archive      = {J_CVIU},
  author       = {Chiara Plizzari and Marco Cannici and Matteo Matteucci},
  doi          = {10.1016/j.cviu.2021.103219},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103219},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Skeleton-based action recognition via spatial and temporal transformer networks},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sejong face database: A multi-modal disguise face database.
<em>CVIU</em>, <em>208-209</em>, 103218. (<a
href="https://doi.org/10.1016/j.cviu.2021.103218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercial application of facial recognition demands robustness to a variety of challenges such as illumination, occlusion, spoofing, disguise, etc. Disguised face recognition is one of the emerging issues for access control systems , such as security checkpoints at the borders. However, the lack of availability of face databases with a variety of disguise add-ons limits the development of academic research in the area. In this paper, we present a multi-modal disguised face dataset to facilitate the disguised face recognition research. The presented database contains 8 facial add-ons and 7 additional combinations of these add-ons to create a variety of disguised face images. Each facial image is captured in visible, visible plus infrared, infrared, and thermal spectra. Specifically, the database contains 100 subjects divided into Subset-A (30 subjects, 1 image per modality) and Subset-B (70 subjects, 5 plus images per modality). We also present baseline face detection results performed on the proposed database to provide reference results and compare the performance in different modalities. Qualitative and quantitative analysis is performed to evaluate the challenging nature of disguise add-ons. The dataset will be publicly available with the acceptance of the research article.},
  archive      = {J_CVIU},
  author       = {Usman Cheema and Seungbin Moon},
  doi          = {10.1016/j.cviu.2021.103218},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103218},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Sejong face database: A multi-modal disguise face database},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards closing the gap in weakly supervised semantic
segmentation with DCNNs: Combining local and global models.
<em>CVIU</em>, <em>208-209</em>, 103209. (<a
href="https://doi.org/10.1016/j.cviu.2021.103209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating training sets for Deep Convolutional Neural Networks (DCNNs) is a bottleneck for modern real-world applications. This is a demanding task for applications where annotating training data is costly, such as in semantic segmentation . In the literature, there is still a gap between the performance achieved by a network trained on full and on weak annotations. In this paper, we establish a simple and natural strategy to measure this gap and to identify the components necessary to reduce it. On scribbles, we establish new state-of-the-art results: we obtain a mIoU of 75.6\% without, and 75.7\% with CRF post-processing. We reduce the gap by 64.2\% whereas the current state-of-the-art reduces it only by 57.5\%. Thanks to a formal reformulation of the weak supervision problem, a systematic study of the different components involved, and an original experimental strategy, we unravel a counter-intuitive mechanism analog to the philosophy of ensemble learning . This strategy is simple and amenable to generalizations to other weakly-supervised scenarios: averaging poor local predicted annotations with a generic naive baseline and reusing them for training a DCNN yields new state-of-the-art results. We show that our strategy accommodates effortlessly other pixel-level weak annotations such as bounding boxes and remains competitive.},
  archive      = {J_CVIU},
  author       = {Christoph Mayer and Radu Timofte and Grégory Paul},
  doi          = {10.1016/j.cviu.2021.103209},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103209},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Towards closing the gap in weakly supervised semantic segmentation with DCNNs: Combining local and global models},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High frame rate optical flow estimation from event sensors
via intensity estimation. <em>CVIU</em>, <em>208-209</em>, 103208. (<a
href="https://doi.org/10.1016/j.cviu.2021.103208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical flow estimation forms the core of several computer vision tasks and its estimation requires accurate spatial and temporal gradient information . However, if there are fast-moving objects in the scene or if the camera moves rapidly, then the acquired images will suffer from motion blur , which will lead to poor optical flow estimation. Such challenging cases can be handled by event sensors which are a novel generation of sensors that acquire pixel-level brightness changes as binary events at a very high temporal resolution. Brightness constancy constraint, which is the basis of several optical flow algorithms cannot be directly used on event sensors making it challenging to estimate optical flow. We overcome this challenge by imposing brightness constancy constraint on intensity images predicted from event sensor data. For this task, we design a recurrent neural network that jointly predicts a sparse optical flow and intensity images from the event data. While intensity estimation is supervised using ground truth frames, optical flow estimation is self-supervised using the predicted intensity frames. However, in our case the temporal resolution of the ground truth intensity frames is far lower than the temporal resolution of the predicted intensity frames, making it challenging to supervise. As we use recurrent neural network, such a challenge can be overcome by sharing the weights for each of the predicted intensity frames. Quantitatively our predicted optical flow is better than previously proposed algorithms for optical flow estimation from event sensors. We also show our algorithm’s robustness against challenging cases of fast motion and high dynamic range scenes.},
  archive      = {J_CVIU},
  author       = {Prasan Shedligeri and Kaushik Mitra},
  doi          = {10.1016/j.cviu.2021.103208},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103208},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {High frame rate optical flow estimation from event sensors via intensity estimation},
  volume       = {208-209},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance-level salient object segmentation. <em>CVIU</em>,
<em>207</em>, 103207. (<a
href="https://doi.org/10.1016/j.cviu.2021.103207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image saliency detection has recently achieved great success due to the development of deep convolutional neural networks . However, most of the existing salient object detection methods cannot identify individual object instances in the detected salient region . In this paper, we present a salient instance segmentation method that produces a saliency map with distinct object instance labels for an input image. Our method consists of three primary steps, i.e. , salient region inference, salient object contours detection, and salient object instances identification. For the first two steps, we propose a multiscale saliency refinement network, which generates high-quality salient region masks and salient object contours. For the last step, we propose a morphology algorithm that incorporates detected salient regions and salient object contours to generate promising salient object instance segmentation results. To promote further research and evaluation of salient instance segmentation, we also construct a new database (ILSO-2K) of 2,000 images with pixel-wise salient instance annotations. Experimental results demonstrate that our proposed method is capable of achieving satisfactory performance over six public benchmarks for salient region detection as well as on our new dataset for salient instance segmentation. The source code and proposed dataset will be public available at https://github.com/Kinpzz/MSRNet-CVIU .},
  archive      = {J_CVIU},
  author       = {Guanbin Li and Pengxiang Yan and Yuan Xie and Guisheng Wang and Liang Lin and Yizhou Yu},
  doi          = {10.1016/j.cviu.2021.103207},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103207},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Instance-level salient object segmentation},
  volume       = {207},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Class knowledge overlay to visual feature learning for
zero-shot image classification. <em>CVIU</em>, <em>207</em>, 103206. (<a
href="https://doi.org/10.1016/j.cviu.2021.103206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New categories can be discovered by transforming semantic features into synthesized visual features without corresponding training samples in zero-shot image classification . Although significant progress has been made in generating high-quality synthesized visual features using generative adversarial networks , guaranteeing semantic consistency between the semantic features and visual features remains very challenging. In this paper, we propose a novel zero-shot learning approach, GAN-CST, based on class knowledge to visual feature learning to tackle the problem. The approach consists of three parts, class knowledge overlay, semi-supervised learning and triplet loss. It applies class knowledge overlay (CKO) to obtain knowledge not only from the corresponding class but also from other classes that have the knowledge overlay. It ensures that the knowledge-to-visual learning process has adequate information to generate synthesized visual features. The approach also applies a semi-supervised learning process to re-train knowledge-to-visual model. It contributes to reinforcing synthesized visual features generation as well as new category prediction. We tabulate results on a number of benchmark datasets demonstrating that the proposed model delivers superior performance over state-of-the-art approaches.},
  archive      = {J_CVIU},
  author       = {Cheng Xie and Ting Zeng and Hongxin Xiang and Keqin Li and Yun Yang and Qing Liu},
  doi          = {10.1016/j.cviu.2021.103206},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103206},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Class knowledge overlay to visual feature learning for zero-shot image classification},
  volume       = {207},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image style disentangling for instance-level facial
attribute transfer. <em>CVIU</em>, <em>207</em>, 103205. (<a
href="https://doi.org/10.1016/j.cviu.2021.103205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance-level facial attribute transfer aims at transferring an attribute including its style from a source face to a target one. Existing studies have limitations on fidelity or correctness. To address this problem, we propose a weakly supervised style disentangling method embedded in Generative Adversarial Network (GAN) for accurate instance-level attribute transfer, using only binary attribute annotations. In our method, the whole attributes transfer process is designed as two steps for easier transfer, which first removes the original attribute or transfers it to a neutral state and then adds the attributes style disentangled from a source face. Moreover, a style disentangling module is proposed to extract the attribute style of an image used in the adding step. Our method aims for accurate attribute style transfer. However, it is also capable of semantic attribute editing as a special case, which is not achievable with existing instance-level attribute transfer methods. Comprehensive experiments on CelebA Dataset show that our method can transfer the style more precisely than existing methods, with an improvement of 39\% in user study, 16.5\% in accuracy, and about 3.3 in FID.},
  archive      = {J_CVIU},
  author       = {Xuyang Guo and Meina Kan and Zhenliang He and Xingguang Song and Shiguang Shan},
  doi          = {10.1016/j.cviu.2021.103205},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103205},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Image style disentangling for instance-level facial attribute transfer},
  volume       = {207},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image retrieval with mixed initiative and multimodal
feedback. <em>CVIU</em>, <em>207</em>, 103204. (<a
href="https://doi.org/10.1016/j.cviu.2021.103204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How would you search for a unique, flamboyant shoe that a friend wore and you want to buy? What if you did not take a picture? Existing approaches propose interactive image search, but they either entrust the user with taking the initiative to provide informative feedback, or give all control to the system which determines informative questions to ask. Instead, we propose a mixed-initiative framework where both the user and system can be active participants, depending on whose input will be more beneficial for obtaining high-quality search results. We develop a reinforcement learning approach which dynamically decides which of four interaction opportunities to give to the user: drawing a sketch, marking images as relevant or not, providing free-form attribute feedback, or answering attribute-based questions. By allowing these four options, our system optimizes both the informativeness of feedback, and the ability of the user to explore the data, allowing faster image retrieval . We outperform five baselines on three datasets under extensive settings.},
  archive      = {J_CVIU},
  author       = {Nils Murrugarra-Llerena and Adriana Kovashka},
  doi          = {10.1016/j.cviu.2021.103204},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103204},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Image retrieval with mixed initiative and multimodal feedback},
  volume       = {207},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluate and improve the quality of neural style transfer.
<em>CVIU</em>, <em>207</em>, 103203. (<a
href="https://doi.org/10.1016/j.cviu.2021.103203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have made tremendous progress in neural style transfer (NST) and various methods have been advanced. However, evaluating and improving the stylization quality remain two important open challenges. Committed to these two aspects, in this paper, we first decompose the quality of style transfer into three quantifiable factors, i.e., the content fidelity (CF), global effects (GE) and local patterns (LP). Then, two novel approaches are further presented for exploiting these factors to improve the stylization quality. The first, named cascade style transfer (CST), utilizes the factors to guide the cascade combination of existing NST methods to absorb their merits and avoid their own shortcomings. The second, dubbed multi-objective network (MO-Net), directly optimizes these factors to balance their performance and achieves more harmonious stylized results. Extensive experiments demonstrate the effectiveness and superiority of our proposed factors and methods.},
  archive      = {J_CVIU},
  author       = {Zhizhong Wang and Lei Zhao and Haibo Chen and Zhiwen Zuo and Ailin Li and Wei Xing and Dongming Lu},
  doi          = {10.1016/j.cviu.2021.103203},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103203},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Evaluate and improve the quality of neural style transfer},
  volume       = {207},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single image super-resolution via hybrid resolution NSST
prediction. <em>CVIU</em>, <em>207</em>, 103202. (<a
href="https://doi.org/10.1016/j.cviu.2021.103202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved great success in single image super-resolution (SR). However, most previous methods predict high-resolution (HR) images in the spatial domain, producing over-smoothed outputs while losing texture details. To address this problem, in this paper we propose to predict nonsubsampled shearlet transform (NSST) coefficients, which better represent the global topology information and local texture details of HR images. On the other hand, we propose a deep hybrid resolution network by a residual-in-residual style, which aggregates features of multiple resolutions so as to gather rich context information in compact representations . When evaluated on a newly released RealSR dataset and traditional simulated datasets, our method, namely hybrid resolution NSST prediction (HRNP), achieves more appealing results, w.r.t. PSNR and SSIM, than the state-of-the-art methods. Moreover, we find our HRNP is more capable of preserving complex edges and curves than other methods.},
  archive      = {J_CVIU},
  author       = {Yunan Liu and Shanshan Zhang and Chunpeng Wang and Jie Xu},
  doi          = {10.1016/j.cviu.2021.103202},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103202},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Single image super-resolution via hybrid resolution NSST prediction},
  volume       = {207},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time and accurate object detection in compressed video
by long short-term feature aggregation. <em>CVIU</em>, <em>206</em>,
103188. (<a href="https://doi.org/10.1016/j.cviu.2021.103188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection is a fundamental problem in computer vision and has a wide spectrum of applications. Based on deep networks, video object detection is actively studied for pushing the limits of detection speed and accuracy. To reduce the computation cost, we sparsely sample key frames in video and treat the rest frames are non-key frames; a large and deep network is used to extract features for key frames and a tiny network is used for non-key frames. To enhance the features of non-key frames, we propose a novel short-term feature aggregation method to propagate the rich information in key frame features to non-key frame features in a fast way. The fast feature aggregation is enabled by the freely available motion cues in compressed videos. Further, key frame features are also aggregated based on optical flow. The propagated deep features are then integrated with the directly extracted features for object detection. The feature extraction and feature integration parameters are optimized in an end-to-end manner. The proposed video object detection network is evaluated on the large-scale ImageNet VID benchmark and achieves 77.2\% mAP, which is on-par with the state-of-the-art accuracy, at the speed of 30 FPS using a Titan X GPU . The source codes are available at https://github.com/hustvl/LSFA .},
  archive      = {J_CVIU},
  author       = {Xinggang Wang and Zhaojin Huang and Bencheng Liao and Lichao Huang and Yongchao Gong and Chang Huang},
  doi          = {10.1016/j.cviu.2021.103188},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103188},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Real-time and accurate object detection in compressed video by long short-term feature aggregation},
  volume       = {206},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video action detection by learning graph-based
spatio-temporal interactions. <em>CVIU</em>, <em>206</em>, 103187. (<a
href="https://doi.org/10.1016/j.cviu.2021.103187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action Detection is a complex task that aims to detect and classify human actions in video clips. Typically, it has been addressed by processing fine-grained features extracted from a video classification backbone. Recently, thanks to the robustness of object and people detectors, a deeper focus has been added on relationship modeling. Following this line, we propose a graph-based framework to learn high-level interactions between people and objects, in both space and time. In our formulation, spatio-temporal relationships are learned through self-attention on a multi-layer graph structure which can connect entities from consecutive clips, thus considering long-range spatial and temporal dependencies. The proposed module is backbone independent by design and does not require end-to-end training. Extensive experiments are conducted on the AVA dataset, where our model demonstrates state-of-the-art results and consistent improvements over baselines built with different backbones. Code is publicly available at https://github.com/aimagelab/STAGE_action_detection .},
  archive      = {J_CVIU},
  author       = {Matteo Tomei and Lorenzo Baraldi and Simone Calderara and Simone Bronzin and Rita Cucchiara},
  doi          = {10.1016/j.cviu.2021.103187},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103187},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video action detection by learning graph-based spatio-temporal interactions},
  volume       = {206},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human action recognition in drone videos using a few aerial
training examples. <em>CVIU</em>, <em>206</em>, 103186. (<a
href="https://doi.org/10.1016/j.cviu.2021.103186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drones are enabling new forms of human actions surveillance due to their low cost and fast mobility. However, using deep neural networks for automatic aerial action recognition is difficult due to the need for a large number of training aerial human action videos. Collecting a large number of human action aerial videos is costly, time-consuming, and difficult. In this paper, we explore two alternative data sources to improve aerial action classification when only a few training aerial examples are available. As a first data source, we resort to video games. We collect plenty of aerial game action videos using two gaming engines. For the second data source, we leverage conditional Wasserstein Generative Adversarial Networks to generate aerial features from ground videos. Given that both data sources have some limitations, e.g. game videos are biased towards specific actions categories (fighting, shooting, etc.,), and it is not easy to generate good discriminative GAN-generated features for all types of actions, we need to efficiently integrate two dataset sources with few available real aerial training videos. To address this challenge of the heterogeneous nature of the data, we propose to use a disjoint multitask learning framework. We feed the network with real and game, or real and GAN-generated data in an alternating fashion to obtain an improved action classifier . We validate the proposed approach on two aerial action datasets and demonstrate that features from aerial game videos and those generated from GAN can be extremely useful for an improved action recognition in real aerial videos when only a few real aerial training examples are available.},
  archive      = {J_CVIU},
  author       = {Waqas Sultani and Mubarak Shah},
  doi          = {10.1016/j.cviu.2021.103186},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103186},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Human action recognition in drone videos using a few aerial training examples},
  volume       = {206},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-image registration of a building’s facade based on
dense semantic segmentation. <em>CVIU</em>, <em>206</em>, 103185. (<a
href="https://doi.org/10.1016/j.cviu.2021.103185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an efficient approach for accurate registration of a building facade model “dressed” with dense semantic information. Localization sensors such as the GPS as well as vision-based methods are able to provide a camera pose in an efficient and stable way, but at the expense of low accuracy. We propose here to rely on semantic maps to improve the accuracy of a rough camera pose. Simultaneously we aim to iteratively improve the quality of the semantic map through the registration. Registration and semantic segmentation are jointly refined in an Expectation–Maximization framework. We especially introduce a Bayesian model that uses prior semantic segmentation as well as geometric structure of the facade reference modeled by Generalized Gaussian Mixtures . We show the advantages of our method in terms of robustness to clutter and change of illumination on urban images from various databases.},
  archive      = {J_CVIU},
  author       = {Antoine Fond and Marie-Odile Berger and Gilles Simon},
  doi          = {10.1016/j.cviu.2021.103185},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103185},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Model-image registration of a building’s facade based on dense semantic segmentation},
  volume       = {206},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to locate for fine-grained image recognition.
<em>CVIU</em>, <em>206</em>, 103184. (<a
href="https://doi.org/10.1016/j.cviu.2021.103184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an end-to-end weakly supervised method for fine-grained image recognition called bounding box-part location method(BBPL), which can locate the object and part precisely without part annotations. The proposed method includes three modules: object detection, ObjectMask, and classification. Firstly, the object detection module predicts the bounding boxes, and the predicted bounding boxes are employed to generate a mask through ObjectMask module. The generated mask can suppress the background interference during recognition. Secondly, the classification module can be further divided into two branches, which are global feature classification and local feature classification. In global feature classification branch, global feature is extracted to get global classification result . While in local feature classification branch, salient point is first detected through our novel salient point detection module, which can greatly reduce the consuming-time compared with the most existing local feature extraction methods. Further, the local feature is extracted in these detected salient points, and local classification result is obtained by local feature classification branch. Finally, we get the final result by fusing the results of two classification branches together. With experiments on three widely used fine-grained image recognition datasets (CUB-200-2011, Stanford Cars, Stanford Dogs), our method can achieve the state-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Jiamin Chen and Jianguo Hu and Shiren Li},
  doi          = {10.1016/j.cviu.2021.103184},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103184},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning to locate for fine-grained image recognition},
  volume       = {206},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detail preserving image denoising with patch-based structure
similarity via sparse representation and SVD. <em>CVIU</em>,
<em>206</em>, 103173. (<a
href="https://doi.org/10.1016/j.cviu.2021.103173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key problem of image denoising methods is to smooth noise while retaining the details of original image. The human vision system is more sensitive to the details (or the high frequency components) of original image, hence the restoration of image details ensures the good quality of denoised image . Different from denoising the image as a whole, this paper proposes a novel denoising method that reconstructs the high and low frequency components respectively. The sparse representation using patch-based structure similarity is proposed to reconstruct the high frequency parts. And the low frequency parts are reconstructed by singular value decomposition (SVD). Finally an energy minimization function that contains high and low frequency parts are presented. Experimental results illustrate that the proposed method is outstanding in both numerical precision and visual performance.},
  archive      = {J_CVIU},
  author       = {Miaowen Shi and Fan Zhang and Suwei Wang and Caiming Zhang and Xuemei Li},
  doi          = {10.1016/j.cviu.2021.103173},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103173},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Detail preserving image denoising with patch-based structure similarity via sparse representation and SVD},
  volume       = {206},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physics-based shading reconstruction for intrinsic image
decomposition. <em>CVIU</em>, <em>205</em>, 103183. (<a
href="https://doi.org/10.1016/j.cviu.2021.103183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the use of photometric invariance and deep learning to compute intrinsic images (albedo and shading). We propose albedo and shading gradient descriptors which are derived from physics-based models. Using the descriptors, albedo transitions are masked out and an initial sparse shading map is calculated directly from the corresponding R G B RGB image gradients in a learning-free unsupervised manner . Then, an optimization method is proposed to reconstruct the full dense shading map. Finally, we integrate the generated shading map into a novel deep learning framework to refine it and also to predict corresponding albedo image to achieve intrinsic image decomposition. By doing so, we are the first to directly address the texture and intensity ambiguity problems of the shading estimations. Large scale experiments show that our approach steered by physics-based invariant descriptors achieve superior results on MIT Intrinsics, NIR-RGB Intrinsics, Multi-Illuminant Intrinsic Images, Spectral Intrinsic Images, As Realistic As Possible, and competitive results on Intrinsic Images in the Wild datasets while achieving state-of-the-art shading estimations.},
  archive      = {J_CVIU},
  author       = {Anil S. Baslamisli and Yang Liu and Sezer Karaoglu and Theo Gevers},
  doi          = {10.1016/j.cviu.2021.103183},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103183},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Physics-based shading reconstruction for intrinsic image decomposition},
  volume       = {205},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Person re-identification with part prediction alignment.
<em>CVIU</em>, <em>205</em>, 103172. (<a
href="https://doi.org/10.1016/j.cviu.2021.103172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to success of person re-identification(re-id) is extracting the discriminative person features. Various part-level feature extraction methods are proposed to capture local person features for re-id. A prerequisite of part feature extraction is that each part should be well located. We believe that ID predictions in different parts of the same image should be consistent. Instead of using the external dataset and pose estimator for guiding, we propose a re-id model with Part Prediction Alignment (PPA), which aims at aligning the predicted distributions between each part. Due to the global feature and local feature contains different spacial information, we consider that the combination of two sides will further improve the detection effect. Therefore, in this paper we adopt the teacher–student training strategy based on PPA for global–local feature extraction, and the global feature extraction branch as a teacher to guide the training of local feature branch. Experimental results on Market-1501, DukeMTMC-reID and CUHK03 (including CUHK03_Detected and CUHK03_Labeled) datasets confirm the effectiveness of our proposal, we achieve Rank1 with 92.4\%, 85.1\%, 65.5\%, 69.2\% on Market-1501, DukeMTMC-reID, CUHK03_Detected and CUHK03_Labeled, respectively.},
  archive      = {J_CVIU},
  author       = {Zhiyong Li and Jingyi Lv and Ying Chen and Jin Yuan},
  doi          = {10.1016/j.cviu.2021.103172},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103172},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Person re-identification with part prediction alignment},
  volume       = {205},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Facial landmarks localization using cascaded neural
networks. <em>CVIU</em>, <em>205</em>, 103171. (<a
href="https://doi.org/10.1016/j.cviu.2021.103171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate localization of facial landmarks is at the core of several face analysis tasks, such as face recognition and facial expression analysis, to name a few. In this work, we propose a novel localization approach based on a deep learning architecture that utilizes two paired cascaded subnetworks with convolutional neural network units. The cascaded units of the first subnetwork estimate heatmap-based encodings of the landmarks’ locations, while the cascaded units of the second subnetwork receive as inputs the outputs of the corresponding heatmap estimation units, and refine them through regression. The proposed scheme is experimentally shown to compare favorably with contemporary state-of-the-art schemes, especially when applied to images depicting challenging localization conditions.},
  archive      = {J_CVIU},
  author       = {Shahar Mahpod and Rig Das and Emanuele Maiorana and Yosi Keller and Patrizio Campisi},
  doi          = {10.1016/j.cviu.2021.103171},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103171},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Facial landmarks localization using cascaded neural networks},
  volume       = {205},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive deep network for blind motion deblurring on
dynamic scenes. <em>CVIU</em>, <em>205</em>, 103169. (<a
href="https://doi.org/10.1016/j.cviu.2021.103169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-uniform blind motion deblurring is a challenging yet important problem in image processing that receives enduring attention in the last decade. The non-uniformity nature of motion blurring leads to great variations on the blurring effects across image regions and over different images, which makes it very difficult to train an end-to-end deblurring neural network (NN) with good generalization performance . This paper introduces an attention mechanism for the blind deblurring NN, including both spatial and channel attention, so as to effectively handle the significant spatial variations on blurring effects. In the attention mechanism, the spatial attention is introduced in both the encoder for discriminative exploitation of image edges and smooth regions and the decoder for discriminative treatment on different regions with different blurring effects. The channel attention is introduced for better generalization performance of the NN, as it allows adaptive weighting on intermediate features for a particular image. Building such an attention mechanism into a multi-scale encoder–decoder framework, an attentive NN is developed for practical non-uniform blind image deblurring. The experiments on several benchmark datasets show that the proposed NN can effectively restore the images degraded by spatially-varying blurring, with state-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Yong Xu and Ye Zhu and Yuhui Quan and Hui Ji},
  doi          = {10.1016/j.cviu.2021.103169},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103169},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Attentive deep network for blind motion deblurring on dynamic scenes},
  volume       = {205},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge distillation for incremental learning in semantic
segmentation. <em>CVIU</em>, <em>205</em>, 103167. (<a
href="https://doi.org/10.1016/j.cviu.2021.103167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning architectures have shown remarkable results in scene understanding problems, however they exhibit a critical drop of performances when they are required to learn incrementally new tasks without forgetting old ones. This catastrophic forgetting phenomenon impacts on the deployment of artificial intelligence in real world scenarios where systems need to learn new and different representations over time. Current approaches for incremental learning deal only with image classification and object detection tasks, while in this work we formally introduce incremental learning for semantic segmentation . We tackle the problem applying various knowledge distillation techniques on the previous model. In this way, we retain the information about learned classes, whilst updating the current model to learn the new ones. We developed four main methodologies of knowledge distillation working on both output layers and internal feature representations. We do not store any image belonging to previous training stages and only the last model is used to preserve high accuracy on previously learned classes. Extensive experimental results on the Pascal VOC2012 and MSRC-v2 datasets show the effectiveness of the proposed approaches in several incremental learning scenarios.},
  archive      = {J_CVIU},
  author       = {Umberto Michieli and Pietro Zanuttigh},
  doi          = {10.1016/j.cviu.2021.103167},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103167},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Knowledge distillation for incremental learning in semantic segmentation},
  volume       = {205},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fake face detection via adaptive manipulation traces
extraction network. <em>CVIU</em>, <em>204</em>, 103170. (<a
href="https://doi.org/10.1016/j.cviu.2021.103170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of face image manipulation (FIM) techniques such as Face2Face and Deepfake , more fake face images are spreading over the internet, which brings serious challenges to public confidence. Face image forgery detection has made considerable progresses in exposing specific FIM, but it is still in scarcity of a robust fake face detector to expose face image forgeries under complex scenarios such as with further compression, blurring, scaling, etc. Due to the relatively fixed structure, convolutional neural network (CNN) tends to learn image content representations. However, CNN should learn subtle manipulation traces for image forensics tasks. Thus, we propose an adaptive manipulation traces extraction network (AMTEN), which serves as pre-processing to suppress image content and highlight manipulation traces. AMTEN exploits an adaptive convolution layer to predict manipulation traces in the image, which are reused in subsequent layers to maximize manipulation artifacts by updating weights during the back-propagation pass. A fake face detector, namely AMTENnet, is constructed by integrating AMTEN with CNN. Experimental results prove that the proposed AMTEN achieves desirable pre-processing. When detecting fake face images generated by various FIM techniques, AMTENnet achieves an average accuracy up to 98.52\%, which outperforms the state-of-the-art works. When detecting face images with unknown post-processing operations, the detector also achieves an average accuracy of 95.17\%.},
  archive      = {J_CVIU},
  author       = {Zhiqing Guo and Gaobo Yang and Jiyou Chen and Xingming Sun},
  doi          = {10.1016/j.cviu.2021.103170},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103170},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fake face detection via adaptive manipulation traces extraction network},
  volume       = {204},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DenseNet-CTC: An end-to-end RNN-free architecture for
context-free string recognition. <em>CVIU</em>, <em>204</em>, 103168.
(<a href="https://doi.org/10.1016/j.cviu.2021.103168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {String recognition is one of the challenging tasks in document analysis and recognition areas. Recently, with the surge of interest in end-to-end segmentation-free methods, CRNN (Convolution Recurrent Neural Network), which is a combination of CNN (Convolutional Neural Network) and RNN-CTC (Recurrent Neural Network-Connectionist Temporal Classification), has been widely applied to string recognition. However, in some context-free cases, where a character is followed by arbitrary characters like the digit string, there may be no or very few context links in these strings. In this paper, we propose a new end-to-end RNN-free architecture especially for context-free string recognition and apply it to Handwritten Digit String Recognition (HDSR) task. The proposed architecture is based on CNN and CTC, but without the usage of RNN, and we apply column-wise fully connected layers to connect the convolutional layers and CTC directly. Moreover, to compensate for the possible reduction in modeling capabilities caused by the absence of RNN, we apply densely connected convolutional layers to extract efficient features. We test this new architecture on three public HDSR benchmarks (ORAND-CAR-A, ORAND-CAR-B and CVL HDS) and three other datasets that include a handwritten telephone/postcode dataset PhPAIS and two non-Arabic digit datasets (C-Bangla and C-Hindi). Furthermore, we generate three handwritten digit string datasets to further analyze the influence of RNN. The recognition results on all datasets demonstrate the superiority of the proposed model.},
  archive      = {J_CVIU},
  author       = {Hongjian Zhan and Shujing Lyu and Yue Lu and Umapada Pal},
  doi          = {10.1016/j.cviu.2021.103168},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103168},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DenseNet-CTC: An end-to-end RNN-free architecture for context-free string recognition},
  volume       = {204},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Curriculum self-paced learning for cross-domain object
detection. <em>CVIU</em>, <em>204</em>, 103166. (<a
href="https://doi.org/10.1016/j.cviu.2021.103166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training (source) domain bias affects state-of-the-art object detectors, such as Faster R-CNN, when applied to new (target) domains. To alleviate this problem, researchers proposed various domain adaptation methods to improve object detection results in the cross-domain setting, e.g. by translating images with ground-truth labels from the source domain to the target domain using Cycle-GAN. On top of combining Cycle-GAN transformations and self-paced learning in a smart and efficient way, in this paper, we propose a novel self-paced algorithm that learns from easy to hard. Our method is simple and effective, without any overhead during inference. It uses only pseudo-labels for samples taken from the target domain, i.e. the domain adaptation is unsupervised. We conduct experiments on four cross-domain benchmarks, showing better results than the state of the art. We also perform an ablation study demonstrating the utility of each component in our framework. Additionally, we study the applicability of our framework to other object detectors. Furthermore, we compare our difficulty measure with other measures from the related literature, proving that it yields superior results and that it correlates well with the performance metric.},
  archive      = {J_CVIU},
  author       = {Petru Soviany and Radu Tudor Ionescu and Paolo Rota and Nicu Sebe},
  doi          = {10.1016/j.cviu.2021.103166},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103166},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Curriculum self-paced learning for cross-domain object detection},
  volume       = {204},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-perspective cross-class domain adaptation for open
logo detection. <em>CVIU</em>, <em>204</em>, 103156. (<a
href="https://doi.org/10.1016/j.cviu.2020.103156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing logo detection methods mostly rely on supervised learning with a large quantity of labelled training data in limited classes. This restricts their scalability to a large number of logo classes subject to limited labelling budget. In this work, we consider a more scalable open logo detection problem where only a fraction of logo classes are fully labelled whilst the remaining classes are only annotated with a clean icon image (e.g. 1-shot icon supervised). To generalise and transfer knowledge of fully supervised logo classes to other 1-shot icon supervised classes, we propose a Multi-Perspective Cross-Class (MPCC) domain adaptation method. In a data augmentation principle, MPCC conducts feature distribution alignment in two perspectives. Specifically, we align the feature distribution between synthetic logo images of 1-shot icon supervised classes and genuine logo images of fully supervised classes, and that between logo images and non-logo images, concurrently. This allows for mitigating the domain shift problem between model training and testing on 1-shot icon supervised logo classes, simultaneously reducing the model overfitting towards fully labelled logo classes. Extensive comparative experiments show the advantage of MPCC over existing state-of-the-art competitors on the challenging QMUL-OpenLogo benchmark (Su et al., 2018).},
  archive      = {J_CVIU},
  author       = {Hang Su and Shaogang Gong and Xiatian Zhu},
  doi          = {10.1016/j.cviu.2020.103156},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103156},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-perspective cross-class domain adaptation for open logo detection},
  volume       = {204},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale attention network for image inpainting.
<em>CVIU</em>, <em>204</em>, 103155. (<a
href="https://doi.org/10.1016/j.cviu.2020.103155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning based inpainting methods have shown promising performance, in which some multi-scale networks are introduced to characterize image content in both details and structures. However, few of these networks explore local spatial components under different receptive fields and internal connection between multi-scale feature maps. In this paper, we propose a novel multi-scale attention network (MSA-Net) to fill the irregular missing regions, in which a multi-scale attention group (MSAG) with several multi-scale attention units (MSAUs) is introduced for fully analysing the features from shallow details to high-level semantics. In each MSAU, an attention based spatial pyramid structure is designed to capture the deep features from different receptive fields. In this network, attention mechanisms are explored for boosting the representation power of MSAU, where spatial attention is combined with each scale to highlight the most probably attentive spatial components and the channel attention is used as a globally semantic detector to build the connection between the multiple scales. Furthermore, for better inpainting results, a max pooling based mask update method is utilized to predict the missing parts from the border regions to the inside. Finally, experiments on Places2 dataset and CelebA dataset demonstrate that the proposed method can achieve better results than the previous inpainting methods.},
  archive      = {J_CVIU},
  author       = {Jia Qin and Huihui Bai and Yao Zhao},
  doi          = {10.1016/j.cviu.2020.103155},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103155},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-scale attention network for image inpainting},
  volume       = {204},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task dependent deep LDA pruning of neural networks.
<em>CVIU</em>, <em>203</em>, 103154. (<a
href="https://doi.org/10.1016/j.cviu.2020.103154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With deep learning’s success, a limited number of popular deep nets have been widely adopted for various vision tasks. However, this usually results in unnecessarily high complexities and possibly many features of low task utility. In this paper, we address this problem by introducing a task-dependent deep pruning framework based on Fisher’s Linear Discriminant Analysis (LDA). The approach can be applied to convolutional, fully-connected, and module-based deep network structures, in all cases leveraging the high decorrelation of neuron motifs found in the pre-decision space and cross-layer deconv dependency. Moreover, we examine our approach’s potential in network architecture search for specific tasks and analyze the influence of our pruning on model robustness to noises and adversarial attacks . Experimental results on datasets of generic objects (ImageNet, CIFAR100) as well as domain specific tasks (Adience, and LFWA) illustrate our framework’s superior performance over state-of-the-art pruning approaches and fixed compact nets (e.g. SqueezeNet, MobileNet). The proposed method successfully maintains comparable accuracies even after discarding most parameters (98\%–99\% for VGG16, up to 82\% for the already compact InceptionNet) and with significant FLOP reductions (83\% for VGG16, up to 64\% for InceptionNet). Through pruning, we can also derive smaller, but more accurate and more robust models suitable for the task.},
  archive      = {J_CVIU},
  author       = {Qing Tian and Tal Arbel and James J. Clark},
  doi          = {10.1016/j.cviu.2020.103154},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103154},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Task dependent deep LDA pruning of neural networks},
  volume       = {203},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive review of past and present image inpainting
methods. <em>CVIU</em>, <em>203</em>, 103147. (<a
href="https://doi.org/10.1016/j.cviu.2020.103147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images can be described as visual representations or likeness of something (person or object) which can be reproduced or captured, e.g. a hand drawing, photographic material. However, for images on photographic material, images can have defects at the point of captured, become damaged, or degrade over time. Historically, these were restored by hand to maintain image quality using a process known as inpainting. The advent of the digital age has seen the rapid shift image storage technologies, from hard-copies to digitalised units in a less burdensome manner with the application of digital tools. This paper presents a comprehensive review of image inpainting methods over the past decade and the commonly used performance metrics and datasets. To increase the clarity of our review, we use a hierarchical representation for the past state-of-the-art traditional methods and the present state-of-the-art deep learning methods. For traditional methods, we divide the techniques into five sub-categories, i.e. Exemplar-based texture synthesis, Exemplar-based structure synthesis, Diffusion-based methods, Sparse representation methods and Hybrid methods . Then we review the deep learning methods, i.e. Convolutional Neural Networks and Generative Adversarial Networks . We detail the strengths and weaknesses of each to provide new insights in the field. To address the challenges raised from our findings, we outline some potential future works.},
  archive      = {J_CVIU},
  author       = {Jireh Jam and Connah Kendrick and Kevin Walker and Vincent Drouard and Jison Gee-Sern Hsu and Moi Hoon Yap},
  doi          = {10.1016/j.cviu.2020.103147},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103147},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A comprehensive review of past and present image inpainting methods},
  volume       = {203},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video deblurring via spatiotemporal pyramid network and
adversarial gradient prior. <em>CVIU</em>, <em>203</em>, 103135. (<a
href="https://doi.org/10.1016/j.cviu.2020.103135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video deblurring is to restore sharp frames from a blurry sequence. It is a challenging low-level vision task because the blur caused by camera shake, object motions and depth variations is heterogeneous in both spatial and temporal dimensions. Traditional methods usually work on a fixed spatiotemporal scale. However, the spatiotemporal scale of blurs in the video can vastly vary in the real-world situation. To address this challenge, we propose a Spatiotemporal Pyramid Network (SPN) to dynamically learn different spatiotemporal cues for video deblurring. Specifically, inside SPN, a spatiotemporal pyramid module is employed to effectively capture both spatial information and temporal information from the blurry sequence in a pyramid mode. An image reconstruction module constructs the sharp center frame through the obtained spatiotemporal information. Additionally, inspired by the statistical image prior and adversarial learning, we extend SPN and propose a Spatiotemporal Pyramid Generative Adversarial Network (SPGAN), which conducts adversarial discrimination in the gradient space. It helps the network produce more realistic sharp video frames. Experiments conducted on benchmarks demonstrate that the proposed methods achieve state-of-the-art results in terms of PSNR, SSIM and visual quality.},
  archive      = {J_CVIU},
  author       = {Tao Wang and Xiaoqin Zhang and Runhua Jiang and Li Zhao and Huiling Chen and Wenhan Luo},
  doi          = {10.1016/j.cviu.2020.103135},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103135},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video deblurring via spatiotemporal pyramid network and adversarial gradient prior},
  volume       = {203},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-image deblurring with neural networks: A comparative
survey. <em>CVIU</em>, <em>203</em>, 103134. (<a
href="https://doi.org/10.1016/j.cviu.2020.103134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) are becoming the tool of choice for sharpening blurred images. We discuss and categorize deblurring NNs. Then we evaluate seven NNs for non-blind deblurring (NBD), and seven NNs and four optimization techniques for blind deblurring (BD). To do this we use several current datasets containing pairs of sharp and blurred images, synthesized either by convolving sharp images with blur kernels or by averaging consecutive sharp images, so as to produce both uniform and non-uniform blurs. We also introduce a newly reorganized benchmark dataset in which blurred images have been classified using attributes that depend on the extent of the blur. We use this dataset to compare the effectiveness of single and multi-scale training in coping with large blurs. On NBD, NNs that use regularization with a denoising prior network outperform other denoising NNs; and NNs that use a deep image prior network outperform other deconvolution NNs. On BD, NNs outperform optimizations in signal-difference terms, but not in terms of perceptual fidelity. We found that multi-scale training helps NNs to deal with large blurs, and RNNs outperform CNNs . We also observed that GANs using a perceptual loss function produce artifacts; but also that some form of perceptual fidelity loss is required to get the best results from NNs. We contend that the domain bias of current datasets works against robustness and generality. And we discuss the potential of more sophisticated perceptual loss functions, attention techniques, and unsupervised learning .},
  archive      = {J_CVIU},
  author       = {Jaihyun Koh and Jangho Lee and Sungroh Yoon},
  doi          = {10.1016/j.cviu.2020.103134},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103134},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Single-image deblurring with neural networks: A comparative survey},
  volume       = {203},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ICycleGAN: Single image dehazing based on iterative dehazing
model and CycleGAN. <em>CVIU</em>, <em>203</em>, 103133. (<a
href="https://doi.org/10.1016/j.cviu.2020.103133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current competitive approaches to restoring haze-free images are mainly based on physical models and learning methods. Maintaining detail information of the image while thoroughly removing fog is a challenging task in single-image dehazing. In this paper, by embedding an iterative dehazing model into the generative process of the Cycle-Consistent Adversarial Network (CycleGAN), we propose a model named ICycleGAN that maintains the defogging thoroughness of the learning-based dehazing method while retaining the good fidelity of the physical model-based dehazing method. Moreover, the proposed ICycleGAN does not require pairs of hazy and relative haze-free images for training. In addition, we develop a detail information-consistency loss that preserves more textural details and color information; this loss is obtained based on the physical features of the hazy image. To recover the high-resolution images, we enlarge the generated images using rational fractal interpolation, which restores fine textures and sharp edges. Extensive comparison results show that the proposed method produces high-quality clear images that are both quantitatively and qualitatively competitive with other state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Ziyi Sun and Yunfeng Zhang and Fangxun Bao and Kai Shao and Xinxin Liu and Caiming Zhang},
  doi          = {10.1016/j.cviu.2020.103133},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103133},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {ICycleGAN: Single image dehazing based on iterative dehazing model and CycleGAN},
  volume       = {203},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedding group and obstacle information in LSTM networks
for human trajectory prediction in crowded scenes. <em>CVIU</em>,
<em>203</em>, 103126. (<a
href="https://doi.org/10.1016/j.cviu.2020.103126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks have shown good abilities in learning the spatio-temporal dependencies of moving agents in crowded scenes. Recently, they have been adopted to predict the motion of pedestrians by learning the relative motion of each individual in the crowd with respect to its neighbors. Crowded scenes present a wide variety of situations, which do not depend solely on the agents’ positions, but also relate to the structure of the environment, the density of the crowd, and the social relationships between pedestrians. In this work we propose a framework to improve the state-of-the-art models of crowd motion prediction by enriching the learning model with the social relationships between pedestrians walking in the crowd, as well as the layout of the environment. We observe that socially-related people tend to exhibit coherent motion patterns. Exploiting the motion coherency , we are able to cluster trajectories with similar motion properties and improve the trajectory prediction, especially at the group level. Furthermore, we incorporate into the model also the layout of the environment, to guarantee a more realistic and reliable learning framework. We evaluate our approach on standard crowd benchmark datasets, demonstrating its efficacy and applicability, improving the accuracy in trajectory prediction.},
  archive      = {J_CVIU},
  author       = {Niccoló Bisagno and Cristiano Saltori and Bo Zhang and Francesco G.B. De Natale and Nicola Conci},
  doi          = {10.1016/j.cviu.2020.103126},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103126},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Embedding group and obstacle information in LSTM networks for human trajectory prediction in crowded scenes},
  volume       = {203},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Small and accurate heatmap-based face alignment via
distillation strategy and cascaded architecture. <em>CVIU</em>,
<em>203</em>, 103125. (<a
href="https://doi.org/10.1016/j.cviu.2020.103125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite face alignment has made significant progress, it is still challenging to get a small and accurate face landmark detection model. In this paper, we focus on compressing heatmap-based face alignment algorithms using a novel proposed distillation strategy. We find that the activated areas of heatmaps generated from the well-trained teacher net can capture more local shape information of the facial parts than the ground-truth ones generated from the standard Gaussian distribution. To simultaneously transfer such shape information and correct the mis-predicted heatmaps generated from the teacher model, we first modify the heatmaps of teacher model by replacing the mis-predicted heatmaps with the ground-truth ones as the labels for the student net. To further improve the accuracy of the student net, we investigate the correlation between the extracted features and the predicted heatmaps, and divide the landmarks into two categories: simple and hard. A cascaded architecture is designed which firstly detects the simple points based on the extracted features, and then predicts the hard points resorting to the heatmaps of simple ones. Finally, a face alignment model with 3.64M parameters is obtained, which is about 6x smaller than the cumbersome model, and outperforms the state-of-the-art algorithms on both AFLW2000-3D and 300W-LP. The model and code are released in https://github.com/snow-rgb/Fast-Face-Alignment .},
  archive      = {J_CVIU},
  author       = {Jiaxin Si and Fei Jiang and Ruimin Shen and Hongtao Lu},
  doi          = {10.1016/j.cviu.2020.103125},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103125},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Small and accurate heatmap-based face alignment via distillation strategy and cascaded architecture},
  volume       = {203},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose invariant age estimation of face images in the wild.
<em>CVIU</em>, <em>202</em>, 103123. (<a
href="https://doi.org/10.1016/j.cviu.2020.103123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current work proposes a method for age estimation of face videos. To attenuate the effect of pose, our method is based on facial u v uv texture maps reconstructed from original frames of videos. A Wasserstein-based GAN is used to restore the full u v uv texture presentation. Age is then predicted from the completed u v uv mappings such that the proposed AgeGAN method simultaneously learns to capture the facial u v uv texture map and age characteristics. To train our method, we have created the UvAge dataset, the largest video dataset of face videos with age annotation (together with identity, gender, and ethnicity labels). The dataset contains videos in-the-wild from celebrities that are recorded in a variety of imaging settings. In total, we collected 6898 video segments (788,640 frames) from 516 celebrities in 57 events. Extensive experiments demonstrate that our proposed method outperforms other advanced age estimation methods.},
  archive      = {J_CVIU},
  author       = {Jian Han and Wei Wang and Sezer Karaoglu and Wei Zeng and Theo Gevers},
  doi          = {10.1016/j.cviu.2020.103123},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103123},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pose invariant age estimation of face images in the wild},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A spacetime model for one-shot active contour extraction
scheme for human detection in image sequences. <em>CVIU</em>,
<em>202</em>, 103113. (<a
href="https://doi.org/10.1016/j.cviu.2020.103113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a simple yet effective 3D convolutional neural network that learns a novel spacetime representation for human contour detection in a sequence of images. Our approach, in one shot, detects the contour of humans while generating high-quality results compared to the traditional binary mask representations. Our time-consistent convolutional neural network takes a sequence of images as its input and generates an implicit level set surface, in which the object boundaries correspond to the zero-level set. Furthermore, we introduce an appropriate way to combine space and time in an interwoven coordinate system tailored to spatiotemporal datasets. We showcase the feasibility of our approach by training the network on a semi-synthetic dataset. We discuss various configurations of our approach, all of which is shown to outperform the typical binary mask representation. We believe that this new approach could potentially improve the performance of all architectures compared to their alternative ones. The code will be made available on https://github.com/orgs/OSUPCVLab/HumanContourDetection .},
  archive      = {J_CVIU},
  author       = {Nima A. Gard and Colin Bunker and Alper Yilmaz},
  doi          = {10.1016/j.cviu.2020.103113},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103113},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A spacetime model for one-shot active contour extraction scheme for human detection in image sequences},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigating the significance of adversarial attacks and
their relation to interpretability for radar-based human activity
recognition systems. <em>CVIU</em>, <em>202</em>, 103111. (<a
href="https://doi.org/10.1016/j.cviu.2020.103111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given their substantial success in addressing a wide range of computer vision challenges, Convolutional Neural Networks (CNNs) are increasingly being used in smart home applications , with many of these applications relying on the automatic recognition of human activities . In this context, low-power radar devices have recently gained in popularity as recording sensors, given that the usage of these devices allows mitigating a number of privacy concerns, a key issue when making use of conventional video cameras. Another concern that is often cited when designing smart home applications is the resilience of these applications against cyberattacks. It is, for instance, well-known that the combination of images and CNNs is vulnerable against adversarial examples , mischievous data points that force machine learning models to generate wrong classifications during testing time. In this paper, we investigate the vulnerability of radar-based CNNs to adversarial attacks , and where these radar-based CNNs have been designed to recognize human gestures. Through experiments with four unique threat models, we show that radar-based CNNs are susceptible to both white- and black-box adversarial attacks. We also expose the existence of an extreme adversarial attack case, where it is possible to change the prediction made by the radar-based CNNs by only perturbing the padding of the inputs, without touching the frames where the action itself occurs. Moreover, we observe that gradient-based attacks exercise perturbation not randomly, but on important features of the input data. We highlight these important features by making use of Grad-CAM, a popular neural network interpretability method, hereby showing the connection between adversarial perturbation and prediction interpretability.},
  archive      = {J_CVIU},
  author       = {Utku Ozbulak and Baptist Vandersmissen and Azarakhsh Jalalvand and Ivo Couckuyt and Arnout Van Messem and Wesley De Neve},
  doi          = {10.1016/j.cviu.2020.103111},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103111},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Investigating the significance of adversarial attacks and their relation to interpretability for radar-based human activity recognition systems},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial feature distribution alignment for
semi-supervised learning. <em>CVIU</em>, <em>202</em>, 103109. (<a
href="https://doi.org/10.1016/j.cviu.2020.103109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks with only a few labeled samples can lead to overfitting. This is problematic in semi-supervised learning where only a few labeled samples are available. In this paper, we show that a consequence of overfitting in SSL is feature distribution misalignment between labeled and unlabeled samples . Hence, we propose a new feature distribution alignment method. Our method is particularly effective when using only a small amount of labeled samples. We test our method on CIFAR-10, SVHN and LSUN. On SVHN we achieve a test error of 3.88\% (250 labeled samples) and 3.39\% (1000 labeled samples), which is close to the fully supervised model 2.89\% (73k labeled samples). In comparison, the current SOTA achieves only 4.29\% and 3.74\%. On LSUN we achieve superior results than a state-of-the- art method even when using 100 × 100× less unlabeled samples (500 labeled samples). Finally, we provide a theoretical insight why feature distribution misalignment occurs and show that our method reduces it.},
  archive      = {J_CVIU},
  author       = {Christoph Mayer and Matthieu Paul and Radu Timofte},
  doi          = {10.1016/j.cviu.2020.103109},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103109},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial feature distribution alignment for semi-supervised learning},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive survey of procedural video datasets.
<em>CVIU</em>, <em>202</em>, 103107. (<a
href="https://doi.org/10.1016/j.cviu.2020.103107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural knowledge is crucial for understanding and performing concrete real-world tasks. Yet, despite the importance of procedural knowledge, research into procedural knowledge understanding is still under-developed. In particular, videos contain rich semantics that are important for understanding procedural knowledge, but have traditionally been less explored than natural language texts for understanding procedural knowledge. Motivated by harnessing procedural knowledge from videos for task assistance (i.e., assisting people in performing procedural tasks), we present the first comprehensive survey of procedural video datasets. Through systematically surveying 23 procedural video datasets, including both instructional and non-instructional videos, in a conceptual framework for task assistance, we seek to understand the trends and gaps in existing datasets, as well as to gain insights into the future of such datasets. This survey examines the current state of procedural video datasets, in terms of their data, content and annotation characteristics, as well as processing function and evaluation. The survey also identifies and suggests a number of possible directions to bring this area to the next level.},
  archive      = {J_CVIU},
  author       = {Hui Li Tan and Hongyuan Zhu and Joo-Hwee Lim and Cheston Tan},
  doi          = {10.1016/j.cviu.2020.103107},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103107},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A comprehensive survey of procedural video datasets},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single image rain removal via multi-module deep grid
network. <em>CVIU</em>, <em>202</em>, 103106. (<a
href="https://doi.org/10.1016/j.cviu.2020.103106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain streaks severely degenerate the performances of image/video processing tasks, therefore effective methods for removing rain streaks are required for a wide range of practical applications. In this paper, we introduce an end-to-end deep network, called GridDerainNet, to remove rain streaks within single image under different conditions. The architecture of GridDerainNet consists of three modules: pre-processing, multi-scale attentive module and post-processing. The pre-processing module can effectively generate several variants of the given rainy image, in order to extract more key features from the input. The multi-scale attentive module implements a novel attention mechanism , which allows more flexible information exchange and aggregation, taking full use of diversities of a given image. In the end, post-processing module furthers to reduce residual artifacts after previous two steps. Quantitative and qualitative experimental results demonstrate that the proposed algorithm outperforms several state-of-the-art methods on both synthetic and real-world images.},
  archive      = {J_CVIU},
  author       = {Nanfeng Jiang and Weiling Chen and Liqun Lin and Tiesong Zhao},
  doi          = {10.1016/j.cviu.2020.103106},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103106},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Single image rain removal via multi-module deep grid network},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of face recognition adversarial attacks.
<em>CVIU</em>, <em>202</em>, 103103. (<a
href="https://doi.org/10.1016/j.cviu.2020.103103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning methods have become state-of-the-art for solving tasks such as Face Recognition (FR). Unfortunately, despite their success, it has been pointed out that these learning models are exposed to adversarial inputs – images to which an imperceptible amount of noise for humans is added to maliciously fool a neural network – thus limiting their adoption in sensitive real-world applications. While it is true that an enormous effort has been spent to train robust models against this type of threat, adversarial detection techniques have recently started to draw attention within the scientific community. The advantage of using a detection approach is that it does not require to re-train any model; thus, it can be added to any system. In this context, we present our work on adversarial detection in forensics mainly focused on detecting attacks against FR systems in which the learning model is typically used only as features extractor. Thus, training a more robust classifier might not be enough to counteract the adversarial threats. In this frame, the contribution of our work is four-fold: (i) we test our proposed adversarial detection approach against classification attacks, i.e., adversarial samples crafted to fool an FR neural network acting as a classifier; (ii) using a k-Nearest Neighbor (k-NN) algorithm as a guide, we generate deep features attacks against an FR system based on a neural network acting as features extractor, followed by a similarity-based procedure which returns the query identity; (iii) we use the deep features attacks to fool an FR system on the 1:1 face verification task, and we show their superior effectiveness with respect to classification attacks in evading such type of system; (iv) we use the detectors trained on the classification attacks to detect the deep features attacks, thus showing that such approach is generalizable to different classes of offensives.},
  archive      = {J_CVIU},
  author       = {Fabio Valerio Massoli and Fabio Carrara and Giuseppe Amato and Fabrizio Falchi},
  doi          = {10.1016/j.cviu.2020.103103},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103103},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Detection of face recognition adversarial attacks},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comprehensive comparative evaluation of background
subtraction algorithms in open sea environments. <em>CVIU</em>,
<em>202</em>, 103101. (<a
href="https://doi.org/10.1016/j.cviu.2020.103101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous-ship and maritime security surveillance operations involving electro-optical sensors, the first phase of foreground segmentation and change detection using background subtraction (BS) algorithms is crucial. However, it is also the most complex in terms of execution time. Despite the development of several BS algorithms, maritime foreground segmentation and change detection remain major challenges owing to the complex, unconstrained, and diverse nature of ocean scenarios. However, only a few studies have investigated the applications of BS algorithms in maritime environments, especially those involving boats in the open sea. This study compares BS methods involving use of a non-static electro-optical sensor in combination with visible-light and infrared cameras to identify the best method for use in open sea scenarios, especially from the viewpoint of avoiding piracy and armed robbery. Thirty-seven methods, ranging from simple temporal differencing to more sophisticated ones, were validated via extensive experiments and analyses using realistic maritime datasets and practical maritime applications. In addition, because most methods considered in this study were not previously evaluated at the pixel level on open sea datasets, this paper proposes an appropriate maritime BS benchmark, based on which the 37 methods were compared to compensate for their prior lack of detailed analyses. The experimental results indicate that BS algorithms of the multiple features category can better handle maritime challenges, thereby realizing higher accuracies when analyzing visible-light and thermal videos. The proposed evaluation, therefore, complements those reported previously. Consequently, the proposed study enables users to identify the most suitable BS algorithm for use in intelligent maritime transportation , maritime security surveillance systems, and autonomous ships in the open sea.},
  archive      = {J_CVIU},
  author       = {Yi-Tung Chan},
  doi          = {10.1016/j.cviu.2020.103101},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103101},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Comprehensive comparative evaluation of background subtraction algorithms in open sea environments},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bi-branch network for dynamic scene deblurring.
<em>CVIU</em>, <em>202</em>, 103100. (<a
href="https://doi.org/10.1016/j.cviu.2020.103100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a bi-branch network for efficient dynamic scene deblurring. The challenge is to simultaneously reduce the computational cost and enhance the restoration accuracy. The proposed network conduct heterogeneous transformations on motion and RGB content in an encoder–decoder structure with skip connections. The computational efficiency is achieved by explicitly decomposing the intertwined mapping of spatiotemporal and cross-channel correlations into the motion branch that processes grayscale frames with our proposed pseudo depth-wise separable 3D convolution and the color branch that conducts depth-wise separable 2D convolution on RGB content. We refine features captured by the motion branch and the color branch by incorporating a lightweight nonlocal fusion layer that adapts the double attention operation to aggregate heterogeneous transformations and generate for each location in the feature space an output based on its correlation with the entire video clip. Our nonlocal fusion maintains low computational cost in processing high-resolution frames and operates in a patch-based manner during inference. The proposed architecture strikes the right balance between complexity and accuracy for dynamic scene deblurring. In comparison with state-of-the-art methods, the proposed network is compact and shows competitive restoration accuracy with a significant reduction in computational cost.},
  archive      = {J_CVIU},
  author       = {Yao Luo and Zhong-Hui Duan and Jinhui Tang},
  doi          = {10.1016/j.cviu.2020.103100},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103100},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Bi-branch network for dynamic scene deblurring},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FIFNET: A convolutional neural network for motion-based
multiframe super-resolution using fusion of interpolated frames.
<em>CVIU</em>, <em>202</em>, 103097. (<a
href="https://doi.org/10.1016/j.cviu.2020.103097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel motion-based multiframe image super-resolution (SR) algorithm using a convolutional neural network (CNN) that fuses multiple interpolated input frames to produce an SR output. We refer to the proposed CNN and associated preprocessing as the Fusion of Interpolated Frames Network (FIFNET). We believe this is the first such CNN approach in the literature to perform motion-based multiframe SR by fusing multiple input frames in a single network. We study the FIFNET using translational interframe motion with both fixed and random frame shifts. The input to the network is a sequence of interpolated and aligned frames. One key innovation is that we compute subpixel interframe registration information for each interpolated pixel and feed this into the network as additional input channels. We demonstrate that this subpixel registration information is critical to network performance. We also employ a realistic camera-specific optical transfer function model that accounts for diffraction and detector integration when generating training data. We present a number of experimental results to demonstrate the efficacy of the proposed FIFNET using both simulated and real camera data. The real data come directly from a camera and are not artificially downsampled or degraded. In the quantitative results with simulated data, we show that the FIFNET performs favorably in comparison to the benchmark methods tested.},
  archive      = {J_CVIU},
  author       = {Hamed Elwarfalli and Russell C. Hardie},
  doi          = {10.1016/j.cviu.2020.103097},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103097},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FIFNET: A convolutional neural network for motion-based multiframe super-resolution using fusion of interpolated frames},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A convergent framework with learnable feasibility for
hadamard-based image recovery. <em>CVIU</em>, <em>202</em>, 103095. (<a
href="https://doi.org/10.1016/j.cviu.2020.103095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a framework for recovering image degradations that can be formulated by the Hadamard product of clear images with degradation factors. By training the mapping from datasets, we show that implicit feasibilities can be learned in forms of latent domains. Then with the feasibilities and acknowledged data priors, the recovery problems are formulated as a general optimization model in which the domain knowledge of degradations are also nicely involved. Then we solve the model based on the classical coordinate update with plugged-in networks so that all the variables can be well estimated. Even better, our updating scheme is designed under the guidance of theoretical analyses, thus its stability can always be guaranteed in practice. We show that different recovery problems can be solved under our unified framework, and the extensive experimental results verify that the proposed framework is superior to state-of-the-art methods in both benchmark datasets and real-world images.},
  archive      = {J_CVIU},
  author       = {Yiyang Wang and Long Ma and Risheng Liu},
  doi          = {10.1016/j.cviu.2020.103095},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103095},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A convergent framework with learnable feasibility for hadamard-based image recovery},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nighttime image dehazing based on retinex and dark channel
prior using taylor series expansion. <em>CVIU</em>, <em>202</em>,
103086. (<a href="https://doi.org/10.1016/j.cviu.2020.103086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze removal from nighttime images is more difficult compared with daytime image dehazing due to the uneven illumination, low contrast and severe color distortion. In this paper, following the approaches based on Dark channel prior, we propose a simple yet effective approach using Retinex theory and Taylor series expansion for nighttime image dehazing, referred to as ‘RDT’. Existing nighttime image dehazing methods do not handle color shift and glow removal very well. In order to address these issues, we first propose to decompose the atmospheric light image from the input image based on the Retinex theory. Taylor series expansion is then introduced for the first time to accurately estimate the pointwise transmission map. Finally, during the following processes of image fusion and color transfer, the atmospheric light image and potential haze-free image are adopted to obtain the final haze-free image. The experimental results on benchmark nighttime haze images demonstrate the superior performance of our proposed RDT dehazing method over the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Qunfang Tang and Jie Yang and Xiangjian He and Wenjing Jia and Qingnian Zhang and Haibo Liu},
  doi          = {10.1016/j.cviu.2020.103086},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103086},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Nighttime image dehazing based on retinex and dark channel prior using taylor series expansion},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal semantic image segmentation. <em>CVIU</em>,
<em>202</em>, 103085. (<a
href="https://doi.org/10.1016/j.cviu.2020.103085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a modality invariant method to obtain high quality semantic object segmentation of human body parts, for four imaging modalities which consist of visible images, X-ray images, thermal images (heatmaps) and infrared radiation (IR) images. We first consider two modalities (i.e. visible and X-ray images) to develop an architecture suitable for multi-modal semantic segmentation . Due to the intrinsic difference between images from the two modalities, state-of-the-art approaches such as Mask R-CNN do not perform satisfactorily. Insights from analysing how the intermediate layers within Mask R-CNN work on both visible and X-ray modalities have led us to propose a new and efficient network architecture which yields highly accurate semantic segmentation results across both visible and X-ray domains. We design multi-task losses to train the network across different modalities. By conducting multiple experiments across visible and X-ray images of the human upper extremity, we validate the proposed approach, which outperforms the traditional Mask R-CNN method through better exploiting the output features of CNNs. Based on the insights gained on these images from visible and X-ray domains, we extend the proposed multi-modal semantic segmentation method to two additional modalities; (viz. heatmap and IR images). Experiments conducted on these two modalities, further confirm our architecture’s capacity to improve the segmentation by exploiting the complementary information in the different modalities of the images. Our method can also be applied to include other modalities and can be effectively utilized for several tasks including medical image analysis tasks such as image registration and 3D reconstruction across modalities.},
  archive      = {J_CVIU},
  author       = {Akila Pemasiri and Kien Nguyen and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.cviu.2020.103085},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103085},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-modal semantic image segmentation},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Controlling biases and diversity in diverse image-to-image
translation. <em>CVIU</em>, <em>202</em>, 103082. (<a
href="https://doi.org/10.1016/j.cviu.2020.103082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of unpaired image-to-image translation is highly challenging due to the lack of explicit cross-domain pairs of instances. We consider here diverse image translation (DIT), an even more challenging setting in which an image can have multiple plausible translations. This is normally achieved by explicitly disentangling content and style in the latent representation and sampling different styles codes while maintaining the image content. Despite the success of current DIT models , they are prone to suffer from bias. In this paper, we study the problem of bias in image-to-image translation. Biased datasets may add undesired changes (e.g. change gender or race in face images) to the output translations as a consequence of the particular underlying visual distribution in the target domain. In order to alleviate the effects of this problem we propose the use of semantic constraints that enforce the preservation of desired image properties. Our proposed model is a step towards unbiased diverse image-to-image translation (UDIT), and results in less unwanted changes in the translated images while still performing the wanted transformation. Experiments on several heavily biased datasets show the effectiveness of the proposed techniques in different domains such as faces, objects, and scenes.},
  archive      = {J_CVIU},
  author       = {Yaxing Wang and Abel Gonzalez-Garcia and Luis Herranz and Joost van de Weijer},
  doi          = {10.1016/j.cviu.2020.103082},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103082},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Controlling biases and diversity in diverse image-to-image translation},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the exact recovery conditions of 3D human motion from 2D
landmark motion with sparse articulated motion. <em>CVIU</em>,
<em>202</em>, 103072. (<a
href="https://doi.org/10.1016/j.cviu.2020.103072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of exact recovery condition in retrieving 3D human motion from 2D landmark motion. We use a skeletal kinematic model to represent the 3D motion as a vector of angular articulation motion. We address this problem based on the observation that at high tracking rate, regardless of the global rigid motion, only few angular articulations have non-zero motion. We propose a first ideal formulation with ℓ 0 ℓ0 -norm to minimize the cardinal of non-zero angular articulation motion given an equality constraint on the time-differentiation of the reprojection error. The second relaxed formulation relies on an ℓ 1 ℓ1 -norm to minimize the sum of absolute values of the angular articulation motion. This formulation has the advantage of being able to provide 3D motion even in the under-determined case when twice the number of 2D landmarks is smaller than the number of angular articulations. We define a specific property which is the Projective Kinematic Space Property (PKSP) that takes into account the reprojection constraint and the kinematic model. We prove that, for the relaxed formulation, we are able to recover the exact 3D human motion from 2D landmarks if and only if the PKSP property is verified. We further demonstrate that solving the relaxed formulation provides the same ground-truth solution as the ideal formulation if and only if the PKSP condition is filled. Results with simulated sparse skeletal angular motion show the ability of the proposed method to recover exact location of angular motion. We provide results on publicly available real data ( Human3.6M , Panoptic , MPI-I3DHP and 3DPW ) and show that our method is able to achieve state-of-the-art 3D reconstructions .},
  archive      = {J_CVIU},
  author       = {Abed Malti},
  doi          = {10.1016/j.cviu.2020.103072},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103072},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {On the exact recovery conditions of 3D human motion from 2D landmark motion with sparse articulated motion},
  volume       = {202},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
