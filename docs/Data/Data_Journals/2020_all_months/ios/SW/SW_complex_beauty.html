<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SW_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sw---59">SW - 59</h2>
<ul>
<li><details>
<summary>
(2020). BOT: The building topology ontology of the W3C linked
building data group. <em>SW</em>, <em>12</em>(1), 143–161. (<a
href="https://doi.org/10.3233/SW-200385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Actors in the Architecture, Engineering, Construction, Owner and Operation (AECOO) industry traditionally exchange building models as files. The Building Information Modelling (BIM) methodology advocates the seamless exchange of all information between related stakeholders using digital technologie s. The ultimate evolution of the methodology, BIM Maturity Level 3, envisions interoperable, distributed, web-based, interdisciplinary information exchange among stakeholders across the life-cycle of buildings. The World Wide Web Consortium Linked Building Data Community Group (W3C LBD-CG) hypothesises that the Linked Data models and best practices can be leveraged to achieve this vision in modern web-based applications. In this paper, we introduce the Building Topology Ontology (BOT) as a core vocabulary to this approach. It provides a high-level description of the topology of buildings including storeys and spaces, the building elements they contain, and their web-friendly 3D models. We describe how existing applications produce and consume datasets combining BOT with other ontologies that describe product catalogues, sensor observations, or Internet of Things (IoT) devices effectively implementing BIM Maturity Level 3. We evaluate our approach by exporting and querying three real-life large building models.},
  archive      = {J_SW},
  author       = {Rasmussen, Mads Holten and Lefrançois, Maxime and Schneider, Georg Ferdinand and Pauwels, Pieter},
  doi          = {10.3233/SW-200385},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {143-161},
  shortjournal = {Semantic Web},
  title        = {BOT: The building topology ontology of the W3C linked building data group},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RDF graph validation using rule-based reasoning.
<em>SW</em>, <em>12</em>(1), 117–142. (<a
href="https://doi.org/10.3233/SW-200384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correct functioning of Semantic Web applications requires that given RDF graphs adhere to an expected shape. This shape depends on the RDF graph and the application’s supported entailments of that graph. During validation, RDF graphs are assessed against sets of constraints, and found violation s help refining the RDF graphs. However, existing validation approaches cannot always explain the root causes of violations (inhibiting refinement), and cannot fully match the entailments supported during validation with those supported by the application. These approaches cannot accurately validate RDF graphs, or combine multiple systems, deteriorating the validator’s performance. In this paper, we present an alternative validation approach using rule-based reasoning, capable of fully customizing the used inferencing steps. We compare to existing approaches, and present a formal ground and practical implementation “Validatrr”, based on N3Logic and the EYE reasoner. Our approach – supporting an equivalent number of constraint types compared to the state of the art – better explains the root cause of the violations due to the reasoner’s generated logical proof, and returns an accurate number of violations due to the customizable inferencing rule set. Performance evaluation shows that Validatrr is performant for smaller datasets, and scales linearly w.r.t. the RDF graph size. The detailed root cause explanations can guide future validation report description specifications, and the fine-grained level of configuration can be employed to support different constraint languages. This foundation allows further research into handling recursion, validating RDF graphs based on their generation description, and providing automatic refinement suggestions.},
  archive      = {J_SW},
  author       = {De Meester, Ben and Heyvaert, Pieter and Arndt, Dörthe and Dimou, Anastasia and Verborgh, Ruben},
  doi          = {10.3233/SW-200384},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {117-142},
  shortjournal = {Semantic Web},
  title        = {RDF graph validation using rule-based&amp;nbsp;reasoning},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). S-paths: Set-based visual exploration of linked data driven
by semantic paths. <em>SW</em>, <em>12</em>(1), 99–116. (<a
href="https://doi.org/10.3233/SW-200383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meaningful information about an RDF resource can be obtained not only by looking at its properties, but by putting it in the broader context of similar resources. Classic navigation paradigms on the Web of Data that employ a follow-your-nose strategy fail to provide such context, and put strong emp hasis on first-level properties, forcing users to drill down in the graph one step at a time. We introduce the concept of semantic paths: starting from a set of resources, we follow and analyse chains of triples and characterize the sets of values at their end. We investigate a navigation strategy based on aggregation, relying on path characteristics to determine the most readable representation. We implement this approach in S-Paths, a browsing tool for linked datasets that systematically identifies the best rated view on a given resource set, leaving users free to switch to another resource set, or to get a different perspective on the same set by selecting other semantic paths to visualize.},
  archive      = {J_SW},
  author       = {Destandau, Marie and Appert, Caroline and Pietriga, Emmanuel},
  doi          = {10.3233/SW-200383},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {99-116},
  shortjournal = {Semantic Web},
  title        = {S-paths: Set-based visual exploration of linked data driven by semantic paths},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introducing the data quality vocabulary (DQV). <em>SW</em>,
<em>12</em>(1), 81–97. (<a
href="https://doi.org/10.3233/SW-200382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Data Quality Vocabulary (DQV) provides a metadata model for expressing data quality. DQV was developed by the Data on the Web Best Practice (DWBP) Working Group of the World Wide Web Consortium (W3C) between 2013 and 2017. This paper aims at providing a deeper understanding of DQV. It introduce s its key design principles, components, and the main discussion points that have been raised in the process of designing it. The paper compares DQV with previous quality documentation vocabularies and demonstrates the early uptake of DQV by collecting tools, papers, projects that have exploited and extended DQV.},
  archive      = {J_SW},
  author       = {Albertoni, Riccardo and Isaac, Antoine},
  doi          = {10.3233/SW-200382},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {81-97},
  shortjournal = {Semantic Web},
  title        = {Introducing the data quality vocabulary (DQV)},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deploying spatial-stream query answering in c-ITS scenarios.
<em>SW</em>, <em>12</em>(1), 41–77. (<a
href="https://doi.org/10.3233/SW-200408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative Intelligent Transport Systems (C-ITS) play an important role for providing the means to collect and exchange spatio-temporal data via V2X-based communication between vehicles and the infrastructure, which will become a central enabler for road safety of (semi)-autonomous vehicles. The L ocal Dynamic Map (LDM) is a key concept for integrating static and streamed data in a spatial context. The LDM has been semantically enhanced to allow for an elaborate domain model that is captured by a mobility ontology, and for queries over data streams that cater for semantic concepts and spatial relationships. Our approach for semantic enhancement is in the context of ontology-mediated query answering (OQA) and features conjunctive queries over DL-LiteA ontologies that support window operators over streams and spatial relations between spatial objects. In this paper, we show how this approach can be extended to address a wider range of use cases in the three C-ITS scenarios traffic statistics, traffic events detection, and advanced driving assistance systems. We define for the mentioned use cases requirements derived from necessary domain-specific features and report, based on them, on extensions of our query language and ontology model. The extensions include temporal relations, numeric predictions and trajectory predictions as well as optimization strategies such as caching. An experimental evaluation of queries that reflect the requirements has been conducted using the real-world traffic simulation tool PTV Vissim. It provides evidence for the feasibility/efficiency of our approach in the new scenarios.},
  archive      = {J_SW},
  author       = {Eiter, Thomas and Ichise, Ryutaro and Parreira, Josiane Xavier and Schneider, Patrik and Zhao, Lihua},
  doi          = {10.3233/SW-200408},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {41-77},
  shortjournal = {Semantic Web},
  title        = {Deploying spatial-stream query answering in C-ITS scenarios},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Network metrics for assessing the quality of entity
resolution between multiple datasets. <em>SW</em>, <em>12</em>(1),
21–40. (<a href="https://doi.org/10.3233/SW-200410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching entities between datasets is a crucial step for combining multiple datasets on the semantic web. A rich literature exists on different approaches to this entity resolution problem. However, much less work has been done on how to assess the quality of such entity links once they have been g enerated. Evaluation methods for link quality are typically limited to either comparison with a ground truth dataset (which is often not available), manual work (which is cumbersome and prone to error), or crowd sourcing (which is not always feasible, especially if expert knowledge is required). Furthermore, the problem of link evaluation is greatly exacerbated for links between more than two datasets, because the number of possible links grows rapidly with the number of datasets. In this paper, we propose a method to estimate the quality of entity links between multiple datasets. We exploit the fact that the links between entities from multiple datasets form a network, and we show how simple metrics on this network can reliably predict their quality. We verify our results in a large experimental study using six datasets from the domain of science, technology and innovation studies, for which we created a gold standard. This gold standard, available online, is an additional contribution of this paper. In addition, we evaluate our metric on a recently published gold standard to confirm our findings.},
  archive      = {J_SW},
  author       = {Idrissou, Al and van Harmelen, Frank and van den Besselaar, Peter},
  doi          = {10.3233/SW-200410},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {21-40},
  shortjournal = {Semantic Web},
  title        = {Network metrics for assessing the quality of entity resolution between multiple datasets},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Typology-based semantic labeling of numeric tabular data.
<em>SW</em>, <em>12</em>(1), 5–20. (<a
href="https://doi.org/10.3233/SW-200397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lot of tabular data are being published on the Web. Semantic labeling of such data may help in their understanding and exploitation. However, many challenges need to be addressed to do this automatically. With numbers, it can be even harder due to the possible difference in measurement accuracy, rounding errors, and even the frequency of their appearance. Multiple approaches have been proposed in the literature to tackle the problem of semantic labeling of numeric values in existing tabular datasets. However, they also suffer from several shortcomings: closely coupled with entity-linking, rely on table context, need to profile the knowledge graph, and require manual training of the model. Above all, however, they all treat different types of numeric values evenly. In this paper, we tackle these problems and validate our hypothesis: whether taking into account the typology of numeric data in semantic labeling yields better results.},
  archive      = {J_SW},
  author       = {Alobaid, Ahmad and Kacprzak, Emilia and Corcho, Oscar},
  doi          = {10.3233/SW-200397},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {5-20},
  shortjournal = {Semantic Web},
  title        = {Typology-based semantic labeling of numeric tabular data},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selected papers from EKAW 2018. <em>SW</em>, <em>12</em>(1),
3–4. (<a href="https://doi.org/10.3233/SW-200411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Faron, Catherine and Ghidini, Chiara},
  doi          = {10.3233/SW-200411},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {3-4},
  shortjournal = {Semantic Web},
  title        = {Selected papers from EKAW 2018},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Difficulty-level modeling of ontology-based factual
questions. <em>SW</em>, <em>11</em>(6), 1023–1036. (<a
href="https://doi.org/10.3233/SW-200381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantics-based knowledge representations such as ontologies are found to be very useful in automatically generating meaningful factual questions. Determining the difficulty-level of these system-generated questions is helpful to effectively utilize them in various educational and professional appl ications. The existing approach for predicting the difficulty-level of factual questions utilizes only few naive features and, its accuracy (F-measure) is found to be close to only 50% while considering our benchmark set of 185 questions. In this paper, we propose a new methodology for this problem by identifying new features and by incorporating an educational theory, related to difficulty-level of a question, called Item Response Theory (IRT). In the IRT, knowledge proficiency of end users (learners) are considered for assigning difficulty-levels, because of the assumptions that a given question is perceived differently by learners of various proficiency levels. We have done a detailed study on the features/factors of a question statement which could possibly determine its difficulty-level for three learner categories (experts, intermediates, and beginners). We formulate ontology-based metrics for the same. We then train three logistic regression models to predict the difficulty-level corresponding to the three learner categories. The output of these models is interpreted using the IRT to find a question’s overall difficulty-level. The accuracy of the three models based on cross-validation is found to be in satisfactory range (67–84%). The proposed model (containing three classifiers) outperforms the existing model by more than 20% in precision, recall and F1-score measures.},
  archive      = {J_SW},
  author       = {Venugopal, Vinu E. and Kumar, P. Sreenivasa},
  doi          = {10.3233/SW-200381},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {1023-1036},
  shortjournal = {Semantic Web},
  title        = {Difficulty-level modeling of ontology-based factual questions},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SQuAP-ont: An ontology of software quality relational
factors from financial systems. <em>SW</em>, <em>11</em>(6), 1007–1021.
(<a href="https://doi.org/10.3233/SW-200372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality, architecture, and process are considered the keystones of software engineering. ISO defines them in three separate standards. However, their interaction has been scarcely studied, so far. The SQuAP model (Software Quality, Architecture, Process) describes twenty-eight main factors that imp act on software quality in banking systems, and each factor is described as a relation among some characteristics from the three ISO standards. Hence, SQuAP makes such relations emerge rigorously, although informally. In this paper, we present SQuAP-Ont, an OWL ontology designed by following a well-established methodology based on the re-use of Ontology Design Patterns (i.e. ODPs). SQuAP-Ont formalises the relations emerging from SQuAP to represent and reason via Linked Data about software engineering in a three-dimensional model consisting of quality, architecture, and process ISO characteristics.},
  archive      = {J_SW},
  author       = {Ciancarini, Paolo and Nuzzolese, Andrea Giovanni and Presutti, Valentina and Russo, Daniel},
  doi          = {10.3233/SW-200372},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {1007-1021},
  shortjournal = {Semantic Web},
  title        = {SQuAP-ont: An ontology of software quality relational factors from financial systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decade of semantic web research through the lenses of a
mixed methods approach. <em>SW</em>, <em>11</em>(6), 979–1005. (<a
href="https://doi.org/10.3233/SW-200371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of research topics and trends is an important scientometric activity, as it can help guide the direction of future research. In the Semantic Web area, initially topic and trend detection was primarily performed through qualitative, top-down style approaches, that rely on expert k nowledge. More recently, data-driven, bottom-up approaches have been proposed that offer a quantitative analysis of the evolution of a research domain. In this paper, we aim to provide a broader and more complete picture of Semantic Web topics and trends by adopting a mixed methods methodology, which allows for the combined use of both qualitative and quantitative approaches. Concretely, we build on a qualitative analysis of the main seminal papers, which adopt a top-down approach, and on quantitative results derived with three bottom-up data-driven approaches (Rexplore, Saffron, PoolParty), on a corpus of Semantic Web papers published between 2006 and 2015. In this process, we both use the latter for “fact-checking” on the former and also to derive key findings in relation to the strengths and weaknesses of top-down and bottom-up approaches to research topic identification. Although we provide a detailed study on the past decade of Semantic Web research, the findings and the methodology are relevant not only for our community but beyond the area of the Semantic Web to other research fields as well.},
  archive      = {J_SW},
  author       = {Kirrane, Sabrina and Sabou, Marta and Fernández, Javier D. and Osborne, Francesco and Robin, Cécile and Buitelaar, Paul and Motta, Enrico and Polleres, Axel},
  doi          = {10.3233/SW-200371},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {979-1005},
  shortjournal = {Semantic Web},
  title        = {A decade of semantic web research through the lenses of a mixed methods approach},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic node-RED for rapid development of interoperable
industrial IoT applications. <em>SW</em>, <em>11</em>(6), 949–975. (<a
href="https://doi.org/10.3233/SW-200405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of IoT has revolutionized industrial automation. Industrial devices at every level such as field devices, control devices, enterprise level devices etc., are connected to the Internet, where they can be accessed easily. It has significantly changed the way applications are developed o n the industrial automation systems. It led to the paradigm shift where novel IoT application development tools such as Node-RED can be used to develop complex industrial applications as IoT orchestrations. However, in the current state, these applications are bound strictly to devices from specific vendors and ecosystems. They cannot be re-used with devices from other vendors and platforms, since the applications are not semantically interoperable. For this purpose, it is desirable to use platform-independent, vendor-neutral application templates for common automation tasks. However, in the current state in Node-RED such reusable and interoperable application templates cannot be developed. The interoperability problem at the data level can be addressed in IoT, using Semantic Web (SW) technologies. However, for an industrial engineer or an IoT application developer, SW technologies are not very easy to use. In order to enable efficient use of SW technologies to create interoperable IoT applications, novel IoT tools are required. For this purpose, in this paper we propose a novel semantic extension to the widely used Node-RED tool by introducing semantic definitions such as iot.schema.org semantic models into Node-RED. The tool guides a non-expert in semantic technologies such as a device vendor, a machine builder to configure the semantics of a device consistently. Moreover, it also enables an engineer, IoT application developer to design and develop semantically interoperable IoT applications with minimal effort. Our approach accelerates the application development process by introducing novel semantic application templates called Recipes in Node-RED. Using Recipes, complex application development tasks such as skill matching between Recipes and existing things can be automated. We will present the approach to perform automated skill matching on the Cloud or on the Edge of an automation system. We performed quantitative and qualitative evaluation of our approach to test the feasibility and scalability of the approach in real world scenarios. The results of the evaluation are presented and discussed in the paper.},
  archive      = {J_SW},
  author       = {Thuluva, Aparna Saisree and Anicic, Darko and Rudolph, Sebastian and Adikari, Malintha},
  doi          = {10.3233/SW-200405},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {949-975},
  shortjournal = {Semantic Web},
  title        = {Semantic node-RED for rapid development of interoperable industrial IoT applications},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining chronicle mining and semantics for predictive
maintenance in manufacturing processes. <em>SW</em>, <em>11</em>(6),
927–948. (<a href="https://doi.org/10.3233/SW-200406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within manufacturing processes, faults and failures may cause severe economic loss. With the vision of Industry 4.0, artificial intelligence techniques such as data mining play a crucial role in automatic fault and failure prediction. However, due to the heterogeneous nature of industrial data, dat a mining results normally lack both machine and human-understandable representation and interpretation of knowledge. This may cause the semantic gap issue, which stands for the incoherence between the knowledge extracted from industrial data and the interpretation of the knowledge from a user. To address this issue, ontology-based approaches have been used to bridge the semantic gap between data mining results and users. However, only a few existing ontology-based approaches provide satisfactory knowledge modeling and representation for all the essential concepts in predictive maintenance. Moreover, most of the existing research works merely focus on the classification of operating conditions of machines, while lacking the extraction of specific temporal information of failure occurrence. This brings obstacles for users to perform maintenance actions with the consideration of temporal constraints. To tackle these challenges, in this paper we introduce a novel hybrid approach to facilitate predictive maintenance tasks in manufacturing processes. The proposed approach is a combination of data mining and semantics, within which chronicle mining is used to predict the future failures of the monitored industrial machinery, and a Manufacturing Predictive Maintenance Ontology (MPMO) with its rule-based extension is used to predict temporal constraints of failures and to represent the predictive results formally. As a result, Semantic Web Rule Language (SWRL) rules are constructed for predicting the occurrence time of machinery failures in the future. The proposed rules provide explicit knowledge representation and semantic enrichment of failure prediction results, thus easing the understanding of the inferred knowledge. A case study on a semi-conductor manufacturing process is used to demonstrate our approach in detail. The evaluation of results shows that the MPMO ontology is free of bad practices in the structural, functional, and usability-profiling dimensions. The constructed SWRL rules posses more than 80% of True Positive Rate, Precision, and F-measure, which shows promising performance in failure prediction.},
  archive      = {J_SW},
  author       = {Cao, Qiushi and Samet, Ahmed and Zanni-Merk, Cecilia and de Bertrand de Beuvron, François and Reich, Christoph},
  doi          = {10.3233/SW-200406},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {927-948},
  shortjournal = {Semantic Web},
  title        = {Combining chronicle mining and semantics for predictive maintenance in manufacturing processes},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SAREF4INMA: A SAREF extension for the industry and
manufacturing domain. <em>SW</em>, <em>11</em>(6), 911–926. (<a
href="https://doi.org/10.3233/SW-200402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IoT landscape is characterized by a fragmentation of standards, platforms and technologies, often scattered among different vertical domains. To prevent the market to continue to be fragmented and power-less, a protocol-independent semantic layer can serve as enabler of interoperability among t he various smart devices from different manufacturers that co-exist in a specific industry domain, but also across different domains. To that end, the SAREF ontology was created in 2015 with the intention to interconnect data, enabling the communication between IoT devices that use different protocols and standards. A number of industrial sectors consequently expressed their interest to extend SAREF into their domains in order to fill the gaps of the semantics not yet covered by their communication protocols. Therefore, the SAREF4INMA ontology was recently created to extend SAREF for describing the Smart Industry &amp; Manufacturing domain. SAREF4INMA is based on several standards and IoT initiatives, as well as on real use cases, and includes classes, properties and instances specifically created to cover the industry and manufacturing domain. This work describes the approach followed to develop this ontology, specifies its requirements and also includes a practical example of how to use it.},
  archive      = {J_SW},
  author       = {de Roode, Mike and Fernández-Izquierdo, Alba and Daniele, Laura and Poveda-Villalón, María and García-Castro, Raúl},
  doi          = {10.3233/SW-200402},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {911-926},
  shortjournal = {Semantic Web},
  title        = {SAREF4INMA: A SAREF extension for the industry and manufacturing domain},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ExtruOnt: An ontology for describing a type of manufacturing
machine for industry 4.0 systems. <em>SW</em>, <em>11</em>(6), 887–909.
(<a href="https://doi.org/10.3233/SW-200376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantically rich descriptions of manufacturing machines, offered in a machine-interpretable code, can provide interesting benefits in Industry 4.0 scenarios. However, the lack of that type of descriptions is evident. In this paper we present the development effort made to build an ontology, called ExtruOnt, for describing a type of manufacturing machine, more precisely, a type that performs an extrusion process (extruder). Although the scope of the ontology is restricted to a concrete domain, it could be used as a model for the development of other ontologies for describing manufacturing machines in Industry 4.0 scenarios. The terms of the ExtruOnt ontology provide different types of information related with an extruder, which are reflected in distinct modules that constitute the ontology. Thus, it contains classes and properties for expressing descriptions about components of an extruder, spatial connections, features, and 3D representations of those components, and finally the sensors used to capture indicators about the performance of this type of machine. The ontology development process has been carried out in close collaboration with domain experts.},
  archive      = {J_SW},
  author       = {Ramírez-Durán, Víctor Julio and Berges, Idoia and Illarramendi, Arantza},
  doi          = {10.3233/SW-200376},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {887-909},
  shortjournal = {Semantic Web},
  title        = {ExtruOnt: An ontology for describing a&amp;nbsp;type&amp;nbsp;of manufacturing machine for&amp;nbsp;Industry&amp;nbsp;4.0&amp;nbsp;systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic web of things for industry 4.0. <em>SW</em>,
<em>11</em>(6), 885–886. (<a
href="https://doi.org/10.3233/SW-200407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to this special issue of the Semantic Web (SWJ) journal. The special issue compiles four technical contributions that significantly advance the state-of-the-art in Semantic Web of Things for Industry 4.0 including the use of Semantic Web technologies and techniques in Industry 4.0 solutions.},
  archive      = {J_SW},
  author       = {Thakker, Dhavalkumar and Patel, Pankesh and Ali, Muhammad Intizar and Shah, Tejal},
  doi          = {10.3233/SW-200407},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {885-886},
  shortjournal = {Semantic Web},
  title        = {Semantic web of things for industry 4.0},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VocBench 3: A collaborative semantic web editor for
ontologies, thesauri and lexicons. <em>SW</em>, <em>11</em>(5), 855–881.
(<a href="https://doi.org/10.3233/SW-200370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VocBench is an open source web platform for the collaborative development of datasets complying with Semantic Web standards. Since its public release – five years ago – as an open source platform, VocBench has attracted a growing user community consisting of public organizations, companies and inde pendent users looking for open source solutions for maintaining their thesauri, code lists and authority resources. The focus on collaboration, the differentiation of user roles and the workflow management for content validation and publication have been the strengths of the platform, especially for those organizations requiring a distributed, yet centrally controlled, publication environment. In 2017, a new, completely reengineered, version of the system has been released, broadening the scope of the platform: funded by the ISA2 programme of the European Commission, VocBench 3 offers a general-purpose collaborative environment for development of any kind of RDF dataset (with dedicated facilities for ontologies, thesauri and lexicons), improving the editing capabilities of its predecessor, while still maintaining the peculiar aspects that determined its success. In this article, we review the requirements and the new objectives set for version 3, and then introduce the new characteristics that were implemented for this new incarnation of the platform},
  archive      = {J_SW},
  author       = {Stellato, Armando and Fiorelli, Manuel and Turbati, Andrea and Lorenzetti, Tiziano and van Gemert, Willem and Dechandon, Denis and Laaboudi-Spoiden, Christine and Gerencsér, Anikó and Waniart, Anne and Costetchi, Eugeniu and Keizer, Johannes},
  doi          = {10.3233/SW-200370},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {855-881},
  shortjournal = {Semantic Web},
  title        = {VocBench 3: A collaborative semantic web editor for ontologies, thesauri and lexicons},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven assessment of structural evolution of RDF
graphs. <em>SW</em>, <em>11</em>(5), 831–853. (<a
href="https://doi.org/10.3233/SW-200368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the birth of the Semantic Web, numerous knowledge bases have appeared. The applications that exploit them rely on the quality of their data through time. In this regard, one of the main dimensions of data quality is conformance to the expected usage of the vocabulary. However, the vocabulary usage (i.e., how classes and properties are actually populated) can vary from one base to another. Moreover, through time, such usage can evolve within a base and diverge from the previous practices. Methods have been proposed to follow the evolution of a knowledge base by the observation of the changes of their intentional schema (or ontology); however, they do not capture the evolution of their actual data, which can vary greatly in practice. In this paper, we propose a data-driven approach to assess the global evolution of vocabulary usage in large RDF graphs. Our proposal relies on two structural measures defined at different granularities (dataset vs update), which are based on pattern mining techniques. We have performed a thorough experimentation which shows that our approach is scalable, and can capture structural evolution through time of both synthetic (LUBM) and real knowledge bases (different snapshots and updates of DBpedia).},
  archive      = {J_SW},
  author       = {Bobed, Carlos and Maillot, Pierre and Cellier, Peggy and Ferré, Sébastien},
  doi          = {10.3233/SW-200368},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {831-853},
  shortjournal = {Semantic Web},
  title        = {Data-driven assessment of structural evolution of RDF graphs},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic detection of relation assertion errors and
induction of relation constraints. <em>SW</em>, <em>11</em>(5), 801–830.
(<a href="https://doi.org/10.3233/SW-200369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the link prediction problem, where missing relation assertions are predicted, has been widely researched, error detection did not receive as much attention. In this paper, we investigate the problem of error detection in relation assertions of knowledge graphs, and we propose an error dete ction method which relies on path and type features used by a classifier for every relation in the graph exploiting local feature selection. Furthermore, we propose an approach for automatically correcting detected errors originated from confusions between entities. Moreover, we present an approach that translates decision trees trained for relation assertion error detection into SHACL-SPARQL relation constraints. We perform an extensive evaluation on a variety of datasets comparing our error detection approach with state-of-the-art error detection and knowledge completion methods, backed by a manual evaluation on DBpedia and NELL. We evaluate our error correction approach results on DBpedia and NELL and show that the relation constraint induction approach benefits from the higher expressiveness of SHACL and can detect errors which could not be found by automatically learned OWL constraints.},
  archive      = {J_SW},
  author       = {Melo, Andre and Paulheim, Heiko},
  doi          = {10.3233/SW-200369},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {801-830},
  shortjournal = {Semantic Web},
  title        = {Automatic detection of relation assertion errors and induction of relation constraints},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuous top-k approximated join of streaming and evolving
distributed data. <em>SW</em>, <em>11</em>(5), 767–799. (<a
href="https://doi.org/10.3233/SW-190367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuously finding the most relevant (shortly, top-k) answer of a query that joins streaming and distributed data is getting a growing attention. In recent years, this is in particular happening in Social Media and IoT. It is well known that, in those settings, remaining reactive can be challengi ng, because accessing the distributed data can be highly time consuming as well as rate-limited. In this paper, we investigate the problem of continuous top-k query evaluation over a data stream joined with a distributed dataset in even a more extreme situation: the distributed data evolves. We propose the Topk+N algorithm and the AcquaTop framework. They keep up to date a local replica of the distributed dataset and guarantees reactiveness by construction, but to do so they may need to approximate the result. Therefore, we propose two maintenance policies to update the replica: the Top Selection Maintenance (AT-TSM) policy maximizes the relevancy, while the Border Selection Maintenance (AT-BSM) policy maximizes the accuracy of the top-k result. We contribute a theoretical proof of the correctness of Topk+N algorithm and we study its complexity. Moreover, we provide empirical evidence that the proposed policies within AcquaTop framework produce more relevant and accurate results than the state of the art.},
  archive      = {J_SW},
  author       = {Zahmatkesh, Shima and Della Valle, Emanuele},
  doi          = {10.3233/SW-190367},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {767-799},
  shortjournal = {Semantic Web},
  title        = {Continuous top-k approximated join of streaming and evolving distributed data},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Findable and reusable workflow data products: A genomic
workflow case study. <em>SW</em>, <em>11</em>(5), 751–763. (<a
href="https://doi.org/10.3233/SW-200374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While workflow systems have improved the repeatability of scientific experiments, the value of the processed (intermediate) data have been overlooked so far. In this paper, we argue that the intermediate data products of workflow executions should be seen as first-class objects that need to be cura ted and published. Not only will this be exploited to save time and resources needed when re-executing workflows, but more importantly, it will improve the reuse of data products by the same or peer scientists in the context of new hypotheses and experiments. To assist curator in annotating (intermediate) workflow data, we exploit in this work multiple sources of information, namely: (i) the provenance information captured by the workflow system, and (ii) domain annotations that are provided by tools registries, such as Bio.Tools. Furthermore, we show, on a concrete bioinformatics scenario, how summarising techniques can be used to reduce the machine-generated provenance information of such data products into concise human- and machine-readable annotations.},
  archive      = {J_SW},
  author       = {Gaignard, Alban and Skaf-Molli, Hala and Belhajjame, Khalid},
  doi          = {10.3233/SW-200374},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {751-763},
  shortjournal = {Semantic Web},
  title        = {Findable and reusable workflow data products: A genomic workflow case study},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large-scale semantic exploration of scientific literature
using topic-based hashing algorithms. <em>SW</em>, <em>11</em>(5),
735–750. (<a href="https://doi.org/10.3233/SW-200373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for similar documents and exploring major themes covered across groups of documents are common activities when browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even lead to unexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms represent documents in a common feature space that abstract them away from the specific sequence of words used in them. Probabilistic Topic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latent space some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematic information gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics to justify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor techniques that uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extending those queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluations on both scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.},
  archive      = {J_SW},
  author       = {Badenes-Olmedo, Carlos and Redondo-García, José Luis and Corcho, Oscar},
  doi          = {10.3233/SW-200373},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {735-750},
  shortjournal = {Semantic Web},
  title        = {Large-scale semantic exploration of scientific literature using topic-based hashing algorithms},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survey on complex ontology matching. <em>SW</em>,
<em>11</em>(4), 689–727. (<a
href="https://doi.org/10.3233/SW-190366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simple ontology alignments, largely studied in the literature, link a single entity of a source ontology to a single entity of a target ontology. A limitation of these alignments is their lack of expressiveness which can be overcome by complex alignments. While diverse state-of-the-art surveys main ly review the matching approaches in general, to the best of our knowledge, there is no study of the specificities of the complex matching problem. In this paper, a review of the different complex matching approaches is provided. It proposes a classification of the complex matching approaches based on their specificities (i.e., type of correspondences, guiding structure). The evaluation aspects and the limitations of these approaches are also discussed. Insights for future work in the field are provided.},
  archive      = {J_SW},
  author       = {Thiéblin, Elodie and Haemmerlé, Ollivier and Hernandez, Nathalie and Trojahn, Cassia},
  doi          = {10.3233/SW-190366},
  journal      = {Semantic Web},
  month        = {8},
  number       = {4},
  pages        = {689-727},
  shortjournal = {Semantic Web},
  title        = {Survey on complex ontology matching},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A systematic survey of temporal requirements of bio-health
ontologies. <em>SW</em>, <em>11</em>(4), 657–688. (<a
href="https://doi.org/10.3233/SW-190357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Description Logic SROIQ(D), as the logical core of the W3C standard Web Ontology Language (OWL 2), is a widely used formalism for ontologies in the life sciences. Bio-health applications including health-care and life science domains commonly have a need to represent temporal information such a s medication frequency or stage-based development. Different classes of temporal phenomena may generate different sorts of requirements on SROIQ(D) or extensions of SROIQ(D). In this paper, we deliver the first precise investigation into identifying exactly what kinds of temporal requirements are most important for bio-health ontologies. We conduct an empirical investigation of the OBO Foundry using a bespoke methodological approach by searching each of its ontologies for specific temporal features and go on to calculate the importance of these features using a sophisticated set of measures. By doing so, we derive a formal set of Temporal Requirements that act as a set of guidelines which a language or logical extension to OWL 2 would need to satisfy in order to meet the temporal requirements of bio-health ontologies.},
  archive      = {J_SW},
  author       = {Leo, Jared and Matentzoglu, Nicolas and Sattler, Uli and Parsia, Bijan},
  doi          = {10.3233/SW-190357},
  journal      = {Semantic Web},
  month        = {8},
  number       = {4},
  pages        = {657-688},
  shortjournal = {Semantic Web},
  title        = {A systematic survey of temporal requirements of bio-health ontologies},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EDR: A generic approach for the distribution of rule-based
reasoning in a cloud–fog continuum. <em>SW</em>, <em>11</em>(4),
623–654. (<a href="https://doi.org/10.3233/SW-200377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The successful deployment of the Semantic Web of Things (SWoT) requires the adaptation of the Semantic Web principles and technologies to the constraints of the IoT domain, which is the challenging research direction we address here. In this context we promote distributed reasoning approaches in Io T systems by implementing a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. Our solution benefits from the complementarity between Cloud and Fog infrastructures. Indeed, remote powerful Cloud computation resources are essential to the deployment of scalable IoT applications, and locally distributed constrained Fog resources, close to data producers, enable low-latency decision making. Moreover, as IoT networks are open and evolutive, the computation should be dynamically distributed across Fog nodes according to the transformation of the network topology. For this purpose, we propose the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud–Fog IoT architecture. We elaborated mechanisms enabling the genericity and the dynamicity of EDR. We evaluated its scalability and applicability in a simulated smart factory use-case. The complementarity between Fog and Cloud in this context is assessed based on the experimentation conducted.},
  archive      = {J_SW},
  author       = {Seydoux, Nicolas and Drira, Khalil and Hernandez, Nathalie and Monteil, Thierry},
  doi          = {10.3233/SW-200377},
  journal      = {Semantic Web},
  month        = {8},
  number       = {4},
  pages        = {623-654},
  shortjournal = {Semantic Web},
  title        = {EDR: A generic approach for the distribution of rule-based reasoning in a Cloud–Fog continuum},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ontologies for observations and actuations in buildings: A
survey. <em>SW</em>, <em>11</em>(4), 593–621. (<a
href="https://doi.org/10.3233/SW-200378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spaces and elements in the built environment have emerged as platforms where materializations of observations and actuations promise to be very profitable. The advent of the Internet of Things (IoT) paves the way to address this challenge but the heterogeneity of the represented knowledge about the se artifact systems poses a real problem. Ontologies can be considered as part of the solution to overcome the IoT’s inherent hurdles. A wise option promoted by recent approaches is to design networks of complementary ontologies. However, different points of view are possible and such diversity could lead to interoperability problems. This article advocates for a networked ontology infrastructure conceived on a principled basis guided by documented judicious conceptualizations. In this regard, this survey points towards ontologies involved in conceptualizations of observations and actuations, where the utility of that conceptualization arises when some features of interest need to be observed or acted upon. For each of the reviewed ontologies, their fundamentals are described, their potential advantages and shortcomings are highlighted, and the use cases where these ontologies have been used are indicated. Additionally, use case examples are annotated with different ontologies in order to illustrate their capabilities and showcase the differences between reviewed ontologies. Finally, this article tries to answer two research questions: Is there a firm basis, broadly admitted by the community, for the development of such a networked ontology infrastructure for the observations and actuations in buildings? What ontologies may be considered helpful towards that goal?},
  archive      = {J_SW},
  author       = {Esnaola-Gonzalez, Iker and Bermúdez, Jesús and Fernandez, Izaskun and Arnaiz, Aitor},
  doi          = {10.3233/SW-200378},
  journal      = {Semantic Web},
  month        = {8},
  number       = {4},
  pages        = {593-621},
  shortjournal = {Semantic Web},
  title        = {Ontologies for observations and actuations in buildings: A survey},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weather data publication on the LOD using SOSA/SSN ontology.
<em>SW</em>, <em>11</em>(4), 581–591. (<a
href="https://doi.org/10.3233/SW-200375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an RDF dataset of meteorological measurements. The measurements come from one weather station at the Irstea experimental farm located in Montoldre. The measurements have been made from August 2018 until now. They have been transformed and published as Linked Open Data (LOD). The data schema is based on the new version of the Semantic Sensor Network ontology. This ontology version integrates the Sensor, Observation, Sample, and Actuator pattern. We first present the network of ontologies used to organize the data. Then, the transformation process for publishing the dataset is detailed. To conclude we present some use cases of queries related to Irstea research projects.},
  archive      = {J_SW},
  author       = {Roussey, Catherine and Bernard, Stephan and André, Géraldine and Boffety, Daniel},
  doi          = {10.3233/SW-200375},
  journal      = {Semantic Web},
  month        = {8},
  number       = {4},
  pages        = {581-591},
  shortjournal = {Semantic Web},
  title        = {Weather data publication on the LOD using SOSA/SSN ontology},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantics in the edge: Sensors and actuators in the web of
linked data and things. <em>SW</em>, <em>11</em>(4), 571–580. (<a
href="https://doi.org/10.3233/SW-200379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Cena, Federica and Haller, Armin and Lefrançois, Maxime},
  doi          = {10.3233/SW-200379},
  journal      = {Semantic Web},
  month        = {8},
  number       = {4},
  pages        = {571-580},
  shortjournal = {Semantic Web},
  title        = {Semantics in the edge: Sensors and actuators in the web of linked data and things},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning expressive linkage rules from sparse data.
<em>SW</em>, <em>11</em>(3), 549–567. (<a
href="https://doi.org/10.3233/SW-190356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central problem in the context of the Web of Data as well as in data integration in general is to identify entities in different data sources that describe the same real-world object. There exists a large body of research on entity resolution. Interestingly, most of the existing research focuses on entity resolution on dense data, meaning data that does not contain too many missing values. This paper sets a different focus and explores learning expressive linkage rules from as well as applying these rules to sparse data, i.e. data exhibiting a large amount of missing values. Sparse data is a common challenge in application domains such as e-commerce, online hotel booking, or online recruiting. We propose and compare three entity resolution methods that employ genetic programming to learn expressive linkage rules from sparse data. First, we introduce the GenLinkGL algorithm which learns groups of matching rules and applies specific rules out of these groups depending on which values are missing from a pair of records. Next, we propose GenLinkSA, which employs selective aggregation operators within rules. These operators exclude misleading similarity scores (which result from missing values) from the aggregations, but on the other hand also penalize the uncertainty that results from missing values. Finally, we introduce GenLinkComb, an algorithm which combines the central ideas of the previous two into one integrated method. We evaluate all methods using six benchmark datasets: three of them are e-commerce product datasets, the other datasets describe restaurants, movies, and drugs. We show improvements of up to 16% F-measure compared to handwritten rules, on average 12% F-measure improvement compared to the original GenLink algorithm, 15% compared to EAGLE, 8% compared to FEBRL, and 5% compared to CoSum-P.},
  archive      = {J_SW},
  author       = {Petrovski, Petar and Bizer, Christian},
  doi          = {10.3233/SW-190356},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {549-567},
  shortjournal = {Semantic Web},
  title        = {Learning expressive linkage rules from sparse data},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic modeling for engineering data analytics solutions.
<em>SW</em>, <em>11</em>(3), 525–547. (<a
href="https://doi.org/10.3233/SW-190352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Analytics Solution (DAS) engineering often involves multiple tasks from data exploration to result presentation which are applied in various contexts and on different datasets. Semantic modeling based on the open world assumption supports flexible modeling of linked knowledge. The objective of this paper is to review existing techniques that leverage semantic web technologies to tackle challenges such as heterogeneity and changing requirements in DAS engineering. We explore the application scope of those techniques, the different types of semantic concepts they use and the role these concepts play during DAS development process. To gather evidence for the study we performed a systematic mapping study by identifying and reviewing 82 papers that incorporate semantic models in engineering DASs. One of the paper’s findings is that existing models can be classified within four types of knowledge spheres: domain knowledge, analytics knowledge, services and user intentions. Another finding is to show how this knowledge is used in literature to enhance different tasks within the analytics process. We conclude our study by discussing the limitations of the existing body of research, showcasing the potential of semantic modeling to enhance DASs and presenting the possibility of leveraging ontologies for effective end-to-end DAS engineering.},
  archive      = {J_SW},
  author       = {Bandara, Madhushi and Rabhi, Fethi A.},
  doi          = {10.3233/SW-190352},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {525-547},
  shortjournal = {Semantic Web},
  title        = {Semantic modeling for engineering data analytics solutions},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of object-triple mapping libraries.
<em>SW</em>, <em>11</em>(3), 483–524. (<a
href="https://doi.org/10.3233/SW-190345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-independent information systems like ontology editors provide only limited usability for non-experts when domain-specific linked data need to be created. On the contrary, domain-specific applications require adequate architecture for data authoring and validation, typically using the object- oriented paradigm. So far, several software libraries mapping the RDF model (representing linked data) to the object model have been introduced in the literature. In this paper, we develop a novel framework for comparison of object-triple mapping solutions in terms of features and performance. For feature comparison, we designed a set of qualitative criteria reflecting object-oriented application developer’s needs. For the performance comparison, we introduce a benchmark based on a real-world information system that we implemented using one of the compared object-triple mapping (OTM) solutions – JOPA. We present a detailed evaluation of a selected set of OTM libraries, showing how they differ in terms of features. We further narrow down the set of selected OTM libraries to contain only RDF4J-compatible ones and use the benchmark to measure their time and memory efficiency.},
  archive      = {J_SW},
  author       = {Ledvinka, Martin and Křemen, Petr},
  doi          = {10.3233/SW-190345},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {483-524},
  shortjournal = {Semantic Web},
  title        = {A&amp;nbsp;comparison of object-triple mapping libraries},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Completeness and soundness guarantees for conjunctive SPARQL
queries over RDF data sources with completeness statements. <em>SW</em>,
<em>11</em>(3), 441–482. (<a
href="https://doi.org/10.3233/SW-190344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDF generally follows the open-world assumption: information is incomplete by default. Consequently, SPARQL queries cannot retrieve with certainty complete answers, and even worse, when they involve negation, it is unclear whether they produce sound answers. Nevertheless, there is hope to lift this limitation. On many specific topics (e.g., children of Trump, Apollo 11 crew, EU founders), RDF data sources contain complete information, a fact that can be made explicit through completeness statements. In this work, we leverage completeness statements over RDF data sources to provide guarantees of completeness and soundness for conjunctive SPARQL queries. We develop a technique to check whether query completeness can be guaranteed by taking into account also the specifics of the queried graph, and analyze the complexity of such checking. For queries with negation, we approach the problem of query soundness checking, and distinguish between answer soundness (i.e., is an answer of a query sound?) and pattern soundness (i.e., is a query as a whole sound?). We provide a formalization and characterize the soundness problem via a reduction to the completeness problem. We further develop heuristic techniques for completeness checking, and conduct experimental evaluations based on Wikidata, a prominent, real-world knowledge base, to demonstrate the feasibility of our approach.},
  archive      = {J_SW},
  author       = {Darari, Fariz and Nutt, Werner and Razniewski, Simon and Rudolph, Sebastian},
  doi          = {10.3233/SW-190344},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {441-482},
  shortjournal = {Semantic Web},
  title        = {Completeness and soundness guarantees for conjunctive SPARQL queries over RDF data sources with completeness statements},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards a question answering system over the semantic web.
<em>SW</em>, <em>11</em>(3), 421–439. (<a
href="https://doi.org/10.3233/SW-190343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the Semantic Web, a lot of new structured data has become available on the Web in the form of knowledge bases (KBs). Making this valuable data accessible and usable for end-users is one of the main goals of question answering (QA) over KBs. Most current QA systems query one KB, in one language (namely English). The existing approaches are not designed to be easily adaptable to new KBs and languages. We first introduce a new approach for translating natural language questions to SPARQL queries. It is able to query several KBs simultaneously, in different languages, and can easily be ported to other KBs and languages. In our evaluation, the impact of our approach is proven using 5 different well-known and large KBs: Wikidata, DBpedia, MusicBrainz, DBLP and Freebase as well as 5 different languages namely English, German, French, Italian and Spanish. Second, we show how we integrated our approach, to make it easily accessible by the research community and by end-users. To summarize, we provide a conceptional solution for multilingual, KB-agnostic question answering over the Semantic Web. The provided first approximation validates this concept.},
  archive      = {J_SW},
  author       = {Diefenbach, Dennis and Both, Andreas and Singh, Kamal and Maret, Pierre},
  doi          = {10.3233/SW-190343},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {421-439},
  shortjournal = {Semantic Web},
  title        = {Towards a question answering system over the semantic web},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining RDF and SPARQL with CP-theories to reason about
preferences in a linked data setting. <em>SW</em>, <em>11</em>(3),
391–419. (<a href="https://doi.org/10.3233/SW-180339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preference representation and reasoning play a central role in supporting users with complex and multi-factorial decision processes. In fact, user tastes can be used to filter information and data in a personalized way, thus maximizing their expected utility. Over the years, many frameworks and lan guages have been proposed to deal with user preferences. Among them, one of the most prominent formalism to represent and reason with (qualitative) conditional preferences (CPs) are conditional preference theories (CP-theories). In this paper, we show how to combine them with Semantic Web technologies in order to encode in a standard SPARQL 1.1 query the semantics of a set of CP statements representing user preferences by means of RDF triples that refer to a “preference” OWL ontology. In particular, here we focus on context-uniform conditional (cuc) acyclic CP-theories [Artif. Intell. 175 2011, 1053–1091]. The framework that we propose allows a standard SPARQL client to query Linked Data datasets, and to order the results of such queries relative to a set of user preferences.},
  archive      = {J_SW},
  author       = {Anelli, Vito Walter and De Leone, Renato and Di Noia, Tommaso and Lukasiewicz, Thomas and Rosati, Jessica},
  doi          = {10.3233/SW-180339},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {391-419},
  shortjournal = {Semantic Web},
  title        = {Combining RDF and SPARQL with CP-theories to reason about preferences in a linked data setting},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A visual modeling approach for the semantic web rule
language. <em>SW</em>, <em>11</em>(2), 361–389. (<a
href="https://doi.org/10.3233/SW-180340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the Semantic Web Rule Language (SWRL) is not a W3C standard, it is widely used for semantic web-based projects as well as for innovative rule-based applications. Thereby, it is used to infer new knowledge from a given fact base. Today, SWRL rules are developed and managed by technical expe rts in text-based editors using software applications such as the Stanford Protégé toolkit. In this paper we present a visual approach which allows users to analyse and modify SWRL rules. By building upon a visual modeling language, the approach includes validation mechanisms and layouting algorithms for visually representing new as well as existing rules. The approach further provides import and export interfaces to common SWRL exchange formats. In this way, its compatibility with widely-used reasoners and semantic web platforms is guaranteed. For ensuring its feasibility, the approach has been prototypically realized using the SeMFIS platform and evaluated using the sample rules as provided in the SWRL specification.},
  archive      = {J_SW},
  author       = {Pittl, Benedikt and Fill, Hans-Georg},
  doi          = {10.3233/SW-180340},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {361-389},
  shortjournal = {Semantic Web},
  title        = {A visual modeling approach for the semantic web rule language},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HDTcrypt: Compression and encryption of RDF datasets.
<em>SW</em>, <em>11</em>(2), 337–359. (<a
href="https://doi.org/10.3233/SW-180335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The publication and interchange of RDF datasets online has experienced significant growth in recent years, promoted by different but complementary efforts, such as Linked Open Data, the Web of Things and RDF stream processing systems. However, the current Linked Data infrastructure does not cater f or the storage and exchange of sensitive or private data. On the one hand, data publishers need means to limit access to confidential data (e.g. health, financial, personal, or other sensitive data). On the other hand, the infrastructure needs to compress RDF graphs in a manner that minimises the amount of data that is both stored and transferred over the wire. In this paper, we demonstrate how HDT – a compressed serialization format for RDF – can be extended to cater for supporting encryption. We propose a number of different graph partitioning strategies and discuss the benefits and tradeoffs of each approach.},
  archive      = {J_SW},
  author       = {Fernández, Javier D. and Kirrane, Sabrina and Polleres, Axel and Steyskal, Simon},
  doi          = {10.3233/SW-180335},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {337-359},
  shortjournal = {Semantic Web},
  title        = {HDTcrypt: Compression and encryption of RDF datasets},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information extraction meets the semantic web: A survey.
<em>SW</em>, <em>11</em>(2), 255–335. (<a
href="https://doi.org/10.3233/SW-180333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a comprehensive survey of the research literature that applies Information Extraction techniques in a Semantic Web setting. Works in the intersection of these two areas can be seen from two overlapping perspectives: using Semantic Web resources (languages/ontologies/knowledge-bases/tools ) to improve Information Extraction, and/or using Information Extraction to populate the Semantic Web. In more detail, we focus on the extraction and linking of three elements: entities, concepts and relations. Extraction involves identifying (textual) mentions referring to such elements in a given unstructured or semi-structured input source. Linking involves associating each such mention with an appropriate disambiguated identifier referring to the same element in a Semantic Web knowledge-base (or ontology), in some cases creating a new identifier where necessary. With respect to entities, works involving (Named) Entity Recognition, Entity Disambiguation, Entity Linking, etc. in the context of the Semantic Web are considered. With respect to concepts, works involving Terminology Extraction, Keyword Extraction, Topic Modeling, Topic Labeling, etc., in the context of the Semantic Web are considered. Finally, with respect to relations, works involving Relation Extraction in the context of the Semantic Web are considered. The focus of the majority of the survey is on works applied to unstructured sources (text in natural language); however, we also provide an overview of works that develop custom techniques adapted for semi-structured inputs, namely markup documents and web tables.},
  archive      = {J_SW},
  author       = {Martinez-Rodriguez, Jose L. and Hogan, Aidan and Lopez-Arevalo, Ivan},
  doi          = {10.3233/SW-180333},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {255-335},
  shortjournal = {Semantic Web},
  title        = {Information extraction meets the semantic web: A survey},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). XMLSchema2ShEx: Converting XML validation to RDF validation.
<em>SW</em>, <em>11</em>(2), 235–253. (<a
href="https://doi.org/10.3233/SW-180329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDF validation is a field where the Semantic Web community is currently focusing attention. Besides, there is a recent trend to migrate data from different sources to semantic web formats. Therefore, in order to facilitate this transformation, we propose: a set of mappings that can be used to conve rt from XML Schema to Shape Expressions (ShEx), a prototype that implements a subset of the proposed mappings, an example application to obtain a ShEx schema from an XML Schema and a discussion on conversion implications of non-deterministic schemata. We demonstrate that an XML and its corresponding XML Schema are still valid when converted to their RDF and ShEx counterparts. This conversion, along with the development of other format mappings, could drive to an improvement of data interoperability due to the reduction of the technological gap.},
  archive      = {J_SW},
  author       = {Garcia-Gonzalez, Herminio and Labra-Gayo, Jose Emilio},
  doi          = {10.3233/SW-180329},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {235-253},
  shortjournal = {Semantic Web},
  title        = {XMLSchema2ShEx: Converting XML validation to RDF validation},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using knowledge anchors to facilitate user exploration of
data graphs. <em>SW</em>, <em>11</em>(2), 205–234. (<a
href="https://doi.org/10.3233/SW-190347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates how to facilitate users’ exploration through data graphs. The prime focus is on knowledge utility, i.e. increasing a user’s domain knowledge while exploring a data graph, which is crucial in the vast number of user-facing semantic web applications where the users are not exp erts in the domain. We introduce a highly unique exploration support mechanism underpinned by the subsumption theory for meaningful learning. A core algorithmic component for operationalising the subsumption theory for meaningful learning is the automatic identification of knowledge anchors in a data graph (KADG). We present several metrics for identifying KADG which are evaluated against familiar concepts in human cognitive structures. The second key component is a subsumption algorithm that utilises KADG for generating exploration paths for knowledge expansion. The implementation of the algorithm is applied in the context of a Semantic data browser in a music domain. The resultant exploration paths are evaluated in a task-driven experimental user study compared to free data graph exploration. The findings show that exploration paths, based on subsumption and using knowledge anchors, lead to significantly higher increase in the users’ conceptual knowledge and better usability than free exploration of data graphs. The work opens a new avenue in semantic data exploration which investigates the link between learning and knowledge exploration. We provide the first framework that adopts educational theories to inform data graph exploration for knowledge expansion which extends the value of exploration and enables broader applications of data graphs in systems where the end users are not experts in the specific domain.},
  archive      = {J_SW},
  author       = {Al-Tawil, Marwan and Dimitrova, Vania and Thakker, Dhavalkumar},
  doi          = {10.3233/SW-190347},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {205-234},
  shortjournal = {Semantic Web},
  title        = {Using knowledge anchors to facilitate user exploration of data graphs},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning for the semantic web: Lessons learnt and
next research directions. <em>SW</em>, <em>11</em>(1), 195–203. (<a
href="https://doi.org/10.3233/SW-200388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning methods have been introduced in the Semantic Web for solving problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level). Whilst initially mainly focussing on symbol-based solutions, recently numeric-based approaches have received major attention, motivated by the need to scale on the very large Web of Data. In this paper, the most representative proposals, belonging to the aforementioned categories are surveyed, jointly with the analysis of their main peculiarities and drawbacks. Afterwards the main envisioned research directions for further developing Machine Learning solutions for the Semantic Web are presented.},
  archive      = {J_SW},
  author       = {d’Amato, Claudia},
  doi          = {10.3233/SW-200388},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {195-203},
  shortjournal = {Semantic Web},
  title        = {Machine learning for the semantic web: Lessons learnt and next research directions},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using the semantic web in digital humanities: Shift from
data publishing to data-analysis and serendipitous knowledge discovery.
<em>SW</em>, <em>11</em>(1), 187–193. (<a
href="https://doi.org/10.3233/SW-190386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses a shift of focus in research on Cultural Heritage semantic portals, based on Linked Data, and envisions and proposes new directions of research. Three generations of portals are identified: Ten years ago the research focus in semantic portal development was on data harmonizatio n, aggregation, search, and browsing (“first generation systems”). At the moment, the rise of Digital Humanities research has started to shift the focus to providing the user with integrated tools for solving research problems in interactive ways (“second generation systems”). This paper envisions and argues that the next step ahead to “third generation systems” is based on Artificial Intelligence: future portals not only provide tools for the human to solve problems but are used for finding research problems in the first place, for addressing them, and even for solving them automatically under the constraints set by the human researcher. Such systems should preferably be able to explain their reasoning, which is an important aspect in the source critical humanities research tradition. The second and third generation systems set new challenges for both computer scientists and humanities researchers.},
  archive      = {J_SW},
  author       = {Hyvönen, Eero},
  doi          = {10.3233/SW-190386},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {187-193},
  shortjournal = {Semantic Web},
  title        = {Using the semantic web in digital humanities: Shift from data publishing to data-analysis and serendipitous knowledge discovery},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The semantic web: Two decades on. <em>SW</em>,
<em>11</em>(1), 169–185. (<a
href="https://doi.org/10.3233/SW-190387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More than two decades have passed since the establishment of the initial cornerstones of the Semantic Web. Since its inception, opinions have remained divided regarding the past, present and potential future impact of the Semantic Web. In this paper – and in light of the results of over two decades of development on both the Semantic Web and related technologies – we reflect on the current status of the Semantic Web, the impact it has had thus far, and future challenges. We first review some of the external criticism of this vision that has been put forward by various authors; we draw together the individual critiques, arguing both for and against each point based on the current state of adoption. We then present the results of a questionnaire that we have posed to the Semantic Web mailing list in order to understand respondents’ perspective(s) regarding the degree to which the original Semantic Web vision has been realised, the impact it can potentially have on the Web (and other settings), its success stories thus far, as well as the degree to which they agree with the aforementioned critiques of the Semantic Web in terms of both its current state and future feasibility. We conclude by reflecting on future challenges and opportunities in the area.},
  archive      = {J_SW},
  author       = {Hogan, Aidan},
  doi          = {10.3233/SW-190387},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {169-185},
  shortjournal = {Semantic Web},
  title        = {The semantic web: Two decades on},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ontological challenges to cohabitation with self-taught
robots. <em>SW</em>, <em>11</em>(1), 161–167. (<a
href="https://doi.org/10.3233/SW-190385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When you meet a delivery robot in a narrow street it stops to let you pass. It was built to give you precedence. What happens if you run into a robot that was not trained by or for humans? The existence in our environment of robots which do not abide by human behavioral rules and social systems mig ht sound odd, but is a case we may encounter in the future. In this paper, self-taught robots are artificial embodied agents that, thanks for instance to AI learning techniques, manage to survive in the environment without embracing behavioral or judgment rules given and used by humans. The paper argues that our ontological systems are not suitable to understand and cope with artificial agents. The arguments are speculative rather than empirical, and the goal is to drive attention to new ontological challenges.},
  archive      = {J_SW},
  author       = {Borgo, Stefano},
  doi          = {10.3233/SW-190385},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {161-167},
  shortjournal = {Semantic Web},
  title        = {Ontological challenges to cohabitation with self-taught robots},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards a new generation of ontology based data access.
<em>SW</em>, <em>11</em>(1), 153–160. (<a
href="https://doi.org/10.3233/SW-190384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology Based Data Access (OBDA) refers to a range of techniques, algorithms and systems that can be used to deal with the heterogeneity of data that is common inside many organisations as well as in inter-organisational settings and more openly on the Web. In OBDA, ontologies are used to provide a global view over multiple local datasets; and mappings are commonly used to describe the relationships between such global and local schemas. Since its inception, this area has evolved in several directions. Initially, the focus was on the translation of original sources into a global schema, and its materialisation, including non-OBDA approaches such as the use of Extract Transform Load (ETL) workflows in data warehouses and, more recently, in data lakes. Then OBDA-based query translation techniques, relying on mappings, were proposed, with the aim of removing the need for materialisation, something especially useful for very dynamic data sources. We think that we are now witnessing the emergence of a new generation of OBDA approaches. It is driven by the fact that a new set of declarative mapping languages, most of which stem from the W3C Recommendation R2RML for Relational Databases (RDB), are being created. In this vision paper, we enumerate the reasons why new mapping languages are being introduced. We discuss why it may be relevant to work on translations among them, so as to benefit from the engines associated to each of them whenever one language and/or engine is more suitable than another. We discuss the emerging concept of “mapping translation”, the basis for this new generation of OBDA, together with some of its desirable properties: information preservation and query result preservation. We show several scenarios where mapping translation can be or is being already applied, even though this term has not necessarily been used in existing literature.},
  archive      = {J_SW},
  author       = {Corcho, Oscar and Priyatna, Freddy and Chaves-Fraga, David},
  doi          = {10.3233/SW-190384},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {153-160},
  shortjournal = {Semantic Web},
  title        = {Towards a new generation of ontology based data access},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Closing the loop between knowledge patterns in cognition and
the semantic web. <em>SW</em>, <em>11</em>(1), 139–151. (<a
href="https://doi.org/10.3233/SW-190383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss currently open issues in the discovery and representation of knowledge patterns in computational processing of meaning, in order to improve interoperability and cognitive validity of web-based semantics. We present the current state of knowledge patterns (KP) in Knowledge Representation, the Semantic Web and Cognitive Sciences, focusing on an intensional abstraction of heterogeneous predicates as a formal foundation for KP.},
  archive      = {J_SW},
  author       = {Gangemi, Aldo},
  doi          = {10.3233/SW-190383},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {139-151},
  shortjournal = {Semantic Web},
  title        = {Closing the loop between knowledge patterns in cognition and the semantic web},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ontology engineering: Current state, challenges, and future
directions. <em>SW</em>, <em>11</em>(1), 125–138. (<a
href="https://doi.org/10.3233/SW-190382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, ontologies have become widely adopted in a variety of fields ranging from biomedicine, to finance, engineering, law, and cultural heritage. The ontology engineering field has been strengthened by the adoption of several standards pertaining to ontologies, by the development or e xtension of ontology building tools, and by a wider recognition of the importance of standardized vocabularies and formalized semantics. Research into ontology engineering has also produced methods and tools that are used more and more in production settings. Despite all these advancements, ontology engineering is still a difficult process, and many challenges still remain to be solved. This paper gives an overview of how the ontology engineering field has evolved in the last decade and discusses some of the unsolved issues and opportunities for future research.},
  archive      = {J_SW},
  author       = {Tudorache, Tania},
  doi          = {10.3233/SW-190382},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {125-138},
  shortjournal = {Semantic Web},
  title        = {Ontology engineering: Current state, challenges, and future directions},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantics for cyber-physical systems: A cross-domain
perspective. <em>SW</em>, <em>11</em>(1), 115–124. (<a
href="https://doi.org/10.3233/SW-190381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern life is increasingly made more comfortable, efficient, and sustainable by the smart systems that surround us: smart buildings monitor and adjust temperature levels to achieve occupant comfort while optimizing energy consumption; smart energy grids reconfigure dynamically to make the best use of ad-hoc energy produced by a host of distributed energy producers; smart factories can be reconfigured on the shop-floor to efficiently produce a diverse range of products. These complex systems can only be realized by tightly integrating components in the physical space (sensors, actuators) with advanced software algorithms in the cyber-space, thus creating so-called Cyber-Physical Systems (CPS). Semantic Web technologies (SWT) have seen a natural uptake in several areas based on CPS, given that CPS are data and knowledge intensive while providing advanced functionalities typical of semantics-based intelligent systems. Yet, so far, this uptake has primarily happened within the boundaries of application domains resulting in somewhat disconnected research communities. In this paper, we take a cross-domain perspective by synthesizing our experiences of using SWTs during the engineering and operation of CPSs in smart manufacturing, smart buildings, and smart grids. We discuss use cases that are amenable to the use of SWTs, benefits and challenges of using these technologies in the CPS lifecycle as well as emerging future trends. While non-exhaustive, our paper aims at opening up a dialog between these fields and at putting the foundation for a research area on semantics in CPS.},
  archive      = {J_SW},
  author       = {Sabou, Marta and Biffl, Stefan and Einfalt, Alfred and Krammer, Lukas and Kastner, Wolfgang and Ekaputra, Fajar J.},
  doi          = {10.3233/SW-190381},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {115-124},
  shortjournal = {Semantic Web},
  title        = {Semantics for cyber-physical systems: A&amp;nbsp;cross-domain perspective},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A more decentralized vision for linked data. <em>SW</em>,
<em>11</em>(1), 101–113. (<a
href="https://doi.org/10.3233/SW-190380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this deliberately provocative position paper, we claim that more than ten years into Linked Data there are still (too?) many unresolved challenges towards arriving at a truly machine-readable and decentralized Web of data. We take a deeper look at key challenges in usage and adoption of Linked D ata from the ever-present “LOD cloud” diagram.1 Herein, we try to highlight and exemplify both key technical and non-technical challenges to the success of LOD, and we outline potential solution strategies. We hope that this paper will serve as a discussion basis for a fresh start towards more actionable, truly decentralized Linked Data, and as a call to the community to join forces.},
  archive      = {J_SW},
  author       = {Polleres, Axel and Kamdar, Maulik Rajendra and Fernández, Javier David and Tudorache, Tania and Musen, Mark Alan},
  doi          = {10.3233/SW-190380},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {101-113},
  shortjournal = {Semantic Web},
  title        = {A more decentralized vision for linked data},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Are we better off with just one ontology on the web?
<em>SW</em>, <em>11</em>(1), 87–99. (<a
href="https://doi.org/10.3233/SW-190379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontologies have been used on the Web to enable semantic interoperability between parties that publish information independently of each other. They have also played an important role in the emergence of Linked Data. However, many ontologies on the Web do not see much use beyond their initial deploy ment and purpose in one dataset and therefore should rather be called what they are – (local) schemas, which per se do not provide any interoperable semantics. Only few ontologies are truly used as a shared conceptualization between different parties, mostly in controlled environments such as the BioPortal. In this paper, we discuss open challenges relating to true re-use of ontologies on the Web and raise the question: “are we better off with just one ontology on the Web?”},
  archive      = {J_SW},
  author       = {Haller, Armin and Polleres, Axel},
  doi          = {10.3233/SW-190379},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {87-99},
  shortjournal = {Semantic Web},
  title        = {Are we better off with just one ontology on the web?},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ontologies as nested facet systems for human–data
interaction. <em>SW</em>, <em>11</em>(1), 79–86. (<a
href="https://doi.org/10.3233/SW-190378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Irrespective of data size and complexity, query and exploration tools for accessing data resources remain a central linkage for human–data interaction. A fundamental barrier in making query interfaces easier to use, ultimately as easy as online shopping, is the lack of faceted, interactive capabili ties. We propose to repurpose existing ontologies by transforming them into nested facet systems (NFS) to support human–data interaction. Two basic issues need to be addressed for this to happen: one is that the structure and quality of ontologies need to be examined and elevated for the purpose of NFS; the second is that mappings from data-source specific metadata to a corresponding NFS need to be developed to support this new generation of NFS-enabled web-interfaces. The purpose of this paper is to introduce the concept of NFS and outline opportunities involved in using ontologies as NFS for querying and exploring data, especially in the biomedical domain.},
  archive      = {J_SW},
  author       = {Zhang, Guo-Qiang and Tao, Shiqiang and Zeng, Ningzhou and Cui, Licong},
  doi          = {10.3233/SW-190378},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {79-86},
  shortjournal = {Semantic Web},
  title        = {Ontologies as nested facet systems for human–data interaction},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Creative AI: A new avenue for the semantic web? <em>SW</em>,
<em>11</em>(1), 69–78. (<a
href="https://doi.org/10.3233/SW-190377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational Creativity (or artificial creativity) is a multidisciplinary field, researching how to construct computer programs that model, simulate, exhibit or enhance creative behaviour. This vision paper explores a potential of the Semantic Web and its technologies for creative AI. Possible use s of the Semantic Web and semantic technologies are discussed, regarding three types of creativity: i) exploratory creativity, ii) combinational creativity, and iii) transformational creativity and relevant research questions. For exploratory creativity, how can we explore the limits of what is possible, while remaining bound by a set of existing domain axioms, templates, and rules, expressed with semantic technologies? To achieve a combinational creativity, how can we combine or blend existing concepts, frames, ontology design patterns, and other constructs, and benefit from cross-fertilization? Ultimately, can we use ontologies and knowledge graphs, which describe an existing domain with its constraints and, applying a meta-rule for transformational creativity, start dropping constraints and adding new constraints to produce novel artifacts? Together with these new challenges, the paper also provides pointers to emerging and growing application domains of Semantic Web related to computational creativity: from recipe generation to scientific discovery and creative design.},
  archive      = {J_SW},
  author       = {Ławrynowicz, Agnieszka},
  doi          = {10.3233/SW-190377},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {69-78},
  shortjournal = {Semantic Web},
  title        = {Creative AI: A new avenue for the&amp;nbsp;Semantic&amp;nbsp;Web?},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A map without a legend. <em>SW</em>, <em>11</em>(1), 63–68.
(<a href="https://doi.org/10.3233/SW-190376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current state of the semantic web is focused on data. This is a worthwhile progress in web content processing and interoperability. However, this does only marginally contribute to knowledge improvement and evolution. Understanding the world, and interpreting data, requires knowledge. Not knowl edge cast in stone for ever, but knowledge that can seamlessly evolve; not knowledge from one single authority, but diverse knowledge sources which stimulate confrontation and robustness; not consistent knowledge at web scale, but local theories that can be combined. We discuss two different ways in which semantic web technologies can greatly contribute to the advancement of knowledge: semantic eScience and cultural knowledge evolution.},
  archive      = {J_SW},
  author       = {Euzenat, Jérôme},
  doi          = {10.3233/SW-190376},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {63-68},
  shortjournal = {Semantic Web},
  title        = {A map without a legend},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid reasoning in knowledge graphs: Combing symbolic
reasoning and statistical reasoning. <em>SW</em>, <em>11</em>(1), 53–62.
(<a href="https://doi.org/10.3233/SW-190375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) contain rich resources that represent human knowledge in the world. There are mainly two kinds of reasoning techniques in knowledge graphs, symbolic reasoning and statistical reasoning. However, both of them have their merits and limitations. Therefore, it is desirable to com bine them to provide hybrid reasoning in a knowledge graph. In this paper, we present the first work on the survey of methods for hybrid reasoning in knowledge graphs. We categorize existing methods based on applications of reasoning techniques, and introduce the key ideas of them. Finally, we re-examine the remaining research problems to be solved and provide an outlook to future directions for hybrid reasoning in knowledge graphs.},
  archive      = {J_SW},
  author       = {Li, Weizhuo and Qi, Guilin and Ji, Qiu},
  doi          = {10.3233/SW-190375},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {53-62},
  shortjournal = {Semantic Web},
  title        = {Hybrid reasoning in knowledge graphs: Combing symbolic reasoning and statistical reasoning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the role of knowledge graphs in explainable AI.
<em>SW</em>, <em>11</em>(1), 41–51. (<a
href="https://doi.org/10.3233/SW-190374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current hype of Artificial Intelligence (AI) mostly refers to the success of machine learning and its sub-domain of deep learning. However, AI is also about other areas, such as Knowledge Representation and Reasoning, or Distributed AI, i.e., areas that need to be combined to reach the level of intelligence initially envisioned in the 1950s. Explainable AI (XAI) now refers to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. This paper reviews XAI not only from a Machine Learning perspective, but also from the other AI research areas, such as AI Planning or Constraint Satisfaction and Search. We expose the XAI challenges of AI fields, their existing approaches, limitations and opportunities for Knowledge Graphs and their underlying technologies.},
  archive      = {J_SW},
  author       = {Lecue, Freddy},
  doi          = {10.3233/SW-190374},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {41-51},
  shortjournal = {Semantic Web},
  title        = {On the role of knowledge graphs in explainable AI},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural language models for the multilingual, transcultural,
and multimodal semantic web. <em>SW</em>, <em>11</em>(1), 29–39. (<a
href="https://doi.org/10.3233/SW-190373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vision of a truly multilingual Semantic Web has found strong support with the Linguistic Linked Open Data community. Standards, such as OntoLex-Lemon, highlight the importance of explicit linguistic modeling in relation to ontologies and knowledge graphs. Nevertheless, there is room for improveme nt in terms of automation, usability, and interoperability. Neural Language Models have achieved several breakthroughs and successes considerably beyond Natural Language Processing (NLP) tasks and recently also in terms of multimodal representations. Several paths naturally open up to port these successes to the Semantic Web, from automatically translating linguistic information associated with structured knowledge resources to multimodal question-answering with machine translation. Language is also an important vehicle for culture, an aspect that deserves considerably more attention. Building on existing approaches, this article envisions joint forces between Neural Language Models and Semantic Web technologies for multilingual, transcultural, and multimodal information access and presents open challenges and opportunities in this direction.},
  archive      = {J_SW},
  author       = {Gromann, Dagmar},
  doi          = {10.3233/SW-190373},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {29-39},
  shortjournal = {Semantic Web},
  title        = {Neural language models for the multilingual, transcultural, and multimodal semantic web},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The semantic web identity crisis: In search of the
trivialities that never were. <em>SW</em>, <em>11</em>(1), 19–27. (<a
href="https://doi.org/10.3233/SW-190372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a domain with a strong focus on unambiguous identifiers and meaning, the Semantic Web research field itself has a surprisingly ill-defined sense of identity. Started at the end of the 1990s at the intersection of databases, logic, and Web, and influenced along the way by all major tech hypes su ch as Big Data and machine learning, our research community needs to look in the mirror to understand who we really are. The key question amid all possible directions is pinpointing the important challenges we are uniquely positioned to tackle. In this article, we highlight the community’s unconscious bias toward addressing the Paretonian 80% of problems through research – handwavingly assuming that trivial engineering can solve the remaining 20%. In reality, that overlooked 20% could actually require 80% of the total effort and involve significantly more research than we are inclined to think, because our theoretical experimentation environments are vastly different from the open Web. As it turns out, these formerly neglected “trivialities” might very well harbor those research opportunities that only our community can seize, thereby giving us a clear hint of how we can orient ourselves to maximize our impact on the future. If we are hesitant to step up, more pragmatic minds will gladly reinvent technology for the real world, only covering a fraction of the opportunities we dream of.},
  archive      = {J_SW},
  author       = {Verborgh, Ruben and Vander Sande, Miel},
  doi          = {10.3233/SW-190372},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {19-27},
  shortjournal = {Semantic Web},
  title        = {The semantic web identity crisis: In search of the trivialities that never were},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging knowledge graphs for big data integration: The XI
pipeline. <em>SW</em>, <em>11</em>(1), 13–17. (<a
href="https://doi.org/10.3233/SW-190371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article gives an overview of recent efforts focusing on integrating heterogeneous data using Knowledge Graphs. I introduce a pipeline consisting of five steps to integrate semi-structured or unstructured content. I discuss some of the key applications of this pipeline through three use-cases, and present the lessons learnt along the way while designing and building data integration systems.},
  archive      = {J_SW},
  author       = {Cudré-Mauroux, Philippe},
  doi          = {10.3233/SW-190371},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {13-17},
  shortjournal = {Semantic Web},
  title        = {Leveraging knowledge graphs for big data integration: The XI pipeline},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural-symbolic integration and the semantic web.
<em>SW</em>, <em>11</em>(1), 3–11. (<a
href="https://doi.org/10.3233/SW-190368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic Systems in Artificial Intelligence which are based on formal logic and deductive reasoning are fundamentally different from Artificial Intelligence systems based on artificial neural networks, such as deep learning approaches. The difference is not only in their inner workings and general approach, but also with respect to capabilities. Neural-symbolic Integration, as a field of study, aims to bridge between the two paradigms. In this paper, we will discuss neural-symbolic integration in its relation to the Semantic Web field, with a focus on promises and possible benefits for both, and report on some current research on the topic.},
  archive      = {J_SW},
  author       = {Hitzler, Pascal and Bianchi, Federico and Ebrahimi, Monireh and Sarker, Md Kamruzzaman},
  doi          = {10.3233/SW-190368},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {3-11},
  shortjournal = {Semantic Web},
  title        = {Neural-symbolic integration and the Semantic&amp;nbsp;Web},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gold-level open access at the semantic web journal.
<em>SW</em>, <em>11</em>(1), 1–2. (<a
href="https://doi.org/10.3233/SW-200389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Janowicz, Krzysztof and Hitzler, Pascal},
  doi          = {10.3233/SW-200389},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Semantic Web},
  title        = {Gold-level open access at the semantic web journal},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
