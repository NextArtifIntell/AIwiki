<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IDA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ida---90">IDA - 90</h2>
<ul>
<li><details>
<summary>
(2020). Addressing model uncertainty in probabilistic forecasting
using monte carlo dropout. <em>IDA</em>, <em>24</em>(S1), 185–205. (<a
href="https://doi.org/10.3233/IDA-200015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning models have been developed to address probabilistic forecasting tasks, assuming an implicit stochastic process that relates past observed values to uncertain future values. These models are capable of capturing the inherent uncertainty of the underlying process, but t hey ignore the model uncertainty that comes from the fact of not having infinite data. This work proposes addressing the model uncertainty problem using Monte Carlo dropout, a variational approach that assigns distributions to the weights of a neural network instead of simply using fixed values. This allows to easily adapt common deep learning models currently in use to produce better probabilistic forecasting estimates, in terms of their consideration of uncertainty. The proposal is validated for prediction intervals estimation on seven energy time series, using a popular probabilistic model called Mean Variance Estimation (MVE), as the deep model adapted using the technique.},
  archive      = {J_IDA},
  author       = {Serpell, Cristián and Araya, Ignacio A. and Valle, Carlos and Allende, Héctor},
  doi          = {10.3233/IDA-200015},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {185-205},
  shortjournal = {Intell. Data Anal.},
  title        = {Addressing model uncertainty in probabilistic forecasting using monte carlo dropout},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the generation of multi-label prototypes. <em>IDA</em>,
<em>24</em>(S1), 167–183. (<a
href="https://doi.org/10.3233/IDA-200014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data reduction techniques play a key role in instance-based classification to lower the amount of data to be processed. Prototype generation aims to obtain a reduced training set in order to obtain accurate results with less effort. This translates into a significant reduction in both algorithms’ s patial and temporal burden. This issue is particularly relevant in multi-label classification, which is a generalization of multiclass classification that allows objects to belong to several classes simultaneously. Although this field is quite active in terms of learning algorithms, there is a lack of data reduction methods. In this paper, we propose several prototype generation methods from multi-label datasets based on Granular Computing. The simulations show that these methods significantly reduce the number of examples to a set of prototypes without significantly affecting classifiers’ performance.},
  archive      = {J_IDA},
  author       = {Bello, Marilyn and Nápoles, Gonzalo and Vanhoof, Koen and Bello, Rafael},
  doi          = {10.3233/IDA-200014},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {167-183},
  shortjournal = {Intell. Data Anal.},
  title        = {On the generation of multi-label prototypes},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Interpretable and effective hashing via bernoulli
variational auto-encoders. <em>IDA</em>, <em>24</em>(S1), 141–166. (<a
href="https://doi.org/10.3233/IDA-200013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid increase in the amount of data generated in many fields of science and engineering, information retrieval methods tailored to large-scale datasets have become increasingly important in the last years. Semantic hashing is an emerging technique for this purpose that works on the idea of representing complex data objects, like images and text, using similarity-preserving binary codes that are then used for indexing and search. In this paper, we investigate a hashing algorithm that uses a deep variational auto-encoder to learn and predict the codes. Unlike previous approaches of this type, that learn a continuous (Gaussian) representation and then project the embedding to obtain hash codes, our method employs Bernoulli latent variables in both the training and the prediction stage. Constraining the model to use a binary encoding allow us to obtain a more interpretable representation for hashing: each factor in the generative model represents a bit that should help to reconstruct and thus identify the input pattern. Interestingly, we found that the binary constraint does not lead to a loss but an increase of search accuracy. We argue that continuous formulations learn a representation that can significantly differ from the code used for search. Minding this gap in the design of the auto-encoder can translate into more accurate retrieval results. Extensive experiments on seven datasets involving image data and text data illustrate these findings and demonstrate the advantages of our approach.},
  archive      = {J_IDA},
  author       = {Mena, Francisco and Ñanculef, Ricardo and Valle, Carlos},
  doi          = {10.3233/IDA-200013},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {141-166},
  shortjournal = {Intell. Data Anal.},
  title        = {Interpretable and effective hashing via bernoulli variational auto-encoders},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A time-sensitive model to predict topic popularity in news
providers. <em>IDA</em>, <em>24</em>(S1), 123–140. (<a
href="https://doi.org/10.3233/IDA-200012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volume of news increases everyday, triggering competition for users’ attention. Predicting which topics will become trendy has many applications in domains like marketing or politics, where it is crucial to anticipate how much interest a product or a person will attract. We propose a model for representing topic popularity behavior across time and to predict if a topic will become trendy in the future. Furthermore, we tested our proposal on a real data set from Yahoo News and analyzed the performance of various classifiers for the topic popularity prediction task. Experiments confirmed the validity of the proposed model.},
  archive      = {J_IDA},
  author       = {Prado-Romero, Mario Alfonso and Coto-Santiesteban, Alex and Celi, Alessandro and Stilo, Giovanni},
  doi          = {10.3233/IDA-200012},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {123-140},
  shortjournal = {Intell. Data Anal.},
  title        = {A time-sensitive model to predict topic popularity in news providers},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating pattern restrictions for associative classifiers.
<em>IDA</em>, <em>24</em>(S1), 105–122. (<a
href="https://doi.org/10.3233/IDA-200011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Associative classification is a pattern recognition approach that integrates classification and association rule discovery to build accurate classification models. These models are formed by a collection of contrast patterns that fulfill some restrictions. In this paper, we introduce an experimenta l comparison of the impact of using different restrictions in the classification accuracy. To the best of our knowledge, this is the first time that such analysis is performed, deriving some interesting findings about how restrictions impact on the classification results. Contrasting these results with previously published papers, we found that their conclusions could be unintentionally biased by the restrictions they used. We found, for example, that the jumping restriction could severely damage the pattern quality in the presence of dataset noise. We also found that the minimal support restriction has a different effect in the accuracy of two associative classifiers, therefore deciding which one is the best depends on the support value. This paper opens some interesting lines of research, mainly in the creation of new restrictions and new pattern types by joining different restrictions.},
  archive      = {J_IDA},
  author       = {Andy, González-Méndez and Diana, Martín and Eduardo, Morales and Milton, García-Borroto},
  doi          = {10.3233/IDA-200011},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {105-122},
  shortjournal = {Intell. Data Anal.},
  title        = {Evaluating pattern restrictions for associative classifiers},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SKAdam: An improved scalar extension of KAdam for function
optimization. <em>IDA</em>, <em>24</em>(S1), 87–104. (<a
href="https://doi.org/10.3233/IDA-200010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an improved extension of the previous algorithm of the authors called KAdam that was proposed as a combination of a first-order gradient-based optimizer of stochastic functions, known as the Adam algorithm and the Kalman filter. In the extension presented here, it is proposed to filter each parameter of the objective function using a 1-D Kalman filter; this allows us to switch from matrix and vector calculations to scalar operations. Moreover, it is reduced the impact of the measurement noise factor from the Kalman filter by using an exponential decay in function of the number of epochs for the training. Therefore in this paper, is introduced our proposed method sKAdam , a straightforward improvement over the original algorithm. This extension of KAdam presents a reduced execution time, a reduced computational complexity, and better accuracy as well as keep the properties from Adam of being well suited for problems with large datasets and/or parameters, non-stationary objectives, noisy and/or sparse gradients.},
  archive      = {J_IDA},
  author       = {Camacho, J.D. and Villaseñor, Carlos and Alanis, Alma Y. and Lopez-Franco, Carlos and Arana-Daniel, Nancy},
  doi          = {10.3233/IDA-200010},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {87-104},
  shortjournal = {Intell. Data Anal.},
  title        = {SKAdam: An improved scalar extension of KAdam for function optimization},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Collective annotation patterns in learning from crowds.
<em>IDA</em>, <em>24</em>(S1), 63–86. (<a
href="https://doi.org/10.3233/IDA-200009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of annotated data is one of the major barriers facing machine learning applications today. Learning from crowds, i.e. collecting ground-truth data from multiple inexpensive annotators, has become a common method to cope with this issue. It has been recently shown that modeling the varying quality of the annotations obtained in this way, is fundamental to obtain satisfactory performance in tasks where inexpert annotators may represent the majority but not the most trusted group. Unfortunately, existing techniques represent annotation patterns for each annotator individually, making the models difficult to estimate in large-scale scenarios. In this paper, we present two models to address these problems. Both methods are based on the hypothesis that it is possible to learn collective annotation patterns by introducing confusion matrices that involve groups of data point annotations or annotators. The first approach clusters data points with a common annotation pattern, regardless the annotators from which the labels have been obtained. Implicitly, this method attributes annotation mistakes to the complexity of the data itself and not to the variable behavior of the annotators. The second approach explicitly maps annotators to latent groups that are collectively parametrized to learn a common annotation pattern. Our experimental results show that, compared with other methods for learning from crowds, both methods have advantages in scenarios with a large number of annotators and a small number of annotations per annotator.},
  archive      = {J_IDA},
  author       = {Mena, Francisco and Ñanculef, Ricardo and Valle, Carlos},
  doi          = {10.3233/IDA-200009},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {63-86},
  shortjournal = {Intell. Data Anal.},
  title        = {Collective annotation patterns in learning from crowds},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of OWA operators for automatic keyphrase extraction
in a semantic context. <em>IDA</em>, <em>24</em>(S1), 43–62. (<a
href="https://doi.org/10.3233/IDA-200008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic keyphrase extraction from texts is useful for many computational systems in the fields of natural language processing and text mining. Although a number of solutions to this problem have been described, semantic analysis is one of the least exploited linguistic features in the most widely -known proposals, causing the results obtained to have low accuracy and performance rates. This paper presents an unsupervised method for keyphrase extraction, based on the use of lexico-syntactic patterns for extracting information from texts, and a fuzzy topic modeling. An OWA operator combining several semantic measures was applied to the topic modeling process. This new approach was evaluated with Inspec and 500N-KPCrowd datasets. Several approaches within our proposal were evaluated against each other. A statistical analysis was performed to substantiate the best approach of the proposal. This best approach was also compared with other reported systems, giving promising results.},
  archive      = {J_IDA},
  author       = {Pérez-Guadarramas, Yamel and Barreiro-Guerrero, Manuel and Simón-Cuevas, Alfredo and Romero, Francisco P. and Olivas, José A.},
  doi          = {10.3233/IDA-200008},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {43-62},
  shortjournal = {Intell. Data Anal.},
  title        = {Analysis of OWA operators for automatic keyphrase extraction in a semantic context},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to combine classifiers outputs with the transformer
for text classification. <em>IDA</em>, <em>24</em>(S1), 15–41. (<a
href="https://doi.org/10.3233/IDA-200007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is a fairly explored task that has allowed dealing with a considerable amount of problems. However, one of its main difficulties is to conduct a learning process in data with class imbalance, i.e., datasets with only a few examples in some classes, which often represent the most interesting cases for the task. In this context, text classifiers overfit some particular classes, showing poor performance. To address this problem, we propose a scheme that combines the outputs of different classifiers, coding them in the encoder of a transformer. Feeding also a BERT encoding of each example, the encoder learns a joint representation of the text and the outputs of the classifiers. These encodings are used to train a new text classifier. Since the transformer is a highly complex model, we introduce a data augmentation technique, which allows the representation learning task to be driven without over-fitting the encoding to a particular class. The data augmentation technique also allows for producing a balanced dataset. The combination of both methods, representation learning, and data augmentation, allows improving the performance of trained classifiers. Results in benchmark data for two text classification tasks (stance classification and online harassment detection) show that the proposed scheme outperforms all of its direct competitors.},
  archive      = {J_IDA},
  author       = {Bugueño, Margarita and Mendoza, Marcelo},
  doi          = {10.3233/IDA-200007},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {15-41},
  shortjournal = {Intell. Data Anal.},
  title        = {Learning to combine classifiers outputs with the transformer for text classification},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multilevel term analysis for adaptive document filtering.
<em>IDA</em>, <em>24</em>(S1), 3–14. (<a
href="https://doi.org/10.3233/IDA-200006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans tend to organize information in documents in a logical and intentional way. This organization, which we call textual structure, is commonly in terms of sections, chapters, paragraphs, or sentences. This structure facilitates the understanding of the content that we want to transmit to the re aders. However, such structure, in which we usually encode the semantic content of information, is not usually exploited by the filtering methods for the construction of a user profile. In this work, we propose the use of term relations considering different context levels for enhancing document filtering. We propose methods for obtaining the representation, considering the existence of imbalance between the documents that satisfy the information needs of users, as well as the Cold Start problem (having scarce information) during the initial construction of the user profile. The experiments carried out allowed to assess the impact, in terms of T11SU measure, on the filtering task of the proposed representation.},
  archive      = {J_IDA},
  author       = {Bruzón, Adrian Fonseca and López-López, Aurelio and Pagola, José E. Medina},
  doi          = {10.3233/IDA-200006},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {3-14},
  shortjournal = {Intell. Data Anal.},
  title        = {Multilevel term analysis for adaptive document filtering},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Editorial. <em>IDA</em>, <em>24</em>(S1), 1–2. (<a
href="https://doi.org/10.3233/IDA-200005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-200005},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {S1},
  pages        = {1-2},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Content boosted hybrid filtering for solving pessimistic
user problem in recommendation systems. <em>IDA</em>, <em>24</em>(6),
1477–1496. (<a href="https://doi.org/10.3233/IDA-205244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of a recommendation system is to recommend items of interest to users by analyzing their historical data. Content-based and collaborative filtering are the traditional recommendation strategies, each with its own strengths and weaknesses. Some of their weaknesses can be overcome by co mbining the two strategies. The resulting hybrid system performs qualitatively better than the traditional recommendation systems. However, historical data of some users may consist largely of only likes or only dislikes. Those users are termed as optimistic or pessimistic users respectively. On an average there are around 10 to 20% of pessimistic users present in a given dataset. For pessimistic users, whose profiles have mostly dislikes and very few likes, content-based filtering can hardly recommend any items of interest. In content-based filtering technique pessimistic users get poor recommendations of either uninteresting movies or no recommendations at all. This can be alleviated by boosting the content profiles of pessimistic users using the top-n recommendations of collaborative filtering. This content boosted hybrid filtering system provides a novel list of recommendations even for pessimistic users, with predictive accuracy better than that of a traditional content-based filtering system.},
  archive      = {J_IDA},
  author       = {Sivanaiah, Rajalakshmi and Sakaya Milton, R. and Mirnalinee, T.T.},
  doi          = {10.3233/IDA-205244},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1477-1496},
  shortjournal = {Intell. Data Anal.},
  title        = {Content boosted hybrid filtering for solving pessimistic user problem in recommendation systems},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving multi-view facial expression recognition through
two novel texture-based feature representations. <em>IDA</em>,
<em>24</em>(6), 1455–1476. (<a
href="https://doi.org/10.3233/IDA-194798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although several automatic computer systems have been proposed to address facial expression recognition problems, the majority of them still fail to cope with some requirements of many practical application scenarios. In this paper, one of the most influential and common issues raised in practical application scenarios when applying automatic facial expression recognition system, head pose variation, is comprehensively explored and investigated. In order to do this, two novel texture feature representations are proposed for implementing multi-view facial expression recognition systems in practical environments. These representations combine the block-based techniques with Local Ternary Pattern-based features, providing a more informative and efficient feature representation of the facial images. In addition, an in-house multi-view facial expression database has been designed and collected to allow us to conduct a detailed research study of the effect of out-of-plane pose angles on the performance of a multi-view facial expression recognition system. Along with the proposed in-house dataset, the proposed system is tested on two well-known facial expression databases, CK+ and BU-3DFE datasets. The obtained results shows that the proposed system outperforms current state-of-the-art 2D facial expression systems in the presence of pose variations.},
  archive      = {J_IDA},
  author       = {Wang, Xuejian and Fairhurst, Michael C. and Canuto, Anne M.P.},
  doi          = {10.3233/IDA-194798},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1455-1476},
  shortjournal = {Intell. Data Anal.},
  title        = {Improving multi-view facial expression recognition through two novel texture-based feature representations},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DAF: An adaptive computing framework for multimedia data
streams analysis. <em>IDA</em>, <em>24</em>(6), 1441–1453. (<a
href="https://doi.org/10.3233/IDA-194640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of efficiently online computing/filtering or analysis multimedia streams. In this scenario, we register a large scale of continuous analysis queries to filter pornographic stream items. Each query is a conjunction of filters. For instance, the query “does this image contain a people basking in the beach?” can be resolved by applying the conjunction of water, people, sand, sea filters successively on the stream item. However, the online evaluation of multimedia filters is indeed very expensive, fortunately there usually exist multiple filters shared among a lot of queries. In other words, each filter may occur in multiple queries. An open problem in such a filtering scenario is how to order the filters in an optimal sequence to achieve significant performance. Existing methods are based on a greedy strategy which orders the filters according to three factors (selectivity, popularity, cost). Although all these methods achieve good results, there are still some problems that haven’t addressed yet. First, the selectivity factor is set empirically, which can not adaptively adjust with multimedia stream. Second, the proportion relationships among the three factors (selectivity, cost, popularity) were not considerably explored. Under these observations,in this paper, we propose a Dynamic-Analytic hierarchy process Framework (DAF) which use a time-based compositional forecasting method, which is based on the idea of exponential smoothing, to deal with the factors’ proportion relationships dynamics. Experiments on both synthetic and real lift multimedia streams demonstrate that our proposed framework (DAF) provides much great adaptability in modeling the factors proportion relationships changing over multimedia stream environment.},
  archive      = {J_IDA},
  author       = {Li, Jun and Li, Chao and Tian, Bin and Liu, Yanzhao and Si, Chengxiang},
  doi          = {10.3233/IDA-194640},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1441-1453},
  shortjournal = {Intell. Data Anal.},
  title        = {DAF: An adaptive computing framework for multimedia data streams analysis},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uni- and multivariate probability density models for numeric
subgroup discovery. <em>IDA</em>, <em>24</em>(6), 1403–1439. (<a
href="https://doi.org/10.3233/IDA-194719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgroup Discovery is a supervised, exploratory data mining paradigm that aims to identify subsets of a dataset that show interesting behaviour with respect to some designated target attribute. The way in which such distributional differences are quantified varies with the target attribute type. Th is work concerns continuous targets, which are important in many practical applications. For such targets, differences are often quantified using z-score and similar measures that compare simple statistics such as the mean and variance of the subset and the data. However, most distributions are not fully determined by their mean and variance alone. As a result, measures of distributional difference solely based on such simple statistics will miss potentially interesting subgroups. This work proposes methods to recognise distributional differences in a much broader sense. To this end, density estimation is performed using histogram and kernel density estimation techniques. In the spirit of Exceptional Model Mining, the proposed methods are extended to deal with multiple continuous target attributes, such that comparisons are not restricted to univariate distributions, but are available for joint distributions of any dimensionality. The methods can be incorporated easily into existing Subgroup Discovery frameworks, so no new frameworks are developed.},
  archive      = {J_IDA},
  author       = {Meeng, Marvin and de Vries, Harm and Flach, Peter and Nijssen, Siegfried and Knobbe, Arno},
  doi          = {10.3233/IDA-194719},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1403-1439},
  shortjournal = {Intell. Data Anal.},
  title        = {Uni- and multivariate probability density models for numeric subgroup discovery},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid deep learning model for predicting and targeting
the less immunized area to improve childrens vaccination rate.
<em>IDA</em>, <em>24</em>(6), 1385–1402. (<a
href="https://doi.org/10.3233/IDA-194820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a major and rising interest in India for increasing vaccination rate among peoples to make the nation healthier and safer. In this paper, a new hybrid deep learning model is proposed to predict and target vaccination rates in the less immunized regions. The Rank-Based Multi-Layer Per ceptron (R-MLP) hybrid deep learning framework uses the data collected from the recently updated District Level Household Survey-4 (DLHS). R-MLP model predicts and categorizes the percentage of partly immunized vaccination rates as extreme, low and medium ranges. This predicted findings are cross-verified by Deep Soft Cosine Semantic and Ranking SVM based model (DSS-RSM). DSS-RSM model uses the data obtained from the medical practitioners through a location-based social network. The proposed model predicts and extracts patterns with high similarity frequency for identifying vulnerable low immunization regions. It classifies the predicted patterns into two classes such as Class 1 is denoted as high ranked regions and Class 2 is denoted as low ranked regions based on the percentage of pattern matches. Finally, the results from R-MLP and DSS-RSM models are cross-linked together using ensemble model. This model finds the loss values to identify the target regions were health care program need to be conducted for increasing the level of immunization among children’s. The proposed hybrid deep learning models trains and validates using python-based Keras and TensorFlow deep learning libraries. The performance of the proposed hybrid deep learning model is compared with other variant machine learning techniques such as Decision Tree C5.0, Naive Bayes and Linear Regression. This comparative results are evaluated using evaluation measures such as Precision, Recall, Accuracy and F1-Measure. Our results show that the hybrid deep learning system is clearly superior to any other alternative approach.},
  archive      = {J_IDA},
  author       = {Mohanraj, G. and Mohanraj, V. and Senthilkumar, J. and Suresh, Y.},
  doi          = {10.3233/IDA-194820},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1385-1402},
  shortjournal = {Intell. Data Anal.},
  title        = {A hybrid deep learning model for predicting and targeting the less immunized area to improve childrens vaccination rate},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter evolution of the classifiers for disease diagnosis
with offline data-driven hybrid systems. <em>IDA</em>, <em>24</em>(6),
1365–1384. (<a href="https://doi.org/10.3233/IDA-194687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic disease diagnosis is, in essence, a classification problem where the classifier has to be trained based on patients’ datasets and not entirely on doctors’ expert knowledge. In this paper, we present the design of such data-driven disease classifiers and fine-tuning classifier performance by a multi-objective evolutionary algorithm. We have used sequential minimal optimization (SMO) classifier as the base classifier and three evolutionary algorithms namely Cat Swarm Optimization (CSO), Invasive Weed Optimization (IWO) and Eagle Search based Invasive Weed Optimization (ESIWO) to diagnose disease from datasets available. In that sense, our approach is an offline data-driven approach with 18 benchmark medical datasets, and the obtained results demonstrate the superiority of the proposed diagnoses in terms of multiple objectives such as classification Prediction accuracy, Sensitivity, and Specificity. Relevant statistical tests have been carried out to substantiate the cogence of the obtained results.},
  archive      = {J_IDA},
  author       = {Nalluri, Madhu Sudana Rao and K, Kannan and Gao, Xiao-Zhi and V, Swaminathan and Roy, Diptendu Sinha},
  doi          = {10.3233/IDA-194687},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1365-1384},
  shortjournal = {Intell. Data Anal.},
  title        = {Parameter evolution of the classifiers for disease diagnosis with offline data-driven hybrid systems},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning based metric filtering for
evolutionary distance metric learning. <em>IDA</em>, <em>24</em>(6),
1345–1364. (<a href="https://doi.org/10.3233/IDA-194887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collection plays an important role in business agility; data can prove valuable and provide insights for important features. However, conventional data collection methods can be costly and time-consuming. This paper proposes a hybrid system R-EDML that combines a sequential feature selection p erformed by Reinforcement Learning (RL) with the evolutionary feature prioritization of Evolutionary Distance Metric Learning (EDML) in a clustering process. The goal is to reduce the features while maintaining or increasing the accuracy leading to less time complexity and future data collection time and cost reduction. In this method, features represented by the diagonal elements of EDML matrices are prioritized using a differential evolution algorithm. Further, a selection control strategy using RL is learned by sequentially inserting and evaluating the prioritized elements. The outcome offers the best accuracy R-EDML matrix with the least number of elements. Diagonal R-EDML focusing on the diagonal elements is compared with EDML and conventional feature selection. Full Matrix R-EDML focusing on the diagonal and non-diagonal elements is tested and compared with Information-Theoretic Metric Learning. Moreover, R-EDML policy is tested for each EDML generation and across all generations. Results show a significant decrease in the number of features while maintaining or increasing accuracy.},
  archive      = {J_IDA},
  author       = {Ali, Bassel and Moriyama, Koichi and Kalintha, Wasin and Numao, Masayuki and Fukui, Ken-Ichi},
  doi          = {10.3233/IDA-194887},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1345-1364},
  shortjournal = {Intell. Data Anal.},
  title        = {Reinforcement learning based metric filtering for evolutionary distance metric learning},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid recommendation model based on deep learning and
stacking integration strategy. <em>IDA</em>, <em>24</em>(6), 1329–1344.
(<a href="https://doi.org/10.3233/IDA-194961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the traditional recommendation algorithms, due to the rapid development of deep learning and Internet technology, user-item rating data is becoming increasingly sparse. The simple inner product interaction mode adopted by the collaborative filtering method has a cold start problem and cannot lea rn the complex nonlinear structural features between users and items, while the content-based algorithm encounters the difficulty of effective feature extraction. In response to this problem, a hybrid model is proposed based on deep learning and Stacking integration strategy. The traditional recommendation algorithm is first fused by using the Stacking integration strategy to make up for the shortcomings of the single recommendation algorithm to achieve better recommendation performance. The fusion-based model learns the more abstract and deeper nonlinear interaction features by deep learning technology, which makes the model performance gain further. The experiment comparison on the MovieLens-1m dataset shows that the proposed hybrid recommendation model can significantly improve the accuracy of rating prediction.},
  archive      = {J_IDA},
  author       = {Xie, Xiaolan and Pang, Shantian and Chen, Jili},
  doi          = {10.3233/IDA-194961},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1329-1344},
  shortjournal = {Intell. Data Anal.},
  title        = {Hybrid recommendation model based on deep learning and stacking integration strategy},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Plant leaf recognition with shallow and deep learning: A
comprehensive study. <em>IDA</em>, <em>24</em>(6), 1311–1328. (<a
href="https://doi.org/10.3233/IDA-194821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays there are hundreds of thousands known plant species on the Earth and many are still unknown yet. The process of plant classification can be performed using different ways but the most popular approach is based on plant leaf characteristics. Most types of plants have unique leaf characteris tics such as shape, color, and texture. Since machine learning and vision considerably developed in the past decade, automatic plant species (or leaf) recognition has become possible. Recently, the automated leaf classification is a standalone research area inside machine learning and several shallow and deep methods were proposed to recognize leaf types. From 2007 to present days several research papers have been published in this topic. In older studies the classifier was a shallow method while in current works many researchers applied deep networks for classification. During the overview of plant leaf classification literature, we found an interesting deficiency (lack of hyper-parameter search) and a key difference between studies (different test sets). This work gives an overall review about the efficiency of shallow and deep methods under different test conditions. It can be a basis to further research.},
  archive      = {J_IDA},
  author       = {Suto, Jozsef},
  doi          = {10.3233/IDA-194821},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1311-1328},
  shortjournal = {Intell. Data Anal.},
  title        = {Plant leaf recognition with shallow and deep learning: A comprehensive study},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explainable and unexpectable recommendations using
relational learning on multiple domains. <em>IDA</em>, <em>24</em>(6),
1289–1309. (<a href="https://doi.org/10.3233/IDA-194729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we combine relational learning with multi-domain to develop a formal framework for a recommendation system. The design of our framework aims at: (i) constructing general rules for recommendations, (ii) providing suggested items with clear and understandable explanations, (iii) del ivering a broad range of recommendations including novel and unexpected items. We use relational learning to find all possible relations, including novel relations, and to form the general rules for recommendations. Each rule is represented in relational logic, a formal language, associating with probability. The rules are used to suggest the items, in any domain, to the user whose preferences or other properties satisfy the conditions of the rule. The information described by the rule serves as an explanation for the suggested item. It states clearly why the items are chosen for the users. The explanation is in if-then logical format which is unambiguous, less redundant and more concise compared to a natural language used in other explanation recommendation systems. The explanation itself can help persuade the user to try out the suggested items, and the associated probability can drive the user to make a decision easier and faster with more confidence. Incorporating information or knowledge from multiple domains allows us to broaden our search space and provides us with more opportunities to discover items which are previously unseen or surprised to a user resulting in a wide range of recommendations. The experiment results show that our proposed algorithm is very promising. Although the quality of recommendations provided by our framework is moderate, our framework does produce interesting recommendations not found in the primitive single-domain based system and with simple and understandable explanations.},
  archive      = {J_IDA},
  author       = {Sopchoke, Sirawit and Fukui, Ken-ichi and Numao, Masayuki},
  doi          = {10.3233/IDA-194729},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1289-1309},
  shortjournal = {Intell. Data Anal.},
  title        = {Explainable and unexpectable recommendations using relational learning on multiple domains},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiclass spectral feature scaling method for
dimensionality reduction. <em>IDA</em>, <em>24</em>(6), 1273–1287. (<a
href="https://doi.org/10.3233/IDA-194942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Irregular features disrupt the desired classification. In this paper, we consider aggressively modifying scales of features in the original space according to the label information to form well-separated clusters in low-dimensional space. The proposed method exploits spectral clustering to derive s caling factors that are used to modify the features. Specifically, we reformulate the Laplacian eigenproblem of the spectral clustering as an eigenproblem of a linear matrix pencil whose eigenvector has the scaling factors. Numerical experiments show that the proposed method outperforms well-established supervised dimensionality reduction methods for toy problems with more samples than features and real-world problems with more features than samples.},
  archive      = {J_IDA},
  author       = {Matsuda, Momo and Morikuni, Keiichi and Imakura, Akira and Ye, Xiucai and Sakurai, Tetsuya},
  doi          = {10.3233/IDA-194942},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1273-1287},
  shortjournal = {Intell. Data Anal.},
  title        = {Multiclass spectral feature scaling method for dimensionality reduction},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sentiment analysis via dually-born-again network and sample
selection. <em>IDA</em>, <em>24</em>(6), 1257–1271. (<a
href="https://doi.org/10.3233/IDA-194909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text sentiment analysis is an important natural language processing (NLP) task and has received considerable attention in recent years. Numerous deep-learning based methods have been proposed in previous literature in terms of new deep neural networks (DNN) including new embedding strategies, new a ttention mechanisms, and new encoding layers. In this study, an alternative technical path is investigated to further improve the state-of-the-art performance of text sentiment analysis. An new effective learning framework is proposed that combines knowledge distillation and sample selection. A dually-born-again network (DBAN) is presented in which the teacher network and the student network are simultaneously trained through an iterative approach. A selection gate is defined to deal with training samples which are useless or even harmful for model training. Moreover, both the DBAN and sample selection are further improved by ensemble. The proposed framework can improve the existing state-of-the-art DNN models in sentiment analysis. Experimental results indicate that the proposed framework enhances the performances of existing networks. In addition, DBAN outperforms existing born-again network.},
  archive      = {J_IDA},
  author       = {Zhao, Pinlong and Han, Zefeng and Yin, Qing and Li, Shuxiao and Wu, Ou},
  doi          = {10.3233/IDA-194909},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1257-1271},
  shortjournal = {Intell. Data Anal.},
  title        = {Sentiment analysis via dually-born-again network and sample selection},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Entropy difference and kernel-based oversampling technique
for imbalanced data learning. <em>IDA</em>, <em>24</em>(6), 1239–1255.
(<a href="https://doi.org/10.3233/IDA-194761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is often a problem in various real-world datasets, where one class contains a small number of data and the other contains a large number of data. It is notably difficult to develop an effective model using traditional data mining and machine learning algorithms without using data pr eprocessing techniques to balance the dataset. Oversampling is often used as a pretreatment method for imbalanced datasets. Specifically, synthetic oversampling techniques focus on balancing the number of training instances between the majority class and the minority class by generating extra artificial minority class instances. However, the current oversampling techniques simply consider the imbalance of quantity and pay no attention to whether the distribution is balanced or not. Therefore, this paper proposes an entropy difference and kernel-based SMOTE (EDKS) which considers the imbalance degree of dataset from distribution by entropy difference and overcomes the limitation of SMOTE for nonlinear problems by oversampling in the feature space of support vector machine classifier. First, the EDKS method maps the input data into a feature space to increase the separability of the data. Then EDKS calculates the entropy difference in kernel space, determines the majority class and minority class, and finds the sparse regions in the minority class. Moreover, the proposed method balances the data distribution by synthesizing new instances and evaluating its retention capability. Our algorithm can effectively distinguish those datasets with the same imbalance ratio but different distribution. The experimental study evaluates and compares the performance of our method against state-of-the-art algorithms, and then demonstrates that the proposed approach is competitive with the state-of-art algorithms on multiple benchmark imbalanced datasets.},
  archive      = {J_IDA},
  author       = {Wu, Xu and Yang, Youlong and Ren, Lingyu},
  doi          = {10.3233/IDA-194761},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1239-1255},
  shortjournal = {Intell. Data Anal.},
  title        = {Entropy difference and kernel-based oversampling technique for imbalanced data learning},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Editorial. <em>IDA</em>, <em>24</em>(6), 1235–1237. (<a
href="https://doi.org/10.3233/IDA-200016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-200016},
  journal      = {Intelligent Data Analysis},
  month        = {12},
  number       = {6},
  pages        = {1235-1237},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). W-Com2Vec: A topic-driven meta-path- based intra-community
embedding for content-based heterogeneous information network.
<em>IDA</em>, <em>24</em>(5), 1207–1233. (<a
href="https://doi.org/10.3233/IDA-194843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous information network (HIN) are becoming popular across multiple applications in forms of complex large-scaled networked data such as social networks, bibliographic networks, biological networks, etc. Recently, information network embedding (INE) has aroused tremendously interests from researchers due to its effectiveness in information network analysis and mining tasks. From recent views of INE, community is considered as the mesoscopic preserving network’s structure which can be combined with traditional approach of network’s node proximities (microscopic structure preserving) to leverage the quality of network’s representation. Most of contemporary INE models, like as: HIN2Vec, Metapath2Vec, HINE, etc. mainly concentrate on microscopic network structure preserving and ignore the mesoscopic (intra-community) structure of HIN. In this paper, we introduce a novel approach of topic-driven meta-path-based embedding, namely W-Com2Vec (Weighted intra-community to vector). Our proposed W-Com2Vec model enables to capture richer semantic of node representation by applying the meta-path-based community-aware, node proximity preserving and topic similarity evaluation at the same time during the process of network embedding. We demonstrate comprehensive empirical studies on our proposed W-Com2Vec model with several real-world HINs. Experimental results show W-Com2Vec outperforms recent state-of-the-art INE models in solving primitive network analysis and mining tasks.},
  archive      = {J_IDA},
  author       = {Pham, Phu and Do, Phuc},
  doi          = {10.3233/IDA-194843},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1207-1233},
  shortjournal = {Intell. Data Anal.},
  title        = {W-Com2Vec: A topic-driven meta-path- based intra-community embedding for content-based heterogeneous information network},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous self-evolving forecasting models for price
movement in high frequency trading: Evidence from taiwan. <em>IDA</em>,
<em>24</em>(5), 1175–1206. (<a
href="https://doi.org/10.3233/IDA-194592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among FinTech research and applications, forecasting financial time series data has been a challenging task because this kind of data is typically quite noisy and non-stationary. A recent line of financial research centers around trading through financial data on the microscopic level, which is the holy grail of high-frequency trading (HFT), as the higher the data frequency, the more profitable opportunities may appear. The advancement in HFT modeling has also facilitated more understanding towards price formation because the supply and demand of a stock can be comprehended more easily from the microstructure of the order book. Instead of traditional statistical methods, there has been increasing demand for the development of more reliable prediction models due to the recent progress in Computational Intelligence (CI) technologies. In this study, we aim to develop novel CI-based methodologies for the forecasting task of price movement in HFT. Our goal is to conduct a study for autonomous genetic-based models that allow the forecasting systems to self-evolve. The results show that our proposed method can improve upon the previous ones and advance the current state of Fintech research.},
  archive      = {J_IDA},
  author       = {Huang, Chien-Feng and Wu, Hsiao-Chi and Chen, Po-Chun and Chang, Bao Rong},
  doi          = {10.3233/IDA-194592},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1175-1206},
  shortjournal = {Intell. Data Anal.},
  title        = {Autonomous self-evolving forecasting models for price movement in high frequency trading: Evidence from taiwan},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the profitability and errors of predicted prices from
deep learning via program trading. <em>IDA</em>, <em>24</em>(5),
1161–1173. (<a href="https://doi.org/10.3233/IDA-194739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researches on using deep learning models to predict prices usually take magnitude-based error measurements (such as R2) to measure the quality of learning models. Whether the forecasted prices for the models with the lowest error measurement can produce the most profit in actual trading is an issue with little research. In this study, we first find the parameter sets of LSTM and TCN models with low magnitude-based error and then use program trading to find out their profitability. The relationships between these profitability and error measurements are analyzed and studied on three commodities: gold, soybean, and crude oil (from GLOBEX). Our findings are: with given parameter sets, if merchandise (gold and soybean) is of low averaged magnitude error, then its profitability is more stable. If it is of a more significant magnitude error (crude oil), then its profitability is unstable. A high positive correlation does not exist between the profitability and error measurement, and TCN outperforms LSTM in almost all our examples. Our research indicates that, in assessing the performance of deep learning, how to use the predicted values in applications and the application results could also be part of the quality measurement for the model assessment in the learning.},
  archive      = {J_IDA},
  author       = {Tai, Lichen and Hsu, Chihcheng},
  doi          = {10.3233/IDA-194739},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1161-1173},
  shortjournal = {Intell. Data Anal.},
  title        = {On the profitability and errors of predicted prices from deep learning via program trading},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Twitter sentiment analysis for the estimation of voting
intention in the 2017 chilean elections. <em>IDA</em>, <em>24</em>(5),
1141–1160. (<a href="https://doi.org/10.3233/IDA-194768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we apply sentiment analysis methods in the context of the first round of the 2017 Chilean elections. The purpose of this work is to estimate the voting intention associated with each candidate in order to contrast this with the results from classical methods (e.g., polls and surveys) . The data are collected from Twitter, because of its high usage in Chile and in the sentiment analysis literature. We obtained tweets associated with the three main candidates: Sebastián Piñera (SP), Alejandro Guillier (AG) and Beatriz Sánchez (BS). For each candidate, we estimated the voting intention and compared it to the traditional methods. To do this, we first acquired the data and labeled the tweets as positive or negative. Afterward, we built a model using machine learning techniques. The classification model had an accuracy of 76.45% using support vector machines, which yielded the best model for our case. Finally, we use a formula to estimate the voting intention from the number of positive and negative tweets for each candidate. For the last period, we obtained a voting intention of 35.84% for SP, compared to a range of 34–44% according to traditional polls and 36% in the actual elections. For AG we obtained an estimate of 37%, compared with a range of 15.40% to 30.00% for traditional polls and 20.27% in the elections. For BS we obtained an estimate of 27.77%, compared with the range of 8.50% to 11.00% given by traditional polls and an actual result of 22.70% in the elections. These results are promising, in some cases providing an estimate closer to reality than traditional polls. Some differences can be explained due to the fact that some candidates have been omitted, even though they held a significant number of votes.},
  archive      = {J_IDA},
  author       = {Alegre Sepúlveda, Tomás and Keith Norambuena, Brian},
  doi          = {10.3233/IDA-194768},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1141-1160},
  shortjournal = {Intell. Data Anal.},
  title        = {Twitter sentiment analysis for the estimation of voting intention in the 2017 chilean elections},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial-time motifs discovery. <em>IDA</em>, <em>24</em>(5),
1121–1140. (<a href="https://doi.org/10.3233/IDA-194759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering motifs in time series data has been widely explored. Various techniques have been developed to tackle this problem. However, when it comes to spatial-time series, a clear gap can be observed according to the literature review. This paper tackles such a gap by presenting an approach to d iscover and rank motifs in spatial-time series, denominated Combined Series Approach (CSA). CSA is based on partitioning the spatial-time series into blocks. Inside each block, subsequences of spatial-time series are combined in a way that hash-based motif discovery algorithm is applied. Motifs are validated according to both temporal and spatial constraints. Later, motifs are ranked according to their entropy, the number of occurrences, and the proximity of their occurrences. The approach was evaluated using both synthetic and seismic datasets. CSA outperforms traditional methods designed only for time series. CSA was also able to prioritize motifs that were meaningful both in the context of synthetic data and also according to seismic specialists.},
  archive      = {J_IDA},
  author       = {Borges, Heraldo and Dutra, Murillo and Bazaz, Amin and Coutinho, Rafaelli and Perosi, Fábio and Porto, Fábio and Masseglia, Florent and Pacitti, Esther and Ogasawara, Eduardo},
  doi          = {10.3233/IDA-194759},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1121-1140},
  shortjournal = {Intell. Data Anal.},
  title        = {Spatial-time motifs discovery},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fourier neural networks: A comparative study. <em>IDA</em>,
<em>24</em>(5), 1107–1120. (<a
href="https://doi.org/10.3233/IDA-195050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We review neural network architectures which were motivated by Fourier series and integrals and which are referred to as Fourier neural networks. These networks are empirically evaluated in synthetic and real-world tasks. Neither of them outperforms the standard neural network with sigmoid activati on function in the real-world tasks. All neural networks, both Fourier and the standard one, empirically demonstrate lower approximation error than the truncated Fourier series when it comes to approximation of a known function of multiple variables.},
  archive      = {J_IDA},
  author       = {Uteuliyeva, Malika and Zhumekenov, Abylay and Takhanov, Rustem and Assylbekov, Zhenisbek and Castro, Alejandro J. and Kabdolov, Olzhas},
  doi          = {10.3233/IDA-195050},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1107-1120},
  shortjournal = {Intell. Data Anal.},
  title        = {Fourier neural networks: A comparative study},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient bayesian network structure learning algorithm
using the strategy of two-stage searches. <em>IDA</em>, <em>24</em>(5),
1087–1106. (<a href="https://doi.org/10.3233/IDA-194844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important for Bayesian network (BN) structure learning, a NP-problem, to improve the accuracy and hybrid algorithms are a kind of effective structure learning algorithms at present. Most hybrid algorithms adopt the strategy of one heuristic search and can be divided into two groups: one heuri stic search based on initial BN skeleton and one heuristic search based on initial solutions. The former often fails to guarantee globality of the optimal structure and the latter fails to get the optimal solution because of large search space. In this paper, an efficient hybrid algorithm is proposed with the strategy of two-stage searches. For first-stage search, it firstly determines the local search space based on Maximal Information Coefficient by introducing penalty factors p1, p2, then searches the local space by Binary Particle Swarm Optimization. For second-stage search, an efficient ADR (the abbreviation of Add, Delete, Reverse) algorithm based on three basic operators is designed to extend the local space to the whole space. Experiment results show that the proposed algorithm can obtain better performance of BN structure learning.},
  archive      = {J_IDA},
  author       = {Guo, Huiping and Li, Hongru},
  doi          = {10.3233/IDA-194844},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1087-1106},
  shortjournal = {Intell. Data Anal.},
  title        = {An efficient bayesian network structure learning algorithm using the strategy of two-stage searches},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recognition of speech emotion using custom 2D-convolution
neural network deep learning algorithm. <em>IDA</em>, <em>24</em>(5),
1065–1086. (<a href="https://doi.org/10.3233/IDA-194747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition has become the heart of most human computer interaction applications in the modern world. The growing need to develop emotionally intelligent devices has opened up a lot of research opportunities. Most researchers in this field have applied the use of handcrafted features and machine learning techniques in recognising speech emotion. However, these techniques require extra processing steps and handcrafted features are usually not robust. They are computationally intensive because the curse of dimensionality results in low discriminating power. Research has shown that deep learning algorithms are effective for extracting robust and salient features in dataset. In this study, we have developed a custom 2D-convolution neural network that performs both feature extraction and classification of vocal utterances. The neural network has been evaluated against deep multilayer perceptron neural network and deep radial basis function neural network using the Berlin database of emotional speech, Ryerson audio-visual emotional speech database and Surrey audio-visual expressed emotion corpus. The described deep learning algorithm achieves the highest precision, recall and F1-scores when compared to other existing algorithms. It is observed that there may be need to develop customized solutions for different language settings depending on the area of applications.},
  archive      = {J_IDA},
  author       = {Zvarevashe, Kudakwashe and Olugbara, Oludayo O.},
  doi          = {10.3233/IDA-194747},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1065-1086},
  shortjournal = {Intell. Data Anal.},
  title        = {Recognition of speech emotion using custom 2D-convolution neural network deep learning algorithm},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Could spatial features help the matching of textual data?
<em>IDA</em>, <em>24</em>(5), 1043–1064. (<a
href="https://doi.org/10.3233/IDA-194749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textual data is available to an increasing extent through different media (social networks, companies data, data catalogues, etc.). New information extraction methods are needed since these new resources are highly heterogeneous. In this article, we propose a text matching process based on spatial features and assessed through heterogeneous textual data. Besides being compatible with heterogeneous data, it comprises two contributions: first, spatial information is extracted for comparison purposes and subsequently stored in a dedicated spatial textual representation (STR); and then two transformations are applied on STR to improve the spatial similarity estimation. This article outlines the proposed approach with new contributions: (i) a new geocoding methods using general co-occurrences between entities, and (ii) a thorough evaluation followed by (iii) an in-depth discussion. The results obtained on two corpora demonstrate that good spatial matches (≈ 80% precision on major criteria) can be obtained between the most similar STRs with further enhancement achieved via STR transformation.},
  archive      = {J_IDA},
  author       = {Fize, Jacques and Roche, Mathieu and Teisseire, Maguelonne},
  doi          = {10.3233/IDA-194749},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1043-1064},
  shortjournal = {Intell. Data Anal.},
  title        = {Could spatial features help the matching of textual data?},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel algorithm for searching frequent gradual patterns
from an ordered data set. <em>IDA</em>, <em>24</em>(5), 1029–1042. (<a
href="https://doi.org/10.3233/IDA-194644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining frequent simultaneous attribute co-variations in numerical databases is also called frequent gradual pattern problem. Few efficient algorithms for automatically extracting such patterns have been reported in the literature. Their main difference resides in the variation semantics used. Howev er in applications with temporal order relations, those algorithms fail to generate correct frequent gradual patterns as they do not take this temporal constraint into account in the mining process. In this paper, we propose an approach for extracting frequent gradual patterns for which the ordering of supporting objects matches the temporal order. This approach considerably reduces the number of gradual patterns within an ordered data set. The experimental results show the benefits of our approach.},
  archive      = {J_IDA},
  author       = {Lonlac, Jerry and Nguifo, Engelbert Mephu},
  doi          = {10.3233/IDA-194644},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1029-1042},
  shortjournal = {Intell. Data Anal.},
  title        = {A novel algorithm for searching frequent gradual patterns from an ordered data set},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boosting meta-learning with simulated data complexity
measures. <em>IDA</em>, <em>24</em>(5), 1011–1028. (<a
href="https://doi.org/10.3233/IDA-194803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-Learning has been largely used over the last years to support the recommendation of the most suitable machine learning algorithm(s) and hyperparameters for new datasets. Traditionally, a meta-base is created containing meta-features extracted from several datasets along with the performance of a pool of machine learning algorithms when applied to these datasets. The meta-features must describe essential aspects of the dataset and distinguish different problems and solutions. However, if one wants the use of Meta-Learning to be computationally efficient, the extraction of the meta-feature values should also show a low computational cost, considering a trade-off between the time spent to run all the algorithms and the time required to extract the meta-features. One class of measures with successful results in the characterization of classification datasets is concerned with estimating the underlying complexity of the classification problem. These data complexity measures take into account the overlap between classes imposed by the feature values, the separability of the classes and distribution of the instances within the classes. However, the extraction of these measures from datasets usually presents a high computational cost. In this paper, we propose an empirical approach designed to decrease the computational cost of computing the data complexity measures, while still keeping their descriptive ability. The proposal consists of a novel Meta-Learning system able to predict the values of the data complexity measures for a dataset by using simpler meta-features as input. In an extensive set of experiments, we show that the predictive performance achieved by Meta-Learning systems which use the predicted data complexity measures is similar to the performance obtained using the original data complexity measures, but the computational cost involved in their computation is significantly reduced.},
  archive      = {J_IDA},
  author       = {Garcia, Luís P.F. and Rivolli, Adriano and Alcoba, Edesio and Lorena, Ana C. and de Carvalho, André C.P.L.F.},
  doi          = {10.3233/IDA-194803},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1011-1028},
  shortjournal = {Intell. Data Anal.},
  title        = {Boosting meta-learning with simulated data complexity measures},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Canal-LASSO: A sparse noise-resilient online linear
regression model. <em>IDA</em>, <em>24</em>(5), 993–1010. (<a
href="https://doi.org/10.3233/IDA-194672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Least absolute shrinkage and selection operator (LASSO) is one of the most commonly used methods for shrinkage estimation and variable selection. Robust variable selection methods via penalized regression, such as least absolute deviation LASSO (LAD-LASSO), etc., have gained growing attention in wo rks of literature. However those penalized regression procedures are still sensitive to noisy data. Furthermore, “concept drift” makes learning from streaming data fundamentally different from the traditional batch learning. Focusing on the shrinkage estimation and variable selection tasks on noisy streaming data, this paper presents a noise-resilient online learning regression model, i.e. canal-LASSO. Comparing with the LASSO and LAD-LASSO, canal-LASSO is resistant to noisy data in both explanatory variables and response variables. Extensive simulation studies demonstrate satisfactory sparseness and noise-resilient performances of canal-LASSO.},
  archive      = {J_IDA},
  author       = {Lei, Hejie and Chen, Xingke and Jian, Ling},
  doi          = {10.3233/IDA-194672},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {993-1010},
  shortjournal = {Intell. Data Anal.},
  title        = {Canal-LASSO: A sparse noise-resilient online linear regression model},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian hierarchical k-means clustering. <em>IDA</em>,
<em>24</em>(5), 977–992. (<a
href="https://doi.org/10.3233/IDA-194807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithm is the foundation and important technology in data mining. In fact, in the real world, the data itself often has a hierarchical structure. Hierarchical clustering aims at constructing a cluster tree, which reveals the underlying modal structure of a complex density. Due to its inherent complexity, most existing hierarchical clustering algorithms are usually designed heuristically without an explicit objective function, which limits its utilization and analysis. K-means clustering, the well-known simple yet effective algorithm which can be expressed from the view of probability distribution, has inherent connection to Mixture of Gaussians (MoG). At this point, we consider combining Bayesian theory analysis with K-means algorithm. This motivates us to develop a hierarchical clustering based on K-means under the probability distribution framework, which is different from existing hierarchical K-means algorithms processing data in a single-pass manner along with heuristic strategies. For this goal, we propose an explicit objective function for hierarchical clustering, termed as Bayesian hierarchical K-means (BHK-means). In our method, a cascaded clustering tree is constructed, in which all layers interact with each other in the network-like manner. In this cluster tree, the clustering results of each layer are influenced by the parent and child nodes. Therefore, the clustering result of each layer is dynamically improved in accordance with the global hierarchical clustering objective function. The objective function is solved using the same algorithm as K-means, the Expectation-maximization algorithm. The experimental results on both synthetic data and benchmark datasets demonstrate the effectiveness of our algorithm over the existing related ones.},
  archive      = {J_IDA},
  author       = {Liu, Yue and Li, Bufang},
  doi          = {10.3233/IDA-194807},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {977-992},
  shortjournal = {Intell. Data Anal.},
  title        = {Bayesian hierarchical K-means clustering},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Editorial. <em>IDA</em>, <em>24</em>(5), 973–975. (<a
href="https://doi.org/10.3233/IDA-200004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-200004},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {973-975},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A community-based algorithm for influence maximization on
dynamic social networks. <em>IDA</em>, <em>24</em>(4), 959–971. (<a
href="https://doi.org/10.3233/IDA-194675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of the influence maximization is to find the top k influential seeds which can maximize the influence spread. Recently, some researchers address this problem through community structure. However, most of these community-based studies only consider the static social network, which ignore s that social networks change frequently. In order to deal with the above problem, we present a community-based algorithm on the dynamic social network, which is divided into three phases: (i) community detection, (ii) candidate seed set on the dynamic network, and (iii) final seed set. In the first phase, we use the Louvain algorithm to obtain the community structure. In each community, we analyze the node location to judge the importance of the node. In the second phase, considering the dynamic social network, when a node is added to the network or removed from the network, we update the structure of the social network. Then, the candidate nodes are those nodes with a large influence in each community. And in the third phase, we select k influential seeds from the candidate seeds by CELF algorithm. Extensive experimental results show that our algorithm obtains a better influence spread than many baseline algorithms as well as an acceptable running time while considering the dynamic social networks.},
  archive      = {J_IDA},
  author       = {Wei, Jia and Cui, Zhenyu and Qiu, Liqing and Niu, Weinan},
  doi          = {10.3233/IDA-194675},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {959-971},
  shortjournal = {Intell. Data Anal.},
  title        = {A community-based algorithm for influence maximization on dynamic social networks},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-fuzzy-constrained graph pattern matching with big
graph data. <em>IDA</em>, <em>24</em>(4), 941–958. (<a
href="https://doi.org/10.3233/IDA-194653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph pattern matching has been widespread used for protein structure analysis, expert finding and social group selection, ect. Recently, the study of graph pattern matching using the abundant attribute information of vertices and edges as constraint conditions has attracted the attention of schola rs, and multi-constrained simulation has been proposed to address the problem in contextual social networks. Actually, multi-constrained graph pattern matching is an NP-complete problem and the fuzziness of constraint variables may exist in many applications. In this paper, we introduce a multi-fuzzy-constrained graph pattern matching problem in big graph data, and propose an efficient first-k algorithm Fuzzy-ETOF-K for solving it. Specifically, exploration-based method based on edge topology is adopted to improve the efficiency of edge connection, and breadth-first bounded search is used for edge matching instead of shortest path query between two nodes to improve the efficiency of edge matching. The results of our experiments conducted on three datasets of real social networks illustrate that our proposed algorithm Fuzzy-ETOF-K significantly outperforms existing approaches in efficiency and the introduction of fuzzy constraints makes our proposed algorithm more efficient and effective.},
  archive      = {J_IDA},
  author       = {Liu, Guliu and Li, Lei and Wu, Xindong},
  doi          = {10.3233/IDA-194653},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {941-958},
  shortjournal = {Intell. Data Anal.},
  title        = {Multi-fuzzy-constrained graph pattern matching with big graph data},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time detection and trend tracing of burst topics based
on negative binomial distribution on spark. <em>IDA</em>,
<em>24</em>(4), 925–940. (<a
href="https://doi.org/10.3233/IDA-194663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks have evolved into a popular information and communication platform, and the vast amount of data it generates are rapidly changing and spreading. Thus, it is essential to detect and trace large events and burst topics in mass social network data based on real-time Big Data parallel c omputing. In this paper, we propose a model that uses the Negative Binomial Distribution to fit the distribution of Weibo topic words. Then, we introduce the concepts of the ‘hot degree’ and the ‘dispersion degree’ of a topic with their corresponding computing methods. And we validate the efficiency of the model using real data. Furthermore, we design a topic detection and trend-tracing algorithm based on stream data, and implement the algorithm on Spark Streaming which is a streaming processing framework that uses memory computing. Finally, the experiments on real data demonstrate that our proposal is effective and efficient in tracking bursting events.},
  archive      = {J_IDA},
  author       = {Dang, Depeng and Yu, Wenhui and Chen, Chuangxia and Yan, Rongen and Zhang, Xiaoran and Zhu, Xiaoming},
  doi          = {10.3233/IDA-194663},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {925-940},
  shortjournal = {Intell. Data Anal.},
  title        = {Real-time detection and trend tracing of burst topics based on negative binomial distribution on spark},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel adaptive k-NN classifier for handling imbalance:
Application to brain MRI. <em>IDA</em>, <em>24</em>(4), 909–924. (<a
href="https://doi.org/10.3233/IDA-194647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of efficiently classifying imbalanced data has become one of the most challenging tasks in machine learning. Some real world examples include medical image analysis, fraud detection, fault diagnosis, and anomaly detection. Although several data-level algorithms have been developed to ad dress imbalance, they are typically subject to some restrictions. We propose a novel variant of the k-NN family of classifiers, and name this as Density-based Adaptive-distance kNN (DAkNN). It can effectively handle data with skewed distributions and varying class-densities using the concept of adaptive distance. Comparative superiority is experimentally established over related data-level algorithms (SMOTE, ADASYN), using ten sets of two-class data, in terms of geometric mean (of the true positive and negative rates) and accuracy. Additionally, five sets of multi-class data are considered and compared with different variants of k-NN, which are currently very popular. Finally, DAkNN is successfully applied on the highly imbalanced Lower Grade Glioma (LGG) MR images, with an Average-Dice score of 0.9082 for delineating the tumor regions. The results demonstrate clear superiority over state-of-the-art algorithms.},
  archive      = {J_IDA},
  author       = {Kirtania, Ritaban and Mitra, Sushmita and Shankar, B. Uma},
  doi          = {10.3233/IDA-194647},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {909-924},
  shortjournal = {Intell. Data Anal.},
  title        = {A novel adaptive k-NN classifier for handling imbalance: Application to brain MRI},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparative analysis of bayesian network structure
learning algorithms applied to crime data. <em>IDA</em>, <em>24</em>(4),
887–907. (<a href="https://doi.org/10.3233/IDA-194534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theories about crime and correction have their inception in the eighteenth century, highly influenced by the anthropological thoughts emerging during the age of Enlightenment. Throughout the decades, the criminological studies observed their sociological essence encompassing practices from othe r scientific fields to explain the more contemporary questions, becoming Criminology an inherently interdisciplinary science as a result. The adoption of concepts from Exact Sciences is a recent moving, originating it a novel research area, called Computational Criminology, which employs procedures from Applied Mathematics, Statistics and Computer Science to provide original or enhanced solutions to such questions. One of the most prominent tasks brought by this rising field is crime prediction, which attempts to uncover potential targets for future police intervention and also help solving already committed offenses. The present comparative analysis thus investigates the employment of statistical inference by means of Bayesian network for predictive policing, using the openly accessible registers from Chicago Police Department. Numerous algorithms are available to learn the structure for a Bayesian network purely from data and a comparative examination about them is hence described, with the purpose to establish the most precise and efficient one, according to the attributes of the said criminal dataset, for the implementation of the intended inference.},
  archive      = {J_IDA},
  author       = {Fazanaro, Dalton Ieda and Pedrini, Helio},
  doi          = {10.3233/IDA-194534},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {887-907},
  shortjournal = {Intell. Data Anal.},
  title        = {A comparative analysis of bayesian network structure learning algorithms applied to crime data},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature learning for representing sparse networks based on
random walks. <em>IDA</em>, <em>24</em>(4), 873–886. (<a
href="https://doi.org/10.3233/IDA-194676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying features to represent graphs such as social networks, protein graphs is increasingly common in both research and business communities, thanks to the fact that data has increased not only in quantity but also in complexity. This results in the graphs to be sparser because not all nodes a re fully connected. In addition, if this whole graph is used as input data for learning algorithms e.g. neural network, a lot of training time will be required. Substantial efforts have been made to convert the graphs to better yet compact representations, among of which is graph embedding. The traditional methods used to map the original graph to its embedding representation had not yielded significant results until deep learning was invented. Many good approaches in this direction, as examples, are DeepWalk, node2vec. However, their general weakness is many important connections in the original graph could be lost. In this paper, we propose another approach to retain more edge information while ensuring the embedding graph is still sufficiently small, compared to the original one. Our experiment results show that the method also increases the accuracy of latter learning models.},
  archive      = {J_IDA},
  author       = {Le, Thanh and Tran, Giang and Le, Bac},
  doi          = {10.3233/IDA-194676},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {873-886},
  shortjournal = {Intell. Data Anal.},
  title        = {Feature learning for representing sparse networks based on random walks},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid node classification mechanism for influential node
prediction in social networks. <em>IDA</em>, <em>24</em>(4), 847–871.
(<a href="https://doi.org/10.3233/IDA-194724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social Networks is an essential phenomenon in all aspects through various perspectives. These networks contain a large number of users better termed as nodes and the connections between the users termed as edges. For efficient information processing and retrieving, accessing the influential node is essential for improving the diffusion process. To identify the influential node inside a heterogeneous community, incorporating probability metrics with regression classifier is put forth stated by proposed method Support Vector Bayesian Machine (SVBM). Node metrics such as degree centrality, closeness centrality is measured for eliminating the nodes primarily. A standardized index based on the centrality values computed for enhancing into SVBM. After the standardized index, similarity dissimilarity index values evaluated by combining the Euclidean, Hamming, Pearson coefficient for valued relations and Jaccard for binary relations which results in a single index value considered as the power degree value(p). The value p determines the node’s boundedness, which indicates the range of influence within the community. The outlier nodes in the bounded region get eliminated, and the nodes remaining taken for the final phase of SVBM, probability regression line predicts the node inhibiting the most influential nature. Experimental evaluation of the proposed system with the existing Support Vector Machine (SVM) technique resulted in 0.95 and 0.41 respectively for Area Under Curve (AUC) denoting that the true positive influential node classification process from the other existing nodes was higher than SVM. In comparison with the existing SVM, the proposed methodology SVBM attained a node detection, which influenced a higher diffusion rate within the networks.},
  archive      = {J_IDA},
  author       = {Prakash, M. and Pabitha, P.},
  doi          = {10.3233/IDA-194724},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {847-871},
  shortjournal = {Intell. Data Anal.},
  title        = {A hybrid node classification mechanism for influential node prediction in social networks},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient algorithm for hiding sensitive-high utility
itemsets. <em>IDA</em>, <em>24</em>(4), 831–845. (<a
href="https://doi.org/10.3233/IDA-194697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving utility itemset mining is the process of hiding sensitive-high utility itemsets (SHUIs) appearing in original database such that they will not be discovered in the sanitized database. The purpose of SHUI hiding algorithm is to conceal the set of SHUIs while minimizing the side ef fects which caused by data distortion process. In this paper, a novel algorithm, named EHSHUI (An Efficient Algorithm for Hiding Sensitive-high utility Itemsets), is proposed to minimize the side effects of the sanitization process. The proposed algorithm includes three heuristic steps: (1) The transaction on which the SHUI achieves maximal utility among transactions containing it is specified as victim transaction; (2) The item that causes minimal impacts on non-SHUIs is selected as victim item; and (3) An exactly number of utility is calculated for reducing internal utility of victim item from victim transaction. This strategy exactly identifies item and transaction for data modification such that it minimizes the impacts on non-SHUIs, data distortions, and the time to access database. The experiment results illustrate that the proposed algorithm achieves higher performance and lower side effects than the state-of-the-art.},
  archive      = {J_IDA},
  author       = {Huynh Trieu, Vy and Le Quoc, Hai and Truong Ngoc, Chau},
  doi          = {10.3233/IDA-194697},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {831-845},
  shortjournal = {Intell. Data Anal.},
  title        = {An efficient algorithm for hiding sensitive-high utility itemsets},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RobustRepStream: Robust stream clustering using
self-controlled connectivity graph. <em>IDA</em>, <em>24</em>(4),
799–830. (<a href="https://doi.org/10.3233/IDA-194715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge in stream clustering is the evolution in the statistical properties of the underlying data. As clustering is inherently unsupervised, selecting suitable parameter values is often difficult. Clustering algorithms with sensitive parameters are often not robust to such changes, leadi ng to poor clustering outputs. Algorithms using K-NN graphs face this problem, as they have a sensitive K-connectivity parameter which prohibits them from adapting to stream concept evolution. We address this by controlling the excess of the skewness of edge length distributions in the underlying K-NN graph by introducing novel skewness excess concept. We demonstrate the asymptotic linear dependency of skewness excess against the graph connectivity and propose the novel RobustRepStream algorithm, which extends the RepStream algorithm, and provides improved robustness against stream evolution. By automatically controlling the skewness excess, the user no longer needs to specify the K-connectivity parameter, and RobustRepStream can adjust the graph connectivity locally in order to achieve performance close to when the optimal K value is known. We demonstrate that RobustRepStream’s skewness threshold parameter is insensitive and universal across all data sets. We comprehensively evaluate RobustRepStream on real-world benchmark data sets against previous stream clustering algorithms, and demonstrate that it provides better clustering performance.},
  archive      = {J_IDA},
  author       = {Callister, Ross and Lazarescu, Mihai and Pham, Duc-Son},
  doi          = {10.3233/IDA-194715},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {799-830},
  shortjournal = {Intell. Data Anal.},
  title        = {RobustRepStream: Robust stream clustering using self-controlled connectivity graph},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two deterministic selection methods for the initial centers
in fuzzy c-means based algorithms. <em>IDA</em>, <em>24</em>(4),
779–798. (<a href="https://doi.org/10.3233/IDA-194588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy C-Means (FCM) is the most commonly used and discussed fuzzy clustering algorithm in the literature. Nevertheless, it is well known that the performance of FCM is strongly affected by the selection of the initial cluster centers. In other words, the selection of a good set of initial cluster c enters plays an important role in the performance of this algorithm. The most common selection method is the trial-and-test random method, in which each execution is performed with different initial centers, randomly generated, resulting in different dataset partitions. This paper proposes two methods to obtain the initial cluster centers which are applied in FCM and its variants. The proposed methods are deterministic, since, for each data set and number of clusters, they will always provide the same cluster centers set. The main advantage of these methods is to provide high quality partitions faster than the original methods as well as other FCM and ckMeans-based algorithms with deterministic selection of cluster centers.},
  archive      = {J_IDA},
  author       = {da Silva, Liliane R. and Arnaldo, Heloina A. and da Silva, Huliane and Moura, Ronildo and Bedregal, Benjamín and de P. Canuto, Anne Magaly},
  doi          = {10.3233/IDA-194588},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {779-798},
  shortjournal = {Intell. Data Anal.},
  title        = {Two deterministic selection methods for the initial centers in fuzzy c-means based algorithms},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An advanced profile hidden markov model for malware
detection. <em>IDA</em>, <em>24</em>(4), 759–778. (<a
href="https://doi.org/10.3233/IDA-194639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of malicious software (malware) production in recent decades and the increasing number of threats posed by malware to network environments, such as the Internet and intelligent environments, emphasize the need for more research on the security of computer networks in information se curity and digital forensics. The method presented in this study identifies “species” of malware families, which are more sophisticated, obfuscated, and structurally diverse. We propose a hybrid technique combining aspects of signature detection with machine learning based methods to classify malware families. The method is carried out by utilizing Profile Hidden Markov Models (PHMMs) on the behavioral characteristics of malware species. This paper explains the process of modeling and training an advanced PHMM using sequences obtained from the extraction of each malware family’s paramount features, and the canonical sequences created in the process of Multiple Sequence Alignment (MSA) production. Due to the fact that not all parts of a file are malicious, the goal is to distinguish the malicious portions from the benign ones and place more emphasis on them in order to increase the likelihood of malware detection by having the least impact from the benign portions. Based on “consensus sequences”, the experimental results show that our proposed approach outperforms other HMM-based techniques even when limited training data is available. All supplementary materials including the code, datasets, and a complete list of results are available for public access on the Web.1},
  archive      = {J_IDA},
  author       = {Alipour, Alireza Abbas and Ansari, Ebrahim},
  doi          = {10.3233/IDA-194639},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {759-778},
  shortjournal = {Intell. Data Anal.},
  title        = {An advanced profile hidden markov model for malware detection},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An outlier ensemble for unsupervised anomaly detection in
honeypots data. <em>IDA</em>, <em>24</em>(4), 743–758. (<a
href="https://doi.org/10.3233/IDA-194656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, computers, as well as smart devices, are connected through communication networks making them more vulnerable to attacks. Honeypots are proposed as deception tools but usually used as part of a proactive defense strategy. Hence, this article demonstrates how honeypots data can be analyzed in an active defense strategy. Furthermore, anomaly detection based on unsupervised machine learning techniques allows to build autonomous systems and to detect unknown anomalies without the need for prior knowledge. However, the unsupervised techniques applied for honeypots data analysis do not value the advantages of these tools’ data, particularly the high probability that they include a large number of previously unseen anomalies with unexpected and diverse patterns. Therefore, in the present work, the aim is to improve the unsupervised anomaly detection in honeypots data by varying the data feature subset and the parameterization of the anomaly detection algorithm. To this purpose, an outlier ensemble with LOF (Local Outlier Factor) as a base algorithm is proposed. The ensemble outperforms existing solutions as depicted in the experiments where a detection rate higher than 92% is achieved.},
  archive      = {J_IDA},
  author       = {Boukela, Lynda and Zhang, Gongxuan and Bouzefrane, Samia and Zhou, Junlong},
  doi          = {10.3233/IDA-194656},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {743-758},
  shortjournal = {Intell. Data Anal.},
  title        = {An outlier ensemble for unsupervised anomaly detection in honeypots data},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). Editorial. <em>IDA</em>, <em>24</em>(4), 741–742. (<a
href="https://doi.org/10.3233/IDA-200003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-200003},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {741-742},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). User-item content awareness in matrix factorization based
collaborative recommender systems. <em>IDA</em>, <em>24</em>(3),
723–739. (<a href="https://doi.org/10.3233/IDA-194599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems promote sales of products and services by helping users alleviate the information overload problem. Collaborative filtering is most extensively used approach to design recommender system. The main idea of collaborative filtering is that recommendation for each active user is rec eived by comparing with the preferences of other users who have rated the product in similar way to the active user. Matrix factorization technique is one of the most widely employed collaborative filtering techniques due to its effectiveness and efficiency in dealing with very large user-item rating matrices. One of the principal disadvantages and challenges of the collaborative filtering type algorithms is content awareness, namely, they use only people’s behavior to produce recommendations and are not aware of the predicted content’s metadata. In this work, we study and compare two ways of incorporating this type of content information directly into the matrix factorization approach. We extend the baseline optimization problem by two techniques. The first one penalizes item and user feature vectors with some small amounts pushing them towards each other in the latent space, and the second one makes two item and user specific latent feature vectors as similar as possible if the two items and users have similar tagging history. The results of the experiments, on the benchmark data sets, show that the proposed model has a better performance compared to some other methods.},
  archive      = {J_IDA},
  author       = {Mohammadi, Maryam and Naree, Somaye Arabi and Lati, Mahsa},
  doi          = {10.3233/IDA-194599},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {723-739},
  shortjournal = {Intell. Data Anal.},
  title        = {User-item content awareness in matrix factorization based collaborative recommender systems},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A semantic-aware collaborative filtering recommendation
method for emergency plans in response to meteorological hazards.
<em>IDA</em>, <em>24</em>(3), 705–721. (<a
href="https://doi.org/10.3233/IDA-194571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meteorological hazards have great influence all over the world. An emergency plan is an important means of coping with meteorological hazards. The preparation of emergency plans needs to refer to historical emergency plans, but these are too numerous and are of uneven quality. We can alleviate thes e problems by means of recommender systems, which are very useful tools in many domains; however, they suffer from information overload. In this paper, we propose a Semantic-Aware Collaborative Filtering method, which is called SACF, for emergency plans recommendation to address the aforementioned challenges. It is designed to effectively present a highly targeted emergency plan recommendation list and recommend the most appropriate emergency plans for a targeted meteorological hazards event. Specifically, we use semantic knowledge to represent scenario-based meteorological hazards, including target and previous events. The search for similar events (i.e., neighbors) for a collaborative filtering recommendation algorithm is adopted. By helping to avoid both the generation of fake neighbors and also the omission of true neighbors the recommendation process is improved. Finally, extensive experiments are conducted on a real-world dataset, and the results demonstrate that SACF improves the accuracy of emergency plan recommendations.},
  archive      = {J_IDA},
  author       = {Dang, Depeng and Chen, Chuangxia and Yu, Wenhui and Hu, Huaxiao},
  doi          = {10.3233/IDA-194571},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {705-721},
  shortjournal = {Intell. Data Anal.},
  title        = {A semantic-aware collaborative filtering recommendation method for emergency plans in response to meteorological hazards},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian approach to linear state space model with unknown
measurement matrix. <em>IDA</em>, <em>24</em>(3), 689–704. (<a
href="https://doi.org/10.3233/IDA-194624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A linear state space model is of two equations, one of state space and the other of measurement. Estimation methods for the parameters of the model have been developed from the historic Kalman filter method. The Bayes estimation has also been used under a variety of conditions on the parameter spac e. We explored the availability of the Bayes method for parameter estimation with no constraints on the parameter space and found that the estimation for the state space is acceptable as long as the priors are not vague on both the state and the parameter space. We also investigated the model where the measurement matrix is contaminated with noise and found that the estimates for the state space were more accurate than those by the methods in literature. We made remarks on extended applications of the Bayes method for the linear state space model where a variety of constraints are imposed on the parameter space.},
  archive      = {J_IDA},
  author       = {Yoon, Wansang and Kim, Sung-Ho},
  doi          = {10.3233/IDA-194624},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {689-704},
  shortjournal = {Intell. Data Anal.},
  title        = {Bayesian approach to linear state space model with unknown measurement matrix},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analyzing concept drift: A case study in the financial
sector. <em>IDA</em>, <em>24</em>(3), 665–688. (<a
href="https://doi.org/10.3233/IDA-194515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method for exploratory data analysis of streaming data based on probabilistic graphical models (latent variable models). This method is illustrated by concept drift tracking, using financial client data from a European regional bank. For this particular setting, the anal yzed data spans the period from April 2007 to March 2014 and therefore starts before the beginning of the financial crisis of 2008. The implied changes in the economic climate during this period manifests itself as concept drift in the underlying data generating distribution. We explore and analyze this financial client data using a probabilistic graphical modeling framework that provides an explicit representation of concept drift as an integral part of the model. We show how learning these types of models from data provides additional insight into the hidden mechanisms governing the drift in the domain. We present an iterative approach for identifying disparate factors that jointly account for the drift in the domain. This includes a semantic characterization of one of the main influencing drift factors. Based on the experiences and results obtained from analyzing the financial data, we discuss the applicability of the framework within a more general context.},
  archive      = {J_IDA},
  author       = {Masegosa, Andrés R. and Martínez, Ana M. and Ramos-López, Darío and Langseth, Helge and Nielsen, Thomas D. and Salmerón, Antonio},
  doi          = {10.3233/IDA-194515},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {665-688},
  shortjournal = {Intell. Data Anal.},
  title        = {Analyzing concept drift: A case study in the financial sector},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extraction of qualitative behavior rules for industrial
processes from reduced concept lattice. <em>IDA</em>, <em>24</em>(3),
643–663. (<a href="https://doi.org/10.3233/IDA-194569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal concept analysis (FCA) become an alternative approach to extract and represent knowledge of real world systems. That knowledge can be obtained from implications rules extracted of concept lattices formed by ordered formal concepts. However, in complex systems the number of formal concepts ca n be large. To deal with this complexity of the FCA, concept reduction techniques can be applied in order to balance the quality of information, and the computational cost for generating and handling the lattice. In this paper, we develop a novel approach to represent the behavior of physical processes through qualitative rules based on proper implications (minimum representation of the data) extracted from the reduced concept lattice. As a case study, the cold rolling process was considered. This process characterize by the strong non-linearity among its parameters. The results show that the qualitative behavior of the rolling process is preserved even when the reduction techniques are applied. The approach can be used to understand the relationship between process parameters through implication rules under different operating conditions of a process. The paper discusses some generic procedures that can be adopted to apply this approach to other industrial processes.},
  archive      = {J_IDA},
  author       = {Dias, Sérgio M. and Zárate, Luis E. and Song, Mark A.J. and Vieira, Newton J. and Kumar, Ch. Aswani},
  doi          = {10.3233/IDA-194569},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {643-663},
  shortjournal = {Intell. Data Anal.},
  title        = {Extraction of qualitative behavior rules for industrial processes from reduced concept lattice},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improvement of SAX representation for time series by
using complexity invariance. <em>IDA</em>, <em>24</em>(3), 625–641. (<a
href="https://doi.org/10.3233/IDA-194574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the area of time series data mining, a challenging task is to design an effectively and efficiently low-dimensional representation of high-dimensional time series data. Such an effective and efficient representation is important for dimensionality reduction of time series while preserving the co re information embedded in the original one. Among popular representations of time series, Symbolic Aggregate approXimation (SAX) has been widely used and is the core of many successful time series data mining systems. SAX firstly normalizes the given time series, then divides a time series into segments and finally assigns each segment a symbol based on its average value. In fact, many segments have different shapes but the same average value are mapped to a sole symbol. In order to overcome this drawback, in this work, we propose an improvement of SAX by using complexity invariance, namely Complexity-invariant SAX (CSAX). In particular, our proposed method transforms a time series into a sequence of symbols based on both average values and the complexity invariance of its segments. By experiments, we demonstrate that CSAX outperforms the SAX and its improvements, i.e., ESAX, SAX_TD, SAX_SD, in time series classification.},
  archive      = {J_IDA},
  author       = {Le, Xuan-May Thi and Tran, Tuan Minh and Nguyen, Hien T.},
  doi          = {10.3233/IDA-194574},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {625-641},
  shortjournal = {Intell. Data Anal.},
  title        = {An improvement of SAX representation for time series by using complexity invariance},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An active learning ensemble method for regression tasks.
<em>IDA</em>, <em>24</em>(3), 607–623. (<a
href="https://doi.org/10.3233/IDA-194608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning is a typical approach for learning from both labeled and unlabeled examples aiming to build efficient and accurate predictive models at minimum expense under an expert’s guidance. Since there is a lack of labeled data in many scientific fields whilst, at the same time, the labeling cost of unlabeled data is typically high in terms of time and expenditure, active learning has grown rapidly over recent years with great success. This is reflected in various studies providing insights and analyzing several active learning methods, especially in the case of classification tasks, whereas, there is only a limited number of studies concerning the implementation of active learning methods for regression ones. Within this context, the present paper sets out to put forward a pool-based active learning regression algorithm employing the query by committee strategy to evaluate the informativeness of unlabeled examples. The experimental results on a plethora of benchmark datasets demonstrate the efficiency of the proposed method, since it prevails over the baseline active learning approach applying the random sampling strategy, as well as familiar supervised methods.},
  archive      = {J_IDA},
  author       = {Fazakis, Nikos and Kostopoulos, Georgios and Karlos, Stamatis and Kotsiantis, Sotiris and Sgarbas, Kyriakos},
  doi          = {10.3233/IDA-194608},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {607-623},
  shortjournal = {Intell. Data Anal.},
  title        = {An active learning ensemble method for regression tasks},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient and robust bat algorithm with fusion of
opposition-based learning and whale optimization algorithm.
<em>IDA</em>, <em>24</em>(3), 581–606. (<a
href="https://doi.org/10.3233/IDA-194641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bat algorithm (BA) has the advantage of fast convergence, but there is still room for improvement in accuracy and stability of solution. An efficient and robust fusion bat algorithm (ERFBA) is proposed to overcome these defects. In the population reconstruction, an effective diversity population (E DP) is reconstructed by designing a multi-strategy opposition-based learning with disturbance. In the exploration, an adaptive constraint step whale optimization algorithm is presented to obtain the promising regions with fewer blind spots by exploring EDP. In the exploitation, we design a new BA local search strategy by novel combination between dynamic regulation and Cauchy mutation to get accurate and stable solution. Numerous experiments show that ERFBA has remarkable advantages in accuracy and stability for many high dimension, unimodal and multimodal problems. Moreover, the proposed algorithm is further tested and applied in areas of intelligent data analysis and intelligent design. The results show that the overall performance of the proposed ERFBA is better than other existing algorithms.},
  archive      = {J_IDA},
  author       = {Luo, Jinkun and He, Fazhi and Yong, Jiashi},
  doi          = {10.3233/IDA-194641},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {581-606},
  shortjournal = {Intell. Data Anal.},
  title        = {An efficient and robust bat algorithm with fusion of opposition-based learning and whale optimization algorithm},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating a one-class naive bayes text classifier.
<em>IDA</em>, <em>24</em>(3), 567–579. (<a
href="https://doi.org/10.3233/IDA-194669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays more and more information extraction projects need to classify large amounts of text data. The common way to classify text is to build a supervised classifier trained on human-labeled positive and negative examples. In many cases, however, it is easy to label positive examples, but hard to label negative examples. In this paper, we address the problem of building a one-class classifier when only the positive examples are labeled. Previous works on building one-class classifier mostly use positive examples and unlabeled data. In this paper, we show that a configurable one-class classifier such as one-class naive Bayes can be optimized by examining the clustering quality of the classification on target data. We propose to use existing and new quality scores for determining clustering quality of the classification. Experimental analysis with real-world data show that our approach generally achieves high classification accuracy, and in some cases improves the accuracy by more than 10% compared to state-of-art baselines.},
  archive      = {J_IDA},
  author       = {Zhang, Yihong and Jatowt, Adam},
  doi          = {10.3233/IDA-194669},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {567-579},
  shortjournal = {Intell. Data Anal.},
  title        = {Estimating a one-class naive bayes text classifier},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised learning of textual pattern based on
propagation in bipartite graph. <em>IDA</em>, <em>24</em>(3), 543–565.
(<a href="https://doi.org/10.3233/IDA-194528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based algorithms have aroused considerable interests in recent years by facilitating pattern recognition and learning via information propagation process through the graph. Here, we propose an unsupervised learning algorithm based on propagation on bipartite graph, referred to as Propagation in Bipartite Graph (PBG) algorithm. The contributions of this approach are threefold: 1) we present an iterative graph-based algorithm and a straight-forward bipartite representation for textual data, in which vertices represent documents and words, and edges between documents and words represent the occurrences of the words in the documents. Additionally, 2) we show that PBG is more flexible and easier to be adapted for different applications than the mathematical formalism of the generative models, and 3) we present a comprehensive evaluation and comparison of PBG to other topic extraction techniques. Here, we describe the strategy employed in PBG algorithm as a problem of maximization of similarity between latent vectors assigned to vertices and edges and demonstrate that the proposed strategy can be improved by assigning good initial values for the vectors. We notice that PBG can be parallelized by a simple adjustment in the algorithm. We also show that the proposed algorithm is competitive with LDA and NMF in the task of textual collection modelling, returning coherent topics, and in the dimensionality reduction task.},
  archive      = {J_IDA},
  author       = {de Paulo Faleiros, Thiago and Valejo, Alan and de Andrade Lopes, Alneu},
  doi          = {10.3233/IDA-194528},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {543-565},
  shortjournal = {Intell. Data Anal.},
  title        = {Unsupervised learning of textual pattern based on propagation in bipartite graph},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online analytical processsing on graph data. <em>IDA</em>,
<em>24</em>(3), 515–541. (<a
href="https://doi.org/10.3233/IDA-194576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Analytical Processing (OLAP) comprises tools and algorithms that allow querying multidimensional databases. It is based on the multidimensional model, where data can be seen as a cube such that each cell contains one or more measures that can be aggregated along dimensions. In a “Big Data” s cenario, traditional data warehousing and OLAP operations are clearly not sufficient to address current data analysis requirements, for example, social network analysis. Furthermore, OLAP operations and models can expand the possibilities of graph analysis beyond the traditional graph-based computation. Nevertheless, there is not much work on the problem of taking OLAP analysis to the graph data model. This paper proposes a formal multidimensional model for graph analysis, that considers the basic graph data, and also background information in the form of dimension hierarchies. The graphs in this model are node- and edge-labelled directed multi-hypergraphs, called graphoids, which can be defined at several different levels of granularity using the dimensions associated with them. Operations analogous to the ones used in typical OLAP over cubes are defined over graphoids. The paper presents a formal definition of the graphoid model for OLAP, proves that the typical OLAP operations on cubes can be expressed over the graphoid model, and shows that the classic data cube model is a particular case of the graphoid data model. Finally, a case study supports the claim that, for many kinds of OLAP-like analysis on graphs, the graphoid model works better than the typical relational OLAP alternative, and for the classic OLAP queries, it remains competitive.},
  archive      = {J_IDA},
  author       = {Gómez, Leticia and Kuijpers, Bart and Vaisman, Alejandro},
  doi          = {10.3233/IDA-194576},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {515-541},
  shortjournal = {Intell. Data Anal.},
  title        = {Online analytical processsing on graph data},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed frequent subgraph mining on evolving graph using
SPARK. <em>IDA</em>, <em>24</em>(3), 495–513. (<a
href="https://doi.org/10.3233/IDA-194601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the graph mining context, frequent subgraph identification plays a key role in retrieving required information or patterns from the huge amount of data in a short period. The problem of finding frequent items in traditional mining changed to the innovation of subgraphs that recurrently occur s in graph datasets containing a single huge graph. Majority of the existing methods target static graphs, and the distributed solution for dynamic graphs has not been explored. But, in modern applications like Facebook, robotics utilizes large evolving graphs. The goal is to design a method to find recurrent subgraphs from a single large evolving graph. In this research paper, a novel approach is proposed called DFSME, which uses SPARK to discover frequent subgraphs from an evolving graph in a distributed environment. DFSME maintains a set of subgraphs between frequent and infrequent subgraphs, which is used to decrease the search space. Our experiments with synthetic and real-world datasets authorize the effectiveness of DFSME for mining of recurrent subgraphs from huge evolving graph datasets.},
  archive      = {J_IDA},
  author       = {Senthilselvan, N. and Subramaniyaswamy, V. and Vijayakumar, V. and Karimi, Hamid Reza and Aswin, N. and Ravi, Logesh},
  doi          = {10.3233/IDA-194601},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {495-513},
  shortjournal = {Intell. Data Anal.},
  title        = {Distributed frequent subgraph mining on evolving graph using SPARK},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). Editorial. <em>IDA</em>, <em>24</em>(3), 491–493. (<a
href="https://doi.org/10.3233/IDA-200002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-200002},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {491-493},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MAPK-means: A clustering algorithm with quantitative
preferences on attributes. <em>IDA</em>, <em>24</em>(2), 459–489. (<a
href="https://doi.org/10.3233/IDA-184468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a new semi-supervised clustering algorithm as part of a more general framework of interactive exploratory clustering, that favors the exploration of possible clustering solutions so that an expert tailors the best clustering according to her domain knowledge and preferences. Co ntrary to most existing approaches, the novel algorithm considers the feature space as a first class citizen for the exploration of alternative solutions. Our proposal represents and integrates quantitative preferences on attributes that will guide the exploration of possible solutions by learning an appropriate space metric. It also achieves a compromise clustering based on expert confidence, between a data-driven and a user-driven solution and converges with a good complexity. We show experimentally that our method is also able to deal with irrelevant user preferences and correct those choices in order to achieve a better solution. Experiments show that the best results may be achieved only with the addition of preferences to traditional metric learning algorithms and that our approach performs better than state-of-the-art algorithms.},
  archive      = {J_IDA},
  author       = {El Moussawi, Adnan and Giacometti, Arnaud and Labroche, Nicolas and Soulet, Arnaud},
  doi          = {10.3233/IDA-184468},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {459-489},
  shortjournal = {Intell. Data Anal.},
  title        = {MAPK-means: A clustering algorithm with quantitative preferences on attributes},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge-embodied attention for distantly supervised
relation extraction. <em>IDA</em>, <em>24</em>(2), 445–457. (<a
href="https://doi.org/10.3233/IDA-194476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge bases (KBs) provide a large amount of structured information for entities and relations, which are successfully leveraged in many natural language processing tasks. However, distantly supervised relation extraction only utilizes KBs to automatically generate datasets, while ignoring the b ackground information in KBs during the relation extraction process. We herein propose a knowledge-embodied attention that leverages knowledge information in KBs to reduce the impact of noisy data for distantly supervised relation extraction. Specifically, we pre-train distributed representations of KBs with the knowledge representation learning (KRL) model, and subsequently incorporate them into relation extraction to learn sentence-level attention weights. The experimental results demonstrate that our approach outperforms all baselines, thus indicating that we can focus our attention on valid data by leveraging background information in KBs.},
  archive      = {J_IDA},
  author       = {Deng, Kejun and Zhang, Xuemiao and Ye, Songtao and Liu, Junfei},
  doi          = {10.3233/IDA-194476},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {445-457},
  shortjournal = {Intell. Data Anal.},
  title        = {Knowledge-embodied attention for distantly supervised relation extraction},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning labeling functions in distantly supervised relation
extraction. <em>IDA</em>, <em>24</em>(2), 427–443. (<a
href="https://doi.org/10.3233/IDA-194492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distant supervision has become the leading method for training large-scale information extractors. It could be encoded in the form of labeling functions, which employ knowledge bases to provide labels for the data. However, most previous works use only simple labeling functions, resulting in too mu ch noise in the training data, and the knowledge bases are far from well-explored. In this paper, in order to improve the labeling quality of the training data for distant supervision relation extraction, we propose to make use of existing knowledge bases to effectively learn labeling functions. Specifically, labeling functions are represented as Markov Logic, which can integrate various resources into a unified model naturally. Experimental results show that the training data produced by the learned labeling functions is significantly improved in quality. Different distantly supervised relation extraction models trained on the produced training data can also achieve better performances.},
  archive      = {J_IDA},
  author       = {Gui, Yaocheng and Liu, Qian and Gao, Zhiqiang},
  doi          = {10.3233/IDA-194492},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {427-443},
  shortjournal = {Intell. Data Anal.},
  title        = {Learning labeling functions in distantly supervised relation extraction},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biased transfer matching for less overlapping degree for
unsupervised domain adaptation. <em>IDA</em>, <em>24</em>(2), 409–425.
(<a href="https://doi.org/10.3233/IDA-194516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is an important branch of transfer learning. Previous studies have always taken efforts to minimize the optimization goal, but they neglect the relative quality of features or instances. For example, a classic work treats different instances equally in a degree and chooses these i nstances which minimize the optimization function value. This method will discard these instances that make the data distribution in source and target data domain different and will neglect the instances’ relative quality. To reduce interference between instances in the process of domain adaptation, we put forward a novel method of ODA that uses the overlapping degree to measure every feature or instance’s relative quality and implement feature or instance reweighting. At the same time, we have noticed that there are many parameters with values that will influence the effect of the method. Previous studies do not have a reasonable method to determine the parameters’ values. We can use the genetic algorithm to find the balance between marginal distribution adaptation and conditional distribution adaptation to find the best combination of multiple parameters. Experiments we have done verify that the ODA method outperforms by 3.26% compared with the best comparison method. We have found that our method of finding the optimal parameters can yield more accurate results than the original method.},
  archive      = {J_IDA},
  author       = {Wen, Yiran and Cao, Xiu and Wang, Xueping and Liu, Fangyuan},
  doi          = {10.3233/IDA-194516},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {409-425},
  shortjournal = {Intell. Data Anal.},
  title        = {Biased transfer matching for less overlapping degree for unsupervised domain adaptation},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient heuristics for learning bayesian network from
labeled and unlabeled data. <em>IDA</em>, <em>24</em>(2), 385–408. (<a
href="https://doi.org/10.3233/IDA-194509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian network classifiers (BNCs) are powerful tools to mine statistical knowledge from data and infer under conditions of uncertainty. However, most of the traditional BNCs focus on mining the dependency relationships existed in labeled data while neglecting the information hidden in unlabeled d ata, which may result in the biased decision boundaries. To address this issue, we introduce a new order-based greedy search heuristic based on mutual information for building efficient structures in tree-augmented naive Bayes (TAN), which is a highly accurate learner while maintaining simplicity and efficiency. Target learning is used to dynamically describe the dependency relationships in each unlabeled test instance. Extensive experimental results on UCI (University of California at Irvine) machine learning repository demonstrate that our proposed algorithm is a competitive alternative to state-of-the-art classifiers like weighted averaged TAN and k-dependence Bayesian classifier, as well as Random forest.},
  archive      = {J_IDA},
  author       = {Duan, Zhiyi and Wang, Limin and Sun, Minghui},
  doi          = {10.3233/IDA-194509},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {385-408},
  shortjournal = {Intell. Data Anal.},
  title        = {Efficient heuristics for learning bayesian network from labeled and unlabeled data},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An intrusion detection method based on active transfer
learning. <em>IDA</em>, <em>24</em>(2), 363–383. (<a
href="https://doi.org/10.3233/IDA-194487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection plays a very important role in the field of network security. In order to improve the intrusion detection rate, intrusion detection algorithms based traditional machine learning are widely used in this field. These methods generally satisfy the following two assumptions: the tra ining and the testing data must be under the condition of the independent and identical distribution; the training samples are sufficient. However, in practice, the above assumptions are difficult to satisfy, which will result in poor intrusion detection. This paper proposes an intrusion detection algorithm based on active transfer learning ACTrAdaBoost. ACTrAdaBoost takes advantage of transfer learning and need not to satisfy the two assumptions of the traditional machine learning. In addition, ACTrAdaBoost utilizes active learning and maximum mean discrepancy knowledge to obtain maximum knowledge with minimum training sample cost and solve the problem of negative transfer. The ACTrAdaBoost compared with the traditional machine learning method on the KDDCUP99, DARPA1998 and ISCX2012 datasets. The experimental results show that the intrusion detection rate of the ACTrAdaBoost algorithm is greater than benchmark algorithms, and the training time efficiency improves at the same time. The performance of ACTrAdaBoost is better than the traditional machine learning classification algorithm. The ACTrAdaBoost algorithm improves the accuracy of intrusion detection and provides a new research method for intrusion detection.},
  archive      = {J_IDA},
  author       = {Li, Jingmei and Wu, Weifei and Xue, Di},
  doi          = {10.3233/IDA-194487},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {363-383},
  shortjournal = {Intell. Data Anal.},
  title        = {An intrusion detection method based on active transfer learning},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining sequences in activities for time use analysis.
<em>IDA</em>, <em>24</em>(2), 339–362. (<a
href="https://doi.org/10.3233/IDA-184361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By providing a complete record of time use for a given population, time use studies enable investigators to test various hypotheses concerning that behavior. However, the large number and variety of activity combinations that are relevant in time allocation choices and, therefore, time use analysis , makes measuring or even fully identifying all of them impossible without the proper data mining tools. In this paper, we propose a framework for mining sequences of activities to capture more complex patterns than those currently available on how individuals organize their days. The proposed framework was applied to the American Time Use Surveys (ATUS) dataset to explore individual time allocation behavior, identifying sequences of activities that are frequent. For example, patterns such as the preferred activities that are performed before and after specific activities (such as paid work or leisure) are discussed in terms of their frequency. Such patterns are not easy to reveal using traditional descriptive analysis.},
  archive      = {J_IDA},
  author       = {Rosales-Salas, Jorge and Maldonado, Sebastián and Seret, Alex},
  doi          = {10.3233/IDA-184361},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {339-362},
  shortjournal = {Intell. Data Anal.},
  title        = {Mining sequences in activities for time use analysis},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved opposition based learning firefly algorithm with
dragonfly algorithm for solving continuous optimization problems.
<em>IDA</em>, <em>24</em>(2), 309–338. (<a
href="https://doi.org/10.3233/IDA-194485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the existence of continuous optimization problems has led researchers to come up with a variety of methods to solve continues optimization problems. The metaheuristic algorithms are one of the most popular and common ways to solve continuous optimization problems. Firefly Algorithm (FA) i s a successful metaheuristic algorithm for solving continuous optimization problems; however, although this algorithm performs very well in local search, it has weaknesses and disadvantages in finding solution in global search. This problem has caused this algorithm to be trapped locally and the balance between exploration and exploitation cannot be well maintained. In this paper, three different approaches based on the Dragonfly Algorithm (DA) processes and the OBL method are proposed to improve exploration, performance, efficiency and information-sharing of the FA and to avoid the FA getting stuck in local trap. In the first proposed method (FADA), the robust processes of DA are used to improve the exploration, performance and efficiency of the FA; and the second proposed method (OFA) uses an Opposition-Based Learning (OBL) algorithm to accelerate the convergence and exploration of the FA. Finally, in the third approach, which is referred to as OFADA in this paper, a hybridization of the hybrid FADA and the OBL method is used to improve the convergence and accuracy of the FA. The three proposed methods were implemented on functions with 2, 4, 10, and 30 dimensions. The results of the implementation of these three proposed methods showed that OFADA approach outperformed the other two proposed methods and other compared metaheuristic algorithms in different dimensions. In addition, all the three proposed methods provided better results compared with other metaheuristic algorithms on small-dimensional functions. However, performance of many metaheuristic algorithms decreased with increasing the dimensions of the functions. While the three proposed methods, in particular the OFADA approach, have been able to make better converge with the higher-dimensional optimization functions toward the target in comparison with other metaheuristic algorithms, and to show a high performance.},
  archive      = {J_IDA},
  author       = {Abedi, Mehdi and Gharehchopogh, Farhad Soleimanian},
  doi          = {10.3233/IDA-194485},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {309-338},
  shortjournal = {Intell. Data Anal.},
  title        = {An improved opposition based learning firefly algorithm with dragonfly algorithm for solving continuous optimization problems},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybridization of population-based ant colony optimization
via data mining. <em>IDA</em>, <em>24</em>(2), 291–307. (<a
href="https://doi.org/10.3233/IDA-184431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a hybrid application of Population Based Ant Colony Optimization that uses a data mining procedure to wisely initialize the pheromone entries. Hybridization of metaheuristics with data mining techniques has been studied by several researchers in recent years. In this line of research, fr equent patterns in a number of initial high-quality solutions are extracted to guide the subsequent iterations of an algorithm, which results in an improvement in solution quality and computational time. Our proposal possesses certain differences from and contributions to existing literature. Instead of one single run that incorporates both the main metaheuristic and the data mining module inside, we propose to carry out independent runs and collect elite sets over these trials. Another contribution is the way we use the knowledge gained from the application of the data mining module. The extracted knowledge is used to initialize the memory model in the algorithm rather than to construct new initial solutions. One additional contribution is the use of a path mining algorithm (a specific sequence mining algorithm) rather than Apriori-like association mining algorithms. Computational experiments, conducted both on symmetric Travelling Salesman Problem and symmetric/asymmetric Quadratic Assignment Problem instances, showed that our proposal produces significantly better results, and is more robust than pure applications of population-based ant colony optimization.},
  archive      = {J_IDA},
  author       = {Adak, Zeynep and Demiriz, Ayhan},
  doi          = {10.3233/IDA-184431},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {291-307},
  shortjournal = {Intell. Data Anal.},
  title        = {Hybridization of population-based ant colony optimization via data mining},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison study on nonlinear dimension reduction methods
with kernel variations: Visualization, optimization and classification.
<em>IDA</em>, <em>24</em>(2), 267–290. (<a
href="https://doi.org/10.3233/IDA-194486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of high dimensionality, correlation among covariates, and noise contained in data, dimension reduction (DR) techniques are often employed to the application of machine learning algorithms. Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and their kernel variants (KPC A, KLDA) are among the most popular DR methods. Recently, Supervised Kernel Principal Component Analysis (SKPCA) has been shown as another successful alternative. In this paper, brief reviews of these popular techniques are presented first. We then conduct a comparative performance study based on three simulated datasets, after which the performance of the techniques are evaluated through application to a pattern recognition problem in face image analysis. The gender classification problem is considered on MORPH-II and FG-NET, two popular longitudinal face aging databases. Several feature extraction methods are used, including biologically-inspired features (BIF), local binary patterns (LBP), histogram of oriented gradients (HOG), and the Active Appearance Model (AAM). After applications of DR methods, a linear support vector machine (SVM) is deployed with gender classification accuracy rates exceeding 95% on MORPH-II, competitive with benchmark results. A parallel computational approach is also proposed, attaining faster processing speeds and similar recognition rates on MORPH-II. Our computational approach can be applied to practical gender classification systems and generalized to other face analysis tasks, such as race classification and age prediction.},
  archive      = {J_IDA},
  author       = {Kempfert, Katherine C. and Wang, Yishi and Chen, Cuixian and Wong, Samuel W.K.},
  doi          = {10.3233/IDA-194486},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {267-290},
  shortjournal = {Intell. Data Anal.},
  title        = {A comparison study on nonlinear dimension reduction methods with kernel variations: Visualization, optimization and classification},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cross-lingual sentiment topic model evolution over time.
<em>IDA</em>, <em>24</em>(2), 253–266. (<a
href="https://doi.org/10.3233/IDA-184449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis in various languages has been a hot research topic with several applications. Most of the existing models have been reported to work well with widely used language. Were the lass directly applying these models to poor-quality corpora often leads to low results. Thus, to deal with these shortcoming we propose a cross-lingual sentiment topic model evolution over time (CLSTOT) which jointly models time with topic and sentiment. In CLSTOT, we consider the mapping between sentiment-aware topics under different cultures and analyze their evolution over time. The topic-specific sentiment is extracted using the entire data and not for each single document. As long as providing sentiment-topic, we can predict the timestamps for each test document by finding its most likely location over the timeline. This is achieved by using inference algorithm which is based on Gibbs Sampling. The experimental results on Chinese and English newsreader dataset; Chinese from SinaNews2, and English from Yahoo1, show that CLSTOT achieves significant improvement over the state-of-the-art.},
  archive      = {J_IDA},
  author       = {Musa, Ibrahim Hussein and Xu, Kang and Liu, Feng and Zamit, Ibrahim and Abro, Waheed Ahmed and Qi, Guilin},
  doi          = {10.3233/IDA-184449},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {253-266},
  shortjournal = {Intell. Data Anal.},
  title        = {A cross-lingual sentiment topic model evolution over time},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain sentiment dictionary construction and optimization
based on multi-source information fusion. <em>IDA</em>, <em>24</em>(2),
229–251. (<a href="https://doi.org/10.3233/IDA-184426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis of text data, such as reviews, can help users and merchants make more favorable decisions. It is difficult to use the popular supervised learning method to complete the sentiment classification task because marking data manually is time-consuming and laborious. Unsupervised senti ment classification methods are mostly based on sentiment lexicons. The existing sentiment lexicons are simply not capable of domain sentiment classification, it still requires to construct a domain sentiment lexicon. There are still many problems with the advanced domain sentiment lexicon construction methods, e.g., rely heavily on labeled data, poor accuracy. We propose a labeled data extension idea to reduce the dependence of supervised learning methods on labeled data. In order to solve the problems of domain sentiment lexicon construction, we proposed a novel framework based on multi-source information fusion (MSIF) for learning. We extracted four kinds of emotional information, which are lexicon emotional information, emotional word co-occurrence information, emotional word polarity information and polarity relationship information of emotional word pair. When extracting the co-occurrence information, a novel method based on the data extension idea is proposed to enhance its accuracy and coverage. In order to accelerate the solution of the fusion model, an optimization method based on the ADMM algorithm is applied. Experimental results on five Amazon product review datasets show that the sentiment dictionary constructed by the proposed method can significantly improve the performance of review sentiment classification compared with the current popular baseline and the state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Chen, Zuo and Li, Xin and Wang, Min and Yang, Shenggang},
  doi          = {10.3233/IDA-184426},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {229-251},
  shortjournal = {Intell. Data Anal.},
  title        = {Domain sentiment dictionary construction and optimization based on multi-source information fusion},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). Editorial. <em>IDA</em>, <em>24</em>(2), 225–227. (<a
href="https://doi.org/10.3233/IDA-200001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-200001},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {225-227},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new optimization layer for real-time bidding advertising
campaigns. <em>IDA</em>, <em>24</em>(1), 199–224. (<a
href="https://doi.org/10.3233/IDA-194527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While it is relatively easy to start an online advertising campaign, obtaining a high Key Performance Indicator (KPI) can be challenging. A large body of work on this subject has already been performed and platforms known as DSPs are available on the market that deal with such an optimization. From the advertiser’s point of view, each DSP is a different black box, with its pros and cons, that needs to be configured. In order to take advantage of the pros of every DSP, advertisers are well-advised to use a combination of them when setting up their campaigns. In this paper, we propose an algorithm for advertisers to add an optimization layer on top of DSPs. The algorithm we introduce, called SKOTT, maximizes the chosen KPI by optimally configuring the DSPs and putting them in competition with each other. SKOTT is a highly specialized iterative algorithm loosely based on gradient descent that is made up of three independent sub-routines, each dealing with a different problem: partitioning the budget, setting the desired average bid, and preventing under-delivery. In particular, one of the novelties of our approch lies in our taking the perspective of the advertisers rather than the DSPs. Synthetic market data is used to evaluate the efficiency of SKOTT against other state-of-the-art approaches adapted from similar problems. The results illustrate the benefits of our proposals, which greatly outperforms the other methods.},
  archive      = {J_IDA},
  author       = {Micchi, Gianluca and Soheily Khah, Saeid and Turner, Jacob},
  doi          = {10.3233/IDA-194527},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {199-224},
  shortjournal = {Intell. Data Anal.},
  title        = {A new optimization layer for real-time bidding advertising campaigns},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Storyline extraction from news articles with dynamic
dependency. <em>IDA</em>, <em>24</em>(1), 183–197. (<a
href="https://doi.org/10.3233/IDA-184448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storyline generation aims to produce a concise summary of related events unfolding over time from a collection of news articles. It can be cast into an evolutionary clustering problem by separating news articles into different epochs. Existing unsupervised approaches to storyline generation are typ ically based on probabilistic graphical models. They assume that the storyline distribution at the current epoch depends on the weighted combination of storyline distributions in the latest previous M epochs. The evolutionary parameters of such long-term dependency are typically set by a fixed exponential decay function to capture the intuition that events in more recent epochs have stronger influence to the storyline generation in the current epoch. However, we argue that the amount of relevant historical contextual information should vary for different storylines. Therefore, in this paper, we propose a new Dynamic Dependency Storyline Extraction Model (D2SEM) in which the dependencies among events in different epochs but belonging to the same storyline are dynamically updated to track the time-varying distributions of storylines over time. The proposed model has been evaluated on three news corpora and the experimental results show that it outperforms the state-of-the-art approaches and is able to capture the dependency on historical contextual information dynamically.},
  archive      = {J_IDA},
  author       = {Guo, Linsen and Zhou, Deyu and He, Yulan and Xu, Haiyang},
  doi          = {10.3233/IDA-184448},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {183-197},
  shortjournal = {Intell. Data Anal.},
  title        = {Storyline extraction from news articles with dynamic dependency},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EEG-based tonic cold pain assessment using extreme learning
machine. <em>IDA</em>, <em>24</em>(1), 163–182. (<a
href="https://doi.org/10.3233/IDA-184388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to present a novel method which can objectively identify the subjective perception of tonic pain. To achieve this goal, scalp EEG data are recorded from 16 subjects under the cold stimuli condition. The proposed method is capable of classifying four classes of tonic pai n states, which include No pain, Minor Pain, Moderate Pain, and Severe Pain. Due to multi-class problem of our research an extended Common Spatial Pattern (ECSP) method is first proposed for accurately extracting features of tonic pain from captured EEG data. Then, a single-hidden-layer feedforward network is used as a classifier for pain identification. With the aid of extreme learning machine (ELM) algorithm, the classifier is trained here. The advantages of ELM-based classifier can obtain an optimal and generalized solution for multi-class tonic cold pain. Experimental results demonstrate that the proposed method discriminates the tonic pain successfully. Additionally, to show the superiority for the ELM-based classifier, compared results with the well-known support vector machine (SVM) method show the ELM-based classifier outperform than the SVM-based classifier. These findings may pay the way for providing a direct and objective measure of the subjective perception of tonic pain.},
  archive      = {J_IDA},
  author       = {Yu, Mingxin and Yan, Hao and Han, Jing and Lin, Yingzi and Zhu, Lianqing and Tang, Xiaoying and Sun, Guangkai and He, Yanlin and Guo, Yikang},
  doi          = {10.3233/IDA-184388},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {163-182},
  shortjournal = {Intell. Data Anal.},
  title        = {EEG-based tonic cold pain assessment using extreme learning machine},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A study on rare fraud predictions with big medicare claims
fraud data. <em>IDA</em>, <em>24</em>(1), 141–161. (<a
href="https://doi.org/10.3233/IDA-184415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Access to copious amounts of information has reached unprecedented levels, and can generate very large data sources. These big data sources often contain a plethora of useful information but, in some cases, finding what is actually useful can be quite problematic. For binary classification problems , such as fraud detection, a major concern therein is one of class imbalance. This is when a dataset has more of one label versus another, such as a large number of non-fraud observations with comparatively few observations of fraud (which we consider the class of interest). Class rarity further delineates class imbalance with significantly smaller numbers in the class of interest. In this study, we assess the impacts of class rarity in big data, and apply data sampling to mitigate some of the performance degradation caused by rarity. Real-world Medicare claims datasets with known excluded providers are used as fraud labels for a fraud detection scenario, incorporating three machine learning models. We discuss the necessary data processing and engineering steps in order to understand, integrate, and use the Medicare data. From these already imbalanced datasets, we generate three additional datasets representing varying levels of class rarity. We show that, as expected, rarity significantly decreases model performance, but data sampling, specifically random undersampling, can help significantly with rare class detection in identifying Medicare claims fraud cases.},
  archive      = {J_IDA},
  author       = {Bauder, Richard A. and Khoshgoftaar, Taghi M.},
  doi          = {10.3233/IDA-184415},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {141-161},
  shortjournal = {Intell. Data Anal.},
  title        = {A study on rare fraud predictions with big medicare claims fraud data},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Community detection in dynamic networks using constraint
non-negative matrix factorization. <em>IDA</em>, <em>24</em>(1),
119–139. (<a href="https://doi.org/10.3233/IDA-184432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community structure, a foundational concept in understanding networks, is one of the most important properties of dynamic networks. A large number of dynamic community detection methods proposed are based on the temporal smoothness framework that the abrupt change of clustering within a short perio d is undesirable. However, how to improve the community detection performance by combining network topology information in a short period is a challenging problem. Additionally, previous efforts on utilizing such properties are insufficient. In this paper, we introduce the geometric structure of a network to represent the temporal smoothness in a short time and propose a novel Dynamic Graph Regularized Symmetric NMF method (DGR-SNMF) to detect the community in dynamic networks. This method combines geometric structure information sufficiently in current detecting process by Symmetric Non-negative Matrix Factorization (SNMF). We also prove the convergence of the iterative update rules by constructing auxiliary functions. Extensive experiments on multiple synthetic networks and two real-world datasets demonstrate that the proposed DGR-SNMF method outperforms the state-of-the-art algorithms on detecting dynamic community.},
  archive      = {J_IDA},
  author       = {Wang, Shuaihui and Li, Guopeng and Hu, Guyu and Wei, Hao and Pan, Yu and Pan, Zhisong},
  doi          = {10.3233/IDA-184432},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {119-139},
  shortjournal = {Intell. Data Anal.},
  title        = {Community detection in dynamic networks using constraint non-negative matrix factorization},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A latent-label denoising method for relation extraction with
self-directed confidence learning. <em>IDA</em>, <em>24</em>(1),
101–117. (<a href="https://doi.org/10.3233/IDA-184414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distant supervision for relation extraction aims to automatically obtain a large number of relational facts as training data, but it often leads to noisy label problem. In this paper, we propose a self-directed confidence learning based latent-label denoising method for distantly supervised relatio n extraction. Concretely, a self-directed algorithm that combines the semantic information of model prediction and distant supervision is designed to predict the confidence score of latent labels. Since this mechanism utilizes the obtained latent labels of easy examples to produce the latent labels of hard examples step by step, it is a robust and reliable learning process. Besides, it facilitates dynamic exploration of the confidence space to achieve better denoising performance. Moreover, to cope with the common imbalance problem in large corpus where the negative instances account for a much larger percentage, we introduce a discriminative loss function to solve the misclassification between non-relational and relational instances. Empirically, in order to verify the generality of the proposed denoising method, we use different neural models – CNN, PCNN and BiLSTM for representation learning. Experimental results show that our method can correct the noisy labels with high accuracy and outperform the state-of-the-art relation extraction systems.},
  archive      = {J_IDA},
  author       = {Sun, Tingting and Zhang, Chunhong and Ji, Yang and Hu, Zheng},
  doi          = {10.3233/IDA-184414},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {101-117},
  shortjournal = {Intell. Data Anal.},
  title        = {A latent-label denoising method for relation extraction with self-directed confidence learning},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An end-to-end distance measuring for mixed data based on
deep relevance learning. <em>IDA</em>, <em>24</em>(1), 83–99. (<a
href="https://doi.org/10.3233/IDA-184399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance Measuring between two mixed data objects is the basis of many learning algorithms. The complex relevance between heterogeneous – various types/scales – attributes has a significant influence on the measured results. In this paper, we propose an End-to-End Distance Measuring method for mixe d data based on deep relevance learning, called E2DM. Existing methods confuse the attributes space by mapping the discrete attribute values to new continuous values, or discretize continuous attributes values without considering the relevance. In contrast, E2DM directly manipulates on the original data with data conversion and relevance learning simultaneously to avoid information loss and attribute space confusion. E2DM firstly estimates internal relevance (i.e., relevance within the attribute) influenced distance by considering the categorical attribute value frequency and mapping numerical attribute values into multiple bins. Then it takes a wrapper approach to iteratively optimize relevance influenced distance and bin boundaries using a Frobenius-norm deviation as its objective function. Co-occurrence Mover’s Distance is proposed to explicitly explore relevance between attributes in each iteration. Finally, the distance for numerical attribute values is refined based on the original values and the fallen bin centers. Experimental results on a number of real-world datasets demonstrate that E2DM outperforms the state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Cheng, Li and Wang, Yijie and Ma, Xingkong},
  doi          = {10.3233/IDA-184399},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {83-99},
  shortjournal = {Intell. Data Anal.},
  title        = {An end-to-end distance measuring for mixed data based on deep relevance learning},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Label distribution learning with climate probability for
ensemble forecasting. <em>IDA</em>, <em>24</em>(1), 69–82. (<a
href="https://doi.org/10.3233/IDA-184446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In meteorology, ensemble forecasting aims to post-process an ensemble of multiple members’ forecasts and make better weather predictions. While multiple individual forecasts are generated to represent the uncertain weather system, the performance of ensemble forecasting is unsatisfactory. In this p aper we conduct data analysis based on the expertise of human forecasters and introduce a machine learning method for ensemble forecasting. The proposed method, Label Distribution Learning with Climate Probability (LDLCP), can improve the accuracy of both deterministic forecasting and probabilistic forecasting. The LDLCP method utilizes the relevant variables of previous forecasts to construct the feature matrix and applies label distribution learning (LDL) to adjust the probability distribution of ensemble forecast. Our proposal is novel in its specialized target function and appropriate conditional probability function for the ensemble forecasting task, which can optimize the forecasts to be consistent with local climate. Experimental testing is performed on both artificial data and the data set for ensemble forecasting of precipitation in East China from August to November, 2017. Experimental results show that, compared with a baseline method and two state-of-the-art machine learning methods, LDLCP shows significantly better performance on measures of RMSE and average continuous ranked probability score.},
  archive      = {J_IDA},
  author       = {Yang, Xuebing and Wu, Yajing and Zhang, Wensheng and Tang, Wei},
  doi          = {10.3233/IDA-184446},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {69-82},
  shortjournal = {Intell. Data Anal.},
  title        = {Label distribution learning with climate probability for ensemble forecasting},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detailed investigation of deep features with sparse
representation and dimensionality reduction in CBIR: A comparative
study. <em>IDA</em>, <em>24</em>(1), 47–68. (<a
href="https://doi.org/10.3233/IDA-184411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on content-based image retrieval (CBIR) has been under development for decades, and numerous methods have been competing to extract the most discriminative features for improved representation of the image content. Recently, deep learning methods have gained attention in computer vision, i ncluding CBIR. In this paper, we present a comparative investigation of different features, including low-level and high-level features, for CBIR. We compare the performance of CBIR systems using different deep features with state-of-the-art low-level features such as SIFT, SURF, HOG, LBP, and LTP, using different dictionaries and coefficient learning techniques. Furthermore, we conduct comparisons with a set of primitive and popular features that have been used in this field, including colour histograms and Gabor features. We also investigate the discriminative power of deep features using certain similarity measures under different validation approaches. Furthermore, we investigate the effects of the dimensionality reduction of deep features on the performance of CBIR systems using principal component analysis, discrete wavelet transform, and discrete cosine transform. Unprecedentedly, the experimental results demonstrate high (95% and 93%) mean average precisions when using the VGG-16 FC7 deep features of Corel-1000 and Coil-20 datasets with 10-D and 20-D K-SVD, respectively.},
  archive      = {J_IDA},
  author       = {Tarawneh, Ahmad S. and Celik, Ceyhun and Hassanat, Ahmad B. and Chetverikov, Dmitry},
  doi          = {10.3233/IDA-184411},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {47-68},
  shortjournal = {Intell. Data Anal.},
  title        = {Detailed investigation of deep features with sparse representation and dimensionality reduction in CBIR: A comparative study},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DNN models based on dimensionality reduction for stock
trading. <em>IDA</em>, <em>24</em>(1), 19–45. (<a
href="https://doi.org/10.3233/IDA-184403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to avoid missing representative features, we should select a lot of features as far as possible when using machine learning algorithms in stock trading. Meanwhile, these high dimensional features can lead to redundancy of information and reduce the efficiency, and accuracy of learning algo rithms. It is worth noting that dimensionality reduction operation (DRO) is one of the main means to deal with stock high-dimensional data. However, there are few studies on whether DRO can significantly improve the trading performance of deep neural network (DNN) algorithms. Therefore, this paper selects large-scale stock datasets in the American market and in the Chinese market as the research objects. For each stock, we firstly apply four most widely used DRO, namely principal component analysis (PCA), least absolute shrinkage and selection operator (LASSO), classification and regression trees (CART), and autoencoder (AE) to deal with original features respectively, and then use the new features as inputs of the most six popular DNN algorithms such as Multilayer Perceptron (MLP), Deep Belief Network (DBN), Stacked Auto-Encoders (SAE), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) to generate trading signals. Finally, we apply the trading signals to conduct a lot of daily trading back-testing and non-parameter statistical testing. The experiments show that LASSO can significantly improve the performance of RNN, LSTM, and GRU. In addition, any DRO mentioned in this paper do not significantly improve trading performance and the speed of generating trading signals of the other DNN algorithms.},
  archive      = {J_IDA},
  author       = {Lv, Dongdong and Wang, Dong and Li, Meizi and Xiang, Yang},
  doi          = {10.3233/IDA-184403},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {19-45},
  shortjournal = {Intell. Data Anal.},
  title        = {DNN models based on dimensionality reduction for stock trading},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Balanced training/test set sampling for proper evaluation of
classification models. <em>IDA</em>, <em>24</em>(1), 5–18. (<a
href="https://doi.org/10.3233/IDA-194477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, classification involves identifying the categories or classes to which a new observation belongs based on a training set. The performance of a classification model is generally measured by the classification accuracy of a test set. The first step in developing a classification model is to divide an acquired dataset into training and test sets through random sampling. In general, random sampling does not guarantee that test accuracy reflects the performance of a developed classification model. If random sampling produces biased training/test sets, the classification model may result in bias. In this study, we show the problems of random sampling and propose balanced sampling as an alternative. We also propose a measure for evaluating sampling methods. We perform empirical experiments using benchmark datasets to verify that our sampling algorithm produces proper training and test sets. The results confirm that our method produces better training and test sets than random and several non-random sampling methods can.},
  archive      = {J_IDA},
  author       = {Kang, Donghoon and Oh, Sejong},
  doi          = {10.3233/IDA-194477},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {5-18},
  shortjournal = {Intell. Data Anal.},
  title        = {Balanced training/test set sampling for proper evaluation of classification models},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020g). Editorial. <em>IDA</em>, <em>24</em>(1), 1–3. (<a
href="https://doi.org/10.3233/IDA-190007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-190007},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {24},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
