<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm---264">TMM - 264</h2>
<ul>
<li><details>
<summary>
(2020a). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(12), C3. (<a
href="https://doi.org/10.1109/TMM.2020.3035459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.3035459},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning non-locally regularized compressed sensing network
with half-quadratic splitting. <em>TMM</em>, <em>22</em>(12), 3236–3248.
(<a href="https://doi.org/10.1109/TMM.2020.2973862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based Compressed Sensing (CS) reconstruction attracts much attention in recent years, due to its significant superiority of reconstruction quality. Its success is mainly attributed to the employment of a large dataset for pre-training the network to learn a reconstruction mapping. In this paper, we propose a non-locally regularized compressed sensing network for reconstructing image sequences, which can achieve high reconstruction quality without pre-training. Specifically, the proposed method attempts to learn a deep network prior for the reconstruction of an individual instance under the constraint that the network output can well match the given CS measurement. The non-local prior is designed to guide the network to capture the long-range dependencies by exploiting the self-similarities among images, and it can also make the network noise-aware. In order to deal with the compound of non-local prior and deep network prior, we construct a half-quadratic splitting based optimization method for network learning, in which the two priors are decoupled into two simple sub-problems by introducing an auxiliary variable and a quadratic fidelity constraint. Extensive experimental results demonstrate that our method is competitive to the popular methods, including sparsity prior based methods and deep learning based methods, even better than them in the cases of low measurement rates.},
  archive      = {J_TMM},
  author       = {Yubao Sun and Ying Yang and Qingshan Liu and Jiwei Chen and Xiao-Tong Yuan and Guodong Guo},
  doi          = {10.1109/TMM.2020.2973862},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3236-3248},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning non-locally regularized compressed sensing network with half-quadratic splitting},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual convolutional LSTM network for referring image
segmentation. <em>TMM</em>, <em>22</em>(12), 3224–3235. (<a
href="https://doi.org/10.1109/TMM.2020.2971171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider referring image segmentation. It is a problem at the intersection of computer vision and natural language understanding. Given an input image and a referring expression in the form of a natural language sentence, the goal is to segment the object of interest in the image referred by the linguistic query. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to tackle this problem. Our model consists of an encoder network and a decoder network, where ConvLSTM is used in both encoder and decoder networks to capture spatial and sequential information. The encoder network extracts visual and linguistic features for each word in the expression sentence, and adopts an attention mechanism to focus on words that are more informative in the multimodal interaction. The decoder network integrates the features generated by the encoder network at multiple levels as its input and produces the final precise segmentation mask. Experimental results on four challenging datasets demonstrate that the proposed network achieves superior segmentation performance compared with other state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Linwei Ye and Zhi Liu and Yang Wang},
  doi          = {10.1109/TMM.2020.2971171},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3224-3235},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual convolutional LSTM network for referring image segmentation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepQoE: A multimodal learning framework for video quality
of experience (QoE) prediction. <em>TMM</em>, <em>22</em>(12),
3210–3223. (<a href="https://doi.org/10.1109/TMM.2020.2973828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many models have been developed to predict video Quality of Experience (QoE), yet the applicability of these models still faces significant challenges. Firstly, many models rely on features that are unique to a specific dataset and thus lack the capability to generalize. Due to the intricate interactions among these features, a unified representation that is independent of datasets with different modalities is needed. Secondly, existing models often lack the configurability to perform both classification and regression tasks. Thirdly, the sample size of the available datasets to develop these models is often very small, and the impact of limited data on the performance of QoE models has not been adequately addressed. To address these issues, in this work we develop a novel and end-to-end framework termed as DeepQoE. The proposed framework first uses a combination of deep learning techniques, such as word embedding and 3D convolutional neural network (C3D), to extract generalized features. Next, these features are combined and fed into a neural network for representation learning. A learned representation will then serve as input for classification or regression tasks. We evaluate the performance of DeepQoE with three datasets. The results show that for small datasets (e.g., WHU-MVQoE2016 and Live-Netflix Video Database), the performance of state-of-the-art machine learning algorithms is greatly improved by using the QoE representation from DeepQoE (e.g., 35.71% to 44.82%); while for the large dataset (e.g., VideoSet), our DeepQoE framework achieves significant performance improvement in comparison to the best baseline method (90.94% vs. 82.84%). In addition to the much improved performance, DeepQoE has the flexibility to fit different datasets, to learn QoE representation, and to perform both classification and regression problems. We also develop a DeepQoE based adaptive bitrate streaming (ABR) system to verify that our framework can be easily applied to multimedia communication service. The software package of the DeepQoE framework has been released to facilitate the current research on QoE.},
  archive      = {J_TMM},
  author       = {Huaizheng Zhang and Linsen Dong and Guanyu Gao and Han Hu and Yonggang Wen and Kyle Guan},
  doi          = {10.1109/TMM.2020.2973828},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3210-3223},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DeepQoE: A multimodal learning framework for video quality of experience (QoE) prediction},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reasoning on the relation: Enhancing visual representation
for visual question answering and cross-modal retrieval. <em>TMM</em>,
<em>22</em>(12), 3196–3209. (<a
href="https://doi.org/10.1109/TMM.2020.2972830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal analysis has become a promising direction for artificial intelligence. Visual representation is crucial for various cross-modal analysis tasks that require visual content understanding. Visual features which contain semantical information can disentangle the underlying correlation between different modalities, thus benefiting the downstream tasks. In this paper, we propose a Visual Reasoning and Attention Network (VRANet) as a plug-and-play module to capture rich visual semantics and help to enhance the visual representation for improving cross-modal analysis. Our proposed VRANet is built based on the bilinear visual attention module which identifies the critical objects. We propose a novel Visual Relational Reasoning (VRR) module to reason about pair-wise and inner-group visual relationships among objects guided by the textual information. The two modules enhance the visual features at both relation level and object level. We demonstrate the effectiveness of the proposed VRANet by applying it to both Visual Question Answering (VQA) and Cross-Modal Information Retrieval (CMIR) tasks. Extensive experiments conducted on VQA 2.0, CLEVR, CMPlaces, and MS-COCO datasets indicate superior performance comparing with state-of-the-art work.},
  archive      = {J_TMM},
  author       = {Jing Yu and Weifeng Zhang and Yuhang Lu and Zengchang Qin and Yue Hu and Jianlong Tan and Qi Wu},
  doi          = {10.1109/TMM.2020.2972830},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3196-3209},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reasoning on the relation: Enhancing visual representation for visual question answering and cross-modal retrieval},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep fusion feature representation learning with hard mining
center-triplet loss for person re-identification. <em>TMM</em>,
<em>22</em>(12), 3180–3195. (<a
href="https://doi.org/10.1109/TMM.2020.2972125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) is a challenging task in the field of computer vision and focuses on matching people across images from different cameras. The extraction of robust feature representations from pedestrian images through CNNs with a single deterministic pooling operation is problematic as the features in real pedestrian images are complex and diverse. To address this problem, we propose a novel center-triplet (CT) model that combines the learning of robust feature representation and the optimization of metric loss function. Firstly, we design a fusion feature learning network (FFLN) with a novel fusion strategy consisting of max pooling and average pooling. Instead of adopting a single deterministic pooling operation, the FFLN combines two pooling operations that can learn high response values, bright features, and low response values, discriminative features simultaneously. Our model obtains more discriminative fusion features by adaptively learning the weights of the features learned by the corresponding pooling operations. In addition, we design a hard mining center-triplet loss (HCTL), a novel improved triplet loss, which effectively optimizes the intra/inter-class distance and reduces the cost of computing and mining hard training samples simultaneously, thereby enhancing the learning of robust feature representation. Finally, we proved our method can learn robust and discriminative feature representations for complex pedestrian images in real scenes. The experimental results also illustrate that our method achieves an 81.8% mAP and a 93.8% rank-1 accuracy on Market1501, a 68.2% mAP and an 83.3% rank-1 accuracy on DukeMTMC-ReID, and a 43.6% mAP and a 74.3% rank-1 accuracy on MSMT17, outperforming most state-of-the-art methods and achieving better performance for person re-identification.},
  archive      = {J_TMM},
  author       = {Cairong Zhao and Xinbi Lv and Zhang Zhang and Wangmeng Zuo and Jun Wu and Duoqian Miao},
  doi          = {10.1109/TMM.2020.2972125},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3180-3195},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep fusion feature representation learning with hard mining center-triplet loss for person re-identification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptation-oriented feature projection for one-shot action
recognition. <em>TMM</em>, <em>22</em>(12), 3166–3179. (<a
href="https://doi.org/10.1109/TMM.2020.2972128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-shot action recognition aims at recognizing actions in unseen classes in cases where only one training video is provided. Compared with one-shot image recognition, one-shot learning on videos is more difficult due to the fact that the temporal dimension of video may lead to greater variation. To handle this variation, it is important to conduct further adaptation in the one-shot training process, despite the scarcity of the training data. While meta-learning is an option for facilitating this adaptation, it cannot be directly applied for two reasons: first, deep networks for action recognition can make current meta-learning methods infeasible to run because of their high computational complexity; second, due to the greater variation in actions, the adapted performance may not be higher than the un-adapted one, making it difficult to train the model by means of meta-learning. To address these problems and facilitate the adaptation, we propose the Adaptation-Oriented Feature (AOF) projection for one-shot action recognition. We first pre-train the base network on seen classes. The output of the network is projected to the adaptation-oriented feature space by fusing the important feature dimensions that are sensitive to adaptation. Subsequently, a small dataset (a.k.a. task) is sampled from seen classes to simulate the unseen-class training and testing settings. The feature adaptation is performed on the training data of this task to integrate the distribution information of the adapted feature. In order to reduce over-fitting, the triplet loss is applied to handle temporal variation with fewer parameters during the adaptation. On the testing data of this task, the losses on both adapted and un-adapted features are calculated to train the projection matrix. This sampling-adaptation-training procedure is then repeated on seen classes until convergence. Extensive experimental results on two challenging one-shot action recognition datasets demonstrate that our proposed method outperforms state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yixiong Zou and Yemin Shi and Daochen Shi and Yaowei Wang and Yongsheng Liang and Yonghong Tian},
  doi          = {10.1109/TMM.2020.2972128},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3166-3179},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptation-oriented feature projection for one-shot action recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepFacade: A deep learning approach to facade parsing with
symmetric loss. <em>TMM</em>, <em>22</em>(12), 3153–3165. (<a
href="https://doi.org/10.1109/TMM.2020.2971431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parsing building facades into procedural grammars plays an important role for 3D building model generation tasks, which have been long desired in computer vision. Deep learning is a promising approach to facade parsing, however, a straightforward solution by directly applying standard deep learning approaches cannot always yield the optimal results. This is primarily due to two reasons: 1) it is nontrivial to train existing semantic segmentation networks for facade parsing, e.g., Fully-Convolutional Neural Networks (FCN) which are usually weak at predicting fine-grained shapes (J. Long et al ., 2015); and 2) building facades are man-made architectures with highly regularized shape priors, and the prior knowledge plays an important role in facade parsing, for which how to integrate the prior knowledge into deep neural networks remains an open problem. In this paper, we present a novel symmetric loss function that can be used in deep neural networks for end-to-end training. This novel loss is based on the assumption that most of windows and doors have a highly symmetric rectangle shape, and it penalizes all window predictions that are non-rectangles. This prior knowledge is smoothly integrated into the end-to-end training process. Quantitative evaluation demonstrates that our method has outperformed previous state-of-art methods significantly on five popular facade parsing datasets. Qualitative results have shown that our method effectively aids deep convolutional neural networks to predict more accurate, visually pleasing, and symmetric shapes. To the best of our knowledge, we are the first to incorporate symmetry constraint into end-to-end training in deep neural networks for facade parsing.},
  archive      = {J_TMM},
  author       = {Hantang Liu and Yinghao Xu and Jialiang Zhang and Jianke Zhu and Yang Li and Steven C. H. Hoi},
  doi          = {10.1109/TMM.2020.2971431},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3153-3165},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DeepFacade: A deep learning approach to facade parsing with symmetric loss},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mobile streaming of live 360-degree videos. <em>TMM</em>,
<em>22</em>(12), 3139–3152. (<a
href="https://doi.org/10.1109/TMM.2020.2973855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live streaming of immersive multimedia content, e.g., 360-degree videos, is getting popular due to the recent availability of commercial devices that support interacting with such content such as smartphones/tablets and head-mounted displays. Streaming live content to mobile users using individual connections (i.e., unicast) consumes substantial network resources and does not scale to large number of users. Multicast, on the other hand, offers a scalable solution but it introduces multiple challenges, including handling user interactivity, ensuring smooth quality, conserving the energy of mobile receivers, and achieving fairness among users. We propose a new solution for the problem of live multicast streaming of 360-degree videos to mobile users, which addresses the aforementioned challenges. The proposed solution, referred to as VRCast, is designed for cellular networks that support multicast, such as LTE. We show through trace-driven simulations that VRCast outperforms the closest algorithms in the literature by wide margins across several performance metrics. For example, compared to the state-of-the-art, VRCast improves the viewport quality by up to 2.5 dB. We have implemented VRCast in an LTE testbed to show its practicality. Our experimental results show that VRCast ensures smooth video quality and saves energy for mobile devices.},
  archive      = {J_TMM},
  author       = {Omar Eltobgy and Omar Arafa and Mohamed Hefeeda},
  doi          = {10.1109/TMM.2020.2973855},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3139-3152},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mobile streaming of live 360-degree videos},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage triplet network training framework for image
retrieval. <em>TMM</em>, <em>22</em>(12), 3128–3138. (<a
href="https://doi.org/10.1109/TMM.2020.2974326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework for instance-level image retrieval. Recent methods focus on fine-tuning the Convolutional Neural Network (CNN) via a Siamese architecture to improve off-the-shelf CNN features. They generally use the ranking loss to train such networks, and do not take full use of supervised information for better network training, especially with more complex neural architectures. To solve this, we propose a two-stage triplet network training framework, which mainly consists of two stages. First, we propose a Double-Loss Regularized Triplet Network (DLRTN), which extends basic triplet network by attaching the classification sub-network, and is trained via simultaneously optimizing two different types of loss functions. Double-loss functions of DLRTN aim at specific retrieval task and can jointly boost the discriminative capability of DLRTN from different aspects via supervised learning. Second, considering feature maps of the last convolution layer extracted from DLRTN and regions detected from the region proposal network as the input, we then introduce the Regional Generalized-Mean Pooling (RGMP) layer for the triplet network, and re-train this network to learn pooling parameters. Through RGMP, we pool feature maps for each region and aggregate features of different regions from each image to Regional Generalized Activations of Convolutions (R-GAC) as final image representation. R-GAC is capable of generalizing existing Regional Maximum Activations of Convolutions (R-MAC) and is thus more robust to scale and translation. We conduct the experiment on six image retrieval datasets including standard benchmarks and recently introduced INSTRE dataset. Extensive experimental results demonstrate the effectiveness of the proposed framework.},
  archive      = {J_TMM},
  author       = {Weiqing Min and Shuhuan Mei and Zhuo Li and Shuqiang Jiang},
  doi          = {10.1109/TMM.2020.2974326},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3128-3138},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A two-stage triplet network training framework for image retrieval},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using cell phone pictures of sheet music to retrieve MIDI
passages. <em>TMM</em>, <em>22</em>(12), 3115–3127. (<a
href="https://doi.org/10.1109/TMM.2020.2973831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a cross-modal retrieval problem in which a user would like to retrieve a passage of music from a MIDI file by taking a cell phone picture of several lines of sheet music. This problem is challenging for two reasons: it has a significant runtime constraint since it is a user-facing application, and there is very little relevant training data containing cell phone images of sheet music. To solve this problem, we introduce a novel feature representation called a bootleg score which encodes the position of noteheads relative to staff lines in sheet music. The MIDI representation can be converted into a bootleg score using deterministic rules of Western musical notation, and the sheet music image can be converted into a bootleg score using classical computer vision techniques for detecting simple geometrical shapes. Once the MIDI and cell phone image have been converted into bootleg scores, we can estimate the alignment using dynamic programming. The most notable characteristic of our system is that it has no trainable weights at all — only a set of about 40 hyperparameters. With a training set of just 400 images, we show that our system generalizes well to a much larger set of 1600 test images from 160 unseen musical scores. Our system achieves a test F measure score of 0.89, has an average runtime of 0.90 seconds, and outperforms baseline systems based on music object detection and sheet–audio alignment. We provide extensive experimental validation and analysis of our system.},
  archive      = {J_TMM},
  author       = {T.J. Tsai and Daniel Yang and Mengyi Shan and Thitaree Tanprasert and Teerapat Jenrungrot},
  doi          = {10.1109/TMM.2020.2973831},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3115-3127},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Using cell phone pictures of sheet music to retrieve MIDI passages},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level correlation adversarial hashing for cross-modal
retrieval. <em>TMM</em>, <em>22</em>(12), 3101–3114. (<a
href="https://doi.org/10.1109/TMM.2020.2969792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing (CMH) has been widely used for similarity search in multimedia retrieval applications, thanks to low storage cost and fast query speed. However, preserving the content similarities in finite-length hash codes between different data modalities is still challenging due to the existing heterogeneity gap. To further address the crucial bottleneck, we propose a Multi-Level Correlation Adversarial Hashing (MLCAH) algorithm to integrate the multi-level correlation information into hash codes. The proposed MLCAH model enjoys several merits. First, to the best of our knowledge, it is the early attempt of leveraging the multi-level correlation information for cross-modal hashing retrieval. Second, we propose global and local semantic alignment mechanisms, which can effectively encode multi-level correlation information, including global information, local information, and label information into hash codes. Third, a label-consistency attention mechanism with adversarial training is designed for exploiting the local cross-modality similarity from multi-modality data. Extensive evaluations on four benchmarks demonstrate that the proposed model brings significant improvements over several state-of-the-art cross-modal hashing methods.},
  archive      = {J_TMM},
  author       = {Xinhong Ma and Tianzhu Zhang and Changsheng Xu},
  doi          = {10.1109/TMM.2020.2969792},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3101-3114},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-level correlation adversarial hashing for cross-modal retrieval},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CI-GNN: Building a category-instance graph for zero-shot
video classification. <em>TMM</em>, <em>22</em>(12), 3088–3100. (<a
href="https://doi.org/10.1109/TMM.2020.2969787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-growing video categories, Zero-Shot Learning (ZSL) in video classification has drawn considerable attention in recent years. To transfer the learned knowledge from seen categories to unseen categories, most existing methods resort to an implicit model that learns a projection between visual features and semantic category-representations. However, such methods ignore the explicit relationships among video instances and categories, which impede the direct information propagation in a Category-Instance graph (CI-graph) consisting of both instances and categories. In fact, exploring the structure of the CI-graph can capture the invariances of the ZSL task with good generality for unseen instances. Inspired by these observations, we propose an end-to-end framework to directly and collectively model the relationships between category-instance, category-category, and instance-instance in the CI-graph. Specifically, to construct node features of this graph, we adopt object semantics as a bridge to generate unified representations for both videos and categories. Motivated by the favorable performance of Graph Neural Networks (GNNs), we design a Category-Instance GNN (CI-GNN) to adaptively model the structure of the CI-graph and propagate information among categories and videos. With the task-driven message passing process, the learned model is able to transfer label information from categories towards unseen videos. Extensive experiments on four video datasets demonstrate the favorable performance of the proposed framework.},
  archive      = {J_TMM},
  author       = {Junyu Gao and Changsheng Xu},
  doi          = {10.1109/TMM.2020.2969787},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3088-3100},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CI-GNN: Building a category-instance graph for zero-shot video classification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring global and local linguistic representations for
text-to-image synthesis. <em>TMM</em>, <em>22</em>(12), 3075–3087. (<a
href="https://doi.org/10.1109/TMM.2020.2972856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of text-to-image synthesis is to generate photographic images conditioned on given textual descriptions. This challenging task has recently attracted considerable attention from the multimedia community due to its potential applications. Most of the up-to-date approaches are built based on generative adversarial network (GAN) models, and they synthesize images conditioned on the global linguistic representation. However, the sparsity of the global representation results in training difficulties on GANs and a shortage of fine-grained information in the generated images. To address this problem, we propose cross-modal global and local linguistic representations-based generative adversarial networks (CGL-GAN) by incorporating the local linguistic representation into the GAN. In our CGL-GAN, we construct a generator to synthesize the target images and a discriminator to judge whether the generated images conform with the text description. In the discriminator, we construct the cross-modal correlation by projecting the image representations at high and low levels onto the global and local linguistic representations, respectively. We design the hinge loss function to train our CGL-GAN model. We evaluate the proposed CGL-GAN on two publicly available datasets, the CUB and the MS-COCO. The extensive experiments demonstrate that incorporating fine-grained local linguistic information with cross-modal correlation can greatly improve the performance of text-to-image synthesis, even when generating high-resolution images.},
  archive      = {J_TMM},
  author       = {Ruifan Li and Ning Wang and Fangxiang Feng and Guangwei Zhang and Xiaojie Wang},
  doi          = {10.1109/TMM.2020.2972856},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3075-3087},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring global and local linguistic representations for text-to-image synthesis},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Illumination-adaptive person re-identification.
<em>TMM</em>, <em>22</em>(12), 3064–3074. (<a
href="https://doi.org/10.1109/TMM.2020.2969782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most person re-identification (ReID) approaches assume that person images are captured under relatively similar illumination conditions. In reality, long-term person retrieval is common, and person images are often captured under different illumination conditions at different times across a day. In this situation, the performances of existing ReID models often degrade dramatically. This paper addresses the ReID problem with illumination variations and names it as Illumination-Adaptive Person Re-identification (IA-ReID). We propose an Illumination-Identity Disentanglement (IID) network to dispel different scales of illuminations away while preserving individuals&#39; identity information. To demonstrate the illumination issue and to evaluate our model, we construct two large-scale simulated datasets with a wide range of illumination variations. Experimental results on the simulated datasets and real-world images demonstrate the effectiveness of the proposed framework.},
  archive      = {J_TMM},
  author       = {Zelong Zeng and Zhixiang Wang and Zheng Wang and Yinqiang Zheng and Yung-Yu Chuang and Shin’ichi Satoh},
  doi          = {10.1109/TMM.2020.2969782},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3064-3074},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Illumination-adaptive person re-identification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new method and benchmark for detecting co-saliency within
a single image. <em>TMM</em>, <em>22</em>(12), 3051–3063. (<a
href="https://doi.org/10.1109/TMM.2020.2972165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, saliency detection in a single image and co-saliency detection in multiple images have drawn extensive research interest in the vision and multimedia communities. In this paper, we investigate a new problem of co-saliency detection within a single image, i.e., detecting within-image co-saliency. By identifying common saliency within an image, e.g., highlighting multiple occurrences of an object class with similar appearance, this work can benefit many important applications, such as the detection of objects of interest, more robust object recognition, reduction of information redundancy, and animation synthesis. We propose a new bottom-up method to address this problem. Specifically, a large number of object proposals are first detected from the image. Then we develop an optimization algorithm to derive a set of proposal groups, each of which contains multiple proposals showing good common saliency in the image. For each proposal group, we calculate a co-saliency map and then use a low-rank based algorithm to fuse the maps calculated from all the proposal groups for the final co-saliency map in the image. In the experiment, we collect a new benchmark dataset of 664 color images (two subsets) for within-image co-saliency detection. Experiment results show that the proposed method can better detect the within-image co-saliency than existing algorithms. The experimental results also show that the proposed method can be applied to detect the repetitive patterns in a single image and detect the co-saliency in multiple images.},
  archive      = {J_TMM},
  author       = {Hongkai Yu and Kang Zheng and Jianwu Fang and Hao Guo and Song Wang},
  doi          = {10.1109/TMM.2020.2972165},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3051-3063},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A new method and benchmark for detecting co-saliency within a single image},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical context features embedding for object
detection. <em>TMM</em>, <em>22</em>(12), 3039–3050. (<a
href="https://doi.org/10.1109/TMM.2020.2971175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixel-level segmentation has been widely used to improve object detection. Most of the existing methods refine detection features by adding the constraint of the segmentation branch or by simply embedding high-level segmentation features into detection features within the local receptive field. However, noisy segmentation features are unavoidable in real-word applications and can easily cause false positives. To address this problem, we propose a novel hierarchical context embedding module to effectively embed segmentation features into detection features. The idea of this module is to capture hierarchical context information that includes local objects or parts and nonlocal context features by learning multiple attention maps, and subsequently utilize interdependencies between features to recalibrate noisy segmentation features. Furthermore, we use this module in the proposed gated encoder-decoder network that adaptively aggregates feature maps of different resolutions based on the gate mechanism so that we can embed multiscale segmentation feature maps into detection features for more accurate detection of objects of all sizes. Experimental results demonstrate the effectiveness of the proposed method on the Pascal VOC 2012Seg dataset, the Pascal VOC dataset and the MS COCO dataset.},
  archive      = {J_TMM},
  author       = {Heqian Qiu and Hongliang Li and Qingbo Wu and Fanman Meng and Linfeng Xu and King Ngi Ngan and Hengcan Shi},
  doi          = {10.1109/TMM.2020.2971175},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3039-3050},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical context features embedding for object detection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-light image enhancement with semi-decoupled
decomposition. <em>TMM</em>, <em>22</em>(12), 3025–3038. (<a
href="https://doi.org/10.1109/TMM.2020.2969790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement is important for high-quality image display and other visual applications. However, it is a challenging task as the enhancement is expected to improve the visibility of an image while keeping its visual naturalness. Retinex-based methods have well been recognized as a representative technique for this task, but they still have the following limitations. First, due to less-effective image decomposition or strong imaging noise, various artifacts can still be brought into enhanced results. Second, although the priori information can be explored to partially solve the first issue, it requires to carefully model the priori by a regularization term and usually makes the optimization process complicated. In this paper, we address these issues by proposing a novel Retinex-based low-light image enhancement method, in which the Retinex image decomposition is achieved in an efficient semi-decoupled way. Specifically, the illumination layer I is gradually estimated only with the input image S based on the proposed Gaussian Total Variation model, while the reflectance layer R is jointly estimated by S and the intermediate I. In addition, the imaging noise can be simultaneously suppressed during the estimation of R. Experimental results on several public datasets demonstrate that our method produces images with both higher visibility and better visual quality, which outperforms the state-of-the-art low-light enhancement methods in terms of several objective and subjective evaluation metrics.},
  archive      = {J_TMM},
  author       = {Shijie Hao and Xu Han and Yanrong Guo and Xin Xu and Meng Wang},
  doi          = {10.1109/TMM.2020.2969790},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  number       = {12},
  pages        = {3025-3038},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Low-light image enhancement with semi-decoupled decomposition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(11), C3. (<a
href="https://doi.org/10.1109/TMM.2020.3030230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.3030230},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D room layout estimation from a single RGB image.
<em>TMM</em>, <em>22</em>(11), 3014–3024. (<a
href="https://doi.org/10.1109/TMM.2020.2967645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D layout is crucial for scene understanding and reconstruction, and very useful in applications like real estate and furniture design. In this paper, we propose a fully automatic solution to estimate 3D layout of an indoor scene from a single 2D image. Our technique contains two key components. Firstly, we train a neural network that directly estimates room structure lines from the input image. Secondly, we propose a novel technique to automatically identify the layout topology of an input image, followed by a nonlinear optimization with equality constraints to estimate the final 3D layout of a scene. Based on our knowledge, this is the first fully automatic technique to achieve single image-based 3D layout estimation of an indoor scene. We evaluate our method on the public datasets LSUN, Hedau and 3DGP and the results show that the proposed method achieves accurate 3D layout reconstruction on various images with different layout topologies.},
  archive      = {J_TMM},
  author       = {Chenggang Yan and Biyao Shao and Hao Zhao and Ruixin Ning and Yongdong Zhang and Feng Xu},
  doi          = {10.1109/TMM.2020.2967645},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {3014-3024},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3D room layout estimation from a single RGB image},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards efficient front-end visual sensing for digital
retina: A model-centric paradigm. <em>TMM</em>, <em>22</em>(11),
3002–3013. (<a href="https://doi.org/10.1109/TMM.2020.2966885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital retina excels at providing enhanced visual sensing and analysis capability for city brain in smart cities, and can feasibly convert the visual data from visual sensors into semantic features. With the deployment of deep learning or handcrafted models, these features are extracted on front-end devices, then delivered to back-end servers for advanced analysis. In this scenario, we propose a model generation, utilization and communication paradigm, aiming at strong front-end sensing capabilities for establishing better artificial visual systems in smart cities. In particular, we propose an integrated multiple deep learning models reuse and prediction strategy, which dramatically increases the feasibility of the digital retina in large-scale visual data analysis in smart cities. The proposed multi-model reuse scheme aims to reuse the knowledge from models cached and transmitted in digital retina to obtain more discriminative capability. To efficiently deliver these newly generated models, a model prediction scheme is further proposed by encoding and reconstructing model differences. Extensive experiments have been conducted to demonstrate the effectiveness of proposed model-centric paradigm.},
  archive      = {J_TMM},
  author       = {Yihang Lou and Ling-Yu Duan and Yong Luo and Ziqian Chen and Tongliang Liu and Shiqi Wang and Wen Gao},
  doi          = {10.1109/TMM.2020.2966885},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {3002-3013},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards efficient front-end visual sensing for digital retina: A model-centric paradigm},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Spatio-temporal attention networks for action recognition
and detection. <em>TMM</em>, <em>22</em>(11), 2990–3001. (<a
href="https://doi.org/10.1109/TMM.2020.2965434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D Convolutional Neural Network (3D CNN) models have been widely studied for video sequences and achieved satisfying performance in action recognition and detection tasks. However, most of the existing 3D CNNs treat all input video frames equally, thus ignoring the spatial and temporal differences across the video frames. To address the problem, we propose a spatio-temporal attention (STA) network that is able to learn the discriminative feature representation for actions, by respectively characterizing the beneficial information at both the frame level and the channel level. By simultaneously exploiting the differences in spatial and temporal dimensions, our STA module enhances the learning capability of the 3D convolutions when handling the complex videos. The proposed STA method can be wrapped as a generic module easily plugged into the state-of-the-art 3D CNN architectures for video action detection and recognition. We extensively evaluate our method on action recognition and detection tasks over three popular datasets (UCF-101, HMDB-51 and THUMOS 2014), and the experimental results demonstrate that adding our STA network module can obtain the state-of-the-art performance on UCF-101 and HMDB-51, which has the top-1 accuracies of 98.4% and 81.4% respectively, and achieve significant improvement on THUMOS 2014 dataset compared against original models.},
  archive      = {J_TMM},
  author       = {Jun Li and Xianglong Liu and Wenxuan Zhang and Mingyuan Zhang and Jingkuan Song and Nicu Sebe},
  doi          = {10.1109/TMM.2020.2965434},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2990-3001},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatio-temporal attention networks for action recognition and detection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cuboid CNN model with an attention mechanism for
skeleton-based action recognition. <em>TMM</em>, <em>22</em>(11),
2977–2989. (<a href="https://doi.org/10.1109/TMM.2019.2962304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of depth sensors such as Microsoft Kinect have driven research in human action recognition. Human skeletal data collected from depth sensors convey a significant amount of information for action recognition. While there has been considerable progress in action recognition, most existing skeleton-based approaches neglect the fact that not all human body parts move during many actions, and they fail to consider the ordinal positions of body joints. Here, and motivated by the fact that an action&#39;s category is determined by local joint movements, we propose a cuboid model for skeleton-based action recognition. Specifically, a cuboid arranging strategy is developed to organize the pairwise displacements between all body joints to obtain a cuboid action representation. Such a representation is well structured and allows deep CNN models to focus analyses on actions. Moreover, an attention mechanism is exploited in the deep model, such that the most relevant features are extracted. Extensive experiments on our new Yunnan University-Chinese Academy of Sciences-Multimodal Human Action Dataset (CAS-YNU MHAD), the NTU RGB+D dataset, the UTD-MHAD dataset, and the UTKinect-Action3D dataset demonstrate the effectiveness of our method compared to the current state-of-the-art.},
  archive      = {J_TMM},
  author       = {Kaijun Zhu and Ruxin Wang and Qingsong Zhao and Jun Cheng and Dapeng Tao},
  doi          = {10.1109/TMM.2019.2962304},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2977-2989},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A cuboid CNN model with an attention mechanism for skeleton-based action recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vabis: Video adaptation bitrate system for time-critical
live streaming. <em>TMM</em>, <em>22</em>(11), 2963–2976. (<a
href="https://doi.org/10.1109/TMM.2019.2962313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of time-critical and interactive scenarios, ultra-low latency has become the most urgent requirement. Adaptive bitrate (ABR) schemes have been widely used in reducing latency for live streaming services. However, the traditional solutions suffer from a key limitation: they only utilize coarse-grained chunk to solve the I-frame misalignment problem in different bitrate switching process at the cost of increasing latency. As a result, existing schemes are difficult to guarantee the timeliness and granularity of control in essence. In this paper, we use a frame-based approach to solve the I-frame misalignment problem and propose a video adaptation bitrate system (Vabis) in units of the frame for time-critical live streaming to obtain the optimal quality of experience (QoE). On the server-side, a Few-Wait ABR algorithm based on Reinforcement Learning (RL) is designed to adaptively select the bitrate of future frames by state information that can be observed, which can subtly solve the problem of I-frame misalignment. A rule-based ABR algorithm is designed to optimize the Vabis system for the weak network. On the client-side, three delay control mechanisms are designed to achieve frame-based fine-grained control. We construct a trace-driven simulator and the real live platform to evaluate the comprehensive live streaming performance. The results show that Vabis is significantly better than the existing methods with decreases in an average delay of 32%-77% and improvements in average QoE of 28-67%.},
  archive      = {J_TMM},
  author       = {Tongtong Feng and Haifeng Sun and Qi Qi and Jingyu Wang and Jianxin Liao},
  doi          = {10.1109/TMM.2019.2962313},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2963-2976},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Vabis: Video adaptation bitrate system for time-critical live streaming},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sketch-based shape retrieval via best view selection and a
cross-domain similarity measure. <em>TMM</em>, <em>22</em>(11),
2950–2962. (<a href="https://doi.org/10.1109/TMM.2020.2966882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieving 3D shapes from 2D human sketches has received increasing attention in computer vision and computer graphics. Most previous methods projected 3D shapes from numerous viewpoints and then extracted features of 3D shapes from these projections and calculated the similarity with sketches. However, due to the unknown pose of 3D shapes, viewpoints were usually sampled uniformly from a sphere coordinate. Hence, some projections acquired insufficient descriptions of 3D shapes. In this paper, we proposed a view selection algorithm to find the most reasonable viewpoints, which can benefit representation learning for 3D shapes. Additionally, to indicate the apparent discrepancy between sketches and 3D shapes, we leveraged a generalized similarity model to encourage the accuracy of cross-domain feature matching. We first computed line renderings of 3D shapes from an enormous number of viewpoints. Then, we calculated the similarity of shapes between line renderings and sketches. In this vein, we obtained several superior projections. Second, we implemented a sketch network to extract features of the sketch and a shape network to extract features of projections. We combined the features of different projections to secure the compact representation for 3D shapes. Finally, a metric network was constructed using a cross-domain similarity model, and we trained the metric network with triplet loss. Online hard sample mining was leveraged to accelerate the convergence of the network. We evaluated our method on SHREC‘13 and SHREC’14 sketch track benchmark datasets. The experimental results demonstrated that both view selection and cross-domain similarity models were able to encourage retrieval performance.},
  archive      = {J_TMM},
  author       = {Yongzhe Xu and Jiangchuan Hu and Kanoksak Wattanachote and Kun Zeng and YongYi Gong},
  doi          = {10.1109/TMM.2020.2966882},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2950-2962},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Sketch-based shape retrieval via best view selection and a cross-domain similarity measure},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning local quality-aware structures of salient regions
for stereoscopic images via deep neural networks. <em>TMM</em>,
<em>22</em>(11), 2938–2949. (<a
href="https://doi.org/10.1109/TMM.2020.2965461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perceptual quality of stereoscopic images plays an essential role in the human perception of visual information. However, most available stereoscopic image quality assessment (SIQA) methods evaluate 3D visual experience using hand-crafted features or shallow architectures, which cannot model the visual properties of stereo images well. In this paper, we use convolutional neural networks (CNNs) to learn deeper local quality-aware structures for stereo images. With different inputs, two CNN models are designed for no-reference SIQA tasks. The one-column CNN model directly accepts a cyclopean view as the input, and the three-column CNN model jointly considers the cyclopean, left and right views as CNN inputs. The two SIQA frameworks share the same implementation approach: First, to overcome the obstacle of limited SIQA datasets, we accept image patches that have been cropped from corresponding stereopairs as inputs for local quality-sensitive feature extraction. Next, a local feature selection algorithm is used to remove related features on non-salient patches, which could cause large prediction errors. Finally, the reserved local visual structures of salient regions are aggregated into a final quality score in an end-to-end manner. Experimental results on three public SIQA databases demonstrate that our method outperforms most state-of-the-art no-reference (NR) SIQA methods. The results of a cross-database experiment also show the robustness and generality of the proposed method.},
  archive      = {J_TMM},
  author       = {Guangming Sun and Bufan Shi and Xiaodong Chen and Andrey S. Krylov and Yong Ding},
  doi          = {10.1109/TMM.2020.2965461},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2938-2949},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning local quality-aware structures of salient regions for stereoscopic images via deep neural networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep manifold-to-manifold transforming network for
skeleton-based action recognition. <em>TMM</em>, <em>22</em>(11),
2926–2937. (<a href="https://doi.org/10.1109/TMM.2020.2966878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we will investigate skeleton-based action recognition by employing high-order statistics feature and first-order statistics feature, where the high-order statistics feature is characterized by symmetric positive definite (SPD) matrices. Noting that SPD matrices are theoretically embedded on Riemannian manifolds, we propose an end-to-end deep manifold-to-manifold transforming network (DMT-Net), which can make SPD matrices flow from one Riemannian manifold to another one for facilitating the action recognition task. To learn discriminative SPD features from both spatial and temporal dependencies, we propose a neural network model with three novel layers on manifolds: i.e., (1) the local SPD convolutional layer, (2) the non-linear SPD activation layer, and (3) the Riemannian-preserved recursive layer. The SPD property is preserved through all layers without the singular value decomposition (SVD) operation, which has to be conducted in the existing methods with expensive computation cost. Furthermore, a diagonalizing SPD layer is designed to efficiently calculate the final metric for the classification task. Finally, DMT-Net is further fused with a first order layer to capture temporal evolution information. To evaluate our proposed method, we conduct extensive experiments on the task of action recognition, where the input signals are represented as SPD matrices. The experimental results demonstrate that the proposed method is competitive over state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Tong Zhang and Wenming Zheng and Zhen Cui and Yuan Zong and Chaolong Li and Xiaoyan Zhou and Jian Yang},
  doi          = {10.1109/TMM.2020.2966878},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2926-2937},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep manifold-to-manifold transforming network for skeleton-based action recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic objectives learning for facial expression
recognition. <em>TMM</em>, <em>22</em>(11), 2914–2925. (<a
href="https://doi.org/10.1109/TMM.2020.2966858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition has been widely used to solve the problems such as lie detection and human-machine interaction. However, due to the difficulties to control the application environments, current methods have the lower recognition accuracy in practice. This paper proposes a new method for facial expression recognition by considering several aspects. First, human beings are easy to recognize some expressions, while difficult to recognize others. Inspired by this intuition, a new loss function is proposed to enlarge the distances between samples from easily confused categories. Second, human learning is divided into many stages, and the learning objective of each stage is different. Thus, dynamic objectives learning is proposed, where each objective at different stage is defined by the corresponding loss function. In order to better realize the above ideas, a new deep neural network for facial expression recognition is proposed, which integrates the covariance pooling layer and residual network units into the deep convolution neural network so as to better perform dynamic objectives learning. The experimental results on the standard databases verify the effectiveness and the superior performance of our methods.},
  archive      = {J_TMM},
  author       = {Guihua Wen and Tianyuan Chang and Huihui Li and Lijun Jiang},
  doi          = {10.1109/TMM.2020.2966858},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2914-2925},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic objectives learning for facial expression recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STNReID: Deep convolutional networks with pairwise spatial
transformer networks for partial person re-identification. <em>TMM</em>,
<em>22</em>(11), 2905–2913. (<a
href="https://doi.org/10.1109/TMM.2020.2965491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial person re-identification (ReID) is a challenging task because only partial information of person images is available for matching target persons. Few studies, especially on deep learning, have focused on matching partial person images with holistic person images. This study presents a novel deep partial ReID framework based on pairwise spatial transformer networks (STNReID), which can be trained on existing holistic person datasets. STNReID includes a spatial transformer network (STN) module and a ReID module. The STN module samples an affined image (a semantically corresponding patch) from the holistic image to match the partial image. The ReID module extracts the features of the holistic, partial, and affined images. Competition (or confrontation) is observed between the STN module and the ReID module, and two-stage training is applied to acquire a strong STNReID for partial ReID. Experimental results show that our STNReID obtains 66.7% and 54.6% rank-1 accuracies on Partial-ReID and Partial-iLIDS datasets, respectively. These values are at par with those obtained with state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Hao Luo and Wei Jiang and Xing Fan and Chi Zhang},
  doi          = {10.1109/TMM.2020.2965491},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2905-2913},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {STNReID: Deep convolutional networks with pairwise spatial transformer networks for partial person re-identification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image compression based on compressive sensing: End-to-end
comparison with JPEG. <em>TMM</em>, <em>22</em>(11), 2889–2904. (<a
href="https://doi.org/10.1109/TMM.2020.2967646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an end-to-end image compression system based on compressive sensing. The presented system integrates the conventional scheme of compressive sampling (on the entire image) and reconstruction with quantization and entropy coding. The compression performance, in terms of decoded image quality versus data rate, is shown to be comparable with JPEG and significantly better at the low rate range. We study the parameters that influence the system performance, including (i) the choice of sensing matrix, (ii) the trade-off between quantization and compression ratio, and (iii) the reconstruction algorithms. We propose an effective method to select, among all possible combinations of quantization step and compression ratio, the ones that yield the near-best quality at any given bit rate. Furthermore, our proposed image compression system can be directly used in the compressive sensing camera, e.g., the single pixel camera, to construct a hardware compressive sampling system.},
  archive      = {J_TMM},
  author       = {Xin Yuan and Raziel Haimi-Cohen},
  doi          = {10.1109/TMM.2020.2967646},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2889-2904},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image compression based on compressive sensing: End-to-end comparison with JPEG},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Jointly sparse locality regression for image feature
extraction. <em>TMM</em>, <em>22</em>(11), 2873–2888. (<a
href="https://doi.org/10.1109/TMM.2019.2961508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel method called Jointly Sparse Locality Regression (JSLR) for feature extraction and selection. JSLR utilizes joint L 2,1 -norm minimization on regularization term, and also introduces the locality to characterize the local geometric structure of the data. There are three main contributions in JSLR for face recognition. Firstly, it eliminates the drawback in ridge regression and Linear Discriminant Analysis (LDA) that when the number of the classes is too small, not enough projections can be obtained for feature extraction. Secondly, by using the local geometric structure as the regularization term, JSLR is able to preserve local information and find an embedding subspace which can detect the most essential data manifold structure. Moreover, since the L2,1-norm based loss function is robust to outliers in data points, JSLR provides the joint sparsity for robust feature selection. The theoretical connections of the proposed method and the previous regression methods are explored and the convergence of the proposed algorithm is also proved. Experimental evaluation on several well-known data sets shows the merits of the proposed method on feature selection and classification.},
  archive      = {J_TMM},
  author       = {Dongmei Mo and Zhihui Lai and Xizhao Wang and Waikeung Wong},
  doi          = {10.1109/TMM.2019.2961508},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2873-2888},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Jointly sparse locality regression for image feature extraction},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tamper-proofing video with hierarchical attention
autoencoder hashing on blockchain. <em>TMM</em>, <em>22</em>(11),
2858–2872. (<a href="https://doi.org/10.1109/TMM.2020.2967640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ARCHANGEL; a novel distributed ledger based system for assuring the long-term integrity of digital video archives. First, we introduce a novel deep network architecture using a hierarchical attention autoencoder (HAAE) to compute temporal content hashes (TCHs) from minutes or hour-long audio-visual streams. Our TCHs are sensitive to accidental or malicious content modification (tampering). The focus of our self-supervised HAAE is to guard against content modification such as frame truncation or corruption but ensure invariance against format shift ( i.e. codec change). This is necessary due to the curatorial requirement for archives to format shift video over time to ensure future accessibility. Second, we describe how the TCHs (and the models used to derive them) are secured via a proof-of-authority blockchain distributed across multiple independent archives. We report on the efficacy of ARCHANGEL within the context of a trial deployment in which the national government archives of the United Kingdom, United States of America, Estonia, Australia and Norway participated.},
  archive      = {J_TMM},
  author       = {Tu Bui and Daniel Cooper and John Collomosse and Mark Bell and Alex Green and John Sheridan and Jez Higgins and Arindra Das and Jared Robert Keller and Olivier Thereaux},
  doi          = {10.1109/TMM.2020.2967640},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2858-2872},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tamper-proofing video with hierarchical attention autoencoder hashing on blockchain},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view multi-label learning with sparse feature
selection for image annotation. <em>TMM</em>, <em>22</em>(11),
2844–2857. (<a href="https://doi.org/10.1109/TMM.2020.2966887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image analysis, image samples are always represented by multiple view features and associated with multiple class labels for better interpretation. However, multiple view data may include noisy, irrelevant and redundant features, while multiple class labels can be noisy and incomplete. Due to the special data characteristic, it is hard to perform feature selection on multi-view multi-label data. To address these challenges, in this paper, we propose a novel multi-view multi-label sparse feature selection (MSFS) method, which exploits both view relations and label correlations to select discriminative features for further learning. Specifically, the multi-labeled information is decomposed into a reduced latent label representation to capture higher level concepts and correlations among multiple labels. Multiple local geometric structures are constructed to exploit visual similarities and relations for different views. By taking full advantage of the latent label representation and multiple local geometric structures, the sparse regression model with an l 2,1 -norm and an Frobenius norm (F-norm) penalty terms is utilized to perform hierarchical feature selection, where the F-norm penalty performs high-level (i.e., view-wise) feature selection to preserve the informative views and the l 2,1 -norm penalty conducts low-level (i.e., row-wise) feature selection to remove noisy features. To solve the proposed formulation, we also devise a simple yet efficient iterative algorithm. Experiments and comparisons on real-world image datasets demonstrate the effectiveness and potential of MSFS.},
  archive      = {J_TMM},
  author       = {Yongshan Zhang and Jia Wu and Zhihua Cai and Philip S. Yu},
  doi          = {10.1109/TMM.2020.2966887},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2844-2857},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-view multi-label learning with sparse feature selection for image annotation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An equalized margin loss for face recognition. <em>TMM</em>,
<em>22</em>(11), 2833–2843. (<a
href="https://doi.org/10.1109/TMM.2020.2966863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new loss function, termed the equalized margin (EqM) loss, which is designed to make both intra-class scopes and inter-class margins similar over all classes, such that all the classes can be evenly distributed on the hypersphere of the feature space. The EqM loss controls both the lower limit of intra-class similarity by exploiting hard-sample mining and the upper limit of inter-class similarity by assuring equalized margins. Therefore, using the EqM loss, we can not only obtain more discriminative features, but also overcome the negative impacts from the data imbalance on the inter-class margins. We also observe that the EqM loss is stable with the variation of the scale in normalized Softmax. Furthermore, by conducting extensive experiments on LFW, YTF, CFP, MegaFace and IJB-B, we are able to verify the effectiveness and superiority of the EqM loss, compared with other state-of-the-art loss functions for face recognition.},
  archive      = {J_TMM},
  author       = {Jingna Sun and Wenming Yang and Jing-Hao Xue and Qingmin Liao},
  doi          = {10.1109/TMM.2020.2966863},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2833-2843},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An equalized margin loss for face recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust visual tracking via constrained multi-kernel
correlation filters. <em>TMM</em>, <em>22</em>(11), 2820–2832. (<a
href="https://doi.org/10.1109/TMM.2020.2965482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative Correlation Filter (DCF) based trackers are quite efficient in tracking objects by exploiting the circulant structure. The kernel trick further improves the performance of such trackers. The unwanted boundary effects, however, are difficult to solve in the kernelized correlation models. In this paper, we propose a novel Constrained Multi-Kernel Correlation tracking Filter (CMKCF), which applies spatial constraints to address this drawback. We build the multi-kernel models for multi-channel features with three different attributes, and then employ a spatial cropping operator on the semi-kernel matrix to address the boundary effects. For the constrained optimization solution, we develop an Alternating Direction Method of Multipliers (ADMM) based algorithm to learn our multi-kernel filters efficiently in the frequency domain. In particular, we suggest an adaptive updating mechanism by exploiting the feedback from high-confidence tracking results to avoid corruption in the model. Extensive experimental results demonstrate that the proposed method performs favorably on OTB-2013, OTB-2015, VOT-2016 and VOT-2018 dataset against several state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Bo Huang and Tingfa Xu and Shenwang Jiang and Yiwen Chen and Yu Bai},
  doi          = {10.1109/TMM.2020.2965482},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2820-2832},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust visual tracking via constrained multi-kernel correlation filters},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning how to smile: Expression video generation with
conditional adversarial recurrent nets. <em>TMM</em>, <em>22</em>(11),
2808–2819. (<a href="https://doi.org/10.1109/TMM.2019.2963621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While several research studies have focused on analyzing human behavior and, in particular, emotional signals from visual data, the problem of synthesizing face video sequences with specific attributes (e.g. age, facial expressions) received much less attention. This paper proposes a novel deep generative model able to produce face videos from a given image of a neutral face and a label indicating a specific facial expression, e.g. spontaneous smile. Our framework consists of two main building blocks: an image generator and a frame sequence generator. The image generator is implemented as a deep neural model which combines generative adversarial networks and variational auto-encoders, while the sequence generator is a label-conditioned recurrent neural network. In the proposed framework, given as input a neural face and a label, the sequence generator outputs a set of hidden representations with smooth transitions corresponding to video frames. Then, the image generator is used to decode the hidden representations into the actual face images. To impose that the net generates videos consistent with the given label, a novel identity adversarial loss is proposed. Our experimental results demonstrate the effectiveness of the framework and the advantage of introducing an adversarial component into recurrent models for face video generation.},
  archive      = {J_TMM},
  author       = {Wei Wang and Xavier Alameda-Pineda and Dan Xu and Elisa Ricci and Nicu Sebe},
  doi          = {10.1109/TMM.2019.2963621},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2808-2819},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning how to smile: Expression video generation with conditional adversarial recurrent nets},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint deep learning of facial expression synthesis and
recognition. <em>TMM</em>, <em>22</em>(11), 2792–2807. (<a
href="https://doi.org/10.1109/TMM.2019.2962317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning based facial expression recognition (FER) methods have attracted considerable attention and they usually require large-scale labelled training data. Nonetheless, the publicly available facial expression databases typically contain a small amount of labelled data. In this paper, to overcome the above issue, we propose a novel joint deep learning of facial expression synthesis and recognition method for effective FER. More specifically, the proposed method involves a two-stage learning procedure. Firstly, a facial expression synthesis generative adversarial network (FESGAN) is pre-trained to generate facial images with different facial expressions. To increase the diversity of the training images, FESGAN is elaborately designed to generate images with new identities from a prior distribution. Secondly, an expression recognition network is jointly learned with the pre-trained FESGAN in a unified framework. In particular, the classification loss computed from the recognition network is used to simultaneously optimize the performance of both the recognition network and the generator of FESGAN. Moreover, in order to alleviate the problem of data bias between the real images and the synthetic images, we propose an intra-class loss with a novel real data-guided back-propagation (RDBP) algorithm to reduce the intra-class variations of images from the same class, which can significantly improve the final performance. Extensive experimental results on public facial expression databases demonstrate the superiority of the proposed method compared with several state-of-the-art FER methods.},
  archive      = {J_TMM},
  author       = {Yan Yan and Ying Huang and Si Chen and Chunhua Shen and Hanzi Wang},
  doi          = {10.1109/TMM.2019.2962317},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2792-2807},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint deep learning of facial expression synthesis and recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind watermarking for 3-d printed objects by locally
modifying layer thickness. <em>TMM</em>, <em>22</em>(11), 2780–2791. (<a
href="https://doi.org/10.1109/TMM.2019.2962306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new blind watermarking algorithm for 3D printed objects that has applications in metadata embedding, robotic grasping, counterfeit prevention, and crime investigation. Our method can be used on fused deposition modeling (FDM) 3D printers and works by modifying the printed layer thickness on small patches of the surface of an object. These patches can be applied to multiple regions of the object, thereby making it resistant to various attacks such as cropping, local deformation, local surface degradation, or printing errors. The novelties of our method are the use of the thickness of printed layers as a one-dimensional carrier signal to embed data, the minimization of distortion by only modifying the layers locally, and one-shot detection using a common paper scanner. To correct encoding or decoding errors, our method combines multiple patches and uses a 2D parity check to estimate the error probability of each bit to obtain a higher correction rate than a naive majority vote. The parity bits included in the patches have a double purpose because, in addition to error detection, they are also used to identify the orientation of the patches. In our experiments, we successfully embedded a watermark into flat surfaces of 3D objects with various filament colors using a standard FDM 3D printer, extracted it using a common 2D paper scanner and evaluated the sensitivity to surface degradation and signal amplitude.},
  archive      = {J_TMM},
  author       = {Arnaud Delmotte and Kenichiro Tanaka and Hiroyuki Kubo and Takuya Funatomi and Yasuhiro Mukaigawa},
  doi          = {10.1109/TMM.2019.2962306},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2780-2791},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind watermarking for 3-D printed objects by locally modifying layer thickness},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced intra prediction for video coding by using multiple
neural networks. <em>TMM</em>, <em>22</em>(11), 2764–2779. (<a
href="https://doi.org/10.1109/TMM.2019.2963620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper enhances the intra prediction by using multiple neural network modes (NM). Each NM serves as an end-to-end mapping from the neighboring reference blocks to the current coding block. For the provided NMs, we present two schemes (appending and substitution) to integrate the NMs with the traditional modes (TM) defined in high efficiency video coding (HEVC). For the appending scheme, each NM is corresponding to a certain range of TMs. The categorization of TMs is based on the expected prediction errors. After determining the relevant TMs for each NM, we present a probability-aware mode signaling scheme. The NMs with higher probabilities to be the best mode are signaled with fewer bits. For the substitution scheme, we propose to replace the highest and lowest probable TMs. New most probable mode (MPM) generation method is also employed when substituting the lowest probable TMs. Experimental results demonstrate that using multiple NMs will improve the coding efficiency apparently compared with the single NM. Specifically, proposed appending scheme with seven NMs can save 2.6%, 3.8%, and 3.1% BD-rate for Y, U, and V components compared with using single NM in the state-of-the-art works.},
  archive      = {J_TMM},
  author       = {Heming Sun and Zhengxue Cheng and Masaru Takeuchi and Jiro Katto},
  doi          = {10.1109/TMM.2019.2963620},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2764-2779},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced intra prediction for video coding by using multiple neural networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partition-aware adaptive switching neural networks for
post-processing in HEVC. <em>TMM</em>, <em>22</em>(11), 2749–2763. (<a
href="https://doi.org/10.1109/TMM.2019.2962310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses neural network based post-processing for the state-of-the-art video coding standard, High Efficiency Video Coding (HEVC). We first propose a partition-aware convolution neural network (CNN) that utilizes the partition information produced by the encoder to assist in the post-processing. In contrast to existing CNN-based approaches, which only take the decoded frame as input, the proposed approach considers the coding unit (CU) size information and combines it with the distorted decoded frame such that the artifacts introduced by HEVC are efficiently reduced. We further introduce an adaptive-switching neural network (ASN) that consists of multiple independent CNNs to adaptively handle the variations in content and distortion within compressed-video frames, providing further reduction in visual artifacts. Additionally, an iterative training procedure is proposed to train these independent CNNs attentively on different local patch-wise classes. Experiments on benchmark sequences demonstrate the effectiveness of our partition-aware and adaptive-switching neural networks.},
  archive      = {J_TMM},
  author       = {Weiyao Lin and Xiaoyi He and Xintong Han and Dong Liu and John See and Junni Zou and Hongkai Xiong and Feng Wu},
  doi          = {10.1109/TMM.2019.2962310},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  number       = {11},
  pages        = {2749-2763},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Partition-aware adaptive switching neural networks for post-processing in HEVC},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(10), C3. (<a
href="https://doi.org/10.1109/TMM.2020.3020758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {&quot;Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.&quot;},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.3020758},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ATMFN: Adaptive-threshold-based multi-model fusion network
for compressed face hallucination. <em>TMM</em>, <em>22</em>(10),
2734–2747. (<a href="https://doi.org/10.1109/TMM.2019.2960586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although tremendous strides have been recently made in face hallucination, exiting methods based on a single deep learning framework can hardly satisfactorily provide fine facial features from tiny faces under complex degradation. This article advocates an adaptive-threshold-based multi-model fusion network (ATMFN) for compressed face hallucination, which unifies different deep learning models to take advantages of their respective learning merits. First of all, we construct CNN-, GAN- and RNN-based underlying super-resolvers to produce candidate SR results. Further, the attention subnetwork is proposed to learn the individual fusion weight matrices capturing the most informative components of the candidate SR faces. Particularly, the hyper-parameters of the fusion matrices and the underlying networks are optimized together in an end-to-end manner to drive them for collaborative learning. Finally, a threshold-based fusion and reconstruction module is employed to exploit the candidates&#39; complementarity and thus generate high-quality face images. Extensive experiments on benchmark face datasets and real-world samples show that our model outperforms the state-of-the-art SR methods in terms of quantitative indicators and visual effects. The code and configurations are released at https://github.com/kuihua/ATMFN.},
  archive      = {J_TMM},
  author       = {Kui Jiang and Zhongyuan Wang and Peng Yi and Guangcheng Wang and Ke Gu and Junjun Jiang},
  doi          = {10.1109/TMM.2019.2960586},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2734-2747},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ATMFN: Adaptive-threshold-based multi-model fusion network for compressed face hallucination},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relation attention for temporal action localization.
<em>TMM</em>, <em>22</em>(10), 2723–2733. (<a
href="https://doi.org/10.1109/TMM.2019.2959977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization aims to accurately localize and recognize all possible action instances from an untrimmed video automatically. Most existing methods perform this task by first generating a set of proposals and then recognizing each independently. However, due to the complex structures and large content variations in action instances, recognizing them individually can be difficult. Fortunately, some proposals often share information regarding one specific action. Such information, which is ignored in existing methods, can be used to boost recognition performance. In this paper, we propose a novel mechanism, called relation attention, to exploit informative relations among proposals based on their appearance or optical flow features. Specifically, we propose a relation attention module to enhance representation power by capturing useful information from other proposals. This module does not change the dimensions of the original input and output and does not rely on any specific proposal generation methods or feature extraction backbone networks. Experimental results show that the proposed relation attention mechanism improves performance significantly on both Thumos14 and ActivityNet1.3 datasets compared to existing architectures. For example, relying on Structured Segment Networks (SSN), the proposed relation attention module helps to increase the mAP from 41.4 to 43.7 on the Thumos14 dataset and outperforms the state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Peihao Chen and Chuang Gan and Guangyao Shen and Wenbing Huang and Runhao Zeng and Mingkui Tan},
  doi          = {10.1109/TMM.2019.2959977},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2723-2733},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Relation attention for temporal action localization},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised video summarization with cycle-consistent
adversarial LSTM networks. <em>TMM</em>, <em>22</em>(10), 2711–2722. (<a
href="https://doi.org/10.1109/TMM.2019.2959451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization is an important technique to browse, manage and retrieve a large amount of videos efficiently. The main objective of video summarization is to minimize the information loss when selecting a subset of video frames from the original video, hence the summary video can faithfully represent the overall story of the original video. Recently developed unsupervised video summarization approaches are free of requiring tedious annotation on important frames to train a video summarization model and thus are practically attractive. However, their performance is still limited due to the difficulty of minimizing information loss between the summary and original videos. In this paper, we address unsupervised video summarization by developing a novel Cycle-consistent Adversarial LSTM architecture to effectively reduce the information loss in the summary video. The proposed model, named Cycle-SUM, consists of a frame selector and a cycle-consistent learning based evaluator. The selector is a bi-directional LSTM network to capture the long-range relationship between video frames. To overcome the difficulty of specifying a suitable information preserving metric between original video and summary video, the evaluator is introduced to “supervise” selector to improve the video summarization quality. Specifically, the evaluator is composed of two generative adversarial networks (GANs), in which the forward GAN component is learned to reconstruct the original video from summary video, while the backward GAN learns to invert the process. We establish the relation between mutual information maximization and such cycle learning procedure and further introduce cycle-consistent loss to regularize the summarization. Extensive experiments on three video summarization benchmark datasets demonstrate a state-of-the-art performance, and show the superiority of the Cycle-SUM model compared with other unsupervised approaches.},
  archive      = {J_TMM},
  author       = {Li Yuan and Francis Eng Hock Tay and Ping Li and Jiashi Feng},
  doi          = {10.1109/TMM.2019.2959451},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2711-2722},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised video summarization with cycle-consistent adversarial LSTM networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble tracking based on diverse collaborative framework
with multi-cue dynamic fusion. <em>TMM</em>, <em>22</em>(10), 2698–2710.
(<a href="https://doi.org/10.1109/TMM.2019.2958759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking with deep neural networks has been verified to arrive at a new level accuracy in many challenging scenarios, but the tracking robustness has been still challenged by model singularity and self-learning loop mechanism. As a promising solution for the limitations, to ensemble diverse tracking strategies into a highly-interactive framework has shown a potential effectiveness in recent studies. In this work, a collaborative tracking framework is proposed by exploiting both discriminative correlation filters and deep classifiers into an ensembling framework. With a multi-cue dynamic fusion scheme performed on all the ensembled members’ outputs, a robust long-term tracking can be achieved by calculating the optimal robustness scores based on a dynamic weighted sum of multi-cue metrics. Meanwhile, the obtained reliable and diverse training samples are also utilized to adaptively update the tracker in each branch with heuristic frequency, which is able to alleviate the training samples’ contamination and model corruption. Experiments on the OTB-2015, Temple color 128, UAV123, VOT2016, and VOT2018 benchmark datasets have shown superior performance in comparison to other state-of-the-art tracking approaches.},
  archive      = {J_TMM},
  author       = {Yamin Han and Peng Zhang and Tao Zhuo and Wei Huang and Yufei Zha and Yanning Zhang},
  doi          = {10.1109/TMM.2019.2958759},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2698-2710},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Ensemble tracking based on diverse collaborative framework with multi-cue dynamic fusion},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Character-oriented video summarization with visual and
textual cues. <em>TMM</em>, <em>22</em>(10), 2684–2697. (<a
href="https://doi.org/10.1109/TMM.2019.2960594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the booming of content “re-creation” in social media platforms, character-orientedvideo summary has become a crucial form of user-generated video content. However, artificial extraction could be time-consuming with high missing rate, while traditional techniques on person search may incur heavy burden of computing resources. At the same time, in social media platforms, videos are usually accompanied with rich textual information, e.g., subtitles or bullet-screen comments which provide the multi-view description of videos. Thus, there exists a potential to leverage textual information to enhance the character-oriented video summarization. To that end, in this paper, we propose a novel framework for jointly modeling visual and textual information. Specifically, we first locate characters indiscriminately through detection methods, and then identify these characters via re-identification to extract potential key-frames, in which appropriate source of textual information will be automatically selected and integrated based on the features of specific frame. Finally, key-frames will be aggregated as the character-oriented summarization. Experiments on real-world data sets validate that our solution outperforms several state-of-the-art baselines on both person search and summarization tasks, which prove the effectiveness of our solution on the character-oriented video summarization problem.},
  archive      = {J_TMM},
  author       = {Peilun Zhou and Tong Xu and Zhizhuo Yin and Dong Liu and Enhong Chen and Guangyi Lv and Changliang Li},
  doi          = {10.1109/TMM.2019.2960594},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2684-2697},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Character-oriented video summarization with visual and textual cues},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical learning based congestion control for real-time
video communication. <em>TMM</em>, <em>22</em>(10), 2672–2683. (<a
href="https://doi.org/10.1109/TMM.2019.2959448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing congestion control is hard to simultaneously achieve low latency, high throughput, good adaptability and fair bandwidth allocation, mainly because of the hardwired control strategy and egocentric convergence objective. To address these issues, we propose an end-to-end statistical learning based congestion control, named Iris. By exploring the underlying principles of self-inflicted delay, we find that RTT variation is linearly related to the difference between sending rate and receiving rate, which inspires us to control video bit rate using a statistical-learning congestion control model. The key idea of Iris is to force all flows to converge to the same queue load and adjust bit rate by the model. All flows keep a small and fixed number of packets queuing in the network, thus the fair bandwidth allocation and low latency are both achieved. Besides, the adjustment step size of sending rate is updated by online learning, to better adapt to dynamically changing networks. We carried out extensive experiments to evaluate the performance of Iris, with the implementations over transport layer and application layer respectively. The testing environment includes emulated network, real-world Internet and commercial cellular networks. Compared against Transmission Control Protocol (TCP) flavors and state-of-the-art protocols, Iris is able to achieve high bandwidth utilization, low latency and good fairness concurrently. Especially for HyperText Transfer Protocol (HTTP) video streaming service, Iris is able to increase the video bitrate up to 25% and Peak Signal to Noise Ratio (PSNR) up to 1 dB.},
  archive      = {J_TMM},
  author       = {Tongyu Dai and Xinggong Zhang and Yihang Zhang and Zongming Guo},
  doi          = {10.1109/TMM.2019.2959448},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2672-2683},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Statistical learning based congestion control for real-time video communication},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Food recommendation: Framework, existing solutions, and
challenges. <em>TMM</em>, <em>22</em>(10), 2659–2671. (<a
href="https://doi.org/10.1109/TMM.2019.2958761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing proportion of the global population is becoming overweight or obese, leading to various diseases (e.g., diabetes, ischemic heart disease and even cancer) due to unhealthy eating patterns, such as increased intake of food with high energy and high fat. Food recommendation is of paramount importance to alleviate this problem. Unfortunately, modern multimedia research has enhanced the performance and experience of multimedia recommendation in many fields such as movies and POI, yet largely lags in the food domain. This article proposes a unified framework for food recommendation, and identifies main issues affecting food recommendation including incorporating various context and domain knowledge, building the personal model, and analyzing unique food characteristics. We then review existing solutions for these issues, and finally elaborate research challenges and future directions in this field. To our knowledge, this is the first survey that targets the study of food recommendation in the multimedia field and offers a collection of research studies and technologies to benefit researchers in this field.},
  archive      = {J_TMM},
  author       = {Weiqing Min and Shuqiang Jiang and Ramesh Jain},
  doi          = {10.1109/TMM.2019.2958761},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2659-2671},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Food recommendation: Framework, existing solutions, and challenges},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Representing modifiable and reusable musical content on the
web with constrained multi-hierarchical structures. <em>TMM</em>,
<em>22</em>(10), 2645–2658. (<a
href="https://doi.org/10.1109/TMM.2019.2961207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most commonly used formats for exchanging musical information today are limited in that they represent music as flat and rigid streams of events or as raw audio signals without any structural information about the content. Such files can only be listened to in a linear way and reused and manipulated in manners determined by a target application such as a Digital Audio Workstation. The publisher has no means to incorporate their intentions or understanding of the content. This article introduces an extension of the music formalism CHARM for the representation of modifiable and reusable musical content on the Web. It discusses how various kinds of multi-hierarchical graph structures together with logical constraints can be useful to model different musical situations. In particular, we focus on presenting solutions on how to interpret, navigate and schedule such structures in order for them to be played back. We evaluate the versatility of the representation in a number of practical examples created with a Web-based implementation based on Semantic Web technologies.},
  archive      = {J_TMM},
  author       = {Florian Thalmann and Geraint A. Wiggins and Mark B. Sandler},
  doi          = {10.1109/TMM.2019.2961207},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2645-2658},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Representing modifiable and reusable musical content on the web with constrained multi-hierarchical structures},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-reference quality evaluation of stereoscopic video based
on spatio-temporal texture. <em>TMM</em>, <em>22</em>(10), 2635–2644.
(<a href="https://doi.org/10.1109/TMM.2019.2961209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the wide application of stereoscopic display technology, stereoscopic video quality assessment (SVQA) is facing great challenges, but worthwhile. Stereoscopic videos contain a great deal of information, which involves not only the spatial domain but also the spatio-temporal domain. Motion in stereoscopic video plays a critical role in quality perception, while the existing SVQA methods rarely refer to motion factors, and the performance of these methods is restrained. In this article, a novel SVQA based on motion perception is introduced and its performance is superior to that of existing excellent methods. Particularly, to appropriately reduce the amount of data processing, we extract the key-frame sequences according to the influence of movement intensity on binocular visual quality perception. The binocular summation and difference operations are implemented on extracted sequences, and then spatial texture and spatio-temporal texture statistic measurement are extracted simultaneously with local binary patterns from three orthogonal planes (LBP-TOP). Experiments are implemented on two publicly available databases and the results demonstrate the effectiveness and robustness of our algorithm for various categories of distortion stereoscopic video pairs.},
  archive      = {J_TMM},
  author       = {Jiachen Yang and Yang Zhao and Bin Jiang and Wen Lu and Xinbo Gao},
  doi          = {10.1109/TMM.2019.2961209},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2635-2644},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {No-reference quality evaluation of stereoscopic video based on spatio-temporal texture},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep multimodality learning for UAV video aesthetic quality
assessment. <em>TMM</em>, <em>22</em>(10), 2623–2634. (<a
href="https://doi.org/10.1109/TMM.2019.2960656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing number of unmanned aerial vehicles (UAVs) and aerial videos, there is a paucity of studies focusing on the aesthetics of aerial videos that can provide valuable information for improving the aesthetic quality of aerial photography. In this article, we present a method of deep multimodality learning for UAV video aesthetic quality assessment. More specifically, a multistream framework is designed to exploit aesthetic attributes from multiple modalities, including spatial appearance, drone camera motion, and scene structure. A novel specially designed motion stream network is proposed for this new multistream framework. We construct a dataset with 6,000 UAV video shots captured by drone cameras. Our model can judge whether a UAV video was shot by professional photographers or amateurs together with the scene type classification. The experimental results reveal that our method outperforms the video classification methods and traditional SVM-based methods for video aesthetics. In addition, we present three application examples of UAV video grading, professional segment detection and aesthetic-based UAV path planning using the proposed method.},
  archive      = {J_TMM},
  author       = {Qi Kuang and Xin Jin and Qinping Zhao and Bin Zhou},
  doi          = {10.1109/TMM.2019.2960656},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2623-2634},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep multimodality learning for UAV video aesthetic quality assessment},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GLNet: Global local network for weakly supervised action
localization. <em>TMM</em>, <em>22</em>(10), 2610–2622. (<a
href="https://doi.org/10.1109/TMM.2019.2959425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the challenging problem of weakly supervised spatio-temporal action localization for which only video-level action labels are available during training. To solve this problem, we propose an end-to-end Global Local Network (GLNet) to predict the probability distribution simultaneously in both spatial and temporal space. The proposed GLNet model includes two key components: a local spatial module and a global temporal module. The local spatial module aims to predict the frame-level spatial distribution by encoding short-term temporal information. In particular, we propose a Region Actionness Network (RAN) to select the target region boxes from the precomputed exhaustive proposals. The global temporal module can predict temporal distribution by a long-term temporal structure modelling. Specifically, we design a temporal fusion-and-excitation architecture on the top of several clips, and trained by a sparse loss function. Therefore, the proposed GLNet model can perform spatio-temporal action localization in an end-to-end manner. We evaluate the performance of GLNet on the J-HMDB and UCF101-24 datasets. The experimental results demonstrate GLNet achieves a significant margin against other state-of-the-art weakly supervised methods and even some fully supervised methods in terms of frame mean Average Precision (mAP) and the video mAP (called frame-mAP and video-mAP, respectively).},
  archive      = {J_TMM},
  author       = {Shiwei Zhang and Lin Song and Changxin Gao and Nong Sang},
  doi          = {10.1109/TMM.2019.2959425},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2610-2622},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GLNet: Global local network for weakly supervised action localization},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A strong baseline and batch normalization neck for deep
person re-identification. <em>TMM</em>, <em>22</em>(10), 2597–2609. (<a
href="https://doi.org/10.1109/TMM.2019.2958756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a simple but strong baseline for deep person re-identification (ReID). Deep person ReID has achieved great progress and high performance in recent years. However, many state-of-the-art methods design complex network structures and concatenate multi-branch features. In the literature, some effective training tricks briefly appear in several papers or source codes. The present study collects and evaluates these effective training tricks in person ReID. By combining these tricks, the model achieves 94.5% rank-1 and 85.9% mean average precision on Market1501 with only using the global features of ResNet50. The performance surpasses all existing global- and part-based baselines in person ReID. We propose a novel neck structure named as batch normalization neck (BNNeck). BNNeck adds a batch normalization layer after global pooling layer to separate metric and classification losses into two different feature spaces because we observe they are inconsistent in one embedding space. Extended experiments show that BNNeck can boost the baseline, and our baseline can improve the performance of existing state-of-the-art methods. Our codes and models are available at: https://github.com/michuanhaohao/reid-strong-baseline},
  archive      = {J_TMM},
  author       = {Hao Luo and Wei Jiang and Youzhi Gu and Fuxu Liu and Xingyu Liao and Shenqi Lai and Jianyang Gu},
  doi          = {10.1109/TMM.2019.2958756},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2597-2609},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A strong baseline and batch normalization neck for deep person re-identification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-grained classification of internet video traffic from
QoS perspective using fractal spectrum. <em>TMM</em>, <em>22</em>(10),
2579–2596. (<a href="https://doi.org/10.1109/TMM.2019.2958764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet video traffic exhibits considerable variation as new video services continue to emerge. Some videos require strict real-time performance, while others may aim for a minimal packet loss rate or sufficient bandwidth. Therefore, it is important to develop fine-grained classification mechanisms to realize effective resource management and quality of service (QoS) provisioning. However, the existing methods for classifying video traffic always suffer from two problems: payload inspection and feature selection. In this paper, we propose a novel method that uses fractal characteristics to achieve traffic classification at a fine-grained level. This method requires neither payload signatures nor statistical features. Through rigorous analysis, we prove the feasibility of employing fractal characteristics for video traffic classification and further develop a theoretical framework for the proposed scheme. For the specific scenario of video flow classification, we improve the theory of fractals in terms of estimated spectrum, core domain, segmentation, and threshold setting. The results of an extensive experimental study on several real-world video traffic datasets show that the classification accuracy of the proposed scheme is higher than that of existing methods.},
  archive      = {J_TMM},
  author       = {Pingping Tang and Yuning Dong and Jiong Jin and Shiwen Mao},
  doi          = {10.1109/TMM.2019.2958764},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2579-2596},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained classification of internet video traffic from QoS perspective using fractal spectrum},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive image sampling using deep learning and its
application on x-ray fluorescence image reconstruction. <em>TMM</em>,
<em>22</em>(10), 2564–2578. (<a
href="https://doi.org/10.1109/TMM.2019.2958760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an adaptive image sampling algorithm based on Deep Learning (DL). It consists of an adaptive sampling mask generation network which is jointly trained with an image inpainting network. The sampling rate is controlled by the mask generation network, and a binarization strategy is investigated to make the sampling mask binary. In addition to the image sampling and reconstruction process, we show how it can be extended and used to speed up raster scanning such as the X-Ray fluorescence (XRF) image scanning process. Recently XRF laboratory-based systems have evolved into lightweight and portable instruments thanks to technological advancements in both X-Ray generation and detection. However, the scanning time of an XRF image is usually long due to the long exposure requirements (e.g., 100 μs - 1 ms per point). We propose an XRF image in painting approach to address the long scanning times, thus speeding up the scanning process, while being able to reconstruct a high quality XRF image. The proposed adaptive image sampling algorithm is applied to the RGB image of the scanning target to generate the sampling mask. The XRF scanner is then driven according to the sampling mask to scan a subset of the total image pixels. Finally, we inpaint the scanned XRF image by fusing the RGB image to reconstruct the full scan XRF image. The experiments show that the proposed adaptive sampling algorithm is able to effectively sample the image and achieve a better reconstruction accuracy than that of existing methods.},
  archive      = {J_TMM},
  author       = {Qiqin Dai and Henry Chopp and Emeline Pouyet and Oliver Cossairt and Marc Walton and Aggelos K. Katsaggelos},
  doi          = {10.1109/TMM.2019.2958760},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2564-2578},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive image sampling using deep learning and its application on X-ray fluorescence image reconstruction},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep gesture video generation with learning on regions of
interest. <em>TMM</em>, <em>22</em>(10), 2551–2563. (<a
href="https://doi.org/10.1109/TMM.2019.2960700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating videos with semantic meaning, such as gestures in sign language, is a challenging problem. The model should not only learn to generate videos with realistic appearance, but also take notice of crucial details in frames to convey precise information. In this paper, we focus on the problem of generating long-term gesture videos containing precise and complete semantic meanings. We develop a novel architecture to learn the temporal and spatial transforms in regions of interest, i.e., gesticulating hands or face in our case. We adopt a hierarchical approach for generating gesture videos, by first making predictions on future pose configurations, and then using the encoder-decoder architecture to synthesize future frames based on the predicted pose structures. We develop the scheme of action progress in our architecture to represent how far the action has been performed during its expected execution, and to instruct our model to synthesize actions with various paces. Our approach is evaluated on two challenging datasets for the task of gesture video generation. Experimental results show that our method can produce gesture videos with more realistic appearance and precise meaning than the state-of-the-art video generation approaches.},
  archive      = {J_TMM},
  author       = {Runpeng Cui and Zhong Cao and Weishen Pan and Changshui Zhang and Jianqiang Wang},
  doi          = {10.1109/TMM.2019.2960700},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2551-2563},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep gesture video generation with learning on regions of interest},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational single image dehazing for enhanced
visualization. <em>TMM</em>, <em>22</em>(10), 2537–2550. (<a
href="https://doi.org/10.1109/TMM.2019.2958755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the challenging task of removing haze from a single natural image. The analysis on the haze formation model shows that the atmospheric veil has much less relevance to chrominance than luminance, which motivates us to neglect the haze in the chrominance channel and concentrate on the luminance channel in the dehazing process. Besides, the experimental study illustrates that the YUV color space is most suitable for image dehazing. Accordingly, a variational model is proposed in the Y channel of the YUV color space by combining the reformulation of the haze model and the two effective priors. As we mainly focus on the Y channel, most of the chrominance information of the image is preserved after dehazing. The numerical procedure based on the alternating direction method of multipliers (ADMM) scheme is presented to obtain the optimal solution. Extensive experimental results on real-world hazy images and synthetic dataset demonstrate clearly that our method can unveil the details and recover vivid color information, which is competitive among many existing dehazing algorithms. Further experiments show that our model also can be applied for image enhancement.},
  archive      = {J_TMM},
  author       = {Faming Fang and Tingting Wang and Yang Wang and Tieyong Zeng and Guixu Zhang},
  doi          = {10.1109/TMM.2019.2958755},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2537-2550},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Variational single image dehazing for enhanced visualization},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Steganographic security analysis from side channel
steganalysis and its complementary attacks. <em>TMM</em>,
<em>22</em>(10), 2526–2536. (<a
href="https://doi.org/10.1109/TMM.2019.2959909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side channel steganalysis refers to detecting a steganographer in social websites via behavior analysis. In this paper, we first design a side channel steganalysis based on the correlation between image sequences of social users, which aims to find out the behaviorally anomalous steganographer. According to the experimental results of side channel steganalysis, it is intuitively secure for the steganographer to act identically to normal social users since she can avoid being detected by side channel steganalysis. However, when faced with various detection methods, is it still secure to behave similar to a normal user? To comprehensively consider the detection means and further explore the secure behavior region of the steganographer, we design a complementary attack of side channel steganalysis. Specifically, we take the correlation of contents of images as side information and take the images with similar content as references to calibrate steganalysis features, which helps improve traditional steganalysis. The proposed side channel steganalysis and its complementary attack efficiently detect steganographers from two different aspects. When the average rank of the steganographer is used to measure the performance, side channel steganalysis can rank the steganographer within the top ten in 100 actors, and the complementary attack can raise the average rank of the steganographer by three places compared with the previous method. From the perspective of the steganographer on social networks, it can help her behave in a more secure region, where her behavior should neither deviate from that of normal users nor be too similar to that of normal users.},
  archive      = {J_TMM},
  author       = {Li Li and Weiming Zhang and Kejiang Chen and Nenghai Yu},
  doi          = {10.1109/TMM.2019.2959909},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2526-2536},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Steganographic security analysis from side channel steganalysis and its complementary attacks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CGR-GAN: CG facial image regeneration for antiforensics
based on generative adversarial network. <em>TMM</em>, <em>22</em>(10),
2511–2525. (<a href="https://doi.org/10.1109/TMM.2019.2959443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a Computer-generated graphics (CG) facial image regeneration scheme for anti-forensics based on generative adversarial network (CGR-GAN) is proposed. The generator of CGR-GAN utilizes a deep U-Net structure, and its discriminator utilizes some stacked convolution layers. Besides, content loss and style loss are both designed to guarantee that the regenerated CG facial images (CGR) retain both the facial profile of the original CG and the characteristics of natural image (NI). Experimental results and analysis demonstrate that the CG facial images regenerated by the proposed anti-forensics scheme can achieve better visual quality compared with those of the existing CG facial image anti-forensics and domain adaptation methods, and it can strike a good balance between visual quality and deception ability.},
  archive      = {J_TMM},
  author       = {Fei Peng and Li-Ping Yin and Le-Bing Zhang and Min Long},
  doi          = {10.1109/TMM.2019.2959443},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2511-2525},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CGR-GAN: CG facial image regeneration for antiforensics based on generative adversarial network},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reference generation with multi-domain hierarchical
constraints for inter prediction. <em>TMM</em>, <em>22</em>(10),
2497–2510. (<a href="https://doi.org/10.1109/TMM.2019.2961504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inter prediction is an important module in video coding for temporal redundancy removal, where similar reference blocks are searched from previously coded frames and employed to predict the block to be coded. Although existing video codecs can estimate and compensate for block-level motions, their inter prediction performance is still heavily affected by the remaining inconsistent pixel-wise displacement caused by irregular rotation and deformation. In this paper, we address the problem by proposing a deep frame interpolation network to generate additional reference frames in coding scenarios. First, we summarize the previous adaptive convolutions used for frame interpolation and propose a factorized kernel convolutional network to improve the modeling capacity and simultaneously keep its compact form. Second, to better train this network, multi-domain hierarchical constraints are introduced to regularize the training of our factorized kernel convolutional network. For spatial domain, we use a gradually down-sampled and up-sampled auto-encoder to generate the factorized kernels for frame interpolation at different scales. For quality domain, considering the inconsistent quality of the input frames, the factorized kernel convolution is modulated with quality-related features to learn to exploit more information from high quality frames. For frequency domain, a sum of absolute transformed difference loss that performs frequency transformation is utilized to facilitate network optimization from the view of coding performance. With the well-designed frame interpolation network regularized by multi-domain hierarchical constraints, our method surpasses HEVC on average 3.8% BD-rate saving for the luma component under the random access configuration and also obtains on average 0.83% BD-rate saving over the upcoming VVC.},
  archive      = {J_TMM},
  author       = {Jiaying Liu and Sifeng Xia and Wenhan Yang},
  doi          = {10.1109/TMM.2019.2961504},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2497-2510},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep reference generation with multi-domain hierarchical constraints for inter prediction},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 2-d skeleton-based action recognition via two-branch stacked
LSTM-RNNs. <em>TMM</em>, <em>22</em>(10), 2481–2496. (<a
href="https://doi.org/10.1109/TMM.2019.2960588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition in video sequences is an interesting field for many computer vision applications, including behavior analysis, event recognition, and video surveillance. In this article, a method based on 2D skeleton and two-branch stacked Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) cells is proposed. Unlike 3D skeletons, usually generated by RGB-D cameras, the 2D skeletons adopted in this article are reconstructed starting from RGB video streams, therefore allowing the use of the proposed approach in both indoor and outdoor environments. Moreover, any case of missing skeletal data is managed by exploiting 3D-Convolutional Neural Networks (3D-CNNs). Comparative experiments with several key works on KTH and Weizmann datasets show that the method described in this paper outperforms the current state-of-the-art. Additional experiments on UCF Sports and IXMAS datasets demonstrate the effectiveness of our method in the presence of noisy data and perspective changes, respectively. Further investigations on UCF Sports, HMDB51, UCF101, and Kinetics400 highlight how the combination between the proposed two-branch stacked LSTM and the 3D-CNN-based network can manage missing skeleton information, greatly improving the overall accuracy. Moreover, additional tests on KTH and UCF Sports datasets also show the robustness of our approach in the presence of partial body occlusions. Finally, comparisons on UT-Kinect and NTU-RGB+D datasets show that the accuracy of the proposed method is fully comparable to that of works based on 3D skeletons.},
  archive      = {J_TMM},
  author       = {Danilo Avola and Marco Cascio and Luigi Cinque and Gian Luca Foresti and Cristiano Massaroni and Emanuele Rodolà},
  doi          = {10.1109/TMM.2019.2960588},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  number       = {10},
  pages        = {2481-2496},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {2-D skeleton-based action recognition via two-branch stacked LSTM-RNNs},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(9), C3. (<a
href="https://doi.org/10.1109/TMM.2020.3015009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.3015009},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Massive-scale genre communities learning using a
noise-tolerant deep architecture. <em>TMM</em>, <em>22</em>(9),
2467–2478. (<a href="https://doi.org/10.1109/TMM.2019.2955240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately categorizing million-scale Internet users (e.g., Flickr or Google Picasa) into multiple communities based on their genre tastes is an indispensable techniquein machine learning and multimedia. It can facilitate a series of applications, such as fashion recommendation and 3D non-realistic photo rendering. Conventional methods cannot handle this task appropriately because of the inherent contaminated image labels, which are produced by auxiliary image label predictors. In this article, we propose a noise-tolerant deep architecture which optimally encodes stable templates, 11 Stable template is our proposed new concept. It denotes the distribution of co-occurring semantic categories toward an image set. Both theoretical and empirical analysises have demonstrated that such distribution (i.e. stable templates) remains almost unchanged in the presence of contaminated image tags. discovered from a collection of images with contaminated semantic labels. Specifically, we first construct a semantic space by encoding image labels using manifold embedding. Afterward, we observe that in the semantic space, the distribution of superpixels from images with the same label remains stable, regardless of the noises from image labels. According to this observation, a probabilistic generative model (Hidden Stable Analysis) is proposed to learn the stable templates toward each image label. To globally represent the composition of a user&#39;s images, a deep aggregation network is developed which statistically concatenates the CNN features learned from all its generated stable templates. Subsequently, an affinity graph is built, in which the genre difference among users is determined by their deep features. Finally, we employ a dense subgraph discovery technique which effectively mines the communities toward various genre tastes. Experiments on a million-scale image set (&gt;1.4 million) compiled from Flickr have demonstrated the effectiveness of our method. Additionally, empirical study on the 33 SIFT-flow categories have shown that the detected stable templates maintain almost unchanged under nearly 32% contaminated image labels.},
  archive      = {J_TMM},
  author       = {Luming Zhang and Xiaoming Ju and Yiyang Yao and Zhenguang Liu},
  doi          = {10.1109/TMM.2019.2955240},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2467-2478},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Massive-scale genre communities learning using a noise-tolerant deep architecture},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Affective video content analysis with adaptive fusion
recurrent network. <em>TMM</em>, <em>22</em>(9), 2454–2466. (<a
href="https://doi.org/10.1109/TMM.2019.2955300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective video content analysis is an important research topic in video content analysis and has extensive applications. Intuitively, multimodal features can depict elicited emotions, and the accumulation of temporal inputs influences the viewer&#39;s emotion. Although a number of research works have been proposed for this task, the adaptive weights of modalities and the correlation of temporal inputs are still not well studied. To address these issues, a novel framework is designed to learn the weights of modalities and temporal inputs from video data. Specifically, three network layers are designed, including statistical-data layer to improve the robustness of data, temporal-adaptive-fusion layer to fuse temporal inputs, and multimodal-adaptive-fusion layer to combine multiple modalities. In particular, the feature vectors of three input modalities are respectively extracted from three pre-trained convolutional neural networks and then fed to three statistical-data layers. Then, the output vectors of these three statistical-data layers are separately connected to three recurrent layers, and the corresponding outputs are fed to a fully-connected layer which shares parameters across modalities and temporal inputs. Finally, the outputs of the fully-connected layer are fused by the temporal-adaptive-fusion layer and then combined by the multimodal-adaptive-fusion layer. To discover the correlation of both multiple modalities and temporal inputs, adaptive weights of modalities and temporal inputs are introduced into loss functions for model training, and these weights are learned by an optimization algorithm. Extensive experiments are conducted on two challenging datasets, which demonstrate that the proposed method achieves better performances than baseline and other state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yun Yi and Hanli Wang and Qinyu Li},
  doi          = {10.1109/TMM.2019.2955300},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2454-2466},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Affective video content analysis with adaptive fusion recurrent network},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging virtual and real person for unsupervised person
re-identification. <em>TMM</em>, <em>22</em>(9), 2444–2453. (<a
href="https://doi.org/10.1109/TMM.2019.2957928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) is a challenging instance retrieval problem, especially when identity annotations are not available for training. Although modern deep re-ID approaches have achieved great improvement, it is still difficult to optimize the deep re-ID model and learn discriminative person representation without annotations in training data. To address this challenge, this study considers the problem of unsupervised person re-ID and introduces a novel approach to solve this problem by leveraging virtual and real data. Our approach includes two components: virtual person generation and training of the deep re-ID model. For virtual person generation, we learn a person generation model and a camera style transfer model using unlabeled real data to generate virtual persons with different poses and camera styles. The virtual data is formed as labeled training data, enabling subsequent training deep re-ID model in supervision. For training of the deep re-ID model, we divide it into three steps: 1) pre-training a coarse re-ID model by using virtual data; 2) collaborative filtering based positive pair mining from the real data; and 3) fine-tuning of the coarse re-ID model by leveraging the mined positive pairs and virtual data. The final re-ID model is achieved by iterating between step 2 and step 3 until convergence. Extensive experiments demonstrate the effectiveness of our method. Experimental results on two large-scale datasets, Market-1501 and DukeMTMC-reID, show the advantages of our method over state-of-the-art approaches in unsupervised person re-ID. Our code is now available online 1 .},
  archive      = {J_TMM},
  author       = {Fengxiang Yang and Zhun Zhong and Zhiming Luo and Sheng Lian and Shaozi Li},
  doi          = {10.1109/TMM.2019.2957928},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2444-2453},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging virtual and real person for unsupervised person re-identification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An attentive sequence to sequence translator for localizing
video clips by natural language. <em>TMM</em>, <em>22</em>(9),
2434–2443. (<a href="https://doi.org/10.1109/TMM.2019.2957854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel attentive sequence to sequence translator (ASST) for localizing video clips by natural language descriptions. We make two contributions. First, we propose an attentive mechanism that aligns natural language descriptions and video content. A bi-directional Recurrent Neural Network (RNN) parses natural language descriptions in two directions. Given a video-description pair, ASST generates a vector sequence representation. Each vector represents a video frame, conditioned by the description. The vector sequence representation not only preserves the temporal dependencies between the frames, but also provides an effective way to perform frame-level video-language matching. The attentive model then aligns words to each frame, thereby resulting in a more detailed understanding of video content and description semantics. Second, we design a hierarchical architecture for the network to jointly model language descriptions and video content. The hierarchical architecture exploits video content with multiple granularities, ranging from subtle details to global context. The integration of the multiple granularities yields a robust representation for multi-level video-language abstraction. We validate the effectiveness of our ASST on two large-scale datasets. Our ASST outperforms the state-of-the-art by $\text{4.28}\%$ in Rank $@1$ on the DiDeMo dataset. On the Charades-STA dataset, we significantly improve the state-of-the-art by $\text{13.41}\%$ in Recall $@\text{1}{,}$ IoU = 0.5.},
  archive      = {J_TMM},
  author       = {Ke Ning and Ming Cai and Di Xie and Fei Wu},
  doi          = {10.1109/TMM.2019.2957854},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2434-2443},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An attentive sequence to sequence translator for localizing video clips by natural language},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted and class-specific maximum mean discrepancy for
unsupervised domain adaptation. <em>TMM</em>, <em>22</em>(9), 2420–2433.
(<a href="https://doi.org/10.1109/TMM.2019.2953375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although maximum mean discrepancy (MMD) has achieved great success in unsupervised domain adaptation (UDA), most of existing UDA methods ignore the issue of class weight bias across domains, which is ubiquitous and evidently gives rise to the degradation of UDA performance. In this work, we propose two improved MMD metrics, i.e., weighted MMD (WMMD) and class-specific MMD (CMMD), to alleviate the adverse effect caused by the changes of class prior distributions between source and target domains. In WMMD, class-specific auxiliary weights are deployed to reweigh the source samples. In CMMD, we calculate the MMD for each class of source and target samples. Since the class labels of target samples are unknown for UDA problem, we present a classification expectation-maximization algorithm to estimate the pseudo-labels of target samples on the fly and update the model parameters using estimated labels. The proposed methods can be flexibly incorporated into deep convolutional neural networks to form WMMD and CMMD based domain adaptation networks, which we called WDAN and CDAN, respectively. By combining WMMD with CMMD, we present a CWMMD based domain adaptation network (CWDAN) to further improve classification performance. Experiments show that, both WMMD and CMMD benefit the classification accuracy, and our CWDAN can achieve compelling UDA performance in comparison with MMD and the state-of-the-art UDA methods.},
  archive      = {J_TMM},
  author       = {Hongliang Yan and Zhetao Li and Qilong Wang and Peihua Li and Yong Xu and Wangmeng Zuo},
  doi          = {10.1109/TMM.2019.2953375},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2420-2433},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weighted and class-specific maximum mean discrepancy for unsupervised domain adaptation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A distance-driven alliance for a P2P live video system.
<em>TMM</em>, <em>22</em>(9), 2409–2419. (<a
href="https://doi.org/10.1109/TMM.2019.2957953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In peer-to-peer (P2P) networks, free-riders and redundant streams including overlapped and folded streams dramatically degrade playback quality and network performance, respectively. Although a locality-aware P2P live video can reduce the topological complexity, it cannot effectively avoid redundant streams while denying free-riders. In this paper, we first model free-rider, redundant streams and a distance-driven P2P system. Based on that model, a distance-driven alliance algorithm is proposed to construct not only an alliance that directly prevents any utility gains of free-riders through inter-user constraints but also a small-world network or a multicast tree that effectively reduces redundant streams. Finally, simulations confirm its advantages in functionality and performance over several existing strategies and distance-driven P2P live video systems.},
  archive      = {J_TMM},
  author       = {Jinyu Zhang and Yifan Zhang and Mengru Shen},
  doi          = {10.1109/TMM.2019.2957953},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2409-2419},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A distance-driven alliance for a P2P live video system},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient NVoD scheme using implicit error correction and
subchannels for wireless networks. <em>TMM</em>, <em>22</em>(9),
2396–2408. (<a href="https://doi.org/10.1109/TMM.2019.2953812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit Error Correction (IEC) is a near Video-on-Demand (nVoD) scheme that trades bandwidth utilization for initial playback delay to potentially support an infinite number of users. Additionally, it provides error protection without any further bandwidth increase by exploiting the implicit redundancy of nVoD protocols, using linear combinations of the segments transmitted in a given time slot. However, IEC packet loss protection is weaker at the beginning of the playback due to the lack of implicit redundancy and lower decoding efficiency, resulting in worse subjective playback quality. In tackling this issue, this paper contributes with an extension of the original nVoD architecture, enhancing its performance by adding a new element namely, subchannels. These subdivisions of the original channels do not provide further packet loss protection but significantly improve the decoding efficiency, which in turn increases playback quality, especially at the beginning. Even for very high packet loss probabilities, subchannels are designed to obtain higher decoding efficiency which results in greater packet loss protection than that provided by IEC. The proposed scheme is especially useful in wireless cooperative networks using techniques such as network coding, as content transmissions can be split into different subchannels in order to maximize network efficiency.},
  archive      = {J_TMM},
  author       = {Rafael Asorey-Cacheda and Antonio-Javier Garcia-Sanchez and Joan Garcia-Haro},
  doi          = {10.1109/TMM.2019.2953812},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2396-2408},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An efficient NVoD scheme using implicit error correction and subchannels for wireless networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tile-based joint caching and delivery of 360° videos in
heterogeneous networks. <em>TMM</em>, <em>22</em>(9), 2382–2395. (<a
href="https://doi.org/10.1109/TMM.2019.2957993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent surge of applications involving the use of 360° video challenges mobile networks infrastructure, as 360° video files are of significant size, and current delivery and edge caching architectures are unable to guarantee their timely delivery. In this paper, we investigate the problem of joint collaborative content-aware caching and delivery of 360° videos in a video on demand setting. The proposed scheme takes advantage of 360° video encoding in multiple tiles and layers to make fine-grained decisions regarding which tiles to cache in each Small Base Station (SBS), and where to deliver them from to the end users, as users may reside in the coverage area of multiple SBSs. This permits to cache the most popular tiles in the SBSs, while the remaining tiles may be obtained through the backhaul. In addition, we explicitly consider the time delivery constraints to ensure continuous video playback. To reduce the computational complexity of the optimization problem, we simplify it by introducing a fairness constraint. This allows us to split the original problem into subproblems corresponding to Groups of Pictures (GOP). Each of the subproblems is then solved with the method of Lagrange partial relaxation. Finally, we evaluate the performance of the proposed method for various system parameters and compare it with schemes that do not consider 360° video encoding into multiple tiles and quality layers, as well as with two variants of the proposed method: one that considers layered encoding and SBSs collaboration and another that uses tiles encoding but with no SBSs collaboration. The results showcase the benefits coming from caching and delivery decisions on per tile basis and the importance of exploiting SBSs collaboration.},
  archive      = {J_TMM},
  author       = {Pantelis Maniotis and Eirina Bourtsoulatze and Nikolaos Thomos},
  doi          = {10.1109/TMM.2019.2957993},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2382-2395},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tile-based joint caching and delivery of 360° videos in heterogeneous networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast FoV-switching DASH system based on tiling mechanism
for practical omnidirectional video services. <em>TMM</em>,
<em>22</em>(9), 2366–2381. (<a
href="https://doi.org/10.1109/TMM.2019.2957976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of multimedia technologies and virtual reality display devices, omnidirectional videos have gained popularity nowadays. To reduce the bandwidth requirement for omnidirectional video transmission, tile-based viewport adaptive streaming methods have been proposed in the literatures. Challenges related to decoding the tiles simultaneously with limited number of decoders, and ensuring user&#39;s viewing experience during the viewport switch are still to be solved. In this paper, a two-layer fast viewport switching dynamic adaptive streaming over HTTP (DASH) system based on tiling mechanism is proposed, which incorporates the viewing trajectory of end users. To deal with the simultaneously decoding problem, an open group of picture (GOP) technique is proposed to enable merging different types of tiles into a composite stream at the client side. To reduce the quality recovery duration after the viewport change, a fast-switching strategy is also proposed. Moreover, considering the priorities of different types of chunks, a download strategy is further proposed to adapt the bandwidth fluctuations and viewport changes. Experimental results showed that the proposed system can significantly reduce the recovery duration of high quality video by approximately 90%, which can provide a better viewing experience to end users.},
  archive      = {J_TMM},
  author       = {Jiarun Song and Fuzheng Yang and Wei Zhang and Wenjie Zou and Yuqun Fan and Peiyun Di},
  doi          = {10.1109/TMM.2019.2957976},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2366-2381},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A fast FoV-switching DASH system based on tiling mechanism for practical omnidirectional video services},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rich features embedding for cross-modal retrieval: A simple
baseline. <em>TMM</em>, <em>22</em>(9), 2354–2365. (<a
href="https://doi.org/10.1109/TMM.2019.2957948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past few years, significant progress has been made on cross-modal retrieval, benefiting from the development of deep neural networks. Meanwhile, the overall frameworks are becoming more and more complex, making the training as well as the analysis more difficult. In this paper, we provide a Rich Features Embedding (RFE) approach to tackle the cross-modal retrieval tasks in a simple yet effective way. RFE proposes to construct rich representations for both images and texts, which is further leveraged to learn the rich features embedding in the common space according to a simple hard triplet loss. Without any bells and whistles in constructing complex components, the proposed RFE is concise and easy to implement. More importantly, our RFE obtains the state-of-the-art results on several popular benchmarks such as MS COCO and Flickr 30 K. In particular, the image-to-text and text-to-image retrieval achieve 76.1% and 61.1% (R@1) on MS COCO, which outperform others more than 3.4% and 2.3%, respectively. We hope our RFE will serve as a solid baseline and help ease future research in cross-modal retrieval.},
  archive      = {J_TMM},
  author       = {Xin Fu and Yao Zhao and Yunchao Wei and Yufeng Zhao and Shikui Wei},
  doi          = {10.1109/TMM.2019.2957948},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2354-2365},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rich features embedding for cross-modal retrieval: A simple baseline},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised cross-modal retrieval with label prediction.
<em>TMM</em>, <em>22</em>(9), 2345–2353. (<a
href="https://doi.org/10.1109/TMM.2019.2954741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval tasks with image-text, audio-image, etc. are gaining increasing importance due to an abundance of data from multiple modalities. In general, supervised approaches give significant improvement over their unsupervised counterparts at the additional cost of labeling or annotation of the training data. Recently, semi-supervised methods are becoming popular as they provide an elegant framework to balance the conflicting requirement of labeling cost and accuracy. In this work, we propose a novel deep semi-supervised framework, which can seamlessly handle both labeled as well as unlabeled data. The network has two important components: (a) first, the labels for the unlabeled portion of the training data are predicted using the label prediction component, and then (b) a common representation for both the modalities is learned for performing cross-modal retrieval. The two parts of the network are trained sequentially one after the other. Extensive experiments on three benchmark datasets, Wiki, Pascal VOC, and NUS-WIDE demonstrate that the proposed framework outperforms the state-of-the-art for both supervised and semi-supervised settings.},
  archive      = {J_TMM},
  author       = {Devraj Mandal and Pramod Rao and Soma Biswas},
  doi          = {10.1109/TMM.2019.2954741},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2345-2353},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semi-supervised cross-modal retrieval with label prediction},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The prediction of saliency map for head and eye movements in
360 degree images. <em>TMM</em>, <em>22</em>(9), 2331–2344. (<a
href="https://doi.org/10.1109/TMM.2019.2957986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By recording the whole scene around the capturer, virtual reality (VR) techniques can provide viewers the sense of presence. To provide a satisfactory quality of experience, there should be at least 60 pixels per degree, so the resolution of panoramas should reach 21600 × 10800. The huge amount of data will put great demands on data processing and transmission. However, when exploring in the virtual environment, viewers only perceive the content in the current field of view (FOV). Therefore if we can predict the head and eye movements which are important behaviors of viewer, more processing resources can be allocated to the active FOV. But conventional saliency prediction methods are not fully adequate for panoramic images. In this paper, a new panorama-oriented model, to predict head and eye movements, is proposed. Due to the superiority of computation in the spherical domain, the spherical harmonics are employed to extract features at different frequency bands and orientations. Related low- and high-level features including the rare components in the frequency domain and color domain, the difference between center vision and peripheral vision, visual equilibrium, person and car detection, and equator bias are extracted to estimate the saliency. To predict head movements, visual mechanisms including visual uncertainty and equilibrium are incorporated, and the graphical model and functional representation for the switch of head orientation are established. Extensive experimental results on the publicly available database demonstrate the effectiveness of our methods.},
  archive      = {J_TMM},
  author       = {Yucheng Zhu and Guangtao Zhai and Xiongkuo Min and Jiantao Zhou},
  doi          = {10.1109/TMM.2019.2957986},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2331-2344},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {The prediction of saliency map for head and eye movements in 360 degree images},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QoE-aware multi-source video streaming in content centric
networks. <em>TMM</em>, <em>22</em>(9), 2321–2330. (<a
href="https://doi.org/10.1109/TMM.2019.2957995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content Centric Networking (CCN), a future Internet architecture, brings new challenges in maintaining the Quality of Experience (QoE) for video streaming. Because of the universal caching capability of CCN routers, streaming from multiple sources will be common, and switching between content sources might affect QoE by inducing delays and consequently stalls in video playback. This paper proposes a new QoE-aware multi-source video streaming scheme for CCN. First, the content distributions of video files among CCN nodes for different caching methods are studied. Second, an adaptive video streaming with distributed caching (ASDC) algorithm is designed to guarantee QoE during the switching between content sources. The ASDC algorithm considers the delivery of scalable video streams. It automatically adapts the layers in a video stream when there is source switching, based on a QoE model that characterizes the effect of stalling. Experimental results show that the ASDC algorithm outperforms dynamic adaptive streaming over HTTP (DASH) in the CCN platform in terms of the QoE obtained from human subjective tests.},
  archive      = {J_TMM},
  author       = {Mohammad Nazmus Sadat and Rui Dai and Lingchao Kong and Jingyi Zhu},
  doi          = {10.1109/TMM.2019.2957995},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2321-2330},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {QoE-aware multi-source video streaming in content centric networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual relationship embedding network for image paragraph
generation. <em>TMM</em>, <em>22</em>(9), 2307–2320. (<a
href="https://doi.org/10.1109/TMM.2019.2954750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image paragraph generation aims to produce a complete description of a given image. This task is more challenging than image captioning, which only generates one sentence to describe the entire image. Traditional paragraph generation methods usually produce paragraph descriptions based on individual regions that are detected by a Region Proposal Network (RPN). However, relationships among visual objects are either ignored or utilized in an implicit manner in previous work. In this paper, we attempt to explore more visual information through a novel paragraph generation network that explicitly incorporates visual relationship semantics when producing descriptions. First, a novel Relation Pair Generative Adversarial Network (RP-GAN) is designed to locate regions that may cover subjective or objective elements. Then, their relationships are inferred through an attention-based network. Finally, the visual features and relationship semantics of valid relation pairs are taken as inputs by a Long Short-Term Memory (LSTM) network for generating sentences. The experimental results show that by explicitly utilizing the predicted relationship information, our proposed method obtains more accurate and informative paragraph descriptions than previous methods.},
  archive      = {J_TMM},
  author       = {Wenbin Che and Xiaopeng Fan and Ruiqin Xiong and Debin Zhao},
  doi          = {10.1109/TMM.2019.2954750},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2307-2320},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual relationship embedding network for image paragraph generation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional networks with channel and STIPs attention
model for action recognition in videos. <em>TMM</em>, <em>22</em>(9),
2293–2306. (<a href="https://doi.org/10.1109/TMM.2019.2953814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the help of convolutional neural networks (CNNs), video-based human action recognition has made significant progress. CNN features that are spatial and channel-wise can provide rich information for powerful image description. However, CNNs lack the ability to process the long-term temporal dependency of an entire video and further cannot well focus on the informative motion regions of actions. Aiming at the two problems, we propose a novel video-based action recognition framework in this paper. We first represent videos with dynamic image sequences (DISs), which effectively describe videos by modeling the local spatial-temporal dynamics and dependencies. Then a channel and spatial-temporal interest points (STIPs) attention model (CSAM) based on CNNs is proposed to focus on the discriminative channels in networks and the informative spatial motion regions of human actions. Specifically, channel attention (CA) is implemented by automatically learning channel-wise convolutional features and assigning different weights for different channels. STIPs attention (SA) is encoded by projecting the detected STIPs on frames of dynamic image sequences into the corresponding convolutional feature map space. The proposed CSAM is embedded after CNN convolutional layers to refine the feature maps, followed by global average pooling to produce effective feature representations for videos. Finally frame-level video representations are fed into an LSTM to capture the temporal dependencies and make classification. Experiments on three challenging RGB-D datasets show that our method has better performance and outperforms the state-of-the-art approaches with only depth data.},
  archive      = {J_TMM},
  author       = {Hanbo Wu and Xin Ma and Yibin Li},
  doi          = {10.1109/TMM.2019.2953814},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2293-2306},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Convolutional networks with channel and STIPs attention model for action recognition in videos},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified deep metric representation for mesh saliency
detection and non-rigid shape matching. <em>TMM</em>, <em>22</em>(9),
2278–2292. (<a href="https://doi.org/10.1109/TMM.2019.2952983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a deep metric for unifying the representation of mesh saliency detection and non-rigid shape matching. While saliency detection and shape matching are two closely related and fundamental tasks in shape analysis, previous methods approach them separately and independently, failing to exploit their mutually beneficial underlying relationship. In view of the existing gap between saliency and matching, we propose to solve them together using a unified metric representation of surface meshes. We show that saliency and matching can be rigorously derived from our representation as the principal eigenvector and the smoothed Laplacian eigenvectors respectively. Learning the representation jointly allows matching to improve the deformation-invariance of saliency while allowing saliency to improve the feature localization of matching. To parameterize the representation from a mesh, we also propose a deep recurrent neural network (RNN) for effectively integrating multi-scale shape features and a soft-thresholding operator for adaptively enhancing the sparsity of saliency. Results show that by jointly learning from a pair of saliency and matching datasets, matching improves the accuracy of detected salient regions on meshes, which is especially obvious for small-scale saliency datasets, such as those having one to two meshes. At the same time, saliency improves the accuracy of shape matchings among meshes with reduced matching errors on surfaces.},
  archive      = {J_TMM},
  author       = {Shanfeng Hu and Hubert P. H. Shum and Nauman Aslam and Frederick W. B. Li and Xiaohui Liang},
  doi          = {10.1109/TMM.2019.2952983},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2278-2292},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A unified deep metric representation for mesh saliency detection and non-rigid shape matching},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer function-guided saliency-aware compression for
transmitting volumetric data. <em>TMM</em>, <em>22</em>(9), 2262–2277.
(<a href="https://doi.org/10.1109/TMM.2017.2757759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a transfer-function-guided three-dimensional (3-D) block-based saliency-aware compression scheme for volumetric data that is both content and spatially scalable. Salient 3-D volumetric blocks are identified and weighted with the help of a transfer function which is used to render the data. We describe our method in the form of a framework for processing, progressive transmission, and visualization of volumetric data on a target device, such as a mobile device with limited computational resources. In particular, we address the transmission bottleneck incurred when transferring 3-D volumetric data. Identified salient regions are progressively transmitted to the target device. The received data are rendered progressively in the respective order with a predefined or user-defined transfer function. Our method is developed with medical applications in mind, where preservation of all information is essential for clinical diagnosis. Because our method is integrated into a resolution scalable coding scheme with an integer wavelet transform of the image, it allows the rendering of each significant region at a different resolution up to fully lossless reconstruction. We perform a thorough qualitative and quantitative evaluation of the saliency detection method and the resulting saliency-aware compression schemes. Our results show reduced error in representation of the volumetric data with our method.},
  archive      = {J_TMM},
  author       = {Ji Hwan Park and Ievgeniia Gutenko and Arie E. Kaufman},
  doi          = {10.1109/TMM.2017.2757759},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2262-2277},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transfer function-guided saliency-aware compression for transmitting volumetric data},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guide to match: Multi-layer feature matching with a hybrid
gaussian mixture model. <em>TMM</em>, <em>22</em>(9), 2246–2261. (<a
href="https://doi.org/10.1109/TMM.2019.2957984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental yet challenging task in computer vision, finding correspondences between two sets of feature points has received extensive attention. Among all the proposed methods, the Gaussian Mixture Model (GMM) based algorithms show their great power in formulating such problems. However, they are vulnerable to large portion of outliers in the extracted feature points. In this paper, a new Hybrid Gaussian Mixture Model (HGMM) combined with a multi-layer matching framework is proposed. Different from existing GMM based methods, HGMM uses a set of seed correspondences to guide the matching procedure. To automatically find seed correspondences, the feature points are divided into multiple layers according to their matching potential. With the help of Locality Sensitive Hashing, this can be done economically and efficiently. Correspondences found in lower layers which contain few outliers will be used as hard constraint when matching features in higher layers where a large portion of outliers exist. Extensive experiments show that the proposed method is efficient and more robust to outliers when images have large viewpoint difference or small scene overlap.},
  archive      = {J_TMM},
  author       = {Kun Sun and Wenbing Tao and Yuhua Qian},
  doi          = {10.1109/TMM.2019.2957984},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2246-2261},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guide to match: Multi-layer feature matching with a hybrid gaussian mixture model},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning discriminative and generative shape embeddings for
three-dimensional shape retrieval. <em>TMM</em>, <em>22</em>(9),
2234–2245. (<a href="https://doi.org/10.1109/TMM.2019.2957933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important solution for 3D shape retrieval, a multi-view shape descriptor has achieved impressive performance. One crucial part of view-based shape descriptors is to interpret 3D structures through various 2D observations. Most existing methods like MVCNN believe that a strong classification model trained with deep learning, can often provide an efficient shape embedding for 3D shape retrieval. However, these methods pay much attention to discriminative models and none of them necessarily incorporate the underlying 3D properties of the objects from 2D images. In this paper, we present a novel encoder-decoder recurrent feature aggregation network (ERFA-Net) to address this problem. Aiming at emphasizing the 3D properties of 3D shapes in the fusion of multiple view features, 3D properties prediction tasks are introduced into the 3D shape retrieval. Specifically, an image sequence of the shape is recurrently aggregated into a discriminative shape embedding based on LSTM network, and then this latent shape embedding is trained to predict the original voxel grids and estimate images of unseen viewpoints. This generation task gives an effective supervision which makes the network exploit 3D properties of shapes through various 2D images. Our method achieves the state-of-the-art performance for 3D shape retrieval, on two large-scale 3D shape datasets, ModelNet and ShapeNetCore55. Extensive experiments show that the proposed 3D representation performs robust discrimination against view occlusion, and strong generation ability for various 3D shape tasks.},
  archive      = {J_TMM},
  author       = {Cheng Xu and Biao Leng and Bo Chen and Cheng Zhang and Xiaochen Zhou},
  doi          = {10.1109/TMM.2019.2957933},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2234-2245},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning discriminative and generative shape embeddings for three-dimensional shape retrieval},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Moving cast shadows segmentation using illumination
invariant feature. <em>TMM</em>, <em>22</em>(9), 2221–2233. (<a
href="https://doi.org/10.1109/TMM.2019.2954752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an effective framework for removing moving cast shadows. Taking the reflection property of object surface for shadow regions under static and fixed scenes, an approximation estimation strategy of bidirectional reflectance distribution function as illumination invariant feature is proposed. It is valid for different types of shadow scenes. In this paper, we propose a new multiple ratios-based technique to justify shadow type for each frame: intensity ratio, area ratio and edge ratio of shadow regions are introduced. According to shadow types, several specified strategies are designed. For weak shadows, multiple features fusion strategy is employed, including color constancy, texture consistency and illumination invariant. For strong shadows, illumination invariant is utilized to detect the umbra and color constancy is utilized to detect the penumbra. Moreover, a suite of shadow direction features is firstly proposed to identify penumbra. The proposed approach is verified in fourteen video sequences varying from weak to strong shadows. The experimental results demonstrate the effectiveness and robustness of the proposed method for both indoor and outdoor scenes compared with some state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Bingshu Wang and Yong Zhao and C. L. Philip Chen},
  doi          = {10.1109/TMM.2019.2954752},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2221-2233},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Moving cast shadows segmentation using illumination invariant feature},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint learning in the spatio-temporal and frequency domains
for skeleton-based action recognition. <em>TMM</em>, <em>22</em>(9),
2207–2220. (<a href="https://doi.org/10.1109/TMM.2019.2953325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from its succinctness and robustness, skeleton-based action recognition has recently attracted much attention. Most existing methods utilize local networks (e.g. recurrent network, convolutional network, and graph convolutional network) to extract spatio-temporal dynamics hierarchically. As a consequence, the local and non-local dependencies, which contain more details and semantics respectively, are asynchronously captured in different level of layers. Moreover, existing methods are limited to the spatio-temporal domain and ignore information in the frequency domain. To better extract synchronous detailed and semantic information from multi-domains, we propose a residual frequency attention (rFA) block to focus on discriminative patterns in the frequency domain, and a synchronous local and non-local (SLnL) block to simultaneously capture the details and semantics in the spatio-temporal domain. In addition, to optimize the whole learning processes of the multi-branch network, we put it under a pseudo multi-task learning paradigm. During training, 1) a soft-margin focal loss (SMFL) is proposed to optimize the intra-branch separated learning process, which can automatically conduct data selection and encourage intrinsic margins in classifiers; 2) A mutual learning policy is also proposed to further facilitate the inter-branch collaborative learning process. Eventually, our approach achieves the state-of-the-art performance on several large-scale datasets for skeleton-based action recognition.},
  archive      = {J_TMM},
  author       = {Guyue Hu and Bo Cui and Shan Yu},
  doi          = {10.1109/TMM.2019.2953325},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2207-2220},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint learning in the spatio-temporal and frequency domains for skeleton-based action recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intra coding strategy for video error resiliency: Behavioral
analysis. <em>TMM</em>, <em>22</em>(9), 2193–2206. (<a
href="https://doi.org/10.1109/TMM.2019.2957991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One challenge in video transmission is to deal with packet loss. Since the compressed video streams are sensitive to data loss, the error resiliency of the encoded video becomes important. When video data is lost and retransmission is not possible, the missed data should be concealed. But loss concealment causes distortion in the lossy frame which also propagates into the next frames even if their data are received correctly. One promising solution to mitigate this error propagation is intra coding. There are three approaches for intra coding: intra coding of a number of blocks selected randomly or regularly, intra coding of some specific blocks selected by an appropriate cost function, or intra coding of a whole frame. But Intra coding reduces the compression ratio; therefore, there exists a trade-off between bitrate and error resiliency achieved by intra coding. In this paper, we study and show the best strategy for getting the best rate-distortion performance. Considering the error propagation, an objective function is formulated, and with some approximations, this objective function is simplified and solved. The solution demonstrates that periodical I-frame coding is preferred over coding only a number of blocks as intra mode in P-frames. Through examination of various test sequences, it is shown that the best intra frame period depends on the coding bitrate as well as the packet loss rate. We then propose a scheme to estimate this period from curve fitting of the experimental results, and show that our proposed scheme outperforms other methods of intra coding especially for higher loss rates and coding bitrates.},
  archive      = {J_TMM},
  author       = {Mohammad Kazemi and Mohammad Ghanbari and Shervin Shirmohammadi},
  doi          = {10.1109/TMM.2019.2957991},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  number       = {9},
  pages        = {2193-2206},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Intra coding strategy for video error resiliency: Behavioral analysis},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(8), C3. (<a
href="https://doi.org/10.1109/TMM.2020.3008958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.3008958},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An unsupervised real-time framework of human pose tracking
from range image sequences. <em>TMM</em>, <em>22</em>(8), 2177–2190. (<a
href="https://doi.org/10.1109/TMM.2019.2953380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose tracking from range image sequences remains a difficult task due to strong noise and serious self-occlusion of human body. Existing work either rely on extremely large and precisely annotated datasets, or rely on accurate human mesh model and GPU acceleration. In this paper, we propose an unsupervised real-time framework of pose tracking from range image sequences. Our framework consists of a visible hybrid model (VHM), a componentwise correspondence optimization (CCO) and a dynamic database lookup (DDL). VHM consists of component sphere sets and component visible spherical point sets which exhibits both simplicity and high accuracy. CCO converts the matching between VHM and input point cloud into several subproblems regarding local rotations of components and a global translation of body abdominal joint, each of which has an efficient closed form solution. DDL is designed to recover correct pose when tracking fails, which effectively mitigates accumulative error during tracking. Experiments on SMMC, PDT, EVAL datasets indicate that our framework not only achieves better or competitive precision compared with state-of-the-art methods, but also produces real-time efficiency in personal computers without GPU acceleration.},
  archive      = {J_TMM},
  author       = {Yongpeng Wu and Dehui Kong and Shaofan Wang and Jinghua Li and Baocai Yin},
  doi          = {10.1109/TMM.2019.2953380},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2177-2190},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An unsupervised real-time framework of human pose tracking from range image sequences},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dilated inception network for visual saliency prediction.
<em>TMM</em>, <em>22</em>(8), 2163–2176. (<a
href="https://doi.org/10.1109/TMM.2019.2947352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the advent of deep convolutional neural networks (DCNN), the improvements in visual saliency prediction research are impressive. One possible direction to approach the next improvement is to fully characterize the multi-scale saliency-influential factors with a computationally-friendly module in DCNN architectures. In this work, we propose an end-to-end dilated inception network (DINet) for visual saliency prediction. It captures multi-scale contextual features effectively with very limited extra parameters. Instead of utilizing parallel standard convolutions with different kernel sizes as the existing inception module, our proposed dilated inception module (DIM) uses parallel dilated convolutions with different dilation rates which can significantly reduce the computation load while enriching the diversity of receptive fields in feature maps. Moreover, the performance of our saliency model is further improved by using a set of linear normalization-based probability distribution distance metrics as loss functions. As such, we can formulate saliency prediction as a global probability distribution prediction task for better saliency inference instead of a pixel-wise regression problem. Experimental results on several challenging saliency benchmark datasets demonstrate that our DINet with proposed loss functions can achieve state-of-the-art performance with shorter inference time.},
  archive      = {J_TMM},
  author       = {Sheng Yang and Guosheng Lin and Qiuping Jiang and Weisi Lin},
  doi          = {10.1109/TMM.2019.2947352},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2163-2176},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A dilated inception network for visual saliency prediction},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Show, tell, and polish: Ruminant decoding for image
captioning. <em>TMM</em>, <em>22</em>(8), 2149–2162. (<a
href="https://doi.org/10.1109/TMM.2019.2951226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The encoder-decoder framework has been the base of popular image captioning models, which typically predicts the target sentence based on the encoded source image one word at a time in sequence. However, such a single-pass decoding framework encounters two problems. First, mistakes in the predicted words cannot be corrected and may propagate to the entire sentence. Second, because the single-pass decoder cannot access the following un-generated words, it can only perform local planning to choose every single word according to the preceding words, while lacks the global planning ability as for maintaining the semantic consistency and fluency of the whole sentence. In order to address the above two problems, in this work, we design a ruminant captioning framework which contains an image encoder, a base decoder, and a ruminant decoder. Specifically, the outputs of the former/base decoder are utilized as the global information to guide the words prediction of the latter/ruminant decoder, in an attempt to mimic human polishing process. We enable jointly training of the whole framework and overcome the non-differential problem of discrete words by designing a novel reinforcement learning based optimization algorithm. Experiments on two datasets (MS COCO and Flickr30 k) demonstrate that our ruminant decoding method can bring significant improvements over traditional single-pass decoding based models and achieves state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Longteng Guo and Jing Liu and Shichen Lu and Hanqing Lu},
  doi          = {10.1109/TMM.2019.2951226},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2149-2162},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Show, tell, and polish: Ruminant decoding for image captioning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning normal patterns via adversarial attention-based
autoencoder for abnormal event detection in videos. <em>TMM</em>,
<em>22</em>(8), 2138–2148. (<a
href="https://doi.org/10.1109/TMM.2019.2950530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically detecting anomalies in videos is a challenging problem due to non-deterministic definitions of abnormal events and lack of sufficient training data. To address these issues, we propose an autoencoder coupled with attention model to discover normal patterns in videos via adversarial learning. Abnormal events are detected by diverging them from the normal patterns with the reconstruction error produced by the autoencoder. To this end, we build an end-to-end trainable adversarial attention-based autoencoder network, called Ada-Net, to make the reconstructed frames indistinguishable from original frames. The Ada-Net combines an autoencoder network and a GAN model that is used to benefit enhancing the reconstruction ability of the autoencoder. To further improve the reconstruction performance, we integrate an attention model into the decoder to dynamically select informative parts of encoding features for decoding. The attenion mechanism is helpful to preserving important information for learning intrinsic normal patterns. Evaluations on four challenging datasets, including the Subway, the UCSD Pedestrian, the CUHK Avenue, and the ShanghaiTech datasets, demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Hao Song and Che Sun and Xinxiao Wu and Mei Chen and Yunde Jia},
  doi          = {10.1109/TMM.2019.2950530},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2138-2148},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning normal patterns via adversarial attention-based autoencoder for abnormal event detection in videos},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pruning 3D filters for accelerating 3D ConvNets.
<em>TMM</em>, <em>22</em>(8), 2126–2137. (<a
href="https://doi.org/10.1109/TMM.2019.2950523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many methods have been proposed to accelerate 2D ConvNets by removing redundant parameters. However, few efforts are devoted to the problem of accelerating 3D Convolutional Networks. The 3D ConvNets, which are mainly designed for extracting spatiotemporal features, have been widely used in many video analytics tasks, such as action recognition and scene analysis. In this paper, we focus on accelerating 3D ConvNets for two motivations: (1) Fast video processing techniques are in dire need due to the explosive growth of video data; (2) Compared with individual images, video data consist of consecutively similar frames, thus are inherently more redundant. In this paper, we present a novel algorithm to dramatically accelerate 3D ConvNets by pruning redundant convolutional filters, while preserving the discriminative power of the networks. Specifically, we formulate the filter pruning from 3D ConvNets as a subset selection problem where each filter is regarded as a candidate. Determinantal Point Processes (DPPs) are employed to discriminatively select the filter candidates which are informative and yet diverse. We evaluate our method using two popular 3D networks, C3D and Pseudo-3D, on Sports-1 M dataset for video classification. Extensive experimental results demonstrate both the efficiency and performance advantages of our method. We also show that the proposed method can be easily generalized to 2D ConvNets pruning with promising experimental results on VGGnet and ResNet.},
  archive      = {J_TMM},
  author       = {Zhenzhen Wang and Weixiang Hong and Yap-Peng Tan and Junsong Yuan},
  doi          = {10.1109/TMM.2019.2950523},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2126-2137},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Pruning 3D filters for accelerating 3D ConvNets},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning-based user clustering and link allocation for
content recommendation based on D2D multicast communications.
<em>TMM</em>, <em>22</em>(8), 2111–2125. (<a
href="https://doi.org/10.1109/TMM.2019.2949434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content recommendation based on device-to-device (D2D) multicast communications is expected to become a promising approach to improve local area services. Importantly, two main challenges should be considered: i) user clustering-in order to be tailored to recommend contents, the members in the same cluster should have great similarity in multiple characteristics, and ii) link allocation-we should use as little resource consumption and information exchange as possible while keeping the recommendation accuracy. In this paper, we firstly quantify the degree of the similarity between two target users with regard to multiple characteristics. Guided by such similarity, we define a clustering validity index in terms of between-within proportion (BWP) to characterize the clustering performance. Then, the issue of user clustering is modeled as a sum BWP maximum problem, and a user clustering algorithm based on modified K-means algorithm is designed to solve it in a fast-operating and low-complexity way. After user clustering, we model the issue of link allocation as a weighted aggregate interference minimization problem, and then transform it to an exact potential game. As such, a link allocation algorithm based on stochastic learning algorithm is proposed which helps to obtain the result of this game in a distributed way without complete information. Also, we analyze its convergence and optimality performance. Simulation results demonstrate the effectiveness of our proposed algorithms.},
  archive      = {J_TMM},
  author       = {Lianxin Yang and Dan Wu and Yueming Cai and Xin Shi and Yan Wu},
  doi          = {10.1109/TMM.2019.2949434},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2111-2125},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning-based user clustering and link allocation for content recommendation based on D2D multicast communications},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge-based topic model for multi-modal social event
analysis. <em>TMM</em>, <em>22</em>(8), 2098–2110. (<a
href="https://doi.org/10.1109/TMM.2019.2951194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the accumulation of data on the Internet and progress in representation learning techniques, knowledge priors learned from a large-scale knowledge base has been increasingly used in probabilistic topic models. However, it is challenging to learn interpretable topics and a discriminative event representation based on multi-modal information. To address these issues, we propose a knowledge priors- and max-margin-based topic model for multi-modal social event analysis, called the KGE-MMSLDA, in which feature representation and knowledge priors are jointly learned. Our model has three main advantages over current methods: (1) It integrates additional knowledge from external knowledge base into a unified topic model in which the max-margin classifier, and multi-modal information are exploited to increase the number of event descriptions obtained. (2) We mined knowledge priors from over 74,000 web documents. Multi-modal data with these knowledge priors are then incorporated into the topic model to increase the number of coherent topics learned. (3) A large-scale multi-modal dataset (containing 10 events, where each event contained approximately 7,000 Flickr pages) was collected and has been released publicly for event topic mining and classification research. In comparative experiments, the proposed method outperformed state-of-the-art models on topic coherence, and obtained a classification accuracy of 85.1%.},
  archive      = {J_TMM},
  author       = {Feng Xue and Richang Hong and Xiangnan He and Jianwei Wang and Shengsheng Qian and Changsheng Xu},
  doi          = {10.1109/TMM.2019.2951194},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2098-2110},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-based topic model for multi-modal social event analysis},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual font pairing. <em>TMM</em>, <em>22</em>(8),
2086–2097. (<a href="https://doi.org/10.1109/TMM.2019.2952266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the problem of automatic font pairing. Font pairing is an important design task that is difficult for novices. Given a font selection for one part of a document (e.g., header), our goal is to recommend a font to be used in another part (e.g., body) such that the two fonts used together look visually pleasing. There are three main challenges in font pairing. First, this is a fine-grained problem, in which the subtle distinctions between fonts may be important. Second, rules and conventions of font pairing given by human experts are difficult to formalize. Third, font pairing is an asymmetric problem in that the roles played by header and body fonts are not interchangeable. To address these challenges, we propose automatic font pairing through learning visual relationships from large-scale human-generated font pairs. We introduce a new database for font pairing constructed from millions of PDF documents available on the Internet. We propose two font pairing algorithms: dual-space k-NN and asymmetric similarity metric learning (ASML). These two methods automatically learn fine-grained relationships from large-scale data. We also investigate several baseline methods based on the rules from professional designers. Experiments and user studies demonstrate the effectiveness of our proposed dataset and methods.},
  archive      = {J_TMM},
  author       = {Shuhui Jiang and Zhaowen Wang and Aaron Hertzmann and Hailin Jin and Yun Fu},
  doi          = {10.1109/TMM.2019.2952266},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2086-2097},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual font pairing},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature matching with intra-group sparse model.
<em>TMM</em>, <em>22</em>(8), 2074–2085. (<a
href="https://doi.org/10.1109/TMM.2019.2951466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching is a fundamental problem in computer vision area. In many real applications, one can usually obtain some potential (candidate) matches C by using some discriminative feature descriptors, such as SIFT descriptor. Then, the feature matching problem can be formulated as the problem of trying to select the correct matches S from the potential match set C. In this paper, we propose to solve matches selection by developing a novel intra-group sparse matching (IGSM) model. Our IGSM is motivated by a simple observation that the potential match set C can be divided into several non-overlapping groups Ci, among which the correct matches S are uniformly distributed. We thus develop an intra-group selection model to conduct matches selection at the intra-group level to incorporate the one-to-one matching constraint more in matches selection process. Our IGSM model has three main advantages: (1) The selection mechanism is parameter-free; (2) it generates an intra-group sparse solution which better maintains the one-to-one matching constraint in nature; (3) a simple yet effective update algorithm has been derived to solve IGSM model. The optimality and convergence of the algorithm are theoretically guaranteed. Experimental results on several image feature matching datasets show the effectiveness and efficiency of the proposed IGSM matching method.},
  archive      = {J_TMM},
  author       = {Bo Jiang and Jin Tang and Bin Luo},
  doi          = {10.1109/TMM.2019.2951466},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2074-2085},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Feature matching with intra-group sparse model},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning for image hashing. <em>TMM</em>,
<em>22</em>(8), 2061–2073. (<a
href="https://doi.org/10.1109/TMM.2019.2951462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing methods have received much attention recently, which achieve promising results by taking advantage of the strong representation power of deep networks. However, most existing deep hashing methods learn a whole set of hashing functions independently, while ignore the correlations between different hashing functions that can promote the retrieval accuracy greatly. Inspired by the sequential decision ability of deep reinforcement learning, we propose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH) . Our proposed DRLIH approach models the hashing learning problem as a sequential decision process, which learns each hashing function by correcting the errors imposed by previous ones and promotes retrieval accuracy. To the best of our knowledge, this is the first work to address hashing problem from deep reinforcement learning perspective. The main contributions of our proposed DRLIH approach can be summarized as follows: (1) We propose a deep reinforcement learning hashing network . In the proposed network, we utilize recurrent neural network (RNN) as agents to model the hashing functions, which take actions of projecting images into binary codes sequentially, so that the current hashing function learning can take previous hashing functions’ error into account. (2) We propose a sequential learning strategy based on proposed DRLIH. We define the state as a tuple of internal features of RNN&#39;s hidden layers and image features, which can reflect history decisions made by the agents. We also propose an action group method to enhance the correlation of hash functions in the same group. Experiments on three widely-used datasets demonstrate the effectiveness of our proposed DRLIH approach.},
  archive      = {J_TMM},
  author       = {Yuxin Peng and Jian Zhang and Zhaoda Ye},
  doi          = {10.1109/TMM.2019.2951462},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2061-2073},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep reinforcement learning for image hashing},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient supervised discrete multi-view hashing for
large-scale multimedia search. <em>TMM</em>, <em>22</em>(8), 2048–2060.
(<a href="https://doi.org/10.1109/TMM.2019.2947358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has recently received substantial attention in large-scale multimedia search for its extremely low-cost storage cost and high retrieval efficiency. However, most existing hashing techniques focus on learning hash codes for single-view or cross-view retrieval. It is still an unsolved problem that how to efficiently learn discriminative binary codes for multi-view data that is common in real world multimedia search. In this paper, we propose an efficient Supervised Discrete Multi-view Hashing (SDMH) to solve the problem. SDMH first properly detects the shared binary hash codes, with an integrated multi-view feature mapping and latent hash coding, by exploiting the complementarity of different view-specific features and removing the involved inter-view redundancy. To further enhance the discriminative capability of hash codes, SDMH directly represses the explicit semantic labels of data samples with their corresponding binary codes. Different from most existing multi-view hashing methods that adopt “relaxing+rounding” hash optimization strategy or the discrete optimization method based on discrete cyclic coordinate descent, an efficient augmented Lagrangian multiplier (ALM) based discrete hash optimization method is developed in this paper to optimize the hash codes within a single step. Experimental results on four benchmark datasets demonstrate the superior performance of the proposed approach over state-of-the-art hashing techniques, in terms of both learning efficiency and retrieval accuracy.},
  archive      = {J_TMM},
  author       = {Xu Lu and Lei Zhu and Jingjing Li and Huaxiang Zhang and Heng Tao Shen},
  doi          = {10.1109/TMM.2019.2947358},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2048-2060},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient supervised discrete multi-view hashing for large-scale multimedia search},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FFTMI: Features fusion for natural tone-mapped images
quality evaluation. <em>TMM</em>, <em>22</em>(8), 2038–2047. (<a
href="https://doi.org/10.1109/TMM.2019.2952256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tone-mapping is a crucial step in the task towards displaying high dynamic range (HDR) images on standard displays. Given the number of possible ways to tone-map such images, development of an objective quality criterion, enabling selection of the most suitable tone-mapping operator (TMO) and setting its parameters in order to maximize the quality of the reproduction, is of high interest. In this paper, a new objective metric for natural tone-mapped images is proposed. It is based on a fusion of several perceptually relevant features that have been carefully selected using an appropriate feature selection procedure. The outcome of the selection also provides a valuable insight into the importance of particular perceptual aspects when judging the quality of tone-mapped HDR content. The performance of the resulting combination of features is thoroughly evaluated with respect to three publicly available databases and compared to several relevant state-of-the-art criteria. The proposed approach is shown to significantly outperform the tested metrics and can, therefore, be considered a competitive alternative for tone-mapped images evaluation.},
  archive      = {J_TMM},
  author       = {Lukáš Krasula and Karel Fliegel and Patrick Le Callet},
  doi          = {10.1109/TMM.2019.2952256},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2038-2047},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FFTMI: Features fusion for natural tone-mapped images quality evaluation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reduced reference stereoscopic image quality assessment
using sparse representation and natural scene statistics. <em>TMM</em>,
<em>22</em>(8), 2024–2037. (<a
href="https://doi.org/10.1109/TMM.2019.2950533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ideal quality assessment model should simulate the properties of the visual brain to be consistent with human evaluation. The visual brain appears to have both evolved to seek an efficient, decorrelated representation of image information and to “match” the statistics of the natural image. On one hand, the theoretical studies suggest that sparse representation resembles the strategy in the primary visual cortex of brain for representing natural images. On the other hand, the natural scene statistics have driven the evolution of human visual system and have also inspired the understanding and simulating of visual perception. Inspired by these observations, in this paper, we propose a novel reduced-reference stereoscopic image quality assessment metric using sparse representation and natural scene statistics to simulate the visual perception of the brain. Specifically, the distribution statistics of the classified visual primitives extracted by sparse representation are used to measure the visual information, which is closely related to the hierarchical progressive process of human visual perception. Particularly, the mutual information of classified primitives between two view images is derived as a binocular cue to simulate the binocular fusion process. The maximum mechanism that is applied to select the visual information is a pooling mechanism with which complex cells use the maximal stimuli from a group of simple cells during the transfer process in the primary visual cortex. The natural scene statistics of locally normalized luminance coefficients are used to evaluate the natural losses due to the presence of distortions. The differences of the visual information and the natural scene statistics between the original and distorted images are used to compute the quality score by a prediction function which is trained using support vector regression. Experimental results show that the proposed metric outperforms the state-of-the-art stereoscopic image quality assessment metrics on LIVE 3D IQA database and NBU-MDSID Phase-II database, and delivers competitive performance on Waterloo IVC 3D database.},
  archive      = {J_TMM},
  author       = {Zhaolin Wan and Ke Gu and Debin Zhao},
  doi          = {10.1109/TMM.2019.2950533},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2024-2037},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reduced reference stereoscopic image quality assessment using sparse representation and natural scene statistics},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast user-guided single image reflection removal via
edge-aware cascaded networks. <em>TMM</em>, <em>22</em>(8), 2012–2023.
(<a href="https://doi.org/10.1109/TMM.2019.2951461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking photos through a glass window leads to glare or reflection, which might distract the viewer from the scene behind the window. In this paper, we involve user interaction to tackle the ill-posedness of the reflection removal problem. Users are allowed to draw strokes or lassos to indicate the background and reflection layers. Instead of designing hand-crafted features, we propose the edge-aware cascaded networks for reflection removal. The proposed network is a two-stage pipeline. The first stage takes the edge hints converted from user guidance and the image with reflection as input, and then separates the input image into the background and reflection layers. The second stage involves a refinement network to recover the missing details of the background layers. We simulate different types of user guidance, and the networks are trained on simulated data. The cascaded networks are end-to-end and perform with a single feed-forward pass, enabling fast editing. Extensive experimental evaluations demonstrate that the proposed used-guided reflection removal network yields better performance than the state-of-the-art methods on real-world scenarios. Furthermore, we show that novice users can easily generate reflection-free images, and large improvements in reflection removal quality can be obtained in just one minute.},
  archive      = {J_TMM},
  author       = {Huaidong Zhang and Xuemiao Xu and Hai He and Shengfeng He and Guoqiang Han and Jing Qin and Dapeng Wu},
  doi          = {10.1109/TMM.2019.2951461},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {2012-2023},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast user-guided single image reflection removal via edge-aware cascaded networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy least squares support vector machine with adaptive
membership for object tracking. <em>TMM</em>, <em>22</em>(8), 1998–2011.
(<a href="https://doi.org/10.1109/TMM.2019.2952252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy learning has been introduced into tracking and achieved great success. However, the membership in the existing fuzzy learning based tracking algorithm is fixed, which lacks the adaptivity to measure the importance of the samples. To improve the tracking adaptivity and flexibility, in this paper, we propose a novel tracking method based on fuzzy least squares support vector machine with adaptive membership (FLS-SVM-AM). First, we formulate tracking as an adaptive membership based fuzzy learning problem, which addresses the issue of fixed membership in existing methods and can better measure the importance of the training samples. Second, we present the FLS-SVM-AM method to build the appearance model, and develop an iterative optimization process to solve the FLS-SVM-AM problem. Third, we define a new membership based on the PASCAL VOC overlap rate and exponential function, which is used to measure the importance of different samples more accurately. Experimental results in the benchmark datasets demonstrate that the proposed method not only outperforms the existing fuzzy learning based tracking methods, but also is comparable to many state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Shunli Zhang and Li Zhang and Alexander G. Hauptmann},
  doi          = {10.1109/TMM.2019.2952252},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {1998-2011},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fuzzy least squares support vector machine with adaptive membership for object tracking},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Jointly learning kernel representation tensor and affinity
matrix for multi-view clustering. <em>TMM</em>, <em>22</em>(8),
1985–1997. (<a href="https://doi.org/10.1109/TMM.2019.2952984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering refers to the task of partitioning numerous unlabeled multimedia data into several distinct clusters using multiple features. In this paper, we propose a novel nonlinear method called joint learning multi-view clustering (JLMVC) to jointly learn kernel representation tensor and affinity matrix. The proposed JLMVC has three advantages: (1) unlike existing low-rank representation-based multi-view clustering methods that learn the representation tensor and affinity matrix in two separate steps, JLMVC jointly learns them both; (2) using the “kernel trick,” JLMVC can handle nonlinear data structures for various real applications; and (3) different from most existing methods that treat representations of all views equally, JLMVC automatically learns a reasonable weight for each view. Based on the alternating direction method of multipliers, an effective algorithm is designed to solve the proposed model. Extensive experiments on eight multimedia datasets demonstrate the superiority of the proposed JLMVC over state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yongyong Chen and Xiaolin Xiao and Yicong Zhou},
  doi          = {10.1109/TMM.2019.2952984},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {1985-1997},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Jointly learning kernel representation tensor and affinity matrix for multi-view clustering},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep multi-scale context aware feature aggregation for
curved scene text detection. <em>TMM</em>, <em>22</em>(8), 1969–1984.
(<a href="https://doi.org/10.1109/TMM.2019.2952978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text plays a significant role in image and video understanding, which has made great progress in recent years. Most existing models on text detection in the wild have the assumption that all the texts are surrounded by a rotated rectangle or quadrangle. While there also exist lots of curved texts in the wild, which would not be bounded by a regular bounding box. In this paper, we develop a novel architecture to localize the text regions, which can deal with curved-shape scene texts. Specifically, we first design a text-related feature enhancement module by incorporating the prior knowledge of the text shape to enhance the feature representations. After that, based on the enhanced features, we employ a region proposal network to generate the candidate boxes of scene texts. For each text candidate, a pyramid region-of-interest pooling attention module is utilized to extract the fixed-size features. Finally, we exploit the box-aware context-based text segmentation module and box refinement network to obtain the location of scene text. Experiments are conducted on four challenging benchmarks ${CTW1500}$ , ${totalTEXT}$ , ${ICDAR-2015}$ and ${MLT}$ , and the experimental results have demonstrated the superiority of our model.},
  archive      = {J_TMM},
  author       = {Pengwen Dai and Hua Zhang and Xiaochun Cao},
  doi          = {10.1109/TMM.2019.2952978},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {1969-1984},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep multi-scale context aware feature aggregation for curved scene text detection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CKD: Cross-task knowledge distillation for text-to-image
synthesis. <em>TMM</em>, <em>22</em>(8), 1955–1968. (<a
href="https://doi.org/10.1109/TMM.2019.2951463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image synthesis (T2IS) has drawn increasing interest recently, which can automatically generate images conditioned on text descriptions. It is a highly challenging task that learns a mapping from a semantic space of text description to a complex RGB pixel space of image. The main issues of T2IS lie in two aspects: semantic consistency and visual quality. The distributions between text descriptions and image contents are inconsistent since they belong to different modalities. So it is ambitious to generate images containing consistent semantic contents with the text descriptions, which is the semantic consistency issue. Moreover, due to the discrepancy of data distributions between real and synthetic images in huge pixel space, it is hard to approximate the real data distribution for synthesizing photo-realistic images, which is the visual quality issue. For addressing the above issues, we propose a cross-task knowledge distillation (CKD) approach to transfer knowledge from multiple image semantic understanding tasks into T2IS task. There is amount of knowledge in image semantic understanding tasks to translate image contents into semantic representation, which is advantageous to address the issues of semantic consistency and visual quality for T2IS. Moreover, we design a multi-stage knowledge distillation paradigm to decompose the distillation process into multiple stages. By this paradigm, it is effective to approximate the distributions of real image and understand textual information for T2IS, which can improve the visual quality and semantic consistency of synthetic images. Comprehensive experiments on widely-used datasets show the effectiveness of our proposed CKD approach.},
  archive      = {J_TMM},
  author       = {Mingkuan Yuan and Yuxin Peng},
  doi          = {10.1109/TMM.2019.2951463},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {1955-1968},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CKD: Cross-task knowledge distillation for text-to-image synthesis},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-attribute blind quality evaluator for tone-mapped
images. <em>TMM</em>, <em>22</em>(8), 1939–1954. (<a
href="https://doi.org/10.1109/TMM.2019.2950570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dynamic range (HDR) imaging enables capturing a wide range of luminance levels existing in real-world scenes. While HDR capturing devices become widespread in the market, the display technology is yet limited in representing full luminance ranges and standard low dynamic range (LDR) displays are currently more prevalent. To visualize the HDR content on traditional displays, tone mapping (TM) operators are introduced that convert HDR content into LDR. The dynamic range compression and different processing steps during TM can lead to loss of scene details, as well as luminance and chrominance changes. Such signal deviations will affect image naturalness and consequently disturb the visual quality of experience. Therefore, research into objective methods for quality evaluation of tone-mapped images has received attention in recent years. In this paper, we proposed a completely blind image quality evaluator for tone-mapped images based on a multi-attribute feature extraction scheme. Due to the diversity of TM distortions, various image characteristics are taken into account to develop an effective metric. The features are designed by considering spectral and spatial entropy, detection probability of visual information, image exposure, sharpness, and color properties. The quality-relevant features are then fed into a machine-learning regression framework to pool a quality score. The validation tests on two benchmark datasets reveal the superior performance of the proposed approach compared to the competing metrics.},
  archive      = {J_TMM},
  author       = {Saeed Mahmoudpour and Peter Schelkens},
  doi          = {10.1109/TMM.2019.2950570},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {1939-1954},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A multi-attribute blind quality evaluator for tone-mapped images},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved reversible data hiding in encrypted images using
parametric binary tree labeling. <em>TMM</em>, <em>22</em>(8),
1929–1938. (<a href="https://doi.org/10.1109/TMM.2019.2952979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes an improved reversible data hiding scheme in encrypted images using parametric binary tree labeling(IPBTL-RDHEI), which takes advantage of the spatial correlation in the entire original image but not in small image blocks to reserve room for hiding data. Then the original image is encrypted with an encryption key and the parametric binary tree is used to label encrypted pixels into two different categories. Finally, one of the two categories of encrypted pixels can embed secret information by bit replacement. According to the experimental results, compared with several state-of-the-art methods, the proposed IPBTL-RDHEI method achieves higher embedding rate and outperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the original plaintext image and the secret information can be restored and extracted losslessly and separately.},
  archive      = {J_TMM},
  author       = {Youqing Wu and Youzhi Xiang and Yutang Guo and Jin Tang and Zhaoxia Yin},
  doi          = {10.1109/TMM.2019.2952979},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {1929-1938},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An improved reversible data hiding in encrypted images using parametric binary tree labeling},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A co-prediction-based compression scheme for correlated
images. <em>TMM</em>, <em>22</em>(8), 1917–1928. (<a
href="https://doi.org/10.1109/TMM.2019.2949393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved a preliminary success in image compression due to the ability to learn the nonlinear spaces with compact features that training samples belong to. Unfortunately, it is not straightforward for the network based image compression methods to code multiple highly related images. In this paper, we propose a co-prediction based image compression (CPIC) which uses the multi-stream autoencoders to collaboratively code the multiple highly correlated images by enforcing the co-reference constraint on the multi-stream features. Patch samples fed into the multi-stream autoencoder, are generated through corresponding patch matching under permutation, which helps the autoencoder to learn the relationship among corresponding patches from the correlated images. Each stream network consists of encoder, decoder, importance map network and binarizer. In order to guide the allocation of local bit rate of the binary features, the important map network is employed to guarantee the compactness of learned features. A proxy function is used to make the binary operation for the code layer of the autoencoder differentiable. Finally, the network optimization is formulated as a rate distortion optimization. Experimental results prove that the proposed compression method outperforms JPEG2000 up to 1.5 dB in terms of PSNR.},
  archive      = {J_TMM},
  author       = {Wenbin Yin and Yunhui Shi and Wangmeng Zuo and Xiaopeng Fan},
  doi          = {10.1109/TMM.2019.2949393},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  number       = {8},
  pages        = {1917-1928},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A co-prediction-based compression scheme for correlated images},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(7), C3. (<a
href="https://doi.org/10.1109/TMM.2020.3002046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.3002046},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). What image features boost housing market predictions?
<em>TMM</em>, <em>22</em>(7), 1904–1916. (<a
href="https://doi.org/10.1109/TMM.2020.2966890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attractiveness of a property is one of the most interesting, yet challenging, categories to model. Image characteristics are used to describe certain attributes, and to examine the influence of visual factors on the price or timeframe of the listing. In this paper, we propose a set of techniques for the extraction of visual features for efficient numerical inclusion in modern-day predictive algorithms. We discuss techniques such as Shannon&#39;s entropy, calculating the center of gravity, employing image segmentation, and using Convolutional Neural Networks. After comparing these techniques as applied to a set of property-related images (indoor, outdoor, and satellite), we conclude the following: (i) the entropy is the most efficient single-digit visual measure for housing price prediction; (ii) image segmentation is the most important visual feature for the prediction of housing lifespan; and (iii) deep image features can be used to quantify interior characteristics and contribute to captivation modeling. The set of 40 image features selected here carries a significant amount of predictive power and outperforms some of the strongest metadata predictors. Without any need to replace a human expert in a real-estate appraisal process, we conclude that the techniques presented in this paper can efficiently describe visible characteristics, thus introducing perceived attractiveness as a quantitative measure into the predictive modeling of housing.},
  archive      = {J_TMM},
  author       = {Zona Kostic and Aleksandar Jevremovic},
  doi          = {10.1109/TMM.2020.2966890},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1904-1916},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {What image features boost housing market predictions?},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards improving robustness of deep neural networks to
adversarial perturbations. <em>TMM</em>, <em>22</em>(7), 1889–1903. (<a
href="https://doi.org/10.1109/TMM.2020.2969784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have presented superlative performance in many machine learning based perception and recognition tasks, where they have even outperformed human precision in some applications. However, it has been found that human perception system is much more robust to adversarial perturbation, as compared to these artificial networks. It has been shown that a deep architecture with a lower Lipschitz constant can generalize better and tolerate higher level of adversarial perturbation. Smooth regularization has been proposed to control the Lipschitz constant of a deep architecture and in this work, we show how a deep convolutional neural network (CNN), based on non-smooth regularization of convolution and fully connected layers, can present enhanced generalization and robustness to adversarial perturbation, simultaneously. We propose two non-smooth regularizers that present specific features for adversarial samples with different levels of signal-to-noise ratios. The regularizers build direct interconnections for the weight matrices in each layer, through which they control the Lipschitz constant of architecture and improve the consistency of input-output mapping of the network. This leads to more reliable and interpretable network mapping and reduces abrupt changes in the networks output. We develop an efficient algorithm to solve the non-smooth learning problems, which presents a gradual complexity addition property. Our simulation results over three benchmark datasets signify the superiority of the proposed formulations over previously reported methods for improving the robustness of deep architecture, towards human robustness to adversarial samples.},
  archive      = {J_TMM},
  author       = {Sajjad Amini and Shahrokh Ghaemmaghami},
  doi          = {10.1109/TMM.2020.2969784},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1889-1903},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards improving robustness of deep neural networks to adversarial perturbations},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iterative deep neural network quantization with lipschitz
constraint. <em>TMM</em>, <em>22</em>(7), 1874–1888. (<a
href="https://doi.org/10.1109/TMM.2019.2949857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network quantization offers an effective solution to deep neural network compression for practical usage. Existing network quantization methods cannot theoretically guarantee the convergence. This paper proposes a novel iterative framework for network quantization with arbitrary bit-widths. We present two Lipschitz constraint based quantization strategies, namely width-level network quantization (WLQ) and multi-level network quantization (MLQ), for high-bit and extremely low-bit (ternary) quantization, respectively. In WLQ, Lipschitz based partition is developed to divide parameters in each layer into two groups: one for quantization and the other for re-training to eliminate the quantization loss. WLQ is further extended to MLQ by introducing layer partition to suppress the quantization loss for extremely low bit-widths. The Lipschitz based partition is proven to guarantee the convergence of the quantized networks. Moreover, the proposed framework is complementary to network compression methods such as activation quantization, pruning and efficient network architectures. The proposed framework is evaluated over extensive state-of-the-art deep neural networks, i.e., AlexNet, VGG-16, GoogleNet and ResNet18. Experimental results show that the proposed framework improves the performance of tasks like classification, object detection and semantic segmentation.},
  archive      = {J_TMM},
  author       = {Yuhui Xu and Wenrui Dai and Yingyong Qi and Junni Zou and Hongkai Xiong},
  doi          = {10.1109/TMM.2019.2949857},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1874-1888},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Iterative deep neural network quantization with lipschitz constraint},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting vulnerabilities of deep neural networks for
privacy protection. <em>TMM</em>, <em>22</em>(7), 1862–1873. (<a
href="https://doi.org/10.1109/TMM.2020.2987694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial perturbations can be added to images to protect their content from unwanted inferences. These perturbations may, however, be ineffective against classifiers that were not seen during the generation of the perturbation, or against defenses based on re-quantization, median filtering or JPEG compression. To address these limitations, we present an adversarial attack that is specifically designed to protect visual content against unseen classifiers and known defenses. We craft perturbations using an iterative process that is based on the Fast Gradient Signed Method and that randomly selects a classifier and a defense, at each iteration. This randomization prevents an undesirable overfitting to a specific classifier or defense. We validate the proposed attack in both targeted and untargeted settings on the private classes of the Places365-Standard dataset. Using ResNet18, ResNet50, AlexNet and DenseNet161 as classifiers, the performance of the proposed attack exceeds that of eleven state-of-the-art attacks.},
  archive      = {J_TMM},
  author       = {Ricardo Sanchez-Matilla and Chau Yi Li and Ali Shahin Shamsabadi and Riccardo Mazzon and Andrea Cavallaro},
  doi          = {10.1109/TMM.2020.2987694},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1862-1873},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploiting vulnerabilities of deep neural networks for privacy protection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature-flow interpretation of deep convolutional neural
networks. <em>TMM</em>, <em>22</em>(7), 1847–1861. (<a
href="https://doi.org/10.1109/TMM.2020.2976985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great success of deep convolutional neural networks (DCNNs) in computer vision tasks, their black-box aspect remains a critical concern. The interpretability of DCNN models has been attracting increasing attention. In this work, we propose a novel model, Feature-fLOW INterpretation (FLOWIN) model, to interpret a DCNN by its feature-flow. The FLOWIN can express deep-layer features as a sparse representation of shallow-layer features. Based on that, it distills the optimal feature-flow for the prediction of a given instance, starting from deep layers to shallow layers. Therefore, the FLOWIN can provide an instance-specific interpretation, which presents its feature-flow units and their interpretable meanings for its network decision. The FLOWIN can also give the quantitative interpretation in which the contribution of each flow unit in different layers is used to interpret the net decision. From the class-level view, we can further understand networks by studying feature-flows within and between classes. The FLOWIN not only provides the visualization of the feature-flow but also studies feature-flow quantitatively by investigating its density and similarity metrics. In our experiments, the FLOWIN is evaluated on different datasets and networks by quantitative and qualitative ways to show its interpretability.},
  archive      = {J_TMM},
  author       = {Xinrui Cui and Dan Wang and Z. Jane Wang},
  doi          = {10.1109/TMM.2020.2976985},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1847-1861},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Feature-flow interpretation of deep convolutional neural networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial attribute-text embedding for person search with
natural language query. <em>TMM</em>, <em>22</em>(7), 1836–1846. (<a
href="https://doi.org/10.1109/TMM.2020.2972168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The newly emerging task of person search with natural language query aims at retrieving the target pedestrian by a text description of the pedestrian. It is more applicable compared to person search with image/video query, i.e., person re-identification. In this paper, we propose a novel Adversarial Attribute-Text Embedding (AATE) network for person search with text query. In particular, a cross-modal adversarial learning module is proposed to learn discriminative and modality-invariant visual-textual features. It consists of a cross-modal learner and a modality discriminator, playing a min-max game in an adversarial learning way. The former is to improve intra-modality discrimination and inter-modality invariance towards confusing the modality discriminator. The latter is to distinguish the features from different modalities and boost the learning of modality-invariant features. Moreover, a visual attribute graph convolutional network is proposed to learn visual attributes of pedestrians, which possess better descriptiveness, interpretability and robustness compared to pedestrian appearance features. A hierarchical text embedding network, consisting of multi-stacked bidirectional LSTMs and a textual attention block, is developed to extract effective textual features from text descriptions of pedestrians. Extensive experimental results on two challenging benchmarks, have demonstrated the effectiveness of the proposed approach.},
  archive      = {J_TMM},
  author       = {Zheng-Jun Zha and Jiawei Liu and Di Chen and Feng Wu},
  doi          = {10.1109/TMM.2020.2972168},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1836-1846},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adversarial attribute-text embedding for person search with natural language query},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimedia intelligence: When multimedia meets artificial
intelligence. <em>TMM</em>, <em>22</em>(7), 1823–1835. (<a
href="https://doi.org/10.1109/TMM.2020.2969791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the rich emerging multimedia applications and services in the past decade, super large amount of multimedia data has been produced for the purpose of advanced research in multimedia. Furthermore, multimedia research has made great progress on image/video content analysis, multimedia search and recommendation, multimedia streaming, multimedia content delivery etc. At the same time, Artificial Intelligence (AI) has undergone a “new” wave of development since being officially regarded as an academic discipline in 1950s, which should give credits to the extreme success of deep learning. Thus, one question naturally arises: What happens when multimedia meets Artificial Intelligence? To answer this question, we introduce the concept of Multimedia Intelligence through investigating the mutual-influence between multimedia and Artificial Intelligence. We explore the mutual influences between multimedia and Artificial Intelligence from two aspects: i) multimedia drives Artificial Intelligence to experience a paradigm shift towards more explainability and ii) Artificial Intelligence in turn injects new ways of thinking for multimedia research. As such, these two aspects form a loop in which multimedia and Artificial Intelligence interactively enhance each other. In this paper, we discuss what and how efforts have been done in literature and share our insights on research directions that deserve further study to produce potentially profound impact on multimedia intelligence.},
  archive      = {J_TMM},
  author       = {Wenwu Zhu and Xin Wang and Wen Gao},
  doi          = {10.1109/TMM.2020.2969791},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1823-1835},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimedia intelligence: When multimedia meets artificial intelligence},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Similarity-aware and variational deep adversarial learning
for robust facial age estimation. <em>TMM</em>, <em>22</em>(7),
1808–1822. (<a href="https://doi.org/10.1109/TMM.2020.2969793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a similarity-aware deep adversarial learning (SADAL) approach for facial age estimation. Instead of making full access to the limited training samples which likely leads to bias age prediction, our SADAL aims to seek batches of unobserved hard-negative samples based on existing training samples, which typically reinforces the discriminativeness of the learned feature representation for facial ages. Motivated by the fact that age labels are usually correlated in real-world scenarios, we carefully develop a similarity-aware function to well measure the distance of each face pair based on the age value gaps. Consequently, the age-difference information is exploited in the synthetic feature space for robust age estimation. During the learning process, we jointly optimize both procedures of generating hard negatives and learning discriminative age ranker via a sequence of adversarial-game iterations. Another major issue lies on that existing methods only enforce the indiscriminativeness within each class, which is probably trapped into model overfitting and thus the generation capacity is limited particularly on unseen age classes with many individuals. To circumvent this problem, we propose a variational deep adversarial learning (VDAL) paradigm, which learns to encode each face sample in two factorized parts, i.e., the intra-class variance distribution and the intra-class invariant class center. Moreover, our VDAL principally optimizes the variational confidence lower bound on the variational factorized feature representation. To better enhance the discriminativeness of the age representation, our VDAL further learns to encode the ordinal relationship among age labels in the reconstructed subspace. Experimental results on folds of widely-evaluated benchmarking datasets demonstrate that our approach achieves promising performance in contrast to most state-of-the-art age estimation methods.},
  archive      = {J_TMM},
  author       = {Hao Liu and Penghui Sun and Jiaqiang Zhang and Suping Wu and Zhenhua Yu and Xuehong Sun},
  doi          = {10.1109/TMM.2020.2969793},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1808-1822},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Similarity-aware and variational deep adversarial learning for robust facial age estimation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning reliable visual saliency for model explanations.
<em>TMM</em>, <em>22</em>(7), 1796–1807. (<a
href="https://doi.org/10.1109/TMM.2019.2949872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By highlighting important features that contribute to model prediction, visual saliency is used as a natural form to interpret the working mechanism of deep neural networks. Numerous methods have been proposed to achieve better saliency results. However, we find that previous visual saliency methods are not reliable enough to provide meaningful interpretation through a simple sanity check: saliency methods are required to explain the output of non-maximum prediction classes, which are usually not ground-truth classes. For example, let the methods interpret an image of “dog” given a wrong class label “fish” as the query. This procedure can test whether these methods reliably interpret model&#39;s predictions based on existing features that appear in the data. Our experiments show that previous methods failed to pass the test by generating similar saliency maps or scattered patterns. This false saliency response can be dangerous in certain scenarios, such as medical diagnosis. We find that these failure cases are mainly due to the attribution vanishing and adversarial noise within these methods. In order to learn reliable visual saliency, we propose a simple method that requires the output of the model to be close to the original output while learning an explanatory saliency mask. To enhance the smoothness of the optimized saliency masks, we then propose a simple Hierarchical Attribution Fusion (HAF) technique. In order to fully evaluate the reliability of visual saliency methods, we propose a new task Disturbed Weakly Supervised Object Localization (D-WSOL) to measure whether these methods can correctly attribute the model&#39;s output to existing features. Experiments show that previous methods fail to meet this standard, and our approach helps to improve the reliability by suppressing false saliency responses. After observing a significant layout difference in saliency masks between real and adversarial samples. we propose to train a simple CNN on these learned hierarchical attribution masks to distinguish adversarial samples. Experiments show that our method can improve detection performance over other approaches significantly.},
  archive      = {J_TMM},
  author       = {Yulong Wang and Hang Su and Bo Zhang and Xiaolin Hu},
  doi          = {10.1109/TMM.2019.2949872},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1796-1807},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning reliable visual saliency for model explanations},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bidirectional attention-recognition model for fine-grained
object classification. <em>TMM</em>, <em>22</em>(7), 1785–1795. (<a
href="https://doi.org/10.1109/TMM.2019.2954747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained object classification (FGOC) is a challenging research topic in multimedia computing with machine learning, which faces two pivotal conundrums: focusing attention on the discriminate part regions, and then processing recognition with the part-based features. Existing approaches generally adopt a unidirectional two-step structure, that first locate the discriminate parts and then recognize the part-based features. However, they neglect the truth that part localization and feature recognition can be reinforced in a bidirectional process. In this paper, we propose a novel bidirectional attention-recognition model (BARM) to actualize the bidirectional reinforcement for FGOC. The proposed BARM consists of one attention agent for discriminate part regions proposing and one recognition agent for feature extraction and recognition. Meanwhile, a feedback flow is creatively established to optimize the attention agent directly by recognition agent. Therefore, in BARM the attention agent and the recognition agent can reinforce each other in a bidirectional way and the overall framework can be trained end-to-end without neither object nor parts annotations. Moreover, a novel Multiple Random Erasing data augmentation is proposed, and it exhibits impressive pertinency and superiority for FGOC. Conducted on several extensive FGOC benchmarks, BARM outperforms the present state-of-the-art methods in classification accuracy. Furthermore, BARM exhibits a clear interpretability and keeps consistent with the human perception in visualization experiments.},
  archive      = {J_TMM},
  author       = {Chuanbin Liu and Hongtao Xie and Zhengjun Zha and Lingyun Yu and Zhineng Chen and Yongdong Zhang},
  doi          = {10.1109/TMM.2019.2954747},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1785-1795},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bidirectional attention-recognition model for fine-grained object classification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatio-temporal VLAD encoding of visual events using
temporal ordering of the mid-level deep semantics. <em>TMM</em>,
<em>22</em>(7), 1769–1784. (<a
href="https://doi.org/10.1109/TMM.2019.2959426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of video events based on frame-level descriptors is a common approach to video recognition. In the meanwhile, proper encoding of the frame-level descriptors is vital to the whole event classification procedure. While there are some pretty efficient video descriptor encoding methods, temporal ordering of the descriptors is often ignored in these encoding algorithms. In this paper, we show that by taking into account the temporal inter-frame dependencies and tracking the chronological order of video sub-events, accuracy of event recognition is further improved. First, the frame-level descriptors are extracted using convolutional neural networks (CNNs) pre-trained on ImageNet, which are fine-tuned on a portion of training video frames. Then, a spatio-temporal encoding is applied to the derived descriptors. The proposed spatio-temporal encoding, as the main contribution of this work, is inspired from the well-known vector of locally aggregated descriptors (VLAD) encoding in spatial domain and from total variation de-noising (TVD) in temporal domain. The proposed unified spatio-temporal encoding is then shown to be in the form of a convex optimization problem which is solved efficiently with alternating direction method of multipliers (ADMM) algorithm. The experimental results show superiority of the proposed encoding method in terms of recognition accuracy over both frame-level video encoding approaches and spatio-temporal video representations. As compared to the state-of-the-art approaches, our encoding method improves the mean average precision (mAP) over both Columbia consumer video (CCV), unstructured social activity attribute (USAA), YouTube-8M, and Kinetics datasets and is very competitive on FCVID dataset.},
  archive      = {J_TMM},
  author       = {Mohammad Soltanian and Sajjad Amini and Shahrokh Ghaemmaghami},
  doi          = {10.1109/TMM.2019.2959426},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1769-1784},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatio-temporal VLAD encoding of visual events using temporal ordering of the mid-level deep semantics},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated colorization of a grayscale image with seed points
propagation. <em>TMM</em>, <em>22</em>(7), 1756–1768. (<a
href="https://doi.org/10.1109/TMM.2020.2976573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a fully automatic image colorization method for grayscale images using neural network and optimization. For a determined training set including the gray images and its corresponding color images, our method segments grayscale images into superpixels and then extracts features of particular points of interest in each superpixel. The obtained features and their RGB values are given as input for, the training colorization neural network of each pixel. To achieve a better image colorization effect in shorter running time, our method further propagates the resulting color points to neighboring pixels for improved colorization results. In the propagation of color, we present a cost function to formalize the premise that neighboring pixels should have the maximum positive similarity of intensities and colors; we then propose our solution to solving the optimization problem. At last, a guided image filter is employed to refine the colorized image. Experiments on a wide variety of images show that the proposed algorithms can achieve superior performance over the state-of-the-art algorithms.},
  archive      = {J_TMM},
  author       = {Shaohua Wan and Yu Xia and Lianyong Qi and Yee-Hong Yang and Mohammed Atiquzzaman},
  doi          = {10.1109/TMM.2020.2976573},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1756-1768},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Automated colorization of a grayscale image with seed points propagation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PointHop: An explainable machine learning method for point
cloud classification. <em>TMM</em>, <em>22</em>(7), 1744–1755. (<a
href="https://doi.org/10.1109/TMM.2019.2963592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An explainable machine learning method for point cloud classification, called the PointHop method, is proposed in this work. The PointHop method consists of two stages: 1) local-to-global attribute building through iterative one-hop information exchange and 2) classification and ensembles. In the attribute building stage, we address the problem of unordered point cloud data using a space partitioning procedure and developing a robust descriptor that characterizes the relationship between a point and its one-hop neighbor in a PointHop unit. When we put multiple PointHop units in cascade, the attributes of a point will grow by taking its relationship with one-hop neighbor points into account iteratively. Furthermore, to control the rapid dimension growth of the attribute vector associated with a point, we use the Saab transform to reduce the attribute dimension in each PointHop unit. In the classification and ensemble stage, we feed the feature vector obtained from multiple PointHop units to a classifier. We explore ensemble methods to improve the classification performance furthermore. It is shown by experimental results that the PointHop method offers classification performance that is comparable with state-of-the-art methods while demanding much lower training complexity.},
  archive      = {J_TMM},
  author       = {Min Zhang and Haoxuan You and Pranav Kadam and Shan Liu and C.-C. Jay Kuo},
  doi          = {10.1109/TMM.2019.2963592},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1744-1755},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PointHop: An explainable machine learning method for point cloud classification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cycle-IR: Deep cyclic image retargeting. <em>TMM</em>,
<em>22</em>(7), 1730–1743. (<a
href="https://doi.org/10.1109/TMM.2019.2959925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised deep learning techniques have achieved great success in various fields due to getting rid of the limitation of handcrafted representations. However, most previous image retargeting algorithms still employ fixed design principles such as using gradient map or handcrafted features to compute saliency map, which inevitably restricts its generality. Deep learning techniques may help to address this issue, but the challenging problem is that we need to build a large-scale image retargeting dataset for the training of deep retargeting models. However, building such a dataset requires huge human efforts. In this paper, we propose a novel deep cyclic image retargeting approach, called Cycle-IR, to firstly implement image retargeting with a single deep model, without relying on any explicit user annotations. Our idea is built on the reverse mapping from the retargeted images to the given images. If the retargeted image has serious distortion or excessive loss of important visual information, the reverse mapping is unlikely to restore the input image well. We constrain this forward-reverse consistency by introducing a cyclic perception coherence loss. In addition, we propose a simple yet effective image retargeting network (IRNet) to implement the image retargeting process. Our IRNet contains a spatial and channel attention layer, which is able to discriminate visually important regions of input images effectively, especially in cluttered images. Given arbitrary sizes of input images and desired aspect ratios, our Cycle-IR can produce visually pleasing target images directly. Extensive experiments on the standard RetargetMe dataset show the superiority of our Cycle-IR.},
  archive      = {J_TMM},
  author       = {Weimin Tan and Bo Yan and Chuming Lin and Xuejing Niu},
  doi          = {10.1109/TMM.2019.2959925},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1730-1743},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cycle-IR: Deep cyclic image retargeting},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VINet: A visually interpretable image diagnosis network.
<em>TMM</em>, <em>22</em>(7), 1720–1729. (<a
href="https://doi.org/10.1109/TMM.2020.2971170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, due to the black box characteristics of deep learning techniques, the deep network-based computer-aided diagnosis (CADx) systems have encountered many difficulties in practical applications. The crux of the problem is that these models should be explainable the model should give doctors rationales that can explain the diagnosis. In this paper, we propose a visually interpretable network (VINet) which can generate diagnostic visual interpretations while making accurate diagnoses. VINet is an end-to-end model consisting of an importance estimation network and a classification network. The former produces a diagnostic visual interpretation for each case, and the classifier diagnoses the case. In the classifier, by exploring the information in the diagnostic visual interpretation, the irrelevant information in the feature maps is eliminated by our proposed feature destruction process. This allows the classification network to concentrate on the important features and use them as the primary references for classification. Through a joint optimization of higher classification accuracy and eliminating as many irrelevant features as possible, a precise, fine-grained diagnostic visual interpretation, along with an accurate diagnosis, can be produced by our proposed network simultaneously. Based on a computed tomography image dataset (LUNA16) on pulmonary nodule, extensive experiments have been conducted, demonstrating that the proposed VINet can produce state-of-the-art diagnostic visual interpretations compared with all baseline methods.},
  archive      = {J_TMM},
  author       = {Donghao Gu and Yaowei Li and Feng Jiang and Zhaojing Wen and Shaohui Liu and Wuzhen Shi and Guangming Lu and Changsheng Zhou},
  doi          = {10.1109/TMM.2020.2971170},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1720-1729},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VINet: A visually interpretable image diagnosis network},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PixelRL: Fully convolutional network with reinforcement
learning for image processing. <em>TMM</em>, <em>22</em>(7), 1704–1719.
(<a href="https://doi.org/10.1109/TMM.2019.2960636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep reinforcement learning (RL) for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. Besides, it is possible to visualize what kind of operation is employed for each pixel at each iteration, which would help us understand why and how such an operation is chosen. We also believe that our technology can enhance the explainability and interpretability of the deep neural networks. In addition, because the operations executed at each pixels are visualized, we can change or modify the operations if necessary. We apply the proposed method to a variety of image processing tasks: image denoising, image restoration, local color enhancement, and saliency-driven image editing. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning. The source code is available on https://github.com/rfuruta/pixelRL.},
  archive      = {J_TMM},
  author       = {Ryosuke Furuta and Naoto Inoue and Toshihiko Yamasaki},
  doi          = {10.1109/TMM.2019.2960636},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1704-1719},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PixelRL: Fully convolutional network with reinforcement learning for image processing},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical prototype learning for zero-shot recognition.
<em>TMM</em>, <em>22</em>(7), 1692–1703. (<a
href="https://doi.org/10.1109/TMM.2019.2959433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Shot Learning (ZSL) has received extensive attention and successes in recent years especially in areas of fine-grained object recognition, retrieval, and image captioning. Key to ZSL is to transfer knowledge from the seen to the unseen classes via auxiliary semantic prototypes (e.g., word or attribute vectors). However, the popularly learned projection functions in previous works cannot generalize well due to non-visual components included in semantic prototypes. Besides, the incompleteness of provided prototypes and captured images has less been considered by the state-of-the-art approaches in ZSL. In this paper, we propose a hierarchical prototype learning formulation to provide a systematical solution (named HPL) for zero-shot recognition. Specifically, HPL is able to obtain discriminability on both seen and unseen class domains by learning visual prototypes respectively under the transductive setting. To narrow the gap of two domains, we further learn the interpretable super-prototypes in both visual and semantic spaces. Meanwhile, the two spaces are further bridged by maximizing their structural consistency. This not only facilitates the representativeness of visual prototypes, but also alleviates the loss of information of semantic prototypes. An extensive group of experiments are then carefully designed and presented, demonstrating that HPL obtains remarkably more favorable efficiency and effectiveness, over currently available alternatives under various settings.},
  archive      = {J_TMM},
  author       = {Xingxing Zhang and Shupeng Gui and Zhenfeng Zhu and Yao Zhao and Ji Liu},
  doi          = {10.1109/TMM.2019.2959433},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1692-1703},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical prototype learning for zero-shot recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The interpretable fast multi-scale deep decoder for the
standard HEVC bitstreams. <em>TMM</em>, <em>22</em>(7), 1680–1691. (<a
href="https://doi.org/10.1109/TMM.2020.2978664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a research hotspot to restore decoded videos with existing bitstreams by applying deep neural network to improve compression efficiency at decoder-end. Existing research has verified that the utilization of redundancy at decoder-end, which is underused by the encoder, can bring an increase of compression efficiency. However, most existing research neglects the abundant multi-scale information among video frames as a typical type of such redundancy. It remains an interesting yet challenging topic how to build an effective, interpretable and fast deep neural network for the purpose of using the multi-scale similarity at decoder-end and further enhancing compression efficiency. To this end, this paper considers the use of underused inter multi-scale information and proposes the Fast Multi-Scale Deep Decoder (Fast MSDD) for the state-of-the-art video coding standard HEVC. The advantages of Fast MSDD are three-fold. First, it achieves a higher coding efficiency without modifying any encoding algorithm. Second, Fast MSDD is interpretable based on the framework of using the underused redundancy. Third, it guarantees the model&#39;s inference speed while fully using the multi-scale similarity among video frames. Extensive experimental results verify Fast MSDD&#39;s effectiveness, interpretability, and computational efficiency. Fast MSDD obtains averagely 14.3%, 10.8%, 8.5% and 7.6% BD gains for AI, LP, LB and RA respectively. Compared with our previous work MSDD, Fast MSDD achieves increases of 59.3%, 49.1%, 61.0% and 29.3%. Meanwhile, 16.9%, 11.2%, 9.2% and 8.3% BD gains are observed on videos with scale changes, which validate the interpretability of the proposed method. Furthermore, Fast MSDD can save at most 56.3% time compared to MSDD.},
  archive      = {J_TMM},
  author       = {Wenhui Xiao and Huiguo He and Tingting Wang and Hongyang Chao},
  doi          = {10.1109/TMM.2020.2978664},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1680-1691},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {The interpretable fast multi-scale deep decoder for the standard HEVC bitstreams},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IWave: CNN-based wavelet-like transform for image
compression. <em>TMM</em>, <em>22</em>(7), 1667–1679. (<a
href="https://doi.org/10.1109/TMM.2019.2957990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wavelet transform is a powerful tool for multiresolution time-frequency analysis. It has been widely adopted in many image processing tasks, such as denoising, enhancement, fusion, and especially compression. Wavelets lead to the successful image coding standard JPEG-2000. Traditionally, wavelets were designed from the signal processing theory with certain assumption on the signal, but natural images are not as ideal as assumed by the theory. How to design content-adaptive wavelets for natural images remains a difficulty. Inspired by the recent progress of convolutional neural network (CNN), we propose iWave as a framework for deriving wavelet-like transform that is more suitable for natural image compression. iWave adopts an update-first lifting scheme, where the prediction filter is a trained CNN, to achieve wavelet-like transform. The CNN can be embedded into a deep network that is analogous to an auto-encoder, which is trained end-to-end. The trained wavelet-like transform still possesses the lifting structure, which ensures perfect reconstruction, supports multiresolution analysis, and is more interpretable than the deep networks trained as “black boxes.” We perform experiments to verify the generality as well as the speciality of iWave in comparison with JPEG-2000. When trained with a generic set of natural images and tested on the Kodak dataset, iWave achieves on average 4.4% and up to 14% BD-rate reductions. When trained and tested with a specific kind of textures, iWave provides as high as 27% BD-rate reduction.},
  archive      = {J_TMM},
  author       = {Haichuan Ma and Dong Liu and Ruiqin Xiong and Feng Wu},
  doi          = {10.1109/TMM.2019.2957990},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1667-1679},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IWave: CNN-based wavelet-like transform for image compression},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial multimedia computing with interpretable
machine learning. <em>TMM</em>, <em>22</em>(7), 1661–1666. (<a
href="https://doi.org/10.1109/TMM.2020.2991292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section is to broadly engage the machine learning and multimedia communities on the emerging yet challenging interpretable machine learning. Multimedia is increasingly becoming the “biggest big data,” among the most important and valuable source for insight and information. Many powerful machine learning algorithms, especially deep learning models such as convolutional neural networks (CNNs), have recently achieved outstanding predictive performance in a wide range of multimedia applications, including visual object classification, scene understanding, speech recognition, and activity prediction. Nevertheless, most deep learning algorithms are generally conceived as blackbox methods, and it is difficult to intuitively and quantitatively understand the results of their prediction and inference. Since this lack of interpretability is a major bottleneck in designing more successful predictive models and exploring wider-range useful applications, there has been an explosion of interest in interpreting the representations learned by these models, with profound implications for research into interpretable machine learning in the multimedia community.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.2991292},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  number       = {7},
  pages        = {1661-1666},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guest editorial multimedia computing with interpretable machine learning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020g). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(6), C3. (<a
href="https://doi.org/10.1109/TMM.2020.2993157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.2993157},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical attention network for visually-aware food
recommendation. <em>TMM</em>, <em>22</em>(6), 1647–1659. (<a
href="https://doi.org/10.1109/TMM.2019.2945180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food recommender systems play an important role in assisting users to identify the desired food to eat. Deciding what food to eat is a complex and multi-faceted process, which is influenced by many factors such as the ingredients, appearance of the recipe, the user&#39;s personal preference on food, and various contexts like what had been eaten in the past meals. This work formulates the food recommendation problem as predicting user preference on recipes based on three key factors that determine a user&#39;s choice on food, namely, 1) the user&#39;s (and other users&#39;) history; 2) the ingredients of a recipe; and 3) the descriptive image of a recipe. To address this challenging problem, this work develops a dedicated neural network-based solution Hierarchical Attention based Food Recommendation (HAFR) which is capable of: 1) capturing the collaborative filtering effect like what similar users tend to eat; 2) inferring a user&#39;s preference at the ingredient level; and 3) learning user preference from the recipe&#39;s visual images. To evaluate our proposed method, this work constructs a large-scale dataset consisting of millions of ratings from AllRecipes.com. Extensive experiments show that our method outperforms several competing recommender solutions like Factorization Machine and Visual Bayesian Personalized Ranking with an average improvement of 12%, offering promising results in predicting user preference on food.},
  archive      = {J_TMM},
  author       = {Xiaoyan Gao and Fuli Feng and Xiangnan He and Heyan Huang and Xinyu Guan and Chong Feng and Zhaoyan Ming and Tat-Seng Chua},
  doi          = {10.1109/TMM.2019.2945180},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1647-1659},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical attention network for visually-aware food recommendation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual-texual emotion analysis with deep coupled video and
danmu neural networks. <em>TMM</em>, <em>22</em>(6), 1634–1646. (<a
href="https://doi.org/10.1109/TMM.2019.2946477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User emotion analysis toward videos is to automatically recognize the general emotional status of viewers from the multimedia content embedded in the online video stream. Existing works fall into two categories: 1) visual-based methods, which focus on visual content and extract a specific set of features of videos. However, it is generally hard to learn a mapping function from low-level video pixels to high-level emotion space due to great intra-class variance. 2) textual-based methods, which focus on the investigation of user-generated comments associated with videos. The learned word representations by traditional linguistic approaches typically lack emotion information and the global comments usually reflect viewers’ high-level understandings rather than instantaneous emotions. To address these limitations, in this paper, we propose to jointly utilize video content and user-generated texts simultaneously for emotion analysis. In particular, we introduce exploiting a new type of user-generated texts, i.e., “danmu,” which are real-time comments floating on the video and contain rich information to convey viewers’ emotional opinions. To enhance the emotion discriminativeness of words in textual feature extraction, we propose Emotional Word Embedding (EWE) to learn text representations by jointly considering their semantics and emotions. Afterward, we propose a novel visual-textual emotion analysis model with Deep Coupled Video and Danmu Neural networks (DCVDN), in which visual and textual features are synchronously extracted and fused to form a comprehensive representation by deep-canonically-correlated-autoencoder-based multi-view learning. Through extensive experiments on a self-crawled real-world video-danmu dataset, we prove that DCVDN significantly outperforms the state-of-the-art baselines.},
  archive      = {J_TMM},
  author       = {Chenchen Li and Jialin Wang and Hongwei Wang and Miao Zhao and Wenjie Li and Xiaotie Deng},
  doi          = {10.1109/TMM.2019.2946477},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1634-1646},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual-texual emotion analysis with deep coupled video and danmu neural networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymmetric joint GANs for normalizing face illumination from
a single image. <em>TMM</em>, <em>22</em>(6), 1619–1633. (<a
href="https://doi.org/10.1109/TMM.2019.2945197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Illumination normalization for face recognition is very important when a face is captured under harsh lighting conditions. Instead of designing hand-crafted features, in this paper we formulate face illumination normalization as an image-to-image translation task. A great challenge of face normalization is that human facial structures are particularly sensitive to image structure distortion, which frequently occurs in traditional image-to-image translation tasks. Unfortunately, sometimes even slight facial structure distortions may prohibit human eyes and machine face recognition methods from identifying face identities. To address this issue, a novel GAN- based network architecture called the asymmetric joint generative adversarial network (AJGAN) is developed to normalize face images under arbitrary illumination conditions, without known face geometry and albedo information. In addition, an illumination normalization GAN $G_1$ and an asymmetric relighting GAN $G_2$ that maps a frontal-illuminated image to images with various lighting conditions are incorporated in AJGAN to maintain personalized facial structures. To avoid image blurring caused by the under-constrained relighting mapping, we introduce a scheme of one-hot lighting labels into $G_2$ and enforce label classification loss. Furthermore, the number of training images starting from a very limited number of labels is dynamically extended by the combination of different lighting labels. Qualitative and quantitative experiments on three databases validate that AJGAN significantly outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xianjun Han and Hongyu Yang and Guanyu Xing and Yanli Liu},
  doi          = {10.1109/TMM.2019.2945197},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1619-1633},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Asymmetric joint GANs for normalizing face illumination from a single image},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Concentrated local part discovery with fine-grained part
representation for person re-identification. <em>TMM</em>,
<em>22</em>(6), 1605–1618. (<a
href="https://doi.org/10.1109/TMM.2019.2946486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism for person re-identification has been widely studied with deep convolutional neural networks. This mechanism works as a good complement to the global features extracted from an image of the entire human body. However, existing works mainly focus on discovering local parts with simple feature representations, such as global average pooling. Moreover, these works either require extra supervision, such as labeling of body joints, or pay little attention to the guidance of part learning, resulting in scattered activation of learned parts. Furthermore, existing works usually extract local features from different body parts via global average pooling and then concatenate them together as good global features. We find that local features acquired in this way contribute little to the overall performance. In this paper, we argue the significance of local part description and explore the attention mechanism from both local part discovery and local part representation aspects. For local part discovery, we propose a new constrained attention module to make the activated regions concentrated and meaningful without extra supervision. For local part representation, we propose a statistical-positional-relational descriptor to represent local parts from a fine-grained viewpoint. Extensive experiments are conducted to validate the overall performance, the effectiveness of each component, and the generalization ability. We achieve a rank-1 accuracy of 95.1% on Market1501, 64.7% on CUHK03, 87.1% on DukeMTMC-ReID, and 79.9% on MSMT17, outperforming state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Chaoqun Wan and Yue Wu and Xinmei Tian and Jianqiang Huang and Xian-Sheng Hua},
  doi          = {10.1109/TMM.2019.2946486},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1605-1618},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Concentrated local part discovery with fine-grained part representation for person re-identification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uni-and-bi-directional video prediction via learning
object-centric transformation. <em>TMM</em>, <em>22</em>(6), 1591–1604.
(<a href="https://doi.org/10.1109/TMM.2019.2946475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video prediction, including uni-directional prediction for future frames and bi-directional prediction for in-between frames, is a challenging task and a problem worth exploring in multimedia and computer vision fields. Existing practices usually make predictions by learning global motion information from the whole given image. However, humans often focus on key objects carrying vital motion information instead of the entire frame. Besides, different objects often show different movement and deformation, even in the same scene. In this connection, we build a novel model of object-centric video prediction, in which the motion signals of key objects are particularly learned. This model can predict new frames by repeatedly transforming objects into the original input images. To focus on these objects automatically, we create an attention module with substitutable strategies. Our method requires no annotated data, and we also use adversarial training to improve sharpness of the predictions. We evaluate our model through Moving MNIST, UCF101 and Penn Action datasets and achieve competitive results in both quantity and quality, compared to existing methods. The experiments demonstrate that our uni-and-bi-directional network can well predict motions for different objects and generate plausible future and in-between frames.},
  archive      = {J_TMM},
  author       = {Xiongtao Chen and Wenmin Wang},
  doi          = {10.1109/TMM.2019.2946475},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1591-1604},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uni-and-bi-directional video prediction via learning object-centric transformation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coarse-to-fine localization of temporal action proposals.
<em>TMM</em>, <em>22</em>(6), 1577–1590. (<a
href="https://doi.org/10.1109/TMM.2019.2943204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Localizing temporal action proposals from long videos is a fundamental challenge in video analysis (e.g., action detection and recognition or dense video captioning). Most existing approaches often overlook the hierarchical granularities of actions and thus fail to discriminate fine-grained action proposals (e.g., hand washing laundry or changing a tire in vehicle repair). In this paper, we propose a novel coarse-to-fine temporal proposal (CFTP) approach to localize temporal action proposals by exploring different action granularities. Our proposed CFTP consists of three stages: a coarse proposal network (CPN) to generate long action proposals, a temporal convolutional anchor network (CAN) to localize finer proposals, and a proposal reranking network (PRN) to further identify proposals from previous stages. Specifically, CPN explores three complementary actionness curves (namely pointwise, pairwise, and recurrent curves) that represent actions at different levels for generating coarse proposals, while CAN refines these proposals by a multiscale cascaded 1D-convolutional anchor network. In contrast to existing works, our coarse-to-fine approach can progressively localize fine-grained action proposals. We conduct extensive experiments on two action benchmarks (THUMOS14 and ActivityNet v1.3) and demonstrate the superior performance of our approach when compared to the state-of-the-art techniques on various video understanding tasks.},
  archive      = {J_TMM},
  author       = {Fuchen Long and Ting Yao and Zhaofan Qiu and Xinmei Tian and Tao Mei and Jiebo Luo},
  doi          = {10.1109/TMM.2019.2943204},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1577-1590},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Coarse-to-fine localization of temporal action proposals},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensor-augmented neural adaptive bitrate video streaming on
UAVs. <em>TMM</em>, <em>22</em>(6), 1567–1576. (<a
href="https://doi.org/10.1109/TMM.2019.2945167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in unmanned aerial vehicle (UAV) technology have revolutionized a broad class of civil and military applications. However, the designs of wireless technologies that enable real-time streaming of high-definition video between UAVs and ground clients present a conundrum. Most existing adaptive bitrate (ABR) algorithms are not optimized for the air-to-ground links, which usually fluctuate dramatically due to the dynamic flight states of the UAV. In this paper, we present SA-ABR, a new sensor-augmented system that generates ABR video streaming algorithms with the assistance of various kinds of inherent sensor data that are used to pilot UAVs. By incorporating the inherent sensor data with network observations, SA-ABR trains a deep reinforcement learning (DRL) model to extract salient features from the flight state information and automatically learn an ABR algorithm to adapt to the varying UAV channel capacity through the training process. SA-ABR does not rely on any assumptions or models about UAV&#39;s flight states or the environment, but instead, it makes decisions by exploiting temporal properties of past throughput through the long short-term memory (LSTM) to adapt itself to a wide range of highly dynamic environments. We have implemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare SA-ABR with a variety of existing state-of-the-art ABR algorithms, and the results show that our system outperforms the best known existing ABR algorithm by 21.4% in terms of the average quality of experience (QoE) reward.},
  archive      = {J_TMM},
  author       = {Xuedou Xiao and Wei Wang and Taobin Chen and Yang Cao and Tao Jiang and Qian Zhang},
  doi          = {10.1109/TMM.2019.2945167},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1567-1576},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Sensor-augmented neural adaptive bitrate video streaming on UAVs},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank regularized multi-representation learning for
fashion compatibility prediction. <em>TMM</em>, <em>22</em>(6),
1555–1566. (<a href="https://doi.org/10.1109/TMM.2019.2944749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The currently flourishing fashion-oriented community websites and the continuous pursuit of fashion have attracted the increased research interest of the fashion analysis community. Many studies show that predicting the compatibility of fashion outfits is a nontrivial task due to the difficulty in capturing the implicit patterns affecting fashion compatibility prediction and the complex relationships presented by raw data. To address these problems, in this paper, we propose a transductive low-rank hypergraph regularizer multiple-representation learning framework (LHMRL), whereby we formulate the processes of feature representation and fashion compatibility prediction in a joint framework. Specifically, we first introduce a low-rank regularized multiple-representation learning framework, in which the lowest-rank multiple representations of samples can be learned to characterize samples from different perspectives. In this framework, we maximize the total difference among multiple representations based on Grassmann manifold theory and incorporate a common hypergraph regularizer to naturally encode the complex relationships between fashion items and an outfit. To enhance the representation ability of our model, we then develop a supervised learning term by exploiting two types of supervision information from labeled data. Experiments on a publicly available large-scale dataset demonstrate the effectiveness of our proposed model over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Peiguang Jing and Shu Ye and Liqiang Nie and Jing Liu and Yuting Su},
  doi          = {10.1109/TMM.2019.2944749},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1555-1566},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Low-rank regularized multi-representation learning for fashion compatibility prediction},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised variational video hashing with 1D-CNN-LSTM
networks. <em>TMM</em>, <em>22</em>(6), 1542–1554. (<a
href="https://doi.org/10.1109/TMM.2019.2946096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing unsupervised video hashing methods generate binary codes by using RNNs in a deterministic manner, which fails to capture the dominant latent variation of videos. In addition, RNN-based video hashing methods suffer the content forgetting of early input frames due to the sequential processing inherency of RNNs, which is detrimental to global information capturing. In this work, we propose an unsupervised variational video hashing (UVVH) method for scalable video retrieval. Our UVVH method aims to capture the salient and global information in a video. Specifically, we introduce a variational autoencoder to learn a probabilistic latent representation of the salient factors of video variations. To better exploit the global information of videos, we design a 1D-CNN-LSTM model. The 1D-CNN-LSTM model processes long frame sequences in a parallel and hierarchical way, and exploits the correlations between frames to reconstruct the frame-level features. As a consequence, the learned hash functions can produce reliable binary codes for video retrieval. We conduct extensive experiments on three widely used benchmark datasets, FCVID, ActivityNet and YFCC to validate the effectiveness of our proposed approach.},
  archive      = {J_TMM},
  author       = {Shuyan Li and Zhixiang Chen and Xiu Li and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TMM.2019.2946096},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1542-1554},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised variational video hashing with 1D-CNN-LSTM networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The importance of context when recommending TV content:
Dataset and algorithms. <em>TMM</em>, <em>22</em>(6), 1531–1541. (<a
href="https://doi.org/10.1109/TMM.2019.2944214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Home entertainment systems feature in a variety of usage scenarios with one or more simultaneous users, for whom the complexity of choosing media to consume has increased rapidly over the last decade. Users&#39; decision processes are complex and highly influenced by contextual settings, but data supporting the development and evaluation of context-aware recommender systems are scarce. In this paper we present a dataset of self-reported TV consumption enriched with contextual information of viewing situations. We show how choice of genre associates with, among others, the number of present users and users&#39; attention levels. Furthermore, we evaluate the performance of predicting chosen genres given different configurations of contextual information, and compare the results to contextless predictions. The results suggest that including contextual features in the prediction cause notable improvements, and both temporal and social context show significant contributions.},
  archive      = {J_TMM},
  author       = {Miklas Strøm Kristoffersen and Sven Ewan Shepstone and Zheng-Hua Tan},
  doi          = {10.1109/TMM.2019.2944214},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1531-1541},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {The importance of context when recommending TV content: Dataset and algorithms},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning scene attribute for scene recognition.
<em>TMM</em>, <em>22</em>(6), 1519–1530. (<a
href="https://doi.org/10.1109/TMM.2019.2944241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene recognition has been a challenging task in the field of computer vision and multimedia for a long time. The current scene recognition works often extract object features and scene features through CNN, and combine these two types of features to obtain complementary and discriminative scene representations. However, when the scene categories are visually similar, the object features might lack of discriminations. Therefore, it may be debatable to consider only object features. In contrast to the existing works, in this paper, we discuss the discrimination of scene attributes in local regions and utilize scene attributes as the complementary features of object and scene features. We extract these visual features from two individual CNN branches, one extracting the global features of the image while the other extracting the features of local regions. Through contextual modeling framework, we aggregate these features and generate more discriminative scene representations, which achieve better performance than the feature aggregation of object and scene. Moreover, we achieve the new state-of-the-art performance on both standard scene recognition benchmarks by aggregating more complementary visual features: MIT67 (88.06%) and SUN397 (74.12%).},
  archive      = {J_TMM},
  author       = {Haitao Zeng and Xinhang Song and Gongwei Chen and Shuqiang Jiang},
  doi          = {10.1109/TMM.2019.2944241},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1519-1530},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning scene attribute for scene recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neighborhood pyramid preserving hashing. <em>TMM</em>,
<em>22</em>(6), 1507–1518. (<a
href="https://doi.org/10.1109/TMM.2019.2943778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we devote our efforts to the approximate nearest neighbour (ANN) search problem and propose a new unsupervised binary hashing method, i.e., Neighbourhood Pyramid preserving Hashing (NPH). We represent the nearest neighbours of each data point in a pyramid, and as the learning objective, we impose that the pyramid neighbourhood in each level is consistently preserved across the original Euclidean space and the transformed Hamming space. The neighbourhood is quantitatively characterized by its size, defined as the average distance from the involved nearest neighbours to the referred data point. Our approach is consistent with the distance-preserving principle of binary hashing and achieves stricter neighbourhood structure preserving over previous graph hashing algorithms. The experiments on several large-scale benchmark datasets demonstrate that NPH achieves promising performances compared with those of the existing state-of-the-art unsupervised binary hashing methods.},
  archive      = {J_TMM},
  author       = {Min Wang and Wengang Zhou and Qi Tian and Houqiang Li},
  doi          = {10.1109/TMM.2019.2943778},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1507-1518},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neighborhood pyramid preserving hashing},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view saliency guided deep neural network for 3-d
object retrieval and classification. <em>TMM</em>, <em>22</em>(6),
1496–1506. (<a href="https://doi.org/10.1109/TMM.2019.2943740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the multi-view saliency guided deep neural network (MVSG-DNN) for 3D object retrieval and classification. This method mainly consists of three key modules. First, the module of model projection rendering is employed to capture the multiple views of one 3D object. Second, the module of visual context learning applies the basic Convolutional Neural Networks for visual feature extraction of individual views and then employs the saliency LSTM to adaptively select the representative views based on multi-view context. Finally, with these information, the module of multi-view representation learning can generate the compile 3D object descriptors with the designed classification LSTM for 3D object retrieval and classification. The proposed MVSG-DNN has two main contributions: 1) It can jointly realize the selection of representative views and the similarity measure by fully exploiting multi-view context; 2) It can discover the discriminative structure of multi-view sequence without constraints of specific camera settings. Consequently, it can support flexible 3D object retrieval and classification for real applications by avoiding the required camera settings. Extensive comparison experiments on ModelNet10, ModelNet40, and ShapeNetCore55 demonstrate the superiority of MVSG-DNN against the state-of-art methods.},
  archive      = {J_TMM},
  author       = {He-Yu Zhou and An-An Liu and Wei-Zhi Nie and Jie Nie},
  doi          = {10.1109/TMM.2019.2943740},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1496-1506},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-view saliency guided deep neural network for 3-D object retrieval and classification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive single image dehazing using joint local-global
illumination adjustment. <em>TMM</em>, <em>22</em>(6), 1485–1495. (<a
href="https://doi.org/10.1109/TMM.2019.2944260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze has a serious impact on the outdoor optical imaging systems, and it will result in image blurring, color shift, and saturation reduction. Recently, many single image dehazing algorithms have been proposed for practical applications, such as surveillance. However, since the widely-used global atmospheric light in image dehazing fails to well describe the local illumination differences of images, these algorithms fail to well adapt to scenes with different haze concentrations and lighting conditions. Therefore, this paper proposes an adaptive single image dehazing algorithm using joint local-global illumination adjustment. A local illumination estimation for hazy image is proposed to replace the global atmospheric light constant in the atmospheric scattering model, and it can better adapt to the local differences of image illumination. Correspondingly, the global atmospheric light constant is proposed to be utilized to adaptively compensate the illumination intensity, which may better overcome the dark illumination problem within the dehazed image. The experimental results demonstrate that the proposed algorithm can outperform the state-of-the-art algorithms in terms of not only the dehazing effect but also the adaptability.},
  archive      = {J_TMM},
  author       = {Hai-Miao Hu and Hongda Zhang and Zichen Zhao and Bo Li and Jin Zheng},
  doi          = {10.1109/TMM.2019.2944260},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1485-1495},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive single image dehazing using joint local-global illumination adjustment},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-direction dictionary learning based depth map
super-resolution with autoregressive modeling. <em>TMM</em>,
<em>22</em>(6), 1470–1484. (<a
href="https://doi.org/10.1109/TMM.2019.2946075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D depth cameras have become more and more popular in recent years. However, depth maps captured by these cameras can hardly be used in 3D reconstruction directly because they often suffer from low resolution and blurring depth discontinuities. Super resolution of depth maps is necessary. In depth maps, the edge areas play more important role and demonstrate distinct geometry directions compared with natural images. However, most existing super-resolution methods ignore this fact, and they can not handle depth edges properly. Motivated by this, we propose a compound method that combines multi-direction dictionary sparse representation and autoregressive (AR) models, so that the depth edges are presented precisely at different levels. In the patch level, the depth edge patches with geometry directions are well represented by the pre-trained multi-directional dictionaries. Compared with a universal dictionary, multiple dictionaries trained from different directional patches can represent the directional depth patch much better. In the finer pixel level, we utilize an adaptive AR model to represent the local correlation patterns in small areas. Extensive experimental results on both synthetic and real datasets demonstrate that, the proposed model outperforms state-of-the-art depth map super-resolution methods in terms of both quantitative metrics and subjective visual quality.},
  archive      = {J_TMM},
  author       = {Jin Wang and Wei Xu and Jian-Feng Cai and Qing Zhu and Yunhui Shi and Baocai Yin},
  doi          = {10.1109/TMM.2019.2946075},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1470-1484},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-direction dictionary learning based depth map super-resolution with autoregressive modeling},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WeGAN: Deep image hashing with weighted generative
adversarial networks. <em>TMM</em>, <em>22</em>(6), 1458–1469. (<a
href="https://doi.org/10.1109/TMM.2019.2947197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hashing has been widely used in image retrieval tasks. Many existing methods generate hashing codes based on image feature representations. They rarely consider the rich information such as image clustering information contained in the image set as well as uncertain relationships between images and tags simultaneously. In this paper, we develop a Weighted Generative Adversarial Networks (WeGAN) to transfer the clustering information of images to construct the hashing code. WeGAN consists three modules: 1) a hashing learning process for transferring knowledge of the image set to hashing codes of single images; 2) by means of hashing codes, a module to generate image content, tag representation, and their joint information which reflects the correlation between the image and the corresponding tags; 3) a discriminator to distinguish the generated data from the original source, and then formulating three loss functions. Different weights are assigned to these loss functions in order to deal with the uncertainties between images and tags. Through introducing the image set to process the image hashing with different tags, WeGAN can naturally provide the information of clustering results, which is useful for image hashing with multi-tags. The generated hashing code has the ability to dynamically process the uncertain relationships between images and tags. Experiments on three challenging datasets show that WeGAN outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yuebin Wang and Liqiang Zhang and Feiping Nie and Xingang Li and Zhijun Chen and Faqiang Wang},
  doi          = {10.1109/TMM.2019.2947197},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1458-1469},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WeGAN: Deep image hashing with weighted generative adversarial networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Light field super-resolution using edge-preserved
graph-based regularization. <em>TMM</em>, <em>22</em>(6), 1447–1457. (<a
href="https://doi.org/10.1109/TMM.2019.2946094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The light field information would be captured through light field cameras and in different directions regarding 3D image view recordings. In this paper, in order to increase the spatio-angular super-resolution quality and to decrease the reconstruction error regarding the light field information, we use a graph-based light field super-resolution strategy. Accordingly, in order to apply the complementary data in the light field views, we use a graph regularizer for the total recovery of the information and an edge-preserving technique that represents an isometry between curves in the 2D manifold and 5D space of the RGB image views. Moreover, the reconstruction of the light field information is based on applying the alternating direction method of multipliers (ADMM) algorithm. Accordingly, a recent enhanced ADMM model has been used in this paper which is denominated as “Plug-and-Play” and permits the user to plug an image reconstruction technique and a denoising methodology as the first and second sub-problems respectively. On that account, we would be able to resolve the light field super-resolution problem considering the graph-based light field structure as the first sub-problem and the edge-preserving technique as the denoising methodology. Consequently, by applying the proposed super-resolution strategy, the super-resolved light field outcome would be more favorable in terms of visual quality and reconstruction errors in comparison with other state-of-the-art methodologies.},
  archive      = {J_TMM},
  author       = {Vahid Khorasani Ghassab and Nizar Bouguila},
  doi          = {10.1109/TMM.2019.2946094},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1447-1457},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Light field super-resolution using edge-preserved graph-based regularization},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 2D pose-based real-time human action recognition with
occlusion-handling. <em>TMM</em>, <em>22</em>(6), 1433–1446. (<a
href="https://doi.org/10.1109/TMM.2019.2944745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Action Recognition (HAR) for CCTV-oriented applications is still a challenging problem. Real-world scenarios HAR implementations is difficult because of the gap between Deep Learning data requirements and what the CCTV-based frameworks can offer in terms of data recording equipments. We propose to reduce this gap by exploiting human poses provided by the OpenPose, which has been already proven to be an effective detector in CCTV-like recordings for tracking applications. Therefore, in this work, we first propose ActionXPose: a novel 2D pose-based approach for pose-level HAR. ActionXPose extracts low- and high-level features from body poses which are provided to a Long Short-Term Memory Neural Network and a 1D Convolutional Neural Network for the classification. We also provide a new dataset, named ISLD, for realistic pose-level HAR in a CCTV-like environment, recorded in the Intelligent Sensing Lab. ActionXPose is extensively tested on ISLD under multiple experimental settings, e.g. Dataset Augmentation and Cross-Dataset setting, as well as revising other existing datasets for HAR. ActionXPose achieves state-of-the-art performance in terms of accuracy, very high robustness to occlusions and missing data, and promising results for practical implementation in real-world applications.},
  archive      = {J_TMM},
  author       = {Federico Angelini and Zeyu Fu and Yang Long and Ling Shao and Syed Mohsen Naqvi},
  doi          = {10.1109/TMM.2019.2944745},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1433-1446},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {2D pose-based real-time human action recognition with occlusion-handling},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interact as you intend: Intention-driven human-object
interaction detection. <em>TMM</em>, <em>22</em>(6), 1423–1432. (<a
href="https://doi.org/10.1109/TMM.2019.2943753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advances in instance-level detection tasks lay strong foundation for genuine comprehension of the visual scenes. However, the ability to fully comprehend a social scene is still in its preliminary stage. In this work, we focus on detecting human-object interactions (HOIs) in social scene images, which is demanding in terms of research and increasingly useful for practical applications. To undertake social tasks interacting with objects, humans direct their attention and move their body based on their intention. Based on this observation, we provide a unique computational perspective to explore human intention in HOI detection. Specifically, the proposed human intention-driven HOI detection (iHOI) framework models human pose with the relative distances from body joints to the object instances. It also utilizes human gaze to guide the attended contextual regions in a weakly-supervised setting. In addition, we propose a hard negative sampling strategy to address the problem of mis-grouping. We perform extensive experiments on two benchmark datasets, namely V-COCO and HICO-DET. The efficacy of each proposed component has also been validated.},
  archive      = {J_TMM},
  author       = {Bingjie Xu and Junnan Li and Yongkang Wong and Qi Zhao and Mohan S. Kankanhalli},
  doi          = {10.1109/TMM.2019.2943753},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1423-1432},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Interact as you intend: Intention-driven human-object interaction detection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A single-image super-resolution method based on
progressive-iterative approximation. <em>TMM</em>, <em>22</em>(6),
1407–1422. (<a href="https://doi.org/10.1109/TMM.2019.2943750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel single image super-resolution (SR) method based on progressive-iterative approximation is proposed. To preserve textures and clear edges, the image SR reconstruction is treated as an image progressive-iterative fitting procedure and achieved by iterative interpolation. Due to different features in different regions, we first employ the nonsubsampled contourlet transform (NSCT) to divide the image into smooth regions, texture regions, and edges. Then, a hybrid interpolation scheme based on curves and surfaces is proposed, which differs from the traditional surface interpolation methods. Specifically, smooth regions are interpolated by the non-uniform rational basis spline (NURBS) surface geometric iteration. To retain textures, control points are increased, and the progressive-iterative approximation of the NURBS surface is employed to interpolate the texture regions. By considering edges in an image as curve segments that are connected by pixels with dramatic changes, we use NURBS curve progressive-iterative approximation to interpolate the edges, which sharpens the edges and can maintain the image edge structure without jaggy and block artifacts. The experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art methods in terms of both subjective and objective measures.},
  archive      = {J_TMM},
  author       = {Yunfeng Zhang and Ping Wang and Fangxun Bao and Xunxiang Yao and Caiming Zhang and Hongwei Lin},
  doi          = {10.1109/TMM.2019.2943750},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1407-1422},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A single-image super-resolution method based on progressive-iterative approximation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rate constrained multiple-QP optimization for HEVC.
<em>TMM</em>, <em>22</em>(6), 1395–1406. (<a
href="https://doi.org/10.1109/TMM.2019.2947351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In High Efficiency Video Coding (HEVC), multiple-QP (quantization parameter) optimization can adapt to a local video content. However, the multiple-QP implementation in the HEVC reference software (HM 16.6) achieves the best QP value for each coding block with a large amount of computational complexity. To address this challenge, we propose a fast rate-constrained multiple-QP optimization approach for the HM platform. We first introduce a template-based transform coefficient selection method which can save the overall complexity of entropy coding. In addition, we model the multiple-QP determination as a new rate-constrained optimization problem, and finally, we get a feasible solution with a lower computation overhead. Experimental results show that our method dramatically reduces the average complexity under the all-intra, low-delay and random-access configuration.},
  archive      = {J_TMM},
  author       = {Miaohui Wang and Jian Xiong and Long Xu and Wuyuan Xie and King Ngi Ngan and Jing Qin},
  doi          = {10.1109/TMM.2019.2947351},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1395-1406},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rate constrained multiple-QP optimization for HEVC},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Acoustic scene clustering using joint optimization of deep
embedding learning and clustering iteration. <em>TMM</em>,
<em>22</em>(6), 1385–1394. (<a
href="https://doi.org/10.1109/TMM.2019.2947199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent efforts have been made on acoustic scene classification in the audio signal processing community. In contrast, few studies have been conducted on acoustic scene clustering, which is a newly emerging problem. Acoustic scene clustering aims at merging the audio recordings of the same class of acoustic scene into a single cluster without using prior information and training classifiers. In this study, we propose a method for acoustic scene clustering that jointly optimizes the procedures of feature learning and clustering iteration. In the proposed method, the learned feature is a deep embedding that is extracted from a deep convolutional neural network (CNN), while the clustering algorithm is the agglomerative hierarchical clustering (AHC). We formulate a unified loss function for integrating and optimizing these two procedures. Various features and methods are compared. The experimental results demonstrate that the proposed method outperforms other unsupervised methods in terms of the normalized mutual information and the clustering accuracy. In addition, the deep embedding outperforms many state-of-the-art features.},
  archive      = {J_TMM},
  author       = {Yanxiong Li and Mingle Liu and Wucheng Wang and Yuhan Zhang and Qianhua He},
  doi          = {10.1109/TMM.2019.2947199},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  number       = {6},
  pages        = {1385-1394},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Acoustic scene clustering using joint optimization of deep embedding learning and clustering iteration},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020h). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(5), C3. (<a
href="https://doi.org/10.1109/TMM.2020.2986955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.2986955},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level policy and reward-based deep reinforcement
learning framework for image captioning. <em>TMM</em>, <em>22</em>(5),
1372–1383. (<a href="https://doi.org/10.1109/TMM.2019.2941820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is one of the most challenging tasks in AI because it requires an understanding of both complex visuals and natural language. Because image captioning is essentially a sequential prediction task, recent advances in image captioning have used reinforcement learning (RL) to better explore the dynamics of word-by-word generation. However, the existing RL-based image captioning methods rely primarily on a single policy network and reward function-an approach that is not well matched to the multi-level (word and sentence) and multi-modal (vision and language) nature of the task. To solve this problem, we propose a novel multi-level policy and reward RL framework for image captioning that can be easily integrated with RNN-based captioning models, language metrics, or visual-semantic functions for optimization. Specifically, the proposed framework includes two modules: 1) a multi-level policy network that jointly updates the word- and sentence-level policies for word generation; and 2) a multi-level reward function that collaboratively leverages both a vision-language reward and a language-language reward to guide the policy. Furthermore, we propose a guidance term to bridge the policy and the reward for RL optimization. The extensive experiments on the MSCOCO and Flickr30k datasets and the analyses show that the proposed framework achieves competitive performances on a variety of evaluation metrics. In addition, we conduct ablation studies on multiple variants of the proposed framework and explore several representative image captioning models and metrics for the word-level policy network and the language-language reward function to evaluate the generalization ability of the proposed framework.},
  archive      = {J_TMM},
  author       = {Ning Xu and Hanwang Zhang and An-An Liu and Weizhi Nie and Yuting Su and Jie Nie and Yongdong Zhang},
  doi          = {10.1109/TMM.2019.2941820},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1372-1383},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-level policy and reward-based deep reinforcement learning framework for image captioning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WSCNet: Weakly supervised coupled networks for visual
sentiment classification and detection. <em>TMM</em>, <em>22</em>(5),
1358–1371. (<a href="https://doi.org/10.1109/TMM.2019.2939744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions online. In this paper, we solve the problem of visual sentiment analysis, which is challenging due to the high-level abstraction in the recognition process. Existing methods based on convolutional neural networks learn sentiment representations from the holistic image, despite the fact that different image regions can have different influence on the evoked sentiment. In this paper, we introduce a weakly supervised coupled convolutional network (WSCNet). Our method is dedicated to automatically selecting relevant soft proposals given weak annotations (e.g., global image labels), thereby significantly reducing the annotation burden, and encompasses the following contributions. First, the proposed WSCNet detects a sentiment-specific soft map by training a fully convolutional network with the cross spatial pooling strategy in the detection branch. Second, both the holistic and localized information are utilized by coupling the sentiment map with deep features as semantic vector in the classification branch. The sentiment detection and classification branches are integrated into a unified deep framework optimized in an end-to-end manner. Extensive experiments demonstrate that the proposed WSCNet outperforms the state-of-the-art results on seven benchmark datasets.},
  archive      = {J_TMM},
  author       = {Dongyu She and Jufeng Yang and Ming-Ming Cheng and Yu-Kun Lai and Paul L. Rosin and Liang Wang},
  doi          = {10.1109/TMM.2019.2939744},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1358-1371},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WSCNet: Weakly supervised coupled networks for visual sentiment classification and detection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Part-aware fine-grained object categorization using weakly
supervised part detection network. <em>TMM</em>, <em>22</em>(5),
1345–1357. (<a href="https://doi.org/10.1109/TMM.2019.2939747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained object categorization aims for distinguishing objects of subordinate categories that belong to the same entry-level object category. It is a rapidly developing subfield in multimedia content analysis. The task is challenging due to the facts that (1) training images with ground-truth labels are difficult to obtain, and (2) variations among different subordinate categories are subtle. It is well established that characterizing features of different subordinate categories are located on local parts of object instances. However, manually annotating object parts requires expertise, which is also difficult to generalize to new fine-grained categorization tasks. In this work, we propose a Weakly Supervised Part Detection Network (PartNet) that is able to detect discriminative local parts for the use of fine-grained categorization. A vanilla PartNet builds on top of a base subnetwork two parallel streams of upper network layers, which respectively compute scores of classification probabilities (over subordinate categories) and detection probabilities (over a specified number of discriminative part detectors) for local regions of interest (RoIs). The image-level prediction is obtained by aggregating element-wise products of these region-level probabilities, and meanwhile diverse part detectors can be learned in an end-to-end fashion under the image-level supervision. To generate a diverse set of RoIs as inputs of PartNet, we propose a simple Discretized Part Proposals module (DPP) that directly targets for proposing candidates of discriminative local parts, with no bridging via object-level proposals. Experiments on benchmark datasets of CUB-200-2011, Oxford Flower 102 and Oxford-IIIT Pet show the efficacy of our proposed method for both discriminative part detection and fine-grained categorization. In particular, we achieve the new state-of-the-art performance on CUB-200-2011 and Oxford-IIIT Pet datasets when ground-truth part annotations are not available.},
  archive      = {J_TMM},
  author       = {Yabin Zhang and Kui Jia and Zhixin Wang},
  doi          = {10.1109/TMM.2019.2939747},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1345-1357},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Part-aware fine-grained object categorization using weakly supervised part detection network},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Referring image segmentation by generative adversarial
learning. <em>TMM</em>, <em>22</em>(5), 1333–1344. (<a
href="https://doi.org/10.1109/TMM.2019.2942480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring expression is a kind of language expression being used for referring to particular objects. In this paper, we focus on the problem of image segmentation from natural language referring expressions. Existing works tackle this problem by augmenting the convolutional semantic segmentation networks with an LSTM sentence encoder, which is optimized by a pixel-wise classification loss. We argue that the distribution similarity between the inference and ground truth plays an important role in referring image segmentation. Therefore we introduce a complementary loss considering the consistency between the two distributions. To this end, we propose to train the referring image segmentation model in a generative adversarial fashion, which well addresses the distribution similarity problem. In particular, the proposed adversarial semantic guidance network (ASGN) includes the following advantages: a) more detailed visual information is incorporated by the detail enhancement; b) semantic information counteracts the word embedding impact; c) the proposed adversarial learning approach relieves the distribution inconsistencies. Experimental results on four standard datasets show significant improvements over all the compared baseline models, demonstrating the effectiveness of our method.},
  archive      = {J_TMM},
  author       = {Shuang Qiu and Yao Zhao and Jianbo Jiao and Yunchao Wei and Shikui Wei},
  doi          = {10.1109/TMM.2019.2942480},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1333-1344},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Referring image segmentation by generative adversarial learning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GENPass: A multi-source deep learning model for password
guessing. <em>TMM</em>, <em>22</em>(5), 1323–1332. (<a
href="https://doi.org/10.1109/TMM.2019.2940877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The password has become today&#39;s dominant method of authentication. While brute-force attack methods such as HashCat and John the Ripper have proven unpractical, the research then switches to password guessing. State-of-the-art approaches such as the Markov Model and probabilistic context-free grammar (PCFG) are all based on statistical probability. These approaches require a large amount of calculation, which is time-consuming. Neural networks have proven more accurate and practical in password guessing than traditional methods. However, a raw neural network model is not qualified for cross-site attacks because each dataset has its own features. Our work aims to generalize those leaked passwords and improves the performance in cross-site attacks. In this paper, we propose GENPass, a multi-source deep learning model for generating “general” password. GENPass learns from several datasets and ensures the output wordlist can maintain high accuracy for different datasets using adversarial generation. The password generator of GENPass is PCFG+LSTM (PL). We are the first to combine a neural network with PCFG. Compared with Long short-term memory (LSTM), PL increases the matching rate by 16%–30% in cross-site tests when learning from a single dataset. GENPass uses several PL models to learn datasets and generate passwords. The results demonstrate that the matching rate of GENPass is 20% higher than by simply mixing datasets in the cross-site test. Furthermore, we propose GENPass with probability (GENPass-pro), the updated version of GENPass, which can further increase the matching rate of GENPass.},
  archive      = {J_TMM},
  author       = {Zhiyang Xia and Ping Yi and Yunyu Liu and Bo Jiang and Wei Wang and Ting Zhu},
  doi          = {10.1109/TMM.2019.2940877},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1323-1332},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GENPass: A multi-source deep learning model for password guessing},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online fast adaptive low-rank similarity learning for
cross-modal retrieval. <em>TMM</em>, <em>22</em>(5), 1310–1322. (<a
href="https://doi.org/10.1109/TMM.2019.2942494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic similarity among cross-modal data objects, e.g., similarities between images and texts, are recognized as the bottleneck of cross-modal retrieval. However, existing batch-style correlation learning methods suffer from prohibitive time complexity and extra memory consumption in handling large-scale high dimensional cross-modal data. In this paper, we propose a Cross-Modal Online Low-Rank Similarity function learning (CMOLRS) method, which learns a low-rank bilinear similarity measurement for cross-modal retrieval. We model the cross-modal relations by relative similarities on the training data triplets and formulate the relative relations as convex hinge loss. By adapting the margin in hinge loss with pair-wise distances in feature space and label space, CMOLRS effectively captures the multi-level semantic correlation and adapts to the content divergence among cross-modal data. Imposed with a low-rank constraint, the similarity function is trained by online learning in the manifold of low-rank matrices. The low-rank constraint not only endows the model learning process with faster speed and better scalability, but also improves the model generality. We further propose fast-CMOLRS combining multiple triplets for each query instead of standard process using single triplet at each model update step, which further reduces the times of gradient updates and retractions. Extensive experiments are conducted on four public datasets, and comparisons with state-of-the-art methods show the effectiveness and efficiency of our approach.},
  archive      = {J_TMM},
  author       = {Yiling Wu and Shuhui Wang and Qingming Huang},
  doi          = {10.1109/TMM.2019.2942494},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1310-1322},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Online fast adaptive low-rank similarity learning for cross-modal retrieval},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Study on 2D feature-based hash learning. <em>TMM</em>,
<em>22</em>(5), 1298–1309. (<a
href="https://doi.org/10.1109/TMM.2019.2940875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is an important topic in image processing, as it can help save a considerable amount of storage and computational cost. Recently, inspired by 2D strategies employed in other areas of image processing, such as feature extraction, some 2D-based hashing methods were proposed. Related papers have shown that these methods may have better image retrieval performance in terms of both effectiveness and efficiency. However, the difference in the retrieval performances of hashing methods resulting from different forms of input (1D or 2D) has not been previously studied. Whether the widely used bilinear strategy in 2D-based hashing can truly help improve the retrieval precision has not been investigated in existing research. In this paper, we conduct a comparison study on 1D and 2D feature-based hashing methods and attempt to theoretically and experimentally analyse the differences in using 1D and 2D features in hashing. Furthermore, two new hashing methods are proposed for conducting the comparison experiments. Through a comprehensive study, we obtain three main conclusions in this paper: 1) Linear projection on 1D features and bilinear projection on 2D features are essentially the same. 2) 2D-based hashing methods are obviously more efficient than 1D-based methods for analysing high-dimensional input features. 3) 2D-based hashing methods show generally better performance for solving small sample size problems.},
  archive      = {J_TMM},
  author       = {Yujuan Ding and Wai Keung Wong and Zhihui Lai and Yudong Chen},
  doi          = {10.1109/TMM.2019.2940875},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1298-1309},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Study on 2D feature-based hash learning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep metric learning with density adaptivity. <em>TMM</em>,
<em>22</em>(5), 1285–1297. (<a
href="https://doi.org/10.1109/TMM.2019.2939711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of distance metric learning is mostly considered from the perspective of learning an embedding space, where the distances between pairs of examples are in correspondence with a similarity metric. With the rise and success of Convolutional Neural Networks (CNN), deep metric learning (DML) involves training a network to learn a nonlinear transformation to the embedding space. Existing DML approaches often express the supervision through maximizing inter-class distance and minimizing intra-class variation. However, the results can suffer from overfitting problem, especially when the training examples of each class are embedded together tightly and the density of each class is very high. In this paper, we integrate density, i.e., the measure of data concentration in the representation, into the optimization of DML frameworks to adaptively balance inter-class similarity and intra-class variation by training the architecture in an end-to-end manner. Technically, the knowledge of density is employed as a regularizer, which is pluggable to any DML architecture with different objective functions such as contrastive loss, N-pair loss and triplet loss. Extensive experiments on three public datasets consistently demonstrate clear improvements by amending three types of embedding with the density adaptivity. More remarkably, our proposal increases Recall@1 from 67.95% to 77.62%, from 52.01% to 55.64% and from 68.20% to 70.56% on Cars196, CUB-200-2011 and Stanford Online Products dataset, respectively.},
  archive      = {J_TMM},
  author       = {Yehao Li and Ting Yao and Yingwei Pan and Hongyang Chao and Tao Mei},
  doi          = {10.1109/TMM.2019.2939711},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1285-1297},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep metric learning with density adaptivity},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flickr image community analytics by deep noise-refined
matrix factorization. <em>TMM</em>, <em>22</em>(5), 1273–1284. (<a
href="https://doi.org/10.1109/TMM.2019.2938664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately categorizing Flickr images into multiple pre-defined communities (e.g., “architecture” and “peaceful”) is an indispensable technique in multimedia analysis, graphic design, fashion recommendation, etc. In practice, these communities are constructed and updated manually, which is subjective and intolerably time consuming. To alleviate these shortcomings, a noise-refined deep matrix factorization (MF) framework is proposed to intelligently discover communities from million-scale Flickr users, wherein the semantic tag correlations and community correlations are simultaneously encoded. More specifically, it is believable that Flickr communities are high-level clues on the basis of human visual semantic perception. Thereby, a MF algorithm is employed to approximate the community label matrix by the product of pairwise factor matrices, which represent the latent representations of user-provided tags and the corresponding basis matrix respectively. Subsequently, an end-to-end deep model is formulated to hierarchically derive the latent deep representation from raw image pixels to semantic tags. To robustly handle contaminated image semantic tags and community labels, an l 1 norm constraint is encoded to enhance the MF. Meanwhile, to optimally exploit the rich context information of Flickr images, the intrinsic structure between image semantic tags and between communities are collaboratively captured. Finally, the upgraded MF and the deep model are seamlessly combined into a unified framework, which is solved by an iterative algorithm. Experiments on 2 M Flickr images have demonstrated the superiority of our approach. Besides, the discovered Flickr communities can improve photo retargeting and visual aesthetics assessment significantly.},
  archive      = {J_TMM},
  author       = {Luming Zhang and Jianwei Yin and Ping Li and Yongheng Shang and Roger Zimmermann and Ling Shao},
  doi          = {10.1109/TMM.2019.2938664},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1273-1284},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Flickr image community analytics by deep noise-refined matrix factorization},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind night-time image quality assessment: Subjective and
objective approaches. <em>TMM</em>, <em>22</em>(5), 1259–1272. (<a
href="https://doi.org/10.1109/TMM.2019.2938612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image quality assessment (BIQA) aims to develop quantitative measures to automatically and accurately estimate the visual quality of an image without any prior information about its reference image. This issue has been attracting a great deal of attention for a long time; however, little work has been done on night-time images, which are crucially important for consumer photography and practical applications such as automated driving systems. In this paper, to the best of our knowledge, we conduct the first exploration on subjective and objective quality assessment of night-time images. First, we build a large-scale natural night-time image database (NNID) containing 2240 images with 448 different image contents captured by different photographic equipment in real-world scenarios. Subsequently, we carry out a subjective experiment to evaluate the perceptual quality of all the images in the NNID database. Thereafter, we perform objective assessment of night-time images by proposing a blind night-time image quality assessment metric using brightness and texture features (BNBT). Finally, extensive experiments are conducted to evaluate the performance and efficiency of the proposed BNBT metric on the NNID database. The experimental results demonstrate that this metric outperforms existing state-of-the-art BIQA methods in terms of all evaluation criteria and has an acceptable computational cost at the same time. We have made the NNID database publicly available for downloading at https://sites.google.com/site/xiangtaooo/.},
  archive      = {J_TMM},
  author       = {Tao Xiang and Ying Yang and Shangwei Guo},
  doi          = {10.1109/TMM.2019.2938612},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1259-1272},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind night-time image quality assessment: Subjective and objective approaches},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How do we experience crossmodal correspondent mulsemedia
content? <em>TMM</em>, <em>22</em>(5), 1249–1258. (<a
href="https://doi.org/10.1109/TMM.2019.2941274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensory studies emerged as a significant influence upon Human Computer Interaction and traditional multimedia. Mulsemedia is an area that extends multimedia addressing issues of multisensorial response through the combination of at least three media, typically a non-traditional media with traditional audio-visual content. In this paper, we explore the concepts of Quality of Experience and crossmodal correspondences through a case study of different types of mulsemedia setups. The content is designed following principles of crossmodal correspondence between different sensory dimensions and delivered through olfactory, auditory and vibrotactile displays. The Quality of Experience is evaluated through both subjective (questionnaire) and objective means (eye gaze and heart rate). Results show that the auditory experience has an influence on the olfactory sensorial responses and lessens the perception of lingering odor. Heat maps of the eye gazes suggest that the crossmodality between olfactory and visual content leads to an increased visual attention on the factors of the employed crossmodal correspondence (e.g., color, brightness, shape).},
  archive      = {J_TMM},
  author       = {Alexandra Covaci and Estêvão Bissoli Saleme and Gebremariam Mesfin and Nadia Hussain and Elahe Kani-Zabihi and Gheorghita Ghinea},
  doi          = {10.1109/TMM.2019.2941274},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1249-1258},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {How do we experience crossmodal correspondent mulsemedia content?},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Disentangled spectrum variations networks for NIR–VIS face
recognition. <em>TMM</em>, <em>22</em>(5), 1234–1248. (<a
href="https://doi.org/10.1109/TMM.2019.2938685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surveillance cameras often capture near infrared images since it provides a low-cost and effective solution to acquire high-quality images under low-light environments. However, visual versus near infrared (VIS-NIR) heterogeneous face recognition (HFR) is still a challenging issue in computer vision community due to the gap between sensing patterns of different spectrums as well as the lack of sufficient training samples. To solve the above problem, in this paper, we present an effective Disentangled Spectrum Variations Networks (DSVNs) for VISNIR HFR. Two key strategies are introduced to the DSVNs for disentangling spectrum variations between two domains: Spectrum-adversarial Discriminative Feature Learning (SaDFL) and Step-wise Spectrum Orthogonal Decomposition (SSOD). The SaDFL consists of Identity-Discriminative subnetwork (IDNet) and Auxiliary Spectrum Adversarial subnetwork (ASANet). On the one hand, the IDNet is composed of a generator G H and a discriminator D U for extracting identity-discriminative feature. On the other hand, the ASANet is built by a generator G H and a discriminator D M for eliminating modality-variant spectrum information under the guidance of the discriminator D M . The identity-label and modality-label HFR datasets are used to train the DSVNs with triplet loss. Both IDNet and ASANet can jointly enhance the domain-invariant feature representations via an adversarial learning. Furthermore, to disentangle spectrum variations effectively as well as making identity information and modality information unrelated to each other, we present a new topology of connection block called Disentangled Spectrum Variations (DSV). An orthogonality constraint is imposed to DSV at the convolution level for channel-wise orthogonal decomposition between the modality-invariant identity information and modalityvariant spectrum information. In particular, the SSOD is built by stacking multiple modularized mirco-block DSV, and thereby enjoys the benefits of disentangling spectrum variation step by step. Moreover, we investigate the similarity calculation method to further improve the HFR performance. To sum up, the designed DSVNs leads to a purification of identity information as well as an elimination of modality information. Extensive experiments are carried out on two challenging NIR-VIS HFR datasets CASIA NIRVIS 2.0 and Oulu-CASIA NIR-VIS, demonstrating the superiority of the proposed method.},
  archive      = {J_TMM},
  author       = {Weipeng Hu and Haifeng Hu},
  doi          = {10.1109/TMM.2019.2938685},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1234-1248},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentangled spectrum variations networks for NIR–VIS face recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contextualized CNN for scene-aware depth estimation from
single RGB image. <em>TMM</em>, <em>22</em>(5), 1220–1233. (<a
href="https://doi.org/10.1109/TMM.2019.2941776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directly benefited from deep learning techniques, depth estimation from single image has gained great momentum in recent years. However, most of the existing approaches treat depth prediction as an isolated problem without taking into consideration high-level semantic context information, which results in inefficient utilization of training dataset and unavoidably requires a large number of captured depth data during the training phase. To ameliorate, this paper develops a novel scene-aware contextualized convolution neural network (CCNN), which characterizes the semantic context relationship at the class-level and refines depth at the pixel-level. Our newly-proposed CCNN is built upon the intrinsic exploitation of context-dependent depth association, including inner-object continuous depth and inter-object depth change priors nearby. Specifically, rather than conducting regression on depth in single CNN, we make the first attempt to integrate both class-level and pixel-level conditional random fields (CRFs) based probabilistic graphical model into the powerful CNN framework to simultaneously learn different-level features within the same CNN layer. With our CCNN, the former model will guide the latter one to learn the contextualized RGB-Depth mapping. Hence, CCNN has desirable properties in both class-level integrity and pixel-level discrimination, which makes it ideal to share such two-level convolutional features in parallel during the end-to-end training with the commonly-used back-propagation algorithm. We conduct extensive experiments and comprehensive evaluations on public benchmarks involving various indoor and outdoor scenes, and all the experiments confirm that, our method outperforms the state-of-the-art depth estimation methods, especially for the cases where only small-scale training data are readily available.},
  archive      = {J_TMM},
  author       = {Wenfeng Song and Shuai Li and Ji Liu and Aimin Hao and Qinping Zhao and Hong Qin},
  doi          = {10.1109/TMM.2019.2941776},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1220-1233},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Contextualized CNN for scene-aware depth estimation from single RGB image},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MSTGAR: Multioperator-based stereoscopic thumbnail
generation with arbitrary resolution. <em>TMM</em>, <em>22</em>(5),
1208–1219. (<a href="https://doi.org/10.1109/TMM.2019.2939707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, thumbnail generation for 2D images has been extensively studied, but the research in thumbnail generation for stereoscopic images is still relatively lacking. This paper presents a novel thumbnail generation technology for stereoscopic images based on multioperator with the following innovations: 1) The warping technique is used to retarget a stereopair into six-scale resolutions with different contexts, and the disparity is uniformly adjusted to a certain value based on just noticeable depth difference (JNDD) model, which overcomes the issues that 3D perception in stereoscopic thumbnail is uncontrollable and the sense of depth disappears in low-resolution stereoscopic images. 2) The six-scale images are cropped via cropping network, and are optimized to a target resolution based on the designed image visual representation energy. As a result, our method has better visual effect than state-of-the-art methods in generating thumbnail for stereoscopic display.},
  archive      = {J_TMM},
  author       = {Xiongli Chai and Feng Shao and Qiuping Jiang and Yo-Sung Ho},
  doi          = {10.1109/TMM.2019.2939707},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1208-1219},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MSTGAR: Multioperator-based stereoscopic thumbnail generation with arbitrary resolution},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Latency-aware adaptive video summarization for mobile edge
clouds. <em>TMM</em>, <em>22</em>(5), 1193–1207. (<a
href="https://doi.org/10.1109/TMM.2019.2939753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the technological advances in wireless multimedia domain, these videos made by mobile edge devices dominate network traffics. The video summarization technology enables users to understand the storyline of a video before a client requests the complete video content. Summarizing a video on edge devices and transmitting the summary between them requires a user-oriented and adaptive solution due to the limited capability and the dynamic wireless links of edge devices. Therefore, it is beneficial to improve the user&#39;s viewing experience and the bandwidth utilization ratio if we generate and transmit a video summary based on network connections and the user&#39;s tolerant latency. Unfortunately, previous summarization approaches are incapable of adjusting the summary size adapted to the varying network bandwidth and the user&#39;s attitude towards latency. To timely and flexibly deal with mobile videos, we first formulate the video summarization optimization problem with the elastic number of selected representative segments and the outlier detection within a bounded time budget. Furthermore, we develop an online greedy algorithm called the Elastic Video Summarization Algorithm (EVS) to solve the NP hard problem. We analyze the properties associated with EVS and further design an improved EVS-II to reduce computation complexity. Finally, the experimental results demonstrate that our proposed algorithms outperform other existing researches in fitting network bandwidth and detecting outliers.},
  archive      = {J_TMM},
  author       = {Ying Wang and Yifan Dong and Songtao Guo and Yuanyuan Yang and Xiaofeng Liao},
  doi          = {10.1109/TMM.2019.2939753},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1193-1207},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Latency-aware adaptive video summarization for mobile edge clouds},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical coding of convolutional features for scene
recognition. <em>TMM</em>, <em>22</em>(5), 1182–1192. (<a
href="https://doi.org/10.1109/TMM.2019.2942478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved great success in visual recognition because of the availability of large-scale image datasets, such as the ImageNet. The transfer of convolutional features to challenging scene recognition remains an open problem. Multiple non-linear transforms endow the convolutional features with abundant information. On the other side, CNNs are adept at capturing the holistic appearances of scenes, whereas the lack of some critical local details may reduce the recognition accuracy. To address these problems, we propose a novel hierarchical coding algorithm to learn effective representations. To adapt the scale variations, many useful patches with various scales sampled from the whole image are considered to provide the sufficient details. Non-negative sparse decomposition model (NNSD) based on convolutional features is proposed to learn the sharable components for each scale and further produce global signatures. Based on the global signatures, inter-class linear coding (ICLC) is proposed to learn the discriminative components and ultimate image representations. Experimental results indicate that our approach significantly improves the recognition accuracy compared with general CNN models and achieves excellent performance on five standard benchmarks.},
  archive      = {J_TMM},
  author       = {Lin Xie and Feifei Lee and Li Liu and Zhong Yin and Qiu Chen},
  doi          = {10.1109/TMM.2019.2942478},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1182-1192},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical coding of convolutional features for scene recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reassembling shredded document stripes using word-path
metric and greedy composition optimal matching solver. <em>TMM</em>,
<em>22</em>(5), 1168–1181. (<a
href="https://doi.org/10.1109/TMM.2019.2941777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a shredded document reassembly algorithm based on character/word detection. A new word compatibility estimation metric and a searching strategy called Greedy Composition and Optimal Matching (GCOM) are proposed to compose documents from their vertically shredded stripes. We reduce the stripe puzzle reassembly problem to the traveling salesman problem (TSP) on a sparse graph. The word-path compatibility metric takes advantages of the optical character recognition (OCR) to compute the compatibility score among a group of stripes. The global composition strategy, based on an integration of greedy composition and optimal matching, is proposed to search for a maximal Hamiltonian path and the final global reassembly. We demonstrate that our solver outperforms the state-of-the-art puzzle solvers on reassembling stripe shredded documents.},
  archive      = {J_TMM},
  author       = {Yongqing Liang and Xin Li},
  doi          = {10.1109/TMM.2019.2941777},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1168-1181},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reassembling shredded document stripes using word-path metric and greedy composition optimal matching solver},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate and robust video saliency detection via self-paced
diffusion. <em>TMM</em>, <em>22</em>(5), 1153–1167. (<a
href="https://doi.org/10.1109/TMM.2019.2940851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional video saliency detection methods frequently follow the common bottom-up thread to estimate video saliency within the short-term fashion. As a result, such methods can not avoid the obstinate accumulation of errors when the collected low-level clues are constantly ill-detected. Also, being noticed that a portion of video frames, which are not nearby the current video frame over the time axis, may potentially benefit the saliency detection in the current video frame. Thus, we propose to solve the aforementioned problem using our newly-designed key frame strategy (KFS), whose core rationale is to utilize both the spatial-temporal coherency of the salient foregrounds and the objectness prior (i.e., how likely it is for an object proposal to contain an object of any class) to reveal the valuable long-term information. We could utilize all this newly-revealed long-term information to guide our subsequent “self-paced” saliency diffusion, which enables each key frame itself to determine its diffusion range and diffusion strength to correct those ill-detected video frames. At the algorithmic level, we first divide a video sequence into short-term frame batches, and the object proposals are obtained in a frame-wise manner. Then, for each object proposal, we utilize a pre-trained deep saliency model to obtain high-dimensional features in order to represent the spatial contrast. Since the contrast computation within multiple neighbored video frames (i.e., the non-local manner) is relatively insensitive to the appearance variation, those object proposals with high-quality low-level saliency estimation frequently exhibit strong similarity over the temporal scale. Next, the long-term common consistency (e.g., appearance models/movement patterns) of the salient foregrounds could be explicitly revealed via similarity analysis accordingly. We further boost the detection accuracy via long-term information guided saliency diffusion in a self-paced manner. We have conducted extensive experiments to compare our method with 16 state-of-the-art methods over 4 largest public available benchmarks, and all results demonstrate the superiority of our method in terms of both accuracy and robustness.},
  archive      = {J_TMM},
  author       = {Yunxiao Li and Shuai Li and Chenglizhao Chen and Aimin Hao and Hong Qin},
  doi          = {10.1109/TMM.2019.2940851},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1153-1167},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Accurate and robust video saliency detection via self-paced diffusion},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Patch-based image hallucination for super resolution with
detail reconstruction from similar sample images. <em>TMM</em>,
<em>22</em>(5), 1139–1152. (<a
href="https://doi.org/10.1109/TMM.2019.2938911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hallucination and super-resolution have been studied for decades, and many approaches have been proposed to upsample low-resolution images using information from the images themselves, multiple example images, or large image databases. However, most of this work has focused exclusively on small magnification levels because the algorithms simply sharpen the blurry edges in the upsampled images – no actual new detail is typically reconstructed in the final result. In this paper, we present a patch-based algorithm for image hallucination which, for the first time, properly synthesizes novel high frequency detail. To do this, we pose the synthesis problem as a patch-based optimization which inserts coherent, high-frequency detail from contextually-similar images of the same physical scene/subject provided from either a personal image collection or a large online database. The resulting image is visually plausible and contains coherent high frequency information. We demonstrate the robustness of our algorithm by testing it on a large number of images and show that its performance is considerably superior to all state-of-the-art approaches, a result that is verified to be statistically significant through a randomized user study.},
  archive      = {J_TMM},
  author       = {Chieh-Chi Kao and Yuxiang Wang and Jonathan Waltman and Pradeep Sen},
  doi          = {10.1109/TMM.2019.2938911},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1139-1152},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Patch-based image hallucination for super resolution with detail reconstruction from similar sample images},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Screen content compression based on enhanced soft context
formation. <em>TMM</em>, <em>22</em>(5), 1126–1138. (<a
href="https://doi.org/10.1109/TMM.2019.2941270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The compression of screen content has attracted the interest of researchers in the last years as the market for transferring data from computer displays is growing. It has already been shown that especially those methods can effectively compress screen content which are able to predict the probability distribution of next pixel values. This prediction is typically based on a kind of learning process. The predictor learns the relationship between probable pixel colours and surrounding texture. Recently, an effective method called `soft context formation&#39; (SCF) had been proposed which achieves much lower bitrates for images with less than 8 000 colours than other state-of-the-art compression schemes. This paper presents an enhanced version of SCF. The average lossless compression performance has increased by about 5% in application to images with less than 8 000 colours and about 10% for images with up to 90 000 colours. In comparison to FLIF, FP8v3, and HEVC (HM - 16.20 + SCM - 8.8), it achieves savings of about 33%, 4%, and 11% on average. The improvements compared to the original version result from various modifications. The largest contribution is achieved by the local estimation of the probability distribution for unpredictable colours in stage II of the compression scheme.},
  archive      = {J_TMM},
  author       = {Tilo Strutz and Phillip Möller},
  doi          = {10.1109/TMM.2019.2941270},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1126-1138},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Screen content compression based on enhanced soft context formation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distinct feature extraction for video-based gait phase
classification. <em>TMM</em>, <em>22</em>(5), 1113–1125. (<a
href="https://doi.org/10.1109/TMM.2019.2942479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in image acquisition and analysis have resulted in disruptive innovation in physical rehabilitation systems facilitating cost-effective, portable, video-based gait assessment. While these inexpensive motion capture systems, suitable for home rehabilitation, do not generally provide accurate kinematics measurements on their own, image processing algorithms ensure gait analysis that is accurate enough for rehabilitation programs. This paper proposes high-accuracy classification of gait phases and muscle actions, using readings from low-cost motion capture systems. First, 12 gait parameters, drawn from the medical literature, are defined to characterize gait patterns. These proposed parameters are then used as input to our proposed multi-channel time-series classification and gait phase reconstruction methods. Proposed methods fully utilize temporal information of gait parameters, thus improving the final classification accuracy. The validation, conducted using 126 experiments, with 6 healthy volunteers and 9 stroke survivors with manually-labelled gait phases, achieves state-of-art classification accuracy of gait phase with lower computational complexity compared to previous solutions. 1},
  archive      = {J_TMM},
  author       = {Minxiang Ye and Cheng Yang and Vladimir Stankovic and Lina Stankovic and Samuel Cheng},
  doi          = {10.1109/TMM.2019.2942479},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  number       = {5},
  pages        = {1113-1125},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Distinct feature extraction for video-based gait phase classification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020i). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(4), C3. (<a
href="https://doi.org/10.1109/TMM.2020.2980725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.2980725},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Corrections to “spatiotemporal recurrent convolutional
networks for recognizing spontaneous micro-expressions.” <em>TMM</em>,
<em>22</em>(4), 1111. (<a
href="https://doi.org/10.1109/TMM.2020.2980722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents corrections to the author&#39;s information in the above named paper.},
  archive      = {J_TMM},
  author       = {Zhaoqiang Xia and Xiaopeng Hong and Xingyu Gao and Xiaoyi Feng and Guoying Zhao},
  doi          = {10.1109/TMM.2020.2980722},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1111},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Corrections to “Spatiotemporal recurrent convolutional networks for recognizing spontaneous micro-expressions”},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sentiment recognition for short annotated GIFs using
visual-textual fusion. <em>TMM</em>, <em>22</em>(4), 1098–1110. (<a
href="https://doi.org/10.1109/TMM.2019.2936805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of social media, visual sentiment analysis from image or video has become a hot spot in visual understanding researches. In this work, we propose an effective approach using visual and textual fusion for sentiment analysis of short GIF videos with textual descriptions. We extract both sequence-level and frame-level visual features for each given GIF video. Next, we build a visual sentiment classifier by using the extracted features. We also define a mapping function, which converts the sentiment probability from the classifier to a sentiment score used in our fusion function. At the same time, for the accompanied textual annotations, we employ the Synset forest to extract the sets of the meaningful sentiment words and utilize the SentiWordNet3.0 model to obtain the textual sentiment score. Then, we design a joint visual-textual sentiment score function weighted with visual sentiment component and textual sentiment one. To make the function more robust, we introduce a noticeable difference threshold to further process the fused sentiment score. Finally, we adopt a grid search technique to obtain relevant model hyper-parameters by optimizing a sentiment aware score function. Experimental results and analysis extensively demonstrate the effectiveness of the proposed sentiment recognition scheme on three benchmark datasets including T-GIF dataset, GSO-2016 dataset and Adjusted-GIFGIF dataset.},
  archive      = {J_TMM},
  author       = {Tianliang Liu and Junwei Wan and Xiubin Dai and Feng Liu and Quanzeng You and Jiebo Luo},
  doi          = {10.1109/TMM.2019.2936805},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1098-1110},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Sentiment recognition for short annotated GIFs using visual-textual fusion},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge-augmented multimodal deep regression bayesian
networks for emotion video tagging. <em>TMM</em>, <em>22</em>(4),
1084–1097. (<a href="https://doi.org/10.1109/TMM.2019.2934824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immanent dependencies between audio and visual modalities extracted from video content and the well-established film grammar (i.e., domain knowledge) are important for emotion video recognition and regression. However, these tools have yet to be exploited successfully. Therefore, we propose a multimodal deep regression Bayesian network (MMDRBN) to capture the relationship between audio and visual modalities for emotion video tagging. We then modify the structure of the MMDRBN to incorporate domain knowledge. A regression Bayesian network (RBN) is formed from one latent layer, one visible layer and directed links from the latent layer to the visible layer. RBN is able to fully represent the data, since it captures the dependencies not only among the visible variables but also among the latent variables given visible variables. For the MMDRBN, first, we learn several layers of RBNs using audio and visual modalities, and then stack these RBNs to form two deep networks. A joint representation is obtained from the top layers of the two deep networks, capturing the deep dependencies between audio and visual modalities. We also summarize the main audio and visual elements used by filmmakers to convey emotions and formulate them as semantical meaningful middle-level representation, i.e., attributes. Through these attributes, we construct the knowledge-augmented MMDRBN, which learns a hybrid middle-level video representation using video data and the summarized attributes. Experimental results of both emotion recognition and regression from videos on the LIRIS-ACCEDE database demonstrate that the proposed model can successfully capture the intrinsic connections between audio and visual modalities, and integrate the middle-level representation learning from video data and semantical attributes summarized from film grammar. Thus, it achieves superior performance on emotion video tagging compared to state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Shangfei Wang and Longfei Hao and Qiang Ji},
  doi          = {10.1109/TMM.2019.2934824},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1084-1097},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-augmented multimodal deep regression bayesian networks for emotion video tagging},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An SDN-based caching decision policy for video caching in
information-centric networking. <em>TMM</em>, <em>22</em>(4), 1069–1083.
(<a href="https://doi.org/10.1109/TMM.2019.2935683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The considerable increase of multimedia services, such as video-on-demand (VoD) services, is a significant contributor to the total Internet traffic. Software-defined networking (SDN) and information-centric networking (ICN) are two promising technologies that can be combined to facilitate video delivery and to reduce network delays. In this paper, we first formulate the caching decision problem as a 0-1 integer linear programming (ILP) problem. Second, in contrast to existing approaches that solve the formulated ILP problem by assuming all future video requests are known, we consider the impact of the time scale, which transforms the static 0-1 ILP problem into a dynamic problem. By solving the dynamic 0-1 ILP problem, we find more accurate optimal solutions compared to existing approaches. Third, since the formulated 0-1 dynamic ILP problem is NP-hard, we leverage the in-network caching of ICN and the global view of the SDN controller to propose a novel SDN-based caching decision policy. Finally, extensive evaluations are performed, and the results demonstrate that the proposed SDN-based caching decision policy provides solutions that are close to the optimum in substantially less computation time. The SDN-based caching decision policy also outperforms existing practical ICN caching decision policies in terms of the cache hit ratio and the average number of hops, which are directly related to the video delivery latency. Moreover, the SDN-based caching decision policy can substantially reduce the number of generated and broadcasted interest packets, which is a shortcoming of the current ICN.},
  archive      = {J_TMM},
  author       = {Zhe Zhang and Chung-Horng Lung and Marc St-Hilaire and Ioannis Lambadaris},
  doi          = {10.1109/TMM.2019.2935683},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1069-1083},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An SDN-based caching decision policy for video caching in information-centric networking},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible deep CNN framework for image restoration.
<em>TMM</em>, <em>22</em>(4), 1055–1068. (<a
href="https://doi.org/10.1109/TMM.2019.2938340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration is a long-standing problem in image processing and low-level computer vision. Recently, discriminative convolutional neural network (CNN)-based approaches have attracted considerable attention due to their superior performance. However, most of these frameworks are designed for one specific image restoration task; hence, they seldom show high performance on other image restoration tasks. To address this issue, we propose a flexible deep CNN framework that exploits the frequency characteristics of different types of artifacts. Hence, the same approach can be employed for a variety of image restoration tasks by adjusting the architecture. For reducing the artifacts with similar frequency characteristics, a quality enhancement network that adopts residual and recursive learning is proposed. Residual learning is utilized to speed up the training process and boost the performance; recursive learning is adopted to significantly reduce the number of training parameters as well as boost the performance. Moreover, lateral connections transmit the extracted features between different frequency streams via multiple paths. One aggregation network combines the outputs of these streams to further enhance the restored images. We demonstrate the capabilities of the proposed framework with three representative applications: image compression artifacts reduction (CAR), image denoising, and single image super-resolution (SISR). Extensive experiments confirm that the proposed framework outperforms the state-of-the-art approaches on benchmark datasets for these applications.},
  archive      = {J_TMM},
  author       = {Zhi Jin and Muhammad Zafar Iqbal and Dmytro Bobkov and Wenbin Zou and Xia Li and Eckehard Steinbach},
  doi          = {10.1109/TMM.2019.2938340},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1055-1068},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A flexible deep CNN framework for image restoration},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MRFN: Multi-receptive-field network for fast and accurate
single image super-resolution. <em>TMM</em>, <em>22</em>(4), 1042–1054.
(<a href="https://doi.org/10.1109/TMM.2019.2937688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural network (CNN) based models have shown great potential in the task of single image superresolution (SISR). However, many state-of-the-art SISR solutions are reproducing some tricks proven effective in other vision tasks, such as pursuing a deeper model. In this paper, we propose a new solution (named as Multi-Receptive-Field Network - MRFN), which outperforms existing SISR solutions in three different aspects. First, from receptive field: a novel multi-receptive-field (MRF) module is proposed to extract and fuse features in different receptive fields from local to global. Integrating these hierarchical features can generate better mappings on recovering high-fidelity details at different scales. Second, from network architectures: both dense skip connections and deep supervision are utilized to combine features from the current MRF module and preceding ones for training more representative features. Moreover, a deconvolution layer is embedded at the end of the network to avoid artificial priors induced by numerical data pre-processing (e.g., bicubic stretching), and speed up the restoration process. Finally, from error modeling: different from L1 and L2 loss functions, we proposed a novel two-parameter training loss called Weighted Huber loss function which can adaptively adjust the value of back-propagated derivative according to the residual value, thus fit the reconstruction error more effectively. Extensive qualitative and quantitative evaluation results on benchmark datasets demonstrate that our proposed MRFN can achieve more accurate recovering results than most state-of-the-art methods with significantly less complexity.},
  archive      = {J_TMM},
  author       = {Zewei He and Yanpeng Cao and Lei Du and Baobei Xu and Jiangxin Yang and Yanlong Cao and Siliang Tang and Yueting Zhuang},
  doi          = {10.1109/TMM.2019.2937688},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1042-1054},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MRFN: Multi-receptive-field network for fast and accurate single image super-resolution},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Frame augmented alternating attention network for video
question answering. <em>TMM</em>, <em>22</em>(4), 1032–1041. (<a
href="https://doi.org/10.1109/TMM.2019.2935678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision and language understanding is one of the most fundamental and challenging problems in Multimedia Intelligence. Simultaneously understanding video actions with a related natural language question, and further produces accurate answer is even more challenging since it requires joint modeling information across modality. In the past few years, some studies begin to attack this problem by utilizing attention enhanced deep neural networks. However, simple attention mechanisms such as unidirectional attention fail to yield a better mapping between different modalities. Moreover, none of these Video QA models explore high-level semantics in augmented video-frame level. In this paper, we augmented each frame representation with its context information by a novel feature extractor that combines the advantages of Resnet and a variant of C3D. In addition, we proposed a novel alternating attention network which can alternately attend frame regions, video frames and words in the question in multi-turns. This yields better joint representations of video and question, further help the deep model to discover the deeper relationship between two modalities. Our method outperforms the state-of-the-art Video QA models on two existing video question answering datasets. Further ablation studies proved that our feature extractor and the alternating attention mechanism can improve the performance jointly.},
  archive      = {J_TMM},
  author       = {Wenqiao Zhang and Siliang Tang and Yanpeng Cao and Shiliang Pu and Fei Wu and Yueting Zhuang},
  doi          = {10.1109/TMM.2019.2935678},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1032-1041},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frame augmented alternating attention network for video question answering},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep co-saliency detection via stacked autoencoder-enabled
fusion and self-trained CNNs. <em>TMM</em>, <em>22</em>(4), 1016–1031.
(<a href="https://doi.org/10.1109/TMM.2019.2936803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image co-saliency detection via fusion-based or learning-based methods faces cross-cutting issues. Fusion-based methods often combine saliency proposals using a majority voting rule. Their performance hence highly depends on the quality and coherence of individual proposals. Learning-based methods typically require ground-truth annotations for training, which are not available for co-saliency detection. In this work, we present a two-stage approach to address these issues jointly. At the first stage, an unsupervised deep learning model with stacked autoencoder (SAE) is proposed to evaluate the quality of saliency proposals. It employs latent representations for image foregrounds, and auto-encodes foreground consistency and foreground-background distinctiveness in a discriminative way. The resultant model, SAE-enabled fusion ( SAEF ), can combine multiple saliency proposals to yield a more reliable saliency map. At the second stage, motivated by the fact that fusion often leads to over-smoothed saliency maps, we develop self-trained convolutional neural networks ( STCNN ) to alleviate this negative effect. STCNN takes the saliency maps produced by SAEF as inputs. It propagates information from regions of high confidence to those of low confidence. During propagation, feature representations are distilled, resulting in sharper and better co-saliency maps. Our approach is comprehensively evaluated on three benchmarks, including MSRC, iCoseg, and Cosal2015, and performs favorably against the state-of-the-arts. In addition, we demonstrate that our method can be applied to object co-segmentation and object co-localization, achieving the state-of-the-art performance in both applications.},
  archive      = {J_TMM},
  author       = {Chung-Chi Tsai and Kuang-Jui Hsu and Yen-Yu Lin and Xiaoning Qian and Yung-Yu Chuang},
  doi          = {10.1109/TMM.2019.2936803},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1016-1031},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep co-saliency detection via stacked autoencoder-enabled fusion and self-trained CNNs},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-party WebRTC services using delay and bandwidth aware
SDN-assisted IP multicasting of scalable video over 5G networks.
<em>TMM</em>, <em>22</em>(4), 1005–1015. (<a
href="https://doi.org/10.1109/TMM.2019.2937170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, multi-party WebRTC videoconferencing between peers with heterogenous network resources and terminals is enabled over the best-effort Internet using a central selective forwarding unit (SFU), where each peer sends a scalable encoded video stream to the SFU. This connection model avoids the upload bandwidth bottleneck associated with mesh connections; however, it increases peer delay and overall network load (resource consumption) in addition to requiring investment in servers since all video traffic must go through SFU servers. To this effect, we propose a new multi-party WebRTC service model over future 5G networks, where a video service provider (VSP) collaborates with a network service providers (NSP) to offer an NSP-managed service to stream scalable video layers using software-defined networking (SDN)-assisted Internet protocol (IP) multicasting between peers using NSP infrastructure. In the proposed service model, each peer sends a scalable coded video upstream, which is selectively duplicated and forwarded as layer streams at SDN switches in the network, instead of at a central SFU, in a multi-party WebRTC session managed by multicast trees maintained by the SDN controller. Experimental results show that the proposed SDN-assisted IP multicast service architecture is more efficient than the SFU model in terms of end-to-end service delay and overall network resource consumption, while avoiding peer upload bandwidth bottleneck and distributing traffic more evenly across the network. The proposed architecture enables efficient provisioning of premium managed WebRTC services over bandwidth-reserved SDN slices to provide videoconferencing experience with guaranteed video quality over 5G networks.},
  archive      = {J_TMM},
  author       = {Riza Arda Kirmizioglu and A. Murat Tekalp},
  doi          = {10.1109/TMM.2019.2937170},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {1005-1015},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-party WebRTC services using delay and bandwidth aware SDN-assisted IP multicasting of scalable video over 5G networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compact hash code learning with binary deep neural network.
<em>TMM</em>, <em>22</em>(4), 992–1004. (<a
href="https://doi.org/10.1109/TMM.2019.2935680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning compact binary codes for image retrieval problem using deep neural networks has recently attracted increasing attention. However, training deep hashing networks is challenging due to the binary constraints on the hash codes. In this paper, we propose deep network models and learning algorithms for learning binary hash codes given image representations under both unsupervised and supervised manners. The novelty of our network design is that we constrain one hidden layer to directly output the binary codes. This design has overcome a challenging problem in some previous works: optimizing non-smooth objective functions because of binarization. In addition, we propose to incorporate independence and balance properties in the direct and strict forms into the learning schemes. We also include a similarity preserving property in our objective functions. The resulting optimizations involving these binary, independence, and balance constraints are difficult to solve. To tackle this difficulty, we propose to learn the networks with alternating optimization and careful relaxation. Furthermore, by leveraging the powerful capacity of convolutional neural networks, we propose an end-to-end architecture that jointly learns to extract visual features and produce binary hash codes. Experimental results for the benchmark datasets show that the proposed methods compare favorably or outperform the state of the art.},
  archive      = {J_TMM},
  author       = {Thanh-Toan Do and Tuan Hoang and Dang-Khoa Le Tan and Anh-Dzung Doan and Ngai-Man Cheung},
  doi          = {10.1109/TMM.2019.2935680},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {992-1004},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Compact hash code learning with binary deep neural network},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexibly connectable light field system for free view
exploration. <em>TMM</em>, <em>22</em>(4), 980–991. (<a
href="https://doi.org/10.1109/TMM.2019.2934819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional captured-image-based virtual reality (VR) systems have three degrees of freedom (DoFs), where only rotational user motion is tracked for view rendering. This is a major cause of the reduced sense of reality. To increase user immersion levels akin to the real world, 3-DoF+ VR systems that support not only rotational but also translational view changes have been proposed. The light-field (LF) approach is suitable for this type of 3-DoF+ VR because it renders a view from a free view position by simply combining lights. Many previous systems have limited scalability because they assume a single LF for the acquisition and representation of light. One recent work connects multiple LFs at a physical intersection to increase the scalability. However, these fixed connection points limit the renderable view range and the layout of multiple LFs. Furthermore, in conventional single- or multiple-LF systems, the representable ranges of the view positions are highly dependent on the input field-of-view (FOV) of the camera used. In order to realize a wide view exploration range, the above-mentioned limitations must be overcome. This paper proposes a flexible connection scheme for multiple-LF systems taking advantage of the constant radiance of rays in LF theory. The proposed flexibly connectable LF system is able to widen the range of the renderable view position under reasonable conditions of the camera FOV. A light-field unit (LFU) which uses the proposed flexible connection is implemented. The LFU has a square-shaped structure and is thus easily stackable. This offers the advantage of dramatically expanding the scope of view explorations. The proposed LFU achieves 3-DoF+ VR with good quality as well as high scalability. Its cost-performance outcome is also better compared to those in previous works.},
  archive      = {J_TMM},
  author       = {Hyunmin Jung and Hyuk-Jae Lee and Chae Eun Rhee},
  doi          = {10.1109/TMM.2019.2934819},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {980-991},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Flexibly connectable light field system for free view exploration},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning the traditional art of chinese calligraphy via
three-dimensional reconstruction and assessment. <em>TMM</em>,
<em>22</em>(4), 970–979. (<a
href="https://doi.org/10.1109/TMM.2019.2937187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional art of Chinese calligraphy, reflecting the wisdom of the grass-roots community, is the soul of Chinese culture. Just like many other types of craftsmanship, it is part of the historical heritage and is worth conserving, from generation to generation. Since the movements of an ink brush are in a 3D style when Chinese calligraphy is written, they embody “The Power of Beauty,” comprising various reflectance properties and rough-surface geometry. To truly understand the powerful significance and beauty of the art of Chinese calligraphy, in this paper, a 3D calligraphy reconstruction method, based on Photometric Stereo, is designed to capture the detailed appearance of the calligraphy&#39;s 3D surface geometry. For assessment, an Iterative Closest Point (ICP) algorithm is applied for registration of 3D intrinsic shapes between the Chinese calligraphy and the calligraphy fans&#39; handwriting. Through matching these two sets of calligraphy characters, the designed system can give a score to the handwriting of a user. Experiments have been performed on Chinese calligraphy from different historical dynasties to evaluate the effectiveness of the proposed scheme, and experimental results show that the developed system is useful and provides a convenient method of calligraphy appreciation and assessment.},
  archive      = {J_TMM},
  author       = {Muwei Jian and Junyu Dong and Maoguo Gong and Hui Yu and Liqiang Nie and Yilong Yin and Kin-Man Lam},
  doi          = {10.1109/TMM.2019.2937187},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {970-979},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning the traditional art of chinese calligraphy via three-dimensional reconstruction and assessment},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training objective image and video quality estimators using
multiple databases. <em>TMM</em>, <em>22</em>(4), 961–969. (<a
href="https://doi.org/10.1109/TMM.2019.2935687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is an essential part of recent advances in computer science. To fully exploit its potential, ML-based algorithms require a considerable amount of annotated data to be used for training. This represents a severe limitation in the field of image and video quality assessment since obtaining large-scale annotated databases is time-consuming and expensive. Moreover, the resulting quality estimators are mainly restricted only to the usecases included in the dataset used for their training. This paper proposes a strategy allowing for combination of multiple databases for training of objective image and video quality assessment algorithms. Using this strategy, the algorithms can be trained using all of the existing relevant databases together which allows to increase the amount of data-points and usecases in orders of magnitude. The potential of the proposed method is demonstrated by re-training the combination of features from Video Multimethod Assessment Fusion (VMAF) algorithm resulting in the significant improvement of its performance with respect to 20 video databases.},
  archive      = {J_TMM},
  author       = {Lukáš Krasula and Yoann Baveye and Patrick Le Callet},
  doi          = {10.1109/TMM.2019.2935687},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {961-969},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Training objective image and video quality estimators using multiple databases},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An EEG-based study on perception of video distortion under
various content motion conditions. <em>TMM</em>, <em>22</em>(4),
949–960. (<a href="https://doi.org/10.1109/TMM.2019.2934425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human perception sensitivity to video distortion is vital for visual quality assessment (VQA). Different from the perception mechanism of image distortion that has been thoroughly studied, the perception of video distortion is inevitably influenced by motion of dynamic content due to the characteristics of the human visual system (HVS). In this paper, electroencephalography (EEG) is used as a novel psychophysiological method to study the human perception sensitivity to quantification-aroused video distortion under various content motion conditions. For this purpose, we conduct experiments to record the EEG signals of the subjects when they are watching distorted videos. According to the feature analysis of EEG data, the P300 component aroused by human perception of video quality change is selected as the indicator of human perception of distortion. By the means of classification based on linear discriminant analysis (LDA), it is found that the separability of the P300 component, which is measured by the area under curve (AUC) of the receiver operating characteristic (ROC), is positively correlated with the perceptibility of distortion. The correlation provides a valid psychophysiological method, which is exempt from being influenced by subjective bias due to human high-level cognitive activities, for evaluating distortion perceptibility. In addition, the regression analysis results demonstrate a sigmoid-typed quantitative relation between the perceptibility of distortion and separability of the P300 component. Based on such relation, the perceptibility thresholds of distortion corresponding to various content motion speeds are calibrated by EEG signals and it is found that the content motion speed has a significant impact on distortion perceptibility.},
  archive      = {J_TMM},
  author       = {Xiwen Liu and Xiaoming Tao and Mai Xu and Yafeng Zhan and Jianhua Lu},
  doi          = {10.1109/TMM.2019.2934425},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {949-960},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An EEG-based study on perception of video distortion under various content motion conditions},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Audio–visual particle flow SMC-PHD filtering for
multi-speaker tracking. <em>TMM</em>, <em>22</em>(4), 934–948. (<a
href="https://doi.org/10.1109/TMM.2019.2937185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential Monte Carlo probability hypothesis density (SMC-PHD) filtering is a popular method used recently for audio-visual (AV) multi-speaker tracking. However, due to the weight degeneracy problem, the posterior distribution can be represented poorly by the estimated probability, when only a few particles are present around the peak of the likelihood density function. To address this issue, we propose a new framework where particle flow (PF) is used to migrate particles smoothly from the prior to the posterior probability density. We consider both zero and non-zero diffusion particle flows (ZPF/NPF), and developed two new algorithms, AV-ZPF-SMC-PHD and AV-NPF-SMC-PHD, where the speaker states from the previous frames are also considered for particle relocation. The proposed algorithms are compared systematically with several baseline tracking methods using the AV16.3, AVDIAR and CLEAR datasets, and are shown to offer improved tracking accuracy and average effective sample size (ESS).},
  archive      = {J_TMM},
  author       = {Yang Liu and Volkan Kılıç and Jian Guan and Wenwu Wang},
  doi          = {10.1109/TMM.2019.2937185},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {934-948},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Audio–Visual particle flow SMC-PHD filtering for multi-speaker tracking},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vibrotactile quality assessment: Hybrid metric design based
on SNR and SSIM. <em>TMM</em>, <em>22</em>(4), 921–933. (<a
href="https://doi.org/10.1109/TMM.2019.2936305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging mulsemedia (MULtiple SEnsorial MEDIA) introduces new sensorial data (haptic, olfaction, gustation, etc.), significantly augmenting the conventional audio-visual communication. This can be used in many areas, such as immersive entertainment and innovative education. Previous research has been dedicated to evaluating the impact of other sensorial data on conventional multimedia; however, standalone quality evaluation of new sensorial data, especially vibrotactile data (a type of haptic data), has not been covered. To the best of our knowledge, this paper is the first to empirically demonstrate that the common statistical metrics in audio and visual domains, i.e. signal-to-noise ratio (SNR) and Structural SIMilarity (SSIM), are highly correlated with human vibrotactile perception as well. To be specific, we propose a testing protocol for vibrotactile quality evaluation and conduct subjective experiments. The results suggest that SNR and SSIM are applicable to vibrotactile quality assessment. We also consider a practical scenario where the quality of vibrotactile data varies with time. Based on the validation of SNR and SSIM in the first part, we present an objective metric as a hybrid composition of SNR and SSIM. Instead of assessing the quality of data using an overall score, the hybrid metric evaluates the quality in a time-varying manner. Subjective experiments are conducted and the results demonstrate that the correlation coefficient can be significantly increased using the hybrid metric.},
  archive      = {J_TMM},
  author       = {Xun Liu and Mischa Dohler and Yansha Deng},
  doi          = {10.1109/TMM.2019.2936305},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {921-933},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Vibrotactile quality assessment: Hybrid metric design based on SNR and SSIM},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kernelized fuzzy modal variation for local change detection
from video scenes. <em>TMM</em>, <em>22</em>(4), 912–920. (<a
href="https://doi.org/10.1109/TMM.2019.2938342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction (BGS) is a popular scheme epitomized in the state-of-the-art literature on video processing. In this context, a novel online kernelized fuzzy modal variation based background subtraction scheme for detecting local changes from the sequences of image frames is proposed. In the proposed scheme, the time varying background at different instances of time are modeled using fuzzy set theory. The proposed background subtraction scheme, utilizes the fuzzy modal variation as the cost function for fitting the pixel values of the image frames. The use of kernel based modal variation helps in projecting the pixel values in a higher dimensional space, linearly separating them into object and background classes. The results of the proposed technique is verified on different challenging sequences including dynamic background, camera jitter, noise, blurred scene, etc. The proposed technique is successfully tested over several test sequences with two major databases (all sequences) and it provides better results compared to the twenty one existing state-of-the-art techniques.},
  archive      = {J_TMM},
  author       = {Badri Narayan Subudhi and Thangaraj Veerakumar and S. Esakkirajan and Ashish Ghosh},
  doi          = {10.1109/TMM.2019.2938342},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {912-920},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Kernelized fuzzy modal variation for local change detection from video scenes},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing the quality of image tagging using a visio-textual
knowledge base. <em>TMM</em>, <em>22</em>(4), 897–911. (<a
href="https://doi.org/10.1109/TMM.2019.2937181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auto-tagging of images is important for image understanding and for tag-based applications viz. image retrieval, visual question-answering, image captioning, etc. Although existing tagging methods incorporate both visual and textual information to assign/refine tags, they lag in tag-image relevance, completeness, and preciseness, thereby resulting in the unsatisfactory performance of tag-based applications. In order to bridge this gap, we propose a novel framework for tag assignment using knowledge embedding (TAKE) from a proposed external knowledge base, considering properties such as Rarity, Newness, Generality, and Naturalness (RNGN properties). These properties help in providing a rich semantic representation to images. Existing knowledge bases provide multiple types of relations extracted through only one modality, either text or visual, which is not effective in image related applications. We construct a simple yet effective Visio-Textual Knowledge Base (VTKB) with only four relations using reliable resources such as Wikipedia, thesauruses, dictionaries, etc. Our large scale experiments demonstrate that the proposed combination of TAKE and VTKB assigns a large number of high quality tags in comparison to the ConceptNet and ImageNet knowledge bases when used in conjunction with TAKE. Also, the effectiveness of knowledge embedding through VTKB is evaluated for image tagging and tag-based image retrieval (TBIR).},
  archive      = {J_TMM},
  author       = {Chandramani Chaudhary and Poonam Goyal and Dhanashree Nellayi Prasad and Yi-Ping Phoebe Chen},
  doi          = {10.1109/TMM.2019.2937181},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {897-911},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing the quality of image tagging using a visio-textual knowledge base},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Saliency detection via a multiple self-weighted graph-based
manifold ranking. <em>TMM</em>, <em>22</em>(4), 885–896. (<a
href="https://doi.org/10.1109/TMM.2019.2934833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important task in the process of image understanding and analysis, saliency detection has recently received increasing attention. In this paper, we propose an efficient multiple self-weighted graph-based manifold ranking method to construct salient maps. First, we extract several different views of features from superpixels, and generate original salient regions as foreground and background cues using boundary information via multiple graph-based manifold ranking. Furthermore, a set of hyperparameters is learned to distinguish the importance between different graphs, which can be viewed as an adaptive weighting of each graph, and then a centroid graph is generated by using these self-weighted multiple graphs. An iterative algorithm is proposed to simultaneously optimize the hyperparameters as well as the centroid graph connection. Thus, an ideal centroid graph can be obtained, offering a more clear profile of the separated structure. Finally, the saliency maps can be produced with an approximate binary image from the manifold ranking. Extensive experiments have demonstrated our method consistently achieves superior detection performance than several state-of-the-arts.},
  archive      = {J_TMM},
  author       = {Cheng Deng and Xu Yang and Feiping Nie and Dapeng Tao},
  doi          = {10.1109/TMM.2019.2934833},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {885-896},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Saliency detection via a multiple self-weighted graph-based manifold ranking},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reversible data hiding in encrypted images based on
multi-MSB prediction and huffman coding. <em>TMM</em>, <em>22</em>(4),
874–884. (<a href="https://doi.org/10.1109/TMM.2019.2936314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cloud storage and privacy protection, reversible data hiding in encrypted images (RDHEI) has attracted increasing attention as a technology that can: embed additional data in the image encryption domain, ensure that the embedded data can be extracted error-free, and the original image can be restored losslessly. In this paper, a high-capacity RDHEI algorithm based on multi-MSB (most significant bit) prediction and Huffman coding is proposed. At first, multi-MSB of each pixel was predicted adaptively and marked by Huffman coding in the original image. Then, the image was encrypted by a stream cipher method. At last, the vacated space can be used to embed additional data by multi-MSB substitution. Experimental results show that our method achieved higher embedding capacity while comparing with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Zhaoxia Yin and Youzhi Xiang and Xinpeng Zhang},
  doi          = {10.1109/TMM.2019.2936314},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {874-884},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reversible data hiding in encrypted images based on multi-MSB prediction and huffman coding},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy compaction-based image compression using
convolutional AutoEncoder. <em>TMM</em>, <em>22</em>(4), 860–873. (<a
href="https://doi.org/10.1109/TMM.2019.2938345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image compression has been an important research topic for many decades. Recently, deep learning has achieved great success in many computer vision tasks, and its use in image compression has gradually been increasing. In this paper, we present an energy compaction-based image compression architecture using a convolutional autoencoder (CAE) to achieve high coding efficiency. Our main contributions include three aspects: 1) we propose a CAE architecture for image compression by decomposing it into several down(up)sampling operations; 2) for our CAE architecture, we offer a mathematical analysis on the energy compaction property and we are the first work to propose a normalized coding gain metric in neural networks, which can act as a measurement of compression capability; 3) based on the coding gain metric, we propose an energy compaction-based bit allocation method, which adds a regularizer to the loss function during the training stage to help the CAE maximize the coding gain and achieve high compression efficiency. The experimental results demonstrate our proposed method outperforms BPG (HEVC-intra), in terms of the MS-SSIM quality metric. Additionally, we achieve better performance in comparison with existing bit allocation methods, and provide higher coding efficiency compared with state-of-the-art learning compression methods at high bit rates.},
  archive      = {J_TMM},
  author       = {Zhengxue Cheng and Heming Sun and Masaru Takeuchi and Jiro Katto},
  doi          = {10.1109/TMM.2019.2938345},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {860-873},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Energy compaction-based image compression using convolutional AutoEncoder},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Content-based light field image compression method with
gaussian process regression. <em>TMM</em>, <em>22</em>(4), 846–859. (<a
href="https://doi.org/10.1109/TMM.2019.2934426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) imaging enables new possibilities for digital imaging, such as digital refocusing, changing of focus plane, changing of viewpoint, scene-depth estimation, and 3D scene reconstruction, by capturing both spatial and angular information of light rays. However, one main problem in dealing with LF data is its sheer volume. In this context, efficient compression methods are needed for such a particular type of content. In this paper, we propose a content-based LF image-compression method with Gaussian process regression to improve the compression efficiency and accelerate the prediction procedure. First, the LF image is fed to the intra-frame codec of HEVC. In the prediction procedure, the prediction units (PUs) are classified as non-homogenous texture units, homogenous texture units, and visually flat units, based on the content property of the LF image. For each category, we design a corresponding Gaussian process regression (GPR)-based prediction method. Moreover, we propose a classification mechanism to exactly decide to which category the current PU belongs, so as to adjust the trade-off between the computational burden and the LF image coding efficiency. Experimental results demonstrate that the proposed LF image compression method is superior to several other state-of-the-art compression methods in terms of different quality metrics. Furthermore, the proposed method can also achieve a good visual quality of views rendered from decoded LF contents.},
  archive      = {J_TMM},
  author       = {Deyang Liu and Ping An and Ran Ma and Wenfa Zhan and Xinpeng Huang and Ali Abdullah Yahya},
  doi          = {10.1109/TMM.2019.2934426},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {846-859},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Content-based light field image compression method with gaussian process regression},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast depth and inter mode prediction for quality scalable
high efficiency video coding. <em>TMM</em>, <em>22</em>(4), 833–845. (<a
href="https://doi.org/10.1109/TMM.2019.2937240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scalable high efficiency video coding (SHVC) is an extension of high efficiency video coding (HEVC). It introduces multiple layers and inter-layer prediction, thus significantly increases the coding complexity on top of the already complicated HEVC encoder. In inter prediction for quality SHVC, in order to determine the best possible mode at each depth level, a coding tree unit can be recursively split into four depth levels, including merge mode, inter2N×2N, inter2N×N, interN×2N, interN×N, inter2N×nU, inter2N×nD, internL×2N and internRx×2N, intra modes and inter-layer reference (ILR) mode. This can obtain the highest coding efficiency, but also result in very high coding complexity. Therefore, it is crucial to improve coding speed while maintaining coding efficiency. In this research, we have proposed a new depth level and inter mode prediction algorithm for quality SHVC. First, the depth level candidates are predicted based on inter-layer correlation, spatial correlation and its correlation degree. Second, for a given depth candidate, we divide mode prediction into square and non-square mode predictions respectively. Third, in the square mode prediction, ILR and merge modes are predicted according to depth correlation, and early terminated whether residual distribution follows a Gaussian distribution. Moreover, ILR mode, merge mode and inter2N×2N are early terminated based on significant differences in Rate Distortion (RD) costs. Fourth, if the early termination condition cannot be satisfied, non-square modes are further predicted based on significant differences in expected values of residual coefficients. Finally, inter-layer and spatial correlations are combined with residual distribution to examine whether to early terminate depth selection. Experimental results have demonstrated that, on average, the proposed algorithm can achieve a time saving of 71.14%, with a bit rate increase of 1.27%.},
  archive      = {J_TMM},
  author       = {Dayong Wang and Yu Sun and Ce Zhu and Weisheng Li and Frederic Dufaux},
  doi          = {10.1109/TMM.2019.2937240},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  number       = {4},
  pages        = {833-845},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast depth and inter mode prediction for quality scalable high efficiency video coding},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020j). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(3), C3. (<a
href="https://doi.org/10.1109/TMM.2020.2973165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.2973165},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Corrections to “STAT: Spatial-temporal attention mechanism
for video captioning.” <em>TMM</em>, <em>22</em>(3), 830. (<a
href="https://doi.org/10.1109/TMM.2020.2966830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents corrections to affiliations in the above named paper.},
  archive      = {J_TMM},
  author       = {Chenggang Yan and Yunbin Tu and Xingzheng Wang and Yongbing Zhang and Xinhong Hao and Yongdong Zhang and Qionghai Dai},
  doi          = {10.1109/TMM.2020.2966830},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {830},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Corrections to “STAT: Spatial-temporal attention mechanism for video captioning”},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep multi-kernel convolutional LSTM networks and an
attention-based mechanism for videos. <em>TMM</em>, <em>22</em>(3),
819–829. (<a href="https://doi.org/10.1109/TMM.2019.2932564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition greatly benefits motion understanding in video analysis. Recurrent networks such as long short-term memory (LSTM) networks are a popular choice for motion-aware sequence learning tasks. Recently, a convolutional extension of LSTM was proposed, in which input-to-hidden and hidden-to-hidden transitions are modeled through convolution with a single kernel. This implies an unavoidable trade-off between effectiveness and efficiency. Herein, we propose a new enhancement to convolutional LSTM networks that supports accommodation of multiple convolutional kernels and layers. This resembles a Network-in-LSTM approach, which improves upon the aforementioned concern. In addition, we propose an attention-based mechanism that is specifically designed for our multi-kernel extension. We evaluated our proposed extensions in a supervised classification setting on the UCF-101 and Sports-1M datasets, with the findings showing that our enhancements improve accuracy. We also undertook qualitative analysis to reveal the characteristics of our system and the convolutional LSTM baseline.},
  archive      = {J_TMM},
  author       = {Sebastian Agethen and Winston H. Hsu},
  doi          = {10.1109/TMM.2019.2932564},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {819-829},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep multi-kernel convolutional LSTM networks and an attention-based mechanism for videos},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recall what you see continually using GridLSTM in image
captioning. <em>TMM</em>, <em>22</em>(3), 808–818. (<a
href="https://doi.org/10.1109/TMM.2019.2931815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of image captioning is to automatically describe an image with a sentence, and the task has attracted research attention from both the computer vision and natural-language processing research communities. The existing encoder–decoder model and its variants, which are the most popular models for image captioning, use the image features in three ways: first, they inject the encoded image features into the decoder only once at the initial step, which does not enable the rich image content to be explored sufficiently while gradually generating a text caption; second, they concatenate the encoded image features with text as extra inputs at every step, which introduces unnecessary noise; and, third, they using an attention mechanism, which increases the computational complexity due to the introduction of extra neural nets to identify the attention regions. Different from the existing methods, in this paper, we propose a novel network, Recall Network, for generating captions that are consistent with the images. The recall network selectively involves the visual features by using a GridLSTM and, thus, is able to recall image contents while generating each word. By importing the visual information as the latent memory along the depth dimension LSTM, the decoder is able to admit the visual features dynamically through the inherent LSTM structure without adding any extra neural nets or parameters. The Recall Network efficiently prevents the decoder from deviating from the original image content. To verify the efficiency of our model, we conducted exhaustive experiments on full and dense image captioning. The experimental results clearly demonstrate that our recall network outperforms the conventional encoder–decoder model by a large margin and that it performs comparably to the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Lingxiang Wu and Min Xu and Jinqiao Wang and Stuart Perry},
  doi          = {10.1109/TMM.2019.2931815},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {808-818},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Recall what you see continually using GridLSTM in image captioning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unmanned aircraft system aided adaptive video streaming: A
joint optimization approach. <em>TMM</em>, <em>22</em>(3), 795–807. (<a
href="https://doi.org/10.1109/TMM.2019.2931441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the coverage constraint of a wireless base station, mobile users suffer from the unstable network connection and poor service quality, especially for the prevalent video services. As an alternative solution, an unmanned aerial vehicle (UAV) is able to reach the cell edge and serve ground users (GUs). In this paper, we extend the UAV applications to the more challenging adaptive streaming service over fading channel. First, we decompose the system into different modules, and present mathematical models for each of them, including a trajectory model of the UAV, fading channels between the UAV and GUs, and video streaming utility. Second, we formulate the problem as a non-convex optimization problem by optimizing the UAV trajectory and transmit power allocation, jointly with transmission schedule and rate allocation for multiple users. The objective is to maximize the overall utility while guaranteeing the fairness among multiple users under the UAV energy budget and rate-outage probability constraints. Third, to tackle this problem, we first analyze the relationship between transmission rate and rate-outage probability over the fading channel, and then divide the original problem into three subproblems, which can be solved by leveraging the successive convex approximation technique. Furthermore, an overall iterative algorithm over the three subproblems is proposed to obtain a locally optimal solution by applying the block coordinate descent technique. Finally, through extensive experiments, we demonstrate that the proposed design can achieve almost 30% performance gain in terms of max-min streaming utility for all users, compared with other benchmark schemes.},
  archive      = {J_TMM},
  author       = {Cheng Zhan and Han Hu and Zhi Wang and Rongfei Fan and Dusit Niyato},
  doi          = {10.1109/TMM.2019.2931441},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {795-807},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unmanned aircraft system aided adaptive video streaming: A joint optimization approach},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An ultra-low complexity and high efficiency approach for
lossless alpha channel coding. <em>TMM</em>, <em>22</em>(3), 786–794.
(<a href="https://doi.org/10.1109/TMM.2019.2931414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alpha channel is being applied in an increasing number of mobile web applications on mobile devices that require ultra-low power consumption in all cases including compute-intensive video encoding and decoding. Thus, we propose an ultra-low coding complexity and high efficiency alpha channel lossless coding approach. A novel coding framework and four new coding schemes are proposed for alpha channel coding. The framework fuses a string matching technique and a proposed prediction coding scheme named bit-depth preserving prediction (BDPP) together to reduce the correlations within and between repeated identical patterns and neighboring pixels. To achieve a good tradeoff between complexity and efficiency, either the unmatchable bytes are coded directly or the BDPP residuals of unmatchable bytes are coded by a proposed bytewise entropy coding scheme named 0.5-1-2byte-size-code. The other string matching parameters are coded by another proposed bytewise entropy coding scheme named byte-size multi-variable-length-code. To speed up the string-matching search, we apply a fast string search scheme that combines special position search and hash-based search. For the selected typical 236 alpha test images, compared with x265 in the fastest configuration and lossless mode, the proposed lossless approach achieves 14.33% less total compressed bytes with only 2.75% encoding and 1.83% decoding runtime. The proposed approach also outperforms the conventional lossless coding techniques such as LZ4HC, ZLIB, and PNG.},
  archive      = {J_TMM},
  author       = {Liping Zhao and Tao Lin and Dongyu Zhang and Kailun Zhou and Shuhui Wang},
  doi          = {10.1109/TMM.2019.2931414},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {786-794},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An ultra-low complexity and high efficiency approach for lossless alpha channel coding},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep top-<span class="math inline"><em>k</em></span> ranking
for image–sentence matching. <em>TMM</em>, <em>22</em>(3), 775–785. (<a
href="https://doi.org/10.1109/TMM.2019.2931352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-sentence matching is a challenging task for the heterogeneity-gap between different modalities. Ranking-based methods have achieved excellent performance in this task in past decades. Given an image query, these methods typically assume that the correct matched image-sentence pair must rank before all other mismatched ones. However, this assumption may be too strict and prone to the overfitting problem, especially when some sentences in a massive database are similar and confusable with one another. In this paper, we relax the traditional ranking loss and propose a novel deep multi-modal network with a top-k ranking loss to mitigate the data ambiguity problem. With this strategy, query results will not be penalized unless the index of ground truth is outside the range of top-k query results. Considering the non-smoothness and non-convexity of the initial top-k ranking loss, we exploit a tight convex upper bound to approximate the loss and then utilize the traditional back-propagation algorithm to optimize the deep multi-modal network. Finally, we apply the method on three benchmark datasets, namely, Flickr8k, Flickr30k, and MSCOCO. Empirical results on metrics R@K (K = 1, 5, 10) show that our method achieves comparable performance in comparison to state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Lingling Zhang and Minnan Luo and Jun Liu and Xiaojun Chang and Yi Yang and Alexander G. Hauptmann},
  doi          = {10.1109/TMM.2019.2931352},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {775-785},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep top-$k$ ranking for Image–Sentence matching},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward making unsupervised graph hashing discriminative.
<em>TMM</em>, <em>22</em>(3), 760–774. (<a
href="https://doi.org/10.1109/TMM.2019.2931808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, hashing has attracted much attention in visual information retrieval due to its low storage cost and fast query speed. The goal of hashing is to map original high-dimensional data into a low-dimensional binary-code space where the similar data points are assigned similar hash codes and dissimilar points are far away from each other. Existing unsupervised hashing methods mainly focus on recovering the pairwise similarity of the original data in hash space, but do not take specific measures to make the generated binary codes to be discriminative. To address this problem, this paper proposes a novel unsupervised hashing method, named “Discriminative Unsupervised Graph Hashing” (DUGH), which takes both similarity and dissimilarity of original data into consideration to learn discriminative binary codes. In particular, a probabilistic model is utilized to learn the encoding of original data in low-dimensional space, which models the original neighbor structure through both positive and negative edges in the KNN graph and then maximizes the likelihood of observing these edges. To efficiently and accurately measure the neighbor structure for large-scale datasets, we propose an effective KNN graph construction algorithm based on the random projection tree and neighbor exploring techniques. The experimental results on one synthetic dataset and four typical real-world image datasets demonstrate that the proposed method significantly outperforms the state-of-the-art unsupervised hashing methods.},
  archive      = {J_TMM},
  author       = {Chao Ma and Chen Gong and Xiang Li and Xiaolin Huang and Wei Liu and Jie Yang},
  doi          = {10.1109/TMM.2019.2931808},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {760-774},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Toward making unsupervised graph hashing discriminative},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing fixation prediction using recurrent neural
networks for 360<span class="math inline"><sup>∘</sup></span> video
streaming in head-mounted virtual reality. <em>TMM</em>, <em>22</em>(3),
744–759. (<a href="https://doi.org/10.1109/TMM.2019.2931807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of predicting the viewing probability of different parts of 3600 videos when streaming them to head-mounted displays. We propose a fixation prediction network based on recurrent neural network, which leverages sensor and content features. The content features are derived by computer vision (CV) algorithms, which may suffer from inferior performance due to various types of distortion caused by diverse 3600 video projection models. We propose a unified approach with overlapping virtual viewports to eliminate such negative effects, and we evaluate our proposed solution using several CV algorithms, such as saliency detection, face detection, and object detection. We find that overlapping virtual viewports increase the performance of these existing CV algorithms that were not trained for 3600 videos. We next fine-tune our fixation prediction network with diverse design options, including: 1) with or without overlapping virtual viewports, 2) with or without future content features, and 3) different feature sampling rates. We empirically choose the best fixation prediction network and use it in a 3600 video streaming system. We conduct extensive trace-driven simulations with a large-scale dataset to quantify the performance of the 3600 video streaming system with different fixation prediction algorithms. The results show that our proposed fixation prediction network outperforms other algorithms in several aspects, such as: 1) achieving comparable video quality (average gaps between -0.05 and 0.92 dB), 2) consuming much less bandwidth (average bandwidth reduction by up to 8 Mb/s), 3) reducing the rebuffering time (on average 40 s in bandwidth-limited 4G cellular networks), and 4) running in real-time (at most 124 ms).},
  archive      = {J_TMM},
  author       = {Ching-Ling Fan and Shou-Cheng Yen and Chun-Ying Huang and Cheng-Hsin Hsu},
  doi          = {10.1109/TMM.2019.2931807},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {744-759},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing fixation prediction using recurrent neural networks for 360$^{\circ }$ video streaming in head-mounted virtual reality},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realistic facial expression reconstruction for VR HMD users.
<em>TMM</em>, <em>22</em>(3), 730–743. (<a
href="https://doi.org/10.1109/TMM.2019.2933338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a system for sensing and reconstructing facial expressions of the virtual reality (VR) head-mounted display (HMD) user. The HMD occludes a large portion of the user&#39;s face, which makes most existing facial performance capturing techniques intractable. To tackle this problem, a novel hardware solution with electromyography (EMG) sensors being attached to the headset frame is applied to track facial muscle movements. For realistic facial expression recovery, we first reconstruct the user&#39;s 3D face from a single image and generate the personalized blendshapes associated with seven facial action units (AUs) on the most emotionally salient facial parts (ESFPs). We then utilize pre-processed EMG signals for measuring activations of AU-coded facial expressions to drive pre-built personalized blendshapes. Since facial expressions appear as important nonverbal cues of the subject&#39;s internal emotional states, we further investigate the relationship between six basic emotions - anger, disgust, fear, happiness, sadness and surprise, and detected AUs using a fern classifier. Experiments show the proposed system can accurately sense and reconstruct high-fidelity common facial expressions while providing useful information regarding the emotional state of the HMD user.},
  archive      = {J_TMM},
  author       = {Jianwen Lou and Yiming Wang and Charles Nduka and Mahyar Hamedi and Ifigeneia Mavridou and Fei-Yue Wang and Hui Yu},
  doi          = {10.1109/TMM.2019.2933338},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {730-743},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Realistic facial expression reconstruction for VR HMD users},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MLC STT-MRAM-aware memory subsystem for smart image
applications. <em>TMM</em>, <em>22</em>(3), 717–729. (<a
href="https://doi.org/10.1109/TMM.2019.2930342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation memories with high storage capacity, high performance, and low power consumption are being researched due to the ever-growing demand for artificial intelligence and high-definition applications. Among such future memories, a multi-level cell (MLC) spin-transfer magnetic torque random access memory (STT-MRAM) attracts considerable attention as an alternative to static or dynamic random access memories. An MCL STT-MRAM has the advantages of capacity and non-volatility, but the disadvantages of performance, power consumption, and endurance resulting from complicated resistance state transition and detection processes. In particular, such issues are exacerbated in the latest smart image applications employing block-based processing algorithms. In this paper, we propose a memory subsystem that mitigates the MLC STT-MRAM disadvantages in smart image applications. Our main idea is threefold: MLC-aware image buffer composing, block-aware pixel-to-memory mapping, and prediction-aware image-to-buffer allocating techniques that all make multi-step resistance state transition and detection processes less required. Experimental results show that the proposed memory subsystem achieves 24.5% shorter application execution time, and 96.4% lower memory power consumption than the conventional memory subsystems for industrial smart image applications. In addition, our memory subsystem increases the lifetime of MLC STT-MRAMs via 93.8% fewer multi-step resistance state transition processes.},
  archive      = {J_TMM},
  author       = {Wooyoung Jang},
  doi          = {10.1109/TMM.2019.2930342},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {717-729},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MLC STT-MRAM-aware memory subsystem for smart image applications},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PDR-net: Perception-inspired single image dehazing network
with refinement. <em>TMM</em>, <em>22</em>(3), 704–716. (<a
href="https://doi.org/10.1109/TMM.2019.2933334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During recent years, we have witnessed a rapid development of wireless network technologies which have revolutionized the way people take and share multimedia content. However, images captured in the outdoor scenes usually suffer from limited visibility due to suspended atmospheric particles, which directly affects the quality of photos. Despite the recent progress of image dehazing methods, the visual quality of dehazed results still needs further improvement. In this paper, we propose a deep convolutional neural network (CNN) for single image dehazing called PDR-Net, which includes a perception-inspired haze removal subnetwork that reconstructs the latent dehazed image and a refinement subnetwork that further enhances the contrast and color properties of the dehazed result by joint multi-term loss optimization. Compared to the previous methods, our method combines the advantages of existing indoor and outdoor image dehazing training data, which makes the proposed PDR-Net generalized to various hazy images and effective for improving the visual quality of the dehazed results. Extensive experiments demonstrate that the proposed method achieves comparable and even better performance on both real and synthetic images in qualitative and quantitative metrics. Additionally, the potential usage of our method in high-level vision tasks is discussed.},
  archive      = {J_TMM},
  author       = {Chongyi Li and Chunle Guo and Jichang Guo and Ping Han and Huazhu Fu and Runmin Cong},
  doi          = {10.1109/TMM.2019.2933334},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {704-716},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PDR-net: Perception-inspired single image dehazing network with refinement},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Snapshot high dynamic range imaging via sparse
representations and feature learning. <em>TMM</em>, <em>22</em>(3),
688–703. (<a href="https://doi.org/10.1109/TMM.2019.2933333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bracketed High Dynamic Range (HDR) imaging architectures acquire a sequence of Low Dynamic Range (LDR) images in order to either produce a HDR image or an “optimally” exposed LDR image, achieving impressive results under static camera and scene conditions. However, in real world conditions, ghost-like artifacts and noise effects limit the quality of HDR reconstruction. We address these limitations by introducing a post-acquisition snapshot HDR enhancement scheme that generates a bracketed sequence from a small set of LDR images, and in the extreme case, directly from a single exposure. We achieve this goal via a sparse-based approach where transformations between differently exposed images are encoded through a dictionary learning process, while we learn appropriate features by employing a stacked sparse autoencoder (SSAE) based framework. Via experiments with real images, we demonstrate the improved performance of our method over the state-of-the-art, while our single-shot based HDR formulation provides a novel paradigm for the enhancement of LDR imaging and video sequences.},
  archive      = {J_TMM},
  author       = {Konstantina Fotiadou and Grigorios Tsagkatakis and Panagiotis Tsakalides},
  doi          = {10.1109/TMM.2019.2933333},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {688-703},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Snapshot high dynamic range imaging via sparse representations and feature learning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic segmentation guided pixel fusion for image
retargeting. <em>TMM</em>, <em>22</em>(3), 676–687. (<a
href="https://doi.org/10.1109/TMM.2019.2932566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image retargeting aims to obtain high visual quality of target images for human vision. Through semantic segmentation and understanding of input images, we can better preserve the important semantic regions, so as to effectively improve the performance of image retargeting. Benefit from the successful application of deep neural network in the field of semantic segmentation, in this paper, we propose a novel image retargeting approach using semantic segmentation and pixel fusion. Compared with existing image retargeting methods, our approach can effectively reduce geometric distortion during image retargeting by finely reallocating scaling factors for each region based on the semantic segmentation results. Experimental results demonstrate that the proposed approach can well preserve important semantic regions while leaving less unnatural geometric distortion. Our approach also shows the important role of semantic segmentation and understanding of scenes in image retargeting in detail.},
  archive      = {J_TMM},
  author       = {Bo Yan and Xuejing Niu and Bahetiyaer Bare and Weimin Tan},
  doi          = {10.1109/TMM.2019.2932566},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {676-687},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic segmentation guided pixel fusion for image retargeting},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PTB-TIR: A thermal infrared pedestrian tracking benchmark.
<em>TMM</em>, <em>22</em>(3), 666–675. (<a
href="https://doi.org/10.1109/TMM.2019.2932615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal infrared (TIR) pedestrian tracking is one of the important components among numerous applications of computer vision, which has a major advantage: it can track pedestrians in total darkness. The ability to evaluate the TIR pedestrian tracker fairly, on a benchmark dataset, is significant for the development of this field. However, there is not a benchmark dataset. In this paper, we develop a TIR pedestrian tracking dataset for the TIR pedestrian tracker evaluation. The dataset includes 60 thermal sequences with manual annotations. Each sequence has nine attribute labels for the attribute based evaluation. In addition to the dataset, we carry out the large-scale evaluation experiments on our benchmark dataset using nine publicly available trackers. The experimental results help us understand the strengths and weaknesses of these trackers. In addition, in order to gain more insight into the TIR pedestrian tracker, we divide its functions into three components: feature extractor, motion model, and observation model. Then, we conduct three comparison experiments on our benchmark dataset to validate how each component affects the tracker&#39;s performance. The findings of these experiments provide some guidelines for future research.},
  archive      = {J_TMM},
  author       = {Qiao Liu and Zhenyu He and Xin Li and Yuan Zheng},
  doi          = {10.1109/TMM.2019.2932615},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {666-675},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PTB-TIR: A thermal infrared pedestrian tracking benchmark},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stereoscopic image stitching via disparity-constrained
warping and blending. <em>TMM</em>, <em>22</em>(3), 655–665. (<a
href="https://doi.org/10.1109/TMM.2019.2932573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a significant branch of virtual reality, stereoscopic image stitching aims to generating wide perspectives and natural-looking scenes. Existing 2D image stitching methods cannot be successfully applied to the stereoscopic images without considering the disparity consistency of stereoscopic images. To address this issue, this paper presents a stereoscopic image stitching method based on disparity-constrained warping and blending, which could avoid visual distortion and preserve disparity consistency. First, a point-line-driven homography based disparity minimization method is designed to pre-align the left and right images and reduce vertical disparity. Afterwards, a multi-constraint warping is proposed to further align the left and right images, where the initial disparity map is introduced to control the consistency of disparities. Finally, a disparity consistency seam-cutting and blending method is presented to determine the optimal seam and conduct stereoscopic image stitching. Experimental results demonstrate that the proposed method achieves competitive performance compared with other state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xiaoting Fan and Jianjun Lei and Yuming Fang and Qingming Huang and Nam Ling and Chunping Hou},
  doi          = {10.1109/TMM.2019.2932573},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {655-665},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Stereoscopic image stitching via disparity-constrained warping and blending},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image retargetability. <em>TMM</em>, <em>22</em>(3),
641–654. (<a href="https://doi.org/10.1109/TMM.2019.2932620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world applications could benefit from the ability to automatically retarget an image to different aspect ratios and resolutions while preserving its visually and semantically important content. However, not all images can be equally processed. This study introduces the notion of image retargetability to describe how well a particular image can be handled by content-aware image retargeting. We propose to learn a deep convolutional neural network to rank photo retargetability, in which the relative ranking of photo retargetability is directly modeled in the loss function. Our model incorporates the joint learning of meaningful photographic attributes and image content information, which can facilitate the regularization of the complicated retargetability rating problem. To train and analyze this model, we collect a dataset that contains retargetability scores and meaningful image attributes assigned by six expert raters. The experiments demonstrate that our unified model can generate retargetability rankings that are highly consistent with human labels. To further validate our model, we show the applications of image retargetability in retargeting method selection, retargeting method assessment and generating a photo collage.},
  archive      = {J_TMM},
  author       = {Fan Tang and Weiming Dong and Yiping Meng and Chongyang Ma and Fuzhang Wu and Xinrui Li and Tong-Yee Lee},
  doi          = {10.1109/TMM.2019.2932620},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {641-654},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image retargetability},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Spatiotemporal recurrent convolutional networks for
recognizing spontaneous micro-expressions. <em>TMM</em>, <em>22</em>(3),
626–640. (<a href="https://doi.org/10.1109/TMM.2019.2931351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the recognition task of spontaneous facial micro-expressions has attracted much attention with its various real-world applications. Plenty of handcrafted or learned features have been employed for a variety of classifiers and achieved promising performances for recognizing micro-expressions. However, the micro-expression recognition is still challenging due to the subtle spatiotemporal changes of micro-expressions. To exploit the merits of deep learning, we propose a novel deep recurrent convolutional networks based micro-expression recognition approach, capturing the spatiotemporal deformations of micro-expression sequence. Specifically, the proposed deep model is constituted of several recurrent convolutional layers for extracting visual features and a classificatory layer for recognition. It is optimized by an end-to-end manner and obviates manual feature design. To handle sequential data, we exploit two ways to extend the connectivity of convolutional networks across temporal domain, in which the spatiotemporal deformations are modeled in views of facial appearance and geometry separately. Besides, to overcome the shortcomings of limited and imbalanced training samples, two temporal data augmentation strategies as well as a balanced loss are jointly used for our deep network. By performing the experiments on three spontaneous micro-expression datasets, we verify the effectiveness of our proposed micro-expression recognition approach compared to the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Zhaoqiang Xia and Xiaopeng Hong and Xingyu Gao and Xiaoyi Feng and Guoying Zhao},
  doi          = {10.1109/TMM.2019.2931351},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {626-640},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatiotemporal recurrent convolutional networks for recognizing spontaneous micro-expressions},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rate-distortion optimal joint texture and depth map coding
for 3-d video streaming. <em>TMM</em>, <em>22</em>(3), 610–625. (<a
href="https://doi.org/10.1109/TMM.2019.2933336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For high compression efficiency, 3-D video coding usually employs a multimode methodology to exploit the dependencies between multiple views as well as between texture and depth. However, different coding modes will posses differentiating error propagation behaviour when the compressed 3-D video bit stream is transmitted over packet-switched networks, and thus lead to different amount of visual distortions. Further, the texture and depth distortions are combined in a highly complex fashion to produce the overall view synthesis distortion. To minimize the expected view synthesis distortion, this paper proposes an efficient rate-distortion optimized algorithm for joint selection of texture and depth modes. Firstly, a statistical model is developed to estimate the overall view synthesis distortion, in which the channel distortions caused by error propagation under different coding modes are analyzed. Then, joint optimization of texture and depth modes is derived within an operational rate-distortion framework using the Lagrange multiplier method. The adjacent block dependency caused by warping operation is explicitly considered in optimization, for which we develop a dynamic programming method to find the optimal solution. Finally, we extend the Lagrange minimization method to the more general variable-block-size prediction case, where the optimal quadtree tree structure and the combined coding modes are jointly determined using a multi-level dual trellis. Experimental results are presented for a wide range of packet loss rates to illustrate the effectiveness of the proposed algorithm.},
  archive      = {J_TMM},
  author       = {Pan Gao and Manoranjan Paul},
  doi          = {10.1109/TMM.2019.2933336},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {610-625},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rate-distortion optimal joint texture and depth map coding for 3-D video streaming},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of compressed sensing system with probability-based
prior information. <em>TMM</em>, <em>22</em>(3), 594–609. (<a
href="https://doi.org/10.1109/TMM.2019.2931400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the design of a sensing matrix along with a sparse recovery algorithm by utilizing the probability-based prior information for compressed sensing systems. With the knowledge of the probability for each atom of the dictionary being used, a diagonal weighted matrix is obtained and then the sensing matrix is designed by minimizing a weighted function such that the Gram of the equivalent dictionary is as close to the Gram of dictionary as possible. An analytical solution for the corresponding sensing matrix is derived that requires low computational complexity. We also exploit this prior information through the sparse recovery stage and propose a probability-driven orthogonal matching pursuit algorithm that improves the accuracy of the recovery. Simulations for synthetic data and application scenarios of video streaming are carried out to compare the performance of the proposed methods with some existing algorithms. The results reveal that the proposed compressed sensing (CS) approach outperforms existing CS systems.},
  archive      = {J_TMM},
  author       = {Qianru Jiang and Sheng Li and Zhihui Zhu and Huang Bai and Xiongxiong He and Rodrigo C. de Lamare},
  doi          = {10.1109/TMM.2019.2931400},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {594-609},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Design of compressed sensing system with probability-based prior information},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Steered mixture-of-experts for light field images and video:
Representation and coding. <em>TMM</em>, <em>22</em>(3), 579–593. (<a
href="https://doi.org/10.1109/TMM.2019.2932614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in light field (LF) processing has heavily increased over the last decade. This is largely driven by the desire to achieve the same level of immersion and navigational freedom for camera-captured scenes as it is currently available for CGI content. Standardization organizations such as MPEG and JPEG continue to follow conventional coding paradigms in which viewpoints are discretely represented on 2-D regular grids. These grids are then further decorrelated through hybrid DPCM/transform techniques. However, these 2-D regular grids are less suited for high-dimensional data, such as LFs. We propose a novel coding framework for higher-dimensional image modalities, called Steered Mixture-of-Experts (SMoE). Coherent areas in the higher-dimensional space are represented by single higher-dimensional entities, called kernels. These kernels hold spatially localized information about light rays at any angle arriving at a certain region. The global model consists thus of a set of kernels which define a continuous approximation of the underlying plenoptic function. We introduce the theory of SMoE and illustrate its application for 2-D images, 4-D LF images, and 5-D LF video. We also propose an efficient coding strategy to convert the model parameters into a bitstream. Even without provisions for high-frequency information, the proposed method performs comparable to the state of the art for low-to-mid range bitrates with respect to subjective visual quality of 4-D LF images. In case of 5-D LF video, we observe superior decorrelation and coding performance with coding gains of a factor of 4x in bitrate for the same quality. At least equally important is the fact that our method inherently has desired functionality for LF rendering which is lacking in other state-of-the-art techniques: (1) full zero-delay random access, (2) light-weight pixel-parallel view reconstruction, and (3) intrinsic view interpolation and super-resolution.},
  archive      = {J_TMM},
  author       = {Ruben Verhack and Thomas Sikora and Glenn Van Wallendael and Peter Lambert},
  doi          = {10.1109/TMM.2019.2932614},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {579-593},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Steered mixture-of-experts for light field images and video: Representation and coding},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task learning for acoustic event detection using event
and frame position information. <em>TMM</em>, <em>22</em>(3), 569–578.
(<a href="https://doi.org/10.1109/TMM.2019.2933330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic event detection deals with the acoustic signals to determine the sound type and to estimate the audio event boundaries. Multi-label classification based approaches are commonly used to detect the frame wise event types with a median filter applied to determine the happening acoustic events. However, the multi-label classifiers are trained only on the acoustic event types ignoring the frame position within the audio events. To deal with this, this paper proposes to construct a joint learning based multi-task system. The first task performs the acoustic event type detection and the second task is to predict the frame position information. By sharing representations between the two tasks, we can enable the acoustic models to generalize better than the original classifier by averaging respective noise patterns to be implicitly regularized. Experimental results on the monophonic UPC-TALP and the polyphonic TUT Sound Event datasets demonstrate the superior performance of the joint learning method by achieving lower error rate and higher F-score compared to the baseline AED system.},
  archive      = {J_TMM},
  author       = {Xianjun Xia and Roberto Togneri and Ferdous Sohel and Yuanjun Zhao and Defeng Huang},
  doi          = {10.1109/TMM.2019.2933330},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  number       = {3},
  pages        = {569-578},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-task learning for acoustic event detection using event and frame position information},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020k). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(2), C3. (<a
href="https://doi.org/10.1109/TMM.2020.2966203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2020.2966203},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video storytelling: Textual summaries for events.
<em>TMM</em>, <em>22</em>(2), 554–565. (<a
href="https://doi.org/10.1109/TMM.2019.2930041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bridging vision and natural language is a longstanding goal in computer vision and multimedia research. While earlier works focus on generating a single-sentence description for visual content, recent works have studied paragraph generation. In this paper, we introduce the problem of video storytelling, which aims at generating coherent and succinct stories for long videos. Video storytelling introduces new challenges, mainly due to the diversity of the story and the length and complexity of the video. We propose novel methods to address the challenges. First, we propose a context-aware framework for multimodal embedding learning, where we design a residual bidirectional recurrent neural network to leverage contextual information from past and future. The multimodal embedding is then used to retrieve sentences for video clips. Second, we propose a Narrator model to select clips that are representative of the underlying storyline. The Narrator is formulated as a reinforcement learning agent, which is trained by directly optimizing the textual metric of the generated story. We evaluate our method on the video story dataset, a new dataset that we have collected to enable the study. We compare our method with multiple state-of-the-art baselines and show that our method achieves better performance, in terms of quantitative measures and user study.},
  archive      = {J_TMM},
  author       = {Junnan Li and Yongkang Wong and Qi Zhao and Mohan S. Kankanhalli},
  doi          = {10.1109/TMM.2019.2930041},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {554-565},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video storytelling: Textual summaries for events},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved deep hashing with soft pairwise similarity for
multi-label image retrieval. <em>TMM</em>, <em>22</em>(2), 540–553. (<a
href="https://doi.org/10.1109/TMM.2019.2929957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hash coding has been widely used in the approximate nearest neighbor search for large-scale image retrieval. Recently, many deep hashing methods have been proposed and shown largely improved performance over traditional feature-learning methods. Most of these methods examine the pairwise similarity on the semantic-level labels, where the pairwise similarity is generally defined in a hard-assignment way. That is, the pairwise similarity is “1” if they share no less than one class label and “0” if they do not share any. However, such similarity definition cannot reflect the similarity ranking for pairwise images that hold multiple labels. In this paper, an improved deep hashing method is proposed to enhance the ability of multi-label image retrieval. We introduce a pairwise quantified similarity calculated on the normalized semantic labels. Based on this, we divide the pairwise similarity into two situations—“hard similarity” and “soft similarity,” where cross-entropy loss and mean square error loss are adapted respectively for more robust feature learning and hash coding. Experiments on four popular datasets demonstrate that the proposed method outperforms the competing methods and achieves the state-of-the-art performance in multi-label image retrieval.},
  archive      = {J_TMM},
  author       = {Zheng Zhang and Qin Zou and Yuewei Lin and Long Chen and Song Wang},
  doi          = {10.1109/TMM.2019.2929957},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {540-553},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improved deep hashing with soft pairwise similarity for multi-label image retrieval},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GAIM: Graph attention interaction model for collective
activity recognition. <em>TMM</em>, <em>22</em>(2), 524–539. (<a
href="https://doi.org/10.1109/TMM.2019.2930344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unbalanced interaction relationships at personal and group levels play a pivotal role in collective activity recognition, which has not been adaptively and jointly explored by previous approaches. In this paper, we propose a graph attention interaction model (GAIM) embedded with the graph attention block (GAB) to explicitly and adaptively infer unbalanced interaction relations at personal and group levels in a unified architecture, and further to learn the spatial and temporal evolutions of the collective activity from these interactions to predict the activity labels. We first design the spatiotemporal graphs tailored to the collective activity where the concurrent person and group nodes, respectively, represent individuals&#39; actions and the collective activity. The graphs provide both spatial structures and semantic appearance features for the collective activity. Then, GAB performs convolution-like filters on the graphs to infer unequal and two-level interaction relations in the collective activity by implementing graph convolutional networks with a shared attention mechanism. At the personal level, the GAB learns different levels of interactions for each person node from its neighbor person nodes under the guidance from the group node. At the group level, the GAB assesses various degrees of interactions to the group node contributed by person nodes. Equipped with the GRUs network, the GAIM learns the spatial and temporal evolutions of individuals&#39; actions as well as the collective activity from the captured interactions, and finally predicts the label of the collective activity. Experiments on four publicly available datasets and ablation studies are conducted to evaluate the performance of our GAIM, and the improved performance demonstrates the effectiveness of our model.},
  archive      = {J_TMM},
  author       = {Lihua Lu and Yao Lu and Ruizhe Yu and Huijun Di and Lin Zhang and Shunzhou Wang},
  doi          = {10.1109/TMM.2019.2930344},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {524-539},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GAIM: Graph attention interaction model for collective activity recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring discriminative representations for image emotion
recognition with CNNs. <em>TMM</em>, <em>22</em>(2), 515–523. (<a
href="https://doi.org/10.1109/TMM.2019.2928998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image emotion recognition aims to automatically categorize the emotion conveyed by an image. The potential of deep representation has been demonstrated in recent research on image emotion recognition. To better understand how CNNs work in emotion recognition, we investigate the deep features by visualizing them in this work. This study shows that the deep models mainly rely on the image content but miss the image style information such as color, texture, and shapes that are low-level visual features but are vital for evoking emotions. To form a more discriminative representation for emotion recognition, we propose a novel CNN model that learns and integrates the content information from the high layers of the deep network with the style information from the lower layers. The uncertainty of image emotion labels is also investigated in this paper. Rather than using the emotion labels for training directly, as in previous work, a new loss function is designed by including the emotion labeling quality to optimize the proposed inference model. Extensive experiments on benchmark datasets are conducted to demonstrate the superiority of the proposed representation.},
  archive      = {J_TMM},
  author       = {Wei Zhang and Xuanyu He and Weizhi Lu},
  doi          = {10.1109/TMM.2019.2928998},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {515-523},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring discriminative representations for image emotion recognition with CNNs},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pay attention to the activations: A modular attention
mechanism for fine-grained image recognition. <em>TMM</em>,
<em>22</em>(2), 502–514. (<a
href="https://doi.org/10.1109/TMM.2019.2928494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained image recognition is central to many multimedia tasks such as search, retrieval, and captioning. Unfortunately, these tasks are still challenging since the appearance of samples of the same class can be more different than those from different classes. This issue is mainly due to changes in deformation, pose, and the presence of clutter. In the literature, attention has been one of the most successful strategies to handle the aforementioned problems. Attention has been typically implemented in neural networks by selecting the most informative regions of the image that improve classification. In contrast, in this paper, attention is not applied at the image level but to the convolutional feature activations. In essence, with our approach, the neural model learns to attend to lower-level feature activations without requiring part annotations and uses those activations to update and rectify the output likelihood distribution. The proposed mechanism is modular, architecture-independent, and efficient in terms of both parameters and computation required. Experiments demonstrate that well-known networks such as wide residual networks and ResNeXt, when augmented with our approach, systematically improve their classification accuracy and become more robust to changes in deformation and pose and to the presence of clutter. As a result, our proposal reaches state-of-the-art classification accuracies in CIFAR-10, the Adience gender recognition task, Stanford Dogs, and UEC-Food100 while obtaining competitive performance in ImageNet, CIFAR-100, CUB200 Birds, and Stanford Cars. In addition, we analyze the different components of our model, showing that the proposed attention modules succeed in finding the most discriminative regions of the image. Finally, as a proof of concept, we demonstrate that with only local predictions, an augmented neural network can successfully classify an image before reaching any fully connected layer, thus reducing the computational amount up to 10%.},
  archive      = {J_TMM},
  author       = {Pau Rodríguez and Diego Velazquez and Guillem Cucurull and Josep M. Gonfaus and F. Xavier Roca and Jordi Gonzàlez},
  doi          = {10.1109/TMM.2019.2928494},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {502-514},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Pay attention to the activations: A modular attention mechanism for fine-grained image recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiscale superpixel-based hyperspectral image
classification using recurrent neural networks with stacked
autoencoders. <em>TMM</em>, <em>22</em>(2), 487–501. (<a
href="https://doi.org/10.1109/TMM.2019.2928491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a novel hyperspectral image (HSI) classification framework by exploiting the spectral-spatial features of multiscale superpixels via recurrent neural networks with stacked autoencoders. The superpixels can be used to segment an HSI into shape-adaptive regions, and multiscale superpixels can capture the object information more accurately. Therefore, the superpixel-based classification methods have been studied by many researchers. In this paper, we propose a multiscale superpixel-based classification method. In contrast to current research, the proposed method not only captures the features of each scale but also considers the correlation among different scales via recurrent neural networks. In this way, the spectral-spatial information within a superpixel is more efficiently exploited. In this paper, we first segment the HSI from coarse to fine scales using the superpixels. Then, the spatial features within each superpixel and among superpixels are sufficiently exploited by the local and nonlocal similarity measure. Finally, recurrent neural networks with stacked autoencoders are proposed to learn the high-level multiscale spectral-spatial features. Experiments are conducted on real HSI datasets. The results demonstrate the superiority of the proposed method over several well-known methods in both visual appearance and classification accuracy.},
  archive      = {J_TMM},
  author       = {Cheng Shi and Chi-Man Pun},
  doi          = {10.1109/TMM.2019.2928491},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {487-501},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiscale superpixel-based hyperspectral image classification using recurrent neural networks with stacked autoencoders},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust QoE-driven DASH over OFDMA networks. <em>TMM</em>,
<em>22</em>(2), 474–486. (<a
href="https://doi.org/10.1109/TMM.2019.2929929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the problem of effective and robust delivery of Dynamic Adaptive Streaming over HTTP (DASH) videos over an orthogonal frequency-division multiplexing access (OFDMA) network is studied. Motivated by a measurement study, we propose to explore the request interval and robust rate prediction for DASH over OFDMA. We first formulate an offline cross-layer optimization problem based on a novel quality of experience (QoE) model. Then the online reformulation is derived and proved to be asymptotically optimal. After analyzing the structure of the online problem, we propose a decomposition approach to obtain a user equipment (UE) rate adaptation problem and a BS resource allocation problem. We introduce stochastic model predictive control (SMPC) to achieve high robustness on video rate adaption and consider the request interval for more efficient resource allocation. Extensive simulations show that the proposed scheme can achieve a better QoE performance compared with other variations and a benchmark algorithm, which is mainly due to its lower rebuffering ratio and more stable bitrate choices.},
  archive      = {J_TMM},
  author       = {Kefan Xiao and Shiwen Mao and Jitendra K. Tugnait},
  doi          = {10.1109/TMM.2019.2929929},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {474-486},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust QoE-driven DASH over OFDMA networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient mobile video streaming via context-aware
RaptorQ-based unequal error protection. <em>TMM</em>, <em>22</em>(2),
459–473. (<a href="https://doi.org/10.1109/TMM.2019.2928497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile video streaming systems typically apply the forward error correction (FEC) at the application layer to cope with packet-level transmission errors, which complements the bit-level correction mechanisms at the physical layer. However, most existing works fail to exploit the block-level dependencies in both intra and interframe coding modes of a single-layer compressed video, and thus are less efficient for the prevailing H.264/AVC and/or H.265/HEVC compatible single-layer video application. To this end, we propose a low-complexity FEC, i.e., context-aware RaptorQ (CA-RQ) with unequal error protection (UEP), to improve the error recovery performance of the singlelayer mobile video streaming, through incorporating the blocklevel dependencies in the compressed video data. We use a packet-level video transmission distortion model that considers the dependencies in both spatial and temporal domains, to quantify the importance of video packets within a group of pictures (GoP). The compressed video packets are categorized and grouped into several classes according to their importance to construct the CA-RQ code with the UEP property. We provide a theoretical analysis on redundancy allocation bounds to demonstrate the superior performance of proposed CA-RQ over the standard RaptorQ code. In the meantime, extensive simulations have shown that our scheme not only offers much better subjective visual quality with less than 50% additional redundant symbols as compared to the Macroblock-Based UEP (MB-UEP) scheme, but also outperforms the MB-UEP and classical equal error protection (EEP)-based schemes, by a 0.45%&#39;5.71% and 0.94%&#39;6.78% margin, respectively, in reconstructed quality evaluated using the structural similarity (SSIM) index, across a reasonable range of redundancy proportions.},
  archive      = {J_TMM},
  author       = {Hao Chen and Xu Zhang and Yiling Xu and Zhan Ma and Wenjun Zhang},
  doi          = {10.1109/TMM.2019.2928497},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {459-473},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient mobile video streaming via context-aware RaptorQ-based unequal error protection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incentive mechanism for cooperative scalable video coding
(SVC) multicast based on contract theory. <em>TMM</em>, <em>22</em>(2),
445–458. (<a href="https://doi.org/10.1109/TMM.2019.2929965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scalable video coding (SVC) multicast, videos are encoded into several layers that represent multiple quality levels. Mobile users with different wireless channel conditions can obtain different numbers of layers and have different quality of experience (QoE). To enhance the QoE of the users that suffer from the worse channel quality, it is beneficial to stimulate users&#39; cooperation in relaying enhancement layers. However, potential relays may be unwilling to truthfully cooperate with receivers, which results in the asymmetric information problem in relay selecting. In this paper, we model the video relaying selection as a market with multiple receivers (principals) and relays (agents), and solve the problem according to the contract theory. The proposed solution is divided into following two steps: first, contract design and item preselection, and second, matching between each principal and agent. We propose a contract parameter determination method termed as the Matching-Aware strategy. Different from traditional strategies, the proposed Matching-Aware strategy makes the contract competitive in principal-agent matching without knowing the probability distribution of relays&#39; types. The matching step is undertaken by the base station with the purpose of maximizing the social welfare. Numerical results corroborate that the contract-based video relaying scheme can tackle the asymmetric information problem. Besides, compared with other two baseline strategies, the proposed Matching-Aware strategy achieves higher QoE.},
  archive      = {J_TMM},
  author       = {Zeyu Xu and Yang Cao and Wei Wang and Tao Jiang and Qian Zhang},
  doi          = {10.1109/TMM.2019.2929965},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {445-458},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Incentive mechanism for cooperative scalable video coding (SVC) multicast based on contract theory},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative content placement among wireless edge caching
stations with time-to-live cache. <em>TMM</em>, <em>22</em>(2), 432–444.
(<a href="https://doi.org/10.1109/TMM.2019.2929004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content caching at the Internet edge using a network of wireless edge caching stations (ECSs) is recently considered as a key solution to alleviating the backhaul traffic burden and improving the quality of experience in 5G networks. This paper studies wireless edge caching systems with the following features: first, content files can be partitioned into many coded packets, which then can be cached in multiple ECSs for collaborative content delivery; second, the service provider (SP) deploys time-to-live cache at ECSs and each cached content file has an occupancy time that needs to be guaranteed; third, the content-to-be-cached arrives at the caching system following a stochastic process as users request new content over time. Unlike existing works that determine which content to cache, this paper focuses on how to distribute the coded packets of content-to-be-cached among the network of ECSs in order to reduce the content downloading time. A novel content placement strategy, called stochastic collaborative content placement is proposed based on Lyapunov techniques. The proposed algorithm makes content placement decisions using only currently available information without foreseeing future content arrivals, takes advantage of the spatial content popularity variation with coded caching, and achieves the provable close-to-optimal long-term caching performance. Simulations are carried out on a real-world YouTube video request trace and the results demonstrate a tremendous caching performance improvement against a variety of benchmark schemes.},
  archive      = {J_TMM},
  author       = {Lixing Chen and Linqi Song and Jacob Chakareski and Jie Xu},
  doi          = {10.1109/TMM.2019.2929004},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {432-444},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Collaborative content placement among wireless edge caching stations with time-to-live cache},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Oriented spatial transformer network for pedestrian
detection using fish-eye camera. <em>TMM</em>, <em>22</em>(2), 421–431.
(<a href="https://doi.org/10.1109/TMM.2019.2929949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection using fish-eye cameras is a principal research focus in computer vision. Lack of pedestrian datasets of fish-eye images and pedestrian distortion in fish-eye images are two primary challenges. In this paper, two approaches are proposed to deal with these two challenges, respectively. On the one hand, the projective model transformation (PMT) algorithm is proposed, which can transform normal images into fish-eye images. The PMT can be applied to most of the pedestrian datasets and generates corresponding fish-eye image datasets. In this way, enough training data can be provided through the PMT. On the other hand, the oriented spatial transformer network (OSTN) is designed to rectify warped pedestrian features using CNNs, so that pedestrians in fish-eye images are easier for detectors to recognize. The OSTN can be embedded into universal deep learning based detectors easily. Moreover, the new pedestrian detector, where the OSTN is embedded, can be trained end to end. Finally, the OSTN based fish-eye pedestrian detectors can be trained using fish-eye images, which are generated using the PMT. Experiments on ETH, KITTI, Citypersons, and real pedestrian datasets show the effectiveness of the PMT and accuracy improvement of pedestrian detection in fish-eye images using the OSTN.},
  archive      = {J_TMM},
  author       = {Yeqiang Qian and Ming Yang and Xu Zhao and Chunxiang Wang and Bing Wang},
  doi          = {10.1109/TMM.2019.2929949},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {421-431},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Oriented spatial transformer network for pedestrian detection using fish-eye camera},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting social signals in user-shared images for
connection discovery using deep learning. <em>TMM</em>, <em>22</em>(2),
407–420. (<a href="https://doi.org/10.1109/TMM.2019.2930043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advance of mobile technology and social media, image sharing has become part of our daily lives. For many applications, such as follower/followee recommendation, shared images are an excellent source to discover connections among users who shared them. Shared images on social media are like invitations for user interactions, such as comment, like, and more. Social signals are in those images, and those signals can be objects that interest related users such that they will start to interact. Conventionally, connections among users are discovered through recognizing objects among those shared images, such as a using convolutional neural network (CNN) to extract features that are sensitive to object recognition. However, social signals are not limited to object, they can be colour, textual, or even a concept that may not be captured effectively by conventional CNN. This paper proposes a CNN-based analytic framework to detect social signals among users. The CNN is optimized using a triplet network with user-shared images, and the relationships among users who upload them. It is observed that images from 2 users with a connection have a shorter distance after encoding, than 2 users without a connection. A framework is implemented, which is verified with over 1.7 million images by over 2000 users from two image-oriented social networks, Skyrock and Flickr. It is proven that the proposed analytic framework shows an up to 89% improvement on approaches using object recognition for follower/followee recommendation. To the best of our knowledge, this paper is the first to propose an analytic framework to detect social signals from visual features for connection discovery.},
  archive      = {J_TMM},
  author       = {Ming Cheung and James She},
  doi          = {10.1109/TMM.2019.2930043},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {407-420},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detecting social signals in user-shared images for connection discovery using deep learning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video anomaly detection and localization based on an
adaptive intra-frame classification network. <em>TMM</em>,
<em>22</em>(2), 394–406. (<a
href="https://doi.org/10.1109/TMM.2019.2929931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection and localization is still a challenging task in the computer vision field. Previous methods took this task as an outlier detection problem, which computed the deviation between the test samples and the normal patterns. In this paper, an adaptive intra-frame classification network (AICN) is proposed to transform this task to a multi-class classification problem. The contributions of our method are as follows. AICN is an end-to-end network for anomaly detection and localization. By using the motion convolutional layers and the shape convolutional layers, spatial-temporal features are extracted without resizing or splitting the frames before forward propagation. AICN enhances the adaptiveness of model. By using the adaptive region pooling layer and the intra-frame classifier, AICN is adaptive to frames with different resolutions and is easier to be applied on other scenes. AICN evaluates the abnormality of frames based on the intra-frame classification results. The intra-frame classification strategy reserves more connection information of sub-regions and makes the model outperform previous methods. The proposed method is examined on four public datasets with different background complexities and resolutions: UCSD Ped1 dataset, UCSD Ped2 dataset, Avenue dataset and Subway dataset. The results are further compared with previous approaches to confirm the effectiveness and the advantage of our method.},
  archive      = {J_TMM},
  author       = {Ke Xu and Tanfeng Sun and Xinghao Jiang},
  doi          = {10.1109/TMM.2019.2929931},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {394-406},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video anomaly detection and localization based on an adaptive intra-frame classification network},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WiderPerson: A diverse dataset for dense pedestrian
detection in the wild. <em>TMM</em>, <em>22</em>(2), 380–393. (<a
href="https://doi.org/10.1109/TMM.2019.2929005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection has achieved significant progress with the availability of existing benchmark datasets. However, there is a gap in the diversity and density between real world requirements and current pedestrian detection benchmarks: first, most existing datasets are taken from a vehicle driving through the regular traffic scenario, usually leading to insufficient diversity; second, crowd scenarios with highly occluded pedestrians are still underrepresented, resulting in low density. To narrow this gap and facilitate future pedestrian detection research, we introduce a large and diverse dataset named WiderPerson for dense pedestrian detection in the wild. This dataset involves five types of annotations in a wide range of scenarios, no longer limited to the traffic scenario. There are a total of 13 382 images with 399 786 annotations, that is, 29.87 annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild. We introduce an improved Faster R-CNN and the vanilla RetinaNet to serve as baselines for the new pedestrian detection benchmark. Several experiments are conducted on previous datasets including Caltech-USA and CityPersons to analyze the generalization capabilities of the proposed dataset, and we achieve state-of-the-art performances on these previous datasets without bells and whistles. Finally, we analyze common failure cases and find the classification ability of pedestrian detector needs to be improved to reduce false alarm and misdetection rates. The proposed dataset is available at http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson.},
  archive      = {J_TMM},
  author       = {Shifeng Zhang and Yiliang Xie and Jun Wan and Hansheng Xia and Stan Z. Li and Guodong Guo},
  doi          = {10.1109/TMM.2019.2929005},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {380-393},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WiderPerson: A diverse dataset for dense pedestrian detection in the wild},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kernel-based mixture mapping for image and text association.
<em>TMM</em>, <em>22</em>(2), 365–379. (<a
href="https://doi.org/10.1109/TMM.2019.2930336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the relationship between multimodal media, including images, videos, and text, can reduce the gap between the modalities and promote cross-media retrieval, image annotation, etc. In this paper, we propose a new approach called kernel-based mixture mapping (KMM) to model the semantic correlations between web images and text. With this approach, we first construct latent high-dimensional feature spaces based on kernel theory to address the nonlinearity of both the data distributions in the input spaces and the cross-model correlation. Second, we present a probabilistic neighborhood model to describe the spatial locality of semantics by assuming that proximate examples in feature spaces generally have the same semantics and a conditional model to describe cross-modal conditional dependency. Finally, we build a probabilistic mixture model to jointly model the spatial locality of semantics and the conditional dependency between different modalities. By combining nonlinear transformation and probabilistic models, KMM can address the nonlinearity of cross-modal correlation, the complexity of semantic distributions at the global scale, and the continuity of semantic distributions at the local scale. We present a hybrid optimization algorithm to find the solution of KMM based on expectation-maximization and subgradient ascent; this algorithm avoids estimating the parameters of KMM in high-dimensional feature space and is proved to converge to an (local) optimal solution. We demonstrate the performance of KMM using four public datasets. The experimental results show that our approach outperforms the compared methods when modeling the relationships between images and text.},
  archive      = {J_TMM},
  author       = {Youtian Du and Xue Wang and Yunbo Cui and Hang Wang and Chang Su},
  doi          = {10.1109/TMM.2019.2930336},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {365-379},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Kernel-based mixture mapping for image and text association},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Refined TV-l1 optical flow estimation using joint filtering.
<em>TMM</em>, <em>22</em>(2), 349–364. (<a
href="https://doi.org/10.1109/TMM.2019.2929934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though the accuracy and robustness of optical flow has been dramatically enhanced over the past few years, the issue of edge-blurring near the image and motion boundaries has remained a challenge in flow field estimation. In this paper, we propose a refined total variation with L 1 norm (TV- L 1 ) optical flow estimation approach using joint filtering, named JOF. First, we divide the image into three categorized regions: mutual-structure regions, inconsistent structure regions, and smooth regions. The mutual-structure guided filter for optical flow estimation is constructed by extracting the mutual-structure regions of the flow field. Second, the refined TV- L 1 optical flow model is proposed by incorporating the non-local term and mutual-structure guided filter objective function into the classical TV- L 1 energy function. Furthermore, the novel TV- L 1 optical flow objective function is minimized using a joint filtering program composed of a weighted median filter and a mutual-structure guided filter to optimize the estimated flow field during the coarse-to-fine optical flow computation scheme. Finally, we compare the proposed JOF method with several state-of-the-art approaches including variational and deep learning based optical flow models using the Middlebury, MPI-Sintel, and UCF101 test databases. The evaluation results indicate that the proposed method has high accuracy and good robustness for flow field computation and, especially, the significant benefit of edge-preserving.},
  archive      = {J_TMM},
  author       = {Congxuan Zhang and Liyue Ge and Zhen Chen and Ming Li and Wen Liu and Hao Chen},
  doi          = {10.1109/TMM.2019.2929934},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {349-364},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Refined TV-l1 optical flow estimation using joint filtering},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale based context-aware net for action detection.
<em>TMM</em>, <em>22</em>(2), 337–348. (<a
href="https://doi.org/10.1109/TMM.2019.2929923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of action detection in continuous untrimmed video streams, based on the two-stage framework: one stage for action proposals generation and the other for proposals classification and refinement. The context features inside and outside a candidate region (proposal) are critical for classification in action detection. Therefore, effective integration of these features with different scales has become a fundamental problem. We contend that different action instances and candidate proposals may need different context features. To address this issue, we present a novel multiple scales based context-aware net (MSCA-Net) to effectively classify the action proposals for action detection in this paper. For each candidate action proposal, MSCA-Net takes its multiple regions with different temporal scales as input and then generates suitable context features. Based on the “candidate-control” mechanism of LSTM, the proposed MSCA-Net specially adopts the two-branch structure: Branch1 generates multi-scale context features for each candidate proposal, whereas Branch2 utilizes the context-aware gate function to control the message passing. Extensive experiments on THUMOS’14, Charades daily and ActivityNet action detection datasets, demonstrate the effectiveness of the designed structure and show how these context features influence the detection results.},
  archive      = {J_TMM},
  author       = {Haijun Liu and Shiguang Wang and Wen Wang and Jian Cheng},
  doi          = {10.1109/TMM.2019.2929923},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {337-348},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale based context-aware net for action detection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Salient object detection via multiple instance joint
re-learning. <em>TMM</em>, <em>22</em>(2), 324–336. (<a
href="https://doi.org/10.1109/TMM.2019.2929943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years deep neural networks have been widely applied to visual saliency detection tasks with remarkable detection performance improvements. As for the salient object detection in single image, the automatically computed convolutional features frequently demonstrate high discriminative power to distinguish salient foregrounds from its non-salient surroundings in most cases. Yet, the obstinate feature conflicts still persist, which naturally gives rise to the learning ambiguity, arriving at massive failure detections. To solve such problem, we propose to jointly re-learn common consistency of inter-image saliency and then use it to boost the detection performance. Its core rationale is to utilize the easy-to-detect cases to re-boost much harder ones. Compared with the conventional methods, which focus on their problem domain within the single image scope, our method attempts to utilize those beyond-scope information to facilitate the current salient object detection. To validate our new approach, we have conducted a comprehensive quantitative comparisons between our approach and 13 state-of-the-art methods over 5 publicly available benchmarks, and all the results suggest the advantage of our approach in terms of accuracy, reliability, and versatility.},
  archive      = {J_TMM},
  author       = {Guangxiao Ma and Chenglizhao Chen and Shuai Li and Chong Peng and Aimin Hao and Hong Qin},
  doi          = {10.1109/TMM.2019.2929943},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {324-336},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Salient object detection via multiple instance joint re-learning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep dual-channel neural network for image-based smoke
detection. <em>TMM</em>, <em>22</em>(2), 311–323. (<a
href="https://doi.org/10.1109/TMM.2019.2929009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoke detection plays an important role in industrial safety warning systems and fire prevention. Due to the complicated changes in the shape, texture, and color of smoke, identifying the smoke from a given image still remains a substantial challenge, and this has accordingly aroused a considerable amount of research attention recently. To address the problem, we devise a new deep dual-channel neural network (DCNN) for smoke detection. In contrast to popular deep convolutional networks (e.g., Alex-Net, VGG-Net, Res-Net, and Dense-Net and the DNCNN that is specifically devoted to detecting smoke), our proposed end-to-end network is mainly composed of dual channels of deep subnetworks. In the first subnetwork, we sequentially connect multiple convolutional layers and max-pooling layers. Then, we selectively append the batch normalization layer to each convolutional layer for overfitting reduction and training acceleration. The first subnetwork is shown to be good at extracting the detailed information of smoke, such as texture. In the second subnetwork, in addition to the convolutional, batch normalization, and max-pooling layers, we further introduce two important components. One is the skip connection for avoiding the vanishing gradient and improving the feature propagation. The other is the global average pooling for reducing the number of parameters and mitigating the overfitting issue. The second subnetwork can capture the base information of smoke, such as contours. We finally deploy a concatenation operation to combine the aforementioned two deep subnetworks to complement each other. Based on the augmented data obtained by rotating the training images, our proposed DCNN can promptly and stably converge to the perfect performance. Experimental results conducted on the publicly available smoke detection database verify that the proposed DCNN has attained a very high detection rate that exceeds 99.5% on average, superior to state-of-the-art relevant competitors. Furthermore, our DCNN only employs approximately one-third of the parameters needed by the comparatively tested deep neural networks. The source code of DCNN will be released at https://kegu.netlify.com/.},
  archive      = {J_TMM},
  author       = {Ke Gu and Zhifang Xia and Junfei Qiao and Weisi Lin},
  doi          = {10.1109/TMM.2019.2929009},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {311-323},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep dual-channel neural network for image-based smoke detection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Style-controlled synthesis of clothing segments for fashion
image manipulation. <em>TMM</em>, <em>22</em>(2), 298–310. (<a
href="https://doi.org/10.1109/TMM.2019.2929000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach for digitally altering people&#39;s outfits in images. Given images of a person and a desired clothing style, our method generates a new clothing item image. The new item displays the color and pattern of the desired style while geometrically mimicking the person&#39;s original item. Through superimposition, the altered image is made to look as if the person is wearing the new item. Unlike recent works with full-image synthesis, our work relies on segment synthesis, yielding benefits in virtual try-on. For the synthesis process, we assume two underlying factors characterizing clothing segments: geometry and style. These two factors are disentangled via preprocessing and combined using a neural network. We explore several networks and introduce important aspects of the architecture and learning process. Our experimental results are three-fold: 1) on images from fashion-parsing datasets, we demonstrate the generation of high-quality clothing segments with fine-level style control; 2) on a virtual try-on benchmark, our method shows superiority over prior synthesis methods; and 3) in transferring clothing styles, we visualize the differences between our method and neural style transfer.},
  archive      = {J_TMM},
  author       = {Bo-Kyeong Kim and Geonmin Kim and Soo-Young Lee},
  doi          = {10.1109/TMM.2019.2929000},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {298-310},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Style-controlled synthesis of clothing segments for fashion image manipulation},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-focus image fusion by hessian matrix based
decomposition. <em>TMM</em>, <em>22</em>(2), 285–297. (<a
href="https://doi.org/10.1109/TMM.2019.2928516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a Hessian matrix based multi-focus image fusion method is proposed. First, the integral map is introduced for fast compute the Hessian matrix of source images at different scales, and the multi-scale Hessian matrix of source image is obtained. Second, the multi-scale Hessian matrix is used to decompose each source image into two kinds of regions: the feature and background regions. In order to improve the fusion performance, two new focus measures based on the multi-scale Hessian matrix and two different fusion strategies for both feature and background regions are utilized to obtain the initial decision maps, respectively. Finally, the final decision map for image fusion is achieved by post-processing on the results of the previous step. The proposed method is a primary attempt to introduce image feature and background regions decomposition strategies in the field of multi-focus image fusion. The experimental results also show that our method outperforms the existing image fusion methods in both visual perception and objective evaluations.},
  archive      = {J_TMM},
  author       = {Bin Xiao and Ge Ou and Han Tang and Xiuli Bi and Weisheng Li},
  doi          = {10.1109/TMM.2019.2928516},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  number       = {2},
  pages        = {285-297},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-focus image fusion by hessian matrix based decomposition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020l). IEEE transactions on multimedia. <em>TMM</em>,
<em>22</em>(1), C3. (<a
href="https://doi.org/10.1109/TMM.2019.2961032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2019.2961032},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IEEE transactions on multimedia},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The role of the input in natural language video description.
<em>TMM</em>, <em>22</em>(1), 271–283. (<a
href="https://doi.org/10.1109/TMM.2019.2924598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language video description (NLVD) has recently received strong interest in the computer vision, natural language processing (NLP), multimedia, and autonomous robotics communities. The state-of-the-art (SotA) approaches obtained remarkable results when tested on the benchmark datasets. However, those approaches poorly generalize to new datasets. In addition, none of the existing works focus on the processing of the input to the NLVD systems, which is both visual and textual. In this paper, an extensive study is presented to deal with the role of the visual input, evaluated with respect to the overall NLP performance. This is achieved by performing data augmentation of the visual component, applying common transformations to model camera distortions, noise, lighting, and camera positioning that are typical in real-world operative scenarios. A t-SNE-based analysis is proposed to evaluate the effects of the considered transformations on the overall visual data distribution. For this study, the English subset of the Microsoft Research Video Description (MSVD) dataset is considered, which is used commonly for NLVD. It was observed that this dataset contains a relevant amount of syntactic and semantic errors. These errors have been amended manually, and the new version of the dataset (called MSVD-v2) is used in the experimentation. The MSVD-v2 dataset is released to help to gain insight into the NLVD problem.},
  archive      = {J_TMM},
  author       = {Silvia Cascianelli and Gabriele Costante and Alessandro Devo and Thomas A. Ciarfuglia and Paolo Valigi and Mario L. Fravolini},
  doi          = {10.1109/TMM.2019.2924598},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {271-283},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {The role of the input in natural language video description},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel convolutional neural network for image steganalysis
with shared normalization. <em>TMM</em>, <em>22</em>(1), 256–270. (<a
href="https://doi.org/10.1109/TMM.2019.2920605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganalysis is to discriminate innocent images (cover images) and those suspected images (stego images) with hidden messages. The task is challenging since modifications to cover images due to message hiding are extremely small. To handle this difficulty, modern approaches proposed using convolutional neural network (CNN) models to detect steganography with paired learning, i.e., cover images and their stegos are both in training set. In this paper, we explore an important technique in CNN models, the batch normalization (BN), for the task of image steganalysis in the paired learning framework. Our theoretical analysis shows that a CNN model with multiple batch normalization layers is difficult to be generalized to new data in the test set when it is well trained with paired learning. To address this problem, we propose a novel normalization technique called shared normalization (SN) in this paper. Unlike the BN layer utilizing the mini-batch mean and standard deviation to normalize each input batch, SN shares consistent statistics for training samples. Based on the proposed SN layer, we further propose a novel neural network model for image steganalysis. Extensive experiments demonstrate that the proposed network with SN layers is stable and can detect the state-of-the-art steganography with better performances than previous methods.},
  archive      = {J_TMM},
  author       = {Songtao Wu and Sheng-hua Zhong and Yan Liu},
  doi          = {10.1109/TMM.2019.2920605},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {256-270},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel convolutional neural network for image steganalysis with shared normalization},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep0Tag: Deep multiple instance learning for zero-shot
image tagging. <em>TMM</em>, <em>22</em>(1), 242–255. (<a
href="https://doi.org/10.1109/TMM.2019.2924511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning aims to perform visual reasoning about unseen objects. In-line with the success of deep learning on object recognition problems, several end-to-end deep models for zero-shot recognition have been proposed in the literature. These models are successful in predicting a single unseen label given an input image but do not scale to cases where multiple unseen objects are present. Here, we focus on the challenging problem of zero-shot image tagging, where multiple labels are assigned to an image, that may relate to objects, attributes, actions, events, and scene type. Discovery of these scene concepts requires the ability to process multi-scale information. To encompass global as well as local image details, we propose an automatic approach to locate relevant image patches and model image tagging within the Multiple Instance Learning (MIL) framework. To the best of our knowledge, we propose the first end-to-end trainable deep MIL framework for the multi-label zero-shot tagging problem. We explore several alternatives for instance-level evidence aggregation and perform an extensive ablation study to identify the optimal pooling strategy. Due to its novel design, the proposed framework has several interesting features: 1) unlike previous deep MIL models, it does not use any off-line procedure (e.g., Selective Search or EdgeBoxes) for bag generation. 2) During test time, it can process any number of unseen labels given their semantic embedding vectors. 3) Using only image-level seen labels as weak annotation, it can produce a localized bounding box for each predicted label. We experiment with the large-scale NUS-WIDE and MS-COCO datasets and achieve superior performance across conventional, zero-shot, and generalized zero-shot tagging tasks.},
  archive      = {J_TMM},
  author       = {Shafin Rahman and Salman Khan and Nick Barnes},
  doi          = {10.1109/TMM.2019.2924511},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {242-255},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep0Tag: Deep multiple instance learning for zero-shot image tagging},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). STAT: Spatial-temporal attention mechanism for video
captioning. <em>TMM</em>, <em>22</em>(1), 229–241. (<a
href="https://doi.org/10.1109/TMM.2019.2924576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning refers to automatic generate natural language sentences, which summarize the video contents. Inspired by the visual attention mechanism of human beings, temporal attention mechanism has been widely used in video description to selectively focus on important frames. However, most existing methods based on temporal attention mechanism suffer from the problems of recognition error and detail missing, because temporal attention mechanism cannot further catch significant regions in frames. In order to address above problems, we propose the use of a novel spatial-temporal attention mechanism (STAT) within an encoder-decoder neural network for video captioning. The proposed STAT successfully takes into account both the spatial and temporal structures in a video, so it makes the decoder to automatically select the significant regions in the most relevant temporal segments for word prediction. We evaluate our STAT on two well-known benchmarks: MSVD and MSR-VTT-10K. Experimental results show that our proposed STAT achieves the state-of-the-art performance with several popular evaluation metrics: BLEU-4, METEOR, and CIDEr.},
  archive      = {J_TMM},
  author       = {Chenggang Yan and Yunbin Tu and Xingzheng Wang and Yongbing Zhang and Xinhong Hao and Yongdong Zhang and Qionghai Dai},
  doi          = {10.1109/TMM.2019.2924576},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {229-241},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {STAT: Spatial-temporal attention mechanism for video captioning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Loopy residual hashing: Filling the quantization gap for
image retrieval. <em>TMM</em>, <em>22</em>(1), 215–228. (<a
href="https://doi.org/10.1109/TMM.2019.2922130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has been widely used in large-scale image retrieval based on approximate nearest neighbor search. Most learning-to-hashing methods adopt a two-stage algorithm to generate binary codes. First, original images are mapped into continuous visual features. Then, binary codes are generated by quantization step or separate projection. Nevertheless, these methods are sensitive to quantization operation, i.e., thresholding. To explicitly address this issue, this study proposes a novel feature quantization scheme with a loopy recurrent neural network, called loopy residual hashing, for the purpose of high accuracy in image retrieval. Instead of one-off thresholding-based feature binarization, the proposed approach performs an iterative threshold-then-approximate operation, which calculates the quantization residual after each thresholding step and then imitates another round of binarization to further approximate the coding residual. The resulting sequences of binary codes possess higher representation accuracy and extensive experiments on image retrieval demonstrate its superior discriminative capability over the prior art. In the meantime, theoretical approximation error analysis is given.},
  archive      = {J_TMM},
  author       = {Jiale Bai and Zefan Li and Bingbing Ni and Minsi Wang and Xiaokang Yang and Chuanping Hu and Wen Gao},
  doi          = {10.1109/TMM.2019.2922130},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {215-228},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Loopy residual hashing: Filling the quantization gap for image retrieval},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic spectrum access for multimedia transmission over
multi-user, multi-channel cognitive radio networks. <em>TMM</em>,
<em>22</em>(1), 201–214. (<a
href="https://doi.org/10.1109/TMM.2019.2925960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimal spectrum access strategy is investigated for multi-user multi-channel scenario in cognitive radio networks. At first, an online learning method based on Dirichlet Process is adopted to predict the channel usage based on ACK/NACK feedbacks, which can avoid frequent information exchange among users. Based on the prediction result, the delay performance can be computed when a user transmits certain percentage of multimedia packets on a specific channel. Second, the packet delivery ratio (PDR) is derived from the prediction result of channel usage to reflect the accessing competition among multiple users. Finally, the quality of service (QoS) of multimedia applications is defined as the joint delay and throughput performances. Moreover, a dynamic spectrum access scheme is proposed to optimize the QoS metrics. The simulation results demonstrate that the QoS and the peak-signal-to-noise ratio (PSNR) of the proposed spectrum access algorithm outperform the three existing spectrum access algorithms, i.e., cognitive cross-layer algorithm, dynamic learning algorithm, and dynamic least interference algorithm. The proposed algorithm achieves more than 21.8%, 5.4%, and 3.9% PDR enhancement and over 3.23 dB, 0.82 dB, and 0.50 dB PSNR gains, compared with those three algorithms, given the transmission power as 10, 20, and 30 units, respectively.},
  archive      = {J_TMM},
  author       = {Xin-Lin Huang and Xiao-Wei Tang and Fei Hu},
  doi          = {10.1109/TMM.2019.2925960},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {201-214},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic spectrum access for multimedia transmission over multi-user, multi-channel cognitive radio networks},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neighbourhood structure preserving cross-modal embedding for
video hyperlinking. <em>TMM</em>, <em>22</em>(1), 188–200. (<a
href="https://doi.org/10.1109/TMM.2019.2923121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video hyperlinking is a task aiming to enhance the accessibility of large archives, by establishing links between fragments of videos. The links model the aboutness between fragments for efficient traversal of video content. This paper addresses the problem of link construction from the perspective of cross-modal embedding. To this end, a generalized multi-modal auto-encoder is proposed. The encoder learns two embeddings from visual and speech modalities, respectively, whereas each of the embeddings performs self-modal and cross-modal translation of modalities. Furthermore, to preserve the neighbourhood structure of fragments, which is important for video hyperlinking, the auto-encoder is devised to model data distribution of fragments in a dataset. Experiments are conducted on Blip10000 dataset using the anchor fragments provided by TRECVid Video Hyperlinking (LNK) task over the years of 2016 and 2017. This paper shares the empirical insights on a number of issues in cross-modal learning, including the preservation of neighbourhood structure in embedding, model fine-tuning and issue of missing modality, for video hyperlinking.},
  archive      = {J_TMM},
  author       = {Yanbin Hao and Chong-Wah Ngo and Benoit Huet},
  doi          = {10.1109/TMM.2019.2923121},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {188-200},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neighbourhood structure preserving cross-modal embedding for video hyperlinking},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-pathway generative adversarial hashing for
unsupervised cross-modal retrieval. <em>TMM</em>, <em>22</em>(1),
174–187. (<a href="https://doi.org/10.1109/TMM.2019.2922128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing aims to map heterogeneous cross-modal data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods, since no intensive labeling work is involved. However, existing unsupervised methods learn the hashing functions by preserving inter- and intra-correlations while ignoring the underlying manifold structure across different modalities, which is extremely helpful in capturing the meaningful nearest neighbors of different modalities for cross-modal retrieval. Furthermore, existing works mainly focus on pairwise relation modeling while ignoring the correlations within multiple modalities. To address the above-mentioned problems, in this paper, we propose a multi-pathway generative adversarial hashing approach for unsupervised cross-modal retrieval, which makes full use of a generative adversarial network&#39;s ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows: First, we propose a multi-pathway generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed network, given the data of one modality, the generative model tries to fit the distribution over the manifold structure and selects informative data of other modalities to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from the correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. Second, we propose a correlation graph -based approach to capture the underlying manifold structure across different modalities so that data of different modalities but within the same manifold can have a smaller Hamming distance to promote retrieval accuracy. Extensive experiments compared with state-of-the-art methods on three widely used datasets verify the effectiveness of our proposed approach.},
  archive      = {J_TMM},
  author       = {Jian Zhang and Yuxin Peng},
  doi          = {10.1109/TMM.2019.2922128},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {174-187},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-pathway generative adversarial hashing for unsupervised cross-modal retrieval},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RGB-t image saliency detection via collaborative graph
learning. <em>TMM</em>, <em>22</em>(1), 160–173. (<a
href="https://doi.org/10.1109/TMM.2019.2924578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image saliency detection is an active research topic in the community of computer vision and multimedia. Fusing complementary RGB and thermal infrared data has been proven to be effective for image saliency detection. In this paper, we propose an effective approach for RGB-T image saliency detection. Our approach relies on a novel collaborative graph learning algorithm. In particular, we take superpixels as graph nodes, and collaboratively use hierarchical deep features to jointly learn graph affinity and node saliency in a unified optimization framework. Moreover, we contribute a more challenging dataset for the purpose of RGB-T image saliency detection, which contains 1000 spatially aligned RGB-T image pairs and their ground truth annotations. Extensive experiments on the public dataset and the newly created dataset suggest that the proposed approach performs favorably against the state-of-the-art RGB-T saliency detection methods.},
  archive      = {J_TMM},
  author       = {Zhengzheng Tu and Tian Xia and Chenglong Li and Xiaoxiao Wang and Yan Ma and Jin Tang},
  doi          = {10.1109/TMM.2019.2924578},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {160-173},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RGB-T image saliency detection via collaborative graph learning},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-task neural approach for emotion attribution,
classification, and summarization. <em>TMM</em>, <em>22</em>(1),
148–159. (<a href="https://doi.org/10.1109/TMM.2019.2922129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional content is a crucial ingredient in user-generated videos. However, the sparsity of emotional expressions in the videos poses an obstacle to visual emotion analysis. In this paper, we propose a new neural approach, Bi-stream Emotion Attribution-Classification Network (BEAC-Net), to solve three related emotion analysis tasks: emotion recognition, emotion attribution, and emotion-oriented summarization, in a single integrated framework. BEAC-Net has two major constituents, an attribution network and a classification network. The attribution network extracts the main emotional segment that classification should focus on in order to mitigate the sparsity issue. The classification network utilizes both the extracted segment and the original video in a bi-stream architecture. We contribute a new dataset for the emotion attribution task with human-annotated ground-truth labels for emotion segments. Experiments on two video datasets demonstrate superior performance of the proposed framework and the complementary nature of the dual classification streams.},
  archive      = {J_TMM},
  author       = {Guoyun Tu and Yanwei Fu and Boyang Li and Jiarui Gao and Yu-Gang Jiang and Xiangyang Xue},
  doi          = {10.1109/TMM.2019.2922129},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {148-159},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A multi-task neural approach for emotion attribution, classification, and summarization},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gestures in-the-wild: Detecting conversational hand gestures
in crowded scenes using a multimodal fusion of bags of video
trajectories and body worn acceleration. <em>TMM</em>, <em>22</em>(1),
138–147. (<a href="https://doi.org/10.1109/TMM.2019.2922122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the detection of hand gestures during free-standing conversations in crowded mingle scenarios. Unlike the scenarios of the previous works in gesture detection and recognition, crowded mingle scenes have additional challenges such as cross-contamination between subjects, strong occlusions, and nonstationary backgrounds. This makes them more complex to analyze using computer vision techniques alone. We propose a multimodal approach using video and wearable acceleration data recorded via smart badges hung around the neck. In the video modality, we propose to treat noisy dense trajectories as bags-of-trajectories. For a given bag, we can have good trajectories corresponding to the subject, and bad trajectories due for instance to cross-contamination. However, we hypothesize that for a given class, it should be possible to learn trajectories that are discriminative while ignoring noisy trajectories. We do this by exploiting multiple instance learning via embedded instance selection as our multiple instance learning approach. This technique also allows us to identify which instances contribute more to the classification. By fusing the decisions of the classifiers from the video and wearable acceleration modalities, we show improvements over the unimodal approaches with an AUC of 0.69. We also present a static analysis and a dynamic analysis to assess the impact of noisy data on the fused detection results, showing that the moments of high occlusion in the video are compensated by the information from the wearables. Finally, we applied our method to detect speaking status, leveraging the close relationship found in the literature between hand gestures and speech.},
  archive      = {J_TMM},
  author       = {Laura Cabrera-Quiros and David M. J. Tax and Hayley Hung},
  doi          = {10.1109/TMM.2019.2922122},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {138-147},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Gestures in-the-wild: Detecting conversational hand gestures in crowded scenes using a multimodal fusion of bags of video trajectories and body worn acceleration},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locally confined modality fusion network with a global
perspective for multimodal human affective computing. <em>TMM</em>,
<em>22</em>(1), 122–137. (<a
href="https://doi.org/10.1109/TMM.2019.2925966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel multimodal fusion framework, called the locally confined modality fusion network (LMFN), that contains a bidirectional multiconnected LSTM (BM-LSTM) to address the multimodal human affective computing problem. In the LMFN, we introduce a generic fusion structure that explores both local and global fusion to obtain an integral comprehension of information. Specifically, we partition the feature vector corresponding to each modality into multiple segments and learn every local interaction through a tensor fusion procedure. Global interaction is then modeled by learning the dependence between local tensors via an originally designed BM-LSTM architecture, establishing a direct connection of cells and states of local tensors that are several time steps apart. With the LMFN, we achieve advantages over other methods in the following aspects: 1) local interactions are successfully modeled using a feasible vector segmentation procedure that can explore cross-modal dynamics in a more specialized manner; 2) global interactions are modeled to obtain an integral view of multimodal information using BM-LSTM, which guarantees an adequate flow of information; and 3) our general fusion structure is highly extendable by applying other local and global fusion methods. Experiments show that the LMFN yields state-of-the-art results. Moreover, the LMFN achieves higher efficiency compared to other models by applying the outer product as the fusion method.},
  archive      = {J_TMM},
  author       = {Sijie Mai and Songlong Xing and Haifeng Hu},
  doi          = {10.1109/TMM.2019.2925966},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {122-137},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Locally confined modality fusion network with a global perspective for multimodal human affective computing},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using blockchain for improved video integrity verification.
<em>TMM</em>, <em>22</em>(1), 108–121. (<a
href="https://doi.org/10.1109/TMM.2019.2925961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A video record plays a crucial role in providing evidence for crime scenes or road accidents. However, the main problem with the video record is that it is often vulnerable to various video tampering attacks. Although visual evidence is required to conduct an integrity verification before investigations, it is still difficult for human vision to detect a forgery. In this paper, we propose a novel video integrity verification method (IVM) that takes advantage of a blockchain framework. The proposed method employs an effective blockchain model in centralized video data, by combining a hash-based message authentication code and elliptic curve cryptography to verify the integrity of a video. In our method, video content with a predetermined size (segments) is key-hashed in a real-time manner and stored in a chronologically chained fashion, thus establishing an irrefutable database. The verification process applies the same procedure to the video segment and generates a hash value that can be compared with the hash in the blockchain. The proposed IVM is implemented on a PC environment, as well as on an accident data recorder-embedded system for verification. The experimental results show that the proposed method has better detection capabilities and robustness toward various kinds of tampering, such as copy–move, insert, and delete, as compared to other state-of-the-art methods. An analysis based on execution time along with an increase in the number of blocks within the blockchain shows a minimal overhead in the proposed method.},
  archive      = {J_TMM},
  author       = {Sarala Ghimire and Jae Young Choi and Bumshik Lee},
  doi          = {10.1109/TMM.2019.2925961},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {108-121},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Using blockchain for improved video integrity verification},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep position-sensitive tracking. <em>TMM</em>,
<em>22</em>(1), 96–107. (<a
href="https://doi.org/10.1109/TMM.2019.2922125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification-based tracking strategies often face more challenges from intra-class discrimination than from inter-class separability. Even for deep convolutional neural networks that have been widely proven to be effective in various vision tasks, their intra-class discriminative capability is still limited by the weakness of softmax loss, especially for targets not seen in the training dataset. By taking intrinsic attributes of training samples into account, in this paper, we propose a position-sensitive loss coupled with softmax loss to achieve intra-class compactness and inter-class explicitness. Particularly, two additive margins are introduced to encode the position attribute for decision boundary maximization, which is also utilized with the proposed loss to supervise the fine-tuned features on the pre-trained model. With the nearest neighbor ranking measurement in the feature embedding domain, the whole scheme is able to reach an optimized balance between the feature-level inter-class semantic separability and instance-level intra-class relative distance ranking. We evaluate the proposed work on different popular benchmarks, and experimental results demonstrate that our tracking strategy performs favorably against most of the state-of-the-art trackers in the comparison of accuracy and robustness.},
  archive      = {J_TMM},
  author       = {Yufei Zha and Tao Ku and Yunqiang Li and Peng Zhang},
  doi          = {10.1109/TMM.2019.2922125},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {96-107},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep position-sensitive tracking},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient and secure image communication system based on
compressed sensing for IoT monitoring applications. <em>TMM</em>,
<em>22</em>(1), 82–95. (<a
href="https://doi.org/10.1109/TMM.2019.2923111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has attracted extensive attention in the information field. Its rapid development has promoted several monitoring application domains. However, the resource constraint of sensor nodes and the security of data transmission have emerged as significant issues. In this paper, an image communication system for IoT monitoring applications is exploited to solve the above-mentioned problems simultaneously. The proposed system can satisfy the requirements of sensor nodes for low computational complexity, low-energy consumption, and low storage overhead. We also present a new compressed sensing (CS) model, as well as the corresponding parallel reconstruction algorithm, which help to reduce the image encryption/decryption time. Based on chaotic systems, we integrate the quantization and diffusion operations into the system to further enhance the transmission security. The simulations are executed to demonstrate the feasibility and the effectiveness of the proposed method. Compared with the traditional CS, our numerical results indicate that the proposed model reduces 413 ms computation time and 3.13 × 10 6 elements stored for large-scale images. Besides, we verify the flexibility and the diversity of choosing two submatrices for different-sized images. Experimental results also show the proposed system performs well in terms of security performance. Particularly the key space reaches 2253.},
  archive      = {J_TMM},
  author       = {Lixiang Li and Guoqian Wen and Zeming Wang and Yixian Yang},
  doi          = {10.1109/TMM.2019.2923111},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {82-95},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient and secure image communication system based on compressed sensing for IoT monitoring applications},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QoE analysis of dense multiview video with head-mounted
devices. <em>TMM</em>, <em>22</em>(1), 69–81. (<a
href="https://doi.org/10.1109/TMM.2019.2924575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a system and methodology for the analysis of quality of experience factors for dense multiview (MV) video using a head-mounted device (HMD). An MV-HMD player has been designed and implemented to immerse the users in a virtual environment, where they are placed in front of a virtual lightfield display that shows a different viewpoint depending on the position of their head. This paper describes a methodology for the analysis of the subjective perception of the transition among views (motion parallax), which is specific to the visualization of MV content. While previous works simulated the user movement by predefined view paths or used complex devices to track them, this system allows the observer to move freely, varying the perspective of the scene while easily tracking the observer&#39;s position. This paper is, up to our knowledge, the first providing a complete framework for the assessment of this subjective factor using an HMD. The subjective results obtained using this framework are used to 1) assess the influence of the user movement, display settings, and content characteristics in the perception of smoothness in the view transition, and 2) analyze the performance and limitations of a prediction model for subjective smoothness scores.},
  archive      = {J_TMM},
  author       = {Javier Cubelos and Pablo Carballeira and Jesús Gutiérrez and Narciso García},
  doi          = {10.1109/TMM.2019.2924575},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {69-81},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {QoE analysis of dense multiview video with head-mounted devices},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online robust principal component analysis with change point
detection. <em>TMM</em>, <em>22</em>(1), 59–68. (<a
href="https://doi.org/10.1109/TMM.2019.2923097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust principal component analysis (PCA) is a key technique for dynamical high-dimensional data analysis, including background subtraction for surveillance video. Typically, robust PCA requires all observations to be stored in memory before processing. The batch manner makes robust PCA inefficient for big data. In this paper, we develop an efficient online robust PCA method, namely, online moving window robust principal component analysis (OMWRPCA). Unlike the existing algorithms, OMWRPCA can successfully track not only slowly changing subspaces but also abruptly changing subspaces. Embedding hypothesis testing into the algorithm enables OMWRPCA to detect change points of the underlying subspaces. Extensive numerical experiments, including real-time background subtraction, demonstrate the superior performance of OMWRPCA compared with other state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Wei Xiao and Xiaolin Huang and Fan He and Jorge Silva and Saba Emrani and Arin Chaudhuri},
  doi          = {10.1109/TMM.2019.2923097},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {59-68},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Online robust principal component analysis with change point detection},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative adversarial network-based intra prediction for
video coding. <em>TMM</em>, <em>22</em>(1), 45–58. (<a
href="https://doi.org/10.1109/TMM.2019.2924591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel intra prediction method is proposed to improve the video coding performance, in which the generative adversarial network (GAN) is adopted to intelligently remove the spatial redundancy with the inference process. The proposed GAN-based method improves the prediction by exploiting more information and generating more flexible prediction patterns. In particular, the intra prediction is modeled as an inpainting task, which is accomplished with the GAN model to fill in the missing part by conditioning on the available reconstructed pixels. As such, the learned GAN model is incorporated into both video encoder and decoder, and the rate-distortion optimization is performed for the competition between GAN-based intra prediction and traditional angular-based intra prediction to achieve better coding performance. The proposed scheme is implemented into the high-efficiency video coding test model (HM 16.17) and the versatile video coding test model (VTM 1.1). The experimental results show that the proposed algorithm can achieve 6.6%, 7.5%, and 7.5% under HM 16.17 and 6.75%, 7.63%, and 7.65% under VTM 1.1 bit rate savings on average for luma and chroma components in the intra coding scenario.},
  archive      = {J_TMM},
  author       = {Linwei Zhu and Sam Kwong and Yun Zhang and Shiqi Wang and Xu Wang},
  doi          = {10.1109/TMM.2019.2924591},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {45-58},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Generative adversarial network-based intra prediction for video coding},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Radiance–reflectance combined optimization and
structure-guided <span
class="math inline"><em>ℓ</em><sub>0</sub></span>-norm for single image
dehazing. <em>TMM</em>, <em>22</em>(1), 30–44. (<a
href="https://doi.org/10.1109/TMM.2019.2922127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outdoor images are subject to degradation regarding contrast and color because atmospheric particles scatter incoming light to a camera. Existing haze models that employ model-based dehazing methods cannot avoid the dehazing artifacts. These artifacts include color distortion and overenhancement around object boundaries because of the incorrect transmission estimation from a depth error in the skyline and the wrong haze information, especially in bright objects. To overcome this problem, we present a novel optimization-based dehazing algorithm that combines radiance and reflectance components with an additional refinement using a structure-guided ℓ 0 -norm filter. More specifically, we first estimate a weak reflectance map and optimize the transmission map based on the estimated reflectance map. Next, we estimate the structure-guided ℓ 0 transmission map to remove the dehazing artifacts. The experimental results show that the proposed method outperforms state-of-the-art algorithms in terms of qualitative and quantitative measures compared with simulated image pairs. In addition, the real-world enhancement results demonstrate that the proposed method can provide a high-quality image without undesired artifacts. Furthermore, the guided ℓ 0 -norm filter can remove textures while preserving edges for general image enhancement algorithms.},
  archive      = {J_TMM},
  author       = {Joongchol Shin and Minseo Kim and Joonki Paik and Sangkeun Lee},
  doi          = {10.1109/TMM.2019.2922127},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {30-44},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Radiance–Reflectance combined optimization and structure-guided $\ell _0$-norm for single image dehazing},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image vectorization with real-time thin-plate spline.
<em>TMM</em>, <em>22</em>(1), 15–29. (<a
href="https://doi.org/10.1109/TMM.2019.2922126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vector graphics with gradient mesh can be attributed to their compactness and scalability; however, they tend to fall short when it comes to real-time editing due to a lack of real-time rasterization and an efficient editing tool for image details. In this paper, we encode global manipulation geometries and local image details within a hybrid vector structure, using parametric patches and detailed features for localized and parallelized thin-plate spline interpolation in order to achieve good compressibility, interactive expressibility, and editability. The proposed system then automatically extracts an optimal set of detailed color features while considering the compression ratio of the image as well as reconstruction error and its characteristics applicable to the preservation of structural and irregular saliency of the image. The proposed real-time vector representation makes it possible to construct an interactive editing system for detail-maintained image magnification and color editing as well as material replacement in cross mapping, without maintaining spatial and temporal consistency while editing in a raster space. Experiments demonstrate that our representation method is superior to several state-of-the-art methods and as good as JPEG, while providing real-time editability and preserving structural and irregular saliency information.},
  archive      = {J_TMM},
  author       = {Kuo-Wei Chen and Ying-Sheng Luo and Yu-Chi Lai and Yan-Lin Chen and Chih-Yuan Yao and Hung-Kuo Chu and Tong-Yee Lee},
  doi          = {10.1109/TMM.2019.2922126},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {15-29},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image vectorization with real-time thin-plate spline},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative model driven representation learning in a hybrid
framework for environmental audio scene and sound event recognition.
<em>TMM</em>, <em>22</em>(1), 3–14. (<a
href="https://doi.org/10.1109/TMM.2019.2925956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of sound information is helpful for audio surveillance, multimedia information retrieval, audio tagging, and forensic applications. Environmental audio scene recognition (EASR) and sound event recognition (SER) for audio surveillance are challenging tasks due to the presence of multiple sound sources, background noises, and the existence of overlapping or polyphonic contexts. We focus on learning robust and compact representations for environmental audio scenes and sound events using mel-frequency cepstral coefficients as basic features, which have proved to be effective in speech and audio-related tasks. In this paper, we propose a common hybrid model-based framework that learns representations with the help of generative models. We explore instance-specific adapted Gaussian mixture models for environmental audio scenes and instance-specific hidden Markov models for sound events to compute a robust, compact, and discriminatory representations. A discriminative model based classifier is then used to recognize these representations as environmental audio scenes and sound events. The performance of the proposed approaches is evaluated using the DCASE2013 scene dataset and TUT-DCASE2016 scene dataset for EASR task. Environmental Sound Classification (ESC-10) and UrbanSound8K datasets are used for SER task. The recognition accuracy of the proposed framework is significantly better than many of the state-of-the-art approaches proposed in the recent literature. The discriminative nature of the model-driven representations leads to improved efficiency for EASR and SER task. The proposed approaches are more suitable for tasks with less training data.},
  archive      = {J_TMM},
  author       = {S. Chandrakala and S. L. Jayalakshmi},
  doi          = {10.1109/TMM.2019.2925956},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {3-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Generative model driven representation learning in a hybrid framework for environmental audio scene and sound event recognition},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial. <em>TMM</em>, <em>22</em>(1), 2. (<a
href="https://doi.org/10.1109/TMM.2019.2958500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2019.2958500},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {2},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Editorial},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Message from the outgoing editor-in-chief. <em>TMM</em>,
<em>22</em>(1), 1. (<a
href="https://doi.org/10.1109/TMM.2019.2956279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the message from the outgoing editor-in-chief for this publication.},
  archive      = {J_TMM},
  doi          = {10.1109/TMM.2019.2956279},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  number       = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Message from the outgoing editor-in-chief},
  volume       = {22},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
