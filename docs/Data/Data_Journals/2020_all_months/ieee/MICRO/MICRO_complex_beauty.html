<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro---114">MICRO - 114</h2>
<ul>
<li><details>
<summary>
(2020). ComputingEdge. <em>MICRO</em>, <em>40</em>(6), C4. (<a
href="https://doi.org/10.1109/MM.2020.3029377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE. ComputingEdge, your one-stop resource for industry hot topics, technical overviews, and in-depth articles. Cutting-edge articles from the IEEE Computer Society&#39;s portfolio of 12 magazines. Unique original content by computing thought leaders, innovators, and experts. Keeps you up to date on what you need to know across the technology spectrum.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3029377},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {C4},
  shortjournal = {IEEE Micro},
  title        = {ComputingEdge},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). The fox and shepherd problem. <em>MICRO</em>,
<em>40</em>(6), 86–88. (<a
href="https://doi.org/10.1109/MM.2020.3026528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on anti-trust issues that are likely to impact Facebook, Google/Alphabet, Apple, and Amazon.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2020.3026528},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {86-88},
  shortjournal = {IEEE Micro},
  title        = {The fox and shepherd problem},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). IEEE computer society. <em>MICRO</em>, <em>40</em>(6), 85.
(<a href="https://doi.org/10.1109/MM.2020.3029406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3029406},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {85},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer society},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). IEEE security &amp; privacy. <em>MICRO</em>,
<em>40</em>(6), 84. (<a
href="https://doi.org/10.1109/MM.2020.3031420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3031420},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {84},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power side-channel attacks in negative capacitance
transistor. <em>MICRO</em>, <em>40</em>(6), 74–84. (<a
href="https://doi.org/10.1109/MM.2020.3005883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side-channel attacks have empowered bypassing of cryptographic components in circuits. Power side-channel (PSC) attacks have received particular traction, owing to their noninvasiveness and proven effectiveness. Aside from prior art focused on conventional technologies, this is the first work to investigate the emerging negative capacitance transistor (NCFET) technology in the context of PSC attacks. We implement a CAD flow for the PSC evaluation at design time. It leverages industry-standard design tools, while also employing the widely accepted correlation power analysis (CPA) attack. Using standard-cell libraries based on the 7-nm FinFET technology for NCFET and its counterpart CMOS setup, our evaluation reveals that NCFET-based circuits are more resilient to the classical CPA attack, due to the considerable effect of negative capacitance on the switching power. We also demonstrate that the thicker the ferroelectric layer, the higher the resiliency of the NCFET-based circuit, which opens new doors for optimization and tradeoffs.},
  archive      = {J_MICRO},
  author       = {Johann Knechtel and Satwik Patnaik and Mohammed Nabeel and Mohammed Ashraf and Yogesh S. Chauhan and Jörg Henkel and Ozgur Sinanoglu and Hussam Amrouch},
  doi          = {10.1109/MM.2020.3005883},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {74-84},
  shortjournal = {IEEE Micro},
  title        = {Power side-channel attacks in negative capacitance transistor},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cloud-optimized transport protocol for elastic and
scalable HPC. <em>MICRO</em>, <em>40</em>(6), 67–73. (<a
href="https://doi.org/10.1109/MM.2020.3016891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amazon Web Services (AWS) took a fresh look at the network to provide consistently low latency required for supercomputing applications, while keeping the benefits of public cloud: scalability, elastic on-demand capacity, cost effectiveness, and fast adoption of newer CPUs and GPUs. We built a new network transport protocol, scalable reliable datagram (SRD), designed to utilize modern commodity multitenant datacenter networks (with a large number of network paths) while overcoming their limitations (load imbalance and inconsistent latency when unrelated flows collide). Instead of preserving packets order, SRD sends the packets over as many network paths as possible, while avoiding overloaded paths. To minimize jitter and to ensure the fastest response to network congestion fluctuations, SRD is implemented in the AWS custom Nitro networking card. SRD is used by HPC/ML frameworks on EC2 hosts via AWS elastic fabric adapter kernel-bypass interface.},
  archive      = {J_MICRO},
  author       = {Leah Shalev and Hani Ayoub and Nafea Bshara and Erez Sabbag},
  doi          = {10.1109/MM.2020.3016891},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {67-73},
  shortjournal = {IEEE Micro},
  title        = {A cloud-optimized transport protocol for elastic and scalable HPC},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). ITProfessional: Call for articles. <em>MICRO</em>,
<em>40</em>(6), 66. (<a
href="https://doi.org/10.1109/MM.2020.3031418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3031418},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {66},
  shortjournal = {IEEE Micro},
  title        = {ITProfessional: Call for articles},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Countering load-to-use stalls in the NVIDIA turing GPU.
<em>MICRO</em>, <em>40</em>(6), 59–66. (<a
href="https://doi.org/10.1109/MM.2020.3012514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among its various improvements over prior NVIDIA GPUs, the NVIDIA Turing GPU boasts of four key performance enhancements to effectively counter memory load-to-use stalls. First, reduced latency on L1 hits for global memory loads helps lower average memory lookup latency. Next, the ability to dynamically configure the L1 data RAM between cacheable memory and scratchpad or shared memory, enables driver software to opportunistically maximize L1 data cache size for programs with low shared memory requirements, increasing L1 hits and reducing load-to-use stalls. Finally, the twin enhancements of doubling of vector register file capacity and the addition of a dedicated scalar or uniform register file along with a uniform datapath, ease vector register pressure and enable higher warp level parallelism, leading to better latency tolerance. We find that the above enhancements combined deliver an average speedup of 11\% on modern gaming applications.},
  archive      = {J_MICRO},
  author       = {Ram Rangan and Naman Turakhia and Alexandre Joly},
  doi          = {10.1109/MM.2020.3012514},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {59-66},
  shortjournal = {IEEE Micro},
  title        = {Countering load-to-use stalls in the NVIDIA turing GPU},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IEEE annals of the history of computing. <em>MICRO</em>,
<em>40</em>(6), 58. (<a
href="https://doi.org/10.1109/MM.2020.3031406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3031406},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {58},
  shortjournal = {IEEE Micro},
  title        = {IEEE annals of the history of computing},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). History of IBM z mainframe processors. <em>MICRO</em>,
<em>40</em>(6), 50–58. (<a
href="https://doi.org/10.1109/MM.2020.3017107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IBM Z is both the oldest and among the most modern of computing platforms. Launched as S/360 in 1964, the mainframe became synonymous with large-scale computing for business and remains the workhorse of enterprise computing for businesses worldwide. Most of the world&#39;s largest banks, insurers, retailers, airlines, and enterprises from many other industries have IBM Z at the center of their IT infrastructure. This article presents an overview of the evolution of the IBM Z microprocessors over the past six generations. The article discusses some of the underlying workload characteristics and how these have influenced the microarchitecture enhancements driving the performance and capacity improvements. The article then describes how the focus shifted over time from speeds and feeds to new features, functions, and accelerators, and presents some examples on improved availability, enhanced security and cryptography, and embedded data compression acceleration.},
  archive      = {J_MICRO},
  author       = {Christian Jacobi and Charles Webb},
  doi          = {10.1109/MM.2020.3017107},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {50-58},
  shortjournal = {IEEE Micro},
  title        = {History of IBM z mainframe processors},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IEEE pervasive computing: Call for articles. <em>MICRO</em>,
<em>40</em>(6), 48. (<a
href="https://doi.org/10.1109/MM.2020.3031404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3031404},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {48},
  shortjournal = {IEEE Micro},
  title        = {IEEE pervasive computing: Call for articles},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FerroElectronics for edge intelligence. <em>MICRO</em>,
<em>40</em>(6), 33–48. (<a
href="https://doi.org/10.1109/MM.2020.3026667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The future data-centric world demands edge intelligence (EI) - the ability to analyze data locally and to decide on a course of action autonomously. Challenges with Moore&#39;s Law scaling and limitations of von Neumann computing architectures are limiting the performance and energy efficiency of conventional electronics. Promising new discoveries of advanced CMOS-compatible HfO2-based ferroelectric devices open the door for FerroElectronics; electronics based on ferroelectric building blocks integrated on advanced CMOS technology nodes. It will enable much needed improvement in computing capabilities making EI a reality. In-memory computing in data-flow architectures is at the core of FerroElectronics. This approach will enable building 1000X more compute-energy-efficient small-system AI engines needed for EI. Smart edge intelligent IoT devices enable new applications, for example, micro Drones(uDrones), that demand higher performance to support local embedded intelligence, real-time learning, and autonomy. They will drive the next phase of growth in the semiconductor industry.},
  archive      = {J_MICRO},
  author       = {Ali Keshavarzi and Kai Ni and Wilbert Van Den Hoek and Suman Datta and Arijit Raychowdhury},
  doi          = {10.1109/MM.2020.3026667},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {33-48},
  shortjournal = {IEEE Micro},
  title        = {FerroElectronics for edge intelligence},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating chip design with machine learning.
<em>MICRO</em>, <em>40</em>(6), 23–32. (<a
href="https://doi.org/10.1109/MM.2020.3026231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in machine learning provide an opportunity to transform chip design workflows. We review recent research applying techniques such as deep convolutional neural networks and graph-based neural networks in the areas of automatic design space exploration, power analysis, VLSI physical design, and analog design. We also present a future vision of an AI-assisted automated chip design workflow to aid designer productivity and automate optimization tasks.},
  archive      = {J_MICRO},
  author       = {Brucek Khailany and Haoxing Ren and Steve Dai and Saad Godil and Ben Keller and Robert Kirby and Alicia Klinefelter and Rangharajan Venkatesan and Yanqing Zhang and Bryan Catanzaro and William J. Dally},
  doi          = {10.1109/MM.2020.3026231},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {23-32},
  shortjournal = {IEEE Micro},
  title        = {Accelerating chip design with machine learning},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). IEEE COMPUTER SOCIETY: Call for papers. <em>MICRO</em>,
<em>40</em>(6), 22. (<a
href="https://doi.org/10.1109/MM.2020.3031233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3031233},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {22},
  shortjournal = {IEEE Micro},
  title        = {IEEE COMPUTER SOCIETY: Call for papers},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Circuits and architectures for in-memory computing-based
machine learning accelerators. <em>MICRO</em>, <em>40</em>(6), 8–22. (<a
href="https://doi.org/10.1109/MM.2020.3025863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning applications, especially deep neural networks (DNNs) have seen ubiquitous use in computer vision, speech recognition, and robotics. However, the growing complexity of DNN models have necessitated efficient hardware implementations. The key compute primitives of DNNs are matrix vector multiplications, which lead to significant data movement between memory and processing units in today&#39;s von Neumann systems. A promising alternative would be colocating memory and processing elements, which can be further extended to performing computations inside the memory itself. We believe in-memory computing is a propitious candidate for future DNN accelerators, since it mitigates the memory wall bottleneck. In this article, we discuss various in-memory computing primitives in both CMOS and emerging nonvolatile memory (NVM) technologies. Subsequently, we describe how such primitives can be incorporated in standalone machine learning accelerator architectures. Finally, we analyze the challenges associated with designing such in-memory computing accelerators and explore future opportunities.},
  archive      = {J_MICRO},
  author       = {Aayush Ankit and Indranil Chakraborty and Amogh Agrawal and Mustafa Ali and Kaushik Roy},
  doi          = {10.1109/MM.2020.3025863},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {8-22},
  shortjournal = {IEEE Micro},
  title        = {Circuits and architectures for in-memory computing-based machine learning accelerators},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chip design 2020. <em>MICRO</em>, <em>40</em>(6), 6–7. (<a
href="https://doi.org/10.1109/MM.2020.3028179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue of IEEE Micro aimed at publishing some of the most significant research that can highlight the trends in IC design in 2020 and provide directions for the future IC design era.},
  archive      = {J_MICRO},
  author       = {Jaydeep P. Kulkarni},
  doi          = {10.1109/MM.2020.3028179},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Chip design 2020},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Chip design 2020. <em>MICRO</em>, <em>40</em>(6), 4–5. (<a
href="https://doi.org/10.1109/MM.2020.3028542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2020.3028542},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Chip design 2020},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Masthead. <em>MICRO</em>, <em>40</em>(6), 1. (<a
href="https://doi.org/10.1109/MM.2020.3029381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3029381},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {1},
  shortjournal = {IEEE Micro},
  title        = {Masthead},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). Triggers, transmissions, and adjustments. <em>MICRO</em>,
<em>40</em>(5), 88–90. (<a
href="https://doi.org/10.1109/MM.2020.3014536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on recent economic conditions. Compare the recession unfolding in the United States at this moment with the two previous downturns. Today’s economic events share a surprising set of common features with the dot-com boom and bust of 1997 to 2001, and fewer similarities with the financial meltdown of 2008–2009. For reasons I will explain later, we ought to be thankful for the latter. To be sure, these are not obvious similarities. How will be arrive at such an observation? These similarities arise from the economic processes behind triggers, transmissions, and adjustments. While that might sound like the vocabulary of a foreign language, the explanation should be intuitive to many at technology firms, who tend to be first responders to the triggers, transmissions, and adjustments of economic recessions.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2020.3014536},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {88-90},
  shortjournal = {IEEE Micro},
  title        = {Triggers, transmissions, and adjustments},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IEEE computer society jobs board. <em>MICRO</em>,
<em>40</em>(5), 87. (<a
href="https://doi.org/10.1109/MM.2020.3020479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3020479},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {87},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer society jobs board},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Call for papers: IEEE transactions on computers.
<em>MICRO</em>, <em>40</em>(5), 86. (<a
href="https://doi.org/10.1109/MM.2020.3020477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3020477},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {86},
  shortjournal = {IEEE Micro},
  title        = {Call for papers: IEEE transactions on computers},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PurpleDrop: A digital microfluidics-based platform for
hybrid molecular-electronics applications. <em>MICRO</em>,
<em>40</em>(5), 76–86. (<a
href="https://doi.org/10.1109/MM.2020.3005615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular manipulation and analysis are the cornerstone of life sciences. With the recent advances in molecular data storage and computing, it has become an increasingly exciting and viable alternative for the post-CMOS scaling era. Widespread use of molecular manipulation/analysis and data storage/computing requires a scalable and low-cost platform for hybrid molecular-electronics systems. This enables us to build on the best of what molecular and electronics systems can offer. In this article, we present PurpleDrop, a full-stack digital microfluidic platform for hybrid molecular-electronic systems in multiple domains, and focus on DNA data storage as a use case. We describe its design principles and relevant aspects such as closed-loop operation with computer vision and capacitive sensing, on-board magnetic bead extraction, and polymerase chain reaction, among other primitives. Importantly, we emphasize the ability to express and execute protocols and computation that include molecular and computational components.},
  archive      = {J_MICRO},
  author       = {Ashley Stephenson and Max Willsey and Jeff McBride and Sharon Newman and Bichlien Nguyen and Christopher Takahashi and Karin Strauss and Luis Ceze},
  doi          = {10.1109/MM.2020.3005615},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {76-86},
  shortjournal = {IEEE Micro},
  title        = {PurpleDrop: A digital microfluidics-based platform for hybrid molecular-electronics applications},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating genome analysis: A primer on an ongoing
journey. <em>MICRO</em>, <em>40</em>(5), 65–75. (<a
href="https://doi.org/10.1109/MM.2020.3013728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genome analysis fundamentally starts with a process known as read mapping, where sequenced fragments of an organism&#39;s genome are compared against a reference genome. Read mapping is currently a major bottleneck in the entire genome analysis pipeline, because state-of-the-art genome sequencing technologies are able to sequence a genome much faster than the computational techniques employed to analyze the genome. We describe the ongoing journey in significantly improving the performance of read mapping. We explain state-of-the-art algorithmic methods and hardware-based acceleration approaches. Algorithmic approaches exploit the structure of the genome as well as the structure of the underlying hardware. Hardware-based acceleration approaches exploit specialized microarchitectures or various execution paradigms (e.g., processing inside or near memory). We conclude with the challenges of adopting these hardware-accelerated read mappers.},
  archive      = {J_MICRO},
  author       = {Mohammed Alser and Zülal Bingöl and Damla Senol Cali and Jeremie Kim and Saugata Ghose and Can Alkan and Onur Mutlu},
  doi          = {10.1109/MM.2020.3013728},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {65-75},
  shortjournal = {IEEE Micro},
  title        = {Accelerating genome analysis: A primer on an ongoing journey},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biology and systems interactions. <em>MICRO</em>,
<em>40</em>(5), 64. (<a
href="https://doi.org/10.1109/MM.2020.3016852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two articles in this special section focus on the intersection of systems and biology. Biological systems carry out computation and store information with efficiency levels and complexity that far exceeds those of synthetic computer systems. Consider, for example, that biological neural networks are estimated to offer at least four orders of magnitude with better ops/Joule than synthetic neural networks, or the storage advantages offered by molecular and DNA substrates, or the fact that sophisticated molecular pathways and mechanisms permit biological systems to interface with the most efficient sensing systems that we know of. At the same time, advancing our understanding of complex biological systems rests on continued scaling of the computational capabilities of high-performance and efficient computer systems, hardware, and software.},
  archive      = {J_MICRO},
  author       = {Abhishek Bhattacharjee},
  doi          = {10.1109/MM.2020.3016852},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {64},
  shortjournal = {IEEE Micro},
  title        = {Biology and systems interactions},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TSA-NoC: Learning-based threat detection and mitigation for
secure network-on-chip architecture. <em>MICRO</em>, <em>40</em>(5),
56–63. (<a href="https://doi.org/10.1109/MM.2020.3003576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks-on-chip (NoCs) are playing a critical role in modern multicore architecture, and NoC security has become a major concern. Maliciously implanted hardware Trojans (HTs) inject faults into on-chip communications that saturate the network, resulting in the leakage of sensitive data via side channels and significant performance degradation. While existing techniques protect NoCs by detecting and isolating HT-infected components, they inevitably incur occasional inaccurate detection with considerable network latency and power overheads. We propose TSA-NoC, a learning-based design framework for secure and efficient on-chip communication. The proposed TSA-NoC uses an artificial neural network for runtime HT-detection with higher accuracy. Furthermore, we propose a deep-reinforcement-learning-based adaptive routing design for HT mitigation with the aim of minimizing network latency and maximizing energy efficiency. Simulation results show that TSA-NoC achieves up to 97\% HT-detection accuracy, 70\% improved energy efficiency, and 29\% reduced network latency as compared to state-of-the-art HT-mitigation techniques.},
  archive      = {J_MICRO},
  author       = {Ke Wang and Hao Zheng and Ahmed Louri},
  doi          = {10.1109/MM.2020.3003576},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {56-63},
  shortjournal = {IEEE Micro},
  title        = {TSA-NoC: Learning-based threat detection and mitigation for secure network-on-chip architecture},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE security &amp; privacy. <em>MICRO</em>,
<em>40</em>(5), 55. (<a
href="https://doi.org/10.1109/MM.2020.3020465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3020465},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {55},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing model parallelism in neural architecture search
for multidevice system. <em>MICRO</em>, <em>40</em>(5), 46–55. (<a
href="https://doi.org/10.1109/MM.2020.3004538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) finds favorable network topologies for better task performance. Existing hardware-aware NAS techniques only target to reduce inference latency on single CPU/GPU systems and the searched model can hardly be parallelized. To address this issue, we propose ColocNAS, the first synchronization-aware, end-to-end NAS framework that automates the design of parallelizable neural networks for multidevice systems while maintaining a high task accuracy. ColocNAS defines a new search space with elaborated connectivity to reduce device communication and synchronization. ColocNAS consists of three phases: 1) offline latency profiling that constructs a lookup table of inference latency of various networks for online runtime approximation; 2) differentiable latency-aware NAS that simultaneously minimizes inference latency and task error; and 3) reinforcement-learning-based device placement fine-tuning to further reduce the latency of the deployed model. Extensive evaluation corroborates ColocNAS&#39;s effectiveness to reduce inference latency while preserving task accuracy.},
  archive      = {J_MICRO},
  author       = {Cheng Fu and Huili Chen and Zhenheng Yang and Farinaz Koushanfar and Yuandong Tian and Jishen Zhao},
  doi          = {10.1109/MM.2020.3004538},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {46-55},
  shortjournal = {IEEE Micro},
  title        = {Enhancing model parallelism in neural architecture search for multidevice system},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). ITProfessional: Call for articles. <em>MICRO</em>,
<em>40</em>(5), 45. (<a
href="https://doi.org/10.1109/MM.2020.3020463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3020463},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {45},
  shortjournal = {IEEE Micro},
  title        = {ITProfessional: Call for articles},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ReLeQ: A reinforcement learning approach for automatic deep
quantization of neural networks. <em>MICRO</em>, <em>40</em>(5), 37–45.
(<a href="https://doi.org/10.1109/MM.2020.3009475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Quantization (below eight bits) can significantly reduce the DNN computation and storage by decreasing the bitwidth of network encodings. However, without arduous manual effort, this deep quantization can lead to significant accuracy loss, leaving it in a position of questionable utility. We propose a systematic approach to tackle this problem, by automating the process of discovering the bitwidths through an end-to-end deep reinforcement learning framework (ReLeQ). This framework utilizes the sample efficiency of proximal policy optimization to explore the exponentially large space of possible assignment of the bitwidths to the layers. We show how ReLeQ can balance speed and quality, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks with minimal accuracy loss ($\leq$ ≤ 0.3\% loss) while minimizing the computation and storage costs. With these DNNs, ReLeQ enables conventional hardware and custom DNN accelerator to achieve $~2.2\times$ 2 . 2 × speedup over 8-bit execution.},
  archive      = {J_MICRO},
  author       = {Ahmed T. Elthakeb and Prannoy Pilligundla and Fatemehsadat Mireshghallah and Amir Yazdanbakhsh and Hadi Esmaeilzadeh},
  doi          = {10.1109/MM.2020.3009475},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {37-45},
  shortjournal = {IEEE Micro},
  title        = {ReLeQ: A reinforcement learning approach for automatic deep quantization of neural networks},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE computer society: Call for papers. <em>MICRO</em>,
<em>40</em>(5), 36. (<a
href="https://doi.org/10.1109/MM.2020.3020459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3020459},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {36},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer society: Call for papers},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A single-shot generalized device placement for large
dataflow graphs. <em>MICRO</em>, <em>40</em>(5), 26–36. (<a
href="https://doi.org/10.1109/MM.2020.3015188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasingly complex neural network architectures and heterogeneous device characteristics, finding a reasonable graph partitioning and device placement strategy is challenging. There have been prior attempts at learned approaches for solving device placement, these approaches are computationally expensive, unable to handle large graphs consisting over 50000 nodes, and do not generalize well to unseen graphs. To address all these limitations, we propose an efficient single-shot, generalized deep RL method (SGDP) based on a scalable sequential attention mechanism over a graph neural network that is transferable to new graphs. On a diverse set of representative deep learning models, our method on average achieves 20\% improvement over human placement and 18\% improvement over the prior art with 15× faster convergence. We are the first to demonstrate super human performance on 8-layer recurrent neural network language model and 8-layer GNMT consisting of over 50000 nodes, on 8-GPUs. We provide rationales and sensitivity study on model architecture selections.},
  archive      = {J_MICRO},
  author       = {Yanqi Zhou and Sudip Roy and Amirali Abdolrashidi and Daniel Lin-Kit Wong and Peter Ma and Qiumin Xu and Azalia Mirhoseini and James Laudon},
  doi          = {10.1109/MM.2020.3015188},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {26-36},
  shortjournal = {IEEE Micro},
  title        = {A single-shot generalized device placement for large dataflow graphs},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A programmable approach to neural network compression.
<em>MICRO</em>, <em>40</em>(5), 17–25. (<a
href="https://doi.org/10.1109/MM.2020.3012391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) frequently contain far more weights, represented at a higher precision, than are required for the specific task, which they are trained to perform. Consequently, they can often be compressed using techniques such as weight pruning and quantization that reduce both the model size and inference time without appreciable loss in accuracy. However, finding the best compression strategy and corresponding target sparsity for a given DNN, hardware platform, and optimization objective currently requires expensive, frequently manual, trial-and-error experimentation. In this article, we introduce a programmable system for model compression called CONDENSA. Users programmatically compose simple operators, in Python, to build more complex and practically interesting compression strategies. Given a strategy and user-provided objective (such as minimization of running time), CONDENSA uses a novel Bayesian optimization-based algorithm to automatically infer desirable sparsities. Our experiments on four real-world DNNs demonstrate memory footprint and hardware runtime throughput improvements of 188× and 2.59×, respectively, using at most ten samples per search.},
  archive      = {J_MICRO},
  author       = {Vinu Joseph and Ganesh L. Gopalakrishnan and Saurav Muralidharan and Michael Garland and Animesh Garg},
  doi          = {10.1109/MM.2020.3012391},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {17-25},
  shortjournal = {IEEE Micro},
  title        = {A programmable approach to neural network compression},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A taxonomy of ML for systems problems. <em>MICRO</em>,
<em>40</em>(5), 8–16. (<a
href="https://doi.org/10.1109/MM.2020.3012883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has the potential to significantly improve systems, but only under certain conditions. We describe a taxonomy to help identify whether or not machine learning should be applied to particular systems problems, and which approaches are most promising. We believe that this taxonomy can help practitioners and researchers decide how to most effectively use machine learning in their systems, and provide the community with a framework and vocabulary to discuss different approaches for applying machine learning in systems.},
  archive      = {J_MICRO},
  author       = {Martin Maas},
  doi          = {10.1109/MM.2020.3012883},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {8-16},
  shortjournal = {IEEE Micro},
  title        = {A taxonomy of ML for systems problems},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning for systems. <em>MICRO</em>,
<em>40</em>(5), 6–7. (<a
href="https://doi.org/10.1109/MM.2020.3016551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The six papers in this special section focus on machine learning for computer systems. Specialized computer systems have driven the performance and capability of deep learning over the past decade.1 However, as machine learning models and systems improve, there is a growing opportunity to also use these models to improve how we design, architect, optimize, and automate computer systems and software. This is a challenging area, both from a learning and a systems perspective. Systems often impose tight size, latency, or reliability constraints on learning mechanisms that do not arise in other applications of machine learning, such as computer vision or natural language processing. From a learning perspective, systems is a challenging application, where input features are often large and sparse, action spaces are gigantic, and generalization is a key attribute.},
  archive      = {J_MICRO},
  author       = {Heiner Litz and Milad Hashemi},
  doi          = {10.1109/MM.2020.3016551},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Machine learning for systems},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). Machine learning for systems, biological computing, and
more. <em>MICRO</em>, <em>40</em>(5), 4–5. (<a
href="https://doi.org/10.1109/MM.2020.3017880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2020.3017880},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Machine learning for systems, biological computing, and more},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Masthead. <em>MICRO</em>, <em>40</em>(5), 1. (<a
href="https://doi.org/10.1109/MM.2020.3016544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3016544},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {1},
  shortjournal = {IEEE Micro},
  title        = {Masthead},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). Uncomfortable economic waters. <em>MICRO</em>,
<em>40</em>(4), 134–136. (<a
href="https://doi.org/10.1109/MM.2020.3001464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on the economics of the corona virus and examines the implications for the future.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2020.3001464},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {134-136},
  shortjournal = {IEEE Micro},
  title        = {Uncomfortable economic waters},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolving career opportunities need your skills.
<em>MICRO</em>, <em>40</em>(4), 131. (<a
href="https://doi.org/10.1109/MM.2020.3006120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3006120},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {131},
  shortjournal = {IEEE Micro},
  title        = {Evolving career opportunities need your skills},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tydi: An open specification for complex data structures over
hardware streams. <em>MICRO</em>, <em>40</em>(4), 120–130. (<a
href="https://doi.org/10.1109/MM.2020.2996373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming dataflow designs describe hardware by connecting components through streams that transport data structures. We introduce a stream-oriented specification and type system that provides a clear and intuitive way to map complex, dynamically sized data structures onto hardware streams. This helps designers to lift the abstraction of streaming dataflow designs, reducing the design effort. The type system allows complex data structures to be as easy to use in streaming dataflow designs as in modern software languages today.},
  archive      = {J_MICRO},
  author       = {Johan Peltenburg and Jeroen Van Straten and Matthijs Brobbel and Zaid Al-Ars and H. Peter Hofstee},
  doi          = {10.1109/MM.2020.2996373},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {120-130},
  shortjournal = {IEEE Micro},
  title        = {Tydi: An open specification for complex data structures over hardware streams},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A case for accelerating software RTL simulation.
<em>MICRO</em>, <em>40</em>(4), 112–119. (<a
href="https://doi.org/10.1109/MM.2020.2997639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RTL simulation is a critical tool for hardware design but its current slow speed often bottlenecks the whole design process. Simulation speed becomes even more crucial for agile and open-source hardware design methodologies, because the designers not only want to iterate on designs quicker, but they may also have less resources with which to simulate them. In this article, we execute multiple simulators and analyze them with hardware performance counters. We find some open-source simulators not only outperform a leading commercial simulator, they also achieve comparable or higher instruction throughput on the host processor. Although advanced optimizations may increase the complexity of the simulator, they do not significantly hinder instruction throughput. Our findings make the case that there is significant room to accelerate software simulation and open-source simulators are a great starting point for researchers.},
  archive      = {J_MICRO},
  author       = {Scott Beamer},
  doi          = {10.1109/MM.2020.2997639},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {112-119},
  shortjournal = {IEEE Micro},
  title        = {A case for accelerating software RTL simulation},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). IEEE security &amp; privacy. <em>MICRO</em>,
<em>40</em>(4), 111. (<a
href="https://doi.org/10.1109/MM.2020.3005262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3005262},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {111},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LastLayer: Toward hardware and software continuous
integration. <em>MICRO</em>, <em>40</em>(4), 103–111. (<a
href="https://doi.org/10.1109/MM.2020.2997610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents LastLayer, an open-source tool that enables hardware and software continuous integration and simulation. Compared to traditional testing approaches based on the register transfer level abstraction, LastLayer provides a mechanism for testing Verilog designs with any programming language that supports the C foreign function interface. Furthermore, it supports a generic C interface that allows external programs convenient access to storage resources such as registers and memories in the design as well as control over the hardware simulation. Moreover, LastLayer achieves this software integration without requiring any hardware modification and automatically generates language bindings for these storage resources according to user specification. Using LastLayer, we evaluated two representative integration examples: a hardware adder written in Verilog operating over NumPy arrays, and a ReLu vector-accelerator written in Chisel processing tensors from PyTorch.},
  archive      = {J_MICRO},
  author       = {Luis Vega and Jared Roesch and Joseph McMahan and Luis Ceze},
  doi          = {10.1109/MM.2020.2997610},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {103-111},
  shortjournal = {IEEE Micro},
  title        = {LastLayer: Toward hardware and software continuous integration},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BlackParrot: An agile open-source RISC-v multicore for
accelerator SoCs. <em>MICRO</em>, <em>40</em>(4), 93–102. (<a
href="https://doi.org/10.1109/MM.2020.2996145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces BlackParrot, which aims to be the default open-source, Linux-capable, cache-coherent, 64-bit RISC-V multicore used by the world. In executing this goal, our research aims to advance the world&#39;s knowledge about the “software engineering of hardware.” Although originally bootstrapped by the University of Washington and Boston University via DARPA funding, BlackParrot strives to be community driven and infrastructure agnostic; a multicore which is Pareto optimal in terms of power, performance, area, and complexity. In order to ensure BlackParrot is easy to use, extend, and, most importantly, trust, development is guided by three core principles: Be Tiny, Be Modular, and Be Friendly. Development efforts have prioritized the use of intentional interfaces and modularity and silicon validation as first-order design metrics, so that users can quickly get started and trust that their design will perform as expected when deployed. BlackParrot has been validated in a GlobalFoundries 12-nm FinFET tapeout. BlackParrot is ideal as a standalone Linux processor or as a malleable fabric for an agile accelerator SoC design flow.},
  archive      = {J_MICRO},
  author       = {Daniel Petrisko and Farzam Gilani and Mark Wyse and Dai Cheol Jung and Scott Davidson and Paul Gao and Chun Zhao and Zahra Azad and Sadullah Canakci and Bandhav Veluri and Tavio Guarino and Ajay Joshi and Mark Oskin and Michael Bedford Taylor},
  doi          = {10.1109/MM.2020.2996145},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {93-102},
  shortjournal = {IEEE Micro},
  title        = {BlackParrot: An agile open-source RISC-V multicore for accelerator SoCs},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). IEEE security &amp; privacy. <em>MICRO</em>,
<em>40</em>(4), 92. (<a
href="https://doi.org/10.1109/MM.2020.3005300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3005300},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {92},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating systolic array accelerators with reusable blocks.
<em>MICRO</em>, <em>40</em>(4), 85–92. (<a
href="https://doi.org/10.1109/MM.2020.2997611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systolic array architecture is widely used in spatial hardware and well-suited for many tensor processing algorithms. Many systolic array architectures are implemented with high-level synthesis (HLS) design flow. However, existing HLS tools do not favor of modular and reusable design, which brings inefficiency for design iteration. In this article, we analyze the systolic array design space, and identify the common structures of different systolic dataflows. We build hardware module templates using Chisel infrastructure, which can be reused for different dataflows and computation algorithms. This remarkably improves the productivity for the development and optimization of systolic architecture. We further build a systolic array generator that transforms the tensor algorithm definition to a complete systolic hardware architecture. Experiments show that we can implement systolic array designs for different applications and dataflows with little engineering effort, and the performance throughput outperforms HLS designs.},
  archive      = {J_MICRO},
  author       = {Liancheng Jia and Liqiang Lu and Xuechao Wei and Yun Liang},
  doi          = {10.1109/MM.2020.2997611},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {85-92},
  shortjournal = {IEEE Micro},
  title        = {Generating systolic array accelerators with reusable blocks},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agile hardware development and instrumentation with PyRTL.
<em>MICRO</em>, <em>40</em>(4), 76–84. (<a
href="https://doi.org/10.1109/MM.2020.2997704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-specific architectures have emerged as a promising solution to meet growing technology demands but with this comes an urgent need to improve hardware methodologies which often have long design cycles, rely on closed source and expensive tools, and have high nonrecurring engineering costs. In this article, we describe how our work developing PyRTL, an open source Python-based Hardware Development Toolkit, has proven to be a powerful agile hardware development and analysis tool with the features to improve current methodologies. We describe how this toolkit-driven approach encourages hardware reuse using modern object-oriented programming features and present an examination of its custom intermediate representation for hardware debugging, analysis, and instrumentation. This approach has proven useful in supporting fast design iteration in a variety of domains including cryptography and machine learning.},
  archive      = {J_MICRO},
  author       = {Deeksha Dangwal and Georgios Tzimpragos and Timothy Sherwood},
  doi          = {10.1109/MM.2020.2997704},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {76-84},
  shortjournal = {IEEE Micro},
  title        = {Agile hardware development and instrumentation with PyRTL},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LiveHD: A productive live hardware development flow.
<em>MICRO</em>, <em>40</em>(4), 67–75. (<a
href="https://doi.org/10.1109/MM.2020.2996508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesis and simulation of hardware design can take hours before results are available even for small changes. In contrast, software development embraced live programming to boost productivity. This article proposes LiveHD, an open-source incremental framework for hardware synthesis and simulation that provides feedback within seconds. Three principles for incremental design automation are presented. LiveHD uses an unified VLSI data model, LGraph, to support the implementation of incremental principles for synthesis and simulation. LiveHD also employs a tree-like high-level intermediate representation to interface modern hardware description languages. We present early results comparing with commercial and open source tools. LiveHD can provides feedback for the synthesis, placement, and routing in &lt; 30 s for most changes tested with negligible QoR impact. For the incremental simulation, LiveHD is capable of getting any simulation cycle in under 2 s for a 256 RISC-V core design.},
  archive      = {J_MICRO},
  author       = {Sheng-Hong Wang and Rafael Trapani Possignolo and Haven Blake Skinner and Jose Renau},
  doi          = {10.1109/MM.2020.2996508},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {67-75},
  shortjournal = {IEEE Micro},
  title        = {LiveHD: A productive live hardware development flow},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PyMTL3: A python framework for open-source hardware
modeling, generation, simulation, and verification. <em>MICRO</em>,
<em>40</em>(4), 58–66. (<a
href="https://doi.org/10.1109/MM.2020.2997638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present PyMTL3, a Python framework for open-source hardware modeling, generation, simulation, and verification. In addition to compelling benefits from using the Python language, PyMTL3 is designed to provide flexible, modular, and extensible workflows for both hardware designers and computer architects. PyMTL3 supports a seamless multilevel modeling environment and carefully designed modular software architecture using a sophisticated in-memory intermediate representation and a collection of passes that analyze, instrument, and transform PyMTL3 hardware models. We believe PyMTL3 can play an important role in jump-starting the open-source hardware ecosystem.},
  archive      = {J_MICRO},
  author       = {Shunning Jiang and Peitian Pan and Yanghui Ou and Christopher Batten},
  doi          = {10.1109/MM.2020.2997638},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {58-66},
  shortjournal = {IEEE Micro},
  title        = {PyMTL3: A python framework for open-source hardware modeling, generation, simulation, and verification},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SymbiFlow and VPR: An open-source design flow for commercial
and novel FPGAs. <em>MICRO</em>, <em>40</em>(4), 49–57. (<a
href="https://doi.org/10.1109/MM.2020.2998435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the benefits of Moore&#39;s Law diminish, computing performance, and efficiency gains are increasingly achieved through specializing hardware to a domain of computation. However, this limits the hardware&#39;s generality and flexibility. Field-programmable gate arrays (FPGAs), microchips which can be reprogrammed to implement arbitrary digital circuits, enable the benefits of specialization while remaining flexible. A challenge to using FPGAs is the complex computer-aided design flow required to efficiently map a computation onto an FPGA. Traditionally, these design flows are closed-source and highly specialized to a particular vendor&#39;s devices. We propose an alternate data-driven approach, which uses highly adaptable and retargettable open-source tools to target both commercial and research FPGA architectures. While challenges remain, we believe this approach makes the development of novel and commercial FPGA architectures faster and more accessible. Furthermore, it provides a path forward for industry, academia, and the open-source community to collaborate and combine their resources to advance FPGA technology.},
  archive      = {J_MICRO},
  author       = {Kevin E. Murray and Mohamed A. Elgammal and Vaughn Betz and Tim Ansell and Keith Rothman and Alessandro Comodi},
  doi          = {10.1109/MM.2020.2998435},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {49-57},
  shortjournal = {IEEE Micro},
  title        = {SymbiFlow and VPR: An open-source design flow for commercial and novel FPGAs},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OpenFPGA: An open-source framework for agile prototyping
customizable FPGAs. <em>MICRO</em>, <em>40</em>(4), 41–48. (<a
href="https://doi.org/10.1109/MM.2020.2995854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demanded by ever-evolving data processing algorithms, field-programmable gate arrays (FPGAs) have become essential components of modern computing systems, thanks to their reconfigurable and distributed computing capabilities. However, FPGAs are among the very few integrated chips that still require long development cycles and high human efforts, even for industrial vendors. In this article, we introduce OpenFPGA, an open-source framework that can automate and significantly accelerate the development cycle of customizable FPGA architectures. OpenFPGA allows users to customize their FPGA architectures down to circuit-level details using a high-level architecture description language and autogenerate associated Verilog netlists which can be used in a backend flow to generate production-ready layouts. A generic Verilog-to-Bitstream generator is also provided, allowing end-users to implement practical applications on any FPGAs that OpenFPGA can support. Using OpenFPGA, we demonstrate less than 24-h layout generation of two FPGA fabrics, which are based on a Stratix-like architecture built with a commercial 12-nm standard cell library and 40-nm custom cells, respectively.},
  archive      = {J_MICRO},
  author       = {Xifan Tang and Edouard Giacomin and Baudouin Chauviere and Aurélien Alacchi and Pierre-Emmanuel Gaillardon},
  doi          = {10.1109/MM.2020.2995854},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {41-48},
  shortjournal = {IEEE Micro},
  title        = {OpenFPGA: An open-source framework for agile prototyping customizable FPGAs},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CHIPKIT: An agile, reusable open-source framework for rapid
test chip development. <em>MICRO</em>, <em>40</em>(4), 32–40. (<a
href="https://doi.org/10.1109/MM.2020.2995809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current trend for domain-specific architectures has led to renewed interest in research test chips to demonstrate new specialized hardware. Tapeouts also offer huge pedagogical value garnered from real hands-on exposure to the whole system stack. However, success with tapeouts requires hard-earned experience, and the design process is time consuming and fraught with challenges. Therefore, custom chips have remained the preserve of a small number of research groups, typically focused on circuit design research. This article describes the CHIPKIT framework: a reusable SoC subsystem which provides basic IO, an on-chip programmable host, off-chip hosting, memory, and peripherals. This subsystem can be readily extended with new IP blocks to generate custom test chips. Central to CHIPKIT is an agile RTL development flow, including a code generation tool called VGEN. Finally, we discuss best practices for full-chip validation across the entire design cycle.},
  archive      = {J_MICRO},
  author       = {Paul N. Whatmough and Marco Donato and Glenn G. Ko and Sae Kyu Lee and David Brooks and Gu-Yeon Wei},
  doi          = {10.1109/MM.2020.2995809},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {32-40},
  shortjournal = {IEEE Micro},
  title        = {CHIPKIT: An agile, reusable open-source framework for rapid test chip development},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OpenPiton at 5: A nexus for open and agile hardware design.
<em>MICRO</em>, <em>40</em>(4), 22–31. (<a
href="https://doi.org/10.1109/MM.2020.2997706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For five years, OpenPiton has provided hardware designs, build and verification scripts, and other infrastructure to enable efficient, detailed research into manycores and systems-on-chip. It enables open-source hardware development through its open design and support of a plethora of open simulators and CAD tools. OpenPiton was first designed to perform cutting-edge computer architecture research at Princeton University and opening it up to the public has led to thousands of downloads and numerous academic publications spanning many subfields within computing. In this article, we share some of the lessons learned during the development of OpenPiton, provide examples of how OpenPiton has been used to efficiently test novel research ideas, and discuss how OpenPiton has evolved due to its open development and feedback from the open-source community.},
  archive      = {J_MICRO},
  author       = {Jonathan Balkind and Ting-Jung Chang and Paul J. Jackson and Georgios Tziantzioulis and Ang Li and Fei Gao and Alexey Lavrov and Grigory Chirkov and Jinzheng Tu and Mohammad Shahrad and David Wentzlaff},
  doi          = {10.1109/MM.2020.2997706},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {22-31},
  shortjournal = {IEEE Micro},
  title        = {OpenPiton at 5: A nexus for open and agile hardware design},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE COMPUTER SOCIETY: Call for papers. <em>MICRO</em>,
<em>40</em>(4), 21. (<a
href="https://doi.org/10.1109/MM.2020.3005255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3005255},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {21},
  shortjournal = {IEEE Micro},
  title        = {IEEE COMPUTER SOCIETY: Call for papers},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chipyard: Integrated design, simulation, and implementation
framework for custom SoCs. <em>MICRO</em>, <em>40</em>(4), 10–21. (<a
href="https://doi.org/10.1109/MM.2020.2996616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continued improvement in computing efficiency requires functional specialization of hardware designs. Agile hardware design methodologies have been proposed to alleviate the increased design costs of custom silicon architectures, but their practice thus far has been accompanied with challenges in integration and validation of complex systems-on-a-chip (SoCs). We present the Chipyard framework, an integrated SoC design, simulation, and implementation environment for specialized compute systems. Chipyard includes configurable, composable, open-source, generator-based IP blocks that can be used across multiple stages of the hardware development flow while maintaining design intent and integration consistency. Through cloud-hosted FPGA accelerated simulation and rapid ASIC implementation, Chipyard enables continuous validation of physically realizable customized systems.},
  archive      = {J_MICRO},
  author       = {Alon Amid and David Biancolin and Abraham Gonzalez and Daniel Grubb and Sagar Karandikar and Harrison Liew and Albert Magyar and Howard Mao and Albert Ou and Nathan Pemberton and Paul Rigge and Colin Schmidt and John Wright and Jerry Zhao and Yakun Sophia Shao and Krste Asanović and Borivoje Nikolić},
  doi          = {10.1109/MM.2020.2996616},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {10-21},
  shortjournal = {IEEE Micro},
  title        = {Chipyard: Integrated design, simulation, and implementation framework for custom SoCs},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). IT professional. <em>MICRO</em>, <em>40</em>(4), 9. (<a
href="https://doi.org/10.1109/MM.2020.3006458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3006458},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {9},
  shortjournal = {IEEE Micro},
  title        = {IT professional},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agile and open-source hardware. <em>MICRO</em>,
<em>40</em>(4), 6–9. (<a
href="https://doi.org/10.1109/MM.2020.3002606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The thirteen articles in this special section focus on agile and open source hardware. These papers cover variety of research topics related to fast, agile, and open hardware design, including methodologies, languages, abstractions, and simulators. As the benefits f traditional technology scaling, like Dennard Scaling and Moore’s Law, slow significantly, computer architecture is poised to enter a golden age of innovation. Domain-specific architectures (DSA) are a promising solution to continue improving computing performance, while maintaining the level of energyand area-efficiency previously found in technology scaling. Unfortunately, traditional methodologies of chip design and hardware development have created significant barriers, requiring extremely high non-recurring engineering costs in tools, labor, IPs, and time, which ultimately hold back broad adoption of DSA.},
  archive      = {J_MICRO},
  author       = {Yungang Bao and Trevor E. Carlson},
  doi          = {10.1109/MM.2020.3002606},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {6-9},
  shortjournal = {IEEE Micro},
  title        = {Agile and open-source hardware},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Agile hardware design. <em>MICRO</em>, <em>40</em>(4), 4–5.
(<a href="https://doi.org/10.1109/MM.2020.3003092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2020.3003092},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Agile hardware design},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Masthead. <em>MICRO</em>, <em>40</em>(4), 1. (<a
href="https://doi.org/10.1109/MM.2020.3003393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.3003393},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {1},
  shortjournal = {IEEE Micro},
  title        = {Masthead},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Pandemics and the dismal technology economy.
<em>MICRO</em>, <em>40</em>(3), 118–120. (<a
href="https://doi.org/10.1109/MM.2020.2984182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on the economic impact of the COVID-19 pandemic. In spite of the lack of precedent, today’s economic crisis broadly contains predictable features. Here is why. The economy is one big circular flow of expenditure in which one person’s purchase is another person’s sale, and that further goes into somebody’s paycheck from which more purchases arise.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2020.2984182},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {118-120},
  shortjournal = {IEEE Micro},
  title        = {Pandemics and the dismal technology economy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). IEEE security &amp; privacy. <em>MICRO</em>,
<em>40</em>(3), 115. (<a
href="https://doi.org/10.1109/MM.2020.2996323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2996323},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {115},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trace wringing for program trace privacy. <em>MICRO</em>,
<em>40</em>(3), 108–115. (<a
href="https://doi.org/10.1109/MM.2020.2986113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A quantitative approach to optimizing computer systems requires a good understanding of how applications exercise a machine, and real program traces from production environments lead to the clearest understanding. Unfortunately, even the simplest program traces can leak sensitive details about users, their recent activity, or even details of trade secret algorithms. Given the cleverness of attackers working to undo well-intentioned, but ultimately insufficient, anonymization techniques, many organizations have simply decided to cease making traces available. Trace wringing is a new formulation of the problem of sharing traces where one knows a priori how much information the trace is leaking in the worst case. The key idea is to squeeze as much information as possible out of the trace without completely compromising its usefulness for optimization. We demonstrate the utility of a wrung trace through cache simulation and examine the sensitivity of wrung traces to a class of attacks on Advanced Encryption Standard (AES) encryption.},
  archive      = {J_MICRO},
  author       = {Deeksha Dangwal and Weilong Cui and Joseph McMahan and Timothy Sherwood},
  doi          = {10.1109/MM.2020.2986113},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {108-115},
  shortjournal = {IEEE Micro},
  title        = {Trace wringing for program trace privacy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Creating foundations for secure microarchitectures with
data-oblivious ISA extensions. <em>MICRO</em>, <em>40</em>(3), 99–107.
(<a href="https://doi.org/10.1109/MM.2020.2985366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is not possible to write microarchitectural side channel-free code on commercial processors today. Even when we try, the resulting code is low performance. This article&#39;s goal is to lay an ISA-level foundation, called a Data-Oblivious ISA (OISA) extension, to address these problems. The key idea with an OISA is to explicitly but abstractly specify security policy, so that the policy can be decoupled from the microarchitecture and even the threat model. Analogous to a traditional ISA, this enables an OISA to serve as a portable security-centric abstraction for software while enabling security-aware implementation and optimization flexibility for hardware. The article starts by giving a deep-dive in OISA principles and formal definitions underpinning OISA security. We also provide a concrete OISA built on top of RISC-V, an implementation prototype on the RISC-V BOOM microarchitecture, a formal analysis and security argument, and finally extensive performance evaluation on a range of data-oblivious benchmarks.},
  archive      = {J_MICRO},
  author       = {Jiyong Yu and Lucas Hsiung and Mohamad El Hajj and Christopher W. Fletcher},
  doi          = {10.1109/MM.2020.2985366},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {99-107},
  shortjournal = {IEEE Micro},
  title        = {Creating foundations for secure microarchitectures with data-oblivious ISA extensions},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MicroScope: Enabling microarchitectural replay attacks.
<em>MICRO</em>, <em>40</em>(3), 91–98. (<a
href="https://doi.org/10.1109/MM.2020.2986204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A microarchitectural replay attack is a novel class of attack where an adversary can denoise nearly arbitrary microarchitectural side channels in a single run of the victim. The idea is to cause the victim to repeatedly replay by inducing pipeline flushes. In this article, we design, implement, and demonstrate our ideas in a framework, called MicroScope, that causes repeated pipeline flushes by inducing page faults. Our main result shows that MicroScope can denoise the port contention channel of execution units. Specifically, we show how MicroScope can reliably detect the presence or absence of as few as two divide instructions in a single logical run of the victim program. We also discuss the broader implications of microarchitectural replay attacks.},
  archive      = {J_MICRO},
  author       = {Dimitrios Skarlatos and Mengjia Yan and Bhargava Gopireddy and Read Sprabery and Josep Torrellas and Christopher W. Fletcher},
  doi          = {10.1109/MM.2020.2986204},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {91-98},
  shortjournal = {IEEE Micro},
  title        = {MicroScope: Enabling microarchitectural replay attacks},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). IEEE computer graphics and applications. <em>MICRO</em>,
<em>40</em>(3), 90. (<a
href="https://doi.org/10.1109/MM.2020.2996321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2996321},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {90},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer graphics and applications},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Speculative taint tracking (STT): A comprehensive protection
for speculatively accessed data. <em>MICRO</em>, <em>40</em>(3), 81–90.
(<a href="https://doi.org/10.1109/MM.2020.2985359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speculative execution attacks present an enormous security threat, capable of reading arbitrary program data under malicious speculation, and later exfiltrating that data over microarchitectural covert channels. This article proposes speculative taint tracking (STT), a high-security and high-performance hardware mechanism to block these attacks. The main idea is that it is safe to execute and selectively forward the results of speculative instructions that read secrets, as long as we can prove that the forwarded results do not reach potential covert channels. The technical core of the article is a new abstraction to help identify all covert channels, and an architecture to quickly identify when a covert channel is no longer a threat. We further conduct a detailed formal analysis on the scheme and prove security in a companion document. When evaluated on SPEC06 workloads, STT incurs 8.5\% or 14.5\% performance overhead relative to an insecure machine.},
  archive      = {J_MICRO},
  author       = {Jiyong Yu and Mengjia Yan and Artem Khyzha and Adam Morrison and Josep Torrellas and Christopher W. Fletcher},
  doi          = {10.1109/MM.2020.2985359},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {81-90},
  shortjournal = {IEEE Micro},
  title        = {Speculative taint tracking (STT): A comprehensive protection for speculatively accessed data},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Architecting noisy intermediate-scale quantum computers: A
real-system study. <em>MICRO</em>, <em>40</em>(3), 73–80. (<a
href="https://doi.org/10.1109/MM.2020.2985683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current quantum computers have very different qubit implementations, instruction sets, qubit connectivity, and noise characteristics. Using real-system evaluations on seven quantum systems from three leading vendors, our work explores fundamental design questions concerning hardware choices, architecture, and compilation.},
  archive      = {J_MICRO},
  author       = {Prakash Murali and Norbert M. Linke and Margaret Martonosi and Ali Javadi Abhari and Nhung Hong Nguyen and Cinthia Huerta Alderete},
  doi          = {10.1109/MM.2020.2985683},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {73-80},
  shortjournal = {IEEE Micro},
  title        = {Architecting noisy intermediate-scale quantum computers: A real-system study},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extending the frontier of quantum computers with qutrits.
<em>MICRO</em>, <em>40</em>(3), 64–72. (<a
href="https://doi.org/10.1109/MM.2020.2985976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We advocate for a fundamentally different way to perform quantum computation by using three-level qutrits instead of qubits. In particular, we substantially reduce the resource requirements of quantum computations by exploiting a third state for temporary variables (ancilla) in quantum circuits. Past work with qutrits has demonstrated only constant factor improvements, owing to the log2(3) binary-to-ternary compression factor. We present a novel technique using qutrits to achieve a logarithmic runtime decomposition of the Generalized Toffoli gate using no ancilla---an exponential improvement over the best qubit-only equivalent. Our approach features a 70x improvement in total two-qudit gate count over the qubit-only decomposition. This results in improvements for important algorithms for arithmetic and QRAM. Simulation results under realistic noise models indicate over 90\% mean reliability (fidelity) for our circuit, versus under 30\% for the qubit-only baseline. These results suggest that qutrits offer a promising path toward extending the frontier of quantum computers.},
  archive      = {J_MICRO},
  author       = {Pranav Gokhale and Jonathan M. Baker and Casey Duckering and Frederic T. Chong and Natalie C. Brown and Kenneth R. Brown},
  doi          = {10.1109/MM.2020.2985976},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {64-72},
  shortjournal = {IEEE Micro},
  title        = {Extending the frontier of quantum computers with qutrits},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AsmDB: Understanding and mitigating front-end stalls in
warehouse-scale computers. <em>MICRO</em>, <em>40</em>(3), 56–63. (<a
href="https://doi.org/10.1109/MM.2020.2986212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that the datacenters hosting today&#39;s cloud services waste a significant number of cycles on front-end stalls. However, prior work has provided little insights about the source of these front-end stalls and how to address them. This work analyzes the cause of instruction cache misses at a fleet-wide scale and proposes a new compiler-driven software code prefetching strategy to reduce instruction caches misses by 90\%.},
  archive      = {J_MICRO},
  author       = {Nayana Prasad Nagendra and Grant Ayers and David I. August and Hyoun Kyu Cho and Svilen Kanev and Christos Kozyrakis and Trivikram Krishnamurthy and Heiner Litz and Tipp Moseley and Parthasarathy Ranganathan},
  doi          = {10.1109/MM.2020.2986212},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {56-63},
  shortjournal = {IEEE Micro},
  title        = {AsmDB: Understanding and mitigating front-end stalls in warehouse-scale computers},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Varifocal storage: Dynamic multiresolution data storage.
<em>MICRO</em>, <em>40</em>(3), 47–55. (<a
href="https://doi.org/10.1109/MM.2020.2985955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Varifocal storage (VS) presents a new architecture that coordinates application demands, hardware accelerators, and intelligent data storage devices to efficiently support various input resolutions of system components, but still maintain the flexibility and quality without additional costs. Instead of faithfully shipping the raw data, the cross-layer design of VS allows an intelligent storage device to work directly with the running application to generate and deliver data sets in the desired resolution and quality before going through the narrower system interconnect. In this way, VS minimizes the bandwidth demand from the data source and allows hardware accelerators to work on received data without additional preprocessing. Without programmers’ hints, VS achieves 1.46× speedup on a computer with approximate hardware accelerators.},
  archive      = {J_MICRO},
  author       = {Yu-Ching Hu and Murtuza Lokhandwala and Te I and Hung-Wei Tseng},
  doi          = {10.1109/MM.2020.2985955},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {47-55},
  shortjournal = {IEEE Micro},
  title        = {Varifocal storage: Dynamic multiresolution data storage},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards general-purpose acceleration: Finding structure in
irregularity. <em>MICRO</em>, <em>40</em>(3), 37–46. (<a
href="https://doi.org/10.1109/MM.2020.2986199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programmable hardware accelerators (e.g., vector processors, GPUs) have been extremely successful at targeting algorithms with regular control and memory patterns to achieve order-of-magnitude performance and energy efficiency improvements. However, they perform far under the peak on important irregular algorithms, like those from graph processing, database querying, genomics, advanced machine learning, and others. This work posits that the primary culprit is specific forms of irregular control flow and memory access. By capturing the problematic behavior at a domain-agnostic level, we propose an accelerator that is sufficiently general, matches domain-specific accelerator performance, and significantly outperforms traditional CPUs and GPUs.},
  archive      = {J_MICRO},
  author       = {Vidushi Dadu and Jian Weng and Sihao Liu and Tony Nowatzki},
  doi          = {10.1109/MM.2020.2986199},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {37-46},
  shortjournal = {IEEE Micro},
  title        = {Towards general-purpose acceleration: Finding structure in irregularity},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-efficient video processing for virtual reality.
<em>MICRO</em>, <em>40</em>(3), 30–36. (<a
href="https://doi.org/10.1109/MM.2020.2985692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) has huge potential to enable radically new applications, behind which spherical panoramic video processing is one of the backbone techniques. However, current VR systems reuse the techniques designed for processing conventional planar videos, resulting in significant energy inefficiencies. Our characterizations show that operations that are unique to processing 360° VR content constitute 40\% of the total processing energy consumption. We present EVR, an end-to-end system for energy-efficient VR video processing. EVR recognizes that the major contributor to the VR tax is the projective transformation (PT) operations. EVR mitigates the overhead of PT through two key techniques: semantic-aware streaming on the server and hardware-accelerated rendering on the client device. Real system measurements show that EVR reduces the energy of VR rendering by up to 58\%, which translates to up to 42\% energy saving for VR devices.},
  archive      = {J_MICRO},
  author       = {Yue Leng and Jian Huang and Chi-Chun Chen and Qiuyue Sun and Yuhao Zhu},
  doi          = {10.1109/MM.2020.2985692},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {30-36},
  shortjournal = {IEEE Micro},
  title        = {Energy-efficient video processing for virtual reality},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MAESTRO: A data-centric approach to understand reuse,
performance, and hardware cost of DNN mappings. <em>MICRO</em>,
<em>40</em>(3), 20–29. (<a
href="https://doi.org/10.1109/MM.2020.2985963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of an accelerator depends on three factors-mapping, deep neural network (DNN) layers, and hardware-constructing extremely complicated design space of DNN accelerators. To demystify such complicated design space and guide the DNN accelerator design for better efficiency, we propose an analytical cost model, MAESTRO. MAESTRO receives DNN model description and hardware resources information as a list, and mapping described in a data-centric representation we propose as inputs. The data-centric representation consists of three directives that enable concise description of mappings in a compiler-friendly form. MAESTRO analyzes various forms of data reuse in an accelerator based on inputs quickly and generates more than 20 statistics including total latency, energy, throughput, etc., as outputs. MAESTRO&#39;s fast analysis enables various optimization tools for DNN accelerators such as hardware design exploration tool we present as an example.},
  archive      = {J_MICRO},
  author       = {Hyoukjun Kwon and Prasanth Chatarasi and Vivek Sarkar and Tushar Krishna and Michael Pellauer and Angshuman Parashar},
  doi          = {10.1109/MM.2020.2985963},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {20-29},
  shortjournal = {IEEE Micro},
  title        = {MAESTRO: A data-centric approach to understand reuse, performance, and hardware cost of DNN mappings},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unveiling the hardware and software implications of
microservices in cloud and edge systems. <em>MICRO</em>, <em>40</em>(3),
10–19. (<a href="https://doi.org/10.1109/MM.2020.2985960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud services progressively shift from monolithic applications to complex graphs of loosely-coupled microservices. This article aims at understanding the implications microservices have across the system stack, from hardware acceleration and server design, to operating systems and networking, cluster management, and programming frameworks. Toward this effort, we have designed an open-sourced DeathstarBench, a benchmark suite for interactive microservices that is both representative and extensible.},
  archive      = {J_MICRO},
  author       = {Yu Gan and Yanqi Zhang and Dailun Cheng and Ankitha Shetty and Priyal Rathi and Nayan Katarki and Ariana Bruno and Justin Hu and Brian Ritchken and Brendon Jackson and Kelvin Hu and Meghna Pancholi and Yuan He and Brett Clancy and Chris Colen and Fukang Wen and Catherine Leung and Siyuan Wang and Leon Zaruvinsky and Mateo Espinosa and Rick Lin and Zhongling Liu and Jake Padilla and Christina Delimitrou},
  doi          = {10.1109/MM.2020.2985960},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {10-19},
  shortjournal = {IEEE Micro},
  title        = {Unveiling the hardware and software implications of microservices in cloud and edge systems},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The 2019 top picks in computer architecture. <em>MICRO</em>,
<em>40</em>(3), 6–9. (<a
href="https://doi.org/10.1109/MM.2020.2992834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This annual publication presents 12 articles selected from major computer architecture conferences of the year. The 12 papers are recognized for their importance, mainly the long-term impact and influence on the industry and other researchers. The selection committee members put enormous effort into picking the papers. We asked what the criteria should be for the top picks, and then we tried to answer that question by looking for significant improvement over previous work, establishing a new area.},
  archive      = {J_MICRO},
  author       = {Hyesoon Kim},
  doi          = {10.1109/MM.2020.2992834},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {6-9},
  shortjournal = {IEEE Micro},
  title        = {The 2019 top picks in computer architecture},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). Enjoy these top picks, while you work from home!
<em>MICRO</em>, <em>40</em>(3), 4–5. (<a
href="https://doi.org/10.1109/MM.2020.2993184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2020.2993184},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Enjoy these top picks, while you work from home!},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). Masthead. <em>MICRO</em>, <em>40</em>(3), 1. (<a
href="https://doi.org/10.1109/MM.2020.2988539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2988539},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {1},
  shortjournal = {IEEE Micro},
  title        = {Masthead},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Computing edge. <em>MICRO</em>, <em>40</em>(2), C4. (<a
href="https://doi.org/10.1109/MM.2020.2978359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2978359},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {C4},
  shortjournal = {IEEE Micro},
  title        = {Computing edge},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Expertise at our fingertips. <em>MICRO</em>,
<em>40</em>(2), 74–76. (<a
href="https://doi.org/10.1109/MM.2020.2971915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This column contrasts the economics behind yesterday’s compendium of expertise and today’s crowd-sourced wiki. Any comparison, even a coarse one, will show that prices fell dramatically.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2020.2971915},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {74-76},
  shortjournal = {IEEE Micro},
  title        = {Expertise at our fingertips},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IEEE quantum week 2020. <em>MICRO</em>, <em>40</em>(2), 72.
(<a href="https://doi.org/10.1109/MM.2020.2981244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2981244},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {72},
  shortjournal = {IEEE Micro},
  title        = {IEEE quantum week 2020},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IT professional. <em>MICRO</em>, <em>40</em>(2), 71. (<a
href="https://doi.org/10.1109/MM.2020.2981241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2981241},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {71},
  shortjournal = {IEEE Micro},
  title        = {IT professional},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TeraPHY: A chiplet technology for low-power, high-bandwidth
in-package optical i/o. <em>MICRO</em>, <em>40</em>(2), 63–71. (<a
href="https://doi.org/10.1109/MM.2020.2976067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present TeraPHY, a monolithic electronic–photonic chiplet technology for low power and low latency, multi-Tb/s chip-to-chip communications. Integration of the TeraPHY optical technology with open source advanced interconnect bus interface enables communication between chips at board, rack, and row level at the energy and latency cost of in-package interconnect. This enables the design of logically connected but physically separated large-scale and high-performance digital systems. The copackaging integration approach is demonstrated by integrating the TeraPHY die into the Intel Stratix10 FPGA multichip package.},
  archive      = {J_MICRO},
  author       = {Mark Wade and Erik Anderson and Shahab Ardalan and Pavan Bhargava and Sidney Buchbinder and Michael L. Davenport and John Fini and Haiwei Lu and Chen Li and Roy Meade and Chandru Ramamurthy and Michael Rust and Forrest Sedgwick and Vladimir Stojanovic and Derek Van Orden and Chong Zhang and Chen Sun and Sergey Y. Shumarayev and Conor O&#39;Keeffe and Tim T. Hoang and David Kehlet and Ravi V. Mahajan and Matthew T. Guzy and Allen Chan and Tina Tran},
  doi          = {10.1109/MM.2020.2976067},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {63-71},
  shortjournal = {IEEE Micro},
  title        = {TeraPHY: A chiplet technology for low-power, high-bandwidth in-package optical I/O},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The arm neoverse n1 platform: Building blocks for the
next-gen cloud-to-edge infrastructure SoC. <em>MICRO</em>,
<em>40</em>(2), 53–62. (<a
href="https://doi.org/10.1109/MM.2020.2972222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen an explosion of demand for high-performance, high-efficiency compute available at scale. This demand has skyrocketed with the move to the public cloud and 5G networking, where compute nodes must operate within strict latency constraints and power budgets. The Neoverse N1 platform is Arm&#39;s latest high end offering from a scalable portfolio of IP for high performance and energy efficient machines.},
  archive      = {J_MICRO},
  author       = {Andrea Pellegrini and Nigel Stephens and Magnus Bruce and Yasuo Ishii and Joseph Pusdesris and Abhishek Raja and Chris Abernathy and Jinson Koppanalil and Tushar Ringe and Ashok Tummala and Jamshed Jalal and Mark Werkheiser and Anitha Kona},
  doi          = {10.1109/MM.2020.2972222},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {53-62},
  shortjournal = {IEEE Micro},
  title        = {The arm neoverse n1 platform: Building blocks for the next-gen cloud-to-edge infrastructure SoC},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). IEEE security &amp; privacy. <em>MICRO</em>,
<em>40</em>(2), 52. (<a
href="https://doi.org/10.1109/MM.2020.2981184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2981184},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {52},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The AMD “zen 2” processor. <em>MICRO</em>, <em>40</em>(2),
45–52. (<a href="https://doi.org/10.1109/MM.2020.2974217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The “Zen 2” processor is designed to meet the needs of diverse markets spanning server, desktop, mobile, and workstation. The core delivers significant performance and energy-efficiency improvements over “Zen” by microarchitectural changes including a new TAGE branch predictor, a double-size op cache, and a double-width floating-point unit. Building upon the core design, a modular chiplet approach provides flexibility and scalability up to 64 cores per socket with a total of 256 MB of L3 cache.},
  archive      = {J_MICRO},
  author       = {David Suggs and Mahesh Subramony and Dan Bouvier},
  doi          = {10.1109/MM.2020.2974217},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {45-52},
  shortjournal = {IEEE Micro},
  title        = {The AMD “Zen 2” processor},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RTX on—the NVIDIA turing GPU. <em>MICRO</em>,
<em>40</em>(2), 36–44. (<a
href="https://doi.org/10.1109/MM.2020.2971677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NVIDIA&#39;s latest processor family, the Turing GPU, was designed to realize a vision for next-generation graphics combining rasterization, ray tracing, and deep learning. It includes fundamental advancements in several key areas: streaming multiprocessor efficiency, a Tensor Core for accelerated AI inferencing, and an RTCore for accelerated ray tracing. With these innovations, Turing unlocks both real-time ray-tracing performance and deep-learning inference in consumer, professional, and datacenter solutions.},
  archive      = {J_MICRO},
  author       = {John Burgess},
  doi          = {10.1109/MM.2020.2971677},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {36-44},
  shortjournal = {IEEE Micro},
  title        = {RTX on—The NVIDIA turing GPU},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compute solution for tesla’s full self-driving computer.
<em>MICRO</em>, <em>40</em>(2), 25–35. (<a
href="https://doi.org/10.1109/MM.2020.2975764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tesla&#39;s full self-driving (FSD) computer is the world&#39;s first purpose-built computer for the highly demanding workloads of autonomous driving. It is based on a new System on a Chip (SoC) that integrates industry standard components such as CPUs, ISP, and GPU, together with our custom neural network accelerators. The FSD computer is capable of processing up to 2300 frames per second, a 21× improvement over Tesla&#39;s previous hardware and at a lower cost, and when fully utilized, enables a new level of safety and autonomy on the road.},
  archive      = {J_MICRO},
  author       = {Emil Talpes and Debjit Das Sarma and Ganesh Venkataramanan and Peter Bannon and Bill McGee and Benjamin Floering and Ankit Jalote and Christopher Hsiong and Sahil Arora and Atchyuth Gorti and Gagandeep S. Sachdev},
  doi          = {10.1109/MM.2020.2975764},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {25-35},
  shortjournal = {IEEE Micro},
  title        = {Compute solution for tesla&#39;s full self-driving computer},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Habana labs purpose-built AI inference and training
processor architectures: Scaling AI training systems using standard
ethernet with gaudi processor. <em>MICRO</em>, <em>40</em>(2), 17–24.
(<a href="https://doi.org/10.1109/MM.2020.2975185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing computational requirements of AI applications are challenging today&#39;s general-purpose CPU and GPU architectures and driving the need for purpose-built, programmable AI solutions. Habana Labs designed its Goya processor to meet the high throughput/low latency demands of Inference workloads, and its Gaudi processor for throughput combined with massive scale up and scale out capability needed to speed training workloads efficiently. To address the need for scaling training, Habana is the first AI chip developer to integrate standard Ethernet onto a training processor.},
  archive      = {J_MICRO},
  author       = {Eitan Medina and Eran Dagan},
  doi          = {10.1109/MM.2020.2975185},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {17-24},
  shortjournal = {IEEE Micro},
  title        = {Habana labs purpose-built AI inference and training processor architectures: Scaling AI training systems using standard ethernet with gaudi processor},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MLPerf: An industry standard benchmark suite for machine
learning performance. <em>MICRO</em>, <em>40</em>(2), 8–16. (<a
href="https://doi.org/10.1109/MM.2020.2974843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we describe the design choices behind MLPerf, a machine learning performance benchmark that has become an industry standard. The first two rounds of the MLPerf Training benchmark helped drive improvements to software-stack performance and scalability, showing a 1.3× speedup in the top 16-chip results despite higher quality targets and a 5.5× increase in system scale. The first round of MLPerf Inference received over 500 benchmark results from 14 different organizations, showing growing adoption.},
  archive      = {J_MICRO},
  author       = {Peter Mattson and Vijay Janapa Reddi and Christine Cheng and Cody Coleman and Greg Diamos and David Kanter and Paulius Micikevicius and David Patterson and Guenther Schmuelling and Hanlin Tang and Gu-Yeon Wei and Carole-Jean Wu},
  doi          = {10.1109/MM.2020.2974843},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {8-16},
  shortjournal = {IEEE Micro},
  title        = {MLPerf: An industry standard benchmark suite for machine learning performance},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The hot chips renaissance. <em>MICRO</em>, <em>40</em>(2),
6–7. (<a href="https://doi.org/10.1109/MM.2020.2977409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles in this special section report on the technologies and events that were part of the 31st Annual Hot Chips symposium was held at Stanford University in August 2019.},
  archive      = {J_MICRO},
  author       = {Christos Kozyrakis and Ian Bratt},
  doi          = {10.1109/MM.2020.2977409},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {The hot chips renaissance},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE computer graphics and applications. <em>MICRO</em>,
<em>40</em>(2), 5. (<a
href="https://doi.org/10.1109/MM.2020.2981222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2981222},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {5},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer graphics and applications},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). Did ML chips heat up the chip design arena? <em>MICRO</em>,
<em>40</em>(2), 4–5. (<a
href="https://doi.org/10.1109/MM.2020.2978375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2020.2978375},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Did ML chips heat up the chip design arena?},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). Masthead. <em>MICRO</em>, <em>40</em>(2), 1. (<a
href="https://doi.org/10.1109/MM.2020.2978353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2978353},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {1},
  shortjournal = {IEEE Micro},
  title        = {Masthead},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Computing edge. <em>MICRO</em>, <em>40</em>(1), C4. (<a
href="https://doi.org/10.1109/MM.2019.2961446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2019.2961446},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {C4},
  shortjournal = {IEEE Micro},
  title        = {Computing edge},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). The vital two percent. <em>MICRO</em>, <em>40</em>(1),
94–96. (<a href="https://doi.org/10.1109/MM.2019.2958726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced economies struggle with paying to stretch the knowledge frontier. It is no surprise why. Research and development (R&amp;D) costs money today, and yields the benefit tomorrow, sometimes decades later. The benefits also disperse widely, making it difficult to trace a direct chain between expenditure and result, or calculate any direct return on investment. Despite the challenges, all modern economies devote some fraction of GDP to stretching the frontier with R&amp;D. In the United States, and for a few decades, that fraction has hovered around 2\%–2.5\% of GDP, with the Federal government covering about one fifth (more than $115 billion in 2017). Presents an analysis of this economic debate.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2019.2958726},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {94-96},
  shortjournal = {IEEE Micro},
  title        = {The vital two percent},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Keep your career options open. <em>MICRO</em>,
<em>40</em>(1), 93. (<a
href="https://doi.org/10.1109/MM.2020.2964407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2964407},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {93},
  shortjournal = {IEEE Micro},
  title        = {Keep your career options open},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In-hardware moving compute to data model to accelerate
thread synchronization on large multicores. <em>MICRO</em>,
<em>40</em>(1), 83–92. (<a
href="https://doi.org/10.1109/MM.2019.2955079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the moving computation to data model (MC2D) is proposed to accelerate thread synchronization by pinning shared data to dedicated cores, and utilize in-hardware core-to-core messaging to communicate critical code execution. The MC2D model optimizes shared data locality by eliminating unnecessary data movement, and alleviates contended synchronization using nonblocking communication between threads. This article evaluates task-parallel algorithms under their synchronization-centric classification to demonstrate that the effectiveness of the MC2D model to exploit performance correlates with the number and frequency of synchronizations. The evaluation on Tilera TILE-Gx72 multicore shows that the MC2D model delivers highest performance scaling gains for ordered and unordered algorithms that expose significant synchronizations due to task and data level dependencies. The MC2D model is also shown to deliver at par performance with the traditional atomic operations based model for highly data parallel algorithms from the unordered category.},
  archive      = {J_MICRO},
  author       = {Masab Ahmad and Halit Dogan and José A. Joao and Omer Khan},
  doi          = {10.1109/MM.2019.2955079},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {83-92},
  shortjournal = {IEEE Micro},
  title        = {In-hardware moving compute to data model to accelerate thread synchronization on large multicores},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AutoML for architecting efficient and specialized neural
networks. <em>MICRO</em>, <em>40</em>(1), 75–82. (<a
href="https://doi.org/10.1109/MM.2019.2953153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient deep learning inference requires algorithm and hardware codesign to enable specialization: we usually need to change the algorithm to reduce memory footprint and improve energy efficiency. However, the extra degree of freedom from the neural architecture design makes the design space much larger: it is not only about designing the hardware architecture but also codesigning the neural architecture to fit the hardware architecture. It is difficult for human engineers to exhaust the design space by heuristics. We propose design automation techniques for architecting efficient neural networks given a target hardware platform. We investigate automatically designing specialized and fast models, auto channel pruning, and auto mixed-precision quantization. We demonstrate that such learning-based, automated design achieves superior performance and efficiency than the rule-based human design. Moreover, we shorten the design cycle by 200× than previous work, so that we can afford to design specialized neural network models for different hardware platforms.},
  archive      = {J_MICRO},
  author       = {Han Cai and Ji Lin and Yujun Lin and Zhijian Liu and Kuan Wang and Tianzhe Wang and Ligeng Zhu and Song Han},
  doi          = {10.1109/MM.2019.2953153},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {75-82},
  shortjournal = {IEEE Micro},
  title        = {AutoML for architecting efficient and specialized neural networks},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <span class="math inline"><em>Δ</em></span>ΔNN:
Power-efficient neural network acceleration using differential weights.
<em>MICRO</em>, <em>40</em>(1), 67–74. (<a
href="https://doi.org/10.1109/MM.2019.2948345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enormous and ever-increasing complexity of state-of-the-art neural networks has impeded the deployment of deep learning on resource-limited embedded and mobile devices. To reduce the complexity of neural networks, this article presents $\Delta$ΔNN, a power-efficient architecture that leverages a combination of the approximate value locality of neuron weights and algorithmic structure of neural networks. $\Delta$ΔNN keeps each weight as its difference ($\Delta$Δ) to the nearest smaller weight: each weight reuses the calculations of the smaller weight, followed by a calculation on the $\Delta$Δ value to make up the difference. We also round up/down the $\Delta$Δ to the closest power of two numbers to further reduce complexity. The experimental results show that $\Delta$ΔNN boosts the average performance by 14\%–37\% and reduces the average power consumption by 17\%–49\% over some state-of-the-art neural network designs.},
  archive      = {J_MICRO},
  author       = {Hoda Mahdiani and Alireza Khadem and Azam Ghanbari and Mehdi Modarressi and Farima Fattahi-Bayat and Masoud Daneshtalab},
  doi          = {10.1109/MM.2019.2948345},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {67-74},
  shortjournal = {IEEE Micro},
  title        = {$\Delta$ΔNN: Power-efficient neural network acceleration using differential weights},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). IEEE computer graphics and applications. <em>MICRO</em>,
<em>40</em>(1), 66. (<a
href="https://doi.org/10.1109/MM.2020.2964473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2964473},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {66},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer graphics and applications},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Warp: A hardware platform for efficient multimodal sensing
with adaptive approximation. <em>MICRO</em>, <em>40</em>(1), 57–66. (<a
href="https://doi.org/10.1109/MM.2019.2951004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present Warp, the first open hardware platform designed explicitly to support research in approximate computing. Warp incorporates 21 sensors, computation, and circuit-level facilities designed explicitly to enable approximate computing research, in a 3.6 cm × 3.3 cm × 0.5 cm device. Warp supports a wide range of precision and accuracy versus power and performance tradeoffs.},
  archive      = {J_MICRO},
  author       = {Phillip Stanley-Marbell and Martin Rinard},
  doi          = {10.1109/MM.2019.2951004},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {57-66},
  shortjournal = {IEEE Micro},
  title        = {Warp: A hardware platform for efficient multimodal sensing with adaptive approximation},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). IT professional. <em>MICRO</em>, <em>40</em>(1), 56. (<a
href="https://doi.org/10.1109/MM.2020.2964470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2964470},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {56},
  shortjournal = {IEEE Micro},
  title        = {IT professional},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A high-throughput network processor architecture for
latency-critical applications. <em>MICRO</em>, <em>40</em>(1), 50–56.
(<a href="https://doi.org/10.1109/MM.2019.2958896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the recent advancements on the Advanced IO Processor (AIOP), a network processor architecture designed by NXP Semiconductors. The AIOP is a multicore accelerated computing architecture where each core is equipped with dedicated hardware for rapid task switching on every hardware accelerator call. A hardware preemption controller snoops on the accelerator completions and sends task preemption requests to the cores, thus reducing the latency of real-time tasks. A technique of priority thresholding is used to avoid latency uncertainty on lower priority tasks and head-of-line blocking. In this way, the AIOP handles the conflicting requirements of high throughput and low latency for next-generation wireless applications such as WiFi and 5G. In presence of frequent preemptions, the throughput reduces by only 3\% on AIOP, compared to 25\% on a similar network processor. Moreover, the absolute throughput and latency numbers are 2X better. The area and power overhead of adding hardware task-scheduling and preemption is only about 3\%.},
  archive      = {J_MICRO},
  author       = {Sourav Roy and Arvind Kaushik and Rajkumar Agrawal and Joseph Gergen and Wim Rouwet and John Arends},
  doi          = {10.1109/MM.2019.2958896},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {50-56},
  shortjournal = {IEEE Micro},
  title        = {A high-throughput network processor architecture for latency-critical applications},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-quality fault resiliency in fat trees. <em>MICRO</em>,
<em>40</em>(1), 44–49. (<a
href="https://doi.org/10.1109/MM.2019.2949978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coupling regular topologies with optimized routing algorithms is key in pushing the performance of interconnection networks of supercomputers. In this article, we present Dmodc, a fast deterministic routing algorithm for parallel generalized fat trees (PGFTs), which minimizes congestion risk even under massive network degradation caused by equipment failure. Dmodc computes forwarding tables with a closed-form arithmetic formula by relying on a fast preprocessing phase. This allows complete rerouting of networks with tens of thousands of nodes in less than a second. In turn, this greatly helps centralized fabric management react to faults with high-quality routing tables and has no impact on running applications in current and future very large scale high-performance computing clusters.},
  archive      = {J_MICRO},
  author       = {John Gliksberg and Antoine Capra and Alexandre Louvet and Pedro Javier García and Devan Sohier},
  doi          = {10.1109/MM.2019.2949978},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {44-49},
  shortjournal = {IEEE Micro},
  title        = {High-quality fault resiliency in fat trees},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020g). IEEE security &amp; privacy. <em>MICRO</em>,
<em>40</em>(1), 43. (<a
href="https://doi.org/10.1109/MM.2020.2964468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2964468},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {43},
  shortjournal = {IEEE Micro},
  title        = {IEEE security &amp; privacy},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Communication profiling and characterization of
deep-learning workloads on clusters with high-performance interconnects.
<em>MICRO</em>, <em>40</em>(1), 35–43. (<a
href="https://doi.org/10.1109/MM.2019.2949986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous high-performance computing systems with GPUs are equipped with high-performance interconnects like InfiniBand, Omni-Path, PCIe, and NVLink. However, little exists in the literature that captures the performance impact of these interconnects on distributed deep learning (DL). In this article, we choose Horovod, a distributed training middleware, to analyze and profile various DNN training workloads using TensorFlow and PyTorch in addition to standard MPI microbenchmarks. We use a wide variety of systems with CPUs like Intel Xeon and IBM POWER9, GPUs like Volta V100, and various interconnects to analyze the following metrics: 1) message-size with Horovod&#39;s tensor-fusion; 2) message-size without tensor-fusion; 3) number of MPI/NCCL calls; and 4) time taken by each MPI/NCCL call. We observed extreme performance variations for non-power-of-two message sizes on different platforms. To address this, we design a message-padding scheme for Horovod, illustrate significantly smoother allreduce latency profiles, and report cases where we observed improvement for end-to-end training.},
  archive      = {J_MICRO},
  author       = {Ammar Ahmad Awan and Arpan Jain and Ching-Hsiang Chu and Hari Subramoni and Dhableswar K. Panda},
  doi          = {10.1109/MM.2019.2949986},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {35-43},
  shortjournal = {IEEE Micro},
  title        = {Communication profiling and characterization of deep-learning workloads on clusters with high-performance interconnects},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IEEE transactions on computers. <em>MICRO</em>,
<em>40</em>(1), 34. (<a
href="https://doi.org/10.1109/MM.2020.2964460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2964460},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {34},
  shortjournal = {IEEE Micro},
  title        = {IEEE transactions on computers},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward FPGA-based HPC: Advancing interconnect technologies.
<em>MICRO</em>, <em>40</em>(1), 25–34. (<a
href="https://doi.org/10.1109/MM.2019.2950655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HPC architects are currently facing myriad challenges from ever tighter power constraints and changing workload characteristics. In this article, we discuss the current state of FPGAs within HPC systems. Recent technological advances show that they are well placed for penetration into the HPC market. However, there are still a number of research problems to overcome; we address the requirements for system architectures and interconnects to enable their proper exploitation, highlighting the necessity of allowing FPGAs to act as full-fledged peers within a distributed system rather than attached to the CPU. We argue that this model requires a reliable, connectionless, hardware-offloaded transport supporting a global memory space. Our results show how our fully fledged hardware implementation gives latency improvements of up to 25\% versus a software-based transport, and demonstrates that our solution can outperform the state of the art in HPC workloads such as matrix–matrix multiplication achieving a 10\% higher computing throughput.},
  archive      = {J_MICRO},
  author       = {Joshua Lant and Javier Navaridas and Mikel Luján and John Goodacre},
  doi          = {10.1109/MM.2019.2950655},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {25-34},
  shortjournal = {IEEE Micro},
  title        = {Toward FPGA-based HPC: Advancing interconnect technologies},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IEEE computer society has you covered! <em>MICRO</em>,
<em>40</em>(1), 24. (<a
href="https://doi.org/10.1109/MM.2020.2964404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2964404},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {24},
  shortjournal = {IEEE Micro},
  title        = {IEEE computer society has you covered!},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bunch-of-wires (BoW) interface for interchiplet
communication. <em>MICRO</em>, <em>40</em>(1), 15–24. (<a
href="https://doi.org/10.1109/MM.2019.2950352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multichiplet system-in-package designs have recently received a lot of attention as a mechanism to combat high SoC design costs and to economically manufacture large ASICs. These designs require low-power area-efficient off-die on-package die-to-die communication. Current technologies either extend on-die high-wire count buses using silicon interposers or off-package serial buses. The former approach leads to expensive packaging. The latter leads to complex and high-power designs. We propose a simple bunch-of-wires interface that combines ease of development with low-cost packaging techniques. We develop the interface and show how it can be used in multichiplet systems.},
  archive      = {J_MICRO},
  author       = {Ramin Farjadrad and Mark Kuemerle and Bapi Vinnakota},
  doi          = {10.1109/MM.2019.2950352},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {15-24},
  shortjournal = {IEEE Micro},
  title        = {A bunch-of-wires (BoW) interface for interchiplet communication},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Top technology trends for 2020 featured in computer.
<em>MICRO</em>, <em>40</em>(1), 14. (<a
href="https://doi.org/10.1109/MM.2020.2964402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2020.2964402},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {14},
  shortjournal = {IEEE Micro},
  title        = {Top technology trends for 2020 featured in computer},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path2SL: Leveraging InfiniBand resources to reduce
head-of-line blocking in fat trees. <em>MICRO</em>, <em>40</em>(1),
8–14. (<a href="https://doi.org/10.1109/MM.2019.2949280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of endnodes in high-performance computing and datacenter systems is constantly increasing. Hence, it is crucial to minimize the impact of network congestion to guarantee a suitable network performance. InfiniBand is a prominent interconnect technology that allows implementing efficient topologies and routing algorithms, as well as queuing schemes that reduce the head-of-line (HoL) blocking effect derived from congestion situations. Here, we explain and evaluate thoroughly a queuing scheme called Path2SL that optimizes the use of the InfiniBand Virtual Lanes to reduce the HoL blocking in fat-tree network topologies.},
  archive      = {J_MICRO},
  author       = {German Maglione-Mathey and Jesus Escudero-Sahuquillo and Pedro Javier Garcia and Francisco J. Quiles and José Duato},
  doi          = {10.1109/MM.2019.2949280},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {8-14},
  shortjournal = {IEEE Micro},
  title        = {Path2SL: Leveraging InfiniBand resources to reduce head-of-line blocking in fat trees},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hot interconnects 26. <em>MICRO</em>, <em>40</em>(1), 6–7.
(<a href="https://doi.org/10.1109/MM.2019.2959114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles in this special section present the best articles on the hottest, most cutting-edge interconnects design occurring in industry and academia from this year’s IEEE Symposium on High Performance Interconnects (Hot Interconnects).},
  archive      = {J_MICRO},
  author       = {Ryan E. Grant and Khaled Hamidouche},
  doi          = {10.1109/MM.2019.2959114},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Hot interconnects 26},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Connectivity! Connectivity! Connectivity! May you be more
connected than ever!! <em>MICRO</em>, <em>40</em>(1), 4–5. (<a
href="https://doi.org/10.1109/MM.2019.2961722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2019.2961722},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Connectivity! connectivity! connectivity! may you be more connected than ever!!},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). Masthead. <em>MICRO</em>, <em>40</em>(1), 1. (<a
href="https://doi.org/10.1109/MM.2019.2961440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_MICRO},
  doi          = {10.1109/MM.2019.2961440},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {1},
  shortjournal = {IEEE Micro},
  title        = {Masthead},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
