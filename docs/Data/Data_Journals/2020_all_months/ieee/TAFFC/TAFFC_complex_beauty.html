<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc---60">TAFFC - 60</h2>
<ul>
<li><details>
<summary>
(2020). HED-ID: An affective adaptation model explaining the
intensity-duration relationship of emotion. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(4), 736–750. (<a
href="https://doi.org/10.1109/TAFFC.2018.2848656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Intensity and duration are both pertinent aspects of an emotional experience, yet how they are related is unclear. Though stronger emotions usually last longer, sometimes they abate faster than the weaker ones. We present a quantitative model of affective adaptation, the process by which emotional responses to unchanging affective stimuli weaken with time, that addresses this intensity-duration problem. The model, described by three simple linear algebraic equations, assumes that the relationship between an affective stimulus and its experiencer can be broken down into three parameters. Self-relevance and explanation level combine multiplicatively to determine emotion intensity whereas the interaction of these with explanatory ease determines its duration. The model makes predictions, consistent with available empirical data, about emotion intensity, its duration, and adaptation speed for different scenarios. It predicts when the intensity-duration correlation is positive, negative or even absent, thus offering a solution to the intensity-duration problem. The model also addresses the shortcomings of past models of affective adaptation with its enhanced predictive power and by offering a more complete explanation to empirical observations that earlier models explain inadequately or fail to explain altogether. The model has potential applications in areas such as virtual reality training, games, human-computer interactions, and robotics.},
  archive  = {J},
  author   = {John E. Steephen and Siva C. Obbineni and Sneha Kummetha and Raju S. Bapi},
  doi      = {10.1109/TAFFC.2018.2848656},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {736-750},
  title    = {HED-ID: An affective adaptation model explaining the intensity-duration relationship of emotion},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mutual information based adaptive windowing of informative
EEG for emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(4), 722–735. (<a
href="https://doi.org/10.1109/TAFFC.2018.2840973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition using brain wave signals involves using high dimensional electroencephalogram (EEG) data. In this paper, a window selection method based on mutual information is introduced to select an appropriate signal window to reduce the length of the signals. The motivation of the windowing method comes from EEG emotion recognition being computationally costly and the data having low signal-to-noise ratio. The aim of the windowing method is to find a reduced signal where the emotions are strongest. In this paper, it is suggested, that using only the signal section which best describes emotions improves the classification of emotions. This is achieved by iteratively comparing different-length EEG signals at different time locations using the mutual information between the reduced signal and emotion labels as criterion. The reduced signal with the highest mutual information is used for extracting the features for emotion classification. In addition, a viable framework for emotion recognition is introduced. Experimental results on publicly available datasets, DEAP and MAHNOB-HCI, show significant improvement in emotion recognition accuracy.},
  archive  = {J},
  author   = {Laura Piho and Tardi Tjahjadi},
  doi      = {10.1109/TAFFC.2018.2840973},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {722-735},
  title    = {A mutual information based adaptive windowing of informative EEG for emotion recognition},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strategies to utilize the positive emotional contagion
optimally in crowd evacuation. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(4), 708–721. (<a
href="https://doi.org/10.1109/TAFFC.2018.2836462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In a crisis situation, negative emotions often spread among the crowd, and they have adverse impacts on human decisions, resulting in stampedes and crushes. Safety officers are often dispatched to scenes of emergencies because their positive emotions can calm the crowd down and avoid serious accidents. However, how to utilize the positive emotional contagion to maximize the “calm-down” effect remains a challenging problem in crowd evacuation. In this paper, we present an approach for optimizing positive emotional contagion in crowd evacuation. First, a computational model of positive emotional contagion is proposed to describe how safety officers calm a crowd down. To capture important influential factors for positive emotional contagion, such as the trust relationships among the individuals involved in a crisis situation and the variations of emotional contagion speed, we construct a trust-based emotional contagion network (Trust-ECN) and a heterogeneous emotional contagion speed computation model (HECS-CM). Based on these models, the emotional contagion process can be analyzed in a parametric way, and the infection probability for each individual in a given time window can be computed analytically with a continuous-time Markov chain (CTMC). Second, a maximization problem of emotional contagion is formulated. Since this optimization problem is NP-hard, an artificial bee colony optimized emotional contagion (ABCEC) algorithm is used to solve for the optimal positions of safety officers. We demonstrate the effectiveness of our method on both synthetic and real-world data at different scales. Finally, we implement a crowd simulation system to visualize the results of our theoretical analysis in a graphical manner. The proposed method can provide guidance for emergency response management.},
  archive  = {J},
  author   = {Guijuan Zhang and Dianjie Lu and Hong Liu},
  doi      = {10.1109/TAFFC.2018.2836462},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {708-721},
  title    = {Strategies to utilize the positive emotional contagion optimally in crowd evacuation},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic recognition of children engagement from facial
video using convolutional neural networks. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(4), 696–707. (<a
href="https://doi.org/10.1109/TAFFC.2018.2834350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic engagement recognition is a technique that is used to measure the engagement level of people in a specific task. Although previous research has utilized expensive and intrusive devices such as physiological sensors and pressure-sensing chairs, methods using RGB video cameras have become the most common because of the cost efficiency and noninvasiveness of video cameras. Automatic engagement recognition methods using video cameras are usually based on hand-crafted features and a statistical temporal dynamics modeling algorithm. This paper proposes a data-driven convolutional neural networks (CNNs)-based engagement recognition method that uses only facial images from input videos. As the amount of data in a dataset of children&#39;s engagement is insufficient for deep learning, pre-trained CNNs are utilized for low-level feature extraction from each video frame. In particular, a new layer combination for temporal dynamics modeling is employed to extract high-level features from low-level features. Experimental results on a database created using images of children from kindergarten demonstrate that the performance of the proposed method is superior to that of previous methods. The results indicate that the engagement level of children can be gauged automatically via deep learning even when the available database is deficient.},
  archive  = {J},
  author   = {Woo-Han Yun and Dongjin Lee and Chankyu Park and Jaehong Kim and Junmo Kim},
  doi      = {10.1109/TAFFC.2018.2834350},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {696-707},
  title    = {Automatic recognition of children engagement from facial video using convolutional neural networks},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can we generate emotional pronunciations for expressive
speech synthesis? <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(4), 684–695. (<a
href="https://doi.org/10.1109/TAFFC.2018.2828429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the field of expressive speech synthesis, a lot of work has been conducted on suprasegmental prosodic features while few has been done on pronunciation variants. However, prosody is highly related to the sequence of phonemes to be expressed. This article raises two issues in the generation of emotional pronunciations for TTS systems. The first issue consists in designing an automatic pronunciation generation method from text, while the second issue addresses the very existence of emotional pronunciations through experiments conducted on emotional speech. To do so, an innovative pronunciation adaptation method which automatically adapts canonical phonemes first to those labeled in the corpus used to create a synthetic voice, then to those labeled in an expressive corpus, is presented. This method consists in training conditional random fields pronunciation models with prosodic, linguistic, phonological and articulatory features. The analysis of emotional pronunciations reveals strong dependencies between prosody and phoneme assimilation or elisions. According to perceptual tests, the double adaptation allows to synthesize expressive speech samples of good quality, but emotion-specific pronunciations are too subtle to be perceived by testers.},
  archive  = {J},
  author   = {Marie Tahon and Gwénolé Lecorvé and Damien Lolive},
  doi      = {10.1109/TAFFC.2018.2828429},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {684-695},
  title    = {Can we generate emotional pronunciations for expressive speech synthesis?},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Look! It’s moving! Is it alive? How movement affects humans’
affinity living and non-living entities. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(4), 669–683. (<a
href="https://doi.org/10.1109/TAFFC.2018.2820707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article is about the relation between human observers and various human and non-human entities. Our focus is on humans&#39; perception of movement. In particular how it affects the relationship to entities. We explore the way the movement of natural entities, locomoting animals and robots or the expressivity of dancers, play a vital part in our perception of these things. Humans&#39; intuitive process of categorizing and attributing characteristics as a dialog and understanding of things, as found in the concept of metaphor, is central to our method. Drawing from the linguistic concept of animacy, expressing how sentient or alive an entity is interpreted we propose a metric of quantitative measures to investigate whether conceptual boundaries of entities, like those between human and non-human, change when movement comes into play. By means of measuring subjective responses, the rating of features in relation to specific types of entities like humans, animals and machines, we develop and validate a measurement tool in two online surveys. In the first (k = 93), we determine particular regions for each type, and in the second (k = 72), we investigate whether these regions change when entities move. We present the methodology and empirical work. Our key findings are alongside the metric, an agency-framework informed by related work to locate shifts in participants&#39; interpretation as degrees of animacy and agency ranging from intentional action to causal movement. We provide results demonstrating the effect of participants&#39; interpretation of entities under two conditions, represented either static or dynamic, we can show that movement affects participants&#39; interpretation. For example the shift of a human represented with mechanical movement, by virtue of breakdancing moves, towards the region designated to machines.},
  archive  = {J},
  author   = {Oliver Olsen Wolf and Geraint A. Wiggins},
  doi      = {10.1109/TAFFC.2018.2820707},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {669-683},
  title    = {Look! it&#39;s moving! is it alive? how movement affects humans’ affinity living and non-living entities},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An investigation of partition-based and phonetically-aware
acoustic features for continuous emotion prediction from speech.
<em>IEEE Transactions on Affective Computing</em>, <em>11</em>(4),
653–668. (<a href="https://doi.org/10.1109/TAFFC.2018.2821135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Phonetic variability has long been considered a confounding factor for emotional speech processing, so phonetic features have been rarely explored. However, surprisingly some features with purely phonetic information have shown state-of-the-art performance for continuous prediction of emotions (e.g., arousal and valence), for which the underlying causes are unknown to date. In this article, we present in-depth investigations into phonetic features on three widely used corpora - RECOLA, SEMAINE and USC CreativeIT - to explore this from two perspectives: acoustic space partitioning information and phonetic content. First, comparisons of multiple different partitioning methods confirm the significance of partitioning information in speech, and reveal the new understanding that varying the number of partitions has a greater effect on valence than arousal prediction: a detailed representation of the acoustic space is needed for valence, whilst a general one is adequate for arousal. Second, phoneme-specific examination of phonetic features suggests that phonetic content is less emotionally informative than partitioning information, and is more important for arousal than for valence. Furthermore, we propose a novel set of phonetically-aware acoustic features, attaining significant improvements for valence (in particular) and arousal prediction across RECOLA, SEMAINE and CreativeIT respectively, compared with conventional reference acoustic features.},
  archive  = {J},
  author   = {Zhaocheng Huang and Julien Epps},
  doi      = {10.1109/TAFFC.2018.2821135},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {653-668},
  title    = {An investigation of partition-based and phonetically-aware acoustic features for continuous emotion prediction from speech},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring domain knowledge for facial expression-assisted
action unit activation recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(4), 640–652. (<a
href="https://doi.org/10.1109/TAFFC.2018.2822303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Current works on facial action unit (AU) activation recognition typically include supervised training using AU-annotated training images. Compared to facial expression labeling, AU annotation is a time-consuming, expensive, and error-prone process. Domain knowledge refers to the strong probabilistic dependencies between facial expressions and AUs, as well as dependencies among AUs. To take advantage of this, we avoid the time-consuming process of AU annotation and introduce a new AU activation recognition method that learns AU classifiers from domain knowledge, and requires only expression-annotated facial images. Specifically, we first generate pseudo AU labels according to the probabilistic dependencies between expressions and AUs as well as correlations among AUs summarized from domain knowledge. Then, we propose to use a Restricted Boltzmann Machine to model AU label prior distribution from the generated pseudo AU data. After that, we train AU classifiers from expression-annotated facial images and the learned prior model by maximizing the log likelihood of AU classifiers with regard to the learned AU label prior. The proposed AU activation recognition can also be extended to semi-supervised learning scenarios with partially AU-annotated facial images. Experimental results on four benchmark databases demonstrate the effectiveness of the proposed approach in learning AU classifiers from domain knowledge.},
  archive  = {J},
  author   = {Shangfei Wang and Guozhu Peng and Qiang Ji},
  doi      = {10.1109/TAFFC.2018.2822303},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {640-652},
  title    = {Exploring domain knowledge for facial expression-assisted action unit activation recognition},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tracking dynamics of opinion behaviors with a content-based
sequential opinion influence model. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(4), 627–639. (<a
href="https://doi.org/10.1109/TAFFC.2018.2821123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, social media has become a popular channel for people to exchange opinions through the user-generated text. Exploring the mechanisms about how customers&#39; opinions towards products are influenced by friends, and further predicting their future opinions have attracted great attention from corporate administrators and researchers. Various influence models have already been proposed for the opinion prediction problem. However, they largely formulate opinions as derived sentiment categories or values but ignore the role of the content information. Besides, existing models only make use of the most recently received information without taking into consideration the long-term historical communication. To keep track of user opinion behaviors and infer user opinion influence from the historical exchanged textual information, we develop a content-based sequential opinion influence framework. Based on this framework, two opinion sentiment prediction models with alternative prediction strategies are proposed. In the experiments conducted on three Twitter datasets, the proposed models outperform other popular influence models. An interesting finding based on a further analysis of user characteristic is that an individuals influence is correlated to her/his style of expressions.},
  archive  = {J},
  author   = {Chengyao Chen and Zhitao Wang and Wenjie Li},
  doi      = {10.1109/TAFFC.2018.2821123},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {627-639},
  title    = {Tracking dynamics of opinion behaviors with a content-based sequential opinion influence model},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel audio features for music emotion recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>11</em>(4), 614–626. (<a
href="https://doi.org/10.1109/TAFFC.2018.2820691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work advances the music emotion recognition state-of-the-art by proposing novel emotionally-relevant audio features. We reviewed the existing audio features implemented in well-known frameworks and their relationships with the eight commonly defined musical concepts. This knowledge helped uncover musical concepts lacking computational extractors, to which we propose algorithms - namely related with musical texture and expressive techniques. To evaluate our work, we created a public dataset of 900 audio clips, with subjective annotations following Russell&#39;s emotion quadrants. The existent audio features (baseline) and the proposed features (novel) were tested using 20 repetitions of 10-fold cross-validation. Adding the proposed features improved the F1-score to 76.4 percent (by 9 percent), when compared to a similar number of baseline-only features. Moreover, analysing the features relevance and results uncovered interesting relations, namely the weight of specific features and musical concepts to each emotion quadrant, and warrant promising new directions for future research in the field of music emotion recognition, interactive media, and novel music interfaces.},
  archive  = {J},
  author   = {Renato Panda and Ricardo Malheiro and Rui Pedro Paiva},
  doi      = {10.1109/TAFFC.2018.2820691},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {614-626},
  title    = {Novel audio features for music emotion recognition},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segment-based methods for facial attribute detection from
partial faces. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(4), 601–613. (<a
href="https://doi.org/10.1109/TAFFC.2018.2820048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {State-of-the-art methods of attribute detection from faces almost always assume the presence of a full, unoccluded face. Hence, their performance degrades for partially visible and occluded faces. In this paper, we introduce SPLITFACE, a deep convolutional neural network-based method that is explicitly designed to perform attribute detection in partially occluded faces. Taking several facial segments and the full face as input, the proposed method takes a data driven approach to determine which attributes are localized in which facial segments. The unique architecture of the network allows each attribute to be predicted by multiple segments, which permits the implementation of committee machine techniques for combining local and global decisions to boost performance. With access to segment-based predictions, SPLITFACE can predict well those attributes which are localized in the visible parts of the face, without having to rely on the presence of the whole face. We use the CelebA and LFWA facial attribute datasets for standard evaluations. We also modify both datasets, to occlude the faces, so that we can evaluate the performance of attribute detection algorithms on partial faces. Our evaluation shows that SPLITFACE significantly outperforms other recent methods especially for partial faces.},
  archive  = {J},
  author   = {Upal Mahbub and Sayantan Sarkar and Rama Chellappa},
  doi      = {10.1109/TAFFC.2018.2820048},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {601-613},
  title    = {Segment-based methods for facial attribute detection from partial faces},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Physiological detection of affective states in children with
autism spectrum disorder. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(4), 588–600. (<a
href="https://doi.org/10.1109/TAFFC.2018.2820049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Autism spectrum disorder (ASD) is associated with emotion processing difficulties, including limitations in understanding the emotional states of others and processing one&#39;s own internal experiences. The nature of these difficulties remains largely unknown. This is due, in part, to challenges in acquiring reliable self-reports of emotional experiences from this population. Automatically characterizing emotional states with the use of physiological signals is a potential means of overcoming this problem, as physiological signals can provide an objective and nonverbal method for assessing affective states. However, this approach has not been well considered with ASD to date. To this end, we investigated detection of autonomic responses to positive and negative stimuli in children with ASD using four physiological measurements. Electrocardiograms, respiration, skin conductance and temperature were measured while 15 children with ASD viewed standard images known to evoke varying levels of valence (positive and negative) and arousal (low and high intensity). Using an ensemble of classifiers, affective states induced by stimuli of positive and negative valence or high and low arousal was differentiated at average accuracies approaching or exceeding 80 percent. These results suggest the feasibility of discerning affective states in individuals with ASD objectively using physiological signals.},
  archive  = {J},
  author   = {Sarah Sarabadani and Larissa C. Schudlo and Ali Akbar Samadani and Azadeh Kushski},
  doi      = {10.1109/TAFFC.2018.2820049},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {588-600},
  title    = {Physiological detection of affective states in children with autism spectrum disorder},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete probability distribution prediction of image
emotions with shared sparse learning. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(4), 574–587. (<a
href="https://doi.org/10.1109/TAFFC.2018.2818685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computationally modelling the affective content of images has been extensively studied recently because of its wide applications in entertainment, advertisement, and education. Significant progress has been made on designing discriminative features to bridge the affective gap. Assuming that viewers can reach a consensus on the emotion of images, most existing works focused on assigning the dominant emotion category or the average dimension values to an image. However, the image emotions perceived by viewers are subjective by nature with the influence of personal and situational factors. In this paper, we propose a novel machine learning approach that characterizes the categorical image emotions as a discrete probability distribution (DPD). To associate emotion with the visual features extracted from images, we present shared sparse learning to learn the combination coefficients, with which the DPD of an unseen image is predicted by linearly combining the DPDs of the training images. Furthermore, we extend our method to the setup where multi-features are available and learn the optimal weights for each feature to reflect the importance of different features. Extensive experiments are carried out on Abstract, Emotion6 and IESN datasets and the results demonstrate the superiority of the proposed method, as compared to the state-of-the-art approaches.},
  archive  = {J},
  author   = {Sicheng Zhao and Guiguang Ding and Yue Gao and Xin Zhao and Youbao Tang and Jungong Han and Hongxun Yao and Qingming Huang},
  doi      = {10.1109/TAFFC.2018.2818685},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {574-587},
  title    = {Discrete probability distribution prediction of image emotions with shared sparse learning},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational analyses of thin-sliced behavior segments in
session-level affect perception. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(4), 560–573. (<a
href="https://doi.org/10.1109/TAFFC.2018.2816654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ability to accurately judge another person&#39;s emotional states with a short duration of observations is a unique perceptual mechanism of humans, termed as the thin-sliced judgment. In this work, we propose a computational framework based on mutual information to identify the thin-sliced emotion-rich behavior segments within each session and further use these segments to train the session-level affect regressors. Our proposed thin-sliced framework obtains regression accuracies measured in Spearman correlations of 0.605, 0.633, and 0.672 on session-level attributes of activation, dominance, and valence, respectively. It outperforms framework using data of the entire session as baseline. The significant improvement in the regression correlations reinforces the thin-sliced nature of human emotion perception. By properly extracting these emotion-rich behavior segments, we obtain not only an improved overall accuracy but also bring additional insights. Specifically, our detailed analyses indicate that this thin-sliced nature in emotion perception is more evident for attributes of activation and valence, and the within-session time distribution of emotion-salient behavior is located more toward the ending portion. Lastly, we observe that there indeed exists a certain set of behavior types that carry high emotion-related content, and this is especially apparent in the extreme emotion levels.},
  archive  = {J},
  author   = {Wei-Cheng Lin and Chi-Chun Lee},
  doi      = {10.1109/TAFFC.2018.2816654},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {560-573},
  title    = {Computational analyses of thin-sliced behavior segments in session-level affect perception},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Defining laughter context for laughter synthesis with
spontaneous speech corpus. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(3), 553–559. (<a
href="https://doi.org/10.1109/TAFFC.2018.2813381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, conversational laughter was synthesized by a statistical model-based speech synthesis framework using spontaneous speech corpora. The phonetic transcriptions of natural laughter in these corpora were annotated, and the context required to synthesize the laughter that accompanies speech sounds was defined from the perspective of the (1) phonetic properties of the current segment, (2) phonetic properties of previous and succeeding segments, and (3) positional factors of the current segment or laughter bout. Laughter was synthesized using the defined context and the framework of HMM-based speech synthesis. To confirm the influence of the contextual factors on the naturalness of speech, a subjective evaluation was performed. As the result of the evaluation, the naturalness of the entire utterance was improved by using the contextual factors defined in this study. This result confirmed the importance of defining the appropriate context to synthesize natural conversational laughter.},
  archive  = {J},
  author   = {Tomohiro Nagata and Hiroki Mori},
  doi      = {10.1109/TAFFC.2018.2813381},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {553-559},
  title    = {Defining laughter context for laughter synthesis with spontaneous speech corpus},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visually interpretable representation learning for
depression recognition from facial images. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(3), 542–552. (<a
href="https://doi.org/10.1109/TAFFC.2018.2828819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent evidence in mental health assessment have demonstrated that facial appearance could be highly indicative of depressive disorder. While previous methods based on the facial analysis promise to advance clinical diagnosis of depressive disorder in a more efficient and objective manner, challenges in visual representation of complex depression pattern prevent widespread practice of automated depression diagnosis. In this paper, we present a deep regression network termed DepressNet to learn a depression representation with visual explanation. Specifically, a deep convolutional neural network equipped with a global average pooling layer is first trained with facial depression data, which allows for identifying salient regions of input image in terms of its severity score based on the generated depression activation map (DAM). We then propose a multi-region DepressNet, with which multiple local deep regression models for different face regions are jointly leaned and their responses are fused to improve the overall recognition performance. We evaluate our method on two benchmark datasets, and the results show that our method significantly boosts state-of-the-art performance of the visual-based depression recognition. Most importantly, the DAM induced by our learned deep model may help reveal the visual depression pattern on faces and understand the insights of automated depression diagnosis.},
  archive  = {J},
  author   = {Xiuzhuang Zhou and Kai Jin and Yuanyuan Shang and Guodong Guo},
  doi      = {10.1109/TAFFC.2018.2828819},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {542-552},
  title    = {Visually interpretable representation learning for depression recognition from facial images},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EEG emotion recognition using dynamical graph convolutional
neural networks. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(3), 532–541. (<a
href="https://doi.org/10.1109/TAFFC.2018.2817622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, a multichannel EEG emotion recognition method based on a novel dynamical graph convolutional neural networks (DGCNN) is proposed. The basic idea of the proposed EEG emotion recognition method is to use a graph to model the multichannel EEG features and then perform EEG emotion classification based on this model. Different from the traditional graph convolutional neural networks (GCNN) methods, the proposed DGCNN method can dynamically learn the intrinsic relationship between different electroencephalogram (EEG) channels, represented by an adjacency matrix, via training a neural network so as to benefit for more discriminative EEG feature extraction. Then, the learned adjacency matrix is used to learn more discriminative features for improving the EEG emotion recognition. We conduct extensive experiments on the SJTU emotion EEG dataset (SEED) and DREAMER dataset. The experimental results demonstrate that the proposed method achieves better recognition performance than the state-of-the-art methods, in which the average recognition accuracy of 90.4 percent is achieved for subject dependent experiment while 79.95 percent for subject independent cross-validation one on the SEED database, and the average accuracies of 86.23, 84.54 and 85.02 percent are respectively obtained for valence, arousal and dominance classifications on the DREAMER database.},
  archive  = {J},
  author   = {Tengfei Song and Wenming Zheng and Peng Song and Zhen Cui},
  doi      = {10.1109/TAFFC.2018.2817622},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {532-541},
  title    = {EEG emotion recognition using dynamical graph convolutional neural networks},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaming away stress: Using biofeedback games to learn paced
breathing. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(3), 519–531. (<a
href="https://doi.org/10.1109/TAFFC.2018.2816945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Biofeedback games are an attractive alternative to standard techniques for learning short-term relaxation skills. In this paper, we present the design, implementation, and evaluation of three respiratory biofeedback games. To validate these games, we compared breathing rate across 100 male participants (23 years ± 3.2 years) playing biofeedback and audio pacing versions of these games as well as a paced breathing app. The games were placed between repeat runs of a cognitively stressful Stroop-based task and the impact of the games on breathing and cognitive performance in the task also assessed. Our results showed that 1) differences in gameplay did not impact player performance; 2) biofeedback not only led to better breath control during play but also during the subsequent cognitively stressful task; and 3) biofeedback led to better attentional-cognitive performance in the subsequent task. Our multi-game experiments show that using respiratory biofeedback in video games is an effective strategy to learn paced breathing—on par with the standalone technique of paced breathing—and to self-regulate stress levels in later stressful scenarios. Furthermore, owing to its entertainment value, our relaxation solution has the potential to be more engaging and accessible than standalone paced breathing, for use over longer durations.},
  archive  = {J},
  author   = {M. Abdullah Zafar and Beena Ahmed and Rami Al Rihawi and Ricardo Gutierrez-Osuna},
  doi      = {10.1109/TAFFC.2018.2816945},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {519-531},
  title    = {Gaming away stress: Using biofeedback games to learn paced breathing},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realistic transformation of facial and vocal smiles in
real-time audiovisual streams. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(3), 507–518. (<a
href="https://doi.org/10.1109/TAFFC.2018.2811465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Research in affective computing and cognitive science has shown the importance of emotional facial and vocal expressions during human-computer and human-human interactions. But, while models exist to control the display and interactive dynamics of emotional expressions, such as smiles, in embodied agents, these techniques can not be applied to video interactions between humans. In this work, we propose an audiovisual smile transformation algorithm able to manipulate an incoming video stream in real-time to parametrically control the amount of smile seen on the user&#39;s face and heard in their voice, while preserving other characteristics such as the user&#39;s identity or the timing and content of the interaction. The transformation is composed of separate audio and visual pipelines, both based on a warping technique informed by real-time detection of audio and visual landmarks. Taken together, these two parts constitute a unique audiovisual algorithm which, in addition to providing simultaneous real-time transformations of a real person&#39;s face and voice, allows to investigate the integration of both modalities of smiles in real-world social interactions.},
  archive  = {J},
  author   = {Pablo Arias and Catherine Soladié and Oussema Bouafif and Axel Roebel and Renaud Séguier and Jean-Julien Aucouturier},
  doi      = {10.1109/TAFFC.2018.2811465},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {507-518},
  title    = {Realistic transformation of facial and vocal smiles in real-time audiovisual streams},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Objectivity and subjectivity in aesthetic quality assessment
of digital photographs. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(3), 493–506. (<a
href="https://doi.org/10.1109/TAFFC.2018.2809752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic prediction of the aesthetic quality of a photograph has been an important research problem in image processing and computer vision. While assessing the aesthetic quality of a photograph by human is highly subjective, most existing studies have considered only objective (or general) opinion of multiple viewers. In this paper, we provide a comprehensive investigation of the issue of subjectivity in aesthetic quality assessment using a large-scale database containing photos, user ratings, and user comments. First, we analyze how the mean aesthetic quality level and the level of subjectivity have evolved over time. Second, we examine the feasibility of automatic prediction of the level of subjectivity based on visual features, and identify which features are effective for the prediction. Third, we analyze the users&#39; comments given to photos to understand the sources of subjectivity of aesthetic quality rating. Our results show that several factors are simultaneously involved in determining the level of subjectivity of a photo, but it can be predicted with reasonable accuracy. We believe that our results provide insight toward personalized aesthetic photo applications.},
  archive  = {J},
  author   = {Won-Hee Kim and Jun-Ho Choi and Jong-Seok Lee},
  doi      = {10.1109/TAFFC.2018.2809752},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {493-506},
  title    = {Objectivity and subjectivity in aesthetic quality assessment of digital photographs},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting personality from book preferences with
user-generated content labels. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(3), 482–492. (<a
href="https://doi.org/10.1109/TAFFC.2018.2808349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Psychological studies have shown that personality traits are associated with book preferences. However, past findings based on questionnaires are limited to conventional book genres and do not capture niche content (e.g., family drama) and reading behaviors (e.g., backburners). For a more comprehensive measure of book content, this study harnesses a massive archive of content labels, also known as ‘tags’, created by users of a book review website, Goodreads.com . Combined with data on preferences and personality scores collected from Facebook users, the tag labels achieve high accuracy in personality prediction by psychological standards. Additionally, we group tags into broader genres to check their validity against past findings. Our results are robust across both tag-level and genre-level analyses and are consistent with existing literature. Moreover, user-generated tag labels reveal unexpected insights, such as cultural differences, book reading behaviors, and other non-content factors affecting preferences. To our knowledge, this is currently the largest study that explores the relationship between personality and book content preferences.},
  archive  = {J},
  author   = {Ng Annalyn and Maarten W. Bos and Leonid Sigal and Boyang Li},
  doi      = {10.1109/TAFFC.2018.2808349},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {482-492},
  title    = {Predicting personality from book preferences with user-generated content labels},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unobtrusive inference of affective states in virtual
rehabilitation from upper limb motions: A feasibility study. <em>IEEE
Transactions on Affective Computing</em>, <em>11</em>(3), 470–481. (<a
href="https://doi.org/10.1109/TAFFC.2018.2808295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Virtual rehabilitation environments may afford greater patient personalization if they could harness the patient&#39;s affective state. Four states: anxiety, pain, engagement and tiredness (either physical or psychological), were hypothesized to be inferable from observable metrics of hand location and gripping strength-relevant for rehabilitation. Contributions are; (a) multiresolution classifier built from Semi-Naïve Bayesian classifiers, and (b) establishing predictive relations for the considered states from the motor proxies capitalizing on the proposed classifier with recognition levels sufficient for exploitation. 3D hand locations and gripping strength streams were recorded from 5 post-stroke patients whilst undergoing motor rehabilitation therapy administered through virtual rehabilitation along 10 sessions over 4 weeks. Features from the streams characterized the motor dynamics, while spontaneous manifestations of the states were labelled from concomitant videos by experts for supervised classification. The new classifier was compared against baseline support vector machine (SVM) and random forest (RF) with all three exhibiting comparable performances. Inference of the aforementioned states departing from chosen motor surrogates appears feasible, expediting increased personalization of virtual motor neurorehabilitation therapies.},
  archive  = {J},
  author   = {Jesús Joel Rivas and Felipe Orihuela-Espina and Lorena Palafox and Nadia Bianchi-Berthouze and María del Carmen Lara and Jorge Hernández-Franco and Luis Enrique Sucar},
  doi      = {10.1109/TAFFC.2018.2808295},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {470-481},
  title    = {Unobtrusive inference of affective states in virtual rehabilitation from upper limb motions: A feasibility study},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classifying affective haptic stimuli through gender-specific
heart rate variability nonlinear analysis. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(3), 459–469. (<a
href="https://doi.org/10.1109/TAFFC.2018.2808261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study reports on how velocity and force levels of caress-like haptic stimuli can elicit different emotional responses, which can be identified through the analysis of Autonomic Nervous System (ANS) dynamics. Affective stimuli were administered on the forearm of 32 healthy volunteers (16 women) through a haptic device with two levels of force, 2 N and 6 N, and two levels of velocity, 9.4 mm/s and 37 mm/s. ANS dynamics was estimated through Heart Rate Variability (HRV) linear and nonlinear analysis on recordings gathered before and after each stimulus. To this extent, we here propose and assess novel features from HRV symbolic analysis and Lagged Poincaré Plot. Classification was performed following a leave-one-subject-out procedure on nonlinear support vector machines. Pattern classification was split according to gender, significantly improving accuracies of recognition with respect to a “all-subjects” classification. Caressing force and velocity levels were recognized with up to 80 percent accuracy for men, and up to 84.38 percent for women. Our results demonstrate that changes in ANS control on cardiovascular dynamics, following emotional changes induced by caress-like haptic stimuli, can be effectively recognized by the proposed computational approach, considering that they occur in a gender-specific and nonlinear manner.},
  archive  = {J},
  author   = {Mimma Nardelli and Alberto Greco and Matteo Bianchi and Enzo Pasquale Scilingo and Gaetano Valenza},
  doi      = {10.1109/TAFFC.2018.2808261},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {459-469},
  title    = {Classifying affective haptic stimuli through gender-specific heart rate variability nonlinear analysis},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pipelined neural networks for phrase-level sentiment
intensity prediction. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(3), 447–458. (<a
href="https://doi.org/10.1109/TAFFC.2018.2807819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Linguistic modifiers such as negators (e.g., not), intensifiers (e.g., very) and modals (e.g., would) are commonly used in expressing opinions. These modifiers play an important role in recognizing the sentiment intensity of multi-word phrases because they may lead to an intensity shift and polarity reversal for the words they modify. Appropriately modeling the effect of such modifiers on the intensity shift can greatly improve the performance of phrase-level sentiment intensity prediction. To this end, this paper proposes two neural network (NN) models organized in a pipelined fashion to determine 1) the intensity of individual words and 2) the shift weights of modifiers representing the degrees of intensity change for the words they modify. The intensity of a phrase can then be determined by combining the intensity of the constituent word and the shift weight of the modifier within the phrase. When measuring the word intensity, the first NN model introduces a hidden layer as a filter to select appropriate similar seed words in the prediction process. Automatic word intensity prediction can address the unknown intensities of words not covered in sentiment lexicons. In learning the modifier weights, the second NN model considers both the weights of individual modifiers and groups of modifiers to capture various intensity shift effects caused by them. Experiments on a SemEval-2016 dataset showed that the proposed method yielded better prediction performance for both single words and multi-word phrases.},
  archive  = {J},
  author   = {Liang-Chih Yu and Jin Wang and K. Robert Lai and Xuejie Zhang},
  doi      = {10.1109/TAFFC.2018.2807819},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {447-458},
  title    = {Pipelined neural networks for phrase-level sentiment intensity prediction},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emotion recognition on twitter: Comparative study and
training a unison model. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(3), 433–446. (<a
href="https://doi.org/10.1109/TAFFC.2018.2807817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite recent successes of deep learning in many fields of natural language processing, previous studies of emotion recognition on Twitter mainly focused on the use of lexicons and simple classifiers on bag-of-words models. The central question of our study is whether we can improve their performance using deep learning. To this end, we exploit hashtags to create three large emotion-labeled data sets corresponding to different classifications of emotions. We then compare the performance of several word- and character-based recurrent and convolutional neural networks with the performance on bag-of-words and latent semantic indexing models. We also investigate the transferability of the final hidden state representations between different classifications of emotions, and whether it is possible to build a unison model for predicting all of them using a shared representation. We show that recurrent neural networks, especially character-based ones, can improve over bag-of-words and latent semantic indexing models. Although the transfer capabilities of these models are poor, the newly proposed training heuristic produces a unison model with performance comparable to that of the three single models.},
  archive  = {J},
  author   = {Niko Colnerič and Janez Demšar},
  doi      = {10.1109/TAFFC.2018.2807817},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {433-446},
  title    = {Emotion recognition on twitter: Comparative study and training a unison model},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised adaptation of a person-specific manifold of
facial expressions. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(3), 419–432. (<a
href="https://doi.org/10.1109/TAFFC.2018.2807430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In order to analyze expressions that are different from the prototypic expressions defined by Ekman, manifold learning has been proposed to build person-specific continuous representations of facial expressions. Yet, it is still a challenging problem to build such a manifold with no prior knowledge on the morphology of the subject. Here, we propose a method to build a person-specific manifold of facial expressions able to adapt to the morphology of the subject in an unsupervised manner. The manifold is initialized with the facial landmarks of the neutral face and 5 synthesized basic expressions. Our first contribution is to detect automatically the neutral face of the subject so that we can build the manifold in an unsupervised manner. Our second and main contribution is to adapt in an unsupervised manner the initialized manifold to the morphology of the subject by detecting the real basic expressions of the subject while maintaining constraints in the manifold. Our third contribution is to perform the adaptation on spontaneous expressions with typical head pose variation for human-computer interaction. The experiments show that the adaptation works well on posed expressions and that the constraints for the adaptation on spontaneous expressions is efficient when head pose variation is considered.},
  archive  = {J},
  author   = {Raphaël Weber and Vincent Barrielle and Catherine Soladié and Renaud Séguier},
  doi      = {10.1109/TAFFC.2018.2807430},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {419-432},
  title    = {Unsupervised adaptation of a person-specific manifold of facial expressions},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-level characterization of expressive head motion through
frequency domain analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(3), 405–418. (<a
href="https://doi.org/10.1109/TAFFC.2018.2805892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For the purpose of understanding how head motions contribute to the perception of emotion in an utterance, we aim to examine the perception of emotion based on Fourier transform-based static and dynamic features of head motion. Our work is to conduct intra-related objective analysis and perceptual experiments on the link between the perception of emotion and the static/dynamic features. The objective analysis outcome shows that the static and dynamic features are effective in characterizing and recognizing emotions. The perceptual experiments enable us to collect human perception of emotion through head motion. The collected perceptual data shows that humans are unable to reliably perceive emotion from head motion alone but reveals that humans are sensitive to the static feature (in reference to the averaged up-down rotation angle) and the dynamic features (which reflect the fluidity and speed of movement). It also indicates that humans perceive emotion carried in head motion and the naturalness of head motion in two different channels. Our work contributes to the understanding and the characterization of head motion in expressive speech through low-level descriptions of motion features, instead of commonly used high-level motion style (e.g., head nods, shakes, tilts, and raises).},
  archive  = {J},
  author   = {Yu Ding and Lei Shi and Zhigang Deng},
  doi      = {10.1109/TAFFC.2018.2805892},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {405-418},
  title    = {Low-level characterization of expressive head motion through frequency domain analysis},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting unipolar and bipolar depressive disorders from
elicited speech responses using latent affective structure model.
<em>IEEE Transactions on Affective Computing</em>, <em>11</em>(3),
393–404. (<a href="https://doi.org/10.1109/TAFFC.2018.2803178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mood disorders, including unipolar depression (UD) and bipolar disorder (BD) [1] , are reported to be one of the most common mental illnesses in recent years. In diagnostic evaluation on the outpatients with mood disorder, a large portion of BD patients are initially misdiagnosed as having UD [2] . As most previous research focused on long-term monitoring of mood disorders, short-term detection which could be used in early detection and intervention is thus desirable. This work proposes an approach to short-term detection of mood disorder based on the patterns in emotion of elicited speech responses. To the best of our knowledge, there is no database for short-term detection on the discrimination between BD and UD currently. This work collected two databases containing an emotional database (MHMC-EM) collected by the Multimedia Human Machine Communication (MHMC) lab and a mood disorder database (CHI-MEI) collected by the CHI-MEI Medical Center, Taiwan. As the collected CHI-MEI mood disorder database is quite small and emotion annotation is difficult, the MHMC-EM emotional database is selected as a reference database for data adaptation. For the CHI-MEI mood disorder data collection, six eliciting emotional videos are selected and used to elicit the participants&#39; emotions. After watching each of the six eliciting emotional video clips, the participants answer the questions raised by the clinician. The speech responses are then used to construct the CHI-MEI mood disorder database. Hierarchical spectral clustering is used to adapt the collected MHMC-EM emotional database to fit the CHI-MEI mood disorder database for dealing with the data bias problem. The adapted MHMC-EM emotional data are then fed to a denoising autoencoder for bottleneck feature extraction. The bottleneck features are used to construct a long short term memory (LSTM)-based emotion detector for generation of emotion profiles from each speech response. The emotion profiles are then clustered into emotion codewords using the K-means algorithm. Finally, a class-specific latent affective structure model (LASM) is proposed to model the structural relationships among the emotion codewords with respect to six emotional videos for mood disorder detection. Leave-one-group-out cross validation scheme was employed for the evaluation of the proposed class-specific LASM-based approaches. Experimental results show that the proposed class-specific LASM-based method achieved an accuracy of 73.33 percent for mood disorder detection, outperforming the classifiers based on SVM and LSTM.},
  archive  = {J},
  author   = {Kun-Yi Huang and Chung-Hsien Wu and Ming-Hsiang Su and Yu-Ting Kuo},
  doi      = {10.1109/TAFFC.2018.2803178},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {393-404},
  title    = {Detecting unipolar and bipolar depressive disorders from elicited speech responses using latent affective structure model},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A case-based reasoning model for depression based on
three-electrode EEG data. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(3), 383–392. (<a
href="https://doi.org/10.1109/TAFFC.2018.2801289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression, threatening the well-being of millions, has become one of the major diseases in the past decade. However, the current method of diagnosing depression is questionnaire-based interviews, which is labor-intensive and highly dependent on doctors’ experience. Thus, objective and cost-efficient methods are needed. In this paper, we present a case-based reasoning model for identifying depression. Electroencephalography data were collected using a portable three-electrode EEG device, and then processed to remove artifacts and extract features. We applied multiple classifiers. The best performing k-Nearest Neighbor (KNN) was selected as the evaluation function to select the effective features which were then used to create the case base. Based on the weight set of standard deviations, the similarity was calculated using normalized Euclidean distance to get the optimal recognition rate of depression. The accuracy of optimal similarity identification of patients with depression was 91.25 percent, which was improved compared to the accuracy using KNN classifier (81.44 percent) or previously reported classifiers. Thus, we provide a novel pervasive and effective method for automatic detection of depression.},
  archive  = {J},
  author   = {Hanshu Cai and Xiangzi Zhang and Yanhao Zhang and Ziyang Wang and Bin Hu},
  doi      = {10.1109/TAFFC.2018.2801289},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {383-392},
  title    = {A case-based reasoning model for depression based on three-electrode EEG data},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection based transfer subspace learning for
speech emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(3), 373–382. (<a
href="https://doi.org/10.1109/TAFFC.2018.2800046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cross-corpus speech emotion recognition has recently received considerable attention due to the widespread existence of various emotional speech. It takes one corpus as the training data aiming to recognize emotions of another corpus, and generally involves two basic problems, i.e., feature matching and feature selection. Many previous works study these two problems independently, or just focus on solving the first problem. In this paper, we propose a novel algorithm, called feature selection based transfer subspace learning (FSTSL), to address these two problems. To deal with the first problem, a latent common subspace is learnt by reducing the difference of different corpora and preserving the important properties. Meanwhile, we adopt the l 2,1 -norm on the projection matrix to deal with the second problem. Besides, to guarantee the subspace to be robust and discriminative, the geometric information of data is exploited simultaneously in the proposed FSTSL framework. Empirical experiments on cross-corpus speech emotion recognition tasks demonstrate that our proposed method can achieve encouraging results in comparison with state-of-the-art algorithms.},
  archive  = {J},
  author   = {Peng Song and Wenming Zheng},
  doi      = {10.1109/TAFFC.2018.2800046},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {373-382},
  title    = {Feature selection based transfer subspace learning for speech emotion recognition},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Speech synthesis for the generation of artificial
personality. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(2), 361–372. (<a
href="https://doi.org/10.1109/TAFFC.2017.2763134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A synthetic voice personifies the system using it. In this work we examine the impact text content, voice quality and synthesis system have on the perceived personality of two synthetic voices. Subjects rated synthetic utterances based on the Big-Five personality traits and naturalness. The naturalness rating of synthesis output did not correlate significantly with any Big-Five characteristic except for a marginal correlation with openness. Although text content is dominant in personality judgments, results showed that voice quality change implemented using a unit selection synthesis system significantly affected the perception of the Big-Five, for example tense voice being associated with being disagreeable and lax voice with lower conscientiousness. In addition a comparison between a parametric implementation and unit selection implementation of the same voices showed that parametric voices were rated as significantly less neurotic than both the text alone and the unit selection system, while the unit selection was rated as more open than both the text alone and the parametric system. The results have implications for synthesis voice and system type selection for applications such as personal assistants and embodied conversational agents where developing an emotional relationship with the user, or developing a branding experience is important.},
  archive  = {J},
  author   = {Matthew P. Aylett and Alessandro Vinciarelli and Mirjam Wester},
  doi      = {10.1109/TAFFC.2017.2763134},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {361-372},
  title    = {Speech synthesis for the generation of artificial personality},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Co-clustering to reveal salient facial features for
expression recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(2), 348–360. (<a
href="https://doi.org/10.1109/TAFFC.2017.2780838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expressions are a strong visual intimation of gestural behaviors. The intelligent ability to learn these non-verbal cues of the humans is the key characteristic to develop efficient human computer interaction systems. Extracting an effective representation from facial expression images is a crucial step that impacts the recognition accuracy. In this paper, we propose a novel feature selection strategy using singular value decomposition (SVD) based co-clustering to search for the most salient regions in terms of facial features that possess a high discriminating ability among all expressions. To the best of our knowledge, this is the first known attempt to explicitly perform co-clustering in the facial expression recognition domain. In our method, Gabor filters are used to extract local features from an image and then discriminant features are selected based on the class membership in co-clusters. Experiments demonstrate that co-clustering localizes the salient regions of the face image. Not only does the procedure reduce the dimensionality but also improves the recognition accuracy. Experiments on CK plus, JAFFE and MMI databases validate the existence and effectiveness of these learned facial features.},
  archive  = {J},
  author   = {Sheheryar Khan and Lijiang Chen and Hong Yan},
  doi      = {10.1109/TAFFC.2017.2780838},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {348-360},
  title    = {Co-clustering to reveal salient facial features for expression recognition},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intensional learning to efficiently build up automatically
annotated emotion corpora. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(2), 335–347. (<a
href="https://doi.org/10.1109/TAFFC.2017.2764470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Textual emotion detection has a high impact on business, society, politics or education with applications such as, detecting depression or personality traits, suicide prevention or identifying cases of cyber-bulling. Given this context, the objective of our research is to contribute to the improvement of emotion recognition task through an automatic technique focused on reducing both the time and cost needed to develop emotion corpora. Our proposal is to exploit a bootstrapping approach based on intensional learning for automatic annotations with two main steps: 1) an initial similarity-based categorization where a set of seed sentences is created and extended by distributional semantic similarity (word vectors or word embeddings); 2) train a supervised classifier on the initially categorized set. The technique proposed allows us an efficient annotation of a large amount of emotion data with standards of reliability according to the evaluation results.},
  archive  = {J},
  author   = {Lea Canales and Carlo Strapparava and Ester Boldrini and Patricio Martínez-Barco},
  doi      = {10.1109/TAFFC.2017.2764470},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {335-347},
  title    = {Intensional learning to efficiently build up automatically annotated emotion corpora},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Context sensitivity of EEG-based workload classification
under different affective valence. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(2), 327–334. (<a
href="https://doi.org/10.1109/TAFFC.2017.2775616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {State of the art brain-computer interfaces (BCIs) largely focus on detecting single, specific, often experimentally induced or manipulated aspects of the user state. In a less controlled, more naturalistic environment, a larger variety of mental processes may be active and possibly interacting. When moving BCI applications from the lab to real-life applications, these additional unaccounted mental processes could interfere with user state decoding, thus decreasing system efficacy and decreasing real-world applicability. Here, we assess the impact of affective valence on classification of working memory load, by re-analyzing a dataset that used an affective N-back task with picture stimuli. Our analyses showed that classification of working memory load under affective valence can lead to good classification accuracies (&gt; 70 percent), which can be further improved via data integration over time. However, positive as well as negative affective valence resulted in decreased classification accuracies, when compared to the neutral affective context. Furthermore, classifiers failed to generalize across affective contexts, highlighting the need for user state models that can account for different contexts or new, context-independent, EEG features.},
  archive  = {J},
  author   = {Sebastian Grissmann and Martin Spüler and Josef Faller and Tanja Krumpe and Thorsten O. Zander and Augustin Kelava and Christian Scharinger and Peter Gerjets},
  doi      = {10.1109/TAFFC.2017.2775616},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {327-334},
  title    = {Context sensitivity of EEG-based workload classification under different affective valence},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Film mood and its quantitative determinants in different
types of scenes. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(2), 313–326. (<a
href="https://doi.org/10.1109/TAFFC.2018.2791529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Films elicit emotions in viewers by infusing the story they tell with an affective character or tone-in a word, a mood. Considerable effort has been made recently to develop computational methods to estimate affective content in film. However, these efforts have focused almost exclusively on style-based features while neglecting to consider different scene types separately. In this study, we investigated the quantitative determinants of film mood across scenes classified by their setting and use of sounds. We examined whether viewers could assess film mood directly in terms of hedonic tone, energetic arousal, and tense arousal; whether their mood ratings differed by scene type; and how various narrative and stylistic film attributes as well as low- and high-level computational features related to the ratings. We found that the viewers were adept at assessing film mood, that sound-based scene classification brought out differences in the mood ratings, and that the low- and high-level features related to different mood dimensions. The study showed that computational film mood estimation can benefit from scene type classification and the use of both low- and high-level features. We have made our clip assessment and annotation data as well as the extracted computational features publicly available.},
  archive  = {J},
  author   = {Jussi Tarvainen and Jorma Laaksonen and Tapio Takala},
  doi      = {10.1109/TAFFC.2018.2791529},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {313-326},
  title    = {Film mood and its quantitative determinants in different types of scenes},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emotion recognition in simulated social interactions.
<em>IEEE Transactions on Affective Computing</em>, <em>11</em>(2),
308–312. (<a href="https://doi.org/10.1109/TAFFC.2018.2799593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social context plays an important role in everyday emotional interactions, and others&#39; faces often provide contextual cues in social situations. Investigating this complex social process is a challenge that can be addressed with the use of computer-generated facial expressions. In the current research, we use synthesized facial expressions to investigate the influence of socioaffective inferential mechanisms on the recognition of social emotions. Participants judged blends of facial expressions of shame-sadness, or of anger-disgust, in a target avatar face presented at the center of a screen while a contextual avatar face expressed an emotion (disgust, contempt, and sadness) or remained neutral. The dynamics of the facial expressions and the head/gaze movements of the two avatars were manipulated in order to create an interaction in which the two avatars shared eye gaze only in the social interaction condition. Results of Experiment 1 revealed that when the avatars engaged in social interaction, target expression blends of shame and sadness were perceived as expressing more shame if the contextual face expressed disgust and more sadness when the contextual face expressed sadness. Interestingly, perceptions of shame were not enhanced when the contextual face expressed contempt. The latter finding is probably attributable to the low recognition rates for the expression of contempt observed in Experiment 2.},
  archive  = {J},
  author   = {C. Mumenthaler and D. Sander and A. S. R. Manstead},
  doi      = {10.1109/TAFFC.2018.2799593},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {308-312},
  title    = {Emotion recognition in simulated social interactions},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis and classification of cold speech using variational
mode decomposition. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(2), 296–307. (<a
href="https://doi.org/10.1109/TAFFC.2017.2761750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents analysis and classification of a pathological speech called cold speech, which is recorded when the person is suffering from common cold. Nose and throat are affected by the common cold. As nose and throat play an important role in speech production, the speech characteristics are altered during this pathology. In this work, variational mode decomposition (VMD) is used for analysis and classification of cold speech. VMD decomposes the speech signal into a number of sub-signals or modes. These sub-signals may better exploit the pathological information for characterization of cold speech. Various statistics, mean, variance, kurtosis and skewness are extracted from each of the decomposed sub-signals. Along with those statistics, center frequency, energy, peak amplitude, spectral entropy, permutation entropy and Renyi&#39;s entropy are evaluated, and used as features. Mutual information (MI) is further employed to assign the weight values to the features. In terms of classification rates, the proposed feature outperforms the linear prediction coefficients (LPC), mel frequency cepstral coefficients (MFCC), Teager energy operator (TEO) based feature and ComParE feature sets (IS09-emotion and IS13-ComParE). The proposed feature shows an average recognition rate of 90.02 percent for IITG cold speech database and 66.84 percent for URTIC database.},
  archive  = {J},
  author   = {Suman Deb and Samarendra Dandapat and Jarek Krajewski},
  doi      = {10.1109/TAFFC.2017.2761750},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {296-307},
  title    = {Analysis and classification of cold speech using variational mode decomposition},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Singing robots: How embodiment affects emotional responses
to non-linguistic utterances. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(2), 284–295. (<a
href="https://doi.org/10.1109/TAFFC.2017.2774815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robots are often envisaged as embodied agents that might be able to intelligently and expressively communicate with humans. This could be due to their physical embodiment, their animated nature, or to other factors, such as cultural associations. In this study, we investigated emotional responses of humans to affective non-linguistic utterances produced by an embodied agent, with special attention to the way that these responses depended on the nature of the embodiment and the extent to which the robot actively moved in proximity to the human. To this end, we developed a new singing robot platform, ROVER, that could interact with humans in its surroundings. We used affective sound design methods to endow ROVER with the ability to communicate through song, via musical, non-linguistic utterances that could, as we demonstrate, evoke emotional responses in humans. We indeed found that the embodiment of the computational agent had an affect on emotional responses. However, contrary to our expectations, we found singing computers to be more emotionally arousing than singing robots. Whether the robot moved or not did not affect arousal. The results may have implications for the design of affective non-speech audio displays for human-computer or human-robot interaction.},
  archive  = {J},
  author   = {Hannah Wolfe and Marko Peljhan and Yon Visell},
  doi      = {10.1109/TAFFC.2017.2774815},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {284-295},
  title    = {Singing robots: How embodiment affects emotional responses to non-linguistic utterances},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized two-stage rank regression framework for
depression score prediction from speech. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(2), 272–283. (<a
href="https://doi.org/10.1109/TAFFC.2017.2766145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper introduces a novel speech-based depression score prediction paradigm, the 2-stage ranking prediction framework, and highlights the benefits it brings to depression prediction. Conventional regression approaches aim to discern a single functional relationship between speech features and depression scores, making an implicit assumption about the existence of a single fixed relationship between the features and scores. However, as the relationship between severity of depression and the clinical score may vary over the range of the assessment scale, this style of analysis may not be suited to depression prediction. The proposed framework on the other hand, imposes a series of partitions on the feature space, with each partition corresponding to a distinct predefined range of depression scores, and predicts the score based on measures of membership to each partition. This approach provides additional flexibility by allowing different rankings to be learnt for different depression scores, and relaxes assumptions made by conventional regression approaches. Results demonstrate the framework&#39;s suitability for depression score prediction: different 2-stage implementations, based on heterogeneous feature extraction and modelling approaches, produce state-of-the-art results on the AVEC-2013 dataset. It is also demonstrated that, unlike fusion of conventional regression systems, the fusion of two-stage systems consistently improves prediction performance.},
  archive  = {J},
  author   = {Nicholas Cummins and Vidhyasaharan Sethu and Julien Epps and James R. Williamson and Thomas F. Quatieri and Jarek Krajewski},
  doi      = {10.1109/TAFFC.2017.2766145},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {272-283},
  title    = {Generalized two-stage rank regression framework for depression score prediction from speech},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational study of primitive emotional contagion in
dyadic interactions. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(2), 258–271. (<a
href="https://doi.org/10.1109/TAFFC.2017.2778154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Interpersonal human-human interaction is a dynamical exchange and coordination of social signals, feelings and emotions usually performed through and across multiple modalities such as facial expressions, gestures, and language. Developing machines able to engage humans in rich and natural interpersonal interactions requires capturing such dynamics. This paper addresses primitive emotional contagion during dyadic interactions in which roles are prefixed. Primitive emotional contagion was defined as the tendency people have to automatically mimic and synchronize their multimodal behavior during interactions and, consequently, to emotionally converge. To capture emotional contagion, a cross-recurrence based methodology that explicitly integrates short and long-term temporal dynamics through the analysis of both facial expressions and sentiment was developed. This approach is employed to assess emotional contagion at unimodal, multimodal and cross-modal levels and is evaluated on the Solid SAL-SEMAINE corpus. Interestingly, the approach is able to show the importance of the adoption of cross-modal strategies for addressing emotional contagion.},
  archive  = {J},
  author   = {Giovanna Varni and Isabelle Hupont and Chloé Clavel and Mohamed Chetouani},
  doi      = {10.1109/TAFFC.2017.2778154},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {258-271},
  title    = {Computational study of primitive emotional contagion in dyadic interactions},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emotion recognition based on high-resolution EEG recordings
and reconstructed brain sources. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(2), 244–257. (<a
href="https://doi.org/10.1109/TAFFC.2017.2768030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalography (EEG)-based emotion recognition is currently a hot issue in the affective computing community. Numerous studies have been published on this topic, following generally the same schema: 1) presentation of emotional stimuli to a number of subjects during the recording of their EEG, 2) application of machine learning techniques to classify the subjects&#39; emotions. The proposed approaches vary mainly in the type of features extracted from the EEG and in the employed classifiers, but it is difficult to compare the reported results due to the use of different datasets. In this paper, we present a new database for the analysis of valence (positive or negative emotions), which is made publicly available. The database comprises physiological recordings and 257-channel EEG data, contrary to all previously published datasets, which include at most 62 EEG channels. Furthermore, we reconstruct the brain activity on the cortical surface by applying source localization techniques. We then compare the performances of valence classification that can be achieved with various features extracted from all source regions (source space features) and from all EEG channels (sensor space features), showing that the source reconstruction improves the classification results. Finally, we discuss the influence of several parameters on the classification scores.},
  archive  = {J},
  author   = {Hanna Becker and Julien Fleureau and Philippe Guillotel and Fabrice Wendling and Isabelle Merlet and Laurent Albera},
  doi      = {10.1109/TAFFC.2017.2768030},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {244-257},
  title    = {Emotion recognition based on high-resolution EEG recordings and reconstructed brain sources},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep physiological affect network for the recognition of
human emotions. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(2), 230–243. (<a
href="https://doi.org/10.1109/TAFFC.2018.2790939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Here we present a robust physiological model for the recognition of human emotions, called Deep Physiological Affect Network. This model is based on a convolutional long short-term memory (ConvLSTM) network and a new temporal margin-based loss function. Formulating the emotion recognition problem as a spectral-temporal sequence classification problem of bipolar EEG signals underlying brain lateralization and photoplethysmogram signals, the proposed model improves the performance of emotion recognition. Specifically, the new loss function allows the model to be more confident as it observes more of specific feelings while training ConvLSTM models. The function is designed to result in penalties for the violation of such confidence. Our experiments on a public dataset show that our deep physiological learning technology significantly increases the recognition rate of state-of-the-art techniques by 15.96 percent increase in accuracy. An extensive analysis of the relationship between participants&#39; emotion ratings and physiological changes in brain lateralization function during the experiment is also presented.},
  archive  = {J},
  author   = {Byung Hyung Kim and Sungho Jo},
  doi      = {10.1109/TAFFC.2018.2790939},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {230-243},
  title    = {Deep physiological affect network for the recognition of human emotions},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human observer and automatic assessment of movement related
self-efficacy in chronic pain: From exercise to functional activity.
<em>IEEE Transactions on Affective Computing</em>, <em>11</em>(2),
214–229. (<a href="https://doi.org/10.1109/TAFFC.2018.2798576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Clinicians tailor intervention in chronic pain rehabilitation to movement related self-efficacy (MRSE). This motivates us to investigate automatic MRSE estimation in this context towards the development of technology that is able to provide appropriate support in the absence of a clinician. We first explored clinical observer estimation, which showed that body movement behaviours, rather than facial expressions or engagement behaviours, were more pertinent to MRSE estimation during physical activity instances. Based on our findings, we built a system that estimates MRSE from bodily expressions and bodily muscle activity captured using wearable sensors. Our results (F1 scores of 0.95 and 0.78 in two physical exercise types) provide evidence of the feasibility of automatic MRSE estimation to support chronic pain physical rehabilitation. We further explored automatic estimation of MRSE with a reduced set of low-cost sensors to investigate the possibility of embedding such capabilities in ubiquitous wearable devices to support functional activity. Our evaluation for both exercise and functional activity resulted in F1 score of 0.79. This result suggests the possibility of (and calls for more studies on) MRSE estimation during everyday functioning in ubiquitous settings. We provide a discussion of the implication of our findings for relevant areas.},
  archive  = {J},
  author   = {Temitayo A. Olugbade and Nadia Bianchi-Berthouze and Nicolai Marquardt and Amanda C. de C. Williams},
  doi      = {10.1109/TAFFC.2018.2798576},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {214-229},
  title    = {Human observer and automatic assessment of movement related self-efficacy in chronic pain: From exercise to functional activity},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Personalized multitask learning for predicting tomorrow’s
mood, stress, and health. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(2), 200–213. (<a
href="https://doi.org/10.1109/TAFFC.2017.2784832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While accurately predicting mood and wellbeing could have a number of important clinical benefits, traditional machine learning (ML) methods frequently yield low performance in this domain. We posit that this is because a one-size-fits-all machine learning model is inherently ill-suited to predicting outcomes like mood and stress, which vary greatly due to individual differences. Therefore, we employ Multitask Learning (MTL) techniques to train personalized ML models which are customized to the needs of each individual, but still leverage data from across the population. Three formulations of MTL are compared: i) MTL deep neural networks, which share several hidden layers but have final layers unique to each task; ii) Multi-task Multi-Kernel learning, which feeds information across tasks through kernel weights on feature types; and iii) a Hierarchical Bayesian model in which tasks share a common Dirichlet Process prior. We offer the code for this work in open source. These techniques are investigated in the context of predicting future mood, stress, and health using data collected from surveys, wearable sensors, smartphone logs, and the weather. Empirical results demonstrate that using MTL to account for individual differences provides large performance improvements over traditional machine learning methods and provides personalized, actionable insights.},
  archive  = {J},
  author   = {Sara Taylor and Natasha Jaques and Ehimwenma Nosakhare and Akane Sano and Rosalind Picard},
  doi      = {10.1109/TAFFC.2017.2784832},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {200-213},
  title    = {Personalized multitask learning for predicting tomorrow&#39;s mood, stress, and health},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Idiom-based features in sentiment analysis: Cutting the
gordian knot. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(2), 189–199. (<a
href="https://doi.org/10.1109/TAFFC.2017.2777842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper we describe an automated approach to enriching sentiment analysis with idiom-based features. Specifically, we automated the development of the supporting lexico-semantic resources, which include (1) a set of rules used to identify idioms in text and (2) their sentiment polarity classifications. Our method demonstrates how idiom dictionaries, which are readily available general pedagogical resources, can be adapted into purpose-specific computational resources automatically. These resources were then used to replace the manually engineered counterparts in an existing system, which originally outperformed the baseline sentiment analysis approaches by 17 percentage points on average, taking the F-measure from 40s into 60s. The new fully automated approach outperformed the baselines by 8 percentage points on average taking the F-measure from 40s into 50s. Although the latter improvement is not as high as the one achieved with the manually engineered features, it has got the advantage of being more general in a sense that it can readily utilize an arbitrary list of idioms without the knowledge acquisition overhead previously associated with this task, thereby fully automating the original approach.},
  archive  = {J},
  author   = {Irena Spasić and Lowri Williams and Andreas Buerki},
  doi      = {10.1109/TAFFC.2017.2777842},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {189-199},
  title    = {Idiom-based features in sentiment analysis: Cutting the gordian knot},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Social signal detection by probabilistic sampling DNN
training. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(1), 164–177. (<a
href="https://doi.org/10.1109/TAFFC.2018.2871450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {When our task is to detect social signals such as laughter and filler events in an audio recording, the most straightforward way is to apply a Hidden Markov Model-or a Hidden Markov Model/Deep Neural Network (HMM/DNN) hybrid, which is considered state-of-the-art nowadays. In this hybrid model, the DNN component is trained on frame-level samples of the classes we are looking for. In such event detection tasks, however, the training labels are seriously imbalanced, as typically only a small fraction of the training data corresponds to these social signals, while the bulk of the utterances consists of speech segments or silence. A strong imbalance of the training classes is known to cause difficulties during DNN training. To alleviate these problems, here we apply the technique called probabilistic sampling, which seeks to balance the class distribution. Probabilistic sampling is a mathematically well-founded combination of upsampling and downsampling, which was found to outperform both of these simple resampling approaches. With this strategy, we managed to achieve a 7-8 percent relative error reduction both at the segment level and frame level, and we efficiently reduced the DNN training times as well.},
  archive  = {J},
  author   = {Gábor Gosztolya and Tamás Grósz and László Tóth},
  doi      = {10.1109/TAFFC.2018.2871450},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {164-177},
  title    = {Social signal detection by probabilistic sampling DNN training},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using temporal features of observers’ physiological measures
to distinguish between genuine and fake smiles. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(1), 163–173. (<a
href="https://doi.org/10.1109/TAFFC.2018.2878029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Future affective computing research could be enhanced by enabling the computer to recognise a displayer&#39;s mental state from an observer&#39;s reaction (measured by physiological signals), using this information to improve recognition algorithms, and eventually to computer systems which are more responsive to human emotions. In this paper, an observer&#39;s physiological signals are analysed to distinguish displayers&#39; genuine from fake smiles. Overall, thirty smile videos were collected from four benchmark database and classified as showing genuine or fake smiles. Overall, forty observers viewed videos. We generally recorded four physiological signals: pupillary response (PR), electrocardiogram (ECG), galvanic skin response (GSR), and blood volume pulse (BVP). A number of temporal features were extracted after a few processing steps, and minimally correlated features between genuine and fake smiles were selected using the NCCA (canonical correlation analysis with neural network) system. Finally, classification accuracy was found to be as high as 98.8 percent from PR features using a leave-one-observer-out process. In comparison, the best current image processing technique [1] on the same video data was 95 percent correct. Observers were 59 percent (on average) to 90 percent (by voting) correct by their conscious choices. Our results demonstrate that humans can non-consciously (or emotionally) recognise the quality of smiles 4 percent better than current image processing techniques and 9 percent better than the conscious choices of groups.},
  archive  = {J},
  author   = {Md Zakir Hossain and Tom Gedeon and Ramesh Sankaranarayana},
  doi      = {10.1109/TAFFC.2018.2878029},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {163-173},
  title    = {Using temporal features of observers’ physiological measures to distinguish between genuine and fake smiles},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic estimation of taste liking through facial
expression dynamics. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(1), 151–163. (<a
href="https://doi.org/10.1109/TAFFC.2018.2832044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The level of taste liking is an important measure for a number of applications such as the prediction of long-term consumer acceptance for different food and beverage products. Based on the fact that facial expressions are spontaneous, instant and heterogeneous sources of information, this paper aims to automatically estimate the level of taste liking through facial expression videos. Instead of using handcrafted features, the proposed approach deep learns the regional expression dynamics, and encodes them to a Fisher vector for video representation. Regional Fisher vectors are then concatenated, and classified by linear SVM classifiers. The aim is to reveal the hidden patterns of taste-elicited responses by exploiting expression dynamics such as the speed and acceleration of facial movements. To this end, we have collected the first large-scale beverage tasting database in the literature. The database has 2,970 videos of taste-induced facial expressions collected from 495 subjects. Our large-scale experiments on this database show that the proposed approach achieves an accuracy of 70.37 percent for distinguishing between three levels of taste-liking. Furthermore, we assess the human performance recruiting 45 participants, and show that humans are significantly less reliable for estimating taste appreciation from facial expressions in comparison to the proposed method.},
  archive  = {J},
  author   = {Hamdi Dibeklioğlu and Theo Gevers},
  doi      = {10.1109/TAFFC.2018.2832044},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {151-163},
  title    = {Automatic estimation of taste liking through facial expression dynamics},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Haptic expression and perception of spontaneous stress.
<em>IEEE Transactions on Affective Computing</em>, <em>11</em>(1),
138–150. (<a href="https://doi.org/10.1109/TAFFC.2018.2830371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Previous studies about kinesthetic expressions of emotions are mainly based on acted expressions of emotions, which can be quite different from spontaneous expressions. This paper describes a study involving a stress induction procedure and stress perception through haptic expressions with N total = 41 young men (aged 19 - 37), all right-handed. We designed a game application to collect spontaneous expressions of stress. This application involved haptic interactions and a stressful event. We observed changes in the haptic behaviors of the participants over different phases: before, during and after the stress induction. In the next step, we investigated how the collected haptic behaviors (both kinematic and force components) were haptically perceived by the other participants. The results suggest the ability of kinesthetic expressions to communicate a spontaneous stress from one person to another.},
  archive  = {J},
  author   = {Yoren Gaffary and Jean-Claude Martin and Mehdi Ammi},
  doi      = {10.1109/TAFFC.2018.2830371},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {138-150},
  title    = {Haptic expression and perception of spontaneous stress},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Facial expression recognition with neighborhood-aware edge
directional pattern (NEDP). <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(1), 125–137. (<a
href="https://doi.org/10.1109/TAFFC.2018.2829707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Currently available local feature descriptors used in facial expression recognition at times suffer from unstable feature descriptions, especially in the presence of weak and distorted edges due to noise, limiting their performances. We propose a novel local descriptor named Neighborhood-aware Edge Directional Pattern (NEDP) to overcome such limitations. Instead of relying solely on the local neighborhood to describe the feature around a pixel, as done by the existing local descriptors, NEDP examines the gradients at the target (center) pixel as well as its neighboring pixels to explore a wider neighborhood for the consistency of the feature in spite of the presence of subtle distortion and noise in local region. We introduce template-orientations for the neighboring pixels, which give importance to the gradients in consistent edge directions, prioritizing the specific neighbors falling in the direction of the local edge to represent the shape of the local textures, unambiguously. Moreover, due to the effective management of the featureless regions, no such region is erroneously encoded as a feature by NEDP. Experiments of the performances for person-independent recognition on benchmark expression datasets also show that NEDP performs better than other existing descriptors, and thereby, improves the overall performance of facial expression recognition.},
  archive  = {J},
  author   = {Md Tauhid Bin Iqbal and M. Abdullah-Al-Wadud and Byungyong Ryu and Farkhod Makhmudkhujaev and Oksam Chae},
  doi      = {10.1109/TAFFC.2018.2829707},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {125-137},
  title    = {Facial expression recognition with neighborhood-aware edge directional pattern (NEDP)},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Personalised, multi-modal, affective state detection for
hybrid brain-computer music interfacing. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(1), 111–124. (<a
href="https://doi.org/10.1109/TAFFC.2018.2801811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Brain-computer music interfaces (BCMIs) may be used to modulate affective states, with applications in music therapy, composition, and entertainment. However, for such systems to work they need to be able to reliably detect their user&#39;s current affective state. We present a method for personalised affective state detection for use in BCMI. We compare it to a population-based detection method trained on 17 users and demonstrate that personalised affective state detection is significantly (p &lt;; 0.01) more accurate, with average improvements in accuracy of 10.2 percent for valence and 9.3 percent for arousal. We also compare a hybrid BCMI (a BCMI that combines physiological signals with neurological signals) to a conventional BCMI design (one based upon the use of only EEG features) and demonstrate that the hybrid design results in a significant (p &lt;; 0.01) 6.2 percent improvement in performance for arousal classification and a significant (p &lt;; 0.01) 5.9 percent improvement for valence classification.},
  archive  = {J},
  author   = {Ian Daly and Duncan Williams and Asad Malik and James Weaver and Alexis Kirke and Faustina Hwang and Eduardo Miranda and Slawomir J. Nasuto},
  doi      = {10.1109/TAFFC.2018.2801811},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {111-124},
  title    = {Personalised, multi-modal, affective state detection for hybrid brain-computer music interfacing},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward constructing a real-time social anxiety evaluation
system: Exploring effective heart rate features. <em>IEEE Transactions
on Affective Computing</em>, <em>11</em>(1), 100–110. (<a
href="https://doi.org/10.1109/TAFFC.2018.2792000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social anxiety is a negative emotion which may impair the health of the heart and social functioning of an individual. This work analyzes the influence of social anxiety on the autonomic nerve control of the heart in two social exposure events: public speaking and thesis defending. In an experiment of public speaking, 59 human subjects were tested, and 11 conventional heartbeat measures and a heartbeat measure named the range of local Hurst exponents (RLHE) were evaluated for their capabilities to reveal the onset of social anxiety. Two-sample t-test between the baseline data and high anxiety data shows that social anxiety significantly reduces the complexity of the heartbeats. In an experiment of thesis defense, heartbeats data were acquired from nine graduate students. With the combination of three conventional features and the RLHE feature, a support vector machine classifier obtained true positive rate and true negative rate of 84.88 and 97.29 percent in the five-fold cross validation process of binary classification between high anxiety status and low anxiety status; the classifier also realized a generalization accuracy of 81.82 percent in detecting the high anxiety status in the thesis defense. A real-time anxiety monitoring system was established based on the above anxiety detecting method.},
  archive  = {J},
  author   = {Wanhui Wen and Guangyuan Liu and Zhi-Hong Mao and Wenjin Huang and Xu Zhang and Hui Hu and Jiemin Yang and Wenyan Jia},
  doi      = {10.1109/TAFFC.2018.2792000},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {100-110},
  title    = {Toward constructing a real-time social anxiety evaluation system: Exploring effective heart rate features},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic ECG-based emotion recognition in music listening.
<em>IEEE Transactions on Affective Computing</em>, <em>11</em>(1),
85–99. (<a href="https://doi.org/10.1109/TAFFC.2017.2781732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents an automatic ECG-based emotion recognition algorithm for human emotion recognition. First, we adopt a musical induction method to induce participants&#39; real emotional states and collect their ECG signals without any deliberate laboratory setting. Afterward, we develop an automatic ECG-based emotion recognition algorithm to recognize human emotions elicited by listening to music. Physiological ECG features extracted from the time-, and frequency-domain, and nonlinear analyses of ECG signals are used to find emotion-relevant features and to correlate them with emotional states. Subsequently, we develop a sequential forward floating selection-kernel-based class separability-based (SFFS-KBCS-based) feature selection algorithm and utilize the generalized discriminant analysis (GDA) to effectively select significant ECG features associated with emotions and to reduce the dimensions of the selected features, respectively. Positive/negative valence, high/low arousal, and four types of emotions (joy, tension, sadness, and peacefulness) are recognized using least squares support vector machine (LS-SVM) recognizers. The results show that the correct classification rates for positive/negative valence, high/low arousal, and four types of emotion classification tasks are 82.78, 72.91, and 61.52 percent, respectively.},
  archive  = {J},
  author   = {Yu-Liang Hsu and Jeen-Shing Wang and Wei-Chun Chiang and Chien-Han Hung},
  doi      = {10.1109/TAFFC.2017.2781732},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {85-99},
  title    = {Automatic ECG-based emotion recognition in music listening},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuous, real-time emotion annotation: A novel
joystick-based analysis framework. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(1), 78–84. (<a
href="https://doi.org/10.1109/TAFFC.2017.2772882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion labels are usually obtained via either manual annotation, which is tedious and time-consuming, or questionnaires, which neglect the time-varying nature of emotions and depend on human&#39;s unreliable introspection. To overcome these limitations, we developed a continuous, real-time, joystick-based emotion annotation framework. To assess the same, 30 subjects each watched 8 emotion-inducing videos. They were asked to indicate their instantaneous emotional state in a valence-arousal (V-A) space, using a joystick. Subsequently, five analyses were undertaken: (i) a System Usability Scale (SUS) questionnaire unveiled the framework&#39;s excellent usability; (ii) MANOVA analysis of the mean V-A ratings and (iii) trajectory similarity analyses of the annotations confirmed the successful elicitation of emotions; (iv) Change point analysis of the annotations, revealed a direct mapping between emotional events and annotations, thereby enabling automatic detection of emotionally salient points in the videos; and (v) Support Vector Machines (SVM) were trained on classification of 5 second chunks of annotations as well as their change-points. The classification results confirmed that ratings patterns were cohesive across the participants. These analyses confirm the value, validity, and usability of our annotation framework. They also showcase novel tools for gaining greater insights into the emotional experience of the participants.},
  archive  = {J},
  author   = {Karan Sharma and Claudio Castellini and Freek Stulp and Egon L. van den Broek},
  doi      = {10.1109/TAFFC.2017.2772882},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {78-84},
  title    = {Continuous, real-time emotion annotation: A novel joystick-based analysis framework},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel technique to develop cognitive models for ambiguous
image identification using eye tracker. <em>IEEE Transactions on
Affective Computing</em>, <em>11</em>(1), 63–77. (<a
href="https://doi.org/10.1109/TAFFC.2017.2768026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Human behavior can be analyzed using Eye tracker. Thus, it is used for revealing the cognitive processes for object identification. Cognitive process is the mental ability for identification of what our eyes see. Vision with 20/20 sometimes may not reveal the purpose. In this study, ambiguous images are taken to observe the cognitive process in participants. During the perception of an object, a participant uses goal-directed search for identifying various objects. Dense gaze coordinates provide the region of interests and are considered as the target regions for object identification in ambiguous images. These data are used to develop cognitive models for identification of ambiguous images. Features such as, eye fixation, pupil diameter, fixation durations, moments of inertia, and polar moments are used for developing the cognitive model. Three different feature selection methods along with six different classifiers are used for the task of classification. The selection of a subset of features using hypothesis testing performed well, compared to principal component analysis based dimensionality reduction method. This study could be used in detecting whether a participants is lying or not while perceiving an ambiguous image.},
  archive  = {J},
  author   = {Anup Kumar Roy and Md. Nadeem Akhtar and Manjunatha Mahadevappa and Rajlakshmi Guha and Jayanta Mukherjee},
  doi      = {10.1109/TAFFC.2017.2768026},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {63-77},
  title    = {A novel technique to develop cognitive models for ambiguous image identification using eye tracker},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Affective recognition in dynamic and interactive virtual
environments. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(1), 45–62. (<a
href="https://doi.org/10.1109/TAFFC.2017.2764896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The past decade has witnessed a significant increase in interest in human emotional behaviours in the future of interactive multimodal computing. Although much consideration has been given to non-interactive affective stimuli (e.g., images and videos), the recognition of emotions within interactive virtual environments has not received an equal level of attention. In the present study, a psychophysiological database, cataloguing the EEG, GSR and heart rate of 30 participants, exposed to an affective virtual environment, has been constructed. 743 features were extracted from the physiological signals. Then, by employing a feature selection technique, the dimensionality of the feature space was reduced to a smaller subset, containing only 30 features. Four classification techniques (KNN, SVM, Discriminant Analysis (DA) and Classification Tree) were employed to classify the affective psychophysiological database into four Affective Clusters (derived from a Valence-Arousal space) and eight Emotion Labels. By employing cross-validation techniques, the performances of more than a quarter of a million different classification settings (various window lengths, classifier settings, etc.) were investigated. The results suggested that the physiological signals could be employed to classify emotional experiences, with high precision. The KNN and SVM outperformed both Classification Tree and DA classifiers; with 97.01 percent and 92.84 percent mean accuracies, respectively.},
  archive  = {J},
  author   = {Mohammadhossein Moghimi and Robert Stone and Pia Rotshtein},
  doi      = {10.1109/TAFFC.2017.2764896},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {45-62},
  title    = {Affective recognition in dynamic and interactive virtual environments},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dimensional affect recognition from HRV: An approach based
on supervised SOM and ELM. <em>IEEE Transactions on Affective
Computing</em>, <em>11</em>(1), 32–44. (<a
href="https://doi.org/10.1109/TAFFC.2017.2763943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Dimensional affect recognition is a challenging topic and current techniques do not yet provide the accuracy necessary for HCI applications. In this work we propose two new methods. The first is a novel self-organizing model that learns from similarity between features and affects. This method produces a graphical representation of the multidimensional data which may assist the expert analysis. The second method uses extreme learning machines, an emerging artificial neural network model. Aiming for minimum intrusiveness, we use only the heart rate variability, which can be recorded using a small set of sensors. The methods were validated with two datasets. The first is composed of 16 sessions with different participants and was used to evaluate the models in a classification task. The second one was the publicly available Remote Collaborative and Affective Interaction (RECOLA) dataset, which was used for dimensional affect estimation. The performance evaluation used the kappa score, unweighted average recall and the concordance correlation coefficient. The concordance coefficient on the RECOLA test partition was 0.421 in arousal and 0.321 in valence. Results show that our models outperform state-of-the-art models on the same data and provides new ways to analyze affective states.},
  archive  = {J},
  author   = {Leandro A. Bugnon and Rafael A. Calvo and Diego H. Milone},
  doi      = {10.1109/TAFFC.2017.2763943},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {32-44},
  title    = {Dimensional affect recognition from HRV: An approach based on supervised SOM and ELM},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature learning from spectrograms for assessment of
personality traits. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(1), 25–31. (<a
href="https://doi.org/10.1109/TAFFC.2017.2763132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Several methods have recently been proposed to analyze speech and automatically infer the personality of the speaker. These methods often rely on prosodic and other hand crafted speech processing features extracted with off-the-shelf toolboxes. To achieve high accuracy, numerous features are typically extracted using complex and highly parameterized algorithms. In this paper, a new method based on feature learning and spectrogram analysis is proposed to simplify the feature extraction process while maintaining a high level of accuracy. The proposed method learns a dictionary of discriminant features from patches extracted in the spectrogram representations of training speech segments. Each speech segment is then encoded using the dictionary, and the resulting feature set is used to perform classification of personality traits. Experiments indicate that the proposed method achieves state-of-the-art results with an important reduction in complexity when compared to the most recent reference methods. The number of features, and difficulties linked to the feature extraction process are greatly reduced as only one type of descriptors is used, for which the 7 parameters can be tuned automatically. In contrast, the simplest reference method uses 4 types of descriptors to which 6 functionals are applied, resulting in over 20 parameters to be tuned.},
  archive  = {J},
  author   = {Marc-André Carbonneau and Eric Granger and Yazid Attabi and Ghyslain Gagnon},
  doi      = {10.1109/TAFFC.2017.2763132},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {25-31},
  title    = {Feature learning from spectrograms for assessment of personality traits},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approaches to automated detection of cyberbullying: A
survey. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(1), 3–24. (<a
href="https://doi.org/10.1109/TAFFC.2017.2761757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Research into cyberbullying detection has increased in recent years, due in part to the proliferation of cyberbullying across social media and its detrimental effect on young people. A growing body of work is emerging on automated approaches to cyberbullying detection. These approaches utilise machine learning and natural language processing techniques to identify the characteristics of a cyberbullying exchange and automatically detect cyberbullying by matching textual data to the identified traits. In this paper, we present a systematic review of published research (as identified via Scopus, ACM and IEEE Xplore bibliographic databases) on cyberbullying detection approaches. On the basis of our extensive literature review, we categorise existing approaches into 4 main classes, namely supervised learning, lexicon-based, rule-based, and mixed-initiative approaches. Supervised learning-based approaches typically use classifiers such as SVM and Naıve Bayes to develop predictive models for cyberbullying detection. Lexicon-based systems utilise word lists and use the presence of words within the lists to detect cyberbullying. Rule-based approaches match text to predefined rules to identify bullying, and mixed-initiatives approaches combine human-based reasoning with one or more of the aforementioned approaches. We found lack of labelled datasets and non-holistic consideration of cyberbullying by researchers when developing detection systems are two key challenges facing cyberbullying detection research. This paper essentially maps out the state-of-the-art in cyberbullying detection research and serves as a resource for researchers to determine where to best direct their future research efforts in this field.},
  archive  = {J},
  author   = {Semiu Salawu and Yulan He and Joanna Lumsden},
  doi      = {10.1109/TAFFC.2017.2761757},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {3-24},
  title    = {Approaches to automated detection of cyberbullying: A survey},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transactions on affective computing—celebrating the 10th
year of publication. <em>IEEE Transactions on Affective Computing</em>,
<em>11</em>(1), 1–2. (<a
href="https://doi.org/10.1109/TAFFC.2020.2972985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Presents the editorial for this issue of the publication.},
  archive  = {J},
  author   = {Elisabeth André},
  doi      = {10.1109/TAFFC.2020.2972985},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {1-2},
  title    = {Transactions on affective Computing—Celebrating the 10th year of publication},
  volume   = {11},
  year     = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
