<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---313">TVCG - 313</h2>
<ul>
<li><details>
<summary>
(2020). StainedView: Variable-intensity light-attenuation display
with cascaded spatial color filtering for improved color fidelity.
<em>TVCG</em>, <em>26</em>(12), 3576–3586. (<a
href="https://doi.org/10.1109/TVCG.2020.3023569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present StainedView, an optical see-through display that spatially filters the spectral distribution of light to form an image with improved color fidelity. Existing light-attenuation displays have limited color fidelity and contrast, resulting in a degraded appearance of virtual images. To use these displays to present virtual images that are more consistent with the real world, we require three things: intensity modulation of incoming light, spatial color filtering with narrower bandwidth, and appropriate light modulation for incoming light with an arbitrary spectral distribution. In StainedView, we address the three requirements by cascading two phase-only spatial light modulators (PSLMs), a digital micromirror device, and polarization optics to control both light intensity and spectrum distribution. We show that our design has a 1.8 times wider color gamut fidelity (75.8\% fulfillment of sRGB color space) compared to the existing single-PSLM approach (41.4\%) under a reference white light. We demonstrated the design with a proof-of-concept display system. We further introduce our optics design and pixel-selection algorithm for the given light input, evaluate the spatial color filter, and discuss the limitation of the current prototype.},
  archive      = {J_TVCG},
  author       = {Takumi Kaminokado and Yuichi Hiroi and Yuta Itoh},
  doi          = {10.1109/TVCG.2020.3023569},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3576-3586},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StainedView: Variable-intensity light-attenuation display with cascaded spatial color filtering for improved color fidelity},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of augmented reality display techniques to
support medical needle insertion. <em>TVCG</em>, <em>26</em>(12),
3568–3575. (<a href="https://doi.org/10.1109/TVCG.2020.3023637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) may be a useful technique to overcome issues of conventionally used navigation systems supporting medical needle insertions, like increased mental workload and complicated hand-eye coordination. Previous research primarily focused on the development of AR navigation systems designed for specific displaying devices, but differences between employed methods have not been investigated before. To this end, a user study involving a needle insertion task was conducted comparing different AR display techniques with a monitor-based approach as baseline condition for the visualization of navigation information. A video see-through stationary display, an optical see-through head-mounted display and a spatial AR projector-camera-system were investigated in this comparison. Results suggest advantages of using projected navigation information in terms of lower task completion time, lower angular deviation and affirmative subjective participant feedback. Techniques requiring the intermediate view on screens, i.e. the stationary display and the baseline condition, showed less favorable results. Thus, benefits of providing AR navigation information compared to a conventionally used method could be identified. Significant objective measures results, as well as an identification of advantages and disadvantages of individual display techniques contribute to the development and design of improved needle navigation systems.},
  archive      = {J_TVCG},
  author       = {Florian Heinrich and Luisa Schwenderling and Fabian Joeres and Kai Lawonn and Christian Hansen},
  doi          = {10.1109/TVCG.2020.3023637},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3568-3575},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparison of augmented reality display techniques to support medical needle insertion},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaze-dependent simulation of light perception in virtual
reality. <em>TVCG</em>, <em>26</em>(12), 3557–3567. (<a
href="https://doi.org/10.1109/TVCG.2020.3023604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perception of light is inherently different inside a virtual reality (VR) or augmented reality (AR) simulation when compared to the real world. Conventional head-worn displays (HWDs) are not able to display the same high dynamic range of brightness and color as the human eye can perceive in the real world. To mimic the perception of real-world scenes in virtual scenes, it is crucial to reproduce the effects of incident light on the human visual system. In order to advance virtual simulations towards perceptual realism, we present an eye-tracked VR/AR simulation comprising effects for gaze-dependent temporal eye adaption, perceptual glare, visual acuity reduction, and scotopic color vision. Our simulation is based on medical expert knowledge and medical studies of the healthy human eye. We conducted the first user study comparing the perception of light in a real-world low-light scene to a VR simulation. Our results show that the proposed combination of simulated visual effects is well received by users and also indicate that an individual adaptation is necessary, because perception of light is highly subjective.},
  archive      = {J_TVCG},
  author       = {Laura R. Luidolt and Michael Wimmer and Katharina Krösl},
  doi          = {10.1109/TVCG.2020.3023604},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3557-3567},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gaze-dependent simulation of light perception in virtual reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Construction of the virtual embodiment questionnaire (VEQ).
<em>TVCG</em>, <em>26</em>(12), 3546–3556. (<a
href="https://doi.org/10.1109/TVCG.2020.3023603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User embodiment is important for many virtual reality (VR) applications, for example, in the context of social interaction, therapy, training, or entertainment. However, there is no data-driven and validated instrument to empirically measure the perceptual aspects of embodiment, necessary to reliably evaluate this important phenomenon. To provide a method to assess components of virtual embodiment in a reliable and consistent fashion, we constructed a Virtual Embodiment Questionnaire (VEQ). We reviewed previous literature to identify applicable constructs and questionnaire items, and performed a confirmatory factor analysis (CFA) on the data from three experiments (N = 196). The analysis confirmed three factors: (1) ownership of a virtual body, (2) agency over a virtual body, and (3) the perceived change in the body schema. A fourth study (N = 22) was conducted to confirm the reliability and validity of the scale, by investigating the impacts of latency and latency jitter present in the simulation. We present the proposed scale and study results and discuss resulting implications.},
  archive      = {J_TVCG},
  author       = {Daniel Roth and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2020.3023603},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3546-3556},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Construction of the virtual embodiment questionnaire (VEQ)},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stage-wise salient object detection in 360° omnidirectional
image via object-level semantical saliency ranking. <em>TVCG</em>,
<em>26</em>(12), 3535–3545. (<a
href="https://doi.org/10.1109/TVCG.2020.3023636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2D image based salient object detection (SOD) has been extensively explored, while the 360° omnidirectional image based SOD has received less research attention and there exist three major bottlenecks that are limiting its performance. Firstly, the currently available training data is insufficient for the training of 360° SOD deep model. Secondly, the visual distortions in 360° omnidirectional images usually result in large feature gap between 360° images and 2D images; consequently, the widely used stage-wise training-a widely-used solution to alleviate the training data shortage problem, becomes infeasible when conducing SOD in 360° omnidirectional images. Thirdly, the existing 360° SOD approach has followed a multi-task methodology that performs salient object localization and segmentation-like saliency refinement at the same time, being faced with extremely large problem domain, making the training data shortage dilemma even worse. To tackle all these issues, this paper divides the 360° SOD into a multi-staqe task, the key rationale of which is to decompose the original complex problem domain into sequential easy sub problems that only demand for small-scale training data. Meanwhile, we learn how to rank the “object-level semantical saliency”, aiming to locate salient viewpoints and objects accurately. Specifically, to alleviate the training data shortage problem, we have released a novel dataset named 360-SSOD, containing 1,105 360° omnidirectional images with manually annotated object-level saliency ground truth, whose semantical distribution is more balanced than that of the existing dataset. Also, we have compared the proposed method with 13 SOTA methods, and all quantitative results have demonstrated the performance superiority.},
  archive      = {J_TVCG},
  author       = {Guangxiao Ma and Shuai Li and Chenglizhao Chen and Aimin Hao and Hong Qin},
  doi          = {10.1109/TVCG.2020.3023636},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3535-3545},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Stage-wise salient object detection in 360° omnidirectional image via object-level semantical saliency ranking},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dream-experiment: A MR user interface with natural
multi-channel interaction for virtual experiments. <em>TVCG</em>,
<em>26</em>(12), 3524–3534. (<a
href="https://doi.org/10.1109/TVCG.2020.3023602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a set of MR technologies for middle school experimental teaching environments and develops a multi-channel MR user interface called Dream-Experiment. The goal of Dream-Experiment is to improve the traditional MR user interface, so that users can get a real, natural 3D interactive experience like real experiments, but without danger and pollution. In terms of visual presentation, we design multi-camera collaborative registration to realize robust 6-DoF MR interactive space, and also define a complete rendering pipeline to provide improved processing of virtual-real objects&#39; occlusion including translucent devices. In the virtual-real interaction, we provide six interaction modes that support visual interaction, tangible interaction, virtual-real gestures with touching, voice, thermal feeling, and olfactory feeling. After users&#39; testing, we find that Dream-Experiment has better interactive efficiency and user experience than traditional MR environments.},
  archive      = {J_TVCG},
  author       = {Tianren Luo and Mingmin Zhang and Zhigeng Pan and Zheng Li and Ning Cai and Jinda Miao and Youbin Chen and Mingxi Xu},
  doi          = {10.1109/TVCG.2020.3023602},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3524-3534},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dream-experiment: A MR user interface with natural multi-channel interaction for virtual experiments},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-grained visual recognition in mobile augmented reality
for technical support. <em>TVCG</em>, <em>26</em>(12), 3514–3523. (<a
href="https://doi.org/10.1109/TVCG.2020.3023635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality is increasingly explored as the new medium for two-way remote collaboration applications to guide the participants more effectively and efficiently via visual instructions. As users strive for more natural interaction and automation in augmented reality applications, new visual recognition techniques are needed to enhance the user experience. Although simple object recognition is often used in augmented reality towards this goal, most collaboration tasks are too complex for such recognition algorithms to suffice. In this paper, we propose a fine-grained visual recognition approach for mobile augmented reality, which leverages RGB video frames and sparse depth feature points identified in real-time, as well as camera pose data to detect various visual states of an object. We demonstrate the value of our approach through a mobile application designed for hardware support, which automatically detects the state of an object to present the right set of information in the right context.},
  archive      = {J_TVCG},
  author       = {Bing Zhou and Sinem Güven},
  doi          = {10.1109/TVCG.2020.3023635},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3514-3523},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fine-grained visual recognition in mobile augmented reality for technical support},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correcting the proximity effect in nanophotonic phased
arrays. <em>TVCG</em>, <em>26</em>(12), 3503–3513. (<a
href="https://doi.org/10.1109/TVCG.2020.3023601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermally modulated Nanophotonic Phased Arrays (NPAs) can be used as phase-only holographic displays. Compared to the holographic displays based on Liquid Crystal on Silicon Spatial Light Modulators (LCoS SLMs), NPAs have the advantage of integrated light source and high refresh rate. However, the formation of the desired wavefront requires accurate modulation of the phase which is distorted by the thermal proximity effect. This problem has been largely overlooked and existing approaches to similar problems are either slow or do not provide a good result in the setting of NPAs. We propose two new algorithms based on the iterative phase retrieval algorithm and the proximal algorithm to address this challenge. We have carried out computational simulations to compare and contrast various algorithms in terms of image quality and computational efficiency. This work is going to benefit the research on NPAs and enable the use of large-scale NPAs as holographic displays.},
  archive      = {J_TVCG},
  author       = {Xuetong Sun and Yang Zhang and Po-Chun Huang and Niloy Acharjee and Mario Dagenais and Martin Peckerar and Amitabh Varshney},
  doi          = {10.1109/TVCG.2020.3023601},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3503-3513},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Correcting the proximity effect in nanophotonic phased arrays},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Breaking the screen: Interaction across touchscreen
boundaries in virtual reality for mobile knowledge workers.
<em>TVCG</em>, <em>26</em>(12), 3490–3502. (<a
href="https://doi.org/10.1109/TVCG.2020.3023567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications.},
  archive      = {J_TVCG},
  author       = {Verena Biener and Daniel Schneider and Travis Gesslein and Alexander Otte and Bastian Kuth and Per Ola Kristensson and Eyal Ofek and Michel Pahud and Jens Grubert},
  doi          = {10.1109/TVCG.2020.3023567},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3490-3502},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Breaking the screen: Interaction across touchscreen boundaries in virtual reality for mobile knowledge workers},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding multimodal user gesture and speech behavior
for object manipulation in augmented reality using elicitation.
<em>TVCG</em>, <em>26</em>(12), 3479–3489. (<a
href="https://doi.org/10.1109/TVCG.2020.3023566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary objective of this research is to understand how users manipulate virtual objects in augmented reality using multimodal interaction (gesture and speech) and unimodal interaction (gesture). Through this understanding, natural-feeling interactions can be designed for this technology. These findings are derived from an elicitation study employing Wizard of Oz design aimed at developing user-defined multimodal interaction sets for building tasks in 3D environments using optical see-through augmented reality headsets. The modalities tested are gesture and speech combined, gesture only, and speech only. The study was conducted with 24 participants. The canonical referents for translation, rotation, and scale were used along with some abstract referents (create, destroy, and select). A consensus set of gestures for interactions is provided. Findings include the types of gestures performed, the timing between co-occurring gestures and speech (130 milliseconds), perceived workload by modality (using NASA TLX), and design guidelines arising from this study. Multimodal interaction, in particular gesture and speech interactions for augmented reality headsets, are essential as this technology becomes the future of interactive computing. It is possible that in the near future, augmented reality glasses will become pervasive.},
  archive      = {J_TVCG},
  author       = {Adam S. Williams and Jason Garcia and Francisco Ortega},
  doi          = {10.1109/TVCG.2020.3023566},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3479-3489},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding multimodal user gesture and speech behavior for object manipulation in augmented reality using elicitation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial presence, performance, and behavior between real,
remote, and virtual immersive environments. <em>TVCG</em>,
<em>26</em>(12), 3467–3478. (<a
href="https://doi.org/10.1109/TVCG.2020.3023574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial presence encompasses the user&#39;s ability to experience a sense of “being there”. While particular attention was given to assess spatial presence in real and virtual environments, few have been interested in measuring it in telepresence situations. To bridge this gap, the present work introduces a study that compares the execution of a task in three conditions: a real physical environment, a remote environment via a telepresence system, and a virtual simulation of the real environment. Following a within-subject design, 27 participants performed a navigation task consisting in following a route while avoiding obstacles. Spatial presence and five related factors (affordance, enjoyment, attention allocation, reality, and cybersickness) were evaluated using a presence questionnaire. In addition, performance measures were gathered regarding environment recollection and task execution. The evaluation also included a behavioral metric measured by obstacle avoidance distance extracted from participants&#39; trajectories. Results indicated a higher presence in the real environment, along with the best performance measures. No difference was found in spatial presence between the remote and the virtual conditions, although a higher degree of affordance and enjoyment was attributed to the virtual environment, and a higher degree of reality was attributed to the remote environment. The number of collisions was found to be lower in the remote condition compared to the virtual condition. Similarly, the avoidance distance was also bigger (and almost similar) in the real and the remote environments compared to the virtual environment indicating a greater caution of participants. These cues highlight that the behavior of participants in the remote condition was closer to their behavior in the real situation than it was in the virtual condition. Furthermore, positive correlations were found between the reality factor and two of the three performance measures, as well as with the behavioral metric. This suggests that the degree of physical existence of the space in which participants operate can influence their performance and behavior.},
  archive      = {J_TVCG},
  author       = {Nawel Khenak and Jeanne Vézien and Patrick Bourdot},
  doi          = {10.1109/TVCG.2020.3023574},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3467-3478},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spatial presence, performance, and behavior between real, remote, and virtual immersive environments},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Photorealistic audio-driven video portraits. <em>TVCG</em>,
<em>26</em>(12), 3457–3466. (<a
href="https://doi.org/10.1109/TVCG.2020.3023573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video portraits are common in a variety of applications, such as videoconferencing, news broadcasting, and virtual education and training. We present a novel method to synthesize photorealistic video portraits for an input portrait video, automatically driven by a person&#39;s voice. The main challenge in this task is the hallucination of plausible, photorealistic facial expressions from input speech audio. To address this challenge, we employ a parametric 3D face model represented by geometry, facial expression, illumination, etc., and learn a mapping from audio features to model parameters. The input source audio is first represented as a high-dimensional feature, which is used to predict facial expression parameters of the 3D face model. We then replace the expression parameters computed from the original target video with the predicted one, and rerender the reenacted face. Finally, we generate a photorealistic video portrait from the reenacted synthetic face sequence via a neural face renderer. One appealing feature of our approach is the generalization capability for various input speech audio, including synthetic speech audio from text-to-speech software. Extensive experimental results show that our approach outperforms previous general-purpose audio-driven video portrait methods. This includes a user study demonstrating that our results are rated as more realistic than previous methods.},
  archive      = {J_TVCG},
  author       = {Xin Wen and Miao Wang and Christian Richardt and Ze-Yin Chen and Shi-Min Hu},
  doi          = {10.1109/TVCG.2020.3023573},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3457-3466},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Photorealistic audio-driven video portraits},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mobile3DRecon: Real-time monocular 3D reconstruction on a
mobile phone. <em>TVCG</em>, <em>26</em>(12), 3446–3456. (<a
href="https://doi.org/10.1109/TVCG.2020.3023634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a real-time monocular 3D reconstruction system on a mobile phone, called Mobile3DRecon. Using an embedded monocular camera, our system provides an online mesh generation capability on back end together with real-time 6DoF pose tracking on front end for users to achieve realistic AR effects and interactions on mobile phones. Unlike most existing state-of-the-art systems which produce only point cloud based 3D models online or surface mesh offline, we propose a novel online incremental mesh generation approach to achieve fast online dense surface mesh reconstruction to satisfy the demand of real-time AR applications. For each keyframe of 6DoF tracking, we perform a robust monocular depth estimation, with a multi-view semi-global matching method followed by a depth refinement post-processing. The proposed mesh generation module incrementally fuses each estimated keyframe depth map to an online dense surface mesh, which is useful for achieving realistic AR effects such as occlusions and collisions. We verify our real-time reconstruction results on two mid-range mobile platforms. The experiments with quantitative and qualitative evaluation demonstrate the effectiveness of the proposed monocular 3D reconstruction system, which can handle the occlusions and collisions between virtual objects and real scenes to achieve realistic AR effects.},
  archive      = {J_TVCG},
  author       = {Xingbin Yang and Liyang Zhou and Hanqing Jiang and Zhongliang Tang and Yuanbo Wang and Hujun Bao and Guofeng Zhang},
  doi          = {10.1109/TVCG.2020.3023634},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3446-3456},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NIID-net: Adapting surface normal knowledge for intrinsic
image decomposition in indoor scenes. <em>TVCG</em>, <em>26</em>(12),
3434–3445. (<a href="https://doi.org/10.1109/TVCG.2020.3023565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic image decomposition, i.e. , decomposing a natural image into a reflectance image and a shading image, is used in many augmented reality applications for achieving better visual coherence between virtual contents and real scenes. The main challenge is that the decomposition is ill-posed, especially in indoor scenes where lighting conditions are complicated, while real training data is inadequate. To solve this challenge, we propose NIID-Net, a novel learning-based framework that adapts surface normal knowledge for improving the decomposition. The knowledge learned from relatively more abundant data for surface normal estimation is integrated into intrinsic image decomposition in two novel ways. First, normal feature adapters are proposed to incorporate scene geometry features when decomposing the image. Secondly, a map of integrated lighting is proposed for propagating object contour and planarity information during shading rendering. Furthermore, this map is capable of representing spatially-varying lighting conditions indoors. Experiments show that NIID-Net achieves competitive performance in reflectance estimation and outperforms all previous methods in shading estimation quantitatively and qualitatively. The source code of our implementation is released at https://github.com/zju3dv/NIID-Net .},
  archive      = {J_TVCG},
  author       = {Jundan Luo and Zhaoyang Huang and Yijin Li and Xiaowei Zhou and Guofeng Zhang and Hujun Bao},
  doi          = {10.1109/TVCG.2020.3023565},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3434-3445},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NIID-net: Adapting surface normal knowledge for intrinsic image decomposition in indoor scenes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Eyes-free target acquisition during walking in immersive
mixed reality. <em>TVCG</em>, <em>26</em>(12), 3423–3433. (<a
href="https://doi.org/10.1109/TVCG.2020.3023570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reaching towards out-of-sight objects during walking is a common task in daily life, however the same task can be challenging when wearing immersive Head-Mounted Displays (HMD). In this paper, we investigate the effects of spatial reference frame, walking path curvature, and target placement relative to the body on user performance of manually acquiring out-of-sight targets located around their bodies, as they walk in a spatial-mapping Mixed Reality (MR) environment wearing an immersive HMD. We found that walking and increased path curvature negatively affected the overall spatial accuracy of the performance, and that the performance benefited more from using the torso as the reference frame than the head. We also found that targets placed at maximum reaching distance yielded less error in angular rotation and depth of the reaching arm. We discuss our findings with regard to human walking kinesthetics and the sensory integration in the peripersonal space during locomotion in immersive MR. We provide design guidelines for future immersive MR experience featuring spatial mapping and full-body motion tracking to provide better embodied experience.},
  archive      = {J_TVCG},
  author       = {Qiushi Zhou and Difeng Yu and Martin N Reinoso and Joshua Newn and Jorge Goncalves and Eduardo Velloso},
  doi          = {10.1109/TVCG.2020.3023570},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3423-3433},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eyes-free target acquisition during walking in immersive mixed reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Invisible boundaries for VR: Auditory and haptic signals as
indicators for real world boundaries. <em>TVCG</em>, <em>26</em>(12),
3414–3422. (<a href="https://doi.org/10.1109/TVCG.2020.3023607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining awareness of real world boundaries whilst being immersed in virtual reality (VR) with head mounted displays (HMDs), is a necessity for the physical integrity of the user. This paper explores whether individual human senses can be allocated to the real and the virtual world and what effect this has on workload, presence, performance and perceived safety. We present the results of a lab study (N=33) where the auditory and haptic sense of participants was trained to be an indicator for real world boundaries, while their visual sense was bound to a VR experience with an HMD. Our results suggests that allocating senses increases workload. However, while performance is comparable to purely visual indications of boundaries, sense allocation seems to improve presence. Participants prefer the signals to be separate or combined subsequently, depending on the priority and proximity to the boundary. This exploratory study is valuable for developers and researchers who want to start including audio and haptic signals as indicators for real world boundaries.},
  archive      = {J_TVCG},
  author       = {Ceenu George and Patrick Tamunjoh and Heinrich Hussmann},
  doi          = {10.1109/TVCG.2020.3023607},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3414-3422},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Invisible boundaries for VR: Auditory and haptic signals as indicators for real world boundaries},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully-occluded target selection in virtual reality.
<em>TVCG</em>, <em>26</em>(12), 3402–3413. (<a
href="https://doi.org/10.1109/TVCG.2020.3023606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of fully-occluded targets is common within virtual environments, ranging from a virtual object located behind a wall to a datapoint of interest hidden in a complex visualization. However, efficient input techniques for locating and selecting these targets are mostly underexplored in virtual reality (VR) systems. In this paper, we developed an initial set of seven techniques techniques for fully-occluded target selection in VR. We then evaluated their performance in a user study and derived a set of design implications for simple and more complex tasks from our results. Based on these insights, we refined the most promising techniques and conducted a second, more comprehensive user study. Our results show how factors, such as occlusion layers, target depths, object densities, and the estimation of target locations, can affect technique performance. Our findings from both studies and distilled recommendations can inform the design of future VR systems that offer selections for fully-occluded targets.},
  archive      = {J_TVCG},
  author       = {Difeng Yu and Qiushi Zhou and Joshua Newn and Tilman Dingler and Eduardo Velloso and Jorge Goncalves},
  doi          = {10.1109/TVCG.2020.3023606},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3402-3413},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fully-occluded target selection in virtual reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing non-visual and visual guidance methods for narrow
field of view augmented reality displays. <em>TVCG</em>,
<em>26</em>(12), 3389–3401. (<a
href="https://doi.org/10.1109/TVCG.2020.3023605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current augmented reality displays still have a very limited field of view compared to the human vision. In order to localize out-of-view objects, researchers have predominantly explored visual guidance approaches to visualize information in the limited (in-view) screen space. Unfortunately, visual conflicts like cluttering or occlusion of information often arise, which can lead to search performance issues and a decreased awareness about the physical environment. In this paper, we compare an innovative non-visual guidance approach based on audio-tactile cues with the state-of-the-art visual guidance technique EyeSee360 for localizing out-of-view objects in augmented reality displays with limited field of view. In our user study, we evaluate both guidance methods in terms of search performance and situation awareness. We show that although audio-tactile guidance is generally slower than the well-performing EyeSee360 in terms of search times, it is on a par regarding the hit rate. Even more so, the audio-tactile method provides a significant improvement in situation awareness compared to the visual approach.},
  archive      = {J_TVCG},
  author       = {Alexander Marquardt and Christina Trepkowski and Tom David Eibich and Jens Maiero and Ernst Kruijff and Johannes Schöning},
  doi          = {10.1109/TVCG.2020.3023605},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3389-3401},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparing non-visual and visual guidance methods for narrow field of view augmented reality displays},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Message from the ISMAR 2020 science and technology program
chairs and TVCG guest editors. <em>TVCG</em>, <em>26</em>(12),
3387–3388. (<a href="https://doi.org/10.1109/TVCG.2020.3021812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG) , we are pleased to present the TVCG papers from the 19th IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2020), which had been originally planned to hold in Recife/Porto de Galinhas, Brazil. In order to preserve the safety and well-being of all participants under the global pandemic of COVID-19, ISMAR 2020 will be held as a virtual conference between November 9 and 13, 2020. ISMAR continues the over twenty year long tradition of IWAR, ISMR, and ISAR, and is undoubtedly the premier conference for Mixed and Augmented Reality in the world.},
  archive      = {J_TVCG},
  author       = {Shi-Min Hu and Denis Kalkofen and Jonathan Ventura and Stefanie Zollmann},
  doi          = {10.1109/TVCG.2020.3021812},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3387-3388},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the ISMAR 2020 science and technology program chairs and TVCG guest editors},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Message from the editor-in-chief and from the associate
editor-in-chief. <em>TVCG</em>, <em>26</em>(12), 3386. (<a
href="https://doi.org/10.1109/TVCG.2020.3021811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the December 2020 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG) . This issue contains selected papers accepted at the IEEE International Symposium on Mixed and Augmented Reality (ISMAR). The conference was scheduled to take place in Recife, Porto de Galinhas (Brazil) but was moved to a virtual event due to the global coronavirus pandemic. This virtual conference took place from November 9-13, 2020.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller and Doug Bowman},
  doi          = {10.1109/TVCG.2020.3021811},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {3386},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief and from the associate editor-in-chief},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural style transfer: A review. <em>TVCG</em>,
<em>26</em>(11), 3365–3385. (<a
href="https://doi.org/10.1109/TVCG.2019.2921336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at: https://osf.io/f8tu4/.},
  archive      = {J_TVCG},
  author       = {Yongcheng Jing and Yezhou Yang and Zunlei Feng and Jingwen Ye and Yizhou Yu and Mingli Song},
  doi          = {10.1109/TVCG.2019.2921336},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3365-3385},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural style transfer: A review},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visualization and outlier detection for multivariate elastic
curve data. <em>TVCG</em>, <em>26</em>(11), 3353–3364. (<a
href="https://doi.org/10.1109/TVCG.2019.2921541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for the construction and visualization of geometrically-motivated boxplot displays for elastic curve data. We use a recent shape analysis framework, based on the square-root velocity function representation of curves, to extract different sources of variability from elastic curves, which include location, scale, shape, orientation and parametrization. We then focus on constructing separate displays for these various components using the Riemannian geometry of their representation spaces. This involves computation of a median, two quartiles, and two extremes based on geometric considerations. The outlyingness of an elastic curve is also defined separately based on each of the five components. We evaluate the proposed methods using multiple simulations, and then focus our attention on real data applications. In particular, we study variability in (a) 3D spirals, (b) handwritten signatures, (c) 3D fibers from diffusion tensor magnetic resonance imaging, and (d) trajectories of the Lorenz system.},
  archive      = {J_TVCG},
  author       = {Weiyi Xie and Oksana Chkrebtii and Sebastian Kurtek},
  doi          = {10.1109/TVCG.2019.2921541},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3353-3364},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization and outlier detection for multivariate elastic curve data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual genealogy of deep neural networks. <em>TVCG</em>,
<em>26</em>(11), 3340–3352. (<a
href="https://doi.org/10.1109/TVCG.2019.2921323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A comprehensive and comprehensible summary of existing deep neural networks (DNNs) helps practitioners understand the behaviour and evolution of DNNs, offers insights for architecture optimization, and sheds light on the working mechanisms of DNNs. However, this summary is hard to obtain because of the complexity and diversity of DNN architectures. To address this issue, we develop DNN Genealogy, an interactive visualization tool, to offer a visual summary of representative DNNs and their evolutionary relationships. DNN Genealogy enables users to learn DNNs from multiple aspects, including architecture, performance, and evolutionary relationships. Central to this tool is a systematic analysis and visualization of 66 representative DNNs based on our analysis of 140 papers. A directed acyclic graph is used to illustrate the evolutionary relationships among these DNNs and highlight the representative DNNs. A focus + context visualization is developed to orient users during their exploration. A set of network glyphs is used in the graph to facilitate the understanding and comparing of DNNs in the context of the evolution. Case studies demonstrate that DNN Genealogy provides helpful guidance in understanding, applying, and optimizing DNNs. DNN Genealogy is extensible and will continue to be updated to reflect future advances in DNNs.},
  archive      = {J_TVCG},
  author       = {Qianwen Wang and Jun Yuan and Shuxin Chen and Hang Su and Huamin Qu and Shixia Liu},
  doi          = {10.1109/TVCG.2019.2921323},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3340-3352},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual genealogy of deep neural networks},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surface registration with eigenvalues and eigenvectors.
<em>TVCG</em>, <em>26</em>(11), 3327–3339. (<a
href="https://doi.org/10.1109/TVCG.2019.2915567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel surface registration technique using the spectrum of the shapes, which can facilitate accurate localization and visualization of non-isometric deformations of the surfaces. In order to register two surfaces, we map both eigenvalues and eigenvectors of the Laplace-Beltrami of the shapes through optimizing an energy function. The function is defined by the integration of a smoothness term to align the eigenvalues and a distance term between the eigenvectors at feature points to align the eigenvectors. The feature points are generated using the static points of certain eigenvectors of the surfaces. By using both the eigenvalues and the eigenvectors on these feature points, the computational efficiency is improved considerably without losing the accuracy in comparison to the approaches that use the eigenvectors for all vertices. In our technique, the variation of the shape is expressed using a scale function defined at each vertex. Consequently, the total energy function to align the two given surfaces can be defined using the linear interpolation of the scale function derivatives. Through the optimization of the energy function, the scale function can be solved and the alignment is achieved. After the alignment, the eigenvectors can be employed to calculate the point-to-point correspondence of the surfaces. Therefore, the proposed method can accurately define the displacement of the vertices. We evaluate our method by conducting experiments on synthetic and real data using hippocampus, heart, and hand models. We also compare our method with non-rigid Iterative Closest Point (ICP) and a similar spectrum-based methods. These experiments demonstrate the advantages and accuracy of our method.},
  archive      = {J_TVCG},
  author       = {Hajar Hamidian and Zichun Zhong and Farshad Fotouhi and Jing Hua},
  doi          = {10.1109/TVCG.2019.2915567},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3327-3339},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Surface registration with eigenvalues and eigenvectors},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time exploration of large spatiotemporal datasets based
on order statistics. <em>TVCG</em>, <em>26</em>(11), 3314–3326. (<a
href="https://doi.org/10.1109/TVCG.2019.2914446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years sophisticated data structures based on datacubes have been proposed to perform interactive visual exploration of large datasets. While powerful, these approaches overlook the important fact that aggregations used to produce datacubes do not represent the actual distribution of the data being analyzed. As a result, these methods might produce biased results as well as hide important features in the data. In this paper, we introduce the Quantile Datacube Structure (QDS) that bridges this gap by supporting interactive visual exploration based on order statistics. To achieve this, QDS makes use of an efficient non-parametric distribution approximation scheme called p-digest and employs a novel datacube indexing scheme that reduces the memory usage of previous datacube methods. This enables interactive slicing and dicing while accurately approximating the distribution of quantitative variables of interest. We present two case studies that illustrate the ability of QDS to not only build order statistics based visualizations interactively but also to perform event detection on very large datasets. Finally, we present extensive experimental results that validate the effectiveness of QDS regarding memory usage and accuracy in the approximation of order statistics for real-world datasets.},
  archive      = {J_TVCG},
  author       = {Cícero A. L. Pahins and Nivan Ferreira and João L. Comba},
  doi          = {10.1109/TVCG.2019.2914446},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3314-3326},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time exploration of large spatiotemporal datasets based on order statistics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ray-based exploration of large time-varying volume data
using per-ray proxy distributions. <em>TVCG</em>, <em>26</em>(11),
3299–3313. (<a href="https://doi.org/10.1109/TVCG.2019.2920130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis and visualization of data created from simulations on modern supercomputers is a daunting challenge because the incredible compute power of modern supercomputers allow scientists to generate datasets with very high spatial and temporal resolutions. The limited bandwidth and capacity of networking and storage devices connecting supercomputers to analysis machines become the major bottleneck for data analysis such that simply moving the whole dataset from the supercomputer to a data analysis machine is infeasible. A common approach to visualize high temporal resolution simulation datasets under constrained I/O is to reduce the sampling rate in the temporal domain while preserving the original spatial resolution at the time steps. Data interpolation between the sampled time steps alone may not be a viable option since it may suffer from large errors, especially when using a lower sampling rate. We present a novel ray-based representation storing ray based histograms and depth information that recovers the evolution of volume data between sampled time steps. Our view-dependent proxy allows for a good trade off between compactly representing the time-varying data and leveraging temporal coherence within the data by utilizing interpolation between time steps, ray histograms, depth information, and codebooks. Our approach is able to provide fast rendering in the context of transfer function exploration to support visualization of feature evolution in time-varying data.},
  archive      = {J_TVCG},
  author       = {Ko-Chih Wang and Tzu-Hsuan Wei and Naeem Shareef and Han-Wei Shen},
  doi          = {10.1109/TVCG.2019.2920130},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3299-3313},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Ray-based exploration of large time-varying volume data using per-ray proxy distributions},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RainBio: Proportional visualization of large sets in
biology. <em>TVCG</em>, <em>26</em>(11), 3285–3298. (<a
href="https://doi.org/10.1109/TVCG.2019.2921544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set visualization is a well-known task in information visualization. In biology, it is used for comparing visually sets of genes or proteins, typically using Venn diagrams. However, limitations of the Venn diagram are well-known: they are limited to 6 sets and difficult to read above 4. Many other set visualization techniques have been proposed, but they have never been widely used in biology. In this paper, we introduce RainBio, a technique for visualizing sets in biology and aimed at providing a global overview showing the size of the main intersections, in a proportional way, and the similarities between sets. We adapt rainbow boxes, a technique for visualizing small datasets, to the visualization of larger sets, using element aggregation and intersection clustering. We present the application of RainBio to three datasets, with 5, 6 and 12 sets. We also describe a small user study comparing RainBio with Venn diagrams, involving 30 students in biology. Results showed that RainBio led to significantly fewer errors on 6-set dataset, and that the majority of students preferred RainBio. RainBio is proposed as a web-based tool for up to 15 sets.},
  archive      = {J_TVCG},
  author       = {Jean-Baptiste Lamy and Rosy Tsopra},
  doi          = {10.1109/TVCG.2019.2921544},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3285-3298},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RainBio: Proportional visualization of large sets in biology},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-window 3D interaction for collaborative virtual
reality. <em>TVCG</em>, <em>26</em>(11), 3271–3284. (<a
href="https://doi.org/10.1109/TVCG.2019.2914677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel collaborative virtual reality system that offers multiple immersive 3D views at large 3D scenes. The physical setup consists of two synchronized multi-user 3D displays: a tabletop and a large vertical projection screen. These displays afford different presentations of the shared 3D scene. The wall display lends itself to the egocentric exploration at 1:1 scale, while the tabletop affords an allocentric overview. Additionally, handheld 3D portals facilitate the personal exploration of the scene, the comparison of views, and the exchange with others. Our developments enable seamless 3D interaction across these independent 3D views. This requires the simultaneous representation of user input in the different viewing contexts. However, the resulting interactions cannot be executed independently. The application must coordinate the interactions and resolve potential ambiguities to provide plausible effects. We analyze and document the challenges of seamless 3D interaction across multiple independent viewing windows, propose a high-level software design to realize the necessary functionality, and apply the design to a set of interaction tools. Our setup was tested in a formal user study, which revealed general advantages of collaborative 3D data exploration with multiple views in terms of user preference, comfort, and task performance.},
  archive      = {J_TVCG},
  author       = {Andre Kunert and Tim Weissker and Bernd Froehlich and Alexander Kulik},
  doi          = {10.1109/TVCG.2019.2914677},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3271-3284},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-window 3D interaction for collaborative virtual reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-patch collaborative point cloud denoising via low-rank
recovery with graph constraint. <em>TVCG</em>, <em>26</em>(11),
3255–3270. (<a href="https://doi.org/10.1109/TVCG.2019.2920817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is the primary source from 3D scanners and depth cameras. It usually contains more raw geometric features, as well as higher levels of noise than the reconstructed mesh. Although many mesh denoising methods have proven to be effective in noise removal, they hardly work well on noisy point clouds. We propose a new multi-patch collaborative method for point cloud denoising, which is solved as a low-rank matrix recovery problem. Unlike the traditional single-patch based denoising approaches, our approach is inspired by the geometric statistics which indicate that a number of surface patches sharing approximate geometric properties always exist within a 3D model. Based on this observation, we define a rotation-invariant height-map patch (HMP) for each point by robust Bi-PCA encoding bilaterally filtered normal information, and group its non-local similar patches together. Within each group, all patches are geometrically similar, while suffering from noise. We pack the height maps of each group into an HMP matrix, whose initial rank is high, but can be significantly reduced. We design an improved low-rank recovery model, by imposing a graph constraint to filter noise. Experiments on synthetic and raw datasets demonstrate that our method outperforms state-of-the-art methods in both noise removal and feature preservation.},
  archive      = {J_TVCG},
  author       = {Honghua Chen and Mingqiang Wei and Yangxing Sun and Xingyu Xie and Jun Wang},
  doi          = {10.1109/TVCG.2019.2920817},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3255-3270},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-patch collaborative point cloud denoising via low-rank recovery with graph constraint},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inviwo — a visualization system with usage abstraction
levels. <em>TVCG</em>, <em>26</em>(11), 3241–3254. (<a
href="https://doi.org/10.1109/TVCG.2019.2920639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of today&#39;s visualization applications demands specific visualization systems tailored for the development of these applications. Frequently, such systems utilize levels of abstraction to improve the application development process, for instance by providing a data flow network editor. Unfortunately, these abstractions result in several issues, which need to be circumvented through an abstraction-centered system design. Often, a high level of abstraction hides low level details, which makes it difficult to directly access the underlying computing platform, which would be important to achieve an optimal performance. Therefore, we propose a layer structure developed for modern and sustainable visualization systems allowing developers to interact with all contained abstraction levels. We refer to this interaction capabilities as usage abstraction levels, since we target application developers with various levels of experience. We formulate the requirements for such a system, derive the desired architecture, and present how the concepts have been exemplary realized within the Inviwo visualization system. Furthermore, we address several specific challenges that arise during the realization of such a layered architecture, such as communication between different computing platforms, performance centered encapsulation, as well as layer-independent development by supporting cross layer documentation and debugging capabilities.},
  archive      = {J_TVCG},
  author       = {Daniel Jönsson and Peter Steneteg and Erik Sundén and Rickard Englund and Sathish Kottravel and Martin Falk and Anders Ynnerman and Ingrid Hotz and Timo Ropinski},
  doi          = {10.1109/TVCG.2019.2920639},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3241-3254},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Inviwo — a visualization system with usage abstraction levels},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impact of different sensory stimuli on presence in credible
virtual environments. <em>TVCG</em>, <em>26</em>(11), 3231–3240. (<a
href="https://doi.org/10.1109/TVCG.2019.2926978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple factors can affect presence in virtual environments, such as the number of human senses engaged in a given experience or the extent to which the virtual experience is credible. The purpose of the present work is to study how the inclusion of credible multisensory stimuli affects the sense of presence, namely, through the use of wind, passive haptics, vibration, and scent. Our sample consisted of 37 participants (27 men and 10 women) whose ages ranged from 17 to 44 years old and were mostly students. The participants were divided randomly into 3 groups: Control Scenario (visual and auditory - N = 12), Passive Haptic Scenario (visual, auditory, and passive haptic - N = 13) and Multisensory Scenario (visual, auditory, wind, passive haptic, vibration, and scent - N = 12). The results indicated a significant increase in the involvement subscale when all multisensory stimuli were delivered. We found a trend where the use of passive haptics by itself has a positive impact on presence, which should be the subject of further work.},
  archive      = {J_TVCG},
  author       = {Guilherme Gonçalves and Miguel Melo and José Vasconcelos-Raposo and Maximino Bessa},
  doi          = {10.1109/TVCG.2019.2926978},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3231-3240},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impact of different sensory stimuli on presence in credible virtual environments},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HeteroFusion: Dense scene reconstruction integrating
multi-sensors. <em>TVCG</em>, <em>26</em>(11), 3217–3230. (<a
href="https://doi.org/10.1109/TVCG.2019.2919619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach to integrate data from multiple sensor types for dense 3D reconstruction of indoor scenes in realtime. Existing algorithms are mainly based on a single RGBD camera and thus require continuous scanning of areas with sufficient geometric features. Otherwise, tracking may fail due to unreliable frame registration. Inspired by the fact that the fusion of multiple sensors can combine their strengths towards a more robust and accurate self-localization, we incorporate multiple types of sensors which are prevalent in modern robot systems, including a 2D range sensor, an inertial measurement unit (IMU), and wheel encoders. We fuse their measurements to reinforce the tracking process and to eventually obtain better 3D reconstructions. Specifically, we develop a 2D truncated signed distance field (TSDF) volume representation for the integration and ray-casting of laser frames, leading to a unified cost function in the pose estimation stage. For validation of the estimated poses in the loop-closure optimization process, we train a classifier for the features extracted from heterogeneous sensors during the registration progress. To evaluate our method on challenging use case scenarios, we assembled a scanning platform prototype to acquire real-world scans. We further simulated synthetic scans based on high-fidelity synthetic scenes for quantitative evaluation. Extensive experimental evaluation on these two types of scans demonstrate that our system is capable of robustly acquiring dense 3D reconstructions and outperforms state-of-the-art RGBD and LiDAR systems.},
  archive      = {J_TVCG},
  author       = {Sheng Yang and Beichen Li and Minghua Liu and Yu-Kun Lai and Leif Kobbelt and Shi-Min Hu},
  doi          = {10.1109/TVCG.2019.2919619},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3217-3230},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HeteroFusion: Dense scene reconstruction integrating multi-sensors},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Fourier opacity optimization for scalable exploration.
<em>TVCG</em>, <em>26</em>(11), 3204–3216. (<a
href="https://doi.org/10.1109/TVCG.2019.2915222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, scientific visualization became a fundamental aspect of modern scientific data analysis. Across all data-intensive research fields, ranging from structural biology to cosmology, data sizes increase rapidly. Dealing with the growing large-scale data is one of the top research challenges of this century. For the visual exploratory data analysis, interactivity, a view-dependent visibility optimization and frame coherence are indispensable. In this work, we extend the recent decoupled opacity optimization framework to enable a navigation without occlusion of important features through large geometric data. By expressing the accumulation of importance and optical depth in Fourier basis, the computation, evaluation and rendering of optimized transparent geometry become not only order-independent, but also operate within a fixed memory bound. We study the quality of our Fourier approximation in terms of accuracy, memory requirements and efficiency for both the opacity computation, as well as the order-independent compositing. We apply the method to different point, line and surface data sets originating from various research fields, including meteorology, health science, astrophysics and organic chemistry.},
  archive      = {J_TVCG},
  author       = {Irene Baeza Rojo and Markus Gross and Tobias Günther},
  doi          = {10.1109/TVCG.2019.2915222},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3204-3216},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fourier opacity optimization for scalable exploration},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embedding meta information into visualizations.
<em>TVCG</em>, <em>26</em>(11), 3189–3203. (<a
href="https://doi.org/10.1109/TVCG.2019.2916098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study how to co-locate meta information with visualizations by directly embedding information into visualizations. This allows for visualizations to carry provenance and authorship information themselves for reproducibility. We call these self-describing visualizations-reproducible, authenticatable, and documentable. Self-describing visualizations can be used to extend existing visualization provenance systems. Herein, we start with a survey of existing digital image watermarking literature. We search for and classify watermarking algorithms that can support scientific visualizations. Using our payload-resilience testing framework, we evaluate and recommend algorithms supporting various use cases in the payload-resiliency space, and present guidelines for optimizing visualizations to improve payload capacities and embedding robustness. We demonstrate the efficacy of self-describing visualizations with two sample application implementations: (1) adding an embedding filter as a part the standard rendering pipeline, (2) creating a web reader to automatically and reliably extract provenance information from scientific publications for review and dissemination.},
  archive      = {J_TVCG},
  author       = {Alok Hota and Jian Huang},
  doi          = {10.1109/TVCG.2019.2916098},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3189-3203},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Embedding meta information into visualizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient representation of geometric tree models with
level-of-detail using compressed 3D chain code. <em>TVCG</em>,
<em>26</em>(11), 3177–3188. (<a
href="https://doi.org/10.1109/TVCG.2019.2924430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we present a method for space-efficient representation of geometric tree models, which are provided as skeletons with radii attached to individual branch segments. The proposed approach uses a new differential 3D chain code to encode orientation changes of consecutive branch segments, which allows optimizing chain code generation for increased compressibility while maintaining control over the model reconstruction error. The presented method is the first to encode the complete branching geometry including the branch radii and provides level-of-detail construction directly from the chain code. It is demonstrated that by using interpolative encoding of the resulting tree descriptors and radii sequences, the storage requirements for geometric description of a mixed all-aged forest can be reduced to less than 15 percent of its raw size while preserving the structural fidelity of tree models.},
  archive      = {J_TVCG},
  author       = {Damjan Strnad and Štefan Kohek and Andrej Nerat and Borut Žalik},
  doi          = {10.1109/TVCG.2019.2924430},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3177-3188},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient representation of geometric tree models with level-of-detail using compressed 3D chain code},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective video stabilization via joint trajectory smoothing
and frame warping. <em>TVCG</em>, <em>26</em>(11), 3163–3176. (<a
href="https://doi.org/10.1109/TVCG.2019.2923196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video stabilization is usually composed of three stages: feature trajectory extraction, trajectory smoothing, and frame warping. Most previous approaches view them as three separate stages. This paper proposes a method combining the last two stages, namely the trajectory smoothing and frame warping stages, into a single optimization framework. The novelty exists in the way of how we combine them: the trajectory smoothing part plays a major role while the frame warping part plays an auxiliary role. With this kind of design, we can conveniently increase the strength of the trajectory smoothing part by a robust first-order derivative term, which makes it possible to produce very aggressive stabilization effects. On the other hand, we adopt adaptive weighting mechanisms in the frame warping part, to follow the smoothed trajectories as much as possible while regularizing other places as similar as possible. Our method is robust to utilize both foreground and background features, and very short trajectories. The utilization of all these information in turn increases the accuracy of the proposed method. We also provide a simplified implementation of our method, which is less accurate but more efficient. Experiments on various kinds of videos demonstrate the effectiveness of our method.},
  archive      = {J_TVCG},
  author       = {Tiezheng Ma and Yongwei Nie and Qing Zhang and Zhensong Zhang and Hanqiu Sun and Guiqing Li},
  doi          = {10.1109/TVCG.2019.2923196},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3163-3176},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effective video stabilization via joint trajectory smoothing and frame warping},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection and visualization of splat and antisplat events in
turbulent flows. <em>TVCG</em>, <em>26</em>(11), 3147–3162. (<a
href="https://doi.org/10.1109/TVCG.2019.2920157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Splat and antisplat events are a widely found phenomenon in three-dimensional turbulent flow fields. Splats are observed when fluid locally impinges on an impermeable surface transferring energy from the normal component to the tangential velocity components, while antisplats relate to the inverted situation. These events affect a variety of flow properties, such as the transfer of kinetic energy between velocity components and the transfer of heat, so that their investigation can provide new insight into these issues. Here, we propose the first Lagrangian method for the detection of splats and antisplats as features of an unsteady flow field. Our method utilizes the concept of strain tensors on flow-embedded flat surfaces to extract disjoint regions in which splat and antisplat events of arbitrary scale occur. We validate the method with artificial flow fields of increasing complexity. Subsequently, the method is used to analyze application data stemming from a direct numerical simulation of the turbulent flow over a backward facing step. Our results show that splat and antisplat events can be identified efficiently and reliably even in such a complex situation, demonstrating that the new method constitutes a well-suited tool for the analysis of turbulent flows.},
  archive      = {J_TVCG},
  author       = {Baldwin Nsonga and Martin Niemann and Jochen Fröhlich and Joachim Staib and Stefan Gumhold and Gerik Scheuermann},
  doi          = {10.1109/TVCG.2019.2920157},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3147-3162},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Detection and visualization of splat and antisplat events in turbulent flows},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A visual analytics system for exploring, monitoring, and
forecasting road traffic congestion. <em>TVCG</em>, <em>26</em>(11),
3133–3146. (<a href="https://doi.org/10.1109/TVCG.2019.2922597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an interactive visual analytics system that enables traffic congestion exploration, surveillance, and forecasting based on vehicle detector data. Through domain expert collaboration, we have extracted task requirements, incorporated the Long Short-Term Memory (LSTM) model for congestion forecasting, and designed a weighting method for detecting the causes of congestion and congestion propagation directions. Our visual analytics system is designed to enable users to explore congestion causes, directions, and severity. Congestion conditions of a city are visualized using a Volume-Speed Rivers (VSRivers) visualization that simultaneously presents traffic volumes and speeds. To evaluate our system, we report performance comparison results, wherein our model is more accurate than other forecasting algorithms. We demonstrate the usefulness of our system in the traffic management and congestion broadcasting domains through three case studies and domain expert feedback.},
  archive      = {J_TVCG},
  author       = {Chunggi Lee and Yeonjun Kim and Seungmin Jin and Dongmin Kim and Ross Maciejewski and David Ebert and Sungahn Ko},
  doi          = {10.1109/TVCG.2019.2922597},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {3133-3146},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual analytics system for exploring, monitoring, and forecasting road traffic congestion},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of a relative visual performance model in a
virtual reality immersive system. <em>TVCG</em>, <em>26</em>(10),
3128–3132. (<a href="https://doi.org/10.1109/TVCG.2019.2909881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As part of an evaluation process of user experience realism in a Virtual Reality (VR) system, we focus in this paper on one of the core characteristics of vision: the relationship between contrast and luminance. The experiment aims at validating in VR reaction time predictions given by Rea and Ouellette&#39;s model. The subjects have to distinguish, as fast as they can, a target object from an uniform background. Our results did not match the predictions of the model. Our subjects showed higher performance in performing the task than expected. At low level of contrast, our subjects could easily perceive a target they should not have been able to see at all. This is explained by the size of the visual field surrounding the target: at low level of visibility, the larger the surrounding, the easier perception the is. We conclude that the Rea and Ouellette&#39;s model could be applied in VR if a specific visual field size factor was added.},
  archive      = {J_TVCG},
  author       = {Benoit Perroud and Stéphane Régnier and Andras Kemeny and Frédéric Mérienne},
  doi          = {10.1109/TVCG.2019.2909881},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3128-3132},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Application of a relative visual performance model in a virtual reality immersive system},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A systematic review of visualization in building information
modeling. <em>TVCG</em>, <em>26</em>(10), 3109–3127. (<a
href="https://doi.org/10.1109/TVCG.2019.2907583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building Information Modeling (BIM) employs data-rich 3D CAD models for large-scale facility design, construction, and operation. These complex datasets contain a large amount and variety of information, ranging from design specifications to real-time sensor data. They are used by architects and engineers for various analysis and simulations throughout a facility&#39;s life cycle. Many techniques from different visualization fields could be used to analyze these data. However, the BIM domain still remains largely unexplored by the visualization community. The goal of this article is to encourage visualization researchers to increase their involvement with BIM. To this end, we present the results of a systematic review of visualization in current BIM practice. We use a novel taxonomy to identify main application areas and analyze commonly employed techniques. From this domain characterization, we highlight future research opportunities brought forth by the unique features of BIM. For instance, exploring the synergies between scientific and information visualization to integrate spatial and non-spatial data. We hope this article raises awareness to interesting new challenges the BIM domain brings to the visualization community.},
  archive      = {J_TVCG},
  author       = {Paulo Ivson and André Moreira and Francisco Queiroz and Wallas Santos and Waldemar Celes},
  doi          = {10.1109/TVCG.2019.2907583},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3109-3127},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A systematic review of visualization in building information modeling},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey of full-body motion reconstruction in immersive
virtual reality applications. <em>TVCG</em>, <em>26</em>(10), 3089–3108.
(<a href="https://doi.org/10.1109/TVCG.2019.2912607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to recent advances in virtual reality (VR) technology, the development of immersive VR applications that track body motions and visualize a full-body avatar is attracting increasing research interest. This paper reviews related research to gather and to critically analyze recent improvements regarding the potential of full-body motion reconstruction in VR applications. We conducted a systematic literature search, matching VR and full-body tracking related keywords on IEEE Xplore, PubMed, ACM, and Scopus. Fifty-three publications were included and assigned in three groups: studies using markerless and marker-based motion tracking systems as well as systems using inertial measurement units. All analyzed research publications track the motions of the user wearing a head-mounted display and visualize a full-body avatar. The analysis confirmed that a full-body avatar can enhance the sense of embodiment and can improve the immersion within the VR. The results indicated that the Kinect device is still the most frequently used sensor (27 out of 53). Furthermore, there is a trend to track the movements of multiple users simultaneously. Many studies that enable multiplayer mode in VR use marker-based systems (7 out of 17) because they are much more robust and can accurately track full-body movements of multiple users in real-time.},
  archive      = {J_TVCG},
  author       = {Polona Caserman and Augusto Garcia-Agundez and Stefan Göbel},
  doi          = {10.1109/TVCG.2019.2912607},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3089-3108},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey of full-body motion reconstruction in immersive virtual reality applications},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wrinkles, folds, creases, buckles: Small-scale surface
deformations as periodic functions on 3D meshes. <em>TVCG</em>,
<em>26</em>(10), 3077–3088. (<a
href="https://doi.org/10.1109/TVCG.2019.2914676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for adding small-scale details to surfaces of 3D geometries in the context of interactive deformation computation of elastic objects. This is relevant in real-time applications, for instance, in surgical simulation or interactive animation. The key idea is the procedural generation of surface details via a weighted sum of periodic functions, applied as an on-surface displacement field. We first calculate local deformation strains of a low-resolution 3D input mesh, which are then employed to estimate amplitudes, orientations, and positions of high-resolution details. The shapes and spatial frequencies of the periodic details are obtained from mechanical parameters, assuming the physical model of a film-substrate aggregate. Finally, our approach creates the highly-detailed output mesh fully on the GPU. The performance is independent of the spatial frequency of the inserted details as well as, within certain limits, of the resolution of the output mesh. We can reproduce numerous commonly observed, characteristic surface deformation patterns, such as wrinkles or buckles, allowing for the representation of a wide variety of simulated materials and interaction processes. We highlight the performance of our method with several examples.},
  archive      = {J_TVCG},
  author       = {Evgeny Zuenko and Matthias Harders},
  doi          = {10.1109/TVCG.2019.2914676},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3077-3088},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Wrinkles, folds, creases, buckles: Small-scale surface deformations as periodic functions on 3D meshes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VeCHArt: Visually enhanced comparison of historic art using
an automated line-based synchronization technique. <em>TVCG</em>,
<em>26</em>(10), 3063–3076. (<a
href="https://doi.org/10.1109/TVCG.2019.2908166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of subtle deviations between different versions of historical prints has been a long-standing challenge in art history research. So far, this challenge has required extensive domain knowledge, fine-tuned expert perception, and time-consuming manual labor. In this paper we introduce an explorative visual approach to facilitate fast and accurate support for the task of comparing differences between prints such as engravings and woodcuts. To this end, we have developed a customized algorithm that detects similar stroke-patterns in prints and matches them in order to allow visual alignment and automated deviation highlighting. Our visual analytics system enables art history researchers to quickly detect, document, and categorize qualitative and quantitative discrepancies, and to analyze these discrepancies using comprehensive interactions. To evaluate our approach, we conducted a user study involving both experts on historical prints and laypeople. Using our new interactive technique, our subjects found about 20 percent more differences compared to regular image viewing software as well as “paper-based” comparison. Moreover, the laypeople found the same differences as the experts when they used our system, which was not the case for conventional methods. Informal feedback showed that both laypeople and experts strongly preferred employing our system to working with conventional methods.},
  archive      = {J_TVCG},
  author       = {Hermann Pflüger and Dennis Thom and Anna Schütz and Daniela Bohde and Thomas Ertl},
  doi          = {10.1109/TVCG.2019.2908166},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3063-3076},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VeCHArt: Visually enhanced comparison of historic art using an automated line-based synchronization technique},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The curse of knowledge in visual data communication.
<em>TVCG</em>, <em>26</em>(10), 3051–3062. (<a
href="https://doi.org/10.1109/TVCG.2019.2917689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A viewer can extract many potential patterns from any set of visualized data values. But that means that two people can see different patterns in the same visualization, potentially leading to miscommunication. Here, we show that when people are primed to see one pattern in the data as visually salient, they believe that naïve viewers will experience the same visual salience. Participants were told one of multiple backstories about political events that affected public polling data, before viewing a graph that depicted those data. One pattern in the data was particularly visually salient to them given the backstory that they heard. They then predicted what naïve viewers would most visually salient on the visualization. They were strongly influenced by their own knowledge, despite explicit instructions to ignore it, predicting that others would find the same patterns to be most visually salient. This result reflects a psychological phenomenon known as the curse of knowledge, where an expert struggles to re-create the state of mind of a novice. The present findings show that the curse of knowledge also plagues the visual perception of data, explaining why people can fail to connect with audiences when they communicate patterns in data.},
  archive      = {J_TVCG},
  author       = {Cindy Xiong and Lisanne Van Weelden and Steven Franconeri},
  doi          = {10.1109/TVCG.2019.2917689},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3051-3062},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The curse of knowledge in visual data communication},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strong 3D printing by TPMS injection. <em>TVCG</em>,
<em>26</em>(10), 3037–3050. (<a
href="https://doi.org/10.1109/TVCG.2019.2914044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D printed objects are rapidly becoming prevalent in science, technology and daily life. An important question is how to obtain strong and durable 3D models using standard printing techniques. This question is often translated to computing smartly designed interior structures that provide strong support and yield resistant 3D models. In this paper we suggest a combination between 3D printing and material injection to achieve strong 3D printed objects. We utilize triply periodic minimal surfaces (TPMS) to define novel interior support structures. TPMS are closed form and can be computed in a simple and straightforward manner. Since TPMS are smooth and connected, we utilize them to define channels that adequately distribute injected materials in the shape interior. To account for weak regions, TPMS channels are locally optimized according to the shape stress field. After the object is printed, we simply inject the TPMS channels with materials that solidify and yield a strong inner structure that supports the shape. Our method allows injecting a wide range of materials in an object interior in a fast and easy manner. Results demonstrate the efficiency of strong printing by combining 3D printing and injection together.},
  archive      = {J_TVCG},
  author       = {Xin Yan and Cong Rao and Lin Lu and Andrei Sharf and Haisen Zhao and Baoquan Chen},
  doi          = {10.1109/TVCG.2019.2914044},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3037-3050},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Strong 3D printing by TPMS injection},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral analysis of quadrature rules and fourier
truncation-based methods applied to shading integrals. <em>TVCG</em>,
<em>26</em>(10), 3022–3036. (<a
href="https://doi.org/10.1109/TVCG.2019.2913418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a theoretical framework, based on the theory of Sobolev spaces, that allows for a comprehensive analysis of quadrature rules for integration over the sphere. We apply this framework to the case of shading integrals in order to predict and analyze the performances of quadrature methods. We show that the spectral distribution of the quadrature error depends not only on the samples set size, distribution and weights, but also on the BRDF and the integrand smoothness. The proposed spectral analysis of quadrature error allows for a better understanding of how the above different factors interact. We also extend our analysis to the case of Fourier truncation-based techniques applied to the shading integral, so as to find the smallest spherical/hemispherical harmonics degree L (truncation) that entails a targeted integration error. This application is very beneficial to global illumination methods such as Precomputed Radiance Transfer and Radiance Caching. Finally, our proposed framework is the first to allow a direct theoretical comparison between quadrature- and truncation-based methods applied to the shading integral. This enables, for example, to determine the spherical harmonics degree L which corresponds to a quadrature-based integration with N samples. Our theoretical findings are validated by a set of rendering experiments.},
  archive      = {J_TVCG},
  author       = {Ricardo Marques and Christian Bouville and Kadi Bouatouch},
  doi          = {10.1109/TVCG.2019.2913418},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3022-3036},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spectral analysis of quadrature rules and fourier truncation-based methods applied to shading integrals},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive visualization and on-demand processing of large
volume data: A fully GPU-based out-of-core approach. <em>TVCG</em>,
<em>26</em>(10), 3008–3021. (<a
href="https://doi.org/10.1109/TVCG.2019.2912752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a wide range of scientific fields, 3D datasets production capabilities have widely evolved in recent years, especially with the rapid increase in their sizes. As a result, many large-scale applications, including visualization or processing, have become challenging to address. A solution to this issue lies in providing out-of-core algorithms specifically designed to handle datasets significantly larger than memory. In this article, we present a new approach that extends the broad interactive addressing principles already established in the field of out-of-core volume rendering on GPUs to allow on-demand processing during the visualization stage. We propose a pipeline designed to manage data as regular 3D grids regardless of the underlying application. It relies on a caching approach with a virtual memory addressing system coupled to an efficient parallel management on GPU to provide efficient access to data in interactive time. It allows any visualization or processing application to leverage the flexibility of its structure by managing multi-modality datasets. Furthermore, we show that our system delivers good performance on a single standard PC with low memory budget on the GPU.},
  archive      = {J_TVCG},
  author       = {Jonathan Sarton and Nicolas Courilleau and Yannick Remion and Laurent Lucas},
  doi          = {10.1109/TVCG.2019.2912752},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3008-3021},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive visualization and on-demand processing of large volume data: A fully GPU-based out-of-core approach},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). InpaintFusion: Incremental RGB-d inpainting for 3D scenes.
<em>TVCG</em>, <em>26</em>(10), 2994–3007. (<a
href="https://doi.org/10.1109/TVCG.2020.3003768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art methods for diminished reality propagate pixel information from a keyframe to subsequent frames for real-time inpainting. However, these approaches produce artifacts, if the scene geometry is not sufficiently planar. In this article, we present InpaintFusion, a new real-time method that extends inpainting to non-planar scenes by considering both color and depth information in the inpainting process. We use an RGB-D sensor for simultaneous localization and mapping, in order to both track the camera and obtain a surfel map in addition to RGB images. We use the RGB-D information in a cost function for both the color and the geometric appearance to derive a global optimization for simultaneous inpainting of color and depth. The inpainted depth is merged in a global map by depth fusion. For the final rendering, we project the map model into image space, where we can use it for effects such as relighting and stereo rendering of otherwise hidden structures. We demonstrate the capabilities of our method by comparing it to inpainting results with methods using planar geometric proxies.},
  archive      = {J_TVCG},
  author       = {Shohei Mori and Okan Erat and Wolfgang Broll and Hideo Saito and Dieter Schmalstieg and Denis Kalkofen},
  doi          = {10.1109/TVCG.2020.3003768},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {2994-3007},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InpaintFusion: Incremental RGB-D inpainting for 3D scenes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implicit frictional boundary handling for SPH.
<em>TVCG</em>, <em>26</em>(10), 2982–2993. (<a
href="https://doi.org/10.1109/TVCG.2020.3004245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel method for the robust handling of static and dynamic rigid boundaries in Smoothed Particle Hydrodynamics (SPH) simulations. We build upon the ideas of the density maps approach which has been introduced recently by Koschier and Bender. They precompute the density contributions of solid boundaries and store them on a spatial grid which can be efficiently queried during runtime. This alleviates the problems of commonly used boundary particles, like bumpy surfaces and inaccurate pressure forces near boundaries. Our method is based on a similar concept but we precompute the volume contribution of the boundary geometry. This maintains all benefits of density maps but offers a variety of advantages which are demonstrated in several experiments. First, in contrast to the density maps method we can compute derivatives in the standard SPH manner by differentiating the kernel function. This results in smooth pressure forces, even for lower map resolutions, such that precomputation times and memory requirements are reduced by more than two orders of magnitude compared to density maps. Furthermore, this directly fits into the SPH concept so that volume maps can be seamlessly combined with existing SPH methods. Finally, the kernel function is not baked into the map such that the same volume map can be used with different kernels. This is especially useful when we want to incorporate common surface tension or viscosity methods that use different kernels than the fluid simulation.},
  archive      = {J_TVCG},
  author       = {Jan Bender and Tassilo Kugelstadt and Marcel Weiler and Dan Koschier},
  doi          = {10.1109/TVCG.2020.3004245},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {2982-2993},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Implicit frictional boundary handling for SPH},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Half-space power diagrams and discrete surface offsets.
<em>TVCG</em>, <em>26</em>(10), 2970–2981. (<a
href="https://doi.org/10.1109/TVCG.2019.2945961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an efficient, trivially parallelizable algorithm to compute offset surfaces of shapes discretized using a dexel data structure. Our algorithm is based on a two-stage sweeping procedure that is simple to implement and efficient, entirely avoiding volumetric distance field computations typical of existing methods. Our construction is based on properties of half-space power diagrams, where each seed is only visible by a half-space, which were never used before for the computation of surface offsets. The primary application of our method is interactive modeling for digital fabrication. Our technique enables a user to interactively process high-resolution models. It is also useful in a plethora of other geometry processing tasks requiring fast, approximate offsets, such as topology optimization, collision detection, and skeleton extraction. We present experimental timings, comparisons with previous approaches, and provide a reference implementation in the supplemental material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TVCG.2019.2945961 .},
  archive      = {J_TVCG},
  author       = {Zhen Chen and Daniele Panozzo and Jérémie Dumas},
  doi          = {10.1109/TVCG.2019.2945961},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {2970-2981},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Half-space power diagrams and discrete surface offsets},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast computation of single scattering in participating media
with refractive boundaries using frequency analysis. <em>TVCG</em>,
<em>26</em>(10), 2961–2969. (<a
href="https://doi.org/10.1109/TVCG.2019.2909875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many materials combine a refractive boundary and a participating media on the interior. If the material has a low opacity, single scattering effects dominate in its appearance. Refraction at the boundary concentrates the incoming light, resulting in an important phenomenon called volume caustics. This phenomenon is hard to simulate. Previous methods used point-based light transport, but attributed point samples inefficiently, resulting in long computation time. In this paper, we use frequency analysis of light transport to allocate point samples efficiently. Our method works in two steps: in the first step, we compute volume samples along with their covariance matrices, encoding the illumination frequency content in a compact way. In the rendering step, we use the covariance matrices to compute the kernel size for each volume sample: small kernel for high-frequency single scattering, large kernel for lower frequencies. Our algorithm computes volume caustics with fewer volume samples, with no loss of quality. Our method is both faster and uses less memory than the original method. It is roughly twice as fast and uses one fifth of the memory. The extra cost of computing covariance matrices for frequency information is negligible.},
  archive      = {J_TVCG},
  author       = {Yulin Liang and Beibei Wang and Lu Wang and Nicolas Holzschuch},
  doi          = {10.1109/TVCG.2019.2909875},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {2961-2969},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast computation of single scattering in participating media with refractive boundaries using frequency analysis},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Eiffel: Evolutionary flow map for influence graph
visualization. <em>TVCG</em>, <em>26</em>(10), 2944–2960. (<a
href="https://doi.org/10.1109/TVCG.2019.2906900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visualization of evolutionary influence graphs is important for performing many real-life tasks such as citation analysis and social influence analysis. The main challenges include how to summarize large-scale, complex, and time-evolving influence graphs, and how to design effective visual metaphors and dynamic representation methods to illustrate influence patterns over time. In this work, we present Eiffel, an integrated visual analytics system that applies triple summarizations on evolutionary influence graphs in the nodal, relational, and temporal dimensions. In numerical experiments, Eiffel summarization results outperformed those of traditional clustering algorithms with respect to the influence-flow-based objective. Moreover, a flow map representation is proposed and adapted to the case of influence graph summarization, which supports two modes of evolutionary visualization (i.e., flip-book and movie) to expedite the analysis of influence graph dynamics. We conducted two controlled user experiments to evaluate our technique on influence graph summarization and visualization respectively. We also showcased the system in the evolutionary influence analysis of two typical scenarios, the citation influence of scientific papers and the social influence of emerging online events. The evaluation results demonstrate the value of Eiffel in the visual analysis of evolutionary influence graphs.},
  archive      = {J_TVCG},
  author       = {Yucheng Huang and Lei Shi and Yue Su and Yifan Hu and Hanghang Tong and Chaoli Wang and Tong Yang and Deyun Wang and Shuo Liang},
  doi          = {10.1109/TVCG.2019.2906900},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {2944-2960},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eiffel: Evolutionary flow map for influence graph visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A superpixel-based variational model for image colorization.
<em>TVCG</em>, <em>26</em>(10), 2931–2943. (<a
href="https://doi.org/10.1109/TVCG.2019.2908363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image colorization refers to a computer-assisted process that adds colors to grayscale images. It is a challenging task since there is usually no one-to-one correspondence between color and local texture. In this paper, we tackle this issue by exploiting weighted nonlocal self-similarity and local consistency constraints at the resolution of superpixels. Given a grayscale target image, we first select a color source image containing similar segments to target image and extract multi-level features of each superpixel in both images after superpixel segmentation. Then a set of color candidates for each target superpixel is selected by adopting a top-down feature matching scheme with confidence assignment. Finally, we propose a variational approach to determine the most appropriate color for each target superpixel from color candidates. Experiments demonstrate the effectiveness of the proposed method and show its superiority to other state-of-the-art methods. Furthermore, our method can be easily extended to color transfer between two color images.},
  archive      = {J_TVCG},
  author       = {Faming Fang and Tingting Wang and Tieyong Zeng and Guixu Zhang},
  doi          = {10.1109/TVCG.2019.2908363},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {2931-2943},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A superpixel-based variational model for image colorization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VoxSegNet: Volumetric CNNs for semantic part segmentation of
3D shapes. <em>TVCG</em>, <em>26</em>(9), 2919–2930. (<a
href="https://doi.org/10.1109/TVCG.2019.2896310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volumetric representation has been widely used for 3D deep learning in shape analysis due to its generalization ability and regular data format. However, for fine-grained tasks like part segmentation, volumetric data has not been widely adopted compared to other representations. Aiming at delivering an effective volumetric method for 3D shape part segmentation, this paper proposes a novel volumetric convolutional neural network. Our method can extract discriminative features encoding detailed information from voxelized 3D data under limited resolution. To this purpose, a spatial dense extraction (SDE) module is designed to preserve spatial resolution during feature extraction procedure, alleviating the loss of details caused by sub-sampling operations such as max pooling. An attention feature aggregation (AFA) module is also introduced to adaptively select informative features from different abstraction levels, leading to segmentation with both semantic consistency and high accuracy of details. Experimental results demonstrate that promising results can be achieved by using volumetric data, with part segmentation accuracy comparable or superior to state-of-the-art non-volumetric methods.},
  archive      = {J_TVCG},
  author       = {Zongji Wang and Feng Lu},
  doi          = {10.1109/TVCG.2019.2896310},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2919-2930},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VoxSegNet: Volumetric CNNs for semantic part segmentation of 3D shapes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using microsaccades to estimate task difficulty during
visual search of layered surfaces. <em>TVCG</em>, <em>26</em>(9),
2904–2918. (<a href="https://doi.org/10.1109/TVCG.2019.2901881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an approach to using microsaccade dynamics for the measurement of task difficulty/cognitive load imposed by a visual search task of a layered surface. Previous studies provide converging evidence that task difficulty/cognitive load can influence microsaccade activity. We corroborate this notion. Specifically, we explore this relationship during visual search for features embedded in a terrain-like surface, with the eyes allowed to move freely during the task. We make two relevant contributions. First, we validate an approach to distinguishing between the ambient and focal phases of visual search. We show that this spectrum of visual behavior can be quantified by a single previously reported estimator, known as Krejtz&#39;s IC coefficient. Second, we use ambient/focal segments based on IC as a moderating factor for microsaccade analysis in response to task difficulty. We find that during the focal phase of visual search (a) microsaccade magnitude increases significantly, and (b) microsaccade rate decreases significantly, with increased task difficulty. We conclude that the combined use of IC and microsaccade analysis may be helpful in building effective tools that provide an indication of the level of cognitive activity within a task while the task is being performed.},
  archive      = {J_TVCG},
  author       = {Andrew T. Duchowski and Krzysztof Krejtz and Justyna Żurawska and Donald H. House},
  doi          = {10.1109/TVCG.2019.2901881},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2904-2918},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Using microsaccades to estimate task difficulty during visual search of layered surfaces},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TTHRESH: Tensor compression for multidimensional visual
data. <em>TVCG</em>, <em>26</em>(9), 2891–2903. (<a
href="https://doi.org/10.1109/TVCG.2019.2904063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory and network bandwidth are decisive bottlenecks when handling high-resolution multidimensional data sets in visualization applications, and they increasingly demand suitable data compression strategies. We introduce a novel lossy compression algorithm for multidimensional data over regular grids. It leverages the higher-order singular-value decomposition (HOSVD), a generalization of the SVD to three dimensions and higher, together with bit-plane, run-length and arithmetic coding to compress the HOSVD transform coefficients. Our scheme degrades the data particularly smoothly and achieves lower mean squared error than other state-of-the-art algorithms at low-to-medium bit rates, as it is required in data archiving and management for visualization purposes. Further advantages of the proposed algorithm include very fine bit rate selection granularity and the ability to manipulate data at very small cost in the compression domain, for example to reconstruct filtered and/or subsampled versions of all (or selected parts) of the data set.},
  archive      = {J_TVCG},
  author       = {Rafael Ballester-Ripoll and Peter Lindstrom and Renato Pajarola},
  doi          = {10.1109/TVCG.2019.2904063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2891-2903},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TTHRESH: Tensor compression for multidimensional visual data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Taxonomizer: Interactive construction of fully labeled
hierarchical groupings from attributes of multivariate data.
<em>TVCG</em>, <em>26</em>(9), 2875–2890. (<a
href="https://doi.org/10.1109/TVCG.2019.2895642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizing multivariate data spaces by their dimensions or attributes can be a rather difficult task. Most of the work in this area focuses on the statistical aspects such as correlation clustering, dimension reduction, and the like. These methods typically produce hierarchies in which the leaf nodes are labeled by the attribute names while the inner nodes are often represented by just a statistical measure and criterion, such as a threshold. This makes them difficult to understand for mainstream users. Taxonomies in science, biology, engineering, etc. on the other hand, are easy to comprehend since they provide meaningful labels at the inner nodes as well. Labeling inner nodes of taxonomies automatically requires the identification of hypernyms. Our proposed framework, called Taxonomizer, takes a visual analytics approach to meet this challenge. It appeals to the wisdom of humans to liaise with state of the art data analytics, neural word embeddings, and lexical databases. It consists of a set of visual tools that starts out with an automatically computed hierarchy where the leaf nodes are the original data attributes, and it then allows users to sculpt high-quality taxonomies for any multivariate dataset.},
  archive      = {J_TVCG},
  author       = {Salman Mahmood and Klaus Mueller},
  doi          = {10.1109/TVCG.2019.2895642},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2875-2890},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Taxonomizer: Interactive construction of fully labeled hierarchical groupings from attributes of multivariate data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal path maps on the GPU. <em>TVCG</em>, <em>26</em>(9),
2863–2874. (<a href="https://doi.org/10.1109/TVCG.2019.2904271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new method for computing optimal path maps on the GPU using OpenGL shaders. Our method explores GPU rasterization as a way to propagate optimal costs on a polygonal 2D environment, producing optimal path maps which can efficiently be queried at run-time. Our method is implemented entirely with GPU shaders, does not require pre-computation, addresses optimal path maps with multiple points and line segments as sources, and introduces a new optimal path map concept not addressed before: maps with weights at vertices representing possible changes in traversal speed. The produced maps offer new capabilities not explored by previous navigation representations and at the same time address paths with global optimality, a characteristic which has been mostly neglected in animated virtual environments. The proposed path maps partition the input environment into the regions sharing a same parent point along the shortest path to the closest source, taking into account possible speed changes at vertices. The proposed approach is particularly suitable for the animation of multiple agents moving toward the entrances or exits of a virtual environment, a situation which is efficiently represented with the proposed path maps.},
  archive      = {J_TVCG},
  author       = {Renato Farias and Marcelo Kallmann},
  doi          = {10.1109/TVCG.2019.2904271},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2863-2874},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Optimal path maps on the GPU},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On evaluating runtime performance of interactive
visualizations. <em>TVCG</em>, <em>26</em>(9), 2848–2862. (<a
href="https://doi.org/10.1109/TVCG.2019.2898435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As our field matures, evaluation of visualization techniques has extended from reporting runtime performance to studying user behavior. Consequently, many methodologies and best practices for user studies have evolved. While maintaining interactivity continues to be crucial for the exploration of large data sets, no similar methodological foundation for evaluating runtime performance has been developed. Our analysis of 50 recent visualization papers on new or improved techniques for rendering volumes or particles indicates that only a very limited set of parameters like different data sets, camera paths, viewport sizes, and GPUs are investigated, which make comparison with other techniques or generalization to other parameter ranges at least questionable. To derive a deeper understanding of qualitative runtime behavior and quantitative parameter dependencies, we developed a framework for the most exhaustive performance evaluation of volume and particle visualization techniques that we are aware of, including millions of measurements on ten different GPUs. This paper reports on our insights from statistical analysis of this data, discussing independent and linear parameter behavior and non-obvious effects. We give recommendations for best practices when evaluating runtime performance of scientific visualization applications, which can serve as a starting point for more elaborate models of performance quantification.},
  archive      = {J_TVCG},
  author       = {Valentin Bruder and Christoph Müller and Steffen Frey and Thomas Ertl},
  doi          = {10.1109/TVCG.2019.2898435},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2848-2862},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On evaluating runtime performance of interactive visualizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Micrography QR codes. <em>TVCG</em>, <em>26</em>(9),
2834–2847. (<a href="https://doi.org/10.1109/TVCG.2019.2896895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel algorithm to generate micrography QR codes, a novel machine-readable graphic generated by embedding a QR code within a micrography image. The unique structure of micrography makes it incompatible with existing methods used to combine QR codes with natural or halftone images. We exploited the high-frequency nature of micrography in the design of a novel deformation model that enables the skillful warping of individual letters and adjustment of font weights to enable the embedding of a QR code within a micrography. The entire process is supervised by a set of visual quality metrics tailored specifically for micrography, in conjunction with a novel QR code quality measure aimed at striking a balance between visual fidelity and decoding robustness. The proposed QR code quality measure is based on probabilistic models learned from decoding experiments using popular decoders with synthetic QR codes to capture the various forms of distortion that result from image embedding. Experiment results demonstrate the efficacy of the proposed method in generating micrography QR codes of high quality from a wide variety of inputs. The ability to embed QR codes with multiple scales makes it possible to produce a wide range of diverse designs. Experiments and user studies were conducted to evaluate the proposed method from a qualitative as well as quantitative perspective.},
  archive      = {J_TVCG},
  author       = {Shih-Hsuan Hung and Chih-Yuan Yao and Yu-Jen Fang and Ping Tan and Ruen-Rone Lee and Alla Sheffer and Hung-Kuo Chu},
  doi          = {10.1109/TVCG.2019.2896895},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2834-2847},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Micrography QR codes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring the effects of scalar and spherical colormaps on
ensembles of DMRI tubes. <em>TVCG</em>, <em>26</em>(9), 2818–2833. (<a
href="https://doi.org/10.1109/TVCG.2019.2898438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We report empirical study results on the color encoding of ensemble scalar and orientation to visualize diffusion magnetic resonance imaging (DMRI) tubes. The experiment tested six scalar colormaps for average fractional anisotropy (FA) tasks (grayscale, blackbody, diverging, isoluminant-rainbow, extended-blackbody, and coolwarm) and four three-dimensional (3D) spherical colormaps for tract tracing tasks (uniform gray, absolute, eigenmaps, and Boy&#39;s surface embedding). We found that extended-blackbody, coolwarm, and blackbody remain the best three approaches for identifying ensemble average in 3D. Isoluminant-rainbow colormap led to the same ensemble mean accuracy as other colormaps. However, more than 50 percent of the answers consistently had higher estimates of the ensemble average, independent of the mean values. The number of hues, not luminance, influences ensemble estimates of mean values. For ensemble orientation-tracing tasks, we found that both Boy&#39;s surface embedding (greatest spatial resolution and contrast) and absolute colormaps (lowest spatial resolution and contrast) led to more accurate answers than the eigenmaps scheme (medium resolution and contrast), acting as the uncanny-valley phenomenon of visualization design in terms of accuracy. Absolute colormap broadly used in brain science is a good default spherical colormap. We could conclude from our study that human visual processing of a chunk of colors differs from that of single colors.},
  archive      = {J_TVCG},
  author       = {Jian Chen and Guohao Zhang and Wesley Chiou and David H. Laidlaw and Alexander P. Auchus},
  doi          = {10.1109/TVCG.2019.2898438},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2818-2833},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring the effects of scalar and spherical colormaps on ensembles of DMRI tubes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mass-driven topology-aware curve skeleton extraction from
incomplete point clouds. <em>TVCG</em>, <em>26</em>(9), 2805–2817. (<a
href="https://doi.org/10.1109/TVCG.2019.2903805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a mass-driven curve skeleton as a curve skeleton representation for 3D point cloud data. The mass-driven curve skeleton presents geometric properties and mass distribution of a curve skeleton simultaneously. The computation of the mass-driven curve skeleton is formulated as a minimization of Wasserstein distance, with an entropic regularization term, between mass distributions of point clouds and curve skeletons. Assuming that the mass of one sampling point should be transported to a line-like structure, a topology-aware rough curve skeleton is extracted via the optimal transport plan. A Dirichlet energy regularization term is then used to obtain a smooth curve skeleton via geometric optimization. Given that rough curve skeleton extraction does not depend on complete point clouds, our algorithm can be directly applied to curve skeleton extraction from incomplete point clouds. We demonstrate that a mass-driven curve skeleton can be directly applied to an unoriented raw point scan with significant noise, outliers and large areas of missing data. In comparison with state-of-the-art methods on curve skeleton extraction, the performance of the proposed mass-driven curve skeleton is more robust in terms of extracting a correct topology.},
  archive      = {J_TVCG},
  author       = {Hongxing Qin and Jia Han and Ning Li and Hui Huang and Baoquan Chen},
  doi          = {10.1109/TVCG.2019.2903805},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2805-2817},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mass-driven topology-aware curve skeleton extraction from incomplete point clouds},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Magic carpet: Interaction fidelity for flying in VR.
<em>TVCG</em>, <em>26</em>(9), 2793–2804. (<a
href="https://doi.org/10.1109/TVCG.2019.2905200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locomotion in virtual environments is currently a difficult and unnatural task to perform. Normally, researchers tend to devise groundor floor-based metaphors to constrain the degrees of freedom (DoFs) during motion. These restrictions enable interactions that accurately emulate the human gait to provide high interaction fidelity. However, flying allows users to reach specific locations in a virtual scene more expeditiously. Our experience suggests that even though flying is not innate to humans, high-interaction-fidelity techniques may also improve the flying experience since flying requires simultaneously controlling additional DoFs. We present the Magic Carpet, an approach to flying that combines a floor proxy with a full-body representation to avoid balance and cybersickness issues. This design space enables DoF separation by treating direction indication and speed control as two separate phases of travel, thereby enabling techniques with higher interaction fidelity. To validate our design space, we conducted two complementary studies, one for each of the travel phases. In this paper, we present the results of both studies and report the best techniques for use within the Magic Carpet design space. To this end, we use both objective and subjective measures to evaluate the efficiency, embodiment effect, and side effects, such as physical fatigue and cybersickness, of the tested techniques in our design space. Our results show that the proposed approach enables high-interaction-fidelity techniques while improving the user experience.},
  archive      = {J_TVCG},
  author       = {Daniel Medeiros and Maurıcio Sousa and Alberto Raposo and Joaquim Jorge},
  doi          = {10.1109/TVCG.2019.2905200},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2793-2804},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Magic carpet: Interaction fidelity for flying in VR},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LDA ensembles for interactive exploration and categorization
of behaviors. <em>TVCG</em>, <em>26</em>(9), 2775–2792. (<a
href="https://doi.org/10.1109/TVCG.2019.2904069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define behavior as a set of actions performed by some actor during a period of time. We consider the problem of analyzing a large collection of behaviors by multiple actors, more specifically, identifying typical behaviors and spotting anomalous behaviors. We propose an approach leveraging topic modeling techniques - LDA (Latent Dirichlet Allocation) Ensembles - to represent categories of typical behaviors by topics that are obtained through topic modeling a behavior collection. When such methods are applied to text in natural languages, the quality of the extracted topics are usually judged based on the semantic relatedness of the terms pertinent to the topics. This criterion, however, is not necessarily applicable to topics extracted from non-textual data, such as action sets, since relationships between actions may not be obvious. We have developed a suite of visual and interactive techniques supporting the construction of an appropriate combination of topics based on other criteria, such as distinctiveness and coverage of the behavior set. Two case studies on analyzing operation behaviors in the security management system and visiting behaviors in an amusement park, and the expert evaluation of the first case study demonstrate the effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Siming Chen and Natalia Andrienko and Gennady Andrienko and Linara Adilova and Jeremie Barlet and Jörg Kindermann and Phong H. Nguyen and Olivier Thonnard and Cagatay Turkay},
  doi          = {10.1109/TVCG.2019.2904069},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2775-2792},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LDA ensembles for interactive exploration and categorization of behaviors},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and evaluation of visualization techniques of
off-screen and occluded targets in virtual reality environments.
<em>TVCG</em>, <em>26</em>(9), 2762–2774. (<a
href="https://doi.org/10.1109/TVCG.2019.2905580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research explores the design and evaluation of visualization techniques of targets that reside outside of users&#39; view and/or are occluded by other elements within a virtual reality environment (VE). We first compare four techniques (3DWedge, 3DArrow, 3DMinimap, and Radar) that use different types of visual elements to provide direction and distance information of targets. To give structure to their evaluation, we also develop a framework of four tasks (one for direction and three for distance) and their assessment criteria. The results of the first study show that 3DWedge is the best-performing and most usable technique. However, all techniques, including 3DWedge, have poor performance in dense scenarios with a large number of targets. To improve support in dense scenarios, a fifth technique, 3DWedge+, is developed by using 3DWedge as its foundation and including additional visual elements that are derived from the other three techniques which are shown to be useful. A second study is conducted to evaluate the performance of 3DWedge+ in relation to the other techniques. The results show that both 3DWedge and 3DWedge+ are significantly better in distinguishing user-to-target distance and that 3DWedge+ is particularly suitable for dense scenarios. Based on these results, we provide a set of recommendations for the design of visualization techniques of off-screen and occluded targets in 3D VE.},
  archive      = {J_TVCG},
  author       = {Difeng Yu and Hai-Ning Liang and Kaixuan Fan and Heng Zhang and Charles Fleming and Konstantinos Papangelis},
  doi          = {10.1109/TVCG.2019.2905580},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2762-2774},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design and evaluation of visualization techniques of off-screen and occluded targets in virtual reality environments},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An evaluation of semantically grouped word cloud designs.
<em>TVCG</em>, <em>26</em>(9), 2748–2761. (<a
href="https://doi.org/10.1109/TVCG.2019.2904683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word clouds continue to be a popular tool for summarizing textual information, despite their well-documented deficiencies for analytic tasks. Much of their popularity rests on their playful visual appeal. In this paper, we present the results of a series of controlled experiments that show that layouts in which words are arranged into semantically and visually distinct zones are more effective for understanding the underlying topics than standard word cloud layouts. White space separators and/or spatially grouped color coding led to significantly stronger understanding of the underlying topics compared to a standard Wordle layout, while simultaneously scoring higher on measures of aesthetic appeal. This work is an advance on prior research on semantic layouts for word clouds because that prior work has either not ensured that the different semantic groupings are visually or semantically distinct, or has not performed usability studies. An additional contribution of this work is the development of a dataset for a semantic category identification task that can be used for replication of these results or future evaluations of word cloud designs.},
  archive      = {J_TVCG},
  author       = {Marti A. Hearst and Emily Pedersen and Lekha Patil and Elsie Lee and Paul Laskowski and Steven Franconeri},
  doi          = {10.1109/TVCG.2019.2904683},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2748-2761},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An evaluation of semantically grouped word cloud designs},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Aggregated dendrograms for visual comparison between many
phylogenetic trees. <em>TVCG</em>, <em>26</em>(9), 2732–2747. (<a
href="https://doi.org/10.1109/TVCG.2019.2898186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the visual comparison of multiple phylogenetic trees that arises in evolutionary biology, specifically between one reference tree and a collection of dozens to hundreds of other trees. We abstract the domain questions of phylogenetic tree comparison as tasks to look for supporting or conflicting evidence for hypotheses that requires inspection of both topological structure and attribute values at different levels of detail in the tree collection. We introduce the new visual encoding idiom of aggregated dendrograms to concisely summarize the topological relationships between interactively chosen focal subtrees according to biologically meaningful criteria, and provide a layout algorithm that automatically adapts to the available screen space. We design and implement the ADView system, which represents trees at multiple levels of detail across multiple views: the entire collection, a subset of trees, an individual tree, specific subtrees of interest, and the individual branch level. We benchmark the algorithms developed for ADView, compare its information density to previous work, and demonstrate its utility for quickly gathering evidence about biological hypotheses through usage scenarios with data from recently published phylogenetic analysis and case studies of expert use with real-world data, drawn from a summative interview study.},
  archive      = {J_TVCG},
  author       = {Zipeng Liu and Shing Hei Zhan and Tamara Munzner},
  doi          = {10.1109/TVCG.2019.2898186},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {2732-2747},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Aggregated dendrograms for visual comparison between many phylogenetic trees},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual quality guidance for document exploration with
focus+context techniques. <em>TVCG</em>, <em>26</em>(8), 2715–2731. (<a
href="https://doi.org/10.1109/TVCG.2019.2895073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magic lens based focus+context techniques are powerful means for exploring document spatializations. Typically, they only offer additional summarized or abstracted views on focused documents. As a consequence, users might miss important information that is either not shown in aggregated form or that never happens to get focused. In this work, we present the design process and user study results for improving a magic lens based document exploration approach with exemplary visual quality cues to guide users in steering the exploration and support them in interpreting the summarization results. We contribute a thorough analysis of potential sources of information loss involved in these techniques, which include the visual spatialization of text documents, user-steered exploration, and the visual summarization. With lessons learned from previous research, we highlight the various ways those information losses could hamper the exploration. Furthermore, we formally define measures for the aforementioned different types of information losses and bias. Finally, we present the visual cues to depict these quality measures that are seamlessly integrated into the exploration approach. These visual cues guide users during the exploration and reduce the risk of misinterpretation and accelerate insight generation. We conclude with the results of a controlled user study and discuss the benefits and challenges of integrating quality guidance in exploration techniques.},
  archive      = {J_TVCG},
  author       = {Qi Han and Dennis Thom and Markus John and Steffen Koch and Florian Heimerl and Thomas Ertl},
  doi          = {10.1109/TVCG.2019.2895073},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2715-2731},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual quality guidance for document exploration with Focus+Context techniques},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). T-viSNE: Interactive assessment and interpretation of t-SNE
projections. <em>TVCG</em>, <em>26</em>(8), 2696–2714. (<a
href="https://doi.org/10.1109/TVCG.2020.2986996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of multidimensional data has proven to be a popular approach, with successful applications in a wide range of domains. Despite their usefulness, t-SNE projections can be hard to interpret or even misleading, which hurts the trustworthiness of the results. Understanding the details of t-SNE itself and the reasons behind specific patterns in its output may be a daunting task, especially for non-experts in dimensionality reduction. In this article, we present t-viSNE, an interactive tool for the visual exploration of t-SNE projections that enables analysts to inspect different aspects of their accuracy and meaning, such as the effects of hyper-parameters, distance and neighborhood preservation, densities and costs of specific neighborhoods, and the correlations between dimensions and visual patterns. We propose a coherent, accessible, and well-integrated collection of different views for the visualization of t-SNE projections. The applicability and usability of t-viSNE are demonstrated through hypothetical usage scenarios with real data sets. Finally, we present the results of a user study where the tool&#39;s effectiveness was evaluated. By bringing to light information that would normally be lost after running t-SNE, we hope to support analysts in using t-SNE and making its results better understandable.},
  archive      = {J_TVCG},
  author       = {Angelos Chatzimparmpas and Rafael M. Martins and Andreas Kerren},
  doi          = {10.1109/TVCG.2020.2986996},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2696-2714},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {T-viSNE: Interactive assessment and interpretation of t-SNE projections},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surfel-based incremental reconstruction of the boundary
between known and unknown space. <em>TVCG</em>, <em>26</em>(8),
2683–2695. (<a href="https://doi.org/10.1109/TVCG.2020.2990315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the first surfel-based method for multi-view 3D reconstruction of the boundary between known and unknown space. The proposed approach integrates multiple views from a moving depth camera and it generates a set of surfels that encloses observed empty space, i.e., it models both the boundary between empty and occupied space, and the boundary between empty and unknown space. One novelty of the method is that it does not require a persistent voxel map of the environment to distinguish between unknown and empty space. The problem is solved thanks to an incremental algorithm that computes the Boolean union of two surfel bounded volumes: the known volume from previous frames and the space observed from the current depth image. A number of strategies were developed to cope with errors in surfel position and orientation. The method, implemented on CPU and GPU, was evaluated on real data acquired in indoor scenarios, and it was compared against state of the art approaches. Results show that the proposed method has a low number of false positive and false negatives, it is faster than a standard volumetric algorithm, it has a lower memory consumption, and it scales better in large environments.},
  archive      = {J_TVCG},
  author       = {Riccardo Monica and Jacopo Aleotti},
  doi          = {10.1109/TVCG.2020.2990315},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2683-2695},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Surfel-based incremental reconstruction of the boundary between known and unknown space},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scribble-based 3D shape segmentation via weakly-supervised
learning. <em>TVCG</em>, <em>26</em>(8), 2671–2682. (<a
href="https://doi.org/10.1109/TVCG.2019.2892076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape segmentation is a fundamental problem in shape analysis. Previous research shows that prior knowledge helps to improve the segmentation accuracy and quality. However, completely labeling each 3D shape in a large training data set requires a heavy manual workload. In this paper, we propose a novel weakly-supervised algorithm for segmenting 3D shapes using deep learning. Our method jointly propagates information from scribbles to unlabeled faces and learns deep neural network parameters. Therefore, it does not rely on completely labeled training shapes and only needs a really simple and convenient scribble-based partially labeling process, instead of the extremely time-consuming and tedious fully labeling processes. Various experimental results demonstrate the proposed method&#39;s superior segmentation performance over the previous unsupervised approaches and comparable segmentation performance to the state-of-the-art fully supervised methods.},
  archive      = {J_TVCG},
  author       = {Zhenyu Shu and Xiaoyong Shen and Shiqing Xin and Qingjun Chang and Jieqing Feng and Ladislav Kavan and Ligang Liu},
  doi          = {10.1109/TVCG.2019.2892076},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2671-2682},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scribble-based 3D shape segmentation via weakly-supervised learning},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Portrait relief modeling from a single image. <em>TVCG</em>,
<em>26</em>(8), 2659–2670. (<a
href="https://doi.org/10.1109/TVCG.2019.2892439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel solution to enable portrait relief modeling from a single image. The main challenges are geometry reconstruction, facial details recovery and depth structure preservation. Previous image-based methods are developed for portrait bas-relief modeling in 2.5D form, but not adequate for 3D-like high relief modeling with undercut features. In this paper, we propose a template-based framework to generate portrait reliefs of various forms. Our method benefits from Shape-from-Shading (SFS). Specifically, we use bi-Laplacian mesh deformation to guide the relief modeling. Given a portrait image, we first use a template face to fit the portrait. We then apply bi-Laplacian mesh deformation to align the facial features. Afterwards, SFS-based reconstruction with a few user interactions is used to optimize the face depth, and create a relief with similar appearance to the input. Both depth structures and geometric details can be well constructed in the final relief. Experiments and comparisons to other methods demonstrate the effectiveness of the proposed method.},
  archive      = {J_TVCG},
  author       = {Yu-Wei Zhang and Caiming Zhang and Wenping Wang and Yanzhao Chen and Zhongping Ji and Hui Liu},
  doi          = {10.1109/TVCG.2019.2892439},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2659-2670},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Portrait relief modeling from a single image},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MARVisT: Authoring glyph-based visualization in mobile
augmented reality. <em>TVCG</em>, <em>26</em>(8), 2645–2658. (<a
href="https://doi.org/10.1109/TVCG.2019.2892415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in mobile augmented reality (AR) techniques have shed new light on personal visualization for their advantages of fitting visualization within personal routines, situating visualization in a real-world context, and arousing users&#39; interests. However, enabling non-experts to create data visualization in mobile AR environments is challenging given the lack of tools that allow in-situ design while supporting the binding of data to AR content. Most existing AR authoring tools require working on personal computers or manually creating each virtual object and modifying its visual attributes. We systematically study this issue by identifying the specificity of AR glyph-based visualization authoring tool and distill four design considerations. Following these design considerations, we design and implement MARVisT, a mobile authoring tool that leverages information from reality to assist non-experts in addressing relationships between data and virtual glyphs, real objects and virtual glyphs, and real objects and data. With MARVisT, users without visualization expertise can bind data to real-world objects to create expressive AR glyph-based visualizations rapidly and effortlessly, reshaping the representation of the real world with data. We use several examples to demonstrate the expressiveness of MARVisT. A user study with non-experts is also conducted to evaluate the authoring experience of MARVisT.},
  archive      = {J_TVCG},
  author       = {Zhutian Chen and Yijia Su and Yifang Wang and Qianwen Wang and Huamin Qu and Yingcai Wu},
  doi          = {10.1109/TVCG.2019.2892415},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2645-2658},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MARVisT: Authoring glyph-based visualization in mobile augmented reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Localization and completion for 3D object interactions.
<em>TVCG</em>, <em>26</em>(8), 2634–2644. (<a
href="https://doi.org/10.1109/TVCG.2019.2892454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding where and what objects to put into an existing scene is a common task for scene synthesis and robot/character motion planning. Existing frameworks require development of hand-crafted features suitable for the task, or full volumetric analysis that could be memory intensive and imprecise. In this paper, we propose a data-driven framework to discover a suitable location and then place the appropriate objects in a scene. Our approach is inspired by computer vision techniques for localizing objects in images: using an all directional depth image (ADD-image) that encodes the 360-degree field of view from samples in the scene, our system regresses the images to the positions where the new object can be located. Given several candidate areas around the host object in the scene, our system predicts the partner object whose geometry fits well to the host object. Our approach is highly parallel and memory efficient, and is especially suitable for handling interactions between large and small objects. We show examples where the system can hang bags on hooks, fit chairs in front of desks, put objects into shelves, insert flowers into vases, and put hangers onto laundry rack.},
  archive      = {J_TVCG},
  author       = {Xi Zhao and Ruizhen Hu and Haisong Liu and Taku Komura and Xinyu Yang},
  doi          = {10.1109/TVCG.2019.2892454},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2634-2644},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Localization and completion for 3D object interactions},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interaction-based human activity comparison. <em>TVCG</em>,
<em>26</em>(8), 2620–2633. (<a
href="https://doi.org/10.1109/TVCG.2019.2893247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods for motion comparison consider features from individual characters. However, the semantic meaning of many human activities is usually defined by the interaction between them, such as a high-five interaction of two characters. There is little success in adapting interaction-based features in activity comparison, as they either do not have a fixed topology or are in high dimensional. In this paper, we propose a unified framework for activity comparison from the interaction point of view. Our new metric evaluates the similarity of interaction by adapting the Earth Mover&#39;s Distance onto a customized geometric mesh structure that represents spatial-temporal interactions. This allows us to compare different classes of interactions and discover their intrinsic semantic similarity. We created five interaction databases of different natures, covering both two-characters (synthetic and real-people) and character-object interactions, which are open for public uses. We demonstrate how the proposed metric aligns well with the semantic meaning of the interaction. We also apply the metric in interaction retrieval and show how it outperforms existing ones. The proposed method can be used for unsupervised activity detection in monitoring systems and activity retrieval in smart animation systems.},
  archive      = {J_TVCG},
  author       = {Yijun Shen and Longzhi Yang and Edmond S. L. Ho and Hubert P. H. Shum},
  doi          = {10.1109/TVCG.2019.2893247},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2620-2633},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interaction-based human activity comparison},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How people visually represent discrete constraint problems.
<em>TVCG</em>, <em>26</em>(8), 2603–2619. (<a
href="https://doi.org/10.1109/TVCG.2019.2895085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problems such as timetabling or personnel allocation can be modeled and solved using discrete constraint programming languages. However, while existing constraint solving software solves such problems quickly in many cases, these systems involve specialized languages that require significant time and effort to learn and apply. These languages are typically text-based and often difficult to interpret and understand quickly, especially for people without engineering or mathematics backgrounds. Visualization could provide an alternative way to model and understand such problems. Although many visual programming languages exist for procedural languages, visual encoding of problem specifications has not received much attention. Future problem visualization languages could represent problem elements and their constraints unambiguously, but without unnecessary cognitive burdens for those needing to translate their problem&#39;s mental representation into diagrams. As a first step towards such languages, we executed a study that catalogs how people represent constraint problems graphically. We studied three groups with different expertise: non-computer scientists, computer scientists and constraint programmers and analyzed their marks on paper (e.g., arrows), gestures (e.g., pointing) and the mappings to problem concepts (e.g., containers, sets). We provide foundations to guide future tool designs allowing people to effectively grasp, model and solve problems through visual representations.},
  archive      = {J_TVCG},
  author       = {Xu Zhu and Miguel A. Nacenta and Özgür Akgün and Peter Nightingale},
  doi          = {10.1109/TVCG.2019.2895085},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2603-2619},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How people visually represent discrete constraint problems},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast quasi-conformal regional flattening of the left atrium.
<em>TVCG</em>, <em>26</em>(8), 2591–2602. (<a
href="https://doi.org/10.1109/TVCG.2020.2966702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-dimensional representation of 3D anatomical structures is a simple and intuitive way for analysing patient information across populations and image modalities. While cardiac ventricles, especially the left ventricle, have an established standard representation (bull&#39;s eye plot), the 2D depiction of the left atrium (LA) remains challenging due to its sub-structural complexity including the pulmonary veins (PV) and the left atrial appendage (LAA). Quasi-conformal flattening techniques, successfully applied to cardiac ventricles, require additional constraints in the case of the LA to place the PV and LAA in the same geometrical 2D location for different cases. Some registration-based methods have been proposed but surface registration is time-consuming and prone to errors when the geometries are very different. We propose a novel atrial flattening methodology where a 2D standardised map of the LA is obtained quickly and without errors related to registration. The LA is divided into five regions which are then mapped to their analogue two-dimensional regions. 67 human left atria from magnetic resonance images (MRI) were studied to derive a population-based template representing the averaged relative locations of the PVs and LAA. The clinical application of our methodology is illustrated on different use cases including the integration of MRI and electroanatomical data.},
  archive      = {J_TVCG},
  author       = {Marta Nuñez-Garcia and Gabriel Bernardino and Francisco Alarcón and Gala Caixal and Lluís Mont and Oscar Camara and Constantine Butakoff},
  doi          = {10.1109/TVCG.2020.2966702},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2591-2602},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast quasi-conformal regional flattening of the left atrium},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring the sensitivity of choropleths under attribute
uncertainty. <em>TVCG</em>, <em>26</em>(8), 2576–2590. (<a
href="https://doi.org/10.1109/TVCG.2019.2892483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The choropleth map is an essential tool for spatial data analysis. However, the underlying attribute values of a spatial unit greatly influence the statistical analyses and map classification procedures when generating a choropleth map. If the attribute values incorporate a range of uncertainty, a critical task is determining how much the uncertainty impacts both the map visualization and the statistical analysis. In this paper, we present a visual analytics system that enhances our understanding of the impact of attribute uncertainty on data visualization and statistical analyses of these data. Our system consists of a parallel coordinates-based uncertainty specification view, an impact river and impact matrix visualization for region-based and simulation-based analysis, and a dual-choropleth map and t-SNE plot for visualizing the changes in classification and spatial autocorrelation over the range of uncertainty in the attribute values. We demonstrate our system through three use cases illustrating the impact of attribute uncertainty in geographic analysis.},
  archive      = {J_TVCG},
  author       = {Zhaosong Huang and Yafeng Lu and Elizabeth A. Mack and Wei Chen and Ross Maciejewski},
  doi          = {10.1109/TVCG.2019.2892483},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2576-2590},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring the sensitivity of choropleths under attribute uncertainty},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Disentangled human body embedding based on deep hierarchical
neural network. <em>TVCG</em>, <em>26</em>(8), 2560–2575. (<a
href="https://doi.org/10.1109/TVCG.2020.2988476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human bodies exhibit various shapes for different identities or poses, but the body shape has certain similarities in structure and thus can be embedded in a low-dimensional space. This article presents an autoencoder-like network architecture to learn disentangled shape and pose embedding specifically for the 3D human body. This is inspired by recent progress of deformation-based latent representation learning. To improve the reconstruction accuracy, we propose a hierarchical reconstruction pipeline for the disentangling process and construct a large dataset of human body models with consistent connectivity for the learning of the neural network. Our learned embedding can not only achieve superior reconstruction accuracy but also provide great flexibility in 3D human body generation via interpolation, bilinear interpolation, and latent space sampling. The results from extensive experiments demonstrate the powerfulness of our learned 3D human body embedding in various applications.},
  archive      = {J_TVCG},
  author       = {Boyi Jiang and Juyong Zhang and Jianfei Cai and Jianmin Zheng},
  doi          = {10.1109/TVCG.2020.2988476},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2560-2575},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Disentangled human body embedding based on deep hierarchical neural network},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth of field rendering using multilayer-neighborhood
optimization. <em>TVCG</em>, <em>26</em>(8), 2546–2559. (<a
href="https://doi.org/10.1109/TVCG.2019.2894627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth of field (DOF) is utilized widely to deliver artistic effects in photography. However, existing post-processing techniques for rendering DOF effects introduce visual artifacts such as color leakage, blurring discontinuity, and the partial occlusion problems which limit the application of DOF. Traditionally, occluded pixels are ignored or not well estimated although they might make key contributions to images. In this paper, we propose a new filtering approach which takes approximated occluded pixels into account to synthesize the DOF effects for images. In our approach, images are separated into different layers based on depth. Besides, we utilize adaptive PatchMatch method to estimate the intensities of occluded pixels, especially in the background region. We again propose a new multilayer-neighborhood optimization to estimate occluded pixels contributions and render the images. Finally, we apply gathering filter to achieve the rendered images with elite DOF effects. Multiple experiments have shown that our approach can handle color leakage, blurring discontinuity and partial occlusion problem while providing high-quality DOF rendering effects.},
  archive      = {J_TVCG},
  author       = {Benxuan Zhang and Bin Sheng and Ping Li and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2019.2894627},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2546-2559},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Depth of field rendering using multilayer-neighborhood optimization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis on mitigation of visually induced motion sickness
by applying dynamical blurring on a user’s retina. <em>TVCG</em>,
<em>26</em>(8), 2535–2545. (<a
href="https://doi.org/10.1109/TVCG.2019.2893668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visually induced motion sickness (MS) experienced in a 3D immersive virtual environment (VE) limits the widespread use of virtual reality (VR). This paper studies the effects of a saliency detection-based approach on the reduction of MS when the display on a user&#39;s retina is dynamic blurred. In the experiment, forty participants were exposed to a VR experience under a control condition without applying dynamic blurring, and an experimental condition applying dynamic blurring. The experimental results show that the participants under the experimental condition report a statistically significant reduction in the severity of MS symptoms on average during the VR experience compared to those under the control condition, which demonstrates that the proposed approach may alleviate visually induced MS in VR and enable users to remain in a VE for a longer period of time.},
  archive      = {J_TVCG},
  author       = {Guang-Yu Nie and Henry Been-Lirn Duh and Yue Liu and Yongtian Wang},
  doi          = {10.1109/TVCG.2019.2893668},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {2535-2545},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analysis on mitigation of visually induced motion sickness by applying dynamical blurring on a user&#39;s retina},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual analysis of collective anomalies using faceted
high-order correlation graphs. <em>TVCG</em>, <em>26</em>(7), 2517–2534.
(<a href="https://doi.org/10.1109/TVCG.2018.2889470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Successfully detecting, analyzing, and reasoning about collective anomalies is important for many real-life application domains (e.g., intrusion detection, fraud analysis, software security). The primary challenges to achieving this goal include the overwhelming number of low-risk events and their multimodal relationships, the diversity of collective anomalies by various data and anomaly types, and the difficulty in incorporating the domain knowledge of experts. In this paper, we propose the novel concept of the faceted High-Order Correlation Graph (HOCG). Compared with previous, low-order correlation graphs, HOCG achieves better user interactivity, computational scalability, and domain generality through synthesizing heterogeneous types of objects, their anomalies, and the multimodal relationships, all in a single graph. We design elaborate visual metaphors, interaction models, and the coordinated multiple view based interface to allow users to fully unleash the visual analytics power of the HOCG. We conduct case studies for three application domains and collect feedback from domain experts who apply our method to these scenarios. The results demonstrate the effectiveness of the HOCG in the overview of point anomalies, the detection of collective anomalies, and the reasoning process of root cause analyses.},
  archive      = {J_TVCG},
  author       = {Jia Yan and Lei Shi and Jun Tao and Xiaolong Yu and Zhou Zhuang and Congcong Huang and Rulei Yu and Purui Su and Chaoli Wang and Yang Chen},
  doi          = {10.1109/TVCG.2018.2889470},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2517-2534},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of collective anomalies using faceted high-order correlation graphs},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supporting story synthesis: Bridging the gap between visual
analytics and storytelling. <em>TVCG</em>, <em>26</em>(7), 2499–2516.
(<a href="https://doi.org/10.1109/TVCG.2018.2889054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual analytics usually deals with complex data and uses sophisticated algorithmic, visual, and interactive techniques supporting the analysis. Findings and results of the analysis often need to be communicated to an audience that lacks visual analytics expertise. This requires analysis outcomes to be presented in simpler ways than that are typically used in visual analytics systems. However, not only analytical visualizations may be too complex for target audiences but also the information that needs to be presented. Analysis results may consist of multiple components, which may involve multiple heterogeneous facets. Hence, there exists a gap on the path from obtaining analysis findings to communicating them, within which two main challenges lie: information complexity and display complexity. We address this problem by proposing a general framework where data analysis and result presentation are linked by story synthesis, in which the analyst creates and organises story contents. Unlike previous research, where analytic findings are represented by stored display states, we treat findings as data constructs. We focus on selecting, assembling and organizing findings for further presentation rather than on tracking analysis history and enabling dual (i.e., explorative and communicative) use of data displays. In story synthesis, findings are selected, assembled, and arranged in meaningful layouts that take into account the structure of information and inherent properties of its components. We propose a workflow for applying the proposed conceptual framework in designing visual analytics systems and demonstrate the generality of the approach by applying it to two diverse domains, social media and movement analysis.},
  archive      = {J_TVCG},
  author       = {Siming Chen and Jie Li and Gennady Andrienko and Natalia Andrienko and Yun Wang and Phong H. Nguyen and Cagatay Turkay},
  doi          = {10.1109/TVCG.2018.2889054},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2499-2516},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Supporting story synthesis: Bridging the gap between visual analytics and storytelling},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic labeling and instance segmentation of 3D point
clouds using patch context analysis and multiscale processing.
<em>TVCG</em>, <em>26</em>(7), 2485–2498. (<a
href="https://doi.org/10.1109/TVCG.2018.2889944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel algorithm for semantic segmentation and labeling of 3D point clouds of indoor scenes, where objects in point clouds can have significant variations and complex configurations. Effective segmentation methods decomposing point clouds into semantically meaningful pieces are highly desirable for object recognition, scene understanding, scene modeling, etc. However, existing segmentation methods based on low-level geometry tend to either under-segment or over-segment point clouds. Our method takes a fundamentally different approach, where semantic segmentation is achieved along with labeling. To cope with substantial shape variation for objects in the same category, we first segment point clouds into surface patches and use unsupervised clustering to group patches in the training set into clusters, providing an intermediate representation for effectively learning patch relationships. During testing, we propose a novel patch segmentation and classification framework with multiscale processing, where the local segmentation level is automatically determined by exploiting the learned cluster based contextual information. Our method thus produces robust patch segmentation and semantic labeling results, avoiding parameter sensitivity. We further learn object-cluster relationships from the training set, and produce semantically meaningful object level segmentation. Our method outperforms state-of-the-art methods on several representative point cloud datasets, including S3DIS, SceneNN, Cornell RGB-D and ETH.},
  archive      = {J_TVCG},
  author       = {Shi-Min Hu and Jun-Xiong Cai and Yu-Kun Lai},
  doi          = {10.1109/TVCG.2018.2889944},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2485-2498},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Semantic labeling and instance segmentation of 3D point clouds using patch context analysis and multiscale processing},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Saliency-aware texture smoothing. <em>TVCG</em>,
<em>26</em>(7), 2471–2484. (<a
href="https://doi.org/10.1109/TVCG.2018.2889055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture smoothing aims to smooth out textures in images, while retaining the prominent structures. This paper presents a saliency-aware approach to the problem with two key contributions. First, we design a deep saliency network with guided non-local blocks (GNLBs) for learning long-range pixel dependencies by taking the predicted saliency map at former layer as the guidance image to help suppress the non-saliency regions in the shallow layer. The GNLB computes the saliency response at a position by a weighted sum of features at all positions, and enables us to produce results that outperform existing deep saliency models. Second, we formulate a joint optimization framework to take saliency information when iteratively separating textures from structures: on the texture layer, we smooth out structures with the help of the saliency information and migrate structures from the texture to structure layer, while on the structure layer, we adopt another deep model to detect edges and simultaneous sparse coding to push textures back to the texture layer. We tested our method on a rich variety of images and compared it with several state-of-the-art methods. Both visual and quantitative comparison results show that our method better preserves structures while removing the texture components.},
  archive      = {J_TVCG},
  author       = {Lei Zhu and Xiaowei Hu and Chi-Wing Fu and Jing Qin and Pheng-Ann Heng},
  doi          = {10.1109/TVCG.2018.2889055},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2471-2484},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Saliency-aware texture smoothing},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Precomputed multiple scattering for rapid light simulation
in participating media. <em>TVCG</em>, <em>26</em>(7), 2456–2470. (<a
href="https://doi.org/10.1109/TVCG.2018.2890466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering translucent materials is costly: light transport algorithms need to simulate a large number of scattering events inside the material before reaching convergence. The cost is especially high for materials with a large albedo or a small mean-free-path, where higher-order scattering effects dominate. We present a new method for fast computation of global illumination with participating media. Our method uses precomputed multiple scattering effects, stored in two compact tables. These precomputed multiple scattering tables are easy to integrate with any illumination simulation algorithm. We give examples for virtual ray lights (VRL), photon mapping with beams and paths (UPBP), Metropolis Light Transport with Manifold Exploration (MEMLT). The original algorithms are in charge of low-order scattering, combined with multiple scattering computed using our table. Our results show significant improvements in convergence speed and memory costs, with negligible impact on accuracy.},
  archive      = {J_TVCG},
  author       = {Beibei Wang and Liangsheng Ge and Nicolas Holzschuch},
  doi          = {10.1109/TVCG.2018.2890466},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2456-2470},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Precomputed multiple scattering for rapid light simulation in participating media},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Penalty force for coupling materials with coulomb friction.
<em>TVCG</em>, <em>26</em>(7), 2443–2455. (<a
href="https://doi.org/10.1109/TVCG.2019.2891591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel penalty force to enforce contacts with accurate Coulomb friction. The force is compatible with fully-implicit time integration and the use of optimization-based integration. The contact force is quite general. In addition to processing collisions between deformable objects, the force can be used to couple rigid bodies to deformable objects or the material point method. The force naturally leads to stable stacking without drift over time, even when solvers are not run to convergence. The force leads to an asymmetrical system, and we provide a practical solution for handling these.},
  archive      = {J_TVCG},
  author       = {Ounan Ding and Craig Schroeder},
  doi          = {10.1109/TVCG.2019.2891591},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2443-2455},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Penalty force for coupling materials with coulomb friction},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal analysis of video collections: Visual exploration
of presentation techniques in TED talks. <em>TVCG</em>, <em>26</em>(7),
2429–2442. (<a href="https://doi.org/10.1109/TVCG.2018.2889081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While much research in the educational field has revealed many presentation techniques, they often overlap and are even occasionally contradictory. Exploring presentation techniques used in TED Talks could provide evidence for a practical guideline. This study aims to explore the verbal and non-verbal presentation techniques from a collection of TED Talks. However, such analysis is challenging due to the difficulties of analyzing multimodal video collections consisted of frame images, text, and metadata. This paper proposes a visual analytic system to analyze multimodal content in video collections. The system features three views at different levels: the Projection view with novel glyphs to facilitate cluster analysis regarding presentation styles; the Comparison View to present temporal distribution and concurrences of presentation techniques and support intra-cluster analysis; and the Video View to enable contextualized exploration of a video. We conduct a case study with language education experts and university students to provide anecdotal evidence about the effectiveness of our approach, and report new findings about presentation techniques in TED Talks. Quantitative feedback from a user study confirms the usefulness of our visual system for multimodal analysis of video collections.},
  archive      = {J_TVCG},
  author       = {Aoyu Wu and Huamin Qu},
  doi          = {10.1109/TVCG.2018.2889081},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2429-2442},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multimodal analysis of video collections: Visual exploration of presentation techniques in TED talks},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inpainting of wide-baseline multiple viewpoint video.
<em>TVCG</em>, <em>26</em>(7), 2417–2428. (<a
href="https://doi.org/10.1109/TVCG.2018.2889297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a non-parametric algorithm for multiple-viewpoint video inpainting. Uniquely, our algorithm addresses the domain of wide baseline multiple-viewpoint video (MVV) with no temporal look-ahead in near real time speed. A Dictionary of Patches (DoP) is built using multi-resolution texture patches reprojected from geometric proxies available in the alternate views. We dynamically update the DoP over time, and a Markov Random Field optimisation over depth and appearance is used to resolve and align a selection of multiple candidates for a given patch, this ensures the inpainting of large regions in a plausible manner conserving both spatial and temporal coherence. We demonstrate the removal of large objects (e.g., people) on challenging indoor and outdoor MVV exhibiting cluttered, dynamic backgrounds and moving cameras.},
  archive      = {J_TVCG},
  author       = {Andrew Gilbert and Matthew Trumble and Adrian Hilton and John Collomosse},
  doi          = {10.1109/TVCG.2018.2889297},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2417-2428},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Inpainting of wide-baseline multiple viewpoint video},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). H-CNN: Spatial hashing based CNN for 3D shape analysis.
<em>TVCG</em>, <em>26</em>(7), 2403–2416. (<a
href="https://doi.org/10.1109/TVCG.2018.2887262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel spatial hashing based data structure to facilitate 3D shape analysis using convolutional neural networks (CNNs). Our method builds hierarchical hash tables for an input model under different resolutions that leverage the sparse occupancy of 3D shape boundary. Based on this data structure, we design two efficient GPU algorithms namely hash2col and col2hash so that the CNN operations like convolution and pooling can be efficiently parallelized. The perfect spatial hashing is employed as our spatial hashing scheme, which is not only free of hash collision but also nearly minimal so that our data structure is almost of the same size as the raw input. Compared with existing 3D CNN methods, our data structure significantly reduces the memory footprint during the CNN training. As the input geometry features are more compactly packed, CNN operations also run faster with our data structure. The experiment shows that, under the same network structure, our method yields comparable or better benchmark results compared with the state-of-the-art while it has only one-third memory consumption when under high resolutions (i.e., 2563).},
  archive      = {J_TVCG},
  author       = {Tianjia Shao and Yin Yang and Yanlin Weng and Qiming Hou and Kun Zhou},
  doi          = {10.1109/TVCG.2018.2887262},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2403-2416},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {H-CNN: Spatial hashing based CNN for 3D shape analysis},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring evolution of dynamic networks via diachronic node
embeddings. <em>TVCG</em>, <em>26</em>(7), 2387–2402. (<a
href="https://doi.org/10.1109/TVCG.2018.2887230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks evolve with their structures changing over time. It is still a challenging problem to efficiently explore the evolution of dynamic networks in terms of both their structural and temporal properties. In this paper, we propose a visual analytics methodology to interactively explore the temporal evolution of dynamic networks in the context of their structure. A novel diachronic node embedding method is first proposed to learn latent representations of the structural and temporal features of nodes in a vector space. Diachronic node embeddings are then used to discover communities with similar structural proximity and temporal evolution patterns. A visual analytics system is designed to enable users to visually explore the evolutions of nodes, communities, and the network as a whole in terms of their structural and temporal properties. We evaluate the effectiveness of our method using artificial and real-world dynamic networks and comparisons with previous methods.},
  archive      = {J_TVCG},
  author       = {Jin Xu and Yubo Tao and Yuyu Yan and Hai Lin},
  doi          = {10.1109/TVCG.2018.2887230},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2387-2402},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring evolution of dynamic networks via diachronic node embeddings},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-based dynamic graph visualisation. <em>TVCG</em>,
<em>26</em>(7), 2373–2386. (<a
href="https://doi.org/10.1109/TVCG.2018.2886901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic graph drawing algorithms take as input a series of timeslices that standard, force-directed algorithms can exploit to compute a layout. However, often dynamic graphs are expressed as a series of events where the nodes and edges have real coordinates along the time dimension that are not confined to discrete timeslices. Current techniques for dynamic graph drawing impose a set of timeslices on this event-based data in order to draw the dynamic graph, but it is unclear how many timeslices should be selected: too many timeslices slows the computation of the layout, while too few timeslices obscures important temporal features, such as causality. To address these limitations, we introduce a novel model for drawing event-based dynamic graphs and the first dynamic graph drawing algorithm, DynNoSlice, that is capable of drawing dynamic graphs in this model. DynNoSlice is an offline, force-directed algorithm that draws event-based, dynamic graphs in the space-time cube (2D+time). We also present a method to extract representative small multiples from the space-time cube. To demonstrate the advantages of our approach, DynNoSlice is compared with state-of-the-art timeslicing methods using a metrics-based experiment. Finally, we present case studies of event-based dynamic data visualised with the new model and algorithm.},
  archive      = {J_TVCG},
  author       = {Paolo Simonetto and Daniel Archambault and Stephen Kobourov},
  doi          = {10.1109/TVCG.2018.2886901},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2373-2386},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Event-based dynamic graph visualisation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Controllable motion-blur effects in still images.
<em>TVCG</em>, <em>26</em>(7), 2362–2372. (<a
href="https://doi.org/10.1109/TVCG.2018.2889485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion blur in a photo is the consequence of object motion during the image acquisition. It results in a visible trail along the motion of a recorded object and can be used by photographers to convey a sense of motion. Nevertheless, it is very challenging to acquire this effect as intended and requires much experience from the photographer. To achieve actual control over the motion blur, one could be added in a post process but current solutions require complex manual intervention and can lead to artifacts that mix moving and static objects incorrectly. In this paper, we propose a novel method to add motion blur to a single image that generates the illusion of a photographed motion. Relying on a minimal user input, a filtering process is employed to produce a virtual motion effect. It carefully handles object boundaries to avoid artifacts produced by standard filtering methods. We illustrate the effectiveness of our solution with various complex examples, including multi-directional blur, reflections, multiple objects, and illustrate how several motion-related artistic effects can be achieved. Our post-processing solution is an alternative to capturing the intended real-world motion blur directly and enables fine-grained control of the motion-blur effect.},
  archive      = {J_TVCG},
  author       = {Xuejiao Luo and Nestor Z. Salamon and Elmar Eisemann},
  doi          = {10.1109/TVCG.2018.2889485},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2362-2372},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Controllable motion-blur effects in still images},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CaricatureShop: Personalized and photorealistic caricature
sketching. <em>TVCG</em>, <em>26</em>(7), 2349–2361. (<a
href="https://doi.org/10.1109/TVCG.2018.2886007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the first sketching system for interactively personalized and photorealistic face caricaturing. Input an image of a human face, the users can create caricature photos by manipulating its facial feature curves. Our system first performs exaggeration on the recovered 3D face model, which is conducted by assigning the laplacian of each vertex a scaling factor according to the edited sketches. The mapping between 2D sketches and the vertex-wise scaling field is constructed by a novel deep learning architecture. Our approach allows outputting different exaggerations when applying the same sketching on different input figures in term of their different geometric characteristics, which makes the generated results “personalized”. With the obtained 3D caricature model, two images are generated, one obtained by applying 2D warping guided by the underlying 3D mesh deformation and the other obtained by re-rendering the deformed 3D textured model. These two images are then seamlessly integrated to produce our final output. Due to the severe stretching of meshes, the rendered texture is of blurry appearances. A deep learning approach is exploited to infer the missing details for enhancing these blurry regions. Moreover, a relighting operation is invented to further improve the photorealism of the result. These further make our results “photorealistic”. The qualitative experiment results validated the efficiency of our sketching system.},
  archive      = {J_TVCG},
  author       = {Xiaoguang Han and Kangcheng Hou and Dong Du and Yuda Qiu and Shuguang Cui and Kun Zhou and Yizhou Yu},
  doi          = {10.1109/TVCG.2018.2886007},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2349-2361},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CaricatureShop: Personalized and photorealistic caricature sketching},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep learning-based framework for intersectional traffic
simulation and editing. <em>TVCG</em>, <em>26</em>(7), 2335–2348. (<a
href="https://doi.org/10.1109/TVCG.2018.2889834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing traffic simulation methods have been focused on simulating vehicles on freeways or city-scale urban networks. However, relatively little research has been done to simulate intersectional traffic to date despite its broad potential applications. In this paper, we propose a novel deep learning-based framework to simulate and edit intersectional traffic. Specifically, based on an in-house collected intersectional traffic dataset, we employ the combination of convolution network (CNN) and recurrent network (RNN) to learn the patterns of vehicle trajectories in intersectional traffic. Besides simulating novel intersectional traffic, our method can be used to edit existing intersectional traffic. Through many experiments as well as comparative user studies, we demonstrate that the results by our method are visually indistinguishable from ground truth, and our method can outperform existing methods.},
  archive      = {J_TVCG},
  author       = {Huikun Bi and Tianlu Mao and Zhaoqi Wang and Zhigang Deng},
  doi          = {10.1109/TVCG.2018.2889834},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {2335-2348},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A deep learning-based framework for intersectional traffic simulation and editing},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Virtual locomotion: A survey. <em>TVCG</em>, <em>26</em>(6),
2315–2334. (<a href="https://doi.org/10.1109/TVCG.2018.2887379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) has enjoyed significant popularity in recent years. Where navigation has been a fundamental appeal of 3D applications for decades, facilitating this in VR has been quite a challenge. Over the past decades, various virtual locomotion techniques (VLTs) have been developed that aim to offer natural, usable and efficient ways of navigating VR without inducing VR sickness. Several studies of these techniques have been conducted in order to evaluate their performance in various study conditions and virtual contexts. Taxonomies have also been proposed to either place similar techniques in meaningful categories or decompose them to their underlying design components. In this survey, we aim to aggregate and understand the current state of the art of VR locomotion research and discuss the design implications of VLTs in terms of strengths, weaknesses and applicability.},
  archive      = {J_TVCG},
  author       = {Majed Al Zayer and Paul MacNeilage and Eelke Folmer},
  doi          = {10.1109/TVCG.2018.2887379},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2315-2334},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual locomotion: A survey},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tile pair-based adaptive multi-rate stereo shading.
<em>TVCG</em>, <em>26</em>(6), 2303–2314. (<a
href="https://doi.org/10.1109/TVCG.2018.2883314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a new stereo shading architecture that enables adaptive shading rates and automatic shading reuse among triangles and between two views. The proposed pipeline presents several novel features. First, the present sort-middle/bin shading is extended to tile pair-based shading to rasterize and shade pixels at two views simultaneously. A new rasterization algorithm utilizing epipolar geometry is then proposed to schedule tile pairs and perform rasterization at stereo views efficiently. Second, this work presents an adaptive multi-rate shading framework to compute shading on pixels at different rates. A novel tile-based screen space cache and a new cache reuse shader are proposed to perform such multi-rate shading across triangles and views. The results show that the newly proposed method outperforms the standard sort-middle shading and the state-of-the-art multi-rate shading by achieving considerably lower shading cost and memory bandwidth.},
  archive      = {J_TVCG},
  author       = {Yazhen Yuan and Rui Wang and Hujun Bao},
  doi          = {10.1109/TVCG.2018.2883314},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2303-2314},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tile pair-based adaptive multi-rate stereo shading},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simulating liquids on dynamically warping grids.
<em>TVCG</em>, <em>26</em>(6), 2288–2302. (<a
href="https://doi.org/10.1109/TVCG.2018.2883628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce dynamically warping grids for adaptive liquid simulation. Our primary contributions are a strategy for dynamically deforming regular grids over the course of a simulation and a method for efficiently utilizing these deforming grids for liquid simulation. Prior work has shown that unstructured grids are very effective for adaptive fluid simulations. However, unstructured grids often lead to complicated implementations and a poor cache hit rate due to inconsistent memory access. Regular grids, on the other hand, provide a fast, fixed memory access pattern and straightforward implementation. Our method combines the advantages of both: we leverage the simplicity of regular grids while still achieving practical and controllable spatial adaptivity. We demonstrate that our method enables adaptive simulations that are fast, flexible, and robust to null-space issues. At the same time, our method is simple to implement and takes advantage of existing highly-tuned algorithms.},
  archive      = {J_TVCG},
  author       = {Hikaru Ibayashi and Chris Wojtan and Nils Thuerey and Takeo Igarashi and Ryoichi Ando},
  doi          = {10.1109/TVCG.2018.2883628},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2288-2302},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Simulating liquids on dynamically warping grids},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scene transitions and teleportation in virtual reality and
the implications for spatial awareness and sickness. <em>TVCG</em>,
<em>26</em>(6), 2273–2287. (<a
href="https://doi.org/10.1109/TVCG.2018.2884468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various viewing and travel techniques are used in immersive virtual reality to allow users to see different areas or perspectives of 3D environments. Our research evaluates techniques for visually showing transitions between two viewpoints in head-tracked virtual reality. We present four experiments that focus on automated viewpoint changes that are controlled by the system rather than by interactive user control. The experiments evaluate three different transition techniques (teleportation, animated interpolation, and pulsed interpolation), different types of visual adjustments for each technique, and different types of viewpoint changes. We evaluated how differences in transition can influence a viewer&#39;s comfort, sickness, and ability to maintain spatial awareness of dynamic objects in a virtual scene. For instant teleportations, the experiments found participants could most easily track scene changes with rotational transitions without translational movements. Among the tested techniques, animated interpolations allowed significantly better spatial awareness of moving objects, but the animated technique was also rated worst in terms of sickness, particularly for rotational viewpoint changes. Across techniques, viewpoint transitions involving both translational and rotational changes together were more difficult to track than either individual type of change.},
  archive      = {J_TVCG},
  author       = {Kasra Rahimi and Colin Banigan and Eric D. Ragan},
  doi          = {10.1109/TVCG.2018.2884468},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2273-2287},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scene transitions and teleportation in virtual reality and the implications for spatial awareness and sickness},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perceptually validated cross-renderer analytical BRDF
parameter remapping. <em>TVCG</em>, <em>26</em>(6), 2258–2272. (<a
href="https://doi.org/10.1109/TVCG.2018.2886877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material appearance of rendered objects depends on the underlying BRDF implementation used by rendering software packages. A lack of standards to exchange material parameters and data (between tools) means that artists in digital 3D prototyping and design, manually match the appearance of materials to a reference image. Since their effect on rendered output is often non-uniform and counter intuitive, selecting appropriate parameterisations for BRDF models is far from straightforward. We present a novel BRDF remapping technique, that automatically computes a mapping (BRDF Difference Probe) to match the appearance of a source material model to a target one. Through quantitative analysis, four user studies and psychometric scaling experiments, we validate our remapping framework and demonstrate that it yields a visually faithful remapping among analytical BRDFs. Most notably, our results show that even when the characteristics of the models are substantially different, such as in the case of a phenomenological model and a physically-based one, our remapped renderings are indistinguishable from the original source model.},
  archive      = {J_TVCG},
  author       = {Dar&#39;ya Guarnera and Giuseppe Claudio Guarnera and Matteo Toscani and Mashhuda Glencross and Baihua Li and Jon Yngve Hardeberg and Karl R. Gegenfurtner},
  doi          = {10.1109/TVCG.2018.2886877},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2258-2272},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptually validated cross-renderer analytical BRDF parameter remapping},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interaction driven enhancement of depth perception in
angiographic volumes. <em>TVCG</em>, <em>26</em>(6), 2247–2257. (<a
href="https://doi.org/10.1109/TVCG.2018.2884940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User interaction has the potential to greatly facilitate the exploration and understanding of 3D medical images for diagnosis and treatment. However, in certain specialized environments such as in an operating room (OR), technical and physical constraints such as the need to enforce strict sterility rules, make interaction challenging. In this paper, we propose to facilitate the intraoperative exploration of angiographic volumes by leveraging the motion of a tracked surgical pointer, a tool that is already manipulated by the surgeon when using a navigation system in the OR. We designed and implemented three interactive rendering techniques based on this principle. The benefit of each of these techniques is compared to its non-interactive counterpart in a psychophysics experiment where 20 medical imaging experts were asked to perform a reaching/targeting task while visualizing a 3D volume of angiographic data. The study showed a significant improvement of the appreciation of local vascular structure when using dynamic techniques, while not having a negative impact on the appreciation of the global structure and only a marginal impact on the execution speed. A qualitative evaluation of the different techniques showed a preference for dynamic chroma-depth in accordance with the objective metrics but a discrepancy between objective and subjective measures for dynamic aerial perspective and shading.},
  archive      = {J_TVCG},
  author       = {Simon Drouin and Daniel A. Di Giovanni and Marta Kersten-Oertel and D. Louis Collins},
  doi          = {10.1109/TVCG.2018.2884940},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2247-2257},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interaction driven enhancement of depth perception in angiographic volumes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fluid simulation with adaptive staggered power particles on
GPUs. <em>TVCG</em>, <em>26</em>(6), 2234–2246. (<a
href="https://doi.org/10.1109/TVCG.2018.2886322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends the recently proposed power-particle-based fluid simulation method with staggered discretization, GPU implementation, and adaptive sampling, largely enhancing the efficiency and usability of the method. In contrast to the original formulation which uses co-located pressures and velocities, in this paper, a staggered scheme is adapted to the Power Particles to benefit visual details and computing efficiency. Meanwhile, we propose a novel facet-based power diagrams construction algorithm suitable for parallelization and explore its GPU implementation, achieving an order of magnitude boost in performance over the existing code library. In addition, to utilize the potential of Power Particles to control individual cell volume, we apply adaptive particle sampling to improve the detail level with varying resolution. The proposed method can be entirely carried out on GPUs, and our extensive experiments validate our method both in terms of efficiency and visual quality.},
  archive      = {J_TVCG},
  author       = {Xiao Zhai and Fei Hou and Hong Qin and Aimin Hao},
  doi          = {10.1109/TVCG.2018.2886322},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2234-2246},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fluid simulation with adaptive staggered power particles on GPUs},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature tracking by two-step optimization. <em>TVCG</em>,
<em>26</em>(6), 2219–2233. (<a
href="https://doi.org/10.1109/TVCG.2018.2883630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking the temporal evolution of features in time-varying data is a key method in visualization. For typical feature definitions, such as vortices, objects are sparsely distributed over the data domain. In this paper, we present a novel approach for tracking both sparse and space-filling features. While the former comprise only a small fraction of the domain, the latter form a set of objects whose union covers the domain entirely while the individual objects are mutually disjunct. Our approach determines the assignment of features between two successive time-steps by solving two graph optimization problems. It first resolves one-to-one assignments of features by computing a maximum-weight, maximum-cardinality matching on a weighted bi-partite graph. Second, our algorithm detects events by creating a graph of potentially conflicting event explanations and finding a weighted, independent set in it. We demonstrate our method&#39;s effectiveness on synthetic and simulation data sets, the former of which enables quantitative evaluation because of the availability of ground-truth information. Here, our method performs on par or better than a well-established reference algorithm. In addition, manual visual inspection by our collaborators confirm the results&#39; plausibility for simulation data.},
  archive      = {J_TVCG},
  author       = {Andrea Schnorr and Dirk N. Helmrich and Dominik Denker and Torsten W. Kuhlen and Bernd Hentschel},
  doi          = {10.1109/TVCG.2018.2883630},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2219-2233},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Feature tracking by two-step optimization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distinction of 3D objects and scenes via classification
network and markov random field. <em>TVCG</em>, <em>26</em>(6),
2204–2218. (<a href="https://doi.org/10.1109/TVCG.2018.2885750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An importance measure of 3D objects inspired by human perception has a range of applications since people want computers to behave like humans in many tasks. This paper revisits a well-defined measure, distinction of 3D surface mesh, which indicates how important a region of a mesh is with respect to classification. We develop a method to compute it based on a classification network and a Markov Random Field (MRF). The classification network learns view-based distinction by handling multiple views of a 3D object. Using a classification network has an advantage of avoiding the training data problem which has become a major obstacle of applying deep learning to 3D object understanding tasks. The MRF estimates the parameters of a linear model for combining the view-based distinction maps. The experiments using several publicly accessible datasets show that the distinctive regions detected by our method are not just significantly different from those detected by methods based on handcrafted features, but more consistent with human perception. We also compare it with other perceptual measures and quantitatively evaluate its performance in the context of two applications. Furthermore, due to the view-based nature of our method, we are able to easily extend mesh distinction to 3D scenes containing multiple objects.},
  archive      = {J_TVCG},
  author       = {Ran Song and Yonghuai Liu and Paul L. Rosin},
  doi          = {10.1109/TVCG.2018.2885750},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2204-2218},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Distinction of 3D objects and scenes via classification network and markov random field},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A user-centered design study in scientific visualization
targeting domain experts. <em>TVCG</em>, <em>26</em>(6), 2192–2203. (<a
href="https://doi.org/10.1109/TVCG.2020.2970525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of usable visualization solutions is essential for ensuring both their adoption and effectiveness. User-centered design principles, which involve users throughout the entire development process, have been shown to be effective in numerous information visualization endeavors. We describe how we applied these principles in scientific visualization over a two year collaboration to develop a hybrid in situ/post hoc solution tailored towards combustion researcher needs. Furthermore, we examine the importance of user-centered design and lessons learned over the design process in an effort to aid others seeking to develop effective scientific visualization solutions.},
  archive      = {J_TVCG},
  author       = {Yucong Ye and Franz Sauer and Kwan-Liu Ma and Konduri Aditya and Jacqueline Chen},
  doi          = {10.1109/TVCG.2020.2970525},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2192-2203},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A user-centered design study in scientific visualization targeting domain experts},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The sprawlter graph readability metric: Combining sprawl and
area-aware clutter. <em>TVCG</em>, <em>26</em>(6), 2180–2191. (<a
href="https://doi.org/10.1109/TVCG.2020.2970523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph drawing readability metrics are routinely used to assess and create node-link layouts of network data. Existing readability metrics fall short in three ways. The many count-based metrics such as edge-edge or node-edge crossings simply provide integer counts, missing the opportunity to quantify the amount of overlap between items, which may vary in size, at a more fine-grained level. Current metrics focus solely on single-level topological structure, ignoring the possibility of multi-level structure such as large and thus highly salient metanodes. Most current metrics focus on the measurement of clutter in the form of crossings and overlaps, and do not take into account the trade-off between the clutter and the information sparsity of the drawing, which we refer to as sprawl. We propose an area-aware approach to clutter metrics that tracks the extent of geometric overlaps between node-node, node-edge, and edge-edge pairs in detail. It handles variable-size nodes and explicitly treats metanodes and leaf nodes uniformly. We call the combination of a sprawl metric and an area-aware clutter metric a sprawlter metric. We present an instantiation of the sprawlter metrics featuring a formal and thorough discussion of the crucial component, the penalty mapping function. We implement and validate our proposed metrics with extensive computational analysis of graph layouts, considering four layout algorithms and 56 layouts encompassing both real-world data and synthetic examples illustrating specific configurations of interest.},
  archive      = {J_TVCG},
  author       = {Zipeng Liu and Takayuki Itoh and Jessica Q. Dawson and Tamara Munzner},
  doi          = {10.1109/TVCG.2020.2970523},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2180-2191},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The sprawlter graph readability metric: Combining sprawl and area-aware clutter},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Touch? Speech? Or touch and speech? Investigating multimodal
interaction for visual network exploration and analysis. <em>TVCG</em>,
<em>26</em>(6), 2168–2179. (<a
href="https://doi.org/10.1109/TVCG.2020.2970512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction plays a vital role during visual network exploration as users need to engage with both elements in the view (e.g., nodes, links) and interface controls (e.g., sliders, dropdown menus). Particularly as the size and complexity of a network grow, interactive displays supporting multimodal input (e.g., touch, speech, pen, gaze) exhibit the potential to facilitate fluid interaction during visual network exploration and analysis. While multimodal interaction with network visualization seems like a promising idea, many open questions remain. For instance, do users actually prefer multimodal input over unimodal input, and if so, why? Does it enable them to interact more naturally, or does having multiple modes of input confuse users? To answer such questions, we conducted a qualitative user study in the context of a network visualization tool, comparing speech- and touch-based unimodal interfaces to a multimodal interface combining the two. Our results confirm that participants strongly prefer multimodal input over unimodal input attributing their preference to: 1) the freedom of expression, 2) the complementary nature of speech and touch, and 3) integrated interactions afforded by the combination of the two modalities. We also describe the interaction patterns participants employed to perform common network visualization operations and highlight themes for future multimodal network visualization systems to consider.},
  archive      = {J_TVCG},
  author       = {Ayshwarya Saktheeswaran and Arjun Srinivasan and John Stasko},
  doi          = {10.1109/TVCG.2020.2970512},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2168-2179},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Touch? speech? or touch and speech? investigating multimodal interaction for visual network exploration and analysis},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Photographic high-dynamic-range scalar visualization.
<em>TVCG</em>, <em>26</em>(6), 2156–2167. (<a
href="https://doi.org/10.1109/TVCG.2020.2970522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a photographic method to show scalar values of high dynamic range (HDR) by color mapping for 2D visualization. We combine (1) tone-mapping operators that transform the data to the display range of the monitor while preserving perceptually important features, based on a systematic evaluation, and (2) simulated glares that highlight high-value regions. Simulated glares are effective for highlighting small areas (of a few pixels) that may not be visible with conventional visualizations; through a controlled perception study, we confirm that glare is preattentive. The usefulness of our overall photographic HDR visualization is validated through the feedback of expert users.},
  archive      = {J_TVCG},
  author       = {Liang Zhou and Marc Rivinius and Chris R. Johnson and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2020.2970522},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2156-2167},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Photographic high-dynamic-range scalar visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling how humans judge dot-label relations in point cloud
visualizations. <em>TVCG</em>, <em>26</em>(6), 2144–2155. (<a
href="https://doi.org/10.1109/TVCG.2020.2970509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When point clouds are labeled in information visualization applications, sophisticated guidelines as in cartography do not yet exist. Existing naive strategies may mislead as to which points belong to which label. To inform improved strategies, we studied factors influencing this phenomenon. We derived a class of labeled point cloud representations from existing applications and we defined different models predicting how humans interpret such complex representations, focusing on their geometric properties. We conducted an empirical study, in which participants had to relate dots to labels in order to evaluate how well our models predict. Our results indicate that presence of point clusters, label size, and angle to the label have an effect on participants&#39; judgment as well as that the distance measure types considered perform differently discouraging the use of label centers as reference points.},
  archive      = {J_TVCG},
  author       = {Martin Reckziegel and Linda Pfeiffer and Christian Heine and Stefan Jänicke},
  doi          = {10.1109/TVCG.2020.2970509},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2144-2155},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling how humans judge dot-label relations in point cloud visualizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editors’ introduction: Special section on IEEE
PacificVis 2020. <em>TVCG</em>, <em>26</em>(6), 2142–2143. (<a
href="https://doi.org/10.1109/TVCG.2020.2974638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The five papers in this special section were from the 2020 IEEE Pacific Visualization Symposium (IEEE PacificVis), which was scheduled to be hosted by Tianjin University and held in Tianjin, China, from April 14 to 17, 2020.},
  archive      = {J_TVCG},
  author       = {Fabian Beck and Jinwook Seo and Chaoli Wang},
  doi          = {10.1109/TVCG.2020.2974638},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2142-2143},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Guest editors’ introduction: Special section on IEEE PacificVis 2020},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Editor’s note. <em>TVCG</em>, <em>26</em>(6), 2135–2141.
(<a href="https://doi.org/10.1109/TVCG.2020.2973745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2020.2973745},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2135-2141},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Editor&#39;s note},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conference committee. <em>TVCG</em>, <em>26</em>(5), viii.
(<a href="https://doi.org/10.1109/TVCG.2020.2978990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2020.2978990},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {viii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Conference committee},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preface. <em>TVCG</em>, <em>26</em>(5), vi. (<a
href="https://doi.org/10.1109/TVCG.2020.2978987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present a subset of papers from the 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2020), held March 22–26, 2020, in Atlanta, Georgia.},
  archive      = {J_TVCG},
  author       = {Maud Marchal and Joseph L. Gabbard and Joaquim Jorge and Torsten W. Kuhlen and Anthony Steed},
  doi          = {10.1109/TVCG.2020.2978987},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {vi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preface},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Introducing the IEEE virtual reality 2020 special issue.
<em>TVCG</em>, <em>26</em>(5), iv–v. (<a
href="https://doi.org/10.1109/TVCG.2020.2978971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the 9th IEEE Transactions on Visualization and Computer Graphics (TVCG ) special issue on IEEE Virtual Reality and 3D User Interfaces. This volume contains a total of 29 full papers selected for and presented at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2020) held in Atlanta, United States on March 22–26, 2020. Founded in 1993, IEEE VR has a long tradition as the premier venue where new research results in the field of Virtual Reality (VR) are presented. With the emergence of VR as a major technology in a diverse set of fields, such as entertainment, education, data analytics, artificial intelligence, medicine, construction, training, and many others, the papers presented at IEEE VR and published in the IEEE TVCG VR special issue mark a major highlight of the year.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller and Doug Bowman},
  doi          = {10.1109/TVCG.2020.2978971},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {iv-v},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Introducing the IEEE virtual reality 2020 special issue},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward standardized classification of foveated displays.
<em>TVCG</em>, <em>26</em>(5), 2126–2134. (<a
href="https://doi.org/10.1109/TVCG.2020.2973053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergent in the field of head mounted display design is a desire to leverage the limitations of the human visual system to reduce the computation, communication, and display workload in power and form-factor constrained systems. Fundamental to this reduced workload is the ability to match display resolution to the acuity of the human visual system, along with a resulting need to follow the gaze of the eye as it moves, a process referred to as foveation . A display that moves its content along with the eye may be called a Foveated Display , though this term is also commonly used to describe displays with non-uniform resolution that attempt to mimic human visual acuity. We therefore recommend a definition for the term Foveated Display that accepts both of these interpretations. Furthermore, we include a simplified model for human visual Acuity Distribution Functions (ADFs) at various levels of visual acuity, across wide fields of view and propose comparison of this ADF with the Resolution Distribution Function of a foveated display for evaluation of its resolution at a particular gaze direction. We also provide a taxonomy to allow the field to meaningfully compare and contrast various aspects of foveated displays in a display and optical technology-agnostic manner.},
  archive      = {J_TVCG},
  author       = {Josef Spjut and Ben Boudaoud and Jonghyun Kim and Trey Greer and Rachel Albert and Michael Stengel and Kaan Akşit and David Luebke},
  doi          = {10.1109/TVCG.2020.2973053},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2126-2134},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Toward standardized classification of foveated displays},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Presence, mixed reality, and risk-taking behavior: A study
in safety interventions. <em>TVCG</em>, <em>26</em>(5), 2115–2125. (<a
href="https://doi.org/10.1109/TVCG.2020.2973055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive environments have been successfully applied to a broad range of safety training in high-risk domains. However, very little research has used these systems to evaluate the risk-taking behavior of construction workers. In this study, we investigated the feasibility and usefulness of providing passive haptics in a mixed-reality environment to capture the risk-taking behavior of workers, identify at-risk workers, and propose injury-prevention interventions to counteract excessive risk-taking and risk-compensatory behavior. Within a mixed-reality environment in a CAVE-like display system, our subjects installed shingles on a (physical) sloped roof of a (virtual) two-story residential building on a morning in a suburban area. Through this controlled, within-subject experimental design, we exposed each subject to three experimental conditions by manipulating the level of safety intervention. Workers&#39; subjective reports, physiological signals, psychophysical responses, and reactionary behaviors were then considered as promising measures of Presence. The results showed that our mixed-reality environment was a suitable platform for triggering behavioral changes under different experimental conditions and for evaluating the risk perception and risk-taking behavior of workers in a risk-free setting. These results demonstrated the value of immersive technology to investigate natural human factors.},
  archive      = {J_TVCG},
  author       = {Sogand Hasanzadeh and Nicholas F. Polys and Jesus M. de la Garza},
  doi          = {10.1109/TVCG.2020.2973055},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2115-2125},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Presence, mixed reality, and risk-taking behavior: A study in safety interventions},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Immersive process model exploration in virtual reality.
<em>TVCG</em>, <em>26</em>(5), 2104–2114. (<a
href="https://doi.org/10.1109/TVCG.2020.2973476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many professional domains, relevant processes are documented as abstract process models, such as event-driven process chains (EPCs). EPCs are traditionally visualized as 2D graphs and their size varies with the complexity of the process. While process modeling experts are used to interpreting complex 2D EPCs, in certain scenarios such as, for example, professional training or education, also novice users inexperienced in interpreting 2D EPC data are facing the challenge of learning and understanding complex process models. To communicate process knowledge in an effective yet motivating and interesting way, we propose a novel virtual reality (VR) interface for non-expert users. Our proposed system turns the exploration of arbitrarily complex EPCs into an interactive and multi-sensory VR experience. It automatically generates a virtual 3D environment from a process model and lets users explore processes through a combination of natural walking and teleportation. Our immersive interface leverages basic gamification in the form of a logical walkthrough mode to motivate users to interact with the virtual process. The generated user experience is entirely novel in the field of immersive data exploration and supported by a combination of visual, auditory, vibrotactile and passive haptic feedback. In a user study with $\mathrm{N}=27$ novice users, we evaluate the effect of our proposed system on process model understandability and user experience, while comparing it to a traditional 2D interface on a tablet device. The results indicate a tradeoff between efficiency and user interest as assessed by the UEQ novelty subscale, while no significant decrease in model understanding performance was found using the proposed VR interface. Our investigation highlights the potential of multi-sensory VR for less time-critical professional application domains, such as employee training, communication, education, and related scenarios focusing on user interest.},
  archive      = {J_TVCG},
  author       = {André Zenner and Akhmajon Makhsadov and Sören Klingner and David Liebemann and Antonio Krüger},
  doi          = {10.1109/TVCG.2020.2973476},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2104-2114},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersive process model exploration in virtual reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pseudo-haptic display of mass and mass distribution during
object rotation in virtual reality. <em>TVCG</em>, <em>26</em>(5),
2094–2103. (<a href="https://doi.org/10.1109/TVCG.2020.2973056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and evaluate novel pseudo-haptic techniques to display mass and mass distribution for proxy-based object manipulation in virtual reality. These techniques are specifically designed to generate haptic effects during the object&#39;s rotation. They rely on manipulating the mapping between visual cues of motion and kinesthetic cues of force to generate a sense of heaviness, which alters the perception of the object&#39;s mass-related properties without changing the physical proxy. First we present a technique to display an object&#39;s mass by scaling its rotational motion relative to its mass. A psycho-physical experiment demonstrates that this technique effectively generates correct perceptions of relative mass between two virtual objects. We then present two pseudo-haptic techniques designed to display an object&#39;s mass distribution. One of them relies on manipulating the pivot point of rotation, while the other adjusts rotational motion based on the real-time dynamics of the moving object. An empirical study shows that both techniques can influence perception of mass distribution, with the second technique being significantly more effective.},
  archive      = {J_TVCG},
  author       = {Run Yu and Doug A. Bowman},
  doi          = {10.1109/TVCG.2020.2973056},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2094-2103},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pseudo-haptic display of mass and mass distribution during object rotation in virtual reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EarVR: Using ear haptics in virtual reality for deaf and
hard-of-hearing people. <em>TVCG</em>, <em>26</em>(5), 2084–2093. (<a
href="https://doi.org/10.1109/TVCG.2020.2973441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has a great potential to improve skills of Deaf and Hard-of-Hearing (DHH) people. Most VR applications and devices are designed for persons without hearing problems. Therefore, DHH persons have many limitations when using VR. Adding special features in a VR environment, such as subtitles, or haptic devices will help them. Previously, it was necessary to design a special VR environment for DHH persons. We introduce and evaluate a new prototype called “EarVR” that can be mounted on any desktop or mobile VR Head-Mounted Display (HMD). EarVR analyzes 3D sounds in a VR environment and locates the direction of the sound source that is closest to a user. It notifies the user about the sound direction using two vibro-motors placed on the user&#39;s ears. EarVR helps DHH persons to complete sound-based VR tasks in any VR application with 3D audio and a mute option for background music. Therefore, DHH persons can use all VR applications with 3D audio, not only those applications designed for them. Our user study shows that DHH participants were able to complete a simple VR task significantly faster with EarVR than without. The completion time of DHH participants was very close to participants without hearing problems. Also, it shows that DHH participants were able to finish a complex VR task with EarVR, while without it, they could not finish the task even once. Finally, our qualitative and quantitative evaluation among DHH participants indicates that they preferred to use EarVR and it encouraged them to use VR technology more.},
  archive      = {J_TVCG},
  author       = {Mohammadreza Mirzaei and Peter Kán and Hannes Kaufmann},
  doi          = {10.1109/TVCG.2020.2973441},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2084-2093},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EarVR: Using ear haptics in virtual reality for deaf and hard-of-hearing people},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Animals in virtual environments. <em>TVCG</em>,
<em>26</em>(5), 2073–2083. (<a
href="https://doi.org/10.1109/TVCG.2020.2973063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core idea in an XR (VR/MR/AR) application is to digitally stimulate one or more sensory systems (e.g. visual, auditory, olfactory) of the human user in an interactive way to achieve an immersive experience. Since the early 2000s biologists have been using Virtual Environments (VE) to investigate the mechanisms of behavior in non-human animals including insects, fish, and mammals. VEs have become reliable tools for studying vision, cognition, and sensory-motor control in animals. In turn, the knowledge gained from studying such behaviors can be harnessed by researchers designing biologically inspired robots, smart sensors, and rnulti-agent artificial intelligence. VE for animals is becoming a widely used application of XR technology but such applications have not previously been reported in the technical literature related to XR. Biologists and computer scientists can benefit greatly from deepening interdisciplinary research in this emerging field and together we can develop new methods for conducting fundamental research in behavioral sciences and engineering. To support our argument we present this review which provides an overview of animal behavior experiments conducted in virtual environments.},
  archive      = {J_TVCG},
  author       = {Hemal Naik and Renaud Bastien and Nassir Navab and Iain D Couzin},
  doi          = {10.1109/TVCG.2020.2973063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2073-2083},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Animals in virtual environments},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Avatar and sense of embodiment: Studying the relative
preference between appearance, control and point of view. <em>TVCG</em>,
<em>26</em>(5), 2062–2072. (<a
href="https://doi.org/10.1109/TVCG.2020.2973077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Virtual Reality, a number of studies have been conducted to assess the influence of avatar appearance, avatar control and user point of view on the Sense of Embodiment (SoE) towards a virtual avatar. However, such studies tend to explore each factor in isolation. This paper aims to better understand the inter-relations among these three factors by conducting a subjective matching experiment. In the presented experiment (n=40), participants had to match a given “optimal” SoE avatar configuration (realistic avatar, full-body motion capture, first-person point of view), starting by a “minimal” SoE configuration (minimal avatar, no control, third-person point of view), by iteratively increasing the level of each factor. The choices of the participants provide insights about their preferences and perception over the three factors considered. Moreover, the subjective matching procedure was conducted in the context of four different interaction tasks with the goal of covering a wide range of actions an avatar can do in a VE. The paper also describes a baseline experiment (n=20) which was used to define the number and order of the different levels for each factor, prior to the subjective matching experiment (e.g. different degrees of realism ranging from abstract to personalised avatars for the visual appearance). The results of the subjective matching experiment show that point of view and control levels were consistently increased by users before appearance levels when it comes to enhancing the SoE. Second, several configurations were identified with equivalent SoE as the one felt in the optimal configuration, but vary between the tasks. Taken together, our results provide valuable insights about which factors to prioritize in order to enhance the SoE towards an avatar in different tasks, and about configurations which lead to fulfilling SoE in VE.},
  archive      = {J_TVCG},
  author       = {Rebecca Fribourg and Ferran Argelaguet and Anatole Lécuyer and Ludovic Hoyet},
  doi          = {10.1109/TVCG.2020.2973077},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2062-2072},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Avatar and sense of embodiment: Studying the relative preference between appearance, control and point of view},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Illuminated focus: Vision augmentation using spatial
defocusing via focal sweep eyeglasses and high-speed projector.
<em>TVCG</em>, <em>26</em>(5), 2051–2061. (<a
href="https://doi.org/10.1109/TVCG.2020.2973496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at realizing novel vision augmentation experiences, this paper proposes the IlluminatedFocus technique, which spatially defocuses real-world appearances regardless of the distance from the user&#39;s eyes to observed real objects. With the proposed technique, a part of a real object in an image appears blurred, while the fine details of the other part at the same distance remain visible. We apply Electrically Focus-Tunable Lenses (ETL) as eyeglasses and a synchronized high-speed projector as illumination for a real scene. We periodically modulate the focal lengths of the glasses (focal sweep) at more than 60 Hz so that a wearer cannot perceive the modulation. A part of the scene to appear focused is illuminated by the projector when it is in focus of the user&#39;s eyes, while another part to appear blurred is illuminated when it is out of the focus. As the basis of our spatial focus control, we build mathematical models to predict the range of distance from the ETL within which real objects become blurred on the retina of a user. Based on the blur range, we discuss a design guideline for effective illumination timing and focal sweep range. We also model the apparent size of a real scene altered by the focal length modulation. This leads to an undesirable visible seam between focused and blurred areas. We solve this unique problem by gradually blending the two areas. Finally, we demonstrate the feasibility of our proposal by implementing various vision augmentation applications.},
  archive      = {J_TVCG},
  author       = {Tatsuyuki Ueda and Daisuke Iwai and Takefumi Hiraki and Kosuke Sato},
  doi          = {10.1109/TVCG.2020.2973496},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2051-2061},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Illuminated focus: Vision augmentation using spatial defocusing via focal sweep eyeglasses and high-speed projector},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On motor performance in virtual 3D object manipulation.
<em>TVCG</em>, <em>26</em>(5), 2041–2050. (<a
href="https://doi.org/10.1109/TVCG.2020.2973034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitts&#39;s law facilitates approximate comparisons of target acquisition performance across a variety of settings. Conceptually, also the index of difficulty of 3D object manipulation with six degrees of freedom can be computed, which allows the comparison of results from different studies. Prior experiments, however, often revealed much worse performance than one would reasonably expect on this basis. We argue that this discrepancy stems from confounding variables and show how Fitts&#39;s law and related research methods can be applied to isolate and identify relevant factors of motor performance in 3D manipulation tasks. The results of a formal user study (n=21) demonstrate competitive performance in compliance with Fitts&#39;s model and provide empirical evidence that simultaneous 3D rotation and translation can be beneficial.},
  archive      = {J_TVCG},
  author       = {Alexander Kulik and André Kunert and Bernd Froehlich},
  doi          = {10.1109/TVCG.2020.2973034},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2041-2050},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On motor performance in virtual 3D object manipulation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FibAR: Embedding optical fibers in 3D printed objects for
active markers in dynamic projection mapping. <em>TVCG</em>,
<em>26</em>(5), 2030–2040. (<a
href="https://doi.org/10.1109/TVCG.2020.2973444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel active marker for dynamic projection mapping (PM) that emits a temporal blinking pattern of infrared (IR) light representing its ID. We used a multi-material three dimensional (3D) printer to fabricate a projection object with optical fibers that can guide IR light from LEDs attached on the bottom of the object. The aperture of an optical fiber is typically very small; thus, it is unnoticeable to human observers under projection and can be placed on a strongly curved part of a projection surface. In addition, the working range of our system can be larger than previous marker-based methods as the blinking patterns can theoretically be recognized by a camera placed at a wide range of distances from markers. We propose an automatic marker placement algorithm to spread multiple active markers over the surface of a projection object such that its pose can be robustly estimated using captured images from arbitrary directions. We also propose an optimization framework for determining the routes of the optical fibers in such a way that collisions of the fibers can be avoided while minimizing the loss of light intensity in the fibers. Through experiments conducted using three fabricated objects containing strongly curved surfaces, we confirmed that the proposed method can achieve accurate dynamic PMs in a significantly wide working range.},
  archive      = {J_TVCG},
  author       = {Daiki Tone and Daisuke Iwai and Shinsaku Hiura and Kosuke Sato},
  doi          = {10.1109/TVCG.2020.2973444},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2030-2040},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FibAR: Embedding optical fibers in 3D printed objects for active markers in dynamic projection mapping},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using facial animation to increase the enfacement illusion
and avatar self-identification. <em>TVCG</em>, <em>26</em>(5),
2023–2029. (<a href="https://doi.org/10.1109/TVCG.2020.2973075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through avatar embodiment in Virtual Reality (VR) we can achieve the illusion that an avatar is substituting our body: the avatar moves as we move and we see it from a first person perspective. However, self-identification, the process of identifying a representation as being oneself, poses new challenges because a key determinant is that we see and have agency in our own face. Providing control over the face is hard with current HMD technologies because face tracking is either cumbersome or error prone. However, limited animation is easily achieved based on speaking. We investigate the level of avatar enfacement, that is believing that a picture of a face is one&#39;s own face, with three levels of facial animation: (i) one in which the facial expressions of the avatars are static, (ii) one in which we implement lip-sync motion and (iii) one in which the avatar presents lip-sync plus additional facial animations, with blinks, designed by a professional animator. We measure self-identification using a face morphing tool that morphs from the face of the participant to the face of a gender matched avatar. We find that self-identification on avatars can be increased through pre-baked animations even when these are not photorealistic nor look like the participant.},
  archive      = {J_TVCG},
  author       = {Mar Gonzalez-Franco and Anthony Steed and Steve Hoogendyk and Eyal Ofek},
  doi          = {10.1109/TVCG.2020.2973075},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2023-2029},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Using facial animation to increase the enfacement illusion and avatar self-identification},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Live semantic 3D perception for immersive augmented reality.
<em>TVCG</em>, <em>26</em>(5), 2012–2022. (<a
href="https://doi.org/10.1109/TVCG.2020.2973477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic understanding of 3D environments is critical for both the unmanned system and the human involved virtual/augmented reality (VR/AR) immersive experience. Spatially-sparse convolution, taking advantage of the intrinsic sparsity of 3D point cloud data, makes high resolution 3D convolutional neural networks tractable with state-of-the-art results on 3D semantic segmentation problems. However, the exhaustive computations limits the practical usage of semantic 3D perception for VR/AR applications in portable devices. In this paper, we identify that the efficiency bottleneck lies in the unorganized memory access of the sparse convolution steps, i.e., the points are stored independently based on a predefined dictionary, which is inefficient due to the limited memory bandwidth of parallel computing devices (GPU). With the insight that points are continuous as 2D surfaces in 3D space, a chunk-based sparse convolution scheme is proposed to reuse the neighboring points within each spatially organized chunk. An efficient multi-layer adaptive fusion module is further proposed for employing the spatial consistency cue of 3D data to further reduce the computational burden. Quantitative experiments on public datasets demonstrate that our approach works 11× faster than previous approaches with competitive accuracy. By implementing both semantic and geometric 3D reconstruction simultaneously on a portable tablet device, we demo a foundation platform for immersive AR applications.},
  archive      = {J_TVCG},
  author       = {Lei Han and Tian Zheng and Yinheng Zhu and Lan Xu and Lu Fang},
  doi          = {10.1109/TVCG.2020.2973477},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2012-2022},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Live semantic 3D perception for immersive augmented reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Physically-inspired deep light estimation from a
homogeneous-material object for mixed reality lighting. <em>TVCG</em>,
<em>26</em>(5), 2002–2011. (<a
href="https://doi.org/10.1109/TVCG.2020.2973050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mixed reality (MR), augmenting virtual objects consistently with real-world illumination is one of the key factors that provide a realistic and immersive user experience. For this purpose, we propose a novel deep learning-based method to estimate high dynamic range (HDR) illumination from a single RGB image of a reference object. To obtain illumination of a current scene, previous approaches inserted a special camera in that scene, which may interfere with user&#39;s immersion, or they analyzed reflected radiances from a passive light probe with a specific type of materials or a known shape. The proposed method does not require any additional gadgets or strong prior cues, and aims to predict illumination from a single image of an observed object with a wide range of homogeneous materials and shapes. To effectively solve this ill-posed inverse rendering problem, three sequential deep neural networks are employed based on a physically-inspired design. These networks perform end-to-end regression to gradually decrease dependency on the material and shape. To cover various conditions, the proposed networks are trained on a large synthetic dataset generated by physically-based rendering. Finally, the reconstructed HDR illumination enables realistic image-based lighting of virtual objects in MR. Experimental results demonstrate the effectiveness of this approach compared against state-of-the-art methods. The paper also suggests some interesting MR applications in indoor and outdoor scenes.},
  archive      = {J_TVCG},
  author       = {Jinwoo Park and Hunmin Park and Sung-Eui Yoon and Woontack Woo},
  doi          = {10.1109/TVCG.2020.2973050},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2002-2011},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Physically-inspired deep light estimation from a homogeneous-material object for mixed reality lighting},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scene-aware audio rendering via deep acoustic analysis.
<em>TVCG</em>, <em>26</em>(5), 1991–2001. (<a
href="https://doi.org/10.1109/TVCG.2020.2973058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new method to capture the acoustic characteristics of real-world rooms using commodity devices, and use the captured characteristics to generate similar sounding sources with virtual models. Given the captured audio and an approximate geometric model of a real-world room, we present a novel learning-based method to estimate its acoustic material properties. Our approach is based on deep neural networks that estimate the reverberation time and equalization of the room from recorded audio. These estimates are used to compute material properties related to room reverberation using a novel material optimization objective. We use the estimated acoustic material characteristics for audio rendering using interactive geometric sound propagation and highlight the performance on many real-world scenarios. We also perform a user study to evaluate the perceptual similarity between the recorded sounds and our rendered audio.},
  archive      = {J_TVCG},
  author       = {Zhenyu Tang and Nicholas J. Bryan and Dingzeyu Li and Timothy R. Langlois and Dinesh Manocha},
  doi          = {10.1109/TVCG.2020.2973058},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1991-2001},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scene-aware audio rendering via deep acoustic analysis},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ThinVR: Heterogeneous microlens arrays for compact, 180
degree FOV VR near-eye displays. <em>TVCG</em>, <em>26</em>(5),
1981–1990. (<a href="https://doi.org/10.1109/TVCG.2020.2973064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s Virtual Reality (VR) displays are dramatically better than the head-worn displays offered 30 years ago, but today&#39;s displays remain nearly as bulky as their predecessors in the 1980&#39;s. Also, almost all consumer VR displays today provide 90-110 degrees field of view (FOV), which is much smaller than the human visual system&#39;s FOV which extends beyond 180 degrees horizontally. In this paper, we propose ThinVR as a new approach to simultaneously address the bulk and limited FOV of head-worn VR displays. ThinVR enables a head-worn VR display to provide 180 degrees horizontal FOV in a thin, compact form factor. Our approach is to replace traditional large optics with a curved microlens array of custom-designed heterogeneous lenslets and place these in front of a curved display. We found that heterogeneous optics were crucial to make this approach work, since over a wide FOV, many lenslets are viewed off the central axis. We developed a custom optimizer for designing custom heterogeneous lenslets to ensure a sufficient eyebox while reducing distortions. The contribution includes an analysis of the design space for curved microlens arrays, implementation of physical prototypes, and an assessment of the image quality, eyebox, FOV, reduction in volume and pupil swim distortion. To our knowledge, this is the first work to demonstrate and analyze the potential for curved, heterogeneous microlens arrays to enable compact, wide FOV head-worn VR displays.},
  archive      = {J_TVCG},
  author       = {Joshua Ratcliff and Alexey Supikov and Santiago Alfaro and Ronald Azuma},
  doi          = {10.1109/TVCG.2020.2973064},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1981-1990},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ThinVR: Heterogeneous microlens arrays for compact, 180 degree FOV VR near-eye displays},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Eye-dominance-guided foveated rendering. <em>TVCG</em>,
<em>26</em>(5), 1972–1980. (<a
href="https://doi.org/10.1109/TVCG.2020.2973442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing rendering performance is critical for a wide variety of virtual reality (VR) applications. Foveated rendering is emerging as an indispensable technique for reconciling interactive frame rates with ever-higher head-mounted display resolutions. Here, we present a simple yet effective technique for further reducing the cost of foveated rendering by leveraging ocular dominance - the tendency of the human visual system to prefer scene perception from one eye over the other. Our new approach, eye-dominance-guided foveated rendering (EFR), renders the scene at a lower foveation level (with higher detail) for the dominant eye than the non-dominant eye. Compared with traditional foveated rendering, EFR can be expected to provide superior rendering performance while preserving the same level of perceived visual quality.},
  archive      = {J_TVCG},
  author       = {Xiaoxu Meng and Ruofei Du and Amitabh Varshney},
  doi          = {10.1109/TVCG.2020.2973442},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1972-1980},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eye-dominance-guided foveated rendering},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of a self-avatar, hand collocation, and hand
proximity on embodiment and stroop interference. <em>TVCG</em>,
<em>26</em>(5), 1964–1971. (<a
href="https://doi.org/10.1109/TVCG.2020.2973061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the effects of hand proximity to objects and tasks is critical for hand-held and near-hand objects. Even though self-avatars have been shown to be beneficial for various tasks in virtual environments, little research has investigated the effect of avatar hand proximity on working memory. This paper presents a between-participants user study investigating the effects of self-avatars and physical hand proximity on a common working memory task, the Stroop interference task. Results show that participants felt embodied when a self-avatar was in the scene, and that the subjective level of embodiment decreased when a participant&#39;s hands were not collocated with the avatar&#39;s hands. Furthermore, a participant&#39;s physical hand placement was significantly related to Stroop interference: proximal hands produced a significant increase in accuracy compared to non-proximal hands. Surprisingly, Stroop interference was not mediated by the existence of a self-avatar or level of embodiment.},
  archive      = {J_TVCG},
  author       = {Tabitha C. Peck and Altan Tutar},
  doi          = {10.1109/TVCG.2020.2973061},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1964-1971},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of a self-avatar, hand collocation, and hand proximity on embodiment and stroop interference},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A steering algorithm for redirected walking using
reinforcement learning. <em>TVCG</em>, <em>26</em>(5), 1955–1963. (<a
href="https://doi.org/10.1109/TVCG.2020.2973060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected Walking (RDW) steering algorithms have traditionally relied on human-engineered logic. However, recent advances in reinforcement learning (RL) have produced systems that surpass human performance on a variety of control tasks. This paper investigates the potential of using RL to develop a novel reactive steering algorithm for RDW. Our approach uses RL to train a deep neural network that directly prescribes the rotation, translation, and curvature gains to transform a virtual environment given a user&#39;s position and orientation in the tracked space. We compare our learned algorithm to steer-to-center using simulated and real paths. We found that our algorithm outperforms steer-to-center on simulated paths, and found no significant difference on distance traveled on real paths. We demonstrate that when modeled as a continuous control problem, RDW is a suitable domain for RL, and moving forward, our general framework provides a promising path towards an optimal RDW steering algorithm.},
  archive      = {J_TVCG},
  author       = {Ryan R. Strauss and Raghuram Ramanujan and Andrew Becker and Tabitha C. Peck},
  doi          = {10.1109/TVCG.2020.2973060},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1955-1963},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A steering algorithm for redirected walking using reinforcement learning},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mind the gap: The underrepresentation of female participants
and authors in virtual reality research. <em>TVCG</em>, <em>26</em>(5),
1945–1954. (<a href="https://doi.org/10.1109/TVCG.2020.2973498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common goal of human-subject experiments in virtual reality (VR) research is evaluating VR hardware and software for use by the general public. A core principle of human-subject research is that the sample included in a given study should be representative of the target population; otherwise, the conclusions drawn from the findings may be biased and may not generalize to the population of interest. In order to assess whether characteristics of participants in VR research are representative of the general public, we investigated participant demographic characteristics from human-subject experiments in the Proceedings of the IEEE Virtual Reality Conferences from 2015-2019. We also assessed the representation of female authors. In the 325 eligible manuscripts, which presented results from 365 human-subject experiments, we found evidence of significant underrepresentation of women as both participants and authors. To investigate whether this underrepresentation may bias researchers&#39; findings, we then conducted a meta-analysis and meta-regression to assess whether demographic characteristics of study participants were associated with a common outcome evaluated in VR research: the change in simulator sickness following head-mounted display VR exposure. As expected, participants in VR studies using HMDs experienced small but significant increases in simulator sickness. However, across the included studies, the change in simulator sickness was systematically associated with the proportion of female participants. We discuss the negative implications of conducting experiments on non-representative samples and provide methodological recommendations for mitigating bias in future VR research.},
  archive      = {J_TVCG},
  author       = {Tabitha C. Peck and Laura E. Sockol and Sarah M. Hancock},
  doi          = {10.1109/TVCG.2020.2973498},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1945-1954},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mind the gap: The underrepresentation of female participants and authors in virtual reality research},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effects of depth information on visual target identification
task performance in shared gaze environments. <em>TVCG</em>,
<em>26</em>(5), 1934–1944. (<a
href="https://doi.org/10.1109/TVCG.2020.2973054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human gaze awareness is important for social and collaborative interactions. Recent technological advances in augmented reality (AR) displays and sensors provide us with the means to extend collaborative spaces with real-time dynamic AR indicators of one&#39;s gaze, for example via three-dimensional cursors or rays emanating from a partner&#39;s head. However, such gaze cues are only as useful as the quality of the underlying gaze estimation and the accuracy of the display mechanism. Depending on the type of the visualization, and the characteristics of the errors, AR gaze cues could either enhance or interfere with collaborations. In this paper, we present two human-subject studies in which we investigate the influence of angular and depth errors, target distance, and the type of gaze visualization on participants&#39; performance and subjective evaluation during a collaborative task with a virtual human partner, where participants identified targets within a dynamically walking crowd. First, our results show that there is a significant difference in performance for the two gaze visualizations ray and cursor in conditions with simulated angular and depth errors: the ray visualization provided significantly faster response times and fewer errors compared to the cursor visualization. Second, our results show that under optimal conditions, among four different gaze visualization methods, a ray without depth information provides the worst performance and is rated lowest, while a combination of a ray and cursor with depth information is rated highest. We discuss the subjective and objective performance thresholds and provide guidelines for practitioners in this field.},
  archive      = {J_TVCG},
  author       = {Austin Erickson and Nahal Norouzi and Kangsoo Kim and Joseph J. LaViola and Gerd Bruder and Gregory F. Welch},
  doi          = {10.1109/TVCG.2020.2973054},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1934-1944},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of depth information on visual target identification task performance in shared gaze environments},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Augmented virtual teleportation for high-fidelity
telecollaboration. <em>TVCG</em>, <em>26</em>(5), 1923–1933. (<a
href="https://doi.org/10.1109/TVCG.2020.2973065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telecollaboration involves the teleportation of a remote collaborator to another real-world environment where their partner is located. The fidelity of the environment plays an important role for allowing corresponding spatial references in remote collaboration. We present a novel asymmetric platform, Augmented Virtual Teleportation (AVT), which provides high-fidelity telepresence of a remote VR user (VR-Traveler) into a real-world collaboration space to interact with a local AR user (AR-Host). AVT uses a 360° video camera (360-camera) that captures and live-streams the omni-directional scenes over a network. The remote VR-Traveler watching the video in a VR headset experiences live presence and co-presence in the real-world collaboration space. The VR-Traveler&#39;s movements are captured and transmitted to a 3D avatar overlaid onto the 360-camera which can be seen in the AR-Host&#39;s display. The visual and audio cues for each collaborator are synchronized in the Mixed Reality Collaboration space (MRC-space), where they can interactively edit virtual objects and collaborate in the real environment using the real objects as a reference. High fidelity, real-time rendering of virtual objects and seamless blending into the real scene allows for unique mixed reality use-case scenarios. Our working prototype has been tested with a user study to evaluate spatial presence, co-presence, and user satisfaction during telecollaboration. Possible applications of AVT are identified and proposed to guide future usage.},
  archive      = {J_TVCG},
  author       = {Taehyun Rhee and Stephen Thompson and Daniel Medeiros and Rafael dos Anjos and Andrew Chalmers},
  doi          = {10.1109/TVCG.2020.2973065},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1923-1933},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmented virtual teleportation for high-fidelity telecollaboration},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Superhuman hearing - virtual prototyping of artificial
hearing: A case study on interactions and acoustic beamforming.
<em>TVCG</em>, <em>26</em>(5), 1912–1922. (<a
href="https://doi.org/10.1109/TVCG.2020.2973059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directivity and gain in microphone array systems for hearing aids or hearable devices allow users to acoustically enhance the information of a source of interest. This source is usually positioned directly in front. This feature is called acoustic beamforming. The current study aimed to improve users&#39; interactions with beamforming via a virtual prototyping approach in immersive virtual environments (VEs). Eighteen participants took part in experimental sessions composed of a calibration procedure and a selective auditory attention voice-pairing task. Eight concurrent speakers were placed in an anechoic environment in two virtual reality (VR) scenarios. The scenarios were a purely virtual scenario and a realistic 360° audio-visual recording. Participants were asked to find an individual optimal parameterization for three different virtual beamformers: (i) head-guided, (ii) eye gaze-guided, and (iii) a novel interaction technique called dual beamformer, where head-guided is combined with an additional hand-guided beamformer. None of the participants were able to complete the task without a virtual beamformer (i.e., in normal hearing condition) due to the high complexity introduced by the experimental design. However, participants were able to correctly pair all speakers using all three proposed interaction metaphors. Providing superhuman hearing abilities in the form of a dual acoustic beamformer guided by head and hand movements resulted in statistically significant improvements in terms of pairing time, suggesting the task-relevance of interacting with multiple points of interests.},
  archive      = {J_TVCG},
  author       = {Michele Geronazzo and Luis S. Vieira and Niels Christian Nilsson and Jesper Udesen and Stefania Serafin},
  doi          = {10.1109/TVCG.2020.2973059},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1912-1922},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Superhuman hearing - virtual prototyping of artificial hearing: A case study on interactions and acoustic beamforming},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DGaze: CNN-based gaze prediction in dynamic scenes.
<em>TVCG</em>, <em>26</em>(5), 1902–1911. (<a
href="https://doi.org/10.1109/TVCG.2020.2973473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct novel analyses of users&#39; gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. We first collect 43 users&#39; eye tracking data in 5 dynamic scenes under free-viewing conditions. Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users&#39; gaze positions. Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users&#39; gaze positions. Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. In terms of realtime prediction, DGaze achieves a 22.0\% improvement over prior method in dynamic scenes and obtains an improvement of 9.5\% in static scenes, based on using the angular distance as the evaluation metric. We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. We further analyze our CNN architecture and verify the effectiveness of each component in our model. We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.},
  archive      = {J_TVCG},
  author       = {Zhiming Hu and Sheng Li and Congyi Zhang and Kangrui Yi and Guoping Wang and Dinesh Manocha},
  doi          = {10.1109/TVCG.2020.2973473},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1902-1911},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DGaze: CNN-based gaze prediction in dynamic scenes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D hand tracking in the presence of excessive motion blur.
<em>TVCG</em>, <em>26</em>(5), 1891–1901. (<a
href="https://doi.org/10.1109/TVCG.2020.2973057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a sensor-fusion method that exploits a depth camera and a gyroscope to track the articulation of a hand in the presence of excessive motion blur. In case of slow and smooth hand motions, the existing methods estimate the hand pose fairly accurately and robustly, despite challenges due to the high dimensionality of the problem, self-occlusions, uniform appearance of hand parts, etc. However, the accuracy of hand pose estimation drops considerably for fast-moving hands because the depth image is severely distorted due to motion blur. Moreover, when hands move fast, the actual hand pose is far from the one estimated in the previous frame, therefore the assumption of temporal continuity on which tracking methods rely, is not valid. In this paper, we track fast-moving hands with the combination of a gyroscope and a depth camera. As a first step, we calibrate a depth camera and a gyroscope attached to a hand so as to identify their time and pose offsets. Following that, we fuse the rotation information of the calibrated gyroscope with model-based hierarchical particle filter tracking. A series of quantitative and qualitative experiments demonstrate that the proposed method performs more accurately and robustly in the presence of motion blur, when compared to state of the art algorithms, especially in the case of very fast hand rotations.},
  archive      = {J_TVCG},
  author       = {Gabyong Park and Antonis Argyros and Juyoung Lee and Woontack Woo},
  doi          = {10.1109/TVCG.2020.2973057},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1891-1901},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D hand tracking in the presence of excessive motion blur},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The security-utility trade-off for iris authentication and
eye animation for social virtual avatars. <em>TVCG</em>, <em>26</em>(5),
1880–1890. (<a href="https://doi.org/10.1109/TVCG.2020.2973052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The gaze behavior of virtual avatars is critical to social presence and perceived eye contact during social interactions in Virtual Reality. Virtual Reality headsets are being designed with integrated eye tracking to enable compelling virtual social interactions. This paper shows that the near infra-red cameras used in eye tracking capture eye images that contain iris patterns of the user. Because iris patterns are a gold standard biometric, the current technology places the user&#39;s biometric identity at risk. Our first contribution is an optical defocus based hardware solution to remove the iris biometric from the stream of eye tracking images. We characterize the performance of this solution with different internal parameters. Our second contribution is a psychophysical experiment with a same-different task that investigates the sensitivity of users to a virtual avatar&#39;s eye movements when this solution is applied. By deriving detection threshold values, our findings provide a range of defocus parameters where the change in eye movements would go unnoticed in a conversational setting. Our third contribution is a perceptual study to determine the impact of defocus parameters on the perceived eye contact, attentiveness, naturalness, and truthfulness of the avatar. Thus, if a user wishes to protect their iris biometric, our approach provides a solution that balances biometric protection while preventing their conversation partner from perceiving a difference in the user&#39;s virtual avatar. This work is the first to develop secure eye tracking configurations for VR/AR/XR applications and motivates future work in the area.},
  archive      = {J_TVCG},
  author       = {Brendan John and Sophie Jörg and Sanjeev Koppal and Eakta Jain},
  doi          = {10.1109/TVCG.2020.2973052},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1880-1890},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The security-utility trade-off for iris authentication and eye animation for social virtual avatars},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Factored occlusion: Single spatial light modulator
occlusion-capable optical see-through augmented reality display.
<em>TVCG</em>, <em>26</em>(5), 1871–1879. (<a
href="https://doi.org/10.1109/TVCG.2020.2973443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion is a powerful visual cue that is crucial for depth perception and realism in optical see-through augmented reality (OST-AR). However, existing OST-AR systems additively overlay physical and digital content with beam combiners - an approach that does not easily support mutual occlusion, resulting in virtual objects that appear semi-transparent and unrealistic. In this work, we propose a new type of occlusion-capable OST-AR system. Rather than additively combining the real and virtual worlds, we employ a single digital micromirror device (DMD) to merge the respective light paths in a multiplicative manner. This unique approach allows us to simultaneously block light incident from the physical scene on a pixel-by-pixel basis while also modulating the light emitted by a light-emitting diode (LED) to display digital content. Our technique builds on mixed binary/continuous factorization algorithms to optimize time-multiplexed binary DMD patterns and their corresponding LED colors to approximate a target augmented reality (AR) scene. In simulations and with a prototype benchtop display, we demonstrate hard-edge occlusions, plausible shadows, and also gaze-contingent optimization of this novel display mode, which only requires a single spatial light modulator.},
  archive      = {J_TVCG},
  author       = {Brooke Krajancich and Nitish Padmanaban and Gordon Wetzstein},
  doi          = {10.1109/TVCG.2020.2973443},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1871-1879},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Factored occlusion: Single spatial light modulator occlusion-capable optical see-through augmented reality display},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Getting there together: Group navigation in distributed
virtual environments. <em>TVCG</em>, <em>26</em>(5), 1860–1870. (<a
href="https://doi.org/10.1109/TVCG.2020.2973474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyzed the design space of group navigation tasks in distributed virtual environments and present a framework consisting of techniques to form groups, distribute responsibilities, navigate together, and eventually split up again. To improve joint navigation, our work focused on an extension of the Multi-Ray Jumping technique that allows adjusting the spatial formation of two distributed users as part of the target specification process. The results of a quantitative user study showed that these adjustments lead to significant improvements in joint two-user travel, which is evidenced by more efficient travel sequences and lower task loads imposed on the navigator and the passenger. In a qualitative expert review involving all four stages of group navigation, we confirmed the effective and efficient use of our technique in a more realistic use-case scenario and concluded that remote collaboration benefits from fluent transitions between individual and group navigation.},
  archive      = {J_TVCG},
  author       = {Tim Weissker and Pauline Bimberg and Bernd Froehlich},
  doi          = {10.1109/TVCG.2020.2973474},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1860-1870},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Getting there together: Group navigation in distributed virtual environments},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weakly supervised adversarial learning for 3D human pose
estimation from point clouds. <em>TVCG</em>, <em>26</em>(5), 1851–1859.
(<a href="https://doi.org/10.1109/TVCG.2020.2973076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds-based 3D human pose estimation that aims to recover the 3D locations of human skeleton joints plays an important role in many AR/VR applications. The success of existing methods is generally built upon large scale data annotated with 3D human joints. However, it is a labor-intensive and error-prone process to annotate 3D human joints from input depth images or point clouds, due to the self-occlusion between body parts as well as the tedious annotation process on 3D point clouds. Meanwhile, it is easier to construct human pose datasets with 2D human joint annotations on depth images. To address this problem, we present a weakly supervised adversarial learning framework for 3D human pose estimation from point clouds. Compared to existing 3D human pose estimation methods from depth images or point clouds, we exploit both the weakly supervised data with only annotations of 2D human joints and fully supervised data with annotations of 3D human joints. In order to relieve the human pose ambiguity due to weak supervision, we adopt adversarial learning to ensure the recovered human pose is valid. Instead of using either 2D or 3D representations of depth images in previous methods, we exploit both point clouds and the input depth image. We adopt 2D CNN to extract 2D human joints from the input depth image, 2D human joints aid us in obtaining the initial 3D human joints and selecting effective sampling points that could reduce the computation cost of 3D human pose regression using point clouds network. The used point clouds network can narrow down the domain gap between the network input i.e. point clouds and 3D joints. Thanks to weakly supervised adversarial learning framework, our method can achieve accurate 3D human pose from point clouds. Experiments on the ITOP dataset and EVAL dataset demonstrate that our method can achieve state-of-the-art performance efficiently.},
  archive      = {J_TVCG},
  author       = {Zihao Zhang and Lei Hu and Xiaoming Deng and Shihong Xia},
  doi          = {10.1109/TVCG.2020.2973076},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1851-1859},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Weakly supervised adversarial learning for 3D human pose estimation from point clouds},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Teleporting through virtual environments: Effects of path
scale and environment scale on spatial updating. <em>TVCG</em>,
<em>26</em>(5), 1841–1850. (<a
href="https://doi.org/10.1109/TVCG.2020.2973051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality systems typically allow users to physically walk and turn, but virtual environments (VEs) often exceed the available walking space. Teleporting has become a common user interface, whereby the user aims a laser pointer to indicate the desired location, and sometimes orientation, in the VE before being transported without self-motion cues. This study evaluated the influence of rotational self-motion cues on spatial updating performance when teleporting, and whether the importance of rotational cues varies across movement scale and environment scale. Participants performed a triangle completion task by teleporting along two outbound path legs before pointing to the unmarked path origin. Rotational self-motion reduced overall errors across all levels of movement scale and environment scale, though it also introduced a slight bias toward under-rotation. The importance of rotational self-motion was exaggerated when navigating large triangles and when the surrounding environment was large. Navigating a large triangle within a small VE brought participants closer to surrounding landmarks and boundaries, which led to greater reliance on piloting (landmark-based navigation) and therefore reduced-but did not eliminate-the impact of rotational self-motion cues. These results indicate that rotational self-motion cues are important when teleporting, and that navigation can be improved by enabling piloting.},
  archive      = {J_TVCG},
  author       = {Jonathan W. Kelly and Alec G. Ostrander and Alex F. Lim and Lucia A. Cherep and Stephen B. Gilbert},
  doi          = {10.1109/TVCG.2020.2973051},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {1841-1850},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Teleporting through virtual environments: Effects of path scale and environment scale on spatial updating},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survey of markov chain monte carlo methods in light
transport simulation. <em>TVCG</em>, <em>26</em>(4), 1821–1840. (<a
href="https://doi.org/10.1109/TVCG.2018.2880455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two decades have passed since the introduction of Markov chain Monte Carlo (MCMC) into light transport simulation by Veach and Guibas, and numerous follow-up works have been published since then. However, up until now no survey has attempted to cover the majority of these methods. The aim of this paper is therefore to offer a first comprehensive survey of MCMC algorithms for light transport simulation. The methods presented in this paper are categorized by their objectives and properties, while we point out their strengths and weaknesses. We discuss how the methods handle the main issues of MCMC and how they could be combined or improved in the near future. To make the paper suitable for readers unacquainted with MCMC methods, we include an introduction to general MCMC and its demonstration on a simple example.},
  archive      = {J_TVCG},
  author       = {Martin Šik and Jaroslav Křivánek},
  doi          = {10.1109/TVCG.2018.2880455},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1821-1840},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Survey of markov chain monte carlo methods in light transport simulation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral mesh segmentation via <span
class="math inline"><em>ℓ</em><sub>0</sub></span>ℓ0 gradient
minimization. <em>TVCG</em>, <em>26</em>(4), 1807–1820. (<a
href="https://doi.org/10.1109/TVCG.2018.2882212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh segmentation is a process of partitioning a mesh model into meaningful parts - a fundamental problem in various disciplines. This paper introduces a novel mesh segmentation method inspired by sparsity pursuit. Based on the local geometric and topological information of a given mesh, we build a Laplacian matrix whose Fiedler vector is used to characterize the uniformity among elements of the same segment. By analyzing the Fiedler vector, we reformulate the mesh segmentation problem as a ℓ 0 gradient minimization problem. To solve this problem efficiently, we adopt a coarse-to-fine strategy. A fast heuristic algorithm is first devised to find a rational coarse segmentation, and then an optimization algorithm based on the alternating direction method of multiplier (ADMM) is proposed to refine the segment boundaries within their local regions. To extract the inherent hierarchical structure of the given mesh, our method performs segmentation in a recursive way. Experimental results demonstrate that the presented method outperforms the state-of-the-art segmentation methods when evaluated on the Princeton Segmentation Benchmark, the LIFL/LIRIS Segmentation Benchmark and a number of other complex meshes.},
  archive      = {J_TVCG},
  author       = {Weihua Tong and Xiankang Yang and Maodong Pan and Falai Chen},
  doi          = {10.1109/TVCG.2018.2882212},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1807-1820},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spectral mesh segmentation via $\ell _0$ℓ0 gradient minimization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantics-space-time cube: A conceptual framework for
systematic analysis of texts in space and time. <em>TVCG</em>,
<em>26</em>(4), 1789–1806. (<a
href="https://doi.org/10.1109/TVCG.2018.2882449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach to analyzing data in which texts are associated with spatial and temporal references with the aim to understand how the text semantics vary over space and time. To represent the semantics, we apply probabilistic topic modeling. After extracting a set of topics and representing the texts by vectors of topic weights, we aggregate the data into a data cube with the dimensions corresponding to the set of topics, the set of spatial locations (e.g., regions), and the time divided into suitable intervals according to the scale of the planned analysis. Each cube cell corresponds to a combination (topic, location, time interval) and contains aggregate measures characterizing the subset of the texts concerning this topic and having the spatial and temporal references within these location and interval. Based on this structure, we systematically describe the space of analysis tasks on exploring the interrelationships among the three heterogeneous information facets, semantics, space, and time. We introduce the operations of projecting and slicing the cube, which are used to decompose complex tasks into simpler subtasks. We then present a design of a visual analytics system intended to support these subtasks. To reduce the complexity of the user interface, we apply the principles of structural, visual, and operational uniformity while respecting the specific properties of each facet. The aggregated data are represented in three parallel views corresponding to the three facets and providing different complementary perspectives on the data. The views have similar look-and-feel to the extent allowed by the facet specifics. Uniform interactive operations applicable to any view support establishing links between the facets. The uniformity principle is also applied in supporting the projecting and slicing operations on the data cube. We evaluate the feasibility and utility of the approach by applying it in two analysis scenarios using geolocated social media data for studying people&#39;s reactions to social and natural events of different spatial and temporal scales.},
  archive      = {J_TVCG},
  author       = {Jie Li and Siming Chen and Wei Chen and Gennady Andrienko and Natalia Andrienko},
  doi          = {10.1109/TVCG.2018.2882449},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1789-1806},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Semantics-space-time cube: A conceptual framework for systematic analysis of texts in space and time},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selection expressions for procedural modeling.
<em>TVCG</em>, <em>26</em>(4), 1775–1788. (<a
href="https://doi.org/10.1109/TVCG.2018.2877614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new approach for procedural modeling. Our main idea is to select shapes using selection-expressions instead of simple string matching used in current state-of-the-art grammars like CGA shape and CGA++. A selection-expression specifies how to select a potentially complex subset of shapes from a shape hierarchy, e.g., ”select all tall windows in the second floor of the main building facade”. This new way of modeling enables us to express modeling ideas in their global context rather than traditional rules that operate only locally. To facilitate selection-based procedural modeling we introduce the procedural modeling language SELEX. An important implication of our work is that enforcing important constraints, such as alignment and same size constraints can be done by construction. Therefore, our procedural descriptions can generate facade and building variations without violating alignment and sizing constraints that plague the current state of the art. While the procedural modeling of architecture is our main application domain, we also demonstrate that our approach nicely extends to other man-made},
  archive      = {J_TVCG},
  author       = {Haiyong Jiang and Dong-Ming Yan and Xiaopeng Zhang and Peter Wonka},
  doi          = {10.1109/TVCG.2018.2877614},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1775-1788},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Selection expressions for procedural modeling},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scientific visualization as a microservice. <em>TVCG</em>,
<em>26</em>(4), 1760–1774. (<a
href="https://doi.org/10.1109/TVCG.2018.2879672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose using a decoupled architecture to create a microservice that can deliver scientific visualization remotely with efficiency, scalability, and superior availability, affordability and accessibility. Through our effort, we have created an open source platform, Tapestry, which can be deployed on Amazon AWS as a production use microservice. The applications we use to demonstrate the efficacy of the Tapestry microservice in this work are: (1) embedding interactive visualizations into lightweight web pages, (2) creating scientific visualization movies that are fully controllable by the viewers, (3) serving as a rendering engine for high-end displays such as power-walls, and (4) embedding data-intensive visualizations into augmented reality devices efficiently. In addition, we show results of an extensive performance study, and suggest how applications can make optimal use of microservices such as Tapestry.},
  archive      = {J_TVCG},
  author       = {Mohammad Raji and Alok Hota and Tanner Hobson and Jian Huang},
  doi          = {10.1109/TVCG.2018.2879672},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1760-1774},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scientific visualization as a microservice},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NNWarp: Neural network-based nonlinear deformation.
<em>TVCG</em>, <em>26</em>(4), 1745–1759. (<a
href="https://doi.org/10.1109/TVCG.2018.2881451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NNWarp is a highly re-usable and efficient neural network (NN) based nonlinear deformable simulation framework. Unlike other machine learning applications such as image recognition, where different inputs have a uniform and consistent format (e.g., an array of all the pixels in an image), the input for deformable simulation is quite variable, high-dimensional, and parametrization-unfriendly. Consequently, even though the neural network is known for its rich expressivity of nonlinear functions, directly using an NN to reconstruct the force-displacement relation for general deformable simulation is nearly impossible. NNWarp obviates this difficulty by partially restoring the force-displacement relation via warping the nodal displacement simulated using a simplistic constitutive model-the linear elasticity. In other words, NNWarp yields an incremental displacement fix per mesh node based on a simplified (therefore incorrect) simulation result other than synthesizing the unknown displacement directly. We introduce a compact yet effective feature vector including geodesic, potential and digression to sort training pairs of per-node linear and nonlinear displacement. NNWarp is robust under different model shapes and tessellations. With the assistance of deformation substructuring, one NN training is able to handle a wide range of 3D models of various geometries. Thanks to the linear elasticity and its constant system matrix, the underlying simulator only needs to perform one pre-factorized matrix solve at each time step, which allows NNWarp to simulate large models in real time.},
  archive      = {J_TVCG},
  author       = {Ran Luo and Tianjia Shao and Huamin Wang and Weiwei Xu and Xiang Chen and Kun Zhou and Yin Yang},
  doi          = {10.1109/TVCG.2018.2881451},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1745-1759},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NNWarp: Neural network-based nonlinear deformation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FlowNet: A deep learning framework for clustering and
selection of streamlines and stream surfaces. <em>TVCG</em>,
<em>26</em>(4), 1732–1744. (<a
href="https://doi.org/10.1109/TVCG.2018.2880207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For effective flow visualization, identifying representative flow lines or surfaces is an important problem which has been studied. However, no work can solve the problem for both lines and surfaces. In this paper, we present FlowNet, a single deep learning framework for clustering and selection of streamlines and stream surfaces. Given a collection of streamlines or stream surfaces generated from a flow field data set, our approach converts them into binary volumes and then employs an autoencoder to learn their respective latent feature descriptors. These descriptors are used to reconstruct binary volumes for error estimation and network training. Once converged, the feature descriptors can well represent flow lines or surfaces in the latent space. We perform dimensionality reduction of these feature descriptors and cluster the projection results accordingly. This leads to a visual interface for exploring the collection of flow lines or surfaces via clustering, filtering, and selection of representatives. Intuitive user interactions are provided for visual reasoning of the collection with ease. We validate and explain our deep learning framework from multiple perspectives, demonstrate the effectiveness of FlowNet using several flow field data sets of different characteristics, and compare our approach against state-of-the-art streamline and stream surface selection algorithms.},
  archive      = {J_TVCG},
  author       = {Jun Han and Jun Tao and Chaoli Wang},
  doi          = {10.1109/TVCG.2018.2880207},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1732-1744},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FlowNet: A deep learning framework for clustering and selection of streamlines and stream surfaces},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EFESTA: Ensemble feature exploration with surface density
estimates. <em>TVCG</em>, <em>26</em>(4), 1716–1731. (<a
href="https://doi.org/10.1109/TVCG.2018.2879866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose surface density estimate (SDE) to model the spatial distribution of surface features-isosurfaces, ridge surfaces, and streamsurfaces-in 3D ensemble simulation data. The inputs of SDE computation are surface features represented as polygon meshes, and no field datasets are required (e.g., scalar fields or vector fields). The SDE is defined as the kernel density estimate of the infinite set of points on the input surfaces and is approximated by accumulating the surface densities of triangular patches. We also propose an algorithm to guide the selection of a proper kernel bandwidth for SDE computation. An ensemble Feature Exploration method based on Surface densiTy EstimAtes (eFESTA) is then proposed to extract and visualize the major trends of ensemble surface features. For an ensemble of surface features, each surface is first transformed into a density field based on its contribution to the SDE, and the resulting density fields are organized into a hierarchical representation based on the pairwise distances between them. The hierarchical representation is then used to guide visual exploration of the density fields as well as the underlying surface features. We demonstrate the application of our method using isosurface in ensemble scalar fields, Lagrangian coherent structures in uncertain unsteady flows, and streamsurfaces in ensemble fluid flows.},
  archive      = {J_TVCG},
  author       = {Wenbin He and Hanqi Guo and Han-Wei Shen and Tom Peterka},
  doi          = {10.1109/TVCG.2018.2879866},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1716-1731},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EFESTA: Ensemble feature exploration with surface density estimates},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven indoor scene modeling from a single color image
with iterative object segmentation and model retrieval. <em>TVCG</em>,
<em>26</em>(4), 1702–1715. (<a
href="https://doi.org/10.1109/TVCG.2018.2880737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for modeling the indoor scene from a single color image. With our system, the user only needs to drag a few semantic bounding boxes surrounding the objects of interest. Our system then automatically finds the most similar 3D models from the ShapeNet model repository and aligns them with the corresponding objects of interest. To achieve this, each 3D model is represented as a group of view-dependent representations generated from a set of synthesized views. We iteratively conduct object segmentation and 3D model retrieval, based on the observation that good segmentation of the objects of interest can significantly improve the accuracy of model retrieval and make it robust to cluttered background and occlusions, and in turn, the retrieved 3D models can be used to assist with object segmentation. Segmentation of all objects of interest is achieved simultaneously under a unified multilabeling framework which fully utilizes the correspondences between the objects of interest and retrieved model images. Besides, we propose a new method to estimate the scene layout of the input image with the segmentation masks, which helps compose the resulting scene and further improves the modeling result remarkably. We verify the effectiveness of our approach through experimenting with a variety of indoor images and comparing against the relevant methods.},
  archive      = {J_TVCG},
  author       = {Mingming Liu and Kexin Zhang and Jie Zhu and Jun Wang and Jie Guo and Yanwen Guo},
  doi          = {10.1109/TVCG.2018.2880737},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1702-1715},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data-driven indoor scene modeling from a single color image with iterative object segmentation and model retrieval},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Beta-complex versus alpha-complex: Similarities and
dissimilarities. <em>TVCG</em>, <em>26</em>(4), 1686–1701. (<a
href="https://doi.org/10.1109/TVCG.2018.2873633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The beta-complex is a construct derived from the Voronoi diagram of spherical balls of arbitrary radii and has proven a powerful capability for proximity reasoning among spherical balls in three-dimensional space. Important applications related to molecular shapes in structural/computational molecular biology have been correctly, efficiently, and conveniently solved in the unified framework of the beta-complex and the Voronoi diagram. The beta-complex is a generalization of the ordinary alpha-complex. However, there are similarities and dissimilarities between the two complexes and it is necessary to correctly understand these similarities and dissimilarities to choose the right complex to solve application problems at hand. This paper presents the similarities and dissimilarities between these constructs and illustrates the consequence of the dissimilarity in application problems from both theoretical and practical points of view using examples of atomic arrangements.},
  archive      = {J_TVCG},
  author       = {Donguk Kim and Mokwon Lee and Youngsong Cho and Deok-Soo Kim},
  doi          = {10.1109/TVCG.2018.2873633},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1686-1701},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Beta-complex versus alpha-complex: Similarities and dissimilarities},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic spatially varying illumination recovery of indoor
scenes based on a single RGB-d image. <em>TVCG</em>, <em>26</em>(4),
1672–1685. (<a href="https://doi.org/10.1109/TVCG.2018.2876541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an automatic framework to recover the illumination of indoor scenes based on a single RGB-D image. Unlike previous works, our method can recover spatially varying illumination without using any lighting capturing devices or HDR information. The recovered illumination can produce realistic rendering results. To model the geometry of the visible and invisible parts of scenes corresponding to the input RGB-D image, we assume that all objects shown in the image are located in a box with six faces and build a planar-based geometry model based on the input depth map. We then present a confidence-scoring based strategy to separate the light sources from the highlight areas. The positions of light sources both in and out of the camera&#39;s view are calculated based on the classification result and the recovered geometry model. Finally, an iterative procedure is proposed to calculate the colors of light sources and the materials in the scene. In addition, a data-driven method is used to set constraints on the light source intensities. Using the estimated light sources and geometry model, environment maps at different points in the scene are generated that can model the spatial variance of illumination. The experimental results demonstrate the validity and flexibility of our approach.},
  archive      = {J_TVCG},
  author       = {Guanyu Xing and Yanli Liu and Haibin Ling and Xavier Granier and Yanci Zhang},
  doi          = {10.1109/TVCG.2018.2876541},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1672-1685},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automatic spatially varying illumination recovery of indoor scenes based on a single RGB-D image},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Glyphboard: Visual exploration of high-dimensional data
combining glyphs with dimensionality reduction. <em>TVCG</em>,
<em>26</em>(4), 1661–1671. (<a
href="https://doi.org/10.1109/TVCG.2020.2969060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rigorous data science is interdisciplinary at its core. In order to make sense of high-dimensional data, data scientists need to enter into a dialogue with domain experts. We present Glyphboard, a visualization tool that aims to support this dialogue. Glyphboard is a zoomable user interface that combines well-known methods such as dimensionality reduction and glyph-based visualizations in a novel, seamless, and integrated tool. While the dimensionality reduction affords a quick overview over the data, glyph-based visualizations are able to show the most relevant dimensions in the data set at one glance. We contribute an open-source prototype of Glyphboard, a general exchange format for high-dimensional data, and a case study with nine data scientists and domain experts from four exemplary domains in order to evaluate how the different visualization and interaction features of Glyphboard are used.},
  archive      = {J_TVCG},
  author       = {Dietrich Kammer and Mandy Keck and Thomas Gründer and Alexander Maasch and Thomas Thom and Martin Kleinsteuber and Rainer Groh},
  doi          = {10.1109/TVCG.2020.2969060},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1661-1671},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Glyphboard: Visual exploration of high-dimensional data combining glyphs with dimensionality reduction},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PeckVis: A visual analytics tool to analyze dominance
hierarchies in small groups. <em>TVCG</em>, <em>26</em>(4), 1650–1660.
(<a href="https://doi.org/10.1109/TVCG.2020.2969056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The formation of social groups is defined by the interactions among the group members. Studying this group formation process can be useful in understanding the status of members, decision-making behaviors, spread of knowledge and diseases, and much more. A defining characteristic of these groups is the pecking order or hierarchy the members form which help groups work towards their goals. One area of social science deals with understanding the formation and maintenance of these hierarchies, and in our work we provide social scientists with a visual analytics tool - PeckVis - to aid this process. While online social groups or social networks have been studied deeply and lead to a variety of analyses and visualization tools, the study of smaller groups in the field of social science lacks the support of suitable tools. Domain experts believe that visualizing their data can save them time as well as reveal findings they may have failed to observe. We worked alongside domain experts to build an interactive visual analytics system to investigate social hierarchies. Our system can discover patterns and relationships between the members of a group as well as compare different groups. The results are presented to the user in the form of an interactive visual analytics dashboard. We demonstrate that domain experts were able to effectively use our tool to analyze animal behavior data.},
  archive      = {J_TVCG},
  author       = {Darius Coelho and Ivan Chase and Klaus Mueller},
  doi          = {10.1109/TVCG.2020.2969056},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1650-1660},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PeckVis: A visual analytics tool to analyze dominance hierarchies in small groups},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Security in process: Visually supported triage analysis in
industrial process data. <em>TVCG</em>, <em>26</em>(4), 1638–1649. (<a
href="https://doi.org/10.1109/TVCG.2020.2969007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operation technology networks, i.e. hard- and software used for monitoring and controlling physical/industrial processes, have been considered immune to cyber attacks for a long time. A recent increase of attacks in these networks proves this assumption wrong. Several technical constraints lead to approaches to detect attacks on industrial processes using available sensor data. This setting differs fundamentally from anomaly detection in IT-network traffic and requires new visualization approaches adapted to the common periodical behavior in OT-network data. We present a tailored visualization system that utilizes inherent features of measurements from industrial processes to full capacity to provide insight into the data and support triage analysis by laymen and experts. The novel combination of spiral plots with results from anomaly detection was implemented in an interactive system. The capabilities of our system are demonstrated using sensor and actuator data from a real-world water treatment process with introduced attacks. Exemplary analysis strategies are presented. Finally, we evaluate effectiveness and usability of our system and perform an expert evaluation.},
  archive      = {J_TVCG},
  author       = {Anna-Pia Lohfink and Simon D. Duque Anton and Hans Dieter Schotten and Heike Leitte and Christoph Garth},
  doi          = {10.1109/TVCG.2020.2969007},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1638-1649},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Security in process: Visually supported triage analysis in industrial process data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Message from the editor-in-chief. <em>TVCG</em>,
<em>26</em>(4), 1637. (<a
href="https://doi.org/10.1109/TVCG.2020.2967233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2020.2967233},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1637},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ViSeq: Visual analytics of learning sequence in massive open
online courses. <em>TVCG</em>, <em>26</em>(3), 1622–1636. (<a
href="https://doi.org/10.1109/TVCG.2018.2872961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on massive open online courses (MOOCs) data analytics has mushroomed recently because of the rapid development of MOOCs. The MOOC data not only contains learner profiles and learning outcomes, but also sequential information about when and which type of learning activities each learner performs, such as reviewing a lecture video before undertaking an assignment. Learning sequence analytics could help understand the correlations between learning sequences and performances, which further characterize different learner groups. However, few works have explored the sequence of learning activities, which have mostly been considered aggregated events. A visual analytics system called ViSeq is introduced to resolve the loss of sequential information, to visualize the learning sequence of different learner groups, and to help better understand the reasons behind the learning behaviors. The system facilitates users in exploring learning sequences from multiple levels of granularity. ViSeq incorporates four linked views: the projection view to identify learner groups, the pattern view to exhibit overall sequential patterns within a selected group, the sequence view to illustrate the transitions between consecutive events, and the individual view with an augmented sequence chain to compare selected personal learning sequences. Case studies and expert interviews were conducted to evaluate the system.},
  archive      = {J_TVCG},
  author       = {Qing Chen and Xuanwu Yue and Xavier Plantaz and Yuanzhe Chen and Conglei Shi and Ting-Chuen Pong and Huamin Qu},
  doi          = {10.1109/TVCG.2018.2872961},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1622-1636},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ViSeq: Visual analytics of learning sequence in massive open online courses},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards BCI-based interfaces for augmented reality:
Feasibility, design and evaluation. <em>TVCG</em>, <em>26</em>(3),
1608–1621. (<a href="https://doi.org/10.1109/TVCG.2018.2873737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-Computer Interfaces (BCIs) enable users to interact with computers without any dedicated movement, bringing new hands-free interaction paradigms. In this paper we study the combination of BCI and Augmented Reality (AR). We first tested the feasibility of using BCI in AR settings based on Optical See-Through Head-Mounted Displays (OST-HMDs). Experimental results showed that a BCI and an OST-HMD equipment (EEG headset and Hololens in our case) are well compatible and that small movements of the head can be tolerated when using the BCI. Second, we introduced a design space for command display strategies based on BCI in AR, when exploiting a famous brain pattern called Steady-State Visually Evoked Potential (SSVEP). Our design space relies on five dimensions concerning the visual layout of the BCI menu; namely: orientation, frame-of-reference, anchorage, size and explicitness. We implemented various BCI-based display strategies and tested them within the context of mobile robot control in AR. Our findings were finally integrated within an operational prototype based on a real mobile robot that is controlled in AR using a BCI and a HoloLens headset. Taken together our results (4 user studies) and our methodology could pave the way to future interaction schemes in Augmented Reality exploiting 3D User Interfaces based on brain activity and BCIs.},
  archive      = {J_TVCG},
  author       = {Hakim Si-Mohammed and Jimmy Petit and Camille Jeunet and Ferran Argelaguet and Fabien Spindler and Andéol Évain and Nicolas Roussel and Gery Casiez and Anatole Lecuyer},
  doi          = {10.1109/TVCG.2018.2873737},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1608-1621},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards BCI-based interfaces for augmented reality: Feasibility, design and evaluation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Topic-based exploration and embedded visualizations for
research idea generation. <em>TVCG</em>, <em>26</em>(3), 1592–1607. (<a
href="https://doi.org/10.1109/TVCG.2018.2873011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work analyzes sensemaking frameworks and experiments with an iteratively designed visual analysis tool to identify design implications for facilitating research idea generation using visualizations. Our tool, ThoughtFlow, structures and visualizes literature collections using topic models to bridge the information gap between core activities during research ideation. To help users stay focused on a topic while discovering relevant documents, we designed and analyzed usage patterns for two types of embedded visualization that help determine document relevance while minimizing distraction. We analyzed how research ideation outcomes and processes differ when using ThoughtFlow and conventional search engines by augmenting insight-based evaluation with concept-map analysis. Our results suggest that operations afforded by topic models match well with later ideation stages when coherent topics have emerged, but not with early stages when users are still relying heavily on individual keywords to gather background knowledge. We also present qualitative evidence that citation sparklines encourage more exploration of recommended references, and that a preference for paper thumbnails may depend on the consistency between the evidence and the current mental frame.},
  archive      = {J_TVCG},
  author       = {Hua Guo and David H. Laidlaw},
  doi          = {10.1109/TVCG.2018.2873011},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1592-1607},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Topic-based exploration and embedded visualizations for research idea generation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effect of color scales on climate scientists’ objective
and subjective performance in spatial data analysis tasks.
<em>TVCG</em>, <em>26</em>(3), 1577–1591. (<a
href="https://doi.org/10.1109/TVCG.2018.2876539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographical maps encoded with rainbow color scales are widely used by climate scientists. Despite a plethora of evidence from the visualization and vision sciences literature about the shortcomings of the rainbow color scale, they continue to be preferred over perceptually optimal alternatives. To study and analyze this mismatch between theory and practice, we present a web-based user study that compares the effect of color scales on performance accuracy for climate-modeling tasks. In this study, we used pairs of continuous geographical maps generated using climatological metrics for quantifying pairwise magnitude difference and spatial similarity. For each pair of maps, 39 scientist-observers judged: i) the magnitude of their difference, ii) their degree of spatial similarity, and iii) the region of greatest dissimilarity between them. Besides the rainbow color scale, two other continuous color scales were chosen such that all three of them covaried two dimensions (luminance monotonicity and hue banding), hypothesized to have an impact on task performance. We also analyzed subjective performance measures, such as user confidence, perceived accuracy, preference, and familiarity in using the different color scales. We found that monotonic luminance scales produced significantly more accurate judgments of magnitude difference but were not superior in spatial comparison tasks, and that hue banding had differential effects based on the task and conditions. Scientists expressed the highest preference and perceived confidence and accuracy with the rainbow, despite its poor performance on the magnitude comparison tasks. We also report on interesting interactions among stimulus conditions, tasks, and color scales, that lead to open research questions.},
  archive      = {J_TVCG},
  author       = {Aritra Dasgupta and Jorge Poco and Bernice Rogowitz and Kyungsik Han and Enrico Bertini and Cláudio T. Silva},
  doi          = {10.1109/TVCG.2018.2876539},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1577-1591},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of color scales on climate scientists’ objective and subjective performance in spatial data analysis tasks},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ScatterNet: A deep subjective similarity model for visual
analysis of scatterplots. <em>TVCG</em>, <em>26</em>(3), 1562–1576. (<a
href="https://doi.org/10.1109/TVCG.2018.2875702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity measuring methods are widely adopted in a broad range of visualization applications. In this work, we address the challenge of representing human perception in the visual analysis of scatterplots by introducing a novel deep-learning-based approach, ScatterNet, captures perception-driven similarities of such plots. The approach exploits deep neural networks to extract semantic features of scatterplot images for similarity calculation. We create a large labeled dataset consisting of similar and dissimilar images of scatterplots to train the deep neural network. We conduct a set of evaluations including performance experiments and a user study to demonstrate the effectiveness and efficiency of our approach. The evaluations confirm that the learned features capture the human perception of scatterplot similarity effectively. We describe two scenarios to show how ScatterNet can be applied in visual analysis applications.},
  archive      = {J_TVCG},
  author       = {Yuxin Ma and Anthony K. H. Tung and Wei Wang and Xiang Gao and Zhigeng Pan and Wei Chen},
  doi          = {10.1109/TVCG.2018.2875702},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1562-1576},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ScatterNet: A deep subjective similarity model for visual analysis of scatterplots},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). P4: Portable parallel processing pipelines for interactive
information visualization. <em>TVCG</em>, <em>26</em>(3), 1548–1561. (<a
href="https://doi.org/10.1109/TVCG.2018.2871139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present P4, an information visualization toolkit that combines declarative design specification and GPU computing for building high-performance interactive systems. Most of the existing information visualization toolkits do not harness the power of parallel processors in today&#39;s mainstream computers. P4 leverages GPU computing to accelerate both data processing and visualization rendering for interactive visualization applications. P4&#39;s programming interface offers a declarative visualization grammar for rapid specifications of data transformations, visual encodings, and interactions. By simplifying the development of GPU-accelerated visualization systems while supporting a high degree of flexibility and customization for design specification, P4 narrows the gap between expressiveness and scalability in information visualization toolkits. Through a range of examples and benchmark tests, we demonstrate that P4 provides high efficiency for creating interactive visualizations and offers drastic performance improvement over current state-of-the-art toolkits.},
  archive      = {J_TVCG},
  author       = {Jianping Kelvin Li and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2018.2871139},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1548-1561},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {P4: Portable parallel processing pipelines for interactive information visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyper-objective vortices. <em>TVCG</em>, <em>26</em>(3),
1532–1547. (<a href="https://doi.org/10.1109/TVCG.2018.2868760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Almost all properties of vector fields, including magnitude, direction, λ 2 and vorticity change under arbitrary movements of the observer. This is undesirable since measurements of physical properties should ideally not depend on the way the (virtual) measurement device moves. There are some properties that are invariant under certain types of reference frame transformations: Galilean invariance (invariance under equal-speed translation) and objectivity (invariance under any smooth rotation and translation of the reference frame). In this paper, we introduce even harder conditions than objectivity: we demand invariance under any smooth similarity transformation (rotation, translation and uniform scale) as well as invariance under any smooth affine transformation of the reference frame. We show that these new hyper-objective measures allow the extraction of vortices that change their volume or deform. Further, we present a generic approach that transforms almost any vortex measure into a hyper-objective one. We apply our methods to vortex extraction in 2D and 3D vector fields, and analyze the numerical robustness, extraction time and the minimization residuals for the Galilean invariant, objective, and the two new hyper-objective approaches.},
  archive      = {J_TVCG},
  author       = {Tobias Günther and Holger Theisel},
  doi          = {10.1109/TVCG.2018.2868760},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1532-1547},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hyper-objective vortices},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Edit distance between merge trees. <em>TVCG</em>,
<em>26</em>(3), 1518–1531. (<a
href="https://doi.org/10.1109/TVCG.2018.2873612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological structures such as the merge tree provide an abstract and succinct representation of scalar fields. They facilitate effective visualization and interactive exploration of feature-rich data. A merge tree captures the topology of sub-level and super-level sets in a scalar field. Estimating the similarity between merge trees is an important problem with applications to feature-directed visualization of time-varying data. We present an approach based on tree edit distance to compare merge trees. The comparison measure satisfies metric properties, it can be computed efficiently, and the cost model for the edit operations is both intuitive and captures well-known properties of merge trees. Experimental results on time-varying scalar fields, 3D cryo electron microscopy data, shape data, and various synthetic datasets show the utility of the edit distance towards a feature-driven analysis of scalar fields.},
  archive      = {J_TVCG},
  author       = {Raghavendra Sridharamurthy and Talha Bin Masood and Adhitya Kamakshidasan and Vijay Natarajan},
  doi          = {10.1109/TVCG.2018.2873612},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1518-1531},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Edit distance between merge trees},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Door and doorway etiquette for virtual humans.
<em>TVCG</em>, <em>26</em>(3), 1502–1517. (<a
href="https://doi.org/10.1109/TVCG.2018.2874050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for simulating a variety of nontrivial, socially motivated behaviors that underlie the orderly passage of pedestrians through doorways, especially the common courtesy of opening and holding doors open for others, an important etiquette that has been overlooked in the literature on autonomous multi-human animation. Emulating such social activity requires serious attention to the interplay of visual perception, navigation in constrained doorway environments, manipulation of a variety of door types, and high-level decision making based on social considerations. To tackle this complex human simulation problem, we take an artificial life approach to modeling autonomous pedestrians, proposing a layered architecture comprising mental, behavioral, and motor layers. The behavioral layer couples two stages: (1) a decentralized, agent-based strategy for dynamically determining the well-mannered ordering of pedestrians around doorways, and (2) a state-based model that directs and coordinates a pedestrian&#39;s interactions with the door. The mental layer is a Bayesian network decision model that dynamically selects appropriate door-holding behaviors by considering both internal and external social factors pertinent to pedestrians interacting with one another in and around doorways. Our framework addresses the various door types in common use and supports a variety of doorway etiquette scenarios with efficient, real-time performance.},
  archive      = {J_TVCG},
  author       = {Wenjia Huang and Demetri Terzopoulos},
  doi          = {10.1109/TVCG.2018.2874050},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1502-1517},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Door and doorway etiquette for virtual humans},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dictionary-based fidelity measure for virtual traffic.
<em>TVCG</em>, <em>26</em>(3), 1490–1501. (<a
href="https://doi.org/10.1109/TVCG.2018.2873695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at objectively measuring the realism of virtual traffic flows and evaluating the effectiveness of different traffic simulation techniques, this paper introduces a general, dictionary-based learning method to evaluate the fidelity of any traffic trajectory data. First, a traffic pattern dictionary that characterizes common patterns of real-world traffic behavior is built offline from pre-collected ground truth traffic data. The corresponding learning error is set as the benchmark of the dictionary-based traffic representation. With the aid of the constructed dictionary, the realism of input simulated traffic flow data can be evaluated by comparing its dictionary-based reconstruction error with the dictionary error benchmark. This evaluation metric can be robustly applied to any simulated traffic flow data; in other words, it is independent of how the traffic data are generated. We demonstrated the effectiveness and robustness of this metric through many experiments on real-world traffic data and various simulated traffic data, comparisons with the state-of-the-art entropy-based similarity metric for aggregate crowd motions, and perceptual evaluation studies.},
  archive      = {J_TVCG},
  author       = {Qianwen Chao and Zhigang Deng and Yangxi Xiao and Dunbang He and Qiguang Miao and Xiaogang Jin},
  doi          = {10.1109/TVCG.2018.2873695},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1490-1501},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dictionary-based fidelity measure for virtual traffic},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BRDF analysis with directional statistics and its
applications. <em>TVCG</em>, <em>26</em>(3), 1476–1489. (<a
href="https://doi.org/10.1109/TVCG.2018.2872709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven BRDF models using real material measurements have become increasingly prevalent due to the development of novel gonioreflectometers, but efficient use of these models in many graphical applications remains challenging due to the few functionalities the raw data could provide. To ameliorate this issue, we propose to analyze BRDFs using directional statistics for better handling and exploring measured materials, especially isotropic materials, with efficient computation and compact storage. We conduct a thorough statistical analysis on both analytical BRDF models and measured materials from the MERL database. We show that different aspects of visual appearance can be characterized by different spherical moments, from which several descriptive measures can be derived to further facilitate their usage. We demonstrate how these measures are best leveraged in some graphical applications including gamut mapping using a new BRDF similarity measure, BRDF or SVBRDF reconstruction based on material clustering, and importance sampling for measured materials based on fast extracted GGX distributions. We finally show the potential of our approach in the categorization of surface reflectance types which is common for traditional photon mapping.},
  archive      = {J_TVCG},
  author       = {Jie Guo and Yanwen Guo and Jingui Pan and Wenzhou Lu},
  doi          = {10.1109/TVCG.2018.2872709},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1476-1489},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {BRDF analysis with directional statistics and its applications},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AutoSweep: Recovering 3D editable objects from a single
photograph. <em>TVCG</em>, <em>26</em>(3), 1466–1475. (<a
href="https://doi.org/10.1109/TVCG.2018.2871190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a fully automatic framework for extracting editable 3D objects directly from a single photograph. Unlike previous methods which recover either depth maps, point clouds, or mesh surfaces, we aim to recover 3D objects with semantic parts and can be directly edited. We base our work on the assumption that most human-made objects are constituted by parts and these parts can be well represented by generalized primitives. Our work makes an attempt towards recovering two types of primitive-shaped objects, namely, generalized cuboids and generalized cylinders. To this end, we build a novel instance-aware segmentation network for accurate part separation. Our GeoNet outputs a set of smooth part-level masks labeled as profiles and bodies. Then in a key stage, we simultaneously identify profile-body relations and recover 3D parts by sweeping the recognized profile along their body contour and jointly optimize the geometry to align with the recovered masks. Qualitative and quantitative experiments show that our algorithm can recover high quality 3D models and outperforms existing methods in both instance segmentation and 3D reconstruction.},
  archive      = {J_TVCG},
  author       = {Xin Chen and Yuwei Li and Xi Luo and Tianjia Shao and Jingyi Yu and Kun Zhou and Youyi Zheng},
  doi          = {10.1109/TVCG.2018.2871190},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1466-1475},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AutoSweep: Recovering 3D editable objects from a single photograph},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel CNN-based poisson solver for fluid simulation.
<em>TVCG</em>, <em>26</em>(3), 1454–1465. (<a
href="https://doi.org/10.1109/TVCG.2018.2873375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving a large-scale Poisson system is computationally expensive for most of the Eulerian fluid simulation applications. We propose a novel machine learning-based approach to accelerate this process. At the heart of our approach is a deep convolutional neural network (CNN), with the capability of predicting the solution (pressure) of a Poisson system given the discretization structure and the intermediate velocities as input. Our system consists of four main components, namely, a deep neural network to solve the large linear equations, a geometric structure to describe the spatial hierarchies of the input vector, a Principal Component Analysis (PCA) process to reduce the dimension of input in training, and a novel loss function to control the incompressibility constraint. We have demonstrated the efficacy of our approach by simulating a variety of high-resolution smoke and liquid phenomena. In particular, we have shown that our approach accelerates the projection step in a conventional Eulerian fluid simulator by two orders of magnitude. In addition, we have also demonstrated the generality of our approach by producing a diversity of animations deviating from the original datasets.},
  archive      = {J_TVCG},
  author       = {Xiangyun Xiao and Yanqing Zhou and Hui Wang and Xubo Yang},
  doi          = {10.1109/TVCG.2018.2873375},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1454-1465},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A novel CNN-based poisson solver for fluid simulation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D objects clouds: Viewing virtual objects in interactive
clouds. <em>TVCG</em>, <em>26</em>(3), 1442–1453. (<a
href="https://doi.org/10.1109/TVCG.2018.2873724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the expanding use of 3D Objects in a variety of fields such as animation, gaming, virtual worlds, commerce, augmented reality and 3D printing, we present a novel system for object browsing and searching. Specifically, the system packs objects into an interactive 3D cloud for browsing and searching. It was designed with the aim of increasing search efficiency in a variety of active environments, while providing a visually engaging layout, and we evaluated this by conducting a comparative user study. We show that our system can significantly decrease search times compared to the classic grid-based layout, and it has been suggested by subjects that cloud-based searching is more interesting and visually-engaging.},
  archive      = {J_TVCG},
  author       = {Xiaoting Hong and Stephen Brooks},
  doi          = {10.1109/TVCG.2018.2873724},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1442-1453},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D objects clouds: Viewing virtual objects in interactive clouds},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). State of the journal. <em>TVCG</em>, <em>26</em>(3),
1440–1441. (<a href="https://doi.org/10.1109/TVCG.2019.2959095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents an editorial which examines the current state of the journal and planned future directions for the publication.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2019.2959095},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1440-1441},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {State of the journal},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FleXeen: Visually manipulating perceived fabric bending
stiffness in spatial augmented reality. <em>TVCG</em>, <em>26</em>(2),
1433–1439. (<a href="https://doi.org/10.1109/TVCG.2018.2871044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The appearance of fabric motion is suggested to affect the human perception of bending stiffness. This study presents a novel spatial augmented reality, or projection mapping, approach that can visually manipulate the perceived bending stiffness of a fabric. Particularly, we proposed a flow enhancement method that can change the apparent fabric motion by using a simple optical flow analysis technique rather than complex physical simulations for interactive applications. Through a psychophysical experiment, we investigated the relationship between the magnification factor of our flow enhancement and the perceived bending stiffness of a fabric. Furthermore, we constructed a prototype application system that allows users to control the stiffness of a fabric without changing the actual physical fabric. By evaluating the prototype, we confirmed that the proposed technique can manipulate the perceived stiffness of various materials (i.e., cotton, polyester, and mixed cotton and linen) at an average accuracy of 90.3 percent.},
  archive      = {J_TVCG},
  author       = {Parinya Punpongsanon and Daisuke Iwai and Kosuke Sato},
  doi          = {10.1109/TVCG.2018.2871044},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1433-1439},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FleXeen: Visually manipulating perceived fabric bending stiffness in spatial augmented reality},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A task-based taxonomy of cognitive biases for information
visualization. <em>TVCG</em>, <em>26</em>(2), 1413–1432. (<a
href="https://doi.org/10.1109/TVCG.2018.2872577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information visualization designers strive to design data displays that allow for efficient exploration, analysis, and communication of patterns in data, leading to informed decisions. Unfortunately, human judgment and decision making are imperfect and often plagued by cognitive biases. There is limited empirical research documenting how these biases affect visual data analysis activities. Existing taxonomies are organized by cognitive theories that are hard to associate with visualization tasks. Based on a survey of the literature we propose a task-based taxonomy of 154 cognitive biases organized in 7 main categories. We hope the taxonomy will help visualization researchers relate their design to the corresponding possible biases, and lead to new research that detects and addresses biased judgment and decision making in data visualization.},
  archive      = {J_TVCG},
  author       = {Evanthia Dimara and Steven Franconeri and Catherine Plaisant and Anastasia Bezerianos and Pierre Dragicevic},
  doi          = {10.1109/TVCG.2018.2872577},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1413-1432},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A task-based taxonomy of cognitive biases for information visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WeSeer: Visual analysis for better information cascade
prediction of WeChat articles. <em>TVCG</em>, <em>26</em>(2), 1399–1412.
(<a href="https://doi.org/10.1109/TVCG.2018.2867776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media, such as Facebook and WeChat, empowers millions of users to create, consume, and disseminate online information on an unprecedented scale. The abundant information on social media intensifies the competition of WeChat Public Official Articles (i.e., posts) for gaining user attention due to the zero-sum nature of attention. Therefore, only a small portion of information tends to become extremely popular while the rest remains unnoticed or quickly disappears. Such a typical “long-tail” phenomenon is very common in social media. Thus, recent years have witnessed a growing interest in predicting the future trend in the popularity of social media posts and understanding the factors that influence the popularity of the posts. Nevertheless, existing predictive models either rely on cumbersome feature engineering or sophisticated parameter tuning, which are difficult to understand and improve. In this paper, we study and enhance a point process-based model by incorporating visual reasoning to support communication between the users and the predictive model for a better prediction result. The proposed system supports users to uncover the working mechanism behind the model and improve the prediction accuracy accordingly based on the insights gained. We use realistic WeChat articles to demonstrate the effectiveness of the system and verify the improved model on a large scale of WeChat articles. We also elicit and summarize the feedback from WeChat domain experts.},
  archive      = {J_TVCG},
  author       = {Quan Li and Ziming Wu and Lingling Yi and Kristanto Sean N. and Huamin Qu and Xiaojuan Ma},
  doi          = {10.1109/TVCG.2018.2867776},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1399-1412},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {WeSeer: Visual analysis for better information cascade prediction of WeChat articles},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effect of focal distance, age, and brightness on
near-field augmented reality depth matching. <em>TVCG</em>,
<em>26</em>(2), 1385–1398. (<a
href="https://doi.org/10.1109/TVCG.2018.2869729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many augmented reality (AR) applications operate within near-field reaching distances, and require matching the depth of a virtual object with a real object. The accuracy of this matching was measured in three experiments, which examined the effect of focal distance, age, and brightness, within distances of 33.3 to 50 cm, using a custom-built AR haploscope. Experiment I examined the effect of focal demand, at the levels of collimated (infinite focal distance), consistent with other depth cues, and at the midpoint of reaching distance. Observers were too young to exhibit age-related reductions in accommodative ability. The depth matches of collimated targets were increasingly overestimated with increasing distance, consistent targets were slightly underestimated, and midpoint targets were accurately estimated. Experiment II replicated Experiment I, with older observers. Results were similar to Experiment I. Experiment III replicated Experiment I with dimmer targets, using young observers. Results were again consistent with Experiment I, except that both consistent and midpoint targets were accurately estimated. In all cases, collimated results were explained by a model, where the collimation biases the eyes&#39; vergence angle outwards by a constant amount. Focal demand and brightness affect near-field AR depth matching, while age-related reductions in accommodative ability have no effect.},
  archive      = {J_TVCG},
  author       = {Gurjot Singh and Stephen R. Ellis and J. Edward Swan},
  doi          = {10.1109/TVCG.2018.2869729},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1385-1398},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of focal distance, age, and brightness on near-field augmented reality depth matching},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realistic procedural plant modeling from multiple view
images. <em>TVCG</em>, <em>26</em>(2), 1372–1384. (<a
href="https://doi.org/10.1109/TVCG.2018.2869784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we describe a novel procedural modeling technique for generating realistic plant models from multi-view photographs. The realism is enhanced via visual and spatial information acquired from images. In contrast to previous approaches that heavily rely on user interaction to segment plants or recover branches in images, our method automatically estimates an accurate depth map of each image and extracts a 3D dense point cloud by exploiting an efficient stereophotogrammetry approach. Taking this point cloud as a soft constraint, we fit a parametric plant representation to simulate the plant growth progress. In this way, we are able to synthesize parametric plant models from real data provided by photos and 3D point clouds. We demonstrate the robustness of the proposed approach by modeling various plants with complex branching structures and significant self-occlusions. We also demonstrate that the proposed framework can be used to reconstruct ground-covering plants, such as bushes and shrubs which have been given little attention in the literature. The effectiveness of our approach is validated by visually and quantitatively comparing with the state-of-the-art approaches.},
  archive      = {J_TVCG},
  author       = {Jianwei Guo and Shibiao Xu and Dong-Ming Yan and Zhanglin Cheng and Marc Jaeger and Xiaopeng Zhang},
  doi          = {10.1109/TVCG.2018.2869784},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1372-1384},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Realistic procedural plant modeling from multiple view images},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Poisson vector graphics (PVG). <em>TVCG</em>,
<em>26</em>(2), 1361–1371. (<a
href="https://doi.org/10.1109/TVCG.2018.2867478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Poisson vector graphics (PVG), an extension of the popular diffusion curves (DC), for generating smooth-shaded images. Armed with two new types of primitives, called Poisson curves and Poisson regions, PVG can easily produce photorealistic effects such as specular highlights, core shadows, translucency and halos. Within the PVG framework, the users specify color as the Dirichlet boundary condition of diffusion curves and control tone by offsetting the Laplacian of colors, where both controls are simply done by mouse click and slider dragging. PVG distinguishes itself from other diffusion based vector graphics for 3 unique features: 1) explicit separation of colors and tones, which follows the basic drawing principle and eases editing; 2) native support of seamless cloning in the sense that PCs and PRs can automatically fit into the target background; and 3) allowed intersecting primitives (except for DC-DC intersection) so that users can create layers. Through extensive experiments and a preliminary user study, we demonstrate that PVG is a simple yet powerful authoring tool that can produce photo-realistic vector graphics from scratch.},
  archive      = {J_TVCG},
  author       = {Fei Hou and Qian Sun and Zheng Fang and Yong-Jin Liu and Shi-Min Hu and Hong Qin and Aimin Hao and Ying He},
  doi          = {10.1109/TVCG.2018.2867478},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1361-1371},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Poisson vector graphics (PVG)},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PANENE: A progressive algorithm for indexing and querying
approximate k-nearest neighbors. <em>TVCG</em>, <em>26</em>(2),
1347–1360. (<a href="https://doi.org/10.1109/TVCG.2018.2869149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present PANENE, a progressive algorithm for approximate nearest neighbor indexing and querying. Although the use of k-nearest neighbor (KNN) libraries is common in many data analysis methods, most KNN algorithms can only be queried when the whole dataset has been indexed, i.e., they are not online. Even the few online implementations are not progressive in the sense that the time to index incoming data is not bounded and cannot satisfy the latency requirements of progressive systems. This long latency has significantly limited the use of many machine learning methods, such as t-SNE, in interactive visual analytics. PANENE is a novel algorithm for Progressive Approximate k-NEarest NEighbors, enabling fast KNN queries while continuously indexing new batches of data. Following the progressive computation paradigm, PANENE operations can be bounded in time, allowing analysts to access running results within an interactive latency. PANENE can also incrementally build and maintain a cache data structure, a KNN lookup table, to enable constant-time lookups for KNN queries. Finally, we present three progressive applications of PANENE, such as regression, density estimation, and responsive t-SNE, opening up new opportunities to use complex algorithms in interactive systems.},
  archive      = {J_TVCG},
  author       = {Jaemin Jo and Jinwook Seo and Jean-Daniel Fekete},
  doi          = {10.1109/TVCG.2018.2869149},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1347-1360},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PANENE: A progressive algorithm for indexing and querying approximate k-nearest neighbors},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intrinsic image decomposition with step and drift shading
separation. <em>TVCG</em>, <em>26</em>(2), 1332–1346. (<a
href="https://doi.org/10.1109/TVCG.2018.2869326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposing an image into the shading and reflectance layers remains challenging due to its severely under-constrained nature. We present an approach based on illumination decomposition that recovers the intrinsic images without additional information, e.g., depth or user interaction. Our approach is based on the rationale that the shading component contains the step and drift channels simultaneously. We decompose the illumination into two channels: the step shading, corresponding to the sharp shading changes due to cast shadow or abrupt shape changes; the drift shading, accounting for the smooth shading variations due to gradual illumination changes or slow shape changes. Due to such transformation of turning the conventional assumption that shading has smoothness as reasonable prior, our model has the advantages in handling real images, especially with the cast shadows or strong shape edges. We also apply a much stricter edge classifier along with a reinforcement process to enhance our method. We formulate the problem using a two-parameter energy function and split it into two energy functions corresponding to the reflectance and step shading. Experiments on the MIT dataset, the IIW dataset and the MPI Sintel dataset have shown the success of our approach over the state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Bin Sheng and Ping Li and Yuxi Jin and Ping Tan and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2018.2869326},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1332-1346},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Intrinsic image decomposition with step and drift shading separation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hamiltonian operator for spectral shape analysis.
<em>TVCG</em>, <em>26</em>(2), 1320–1331. (<a
href="https://doi.org/10.1109/TVCG.2018.2867513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many shape analysis methods treat the geometry of an object as a metric space that can be captured by the Laplace-Beltrami operator. In this paper, we propose to adapt the classical Hamiltonian operator from quantum mechanics to the field of shape analysis. To this end, we study the addition of a potential function to the Laplacian as a generator for dual spaces in which shape processing is performed. We present general optimization approaches for solving variational problems involving the basis defined by the Hamiltonian using perturbation theory for its eigenvectors. The suggested operator is shown to produce better functional spaces to operate with, as demonstrated on different shape analysis tasks.},
  archive      = {J_TVCG},
  author       = {Yoni Choukroun and Alon Shtern and Alex Bronstein and Ron Kimmel},
  doi          = {10.1109/TVCG.2018.2867513},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1320-1331},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hamiltonian operator for spectral shape analysis},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature level-sets: Generalizing iso-surfaces to
multi-variate data. <em>TVCG</em>, <em>26</em>(2), 1308–1319. (<a
href="https://doi.org/10.1109/TVCG.2018.2867488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iso-surfaces or level-sets provide an effective and frequently used means for feature visualization. However, they are restricted to simple features for uni-variate data. The approach does not scale when moving to multi-variate data or when considering more complex feature definitions. In this paper, we introduce the concept of traits and feature level-sets, which can be understood as a generalization of level-sets as it includes iso-surfaces, and fiber surfaces as special cases. The concept is applicable to a large class of traits defined as subsets in attribute space, which can be arbitrary combinations of points, lines, surfaces and volumes. It is implemented into a system that provides an interface to define traits in an interactive way and multiple rendering options. We demonstrate the effectiveness of the approach using multi-variate data sets of different nature, including vector and tensor data, from different application domains.},
  archive      = {J_TVCG},
  author       = {Jochen Jankowai and Ingrid Hotz},
  doi          = {10.1109/TVCG.2018.2867488},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1308-1319},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Feature level-sets: Generalizing iso-surfaces to multi-variate data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Equalizer 2.0–convergence of a parallel rendering framework.
<em>TVCG</em>, <em>26</em>(2), 1292–1307. (<a
href="https://doi.org/10.1109/TVCG.2018.2870822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing complex, real world graphics applications which leverage multiple GPUs and computers for interactive 3D rendering tasks is a complex task. It requires expertise in distributed systems and parallel rendering in addition to the application domain itself. We present a mature parallel rendering framework which provides a large set of features, algorithms and system integration for a wide range of real-world research and industry applications. Using the Equalizer parallel rendering framework, we show how a wide set of generic algorithms can be integrated in the framework to help application scalability and development in many different domains, highlighting how concrete applications benefit from the diverse aspects and use cases of Equalizer. We present novel parallel rendering algorithms, powerful abstractions for large visualization setups and virtual reality, as well as new experimental results for parallel rendering and data distribution.},
  archive      = {J_TVCG},
  author       = {Stefan Eilemann and David Steiner and Renato Pajarola},
  doi          = {10.1109/TVCG.2018.2870822},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1292-1307},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Equalizer 2.0–Convergence of a parallel rendering framework},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assigning rated items to locations in non-list display
layouts. <em>TVCG</em>, <em>26</em>(2), 1278–1291. (<a
href="https://doi.org/10.1109/TVCG.2018.2870164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most common ways in which results are displayed by an information retrieval system is in the form of a list, in which the most relevant results appear in the first positions. Today&#39;s large screens, however, allow one to create more complex displays of results, especially in cases such as image retrieval, in which each unit returned is fairly compact. For these layouts the simple list model is no longer valid, since the relations between the slots in which the results are placed do not form a sequence, that is, the relation among them is no longer that of a total order. In this paper we model these layouts as partial orders and show that a “stalwart display” property (a layout in which items&#39; relevance is unambiguously conveyed by their display position) can be obtained only in the case of lists. For the other layouts, we define two classes of representation functions: “safe” functions (which display results without adding spurious structure) and “rich” functions (which do not drop any structure from the result set), as well as an algorithm to optimally display fully ordered result sets in arbitrary display layouts.},
  archive      = {J_TVCG},
  author       = {Simone Santini},
  doi          = {10.1109/TVCG.2018.2870164},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1278-1291},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Assigning rated items to locations in non-list display layouts},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Committees. <em>TVCG</em>, <em>26</em>(1), xxiii. (<a
href="https://doi.org/10.1109/TVCG.2019.2935637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2019.2935637},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxiii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Committees},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SciVis international program committee. <em>TVCG</em>,
<em>26</em>(1), xxii. (<a
href="https://doi.org/10.1109/TVCG.2019.2935659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2019.2935659},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SciVis international program committee},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). InfoVis international program committee. <em>TVCG</em>,
<em>26</em>(1), xxi. (<a
href="https://doi.org/10.1109/TVCG.2019.2935658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2019.2935658},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xxi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InfoVis international program committee},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VAST international program committee. <em>TVCG</em>,
<em>26</em>(1), xx. (<a
href="https://doi.org/10.1109/TVCG.2019.2935657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2019.2935657},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xx},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VAST international program committee},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VIS conference committee. <em>TVCG</em>, <em>26</em>(1),
xviii–xix. (<a href="https://doi.org/10.1109/TVCG.2019.2935609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2019.2935609},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xviii-xix},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIS conference committee},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IEEE visualization and graphics technical committee (VGTC):
Http://vgtc.org/. <em>TVCG</em>, <em>26</em>(1), xvii. (<a
href="https://doi.org/10.1109/TVCG.2019.2935653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2019.2935653},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xvii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE visualization and graphics technical committee (VGTC): http://vgtc.org/},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preface. <em>TVCG</em>, <em>26</em>(1), xi–xvi. (<a
href="https://doi.org/10.1109/TVCG.2019.2935644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This January 2020 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG) contains the proceedings of IEEE VIS 2019, held during 20-25 October 2019 at the Vancouver Convention Center in Vancouver, BC, Canada. With IEEE VIS 2019, the conference series is in its 30 th year. IEEE VIS consists of three conferences, held concurrently: the IEEE Visual Analytics Science and Technology Conference (VAST), the IEEE Information Visualization Conference (InfoVis), and the IEEE Scientific Visualization Conference (SciVis). These three conferences are the premier venues for the visualization community to exchange the latest ideas and developments, attracting researchers and practitioners alike.},
  archive      = {J_TVCG},
  author       = {Remco Chang and Issei Fujishiro and Petra Isenberg and Daniel Keim and Peter Lindstrom and Ross Maciejewski and Miriah Meyer and Gunther H. Weber and Daniel Weiskopf and Jo Wood},
  doi          = {10.1109/TVCG.2019.2935644},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xi-xvi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preface},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Message from the editor-in-chief. <em>TVCG</em>,
<em>26</em>(1), x. (<a
href="https://doi.org/10.1109/TVCG.2019.2935643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2019.2935643},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {x},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). You can’t always sketch what you want: Understanding
sensemaking in visual query systems. <em>TVCG</em>, <em>26</em>(1),
1267–1277. (<a href="https://doi.org/10.1109/TVCG.2019.2934666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual query systems (VQSs) empower users to interactively search for line charts with desired visual patterns, typically specified using intuitive sketch-based interfaces. Despite decades of past work on VQSs, these efforts have not translated to adoption in practice, possibly because VQSs are largely evaluated in unrealistic lab-based settings. To remedy this gap in adoption, we collaborated with experts from three diverse domains-astronomy, genetics, and material science-via a year-long user-centered design process to develop a VQS that supports their workflow and analytical needs, and evaluate how VQSs can be used in practice. Our study results reveal that ad-hoc sketch-only querying is not as commonly used as prior work suggests, since analysts are often unable to precisely express their patterns of interest. In addition, we characterize three essential sensemaking processes supported by our enhanced VQS. We discover that participants employ all three processes, but in different proportions, depending on the analytical needs in each domain. Our findings suggest that all three sensemaking processes must be integrated in order to make future VQSs useful for a wide range of analytical inquiries.},
  archive      = {J_TVCG},
  author       = {Doris Jung-Lin Lee and John Lee and Tarique Siddiqui and Jaewoo Kim and Karrie Karahalios and Aditya Parameswaran},
  doi          = {10.1109/TVCG.2019.2934666},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1267-1277},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {You can&#39;t always sketch what you want: Understanding sensemaking in visual query systems},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A natural-language-based visual query approach of uncertain
human trajectories. <em>TVCG</em>, <em>26</em>(1), 1256–1266. (<a
href="https://doi.org/10.1109/TVCG.2019.2934671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POIs and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Zhaosong Huang and Ye Zhao and Wei Chen and Shengjie Gao and Kejie Yu and Weixia Xu and Mingjie Tang and Minfeng Zhu and Mingliang Xu},
  doi          = {10.1109/TVCG.2019.2934671},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1256-1266},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A natural-language-based visual query approach of uncertain human trajectories},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The role of latency and task complexity in predicting visual
search behavior. <em>TVCG</em>, <em>26</em>(1), 1246–1255. (<a
href="https://doi.org/10.1109/TVCG.2019.2934556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latency in a visualization system is widely believed to affect user behavior in measurable ways, such as requiring the user to wait for the visualization system to respond, leading to interruption of the analytic flow. While this effect is frequently observed and widely accepted, precisely how latency affects different analysis scenarios is less well understood. In this paper, we examine the role of latency in the context of visual search, an essential task in data foraging and exploration using visualization. We conduct a series of studies on Amazon Mechanical Turk and find that under certain conditions, latency is a statistically significant predictor of visual search behavior, which is consistent with previous studies. However, our results also suggest that task type, task complexity, and other factors can modulate the effect of latency, in some cases rendering latency statistically insignificant in predicting user behavior. This suggests a more nuanced view of the role of latency than previously reported. Building on these results and the findings of prior studies, we propose design guidelines for measuring and interpreting the effects of latency when evaluating performance on visual search tasks.},
  archive      = {J_TVCG},
  author       = {Leilani Battle and R. Jordan Crouser and Audace Nakeshimana and Ananda Montoly and Remco Chang and Michael Stonebraker},
  doi          = {10.1109/TVCG.2019.2934556},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1246-1255},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The role of latency and task complexity in predicting visual search behavior},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Searching the visual style and structure of d3
visualizations. <em>TVCG</em>, <em>26</em>(1), 1236–1245. (<a
href="https://doi.org/10.1109/TVCG.2019.2934431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a search engine for D3 visualizations that allows queries based on their visual style and underlying structure. To build the engine we crawl a collection of 7860 D3 visualizations from the Web and deconstruct each one to recover its data, its data-encoding marks and the encodings describing how the data is mapped to visual attributes of the marks. We also extract axes and other non-data-encoding attributes of marks (e.g., typeface, background color). Our search engine indexes this style and structure information as well as metadata about the webpage containing the chart. We show how visualization developers can search the collection to find visualizations that exhibit specific design characteristics and thereby explore the space of possible designs. We also demonstrate how researchers can use the search engine to identify commonly used visual design patterns and we perform such a demographic design analysis across our collection of D3 charts. A user study reveals that visualization developers found our style and structure based search engine to be significantly more useful and satisfying for finding different designs of D3 charts, than a baseline search engine that only allows keyword search over the webpage containing a chart.},
  archive      = {J_TVCG},
  author       = {Enamul Hoque and Maneesh Agrawala},
  doi          = {10.1109/TVCG.2019.2934431},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1236-1245},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Searching the visual style and structure of d3 visualizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating color-concept associations from image statistics.
<em>TVCG</em>, <em>26</em>(1), 1226–1235. (<a
href="https://doi.org/10.1109/TVCG.2019.2934536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To interpret the meanings of colors in visualizations of categorical information, people must determine how distinct colors correspond to different concepts. This process is easier when assignments between colors and concepts in visualizations match people&#39;s expectations, making color palettes semantically interpretable. Efforts have been underway to optimize color palette design for semantic interpretablity, but this requires having good estimates of human color-concept associations. Obtaining these data from humans is costly, which motivates the need for automated methods. We developed and evaluated a new method for automatically estimating color-concept associations in a way that strongly correlates with human ratings. Building on prior studies using Google Images, our approach operates directly on Google Image search results without the need for humans in the loop. Specifically, we evaluated several methods for extracting raw pixel content of the images in order to best estimate color-concept associations obtained from human ratings. The most effective method extracted colors using a combination of cylindrical sectors and color categories in color space. We demonstrate that our approach can accurately estimate average human color-concept associations for different fruits using only a small set of images. The approach also generalizes moderately well to more complicated recycling-related concepts of objects that can appear in any color.},
  archive      = {J_TVCG},
  author       = {Ragini Rathore and Zachary Leggon and Laurent Lessard and Karen B. Schloss},
  doi          = {10.1109/TVCG.2019.2934536},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1226-1235},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Estimating color-concept associations from image statistics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color crafting: Automating the construction of designer
quality color ramps. <em>TVCG</em>, <em>26</em>(1), 1215–1225. (<a
href="https://doi.org/10.1109/TVCG.2019.2934284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color. We do this using an algorithmic approach that models designer practices by analyzing patterns in the structure of designer-crafted color ramps. We construct these models from a corpus of 222 expert-designed color ramps, and use the results to automatically generate ramps that mimic designer practices. We evaluate our approach through an empirical study comparing the outputs of our approach with designer-crafted color ramps. Our models produce ramps that support accurate and aesthetically pleasing visualizations at least as well as designer ramps and that outperform conventional mathematical approaches.},
  archive      = {J_TVCG},
  author       = {Stephen Smart and Keke Wu and Danielle Albers Szafir},
  doi          = {10.1109/TVCG.2019.2934284},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1215-1225},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Color crafting: Automating the construction of designer quality color ramps},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). R-map: A map metaphor for visualizing information reposting
process in social media. <em>TVCG</em>, <em>26</em>(1), 1204–1214. (<a
href="https://doi.org/10.1109/TVCG.2019.2934263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.},
  archive      = {J_TVCG},
  author       = {Shuai Chen and Sihang Li and Siming Chen and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2019.2934263},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1204-1214},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {R-map: A map metaphor for visualizing information reposting process in social media},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MetricsVis: A visual analytics system for evaluating
employee performance in public safety agencies. <em>TVCG</em>,
<em>26</em>(1), 1193–1203. (<a
href="https://doi.org/10.1109/TVCG.2019.2934603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating employee performance in organizations with varying workloads and tasks is challenging. Specifically, it is important to understand how quantitative measurements of employee achievements relate to supervisor expectations, what the main drivers of good performance are, and how to combine these complex and flexible performance evaluation metrics into an accurate portrayal of organizational performance in order to identify shortcomings and improve overall productivity. To facilitate this process, we summarize common organizational performance analyses into four visual exploration task categories. Additionally, we develop MetricsVis, a visual analytics system composed of multiple coordinated views to support the dynamic evaluation and comparison of individual, team, and organizational performance in public safety organizations. MetricsVis provides four primary visual components to expedite performance evaluation: (1) a priority adjustment view to support direct manipulation on evaluation metrics; (2) a reorderable performance matrix to demonstrate the details of individual employees; (3) a group performance view that highlights aggregate performance and individual contributions for each group; and (4) a projection view illustrating employees with similar specialties to facilitate shift assignments and training. We demonstrate the usability of our framework with two case studies from medium-sized law enforcement agencies and highlight its broader applicability to other domains.},
  archive      = {J_TVCG},
  author       = {Jieqiong Zhao and Morteza Karimzadeh and Luke S. Snyder and Chittayong Surakitbanharn and Zhenyu Cheryl Qian and David S. Ebert},
  doi          = {10.1109/TVCG.2019.2934603},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1193-1203},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MetricsVis: A visual analytics system for evaluating employee performance in public safety agencies},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Galex: Exploring the evolution and intersection of
disciplines. <em>TVCG</em>, <em>26</em>(1), 1182–1192. (<a
href="https://doi.org/10.1109/TVCG.2019.2934667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Revealing the evolution of science and the intersections among its sub-fields is extremely important to understand the characteristics of disciplines, discover new topics, and predict the future. The current work focuses on either building the skeleton of science, lacking interaction, detailed exploration and interpretation or on the lower topic level, missing high-level macro-perspective. To fill this gap, we design and implement Galaxy Evolution Explorer (Galex), a hierarchical visual analysis system, in combination with advanced text mining technologies, that could help analysts to comprehend the evolution and intersection of one discipline rapidly. We divide Galex into three progressively fine-grained levels: discipline, area, and institution levels. The combination of interactions enables analysts to explore an arbitrary piece of history and an arbitrary part of the knowledge space of one discipline. Using a flexible spotlight component, analysts could freely select and quickly understand an exploration region. A tree metaphor allows analysts to perceive the expansion, decline, and intersection of topics intuitively. A synchronous spotlight interaction aids in comparing research contents among institutions easily. Three cases demonstrate the effectiveness of our system.},
  archive      = {J_TVCG},
  author       = {Zeyu Li and Changhong Zhang and Shichao Jia and Jiawan Zhang},
  doi          = {10.1109/TVCG.2019.2934667},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1182-1192},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Galex: Exploring the evolution and intersection of disciplines},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPGPU linear complexity t-SNE optimization. <em>TVCG</em>,
<em>26</em>(1), 1172–1181. (<a
href="https://doi.org/10.1109/TVCG.2019.2934307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has become one of the most used and insightful techniques for exploratory data analysis of high-dimensional data. It reveals clusters of high-dimensional data points at different scales while only requiring minimal tuning of its parameters. However, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of t-SNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the t-SNE embedding for large datasets. In this work, we present a novel approach to the minimization of the t-SNE objective function that heavily relies on graphics hardware and has linear computational complexity. Our technique decreases the computational cost of running t-SNE on datasets by orders of magnitude and retains or improves on the accuracy of past approximated techniques. We propose to approximate the repulsive forces between data points by splatting kernel textures for each data point. This approximation allows us to reformulate the t-SNE minimization problem as a series of tensor operations that can be efficiently executed on the graphics card. An efficient implementation of our technique is integrated and available for use in the widely used Google TensorFlow.js, and an open-source C++ library.},
  archive      = {J_TVCG},
  author       = {Nicola Pezzotti and Julian Thijssen and Alexander Mordvintsev and Thomas Höllt and Baldur Van Lew and Boudewijn P.F. Lelieveldt and Elmar Eisemann and Anna Vilanova},
  doi          = {10.1109/TVCG.2019.2934307},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1172-1181},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GPGPU linear complexity t-SNE optimization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RSATree: Distribution-aware data representation of
large-scale tabular datasets for flexible visual query. <em>TVCG</em>,
<em>26</em>(1), 1161–1171. (<a
href="https://doi.org/10.1109/TVCG.2019.2934800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts&#39; flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity.},
  archive      = {J_TVCG},
  author       = {Honghui Mei and Wei Chen and Yating Wei and Yuanzhe Hu and Shuyue Zhou and Bingru Lin and Ying Zhao and Jiazhi Xia},
  doi          = {10.1109/TVCG.2019.2934800},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1161-1171},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RSATree: Distribution-aware data representation of large-scale tabular datasets for flexible visual query},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). P5: Portable progressive parallel processing pipelines for
interactive data analysis and visualization. <em>TVCG</em>,
<em>26</em>(1), 1151–1160. (<a
href="https://doi.org/10.1109/TVCG.2019.2934537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present P5, a web-based visualization toolkit that combines declarative visualization grammar and GPU computing for progressive data analysis and visualization. To interactively analyze and explore big data, progressive analytics and visualization methods have recently emerged. Progressive visualizations of incrementally refining results have the advantages of allowing users to steer the analysis process and make early decisions. P5 leverages declarative grammar for specifying visualization designs and exploits GPU computing to accelerate progressive data processing and rendering. The declarative specifications can be modified during progressive processing to create different visualizations for analyzing the intermediate results. To enable user interactions for progressive data analysis, P5 utilizes the GPU to automatically aggregate and index data based on declarative interaction specifications to facilitate effective interactive visualization. We demonstrate the effectiveness and usefulness of P5 through a variety of example applications and several performance benchmark tests.},
  archive      = {J_TVCG},
  author       = {Jianping Kelvin Li and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2019.2934537},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1151-1160},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {P5: Portable progressive parallel processing pipelines for interactive data analysis and visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OntoPlot: A novel visualisation for non-hierarchical
associations in large ontologies. <em>TVCG</em>, <em>26</em>(1),
1140–1150. (<a href="https://doi.org/10.1109/TVCG.2019.2934557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontologies are formal representations of concepts and complex relationships among them. They have been widely used to capture comprehensive domain knowledge in areas such as biology and medicine, where large and complex ontologies can contain hundreds of thousands of concepts. Especially due to the large size of ontologies, visualisation is useful for authoring, exploring and understanding their underlying data. Existing ontology visualisation tools generally focus on the hierarchical structure, giving much less emphasis to non-hierarchical associations. In this paper we present OntoPlot, a novel visualisation specifically designed to facilitate the exploration of all concept associations whilst still showing an ontology&#39;s large hierarchical structure. This hybrid visualisation combines icicle plots, visual compression techniques and interactivity, improving space-efficiency and reducing visual structural complexity. We conducted a user study with domain experts to evaluate the usability of OntoPlot, comparing it with the de facto ontology editor Protégé. The results confirm that OntoPlot attains our design goals for association-related tasks and is strongly favoured by domain experts.},
  archive      = {J_TVCG},
  author       = {Ying Yang and Michael Wybrow and Yuan-Fang Li and Tobias Czauderna and Yongqun He},
  doi          = {10.1109/TVCG.2019.2934557},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1140-1150},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OntoPlot: A novel visualisation for non-hierarchical associations in large ontologies},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploranative code quality documents. <em>TVCG</em>,
<em>26</em>(1), 1129–1139. (<a
href="https://doi.org/10.1109/TVCG.2019.2934669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.},
  archive      = {J_TVCG},
  author       = {Haris Mumtaz and Shahid Latif and Fabian Beck and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2019.2934669},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1129-1139},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploranative code quality documents},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visualizing a moving target: A design study on task parallel
programs in the presence of evolving data and concerns. <em>TVCG</em>,
<em>26</em>(1), 1118–1128. (<a
href="https://doi.org/10.1109/TVCG.2019.2934285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common pitfalls in visualization projects include lack of data availability and the domain users&#39; needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a “chicken and egg” problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the “moving target” of both the data and the domain experts&#39; concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs.},
  archive      = {J_TVCG},
  author       = {Katy Williams and Alex Bigelow and Kate Isaacs},
  doi          = {10.1109/TVCG.2019.2934285},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1118-1128},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing a moving target: A design study on task parallel programs in the presence of evolving data and concerns},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CloudDet: Interactive visual analysis of anomalous
performances in cloud computing systems. <em>TVCG</em>, <em>26</em>(1),
1107–1117. (<a href="https://doi.org/10.1109/TVCG.2019.2934613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and analyzing potential anomalous performances in cloud computing systems is essential for avoiding losses to customers and ensuring the efficient operation of the systems. To this end, a variety of automated techniques have been developed to identify anomalies in cloud computing. These techniques are usually adopted to track the performance metrics of the system (e.g., CPU, memory, and disk I/O), represented by a multivariate time series. However, given the complex characteristics of cloud computing data, the effectiveness of these automated methods is affected. Thus, substantial human judgment on the automated analysis results is required for anomaly interpretation. In this paper, we present a unified visual analytics system named CloudDet to interactively detect, inspect, and diagnose anomalies in cloud computing systems. A novel unsupervised anomaly detection algorithm is developed to identify anomalies based on the specific temporal patterns of the given metrics data (e.g., the periodic pattern). Rich visualization and interaction designs are used to help understand the anomalies in the spatial and temporal context. We demonstrate the effectiveness of CloudDet through a quantitative evaluation, two case studies with real-world data, and interviews with domain experts.},
  archive      = {J_TVCG},
  author       = {Ke Xu and Yun Wang and Leni Yang and Yifang Wang and Bo Qiao and Si Qin and Yong Xu and Haidong Zhang and Huamin Qu},
  doi          = {10.1109/TVCG.2019.2934613},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1107-1117},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CloudDet: Interactive visual analysis of anomalous performances in cloud computing systems},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Summit: Scaling deep learning interpretability by
visualizing activation and attribution summarizations. <em>TVCG</em>,
<em>26</em>(1), 1096–1106. (<a
href="https://doi.org/10.1109/TVCG.2019.2934659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model&#39;s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier&#39;s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.},
  archive      = {J_TVCG},
  author       = {Fred Hohman and Haekyu Park and Caleb Robinson and Duen Horng Polo Chau},
  doi          = {10.1109/TVCG.2019.2934659},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1096-1106},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FairSight: Visual analytics for fairness in decision making.
<em>TVCG</em>, <em>26</em>(1), 1086–1095. (<a
href="https://doi.org/10.1109/TVCG.2019.2934262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions - understanding, measuring, diagnosing and mitigating biases - that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.},
  archive      = {J_TVCG},
  author       = {Yongsu Ahn and Yu-Ru Lin},
  doi          = {10.1109/TVCG.2019.2934262},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1086-1095},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FairSight: Visual analytics for fairness in decision making},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explaining vulnerabilities to adversarial machine learning
through visual analytics. <em>TVCG</em>, <em>26</em>(1), 1075–1085. (<a
href="https://doi.org/10.1109/TVCG.2019.2934631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.},
  archive      = {J_TVCG},
  author       = {Yuxin Ma and Tiankai Xie and Jundong Li and Ross Maciejewski},
  doi          = {10.1109/TVCG.2019.2934631},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1075-1085},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explaining vulnerabilities to adversarial machine learning through visual analytics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ExplAIner: A visual analytics framework for interactive and
explainable machine learning. <em>TVCG</em>, <em>26</em>(1), 1064–1074.
(<a href="https://doi.org/10.1109/TVCG.2019.2934629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.},
  archive      = {J_TVCG},
  author       = {Thilo Spinner and Udo Schlegel and Hanna Schäfer and Mennatallah El-Assady},
  doi          = {10.1109/TVCG.2019.2934629},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1064-1074},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ExplAIner: A visual analytics framework for interactive and explainable machine learning},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STBins: Visual tracking and comparison of multiple data
sequences using temporal binning. <em>TVCG</em>, <em>26</em>(1),
1054–1063. (<a href="https://doi.org/10.1109/TVCG.2019.2934289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change overtime. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.},
  archive      = {J_TVCG},
  author       = {Ji Qi and Vincent Bloemen and Shihan Wang and Jarke van Wijk and Huub van de Wetering},
  doi          = {10.1109/TVCG.2019.2934289},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1054-1063},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {STBins: Visual tracking and comparison of multiple data sequences using temporal binning},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Separating the wheat from the chaff: Comparative visual cues
for transparent diagnostics of competing models. <em>TVCG</em>,
<em>26</em>(1), 1043–1053. (<a
href="https://doi.org/10.1109/TVCG.2019.2934540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experts in data and physical sciences have to regularly grapple with the problem of competing models. Be it analytical or physics-based models, a cross-cutting challenge for experts is to reliably diagnose which model outcomes appropriately predict or simulate real-world phenomena. Expert judgment involves reconciling information across many, and often, conflicting criteria that describe the quality of model outcomes. In this paper, through a design study with climate scientists, we develop a deeper understanding of the problem and solution space of model diagnostics, resulting in the following contributions: i) a problem and task characterization using which we map experts&#39; model diagnostics goals to multi-way visual comparison tasks, ii) a design space of comparative visual cues for letting experts quickly understand the degree of disagreement among competing models and gauge the degree of stability of model outputs with respect to alternative criteria, and iii) design and evaluation of MyriadCues, an interactive visualization interface for exploring alternative hypotheses and insights about good and bad models by leveraging comparative visual cues. We present case studies and subjective feedback by experts, which validate how MyriadCues enables more transparent model diagnostic mechanisms, as compared to the state of the art.},
  archive      = {J_TVCG},
  author       = {Aritra Dasgupta and Hong Wang and Nancy O&#39;Brien and Susannah Burrows},
  doi          = {10.1109/TVCG.2019.2934540},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1043-1053},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Separating the wheat from the chaff: Comparative visual cues for transparent diagnostics of competing models},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of radial and linear charts for visualizing
daily patterns. <em>TVCG</em>, <em>26</em>(1), 1033–1042. (<a
href="https://doi.org/10.1109/TVCG.2019.2934784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radial charts are generally considered less effective than linear charts. Perhaps the only exception is in visualizing periodical time-dependent data, which is believed to be naturally supported by the radial layout. It has been demonstrated that the drawbacks of radial charts outweigh the benefits of this natural mapping. Visualization of daily patterns, as a special case, has not been systematically evaluated using radial charts. In contrast to yearly or weekly recurrent trends, the analysis of daily patterns on a radial chart may benefit from our trained skill on reading radial clocks that are ubiquitous in our culture. In a crowd-sourced experiment with 92 non-expert users, we evaluated the accuracy, efficiency, and subjective ratings of radial and linear charts for visualizing daily traffic accident patterns. We systematically compared juxtaposed 12-hours variants and single 24-hours variants for both layouts in four low-level tasks and one high-level interpretation task. Our results show that over all tasks, the most elementary 24-hours linear bar chart is most accurate and efficient and is also preferred by the users. This provides strong evidence for the use of linear layouts - even for visualizing periodical daily patterns.},
  archive      = {J_TVCG},
  author       = {Manuela Waldner and Alexandra Diehl and Denis Gračanin and Rainer Splechtna and Claudio Delrieux and Krešimir Matković},
  doi          = {10.1109/TVCG.2019.2934784},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1033-1042},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparison of radial and linear charts for visualizing daily patterns},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BarcodeTree: Scalable comparison of multiple hierarchies.
<em>TVCG</em>, <em>26</em>(1), 1022–1032. (<a
href="https://doi.org/10.1109/TVCG.2019.2934535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose BarcodeTree (BCT), a novel visualization technique for comparing topological structures and node attribute values of multiple trees. BCT can provide an overview of one hundred shallow and stable trees simultaneously, without aggregating individual nodes. Each BCT is shown within a single row using a style similar to a barcode, allowing trees to be stacked vertically with matching nodes aligned horizontally to ease comparison and maintain space efficiency. We design several visual cues and interactive techniques to help users understand the topological structure and compare trees. In an experiment comparing two variants of BCT with icicle plots, the results suggest that BCTs make it easier to visually compare trees by reducing the vertical distance between different trees. We also present two case studies involving a dataset of hundreds of trees to demonstrate BCT&#39;s utility.},
  archive      = {J_TVCG},
  author       = {Guozheng Li and Yu Zhang and Yu Dong and Jie Liang and Jinson Zhang and Jinsong Wang and Michael J. Mcguffin and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2019.2934535},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1022-1032},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {BarcodeTree: Scalable comparison of multiple hierarchies},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The perceptual proxies of visual comparison. <em>TVCG</em>,
<em>26</em>(1), 1012–1021. (<a
href="https://doi.org/10.1109/TVCG.2019.2934786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., “biggest delta”, “biggest correlation”) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the “biggest mean” and “biggest range” between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a “Mean length” proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a “Hull Area” proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.},
  archive      = {J_TVCG},
  author       = {Nicole Jardine and Brian D. Ondov and Niklas Elmqvist and Steven Franconeri},
  doi          = {10.1109/TVCG.2019.2934786},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1012-1021},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The perceptual proxies of visual comparison},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic concept spaces: Guided topic model refinement using
word-embedding projections. <em>TVCG</em>, <em>26</em>(1), 1001–1011.
(<a href="https://doi.org/10.1109/TVCG.2019.2934654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users&#39; decisionmaking process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.},
  archive      = {J_TVCG},
  author       = {Mennatallah El-Assady and Rebecca Kehlbeck and Christopher Collins and Daniel Keim and Oliver Deussen},
  doi          = {10.1109/TVCG.2019.2934654},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1001-1011},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Semantic concept spaces: Guided topic model refinement using word-embedding projections},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ShapeWordle: Tailoring wordles using shape-aware archimedean
spirals. <em>TVCG</em>, <em>26</em>(1), 991–1000. (<a
href="https://doi.org/10.1109/TVCG.2019.2934783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new technique to enable the creation of shape-bounded Wordles, we call ShapeWordle, in which we fit words to form a given shape. To guide word placement within a shape, we extend the traditional Archimedean spirals to be shape-aware by formulating the spirals in a differential form using the distance field of the shape. To handle non-convex shapes, we introduce a multi-centric Wordle layout method that segments the shape into parts for our shape-aware spirals to adaptively fill the space and generate word placements. In addition, we offer a set of editing interactions to facilitate the creation of semantically-meaningful Wordles. Lastly, we present three evaluations: a comprehensive comparison of our results against the state-of-the-art technique (WordArt), case studies with 14 users, and a gallery to showcase the coverage of our technique.},
  archive      = {J_TVCG},
  author       = {Yunhai Wang and Xiaowei Chu and Kaiyi Zhang and Chen Bao and Xiaotong Li and Jian Zhang and Chi-Wing Fu and Christophe Hurter and Oliver Deussen and Bongshin Lee},
  doi          = {10.1109/TVCG.2019.2934783},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {991-1000},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ShapeWordle: Tailoring wordles using shape-aware archimedean spirals},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motion browser: Visualizing and understanding complex upper
limb movement under obstetrical brachial plexus injuries. <em>TVCG</em>,
<em>26</em>(1), 981–990. (<a
href="https://doi.org/10.1109/TVCG.2019.2934280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brachial plexus is a complex network of peripheral nerves that enables sensing from and control of the movements of the arms and hand. Nowadays, the coordination between the muscles to generate simple movements is still not well understood, hindering the knowledge of how to best treat patients with this type of peripheral nerve injury. To acquire enough information for medical data analysis, physicians conduct motion analysis assessments with patients to produce a rich dataset of electromyographic signals from multiple muscles recorded with joint movements during real-world tasks. However, tools for the analysis and visualization of the data in a succinct and interpretable manner are currently not available. Without the ability to integrate, compare, and compute multiple data sources in one platform, physicians can only compute simple statistical values to describe patient&#39;s behavior vaguely, which limits the possibility to answer clinical questions and generate hypotheses for research. To address this challenge, we have developed MOTION BROWSER, an interactive visual analytics system which provides an efficient framework to extract and compare muscle activity patterns from the patient&#39;s limbs and coordinated views to help users analyze muscle signals, motion data, and video information to address different tasks. The system was developed as a result of a collaborative endeavor between computer scientists and orthopedic surgery and rehabilitation physicians. We present case studies showing physicians can utilize the information displayed to understand how individuals coordinate their muscles to initiate appropriate treatment and generate new hypotheses for future research.},
  archive      = {J_TVCG},
  author       = {Gromit Yeuk-Yin Chan and Luis Gustavo Nonato and Alice Chu and Preeti Raghavan and Viswanath Aluru and Cláudio T. Silva},
  doi          = {10.1109/TVCG.2019.2934280},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {981-990},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Motion browser: Visualizing and understanding complex upper limb movement under obstetrical brachial plexus injuries},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Temporal views of flattened mitral valve geometries.
<em>TVCG</em>, <em>26</em>(1), 971–980. (<a
href="https://doi.org/10.1109/TVCG.2019.2934337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mitral valve, one of the four valves in the human heart, controls the bloodflow between the left atrium and ventricle and may suffer from various pathologies. Malfunctioning valves can be treated by reconstructive surgeries, which have to be carefully planned and evaluated. While current research focuses on the modeling and segmentation of the valve, we base our work on existing segmentations of patient-specific mitral valves, that are also time-resolved (3D+t) over the cardiac cycle. The interpretation of the data can be ambiguous, due to the complex surface of the valve and multiple time steps. We therefore propose a software prototype to analyze such 3D+t data, by extracting pathophysiological parameters and presenting them via dimensionally reduced visualizations. For this, we rely on an existing algorithm to unroll the convoluted valve surface towards a flattened 2D representation. In this paper, we show that the 3D+t data can be transferred to 3D or 2D representations in a way that allows the domain expert to faithfully grasp important aspects of the cardiac cycle. In this course, we not only consider common pathophysiological parameters, but also introduce new observations that are derived from landmarks within the segmentation model. Our analysis techniques were developed in collaboration with domain experts and a survey showed that the insights have the potential to support mitral valve diagnosis and the comparison of the preand post-operative condition of a patient.},
  archive      = {J_TVCG},
  author       = {Pepe Eulzer and Sandy Engelhardt and Nils Lichtenberg and Raffaele de Simone and Kai Lawonn},
  doi          = {10.1109/TVCG.2019.2934337},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {971-980},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Temporal views of flattened mitral valve geometries},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepOrganNet: On-the-fly reconstruction and visualization of
3D / 4D lung models from single-view projections by deep deformation
network. <em>TVCG</em>, <em>26</em>(1), 960–970. (<a
href="https://doi.org/10.1109/TVCG.2019.2934369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a deep neural network based method, i.e., DeepOrganNet, to generate and visualize fully high-fidelity 3D / 4D organ geometric models from single-view medical images with complicated background in real time. Traditional 3D / 4D medical image reconstruction requires near hundreds of projections, which cost insufferable computational time and deliver undesirable high imaging / radiation dose to human subjects. Moreover, it always needs further notorious processes to segment or extract the accurate 3D organ models subsequently. The computational time and imaging dose can be reduced by decreasing the number of projections, but the reconstructed image quality is degraded accordingly. To our knowledge, there is no method directly and explicitly reconstructing multiple 3D organ meshes from a single 2D medical grayscale image on the fly. Given single-view 2D medical images, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end DeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung models with a variety of geometric shapes by learning the smooth deformation fields from multiple templates based on a trivariate tensor-product deformation technique, leveraging an informative latent descriptor extracted from input 2D images. The proposed method can guarantee to generate high-quality and high-fidelity manifold meshes for 3D / 4D lung models; while, all current deep learning based approaches on the shape reconstruction from a single image cannot. The major contributions of this work are to accurately reconstruct the 3D organ shapes from 2D single-view projection, significantly improve the procedure time to allow on-the-fly visualization, and dramatically reduce the imaging dose for human subjects. Experimental results are evaluated and compared with the traditional reconstruction method and the state-of-the-art in deep learning, by using extensive 3D and 4D examples, including both synthetic phantom and real patient datasets. The efficiency of the proposed method shows that it only needs several milliseconds to generate organ meshes with 10K vertices, which has great potential to be used in real-time image guided radiation therapy (IGRT).},
  archive      = {J_TVCG},
  author       = {Yifan Wang and Zichun Zhong and Jing Hua},
  doi          = {10.1109/TVCG.2019.2934369},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {960-970},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DeepOrganNet: On-the-fly reconstruction and visualization of 3D / 4D lung models from single-view projections by deep deformation network},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cohort-based t-SSIM visual computing for radiation therapy
prediction and exploration. <em>TVCG</em>, <em>26</em>(1), 949–959. (<a
href="https://doi.org/10.1109/TVCG.2019.2934546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a visual computing approach to radiation therapy (RT) planning, based on spatial similarity within a patient cohort. In radiotherapy for head and neck cancer treatment, dosage to organs at risk surrounding a tumor is a large cause of treatment toxicity. Along with the availability of patient repositories, this situation has lead to clinician interest in understanding and predicting RT outcomes based on previously treated similar patients. To enable this type of analysis, we introduce a novel topology-based spatial similarity measure, T-SSIM, and a predictive algorithm based on this similarity measure. We couple the algorithm with a visual steering interface that intertwines visual encodings for the spatial data and statistical results, including a novel parallel-marker encoding that is spatially aware. We report quantitative results on a cohort of 165 patients, as well as a qualitative evaluation with domain experts in radiation oncology, data management, biostatistics, and medical imaging, who are collaborating remotely.},
  archive      = {J_TVCG},
  author       = {A. Wentzel and P. Hanula and T. Luciani and B. Elgohari and H. Elhalawani and G. Canahuate and D. Vock and C.D. Fuller and G.E. Marai},
  doi          = {10.1109/TVCG.2019.2934546},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {949-959},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cohort-based T-SSIM visual computing for radiation therapy prediction and exploration},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CerebroVis: Designing an abstract yet spatially
contextualized cerebral artery network visualization. <em>TVCG</em>,
<em>26</em>(1), 938–948. (<a
href="https://doi.org/10.1109/TVCG.2019.2934402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blood circulation in the human brain is supplied through a network of cerebral arteries. If a clinician suspects a patient has a stroke or other cerebrovascular condition, they order imaging tests. Neuroradiologists visually search the resulting scans for abnormalities. Their visual search tasks correspond to the abstract network analysis tasks of browsing and path following. To assist neuroradiologists in identifying cerebral artery abnormalities, we designed CerebroVis, a novel abstract-yet spatially contextualized-cerebral artery network visualization. In this design study, we contribute a novel framing and definition of the cerebral artery system in terms of network theory and characterize neuroradiologist domain goals as abstract visualization and network analysis tasks. Through an iterative, user-centered design process we developed an abstract network layout technique which incorporates cerebral artery spatial context. The abstract visualization enables increased domain task performance over 3D geometry representations, while including spatial context helps preserve the user&#39;s mental map of the underlying geometry. We provide open source implementations of our network layout technique and prototype cerebral artery visualization tool. We demonstrate the robustness of our technique by successfully laying out 61 open source brain scans. We evaluate the effectiveness of our layout through a mixed methods study with three neuroradiologists. In a formative controlled experiment our study participants used CerebroVis and a conventional 3D visualization to examine real cerebral artery imaging data to identify a simulated intracranial artery stenosis. Participants were more accurate at identifying stenoses using CerebroVis (absolute risk difference 13\%). A free copy of this paper, the evaluation stimuli and data, and source code are available at osf.io/e5sxt.},
  archive      = {J_TVCG},
  author       = {Aditeya Pandey and Harsh Shukla and Geoffrey S. Young and Lei Qin and Amir A. Zamani and Liangge Hsu and Raymond Huang and Cody Dunne and Michelle A. Borkin},
  doi          = {10.1109/TVCG.2019.2934402},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {938-948},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CerebroVis: Designing an abstract yet spatially contextualized cerebral artery network visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EmoCo: Visual analysis of emotion coherence in presentation
videos. <em>TVCG</em>, <em>26</em>(1), 927–937. (<a
href="https://doi.org/10.1109/TVCG.2019.2934656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.},
  archive      = {J_TVCG},
  author       = {Haipeng Zeng and Xingbo Wang and Aoyu Wu and Yong Wang and Quan Li and Alex Endert and Huamin Qu},
  doi          = {10.1109/TVCG.2019.2934656},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {927-937},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EmoCo: Visual analysis of emotion coherence in presentation videos},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards automated infographic design: Deep learning-based
auto-extraction of extensible timeline. <em>TVCG</em>, <em>26</em>(1),
917–926. (<a href="https://doi.org/10.1109/TVCG.2019.2934810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designers need to consider not only perceptual effectiveness but also visual styles when creating an infographic. This process can be difficult and time consuming for professional designers, not to mention non-expert users, leading to the demand for automated infographics design. As a first step, we focus on timeline infographics, which have been widely used for centuries. We contribute an end-to-end approach that automatically extracts an extensible timeline template from a bitmap image. Our approach adopts a deconstruction and reconstruction paradigm. At the deconstruction stage, we propose a multi-task deep neural network that simultaneously parses two kinds of information from a bitmap timeline: 1) the global information, i.e., the representation, scale, layout, and orientation of the timeline, and 2) the local information, i.e., the location, category, and pixels of each visual element on the timeline. At the reconstruction stage, we propose a pipeline with three techniques, i.e., Non-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an extensible template from the infographic, by utilizing the deconstruction results. To evaluate the effectiveness of our approach, we synthesize a timeline dataset (4296 images) and collect a real-world timeline dataset (393 images) from the Internet. We first report quantitative evaluation results of our approach over the two datasets. Then, we present examples of automatically extracted templates and timelines automatically generated based on these templates to qualitatively demonstrate the performance. The results confirm that our approach can effectively extract extensible templates from real-world timeline infographics.},
  archive      = {J_TVCG},
  author       = {Zhutian Chen and Yun Wang and Qianwen Wang and Yong Wang and Huamin Qu},
  doi          = {10.1109/TVCG.2019.2934810},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {917-926},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards automated infographic design: Deep learning-based auto-extraction of extensible timeline},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Text-to-viz: Automatic generation of infographics from
proportion-related natural language statements. <em>TVCG</em>,
<em>26</em>(1), 906–916. (<a
href="https://doi.org/10.1109/TVCG.2019.2934785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.},
  archive      = {J_TVCG},
  author       = {Weiwei Cui and Xiaoyu Zhang and Yun Wang and He Huang and Bei Chen and Lei Fang and Haidong Zhang and Jian-Guan Lou and Dongmei Zhang},
  doi          = {10.1109/TVCG.2019.2934785},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {906-916},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Text-to-viz: Automatic generation of infographics from proportion-related natural language statements},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DataShot: Automatic generation of fact sheets from tabular
data. <em>TVCG</em>, <em>26</em>(1), 895–905. (<a
href="https://doi.org/10.1109/TVCG.2019.2934398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fact sheets with vivid graphical design and intriguing statistical insights are prevalent for presenting raw data. They help audiences understand data-related facts effectively and make a deep impression. However, designing a fact sheet requires both data and design expertise and is a laborious and time-consuming process. One needs to not only understand the data in depth but also produce intricate graphical representations. To assist in the design process, we present DataShot which, to the best of our knowledge, is the first automated system that creates fact sheets automatically from tabular data. First, we conduct a qualitative analysis of 245 infographic examples to explore general infographic design space at both the sheet and element levels. We identify common infographic structures, sheet layouts, fact types, and visualization styles during the study. Based on these findings, we propose a fact sheet generation pipeline, consisting of fact extraction, fact composition, and presentation synthesis, for the auto-generation workflow. To validate our system, we present use cases with three real-world datasets. We conduct an in-lab user study to understand the usage of our system. Our evaluation results show that DataShot can efficiently generate satisfactory fact sheets to support further customization and data presentation.},
  archive      = {J_TVCG},
  author       = {Yun Wang and Zhida Sun and Haidong Zhang and Weiwei Cui and Ke Xu and Xiaojuan Ma and Dongmei Zhang},
  doi          = {10.1109/TVCG.2019.2934398},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {895-905},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DataShot: Automatic generation of fact sheets from tabular data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual interaction with deep learning models through
collaborative semantic inference. <em>TVCG</em>, <em>26</em>(1),
884–894. (<a href="https://doi.org/10.1109/TVCG.2019.2934595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.},
  archive      = {J_TVCG},
  author       = {Sebastian Gehrmann and Hendrik Strobelt and Robert Krüger and Hanspeter Pfister and Alexander M. Rush},
  doi          = {10.1109/TVCG.2019.2934595},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {884-894},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual interaction with deep learning models through collaborative semantic inference},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VASSL: A visual analytics toolkit for social spambot
labeling. <em>TVCG</em>, <em>26</em>(1), 874–883. (<a
href="https://doi.org/10.1109/TVCG.2019.2934266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.},
  archive      = {J_TVCG},
  author       = {Mosab Khayat and Morteza Karimzadeh and Jieqiong Zhao and David S. Ebert},
  doi          = {10.1109/TVCG.2019.2934266},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {874-883},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VASSL: A visual analytics toolkit for social spambot labeling},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ablate, variate, and contemplate: Visual analytics for
discovering neural architectures. <em>TVCG</em>, <em>26</em>(1),
863–873. (<a href="https://doi.org/10.1109/TVCG.2019.2934261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.},
  archive      = {J_TVCG},
  author       = {Dylan Cashman and Adam Perer and Remco Chang and Hendrik Strobelt},
  doi          = {10.1109/TVCG.2019.2934261},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {863-873},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Ablate, variate, and contemplate: Visual analytics for discovering neural architectures},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Illusion of causality in visualized data. <em>TVCG</em>,
<em>26</em>(1), 853–862. (<a
href="https://doi.org/10.1109/TVCG.2019.2934399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Students who eat breakfast more frequently tend to have a higher grade point average. From this data, many people might confidently state that a before-school breakfast program would lead to higher grades. This is a reasoning error, because correlation does not necessarily indicate causation - X and Y can be correlated without one directly causing the other. While this error is pervasive, its prevalence might be amplified or mitigated by the way that the data is presented to a viewer. Across three crowdsourced experiments, we examined whether how simple data relations are presented would mitigate this reasoning error. The first experiment tested examples similar to the breakfast-GPA relation, varying in the plausibility of the causal link. We asked participants to rate their level of agreement that the relation was correlated, which they rated appropriately as high. However, participants also expressed high agreement with a causal interpretation of the data. Levels of support for the causal interpretation were not equally strong across visualization types: causality ratings were highest for text descriptions and bar graphs, but weaker for scatter plots. But is this effect driven by bar graphs aggregating data into two groups or by the visual encoding type? We isolated data aggregation versus visual encoding type and examined their individual effect on perceived causality. Overall, different visualization designs afford different cognitive reasoning affordances across the same data. High levels of data aggregation by graphs tend to be associated with higher perceived causality in data. Participants perceived line and dot visual encodings as more causal than bar encodings. Our results demonstrate how some visualization designs trigger stronger causal links while choosing others can help mitigate unwarranted perceptions of causality.},
  archive      = {J_TVCG},
  author       = {Cindy Xiong and Joel Shapiro and Jessica Hullman and Steven Franconeri},
  doi          = {10.1109/TVCG.2019.2934399},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {853-862},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Illusion of causality in visualized data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiscale visual drilldown for the analysis of large
ensembles of multi-body protein complexes. <em>TVCG</em>,
<em>26</em>(1), 843–852. (<a
href="https://doi.org/10.1109/TVCG.2019.2934333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When studying multi-body protein complexes, biochemists use computational tools that can suggest hundreds or thousands of their possible spatial configurations. However, it is not feasible to experimentally verify more than only a very small subset of them. In this paper, we propose a novel multiscale visual drilldown approach that was designed in tight collaboration with proteomic experts, enabling a systematic exploration of the configuration space. Our approach takes advantage of the hierarchical structure of the data - from the whole ensemble of protein complex configurations to the individual configurations, their contact interfaces, and the interacting amino acids. Our new solution is based on interactively linked 2D and 3D views for individual hierarchy levels. At each level, we offer a set of selection and filtering operations that enable the user to narrow down the number of configurations that need to be manually scrutinized. Furthermore, we offer a dedicated filter interface, which provides the users with an overview of the applied filtering operations and enables them to examine their impact on the explored ensemble. This way, we maintain the history of the exploration process and thus enable the user to return to an earlier point of the exploration. We demonstrate the effectiveness of our approach on two case studies conducted by collaborating proteomic experts.},
  archive      = {J_TVCG},
  author       = {Katarína Furmanová and Adam Jurčík and Barbora Kozlíková and Helwig Hauser and Jan Byška},
  doi          = {10.1109/TVCG.2019.2934333},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {843-852},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multiscale visual drilldown for the analysis of large ensembles of multi-body protein complexes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A structural average of labeled merge trees for uncertainty
visualization. <em>TVCG</em>, <em>26</em>(1), 832–842. (<a
href="https://doi.org/10.1109/TVCG.2019.2934242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical phenomena in science and engineering are frequently modeled using scalar fields. In scalar field topology, graph-based topological descriptors such as merge trees, contour trees, and Reeb graphs are commonly used to characterize topological changes in the (sub)level sets of scalar fields. One of the biggest challenges and opportunities to advance topology-based visualization is to understand and incorporate uncertainty into such topological descriptors to effectively reason about their underlying data. In this paper, we study a structural average of a set of labeled merge trees and use it to encode uncertainty in data. Specifically, we compute a 1-center tree that minimizes its maximum distance to any other tree in the set under a well-defined metric called the interleaving distance. We provide heuristic strategies that compute structural averages of merge trees whose labels do not fully agree. We further provide an interactive visualization system that resembles a numerical calculator that takes as input a set of merge trees and outputs a tree as their structural average. We also highlight structural similarities between the input and the average and incorporate uncertainty information for visual exploration. We develop a novel measure of uncertainty, referred to as consistency, via a metric-space view of the input trees. Finally, we demonstrate an application of our framework through merge trees that arise from ensembles of scalar fields. Our work is the first to employ interleaving distances and consistency to study a global, mathematically rigorous, structural average of merge trees in the context of uncertainty visualization.},
  archive      = {J_TVCG},
  author       = {Lin Yan and Yusu Wang and Elizabeth Munch and Ellen Gasparovic and Bei Wang},
  doi          = {10.1109/TVCG.2019.2934242},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {832-842},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A structural average of labeled merge trees for uncertainty visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty-aware principal component analysis.
<em>TVCG</em>, <em>26</em>(1), 822–831. (<a
href="https://doi.org/10.1109/TVCG.2019.2934812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a technique to perform dimensionality reduction on data that is subject to uncertainty. Our method is a generalization of traditional principal component analysis (PCA) to multivariate probability distributions. In comparison to non-linear methods, linear dimensionality reduction techniques have the advantage that the characteristics of such probability distributions remain intact after projection. We derive a representation of the PCA sample covariance matrix that respects potential uncertainty in each of the inputs, building the mathematical foundation of our new method: uncertainty-aware PCA. In addition to the accuracy and performance gained by our approach over sampling-based strategies, our formulation allows us to perform sensitivity analysis with regard to the uncertainty in the data. For this, we propose factor traces as a novel visualization that enables to better understand the influence of uncertainty on the chosen principal components. We provide multiple examples of our technique using real-world datasets. As a special case, we show how to propagate multivariate normal distributions through PCA in closed form. Furthermore, we discuss extensions and limitations of our approach.},
  archive      = {J_TVCG},
  author       = {Jochen Görtler and Thilo Spinner and Dirk Streeb and Daniel Weiskopf and Oliver Deussen},
  doi          = {10.1109/TVCG.2019.2934812},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {822-831},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncertainty-aware principal component analysis},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OD morphing: Balancing simplicity with faithfulness for OD
bundling. <em>TVCG</em>, <em>26</em>(1), 811–821. (<a
href="https://doi.org/10.1109/TVCG.2019.2934657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OD bundling is a promising method to identify key origin-destination (OD) patterns, but the bundling can mislead the interpretation of actual trajectories traveled. We present OD Morphing, an interactive OD bundling technique that improves geographical faithfulness to actual trajectories while preserving visual simplicity for OD patterns. OD Morphing iteratively identifies critical waypoints from the actual trajectory network with a min-cut algorithm and transitions OD bundles to pass through the identified waypoints with a smooth morphing method. Furthermore, we extend OD Morphing to support bundling at interaction speeds to enable users to interactively transition between degrees of faithfulness to aid sensemaking. We introduce metrics for faithfulness and simplicity to evaluate their trade-off achieved by OD morphed bundling. We demonstrate OD Morphing on real-world city-scale taxi trajectory and USA domestic planned flight datasets.},
  archive      = {J_TVCG},
  author       = {Yan Lyu and Xu Liu and Hanyi Chen and Arpan Mangal and Kai Liu and Chao Chen and Brian Lim},
  doi          = {10.1109/TVCG.2019.2934657},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {811-821},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OD morphing: Balancing simplicity with faithfulness for OD bundling},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AirVis: Visual analytics of air pollution propagation.
<em>TVCG</em>, <em>26</em>(1), 800–810. (<a
href="https://doi.org/10.1109/TVCG.2019.2934670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.},
  archive      = {J_TVCG},
  author       = {Zikun Deng and Di Weng and Jiahui Chen and Ren Liu and Zhibin Wang and Jie Bao and Yu Zheng and Yingcai Wu},
  doi          = {10.1109/TVCG.2019.2934670},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {800-810},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AirVis: Visual analytics of air pollution propagation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SmartCube: An adaptive data management architecture for the
real-time visualization of spatiotemporal datasets. <em>TVCG</em>,
<em>26</em>(1), 790–799. (<a
href="https://doi.org/10.1109/TVCG.2019.2934434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive visualization and exploration of large spatiotemporal data sets is difficult without carefully-designed data preprocessing and management tools. We propose a novel architecture for spatiotemporal data management. The architecture can dynamically update itself based on user queries. Datasets is stored in a tree-like structure to support memory sharing among cuboids in a logical structure of data cubes. An update mechanism is designed to create or remove cuboids on it, according to the analysis of the user queries, with the consideration of memory size limitation. Data structure is dynamically optimized according to different user queries. During a query process, user queries are recorded to predict the performance increment of the new cuboid. The creation or deletion of a cuboid is determined by performance increment. Experiment results show that our prototype system deliveries good performance towards user queries on different spatiotemporal datasets, which costing small memory size with comparable performance compared with other state-of-the-art algorithms.},
  archive      = {J_TVCG},
  author       = {Can Liu and Cong Wu and Hanning Shao and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2019.2934434},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {790-799},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SmartCube: An adaptive data management architecture for the real-time visualization of spatiotemporal datasets},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Void-and-cluster sampling of large scattered data and
trajectories. <em>TVCG</em>, <em>26</em>(1), 780–789. (<a
href="https://doi.org/10.1109/TVCG.2019.2934335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a data reduction technique for scattered data based on statistical sampling. Our void-and-cluster sampling technique finds a representative subset that is optimally distributed in the spatial domain with respect to the blue noise property. In addition, it can adapt to a given density function, which we use to sample regions of high complexity in the multivariate value domain more densely. Moreover, our sampling technique implicitly defines an ordering on the samples that enables progressive data loading and a continuous level-of-detail representation. We extend our technique to sample time-dependent trajectories, for example pathlines in a time interval, using an efficient and iterative approach. Furthermore, we introduce a local and continuous error measure to quantify how well a set of samples represents the original dataset. We apply this error measure during sampling to guide the number of samples that are taken. Finally, we use this error measure and other quantities to evaluate the quality, performance, and scalability of our algorithm.},
  archive      = {J_TVCG},
  author       = {Tobias Rapp and Christoph Peters and Carsten Dachsbacher},
  doi          = {10.1109/TVCG.2019.2934335},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {780-789},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Void-and-cluster sampling of large scattered data and trajectories},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Winglets: Visualizing association with uncertainty in
multi-class scatterplots. <em>TVCG</em>, <em>26</em>(1), 770–779. (<a
href="https://doi.org/10.1109/TVCG.2019.2934811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes Winglets, an enhancement to the classic scatterplot to better perceptually pronounce multiple classes by improving the perception of association and uncertainty of points to their related cluster. Designed as a pair of dual-sided strokes belonging to a data point, Winglets leverage the Gestalt principle of Closure to shape the perception of the form of the clusters, rather than use an explicit divisive encoding. Through a subtle design of two dominant attributes, length and orientation, Winglets enable viewers to perform a mental completion of the clusters. A controlled user study was conducted to examine the efficiency of Winglets in perceiving the cluster association and the uncertainty of certain points. The results show Winglets form a more prominent association of points into clusters and improve the perception of associating uncertainty.},
  archive      = {J_TVCG},
  author       = {Min Lu and Shuaiqi Wang and Joel Lanir and Noa Fish and Yang Yue and Daniel Cohen-Or and Hui Huang},
  doi          = {10.1109/TVCG.2019.2934811},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {770-779},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Winglets: Visualizing association with uncertainty in multi-class scatterplots},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the robustness of scagnostics. <em>TVCG</em>,
<em>26</em>(1), 759–769. (<a
href="https://doi.org/10.1109/TVCG.2019.2934796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy , are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics ( RScag ) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.},
  archive      = {J_TVCG},
  author       = {Yunhai Wang and Zeyu Wang and Tingting Liu and Michael Correll and Zhanglin Cheng and Oliver Deussen and Michael Sedlmair},
  doi          = {10.1109/TVCG.2019.2934796},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {759-769},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving the robustness of scagnostics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminability tests for visualization effectiveness and
scalability. <em>TVCG</em>, <em>26</em>(1), 749–758. (<a
href="https://doi.org/10.1109/TVCG.2019.2934432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure&#39;s utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.},
  archive      = {J_TVCG},
  author       = {Rafael Veras and Christopher Collins},
  doi          = {10.1109/TVCG.2019.2934432},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {749-758},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Discriminability tests for visualization effectiveness and scalability},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data sampling in multi-view and multi-class scatterplots via
set cover optimization. <em>TVCG</em>, <em>26</em>(1), 739–748. (<a
href="https://doi.org/10.1109/TVCG.2019.2934799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.},
  archive      = {J_TVCG},
  author       = {Ruizhen Hu and Tingkai Sha and Oliver Van Kaick and Oliver Deussen and Hui Huang},
  doi          = {10.1109/TVCG.2019.2934799},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {739-748},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data sampling in multi-view and multi-class scatterplots via set cover optimization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A recursive subdivision technique for sampling multi-class
scatterplots. <em>TVCG</em>, <em>26</em>(1), 729–738. (<a
href="https://doi.org/10.1109/TVCG.2019.2934541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.},
  archive      = {J_TVCG},
  author       = {Xin Chen and Tong Ge and Jian Zhang and Baoquan Chen and Chi-Wing Fu and Oliver Deussen and Yunhai Wang},
  doi          = {10.1109/TVCG.2019.2934541},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {729-738},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A recursive subdivision technique for sampling multi-class scatterplots},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of the near-wall flow in a turbine cascade by splat
visualization. <em>TVCG</em>, <em>26</em>(1), 719–728. (<a
href="https://doi.org/10.1109/TVCG.2019.2934367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Turbines are essential components of jet planes and power plants. Therefore, their efficiency and service life are of central engineering interest. In the case of jet planes or thermal power plants, the heating of the turbines due to the hot gas flow is critical. Besides effective cooling, it is a major goal of engineers to minimize heat transfer between gas flow and turbine by design. Since it is known that splat events have a substantial impact on the heat transfer between flow and immersed surfaces, we adapt a splat detection and visualization method to a turbine cascade simulation in this case study. Because splat events are small phenomena, we use a direct numerical simulation resolving the turbulence in the flow as the base of our analysis. The outcome shows promising insights into splat formation and its relation to vortex structures. This may lead to better turbine design in the future.},
  archive      = {J_TVCG},
  author       = {Baldwin Nsonga and Gerik Scheuermann and Stefan Gumhold and Jordi Ventosa-Molina and Denis Koschichow and Jochen Fröhlich},
  doi          = {10.1109/TVCG.2019.2934367},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {719-728},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analysis of the near-wall flow in a turbine cascade by splat visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Accelerated monte carlo rendering of finite-time lyapunov
exponents. <em>TVCG</em>, <em>26</em>(1), 708–718. (<a
href="https://doi.org/10.1109/TVCG.2019.2934313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-dependent fluid flows often contain numerous hyperbolic Lagrangian coherent structures, which act as transport barriers that guide the advection. The finite-time Lyapunov exponent is a commonly-used approximation to locate these repelling or attracting structures. Especially on large numerical simulations, the FTLE ridges can become arbitrarily sharp and very complex. Thus, the discrete sampling onto a grid for a subsequent direct volume rendering is likely to miss sharp ridges in the visualization. For this reason, an unbiased Monte Carlo-based rendering approach was recently proposed that treats the FTLE field as participating medium with single scattering. This method constructs a ground truth rendering without discretization, but it is prohibitively slow with render times in the order of days or weeks for a single image. In this paper, we accelerate the rendering process significantly, which allows us to compute video sequence of high-resolution FTLE animations in a much more reasonable time frame. For this, we follow two orthogonal approaches to improve on the rendering process: the volumetric light path integration in gradient domain and an acceleration of the transmittance estimation. We analyze the convergence and performance of the proposed method and demonstrate the approach by rendering complex FTLE fields in several 3D vector fields.},
  archive      = {J_TVCG},
  author       = {Irene Baeza Rojo and Markus Gross and Tobias Günther},
  doi          = {10.1109/TVCG.2019.2934313},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {708-718},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Accelerated monte carlo rendering of finite-time lyapunov exponents},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Persistent homology guided force-directed graph layouts.
<em>TVCG</em>, <em>26</em>(1), 697–707. (<a
href="https://doi.org/10.1109/TVCG.2019.2934802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are commonly used to encode relationships among entities, yet their abstractness makes them difficult to analyze. Node-link diagrams are popular for drawing graphs, and force-directed layouts provide a flexible method for node arrangements that use local relationships in an attempt to reveal the global shape of the graph. However, clutter and overlap of unrelated structures can lead to confusing graph visualizations. This paper leverages the persistent homology features of an undirected graph as derived information for interactive manipulation of force-directed layouts. We first discuss how to efficiently extract 0-dimensional persistent homology features from both weighted and unweighted undirected graphs. We then introduce the interactive persistence barcode used to manipulate the force-directed graph layout. In particular, the user adds and removes contracting and repulsing forces generated by the persistent homology features, eventually selecting the set of persistent homology features that most improve the layout. Finally, we demonstrate the utility of our approach across a variety of synthetic and real datasets.},
  archive      = {J_TVCG},
  author       = {Ashley Suh and Mustafa Hajij and Bei Wang and Carlos Scheidegger and Paul Rosen},
  doi          = {10.1109/TVCG.2019.2934802},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {697-707},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Persistent homology guided force-directed graph layouts},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive structure-aware blending of diverse edge
bundling visualizations. <em>TVCG</em>, <em>26</em>(1), 687–696. (<a
href="https://doi.org/10.1109/TVCG.2019.2934805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many edge bundling techniques (i.e., data simplification as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and difficult to set up. As a result, this hinders the user ability to create efficient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: first, the user investigates different edge bundling results and specifies areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these specified areas. Lastly, the user can further fine-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations.},
  archive      = {J_TVCG},
  author       = {Yunhai Wang and Mingliang Xue and Yanyan Wang and Xinyuan Yan and Baoquan Chen and Chi-Wing Fu and Christophe Hurter},
  doi          = {10.1109/TVCG.2019.2934805},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {687-696},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive structure-aware blending of diverse edge bundling visualizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepDrawing: A deep learning approach to graph drawing.
<em>TVCG</em>, <em>26</em>(1), 676–686. (<a
href="https://doi.org/10.1109/TVCG.2019.2934798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node-link diagrams are widely used to facilitate network explorations. However, when using a graph drawing technique to visualize networks, users often need to tune different algorithm-specific parameters iteratively by comparing the corresponding drawing results in order to achieve a desired visual effect. This trial and error process is often tedious and time-consuming, especially for non-expert users. Inspired by the powerful data modelling and prediction capabilities of deep learning techniques, we explore the possibility of applying deep learning techniques to graph drawing. Specifically, we propose using a graph-LSTM-based approach to directly map network structures to graph drawings. Given a set of layout examples as the training dataset, we train the proposed graph-LSTM-based model to capture their layout characteristics. Then, the trained model is used to generate graph drawings in a similar style for new networks. We evaluated the proposed approach on two special types of layouts (i.e., grid layouts and star layouts) and two general types of layouts (i.e., ForceAtlas2 and PivotMDS) in both qualitative and quantitative ways. The results provide support for the effectiveness of our approach. We also conducted a time cost assessment on the drawings of small graphs with 20 to 50 nodes. We further report the lessons we learned and discuss the limitations and future work.},
  archive      = {J_TVCG},
  author       = {Yong Wang and Zhihua Jin and Qianwen Wang and Weiwei Cui and Tengfei Ma and Huamin Qu},
  doi          = {10.1109/TVCG.2019.2934798},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {676-686},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DeepDrawing: A deep learning approach to graph drawing},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep generative model for graph layout. <em>TVCG</em>,
<em>26</em>(1), 665–675. (<a
href="https://doi.org/10.1109/TVCG.2019.2934396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different layouts can characterize different aspects of the same graph. Finding a “good” layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.},
  archive      = {J_TVCG},
  author       = {Oh-Hyun Kwon and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2019.2934396},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {665-675},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A deep generative model for graph layout},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scale trotter: Illustrative visual travels across negative
scales. <em>TVCG</em>, <em>26</em>(1), 654–664. (<a
href="https://doi.org/10.1109/TVCG.2019.2934334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels—the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out—instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.},
  archive      = {J_TVCG},
  author       = {Sarkis Halladjian and Haichao Miao and David Kouřil and M. Eduard Gröller and Ivan Viola and Tobias Isenberg},
  doi          = {10.1109/TVCG.2019.2934334},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {654-664},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scale trotter: Illustrative visual travels across negative scales},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scale-space splatting: Reforming spacetime for cross-scale
exploration of integral measures in molecular dynamics. <em>TVCG</em>,
<em>26</em>(1), 643–653. (<a
href="https://doi.org/10.1109/TVCG.2019.2934258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space/time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation.},
  archive      = {J_TVCG},
  author       = {Juraj Pálenik and Jan Byška and Stefan Bruckner and Helwig Hauser},
  doi          = {10.1109/TVCG.2019.2934258},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {643-653},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scale-space splatting: Reforming spacetime for cross-scale exploration of integral measures in molecular dynamics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OpenSpace: A system for astrographics. <em>TVCG</em>,
<em>26</em>(1), 633–642. (<a
href="https://doi.org/10.1109/TVCG.2019.2934259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human knowledge about the cosmos is rapidly increasing as instruments and simulations are generating new data supporting the formation of theory and understanding of the vastness and complexity of the universe. OpenSpace is a software system that takes on the mission of providing an integrated view of all these sources of data and supports interactive exploration of the known universe from the millimeter scale showing instruments on spacecrafts to billions of light years when visualizing the early universe. The ambition is to support research in astronomy and space exploration, science communication at museums and in planetariums as well as bringing exploratory astrographics to the class room. There is a multitude of challenges that need to be met in reaching this goal such as the data variety, multiple spatio-temporal scales, collaboration capabilities, etc. Furthermore, the system has to be flexible and modular to enable rapid prototyping and inclusion of new research results or space mission data and thereby shorten the time from discovery to dissemination. To support the different use cases the system has to be hardware agnostic and support a range of platforms and interaction paradigms. In this paper we describe how OpenSpace meets these challenges in an open source effort that is paving the path for the next generation of interactive astrographics.},
  archive      = {J_TVCG},
  author       = {Alexander Bock and Emil Axelsson and Jonathas Costa and Gene Payne and Micah Acinapura and Vivian Trakinski and Carter Emmart and Cláudio Silva and Charles Hansen and Anders Ynnerman},
  doi          = {10.1109/TVCG.2019.2934259},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {633-642},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OpenSpace: A system for astrographics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale procedural animations of microtubule dynamics
based on measured data. <em>TVCG</em>, <em>26</em>(1), 622–632. (<a
href="https://doi.org/10.1109/TVCG.2019.2934612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biologists often use computer graphics to visualize structures, which due to physical limitations are not possible to image with a microscope. One example for such structures are microtubules, which are present in every eukaryotic cell. They are part of the cytoskeleton maintaining the shape of the cell and playing a key role in the cell division. In this paper, we propose a scientifically-accurate multi-scale procedural model of microtubule dynamics as a novel application scenario for procedural animation, which can generate visualizations of their overall shape, molecular structure, as well as animations of the dynamic behaviour of their growth and disassembly. The model is spanning from tens of micrometers down to atomic resolution. All the aspects of the model are driven by scientific data. The advantage over a traditional, manual animation approach is that when the underlying data change, for instance due to new evidence, the model can be recreated immediately. The procedural animation concept is presented in its generic form, with several novel extensions, facilitating an easy translation to other domains with emergent multi-scale behavior.},
  archive      = {J_TVCG},
  author       = {Tobias Klein and Ivan Viola and Eduard Gröller and Peter Mindek},
  doi          = {10.1109/TVCG.2019.2934612},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {622-632},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-scale procedural animations of microtubule dynamics based on measured data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pattern-driven navigation in 2D multiscale visualizations
with scalable insets. <em>TVCG</em>, <em>26</em>(1), 611–621. (<a
href="https://doi.org/10.1109/TVCG.2019.2934555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario.},
  archive      = {J_TVCG},
  author       = {Fritz Lekschas and Michael Behrisch and Benjamin Bach and Peter Kerpedjiev and Nils Gehlenborg and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2019.2934555},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {611-621},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pattern-driven navigation in 2D multiscale visualizations with scalable insets},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SPortfolio: Stratified visual analysis of stock portfolios.
<em>TVCG</em>, <em>26</em>(1), 601–610. (<a
href="https://doi.org/10.1109/TVCG.2019.2934660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantitative Investment, built on the solid foundation of robust financial theories, is at the center stage in investment industry today. The essence of quantitative investment is the multi-factor model, which explains the relationship between the risk and return of equities. However, the multi-factor model generates enormous quantities of factor data, through which even experienced portfolio managers find it difficult to navigate. This has led to portfolio analysis and factor research being limited by a lack of intuitive visual analytics tools. Previous portfolio visualization systems have mainly focused on the relationship between the portfolio return and stock holdings, which is insufficient for making actionable insights or understanding market trends. In this paper, we present sPortfolio, which, to the best of our knowledge, is the first visualization that attempts to explore the factor investment area. In particular, sPortfolio provides a holistic overview of the factor data and aims to facilitate the analysis at three different levels: a Risk-Factor level, for a general market situation analysis; a Multiple-Portfolio level, for understanding the portfolio strategies; and a Single-Portfolio level, for investigating detailed operations. The system&#39;s effectiveness and usability are demonstrated through three case studies. The system has passed its pilot study and is soon to be deployed in industry.},
  archive      = {J_TVCG},
  author       = {Xuanwu Yue and Jiaxin Bai and Qinhan Liu and Yiyang Tang and Abishek Puri and Ke Li and Huamin Qu},
  doi          = {10.1109/TVCG.2019.2934660},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {601-610},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SPortfolio: Stratified visual analysis of stock portfolios},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual analytics for electromagnetic situation awareness in
radio monitoring and management. <em>TVCG</em>, <em>26</em>(1), 590–600.
(<a href="https://doi.org/10.1109/TVCG.2019.2934655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional radio monitoring and management largely depend on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and frequently results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environments, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal clustering method to process radio signal data and a situation assessment model to obtain qualitative and quantitative descriptions of the electromagnetic situations. We then design a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand the electromagnetic situations by a joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of the proposed approach and provide future work directions.},
  archive      = {J_TVCG},
  author       = {Ying Zhao and Xiaobo Luo and Xiaoru Lin and Hairong Wang and Xiaoyan Kui and Fangfang Zhou and Jinsong Wang and Yi Chen and Wei Chen},
  doi          = {10.1109/TVCG.2019.2934655},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {590-600},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analytics for electromagnetic situation awareness in radio monitoring and management},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PlanningVis: A visual analytics approach to production
planning in smart factories. <em>TVCG</em>, <em>26</em>(1), 579–589. (<a
href="https://doi.org/10.1109/TVCG.2019.2934275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Production planning in the manufacturing industry is crucial for fully utilizing factory resources (e.g., machines, raw materials and workers) and reducing costs. With the advent of industry 4.0, plenty of data recording the status of factory resources have been collected and further involved in production planning, which brings an unprecedented opportunity to understand, evaluate and adjust complex production plans through a data-driven approach. However, developing a systematic analytics approach for production planning is challenging due to the large volume of production data, the complex dependency between products, and unexpected changes in the market and the plant. Previous studies only provide summarized results and fail to show details for comparative analysis of production plans. Besides, the rapid adjustment to the plan in the case of an unanticipated incident is also not supported. In this paper, we propose PlanningVis, a visual analytics system to support the exploration and comparison of production plans with three levels of details: a plan overview presenting the overall difference between plans, a product view visualizing various properties of individual products, and a production detail view displaying the product dependency and the daily production details in related factories. By integrating an automatic planning algorithm with interactive visual explorations, PlanningVis can facilitate the efficient optimization of daily production planning as well as support a quick response to unanticipated incidents in manufacturing. Two case studies with real-world data and carefully designed interviews with domain experts demonstrate the effectiveness and usability of PlanningVis.},
  archive      = {J_TVCG},
  author       = {Dong Sun and Renfei Huang and Yuanzhe Chen and Yong Wang and Jia Zeng and Mingxuan Yuan and Ting-Chuen Pong and Huamin Qu},
  doi          = {10.1109/TVCG.2019.2934275},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {579-589},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PlanningVis: A visual analytics approach to production planning in smart factories},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LightGuider: Guiding interactive lighting design using
suggestions, provenance, and quality visualization. <em>TVCG</em>,
<em>26</em>(1), 569–578. (<a
href="https://doi.org/10.1109/TVCG.2019.2934658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that can deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree. This provenance tree integrates snapshot visualizations of how well a design meets the given quality criteria weighted by the designer&#39;s preferences. This integration facilitates the analysis of quality improvements over the course of a modeling workflow as well as the comparison of alternative design solutions. We evaluate our approach with three lighting designers to illustrate its usefulness.},
  archive      = {J_TVCG},
  author       = {Andreas Walch and Michael Schwärzler and Christian Luksch and Elmar Eisemann and Theresia Gschwandtner},
  doi          = {10.1109/TVCG.2019.2934658},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {569-578},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LightGuider: Guiding interactive lighting design using suggestions, provenance, and quality visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive learning for identifying relevant tweets to
support real-time situational awareness. <em>TVCG</em>, <em>26</em>(1),
558–568. (<a href="https://doi.org/10.1109/TVCG.2019.2934614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users&#39; knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework&#39;s effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.},
  archive      = {J_TVCG},
  author       = {Luke S. Snyder and Yi-Shan Lin and Morteza Karimzadeh and Dan Goldwasser and David S. Ebert},
  doi          = {10.1109/TVCG.2019.2934614},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {558-568},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive learning for identifying relevant tweets to support real-time situational awareness},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deadeye visualization revisited: Investigation of
preattentiveness and applicability in virtual environments.
<em>TVCG</em>, <em>26</em>(1), 547–557. (<a
href="https://doi.org/10.1109/TVCG.2019.2934370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizations rely on highlighting to attract and guide our attention. To make an object of interest stand out independently from a number of distractors, the underlying visual cue, e.g., color, has to be preattentive. In our prior work, we introduced Deadeye as an instantly recognizable highlighting technique that works by rendering the target object for one eye only. In contrast to prior approaches, Deadeye excels by not modifying any visual properties of the target. However, in the case of 2D visualizations, the method requires an additional setup to allow dichoptic presentation, which is a considerable drawback. As a follow-up to requests from the community, this paper explores Deadeye as a highlighting technique for 3D visualizations, because such stereoscopic scenarios support dichoptic presentation out of the box. Deadeye suppresses binocular disparities for the target object, so we cannot assume the applicability of our technique as a given fact. With this motivation, the paper presents quantitative evaluations of Deadeye in VR, including configurations with multiple heterogeneous distractors as an important robustness challenge. After confirming the preserved preattentiveness (all average accuracies above 90\%) under such real-world conditions, we explore VR volume rendering as an example application scenario for Deadeye. We depict a possible workflow for integrating our technique, conduct an exploratory survey to demonstrate benefits and limitations, and finally provide related design implications.},
  archive      = {J_TVCG},
  author       = {Andrey Krekhov and Sebastian Cmentowski and Andre Waschk and Jens Krüger},
  doi          = {10.1109/TVCG.2019.2934370},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {547-557},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deadeye visualization revisited: Investigation of preattentiveness and applicability in virtual environments},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). There is no spoon: Evaluating performance, space use, and
presence with expert domain users in immersive analytics. <em>TVCG</em>,
<em>26</em>(1), 536–546. (<a
href="https://doi.org/10.1109/TVCG.2019.2934803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.},
  archive      = {J_TVCG},
  author       = {Andrea Batch and Andrew Cunningham and Maxime Cordeil and Niklas Elmqvist and Tim Dwyer and Bruce H. Thomas and Kim Marriott},
  doi          = {10.1109/TVCG.2019.2934803},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {536-546},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {There is no spoon: Evaluating performance, space use, and presence with expert domain users in immersive analytics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of immersion on cluster identification tasks.
<em>TVCG</em>, <em>26</em>(1), 525–535. (<a
href="https://doi.org/10.1109/TVCG.2019.2934395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.},
  archive      = {J_TVCG},
  author       = {M. Kraus and N. Weiler and D. Oelke and J. Kehrer and D. A. Keim and J. Fuchs},
  doi          = {10.1109/TVCG.2019.2934395},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {525-535},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of immersion on cluster identification tasks},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating an immersive space-time cube geovisualization for
intuitive trajectory data exploration. <em>TVCG</em>, <em>26</em>(1),
514–524. (<a href="https://doi.org/10.1109/TVCG.2019.2934415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst&#39;s real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.},
  archive      = {J_TVCG},
  author       = {Jorge A. Wagner Filho and Wolfgang Stuerzlinger and Luciana Nedel},
  doi          = {10.1109/TVCG.2019.2934415},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {514-524},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating an immersive space-time cube geovisualization for intuitive trajectory data exploration},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing for mobile and immersive visual analytics in the
field. <em>TVCG</em>, <em>26</em>(1), 503–513. (<a
href="https://doi.org/10.1109/TVCG.2019.2934282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face dataand platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud, and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data&#39;s utility for various field operations and new directions for visual analytics tools to transform fieldwork.},
  archive      = {J_TVCG},
  author       = {Matt Whitlock and Keke Wu and Danielle Albers Szafir},
  doi          = {10.1109/TVCG.2019.2934282},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {503-513},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Designing for mobile and immersive visual analytics in the field},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artifact-based rendering: Harnessing natural and traditional
visual media for more expressive and engaging 3D visualizations.
<em>TVCG</em>, <em>26</em>(1), 492–502. (<a
href="https://doi.org/10.1109/TVCG.2019.2934260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Artifact-Based Rendering (ABR), a framework of tools, algorithms, and processes that makes it possible to produce real, data-driven 3D scientific visualizations with a visual language derived entirely from colors, lines, textures, and forms created using traditional physical media or found in nature. A theory and process for ABR is presented to address three current needs: (i) designing better visualizations by making it possible for non-programmers to rapidly design and critique many alternative data-to-visual mappings; (ii) expanding the visual vocabulary used in scientific visualizations to depict increasingly complex multivariate data; (iii) bringing a more engaging, natural, and human-relatable handcrafted aesthetic to data visualization. New tools and algorithms to support ABR include front-end applets for constructing artifact-based colormaps, optimizing 3D scanned meshes for use in data visualization, and synthesizing textures from artifacts. These are complemented by an interactive rendering engine with custom algorithms and interfaces that demonstrate multiple new visual styles for depicting point, line, surface, and volume data. A within-the-research-team design study provides early evidence of the shift in visualization design processes that ABR is believed to enable when compared to traditional scientific visualization systems. Qualitative user feedback on applications to climate science and brain imaging support the utility of ABR for scientific discovery and public communication.},
  archive      = {J_TVCG},
  author       = {Seth Johnson and Francesca Samsel and Gregory Abram and Daniel Olson and Andrew J. Solis and Bridger Herman and Phillip J. Wolfram and Christophe Lenglet and Daniel F. Keefe},
  doi          = {10.1109/TVCG.2019.2934260},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {492-502},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Artifact-based rendering: Harnessing natural and traditional visual media for more expressive and engaging 3D visualizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigating direct manipulation of graphical encodings as
a method for user interaction. <em>TVCG</em>, <em>26</em>(1), 482–491.
(<a href="https://doi.org/10.1109/TVCG.2019.2934534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.},
  archive      = {J_TVCG},
  author       = {Bahador Saket and Samuel Huron and Charles Perin and Alex Endert},
  doi          = {10.1109/TVCG.2019.2934534},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {482-491},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating direct manipulation of graphical encodings as a method for user interaction},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decoding a complex visualization in a science museum – an
empirical study. <em>TVCG</em>, <em>26</em>(1), 472–481. (<a
href="https://doi.org/10.1109/TVCG.2019.2934401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study describes a detailed analysis of museum visitors&#39; decoding process as they used a visualization designed to support exploration of a large, complex dataset. Quantitative and qualitative analyses revealed that it took, on average, 43 seconds for visitors to decode enough of the visualization to see patterns and relationships in the underlying data represented, and 54 seconds to arrive at their first correct data interpretation. Furthermore, visitors decoded throughout and not only upon initial use of the visualization. The study analyzed think-aloud data to identify issues visitors had mapping the visual representations to their intended referents, examine why they occurred, and consider if and how these decoding issues were resolved. The paper also describes how multiple visual encodings both helped and hindered decoding and concludes with implications on the design and adaptation of visualizations for informal science learning venues.},
  archive      = {J_TVCG},
  author       = {Joyce Ma and Kwan-Liu Ma and Jennifer Frazier},
  doi          = {10.1109/TVCG.2019.2934401},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {472-481},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Decoding a complex visualization in a science museum – an empirical study},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Critical reflections on visualization authoring systems.
<em>TVCG</em>, <em>26</em>(1), 461–471. (<a
href="https://doi.org/10.1109/TVCG.2019.2934281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.},
  archive      = {J_TVCG},
  author       = {Arvind Satyanarayan and Bongshin Lee and Donghao Ren and Jeffrey Heer and John Stasko and John Thompson and Matthew Brehmer and Zhicheng Liu},
  doi          = {10.1109/TVCG.2019.2934281},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {461-471},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Critical reflections on visualization authoring systems},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Construct-a-vis: Exploring the free-form visualization
processes of children. <em>TVCG</em>, <em>26</em>(1), 451–460. (<a
href="https://doi.org/10.1109/TVCG.2019.2934804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building data analysis skills is part of modern elementary school curricula. Recent research has explored how to facilitate children&#39;s understanding of visual data representations through completion exercises which highlight links between concrete and abstract mappings. This approach scaffolds visualization activities by presenting a target visualization to children. But how can we engage children in more free-form visual data mapping exercises that are driven by their own mapping ideas? How can we scaffold a creative exploration of visualization techniques and mapping possibilities? We present Construct-A-Vis, a tablet-based tool designed to explore the feasibility of free-form and constructive visualization activities with elementary school children. Construct-A-Vis provides adjustable levels of scaffolding visual mapping processes. It can be used by children individually or as part of collaborative activities. Findings from a study with elementary school children using Construct-A-Vis individually and in pairs highlight the potential of this free-form constructive approach, as visible in children&#39;s diverse visualization outcomes and their critical engagement with the data and mapping processes. Based on our study findings we contribute insights into the design of free-form visualization tools for children, including the role of tool-based scaffolding mechanisms and shared interactions to guide visualization activities with children.},
  archive      = {J_TVCG},
  author       = {Fearn Bishop and Johannes Zagermann and Ulrike Pfeil and Gemma Sanderson and Harald Reiterer and Uta Hinrichs},
  doi          = {10.1109/TVCG.2019.2934804},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {451-460},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Construct-A-vis: Exploring the free-form visualization processes of children},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual analysis of high-dimensional event sequence data via
dynamic hierarchical aggregation. <em>TVCG</em>, <em>26</em>(1),
440–450. (<a href="https://doi.org/10.1109/TVCG.2019.2934661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal event data are collected across a broad range of domains, and a variety of visual analytics techniques have been developed to empower analysts working with this form of data. These techniques generally display aggregate statistics computed over sets of event sequences that share common patterns. Such techniques are often hindered, however, by the high-dimensionality of many real-world event sequence datasets which can prevent effective aggregation. A common coping strategy for this challenge is to group event types together prior to visualization, as a pre-process, so that each group can be represented within an analysis as a single event type. However, computing these event groupings as a pre-process also places significant constraints on the analysis. This paper presents a new visual analytics approach for dynamic hierarchical dimension aggregation. The approach leverages a predefined hierarchy of dimensions to computationally quantify the informativeness, with respect to a measure of interest, of alternative levels of grouping within the hierarchy at runtime. This information is then interactively visualized, enabling users to dynamically explore the hierarchy to select the most appropriate level of grouping to use at any individual step within an analysis. Key contributions include an algorithm for interactively determining the most informative set of event groupings for a specific analysis context, and a scented scatter-plus-focus visualization design with an optimization-based layout algorithm that supports interactive hierarchical exploration of alternative event type groupings. We apply these techniques to high-dimensional event sequence data from the medical domain and report findings from domain expert interviews.},
  archive      = {J_TVCG},
  author       = {David Gotz and Jonathan Zhang and Wenyuan Wang and Joshua Shrestha and David Borland},
  doi          = {10.1109/TVCG.2019.2934661},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {440-450},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of high-dimensional event sequence data via dynamic hierarchical aggregation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selection bias tracking and detailed subset comparison for
high-dimensional data. <em>TVCG</em>, <em>26</em>(1), 429–439. (<a
href="https://doi.org/10.1109/TVCG.2019.2934209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collection of large, complex datasets has become common across a wide variety of domains. Visual analytics tools increasingly play a key role in exploring and answering complex questions about these large datasets. However, many visualizations are not designed to concurrently visualize the large number of dimensions present in complex datasets (e.g. tens of thousands of distinct codes in an electronic health record system). This fact, combined with the ability of many visual analytics systems to enable rapid, ad-hoc specification of groups, or cohorts, of individuals based on a small subset of visualized dimensions, leads to the possibility of introducing selection bias-when the user creates a cohort based on a specified set of dimensions, differences across many other unseen dimensions may also be introduced. These unintended side effects may result in the cohort no longer being representative of the larger population intended to be studied, which can negatively affect the validity of subsequent analyses. We present techniques for selection bias tracking and visualization that can be incorporated into high-dimensional exploratory visual analytics systems, with a focus on medical data with existing data hierarchies. These techniques include: (1) tree-based cohort provenance and visualization, including a user-specified baseline cohort that all other cohorts are compared against, and visual encoding of cohort “drift”, which indicates where selection bias may have occurred, and (2) a set of visualizations, including a novel icicle-plot based visualization, to compare in detail the per-dimension differences between the baseline and a user-specified focus cohort. These techniques are integrated into a medical temporal event sequence visual analytics tool. We present example use cases and report findings from domain expert user interviews.},
  archive      = {J_TVCG},
  author       = {David Borland and Wenyuan Wang and Jonathan Zhang and Joshua Shrestha and David Gotz},
  doi          = {10.1109/TVCG.2019.2934209},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {429-439},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Selection bias tracking and detailed subset comparison for high-dimensional data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An incremental dimensionality reduction method for
visualizing streaming multidimensional data. <em>TVCG</em>,
<em>26</em>(1), 418–428. (<a
href="https://doi.org/10.1109/TVCG.2019.2934433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) methods are commonly used for analyzing and visualizing multidimensional data. However, when data is a live streaming feed, conventional DR methods cannot be directly used because of their computational complexity and inability to preserve the projected data positions at previous time points. In addition, the problem becomes even more challenging when the dynamic data records have a varying number of dimensions as often found in real-world applications. This paper presents an incremental DR solution. We enhance an existing incremental PCA method in several ways to ensure its usability for visualizing streaming multidimensional data. First, we use geometric transformation and animation methods to help preserve a viewer&#39;s mental map when visualizing the incremental results. Second, to handle data dimension variants, we use an optimization method to estimate the projected data positions, and also convey the resulting uncertainty in the visualization. We demonstrate the effectiveness of our design with two case studies using real-world datasets.},
  archive      = {J_TVCG},
  author       = {Takanori Fujiwara and Jia-Kai Chou and Shilpika Shilpika and Panpan Xu and Liu Ren and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2019.2934433},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {418-428},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An incremental dimensionality reduction method for visualizing streaming multidimensional data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tac-simur: Tactic-based simulative visual analytics of table
tennis. <em>TVCG</em>, <em>26</em>(1), 407–417. (<a
href="https://doi.org/10.1109/TVCG.2019.2934630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulative analysis in competitive sports can provide prospective insights, which can help improve the performance of players in future matches. However, adequately simulating the complex competition process and effectively explaining the simulation result to domain experts are typically challenging. This work presents a design study to address these challenges in table tennis. We propose a well-established hybrid second-order Markov chain model to characterize and simulate the competition process in table tennis. Compared with existing methods, our approach is the first to support the effective simulation of tactics, which represent high-level competition strategies in table tennis. Furthermore, we introduce a visual analytics system called Tac-Simur based on the proposed model for simulative visual analytics. Tac-Simur enables users to easily navigate different players and their tactics based on their respective performance in matches to identify the player and the tactics of interest for further analysis. Then, users can utilize the system to interactively explore diverse simulation tasks and visually explain the simulation results. The effectiveness and usefulness of this work are demonstrated by two case studies, in which domain experts utilize Tac-Simur to find interesting and valuable insights. The domain experts also provide positive feedback on the usability of Tac-Simur. Our work can be extended to other similar sports such as tennis and badminton.},
  archive      = {J_TVCG},
  author       = {Jiachen Wang and Kejian Zhao and Dazhen Deng and Anqi Cao and Xiao Xie and Zheng Zhou and Hui Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2019.2934630},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {407-417},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tac-simur: Tactic-based simulative visual analytics of table tennis},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CourtTime: Generating actionable insights into tennis
matches using visual analytics. <em>TVCG</em>, <em>26</em>(1), 397–406.
(<a href="https://doi.org/10.1109/TVCG.2019.2934243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tennis players and coaches of all proficiency levels seek to understand and improve their play. Summary statistics alone are inadequate to provide the insights players need to improve their games. Spatio-temporal data capturing player and ball movements is likely to provide the actionable insights needed to identify player strengths, weaknesses, and strategies. To fully utilize this spatio-temporal data, we need to integrate it with domain-relevant context meta-data. In this paper, we propose CourtTime, a novel approach to perform data-driven visual analysis of individual tennis matches. Our visual approach introduces a novel visual metaphor, namely 1-D Space-Time Charts that enable the analysis of single points at a glance based on small multiples. We also employ user-driven sorting and clustering techniques and a layout technique that aligns the last few shots in a point to facilitate shot pattern discovery. We discuss the usefulness of CourtTime via an extensive case study and report on feedback from an amateur tennis player and three tennis coaches.},
  archive      = {J_TVCG},
  author       = {Tom Polk and Dominik Jäckle and Johannes Häußler and Jing Yang},
  doi          = {10.1109/TVCG.2019.2934243},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {397-406},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CourtTime: Generating actionable insights into tennis matches using visual analytics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Common fate for animated transitions in visualization.
<em>TVCG</em>, <em>26</em>(1), 386–396. (<a
href="https://doi.org/10.1109/TVCG.2019.2934288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Law of Common Fate from Gestalt psychology states that visual objects moving with the same velocity along parallel trajectories will be perceived by a human observer as grouped. However, the concept of common fate is much broader than mere velocity; in this paper we explore how common fate results from coordinated changes in luminance and size. We present results from a crowdsourced graphical perception study where we asked workers to make perceptual judgments on a series of trials involving four graphical objects under the influence of conflicting static and dynamic visual factors (position, size and luminance) used in conjunction. Our results yield the following rankings for visual grouping: motion&gt; (dynamic luminance, size, luminance); dynamic size&gt; (dynamic luminance, position); and dynamic luminance &gt; size. We also conducted a follow-up experiment to evaluate the three dynamic visual factors in a more ecologically valid setting, using both a Gapminder-like animated scatterplot and a thematic map of election data. The results indicate that in practice the relative grouping strengths of these factors may depend on various parameters including the visualization characteristics and the underlying data. We discuss design implications for animated transitions in data visualization.},
  archive      = {J_TVCG},
  author       = {Amira Chalbi and Jacob Ritchie and Deokgun Park and Jungu Choi and Nicolas Roussel and Niklas Elmqvist and Fanny Chevalier},
  doi          = {10.1109/TVCG.2019.2934288},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {386-396},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Common fate for animated transitions in visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of visualizations for identifying correlation
over space and time. <em>TVCG</em>, <em>26</em>(1), 375–385. (<a
href="https://doi.org/10.1109/TVCG.2019.2934807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observing the relationship between two or more variables over space and time is essential in many domains. For instance, looking, for different countries, at the evolution of both the life expectancy at birth and the fertility rate will give an overview of their demographics. The choice of visual representation for such multivariate data is key to enabling analysts to extract patterns and trends. Prior work has compared geo-temporal visualization techniques for a single thematic variable that evolves over space and time, or for two variables at a specific point in time. But how effective visualization techniques are at communicating correlation between two variables that evolve over space and time remains to be investigated. We report on a study comparing three techniques that are representative of different strategies to visualize geo-temporal multivariate data: either juxtaposing all locations for a given time step, or juxtaposing all time steps for a given location; and encoding thematic attributes either using symbols overlaid on top of map features, or using visual channels of the map features themselves. Participants performed a series of tasks that required them to identify if two variables were correlated over time and if there was a pattern in their evolution. Tasks varied in granularity for both dimensions: time (all time steps, a subrange of steps, one step only) and space (all locations, locations in a subregion, one location only). Our results show that a visualization&#39;s effectiveness depends strongly on the task to be carried out. Based on these findings we present a set of design guidelines about geo-temporal visualization techniques for communicating correlation.},
  archive      = {J_TVCG},
  author       = {Vanessa Peña-Araya and Emmanuel Pietriga and Anastasia Bezerianos},
  doi          = {10.1109/TVCG.2019.2934807},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {375-385},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparison of visualizations for identifying correlation over space and time},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparative evaluation of animation and small multiples
for trend visualization on mobile phones. <em>TVCG</em>, <em>26</em>(1),
364–374. (<a href="https://doi.org/10.1109/TVCG.2019.2934397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We compare the efficacy of animated and small multiples variants of scatterplots on mobile phones for comparing trends in multivariate datasets. Visualization is increasingly prevalent in mobile applications and mobile-first websites, yet there is little prior visualization research dedicated to small displays. In this paper, we build upon previous experimental research carried out on larger displays that assessed animated and non-animated variants of scatterplots. Incorporating similar experimental stimuli and tasks, we conducted an experiment where 96 crowdworker participants performed nine trend comparison tasks using their mobile phones. We found that those using a small multiples design consistently completed tasks in less time, albeit with slightly less confidence than those using an animated design. The accuracy results were more task-dependent, and we further interpret our results according to the characteristics of the individual tasks, with a specific focus on the trajectories of target and distractor data items in each task. We identify cases that appear to favor either animation or small multiples, providing new questions for further experimental research and implications for visualization design on mobile devices. Lastly, we provide a reflection on our evaluation methodology.},
  archive      = {J_TVCG},
  author       = {Matthew Brehmer and Bongshin Lee and Petra Isenberg and Eun Kyoung Choe},
  doi          = {10.1109/TVCG.2019.2934397},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {364-374},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparative evaluation of animation and small multiples for trend visualization on mobile phones},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The validity, generalizability and feasibility of summative
evaluation methods in visual analytics. <em>TVCG</em>, <em>26</em>(1),
353–363. (<a href="https://doi.org/10.1109/TVCG.2019.2934264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.},
  archive      = {J_TVCG},
  author       = {Mosab Khayat and Morteza Karimzadeh and David S. Ebert and Arif Ghafoor},
  doi          = {10.1109/TVCG.2019.2934264},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {353-363},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The validity, generalizability and feasibility of summative evaluation methods in visual analytics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VisTA: Integrating machine intelligence with visualization
to support the investigation of think-aloud sessions. <em>TVCG</em>,
<em>26</em>(1), 343–352. (<a
href="https://doi.org/10.1109/TVCG.2019.2934797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML&#39;s input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind).},
  archive      = {J_TVCG},
  author       = {Mingming Fan and Ke Wu and Jian Zhao and Yue Li and Winter Wei and Khai N. Truong},
  doi          = {10.1109/TVCG.2019.2934797},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {343-352},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisTA: Integrating machine intelligence with visualization to support the investigation of think-aloud sessions},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward objective evaluation of working memory in
visualizations: A case study using pupillometry and a dual-task
paradigm. <em>TVCG</em>, <em>26</em>(1), 332–342. (<a
href="https://doi.org/10.1109/TVCG.2019.2934286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive science has established widely used and validated procedures for evaluating working memory in numerous applied domains, but surprisingly few studies have employed these methodologies to assess claims about the impacts of visualizations on working memory. The lack of information visualization research that uses validated procedures for measuring working memory may be due, in part, to the absence of cross-domain methodological guidance tailored explicitly to the unique needs of visualization research. This paper presents a set of clear, practical, and empirically validated methods for evaluating working memory during visualization tasks and provides readers with guidance in selecting an appropriate working memory evaluation paradigm. As a case study, we illustrate multiple methods for evaluating working memory in a visual-spatial aggregation task with geospatial data. The results show that the use of dual-task experimental designs (simultaneous performance of several tasks compared to single-task performance) and pupil dilation can reveal working memory demands associated with task difficulty and dual-tasking. In a dual-task experimental design, measures of task completion times and pupillometry revealed the working memory demands associated with both task difficulty and dual-tasking. Pupillometry demonstrated that participants&#39; pupils were significantly larger when they were completing a more difficult task and when multitasking. We propose that researchers interested in the relative differences in working memory between visualizations should consider a converging methods approach, where physiological measures and behavioral measures of working memory are employed to generate a rich evaluation of visualization effort.},
  archive      = {J_TVCG},
  author       = {Lace M.K. Padilla and Spencer C. Castro and P. Samuel Quinan and Ian T. Ruginski and Sarah H. Creem-Regehr},
  doi          = {10.1109/TVCG.2019.2934286},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {332-342},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Toward objective evaluation of working memory in visualizations: A case study using pupillometry and a dual-task paradigm},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating perceptual bias during geometric scaling of
scatterplots. <em>TVCG</em>, <em>26</em>(1), 321–331. (<a
href="https://doi.org/10.1109/TVCG.2019.2934208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.},
  archive      = {J_TVCG},
  author       = {Yating Wei and Honghui Mei and Ying Zhao and Shuyue Zhou and Bingru Lin and Haojing Jiang and Wei Chen},
  doi          = {10.1109/TVCG.2019.2934208},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {321-331},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating perceptual bias during geometric scaling of scatterplots},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measures of the benefit of direct encoding of data deltas
for data pair relation perception. <em>TVCG</em>, <em>26</em>(1),
311–320. (<a href="https://doi.org/10.1109/TVCG.2019.2934801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power of data visualization is not to convey absolute values of individual data points, but to allow the exploration of relations (increases or decreases in a data value) among them. One approach to highlighting these relations is to explicitly encode the numeric differences (deltas) between data values. Because this approach removes the context of the individual data values, it is important to measure how much of a performance improvement it actually offers, especially across differences in encodings and tasks, to ensure that it is worth adding to a visualization design. Across 3 different tasks, we measured the increase in visual processing efficiency for judging the relations between pairs of data values, from when only the values were shown, to when the deltas between the values were explicitly encoded, across position and length visual feature encodings (and slope encodings in Experiments 1 &amp; 2). In Experiment 1, the participant&#39;s task was to locate a pair of data values with a given relation (e.g., Find the `small bar to the left of a tall bar&#39; pair) among pairs of the opposite relation, and we measured processing efficiency from the increase in response times as the number of pairs increased. In Experiment 2, the task was to judge which of two relation types was more prevalent in a briefly presented display of 10 data pairs (e.g., Are there more `small bar to the left of a tall bar&#39; pairs or more `tall bar to the left of a small bar&#39; pairs?). In the final experiment, the task was to estimate the average delta within a briefly presented display of 6 data pairs (e.g., What is the average bar height difference across all `small bar to the left of a tall bar&#39; pairs?). Across all three experiments, visual processing of relations between data value pairs was significantly better when directly encoded as deltas rather than implicitly between individual data points, and varied substantially depending on the task (improvement ranged from 25\% to 95\%). Considering the ubiquity of bar charts and dot plots, relation perception for individual data values is highly inefficient, and confirms the need for alternative designs that provide not only absolute values, but also direct encoding of critical relationships between those values.},
  archive      = {J_TVCG},
  author       = {Christine Nothelfer and Steven Franconeri},
  doi          = {10.1109/TVCG.2019.2934801},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {311-320},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measures of the benefit of direct encoding of data deltas for data pair relation perception},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biased average position estimates in line and bar graphs:
Underestimation, overestimation, and perceptual pull. <em>TVCG</em>,
<em>26</em>(1), 301–310. (<a
href="https://doi.org/10.1109/TVCG.2019.2934400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual depictions of data, position (i.e., the vertical height of a line or a bar) is believed to be the most precise way to encode information compared to other encodings (e.g., hue). Not only are other encodings less precise than position, but they can also be prone to systematic biases (e.g., color category boundaries can distort perceived differences between hues). By comparison, position&#39;s high level of precision may seem to protect it from such biases. In contrast, across three empirical studies, we show that while position may be a precise form of data encoding, it can also produce systematic biases in how values are visually encoded, at least for reports of average position across a short delay. In displays with a single line or a single set of bars, reports of average positions were significantly biased, such that line positions were underestimated and bar positions were overestimated. In displays with multiple data series (i.e., multiple lines and/or sets of bars), this systematic bias still persisted. We also observed an effect of “perceptual pull”, where the average position estimate for each series was `pulled&#39; toward the other. These findings suggest that, although position may still be the most precise form of visual data encoding, it can also be systematically biased.},
  archive      = {J_TVCG},
  author       = {Cindy Xiong and Cristina R. Ceja and Casimir J.H. Ludwig and Steven Franconeri},
  doi          = {10.1109/TVCG.2019.2934400},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {301-310},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Biased average position estimates in line and bar graphs: Underestimation, overestimation, and perceptual pull},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable topological data analysis and visualization for
evaluating data-driven models in scientific applications. <em>TVCG</em>,
<em>26</em>(1), 291–300. (<a
href="https://doi.org/10.1109/TVCG.2019.2934594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid adoption of machine learning techniques for large-scale applications in science and engineering comes the convergence of two grand challenges in visualization. First, the utilization of black box models (e.g., deep neural networks) calls for advanced techniques in exploring and interpreting model behaviors. Second, the rapid growth in computing has produced enormous datasets that require techniques that can handle millions or more samples. Although some solutions to these interpretability challenges have been proposed, they typically do not scale beyond thousands of samples, nor do they provide the high-level intuition scientists are looking for. Here, we present the first scalable solution to explore and analyze high-dimensional functions often encountered in the scientific data analysis pipeline. By combining a new streaming neighborhood graph construction, the corresponding topology computation, and a novel data aggregation scheme, namely topology aware datacubes , we enable interactive exploration of both the topological and the geometric aspect of high-dimensional data. Following two use cases from high-energy-density (HED) physics and computational biology, we demonstrate how these capabilities have led to crucial new insights in both applications.},
  archive      = {J_TVCG},
  author       = {Shusen Liu and Di Wang and Dan Maljovec and Rushil Anirudh and Jayaraman J. Thiagarajan and Sam Ade Jacobs and Brian C. Van Essen and David Hysom and Jae-Seung Yeom and Jim Gaffney and Luc Peterson and Peter B. Robinson and Harsh Bhatia and Valerio Pascucci and Brian K. Spears and Peer-Timo Bremer},
  doi          = {10.1109/TVCG.2019.2934594},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {291-300},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalable topological data analysis and visualization for evaluating data-driven models in scientific applications},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vector field topology of time-dependent flows in a steady
reference frame. <em>TVCG</em>, <em>26</em>(1), 280–290. (<a
href="https://doi.org/10.1109/TVCG.2019.2934375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topological analysis of unsteady vector fields remains to this day one of the largest challenges in flow visualization. We build up on recent work on vortex extraction to define a time-dependent vector field topology for 2D and 3D flows. In our work, we split the vector field into two components: a vector field in which the flow becomes steady, and the remaining ambient flow that describes the motion of topological elements (such as sinks, sources and saddles) and feature curves (vortex corelines and bifurcation lines). To this end, we expand on recent local optimization approaches by modeling spatially-varying deformations through displacement transformations from continuum mechanics. We compare and discuss the relationships with existing local and integration-based topology extraction methods, showing for instance that separatrices seeded from saddles in the optimal frame align with the integration-based streakline vector field topology. In contrast to the streakline-based approach, our method gives a complete picture of the topology for every time slice, including the steps near the temporal domain boundaries. With our work it now becomes possible to extract topological information even when only few time slices are available. We demonstrate the method in several analytical and numerically-simulated flows and discuss practical aspects, limitations and opportunities for future work.},
  archive      = {J_TVCG},
  author       = {Irene Baeza Rojo and Tobias Günther},
  doi          = {10.1109/TVCG.2019.2934375},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {280-290},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vector field topology of time-dependent flows in a steady reference frame},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale topological analysis of asymmetric tensor fields
on surfaces. <em>TVCG</em>, <em>26</em>(1), 270–279. (<a
href="https://doi.org/10.1109/TVCG.2019.2934314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric tensor fields have found applications in many science and engineering domains, such as fluid dynamics. Recent advances in the visualization and analysis of 2D asymmetric tensor fields focus on pointwise analysis of the tensor field and effective visualization metaphors such as colors, glyphs, and hyperstreamlines. In this paper, we provide a novel multi-scale topological analysis framework for asymmetric tensor fields on surfaces. Our multi-scale framework is based on the notions of eigenvalue and eigenvector graphs. At the core of our framework are the identification of atomic operations that modify the graphs and the scale definition that guides the order in which the graphs are simplified to enable clarity and focus for the visualization of topological analysis on data of different sizes. We also provide efficient algorithms to realize these operations. Furthermore, we provide physical interpretation of these graphs. To demonstrate the utility of our system, we apply our multi-scale analysis to data in computational fluid dynamics.},
  archive      = {J_TVCG},
  author       = {Fariba Khan and Lawrence Roy and Eugene Zhang and Botong Qu and Shih-Hsuan Hung and Harry Yeh and Robert S. Laramee and Yue Zhang},
  doi          = {10.1109/TVCG.2019.2934314},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {270-279},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-scale topological analysis of asymmetric tensor fields on surfaces},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extraction and visual analysis of potential vorticity
banners around the alps. <em>TVCG</em>, <em>26</em>(1), 259–269. (<a
href="https://doi.org/10.1109/TVCG.2019.2934310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Potential vorticity is among the most important scalar quantities in atmospheric dynamics. For instance, potential vorticity plays a key role in particularly strong wind peaks in extratropical cyclones and it is able to explain the occurrence of frontal rain bands. Potential vorticity combines the key quantities of atmospheric dynamics, namely rotation and stratification. Under suitable wind conditions elongated banners of potential vorticity appear in the lee of mountains. Their role in atmospheric dynamics has recently raised considerable interest in the meteorological community for instance due to their influence in aviation wind hazards and maritime transport. In order to support meteorologists and climatologists in the analysis of these structures, we developed an extraction algorithm and a visual exploration framework consisting of multiple linked views. For the extraction we apply a predictor-corrector algorithm that follows streamlines and realigns them with extremal lines of potential vorticity. Using the agglomerative hierarchical clustering algorithm, we group banners from different sources based on their proximity. To visually analyze the time-dependent banner geometry, we provide interactive overviews and enable the query for detail on demand, including the analysis of different time steps, potentially correlated scalar quantities, and the wind vector field. In particular, we study the relationship between relative humidity and the banners for their potential in indicating the development of precipitation. Working with our method, the collaborating meteorologists gained a deeper understanding of the three-dimensional processes, which may spur follow-up research in the future.},
  archive      = {J_TVCG},
  author       = {Robin Bader and Michael Sprenger and Nikolina Ban and Stefan Rüdisühli and Christoph Schär and Tobias Günther},
  doi          = {10.1109/TVCG.2019.2934310},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {259-269},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Extraction and visual analysis of potential vorticity banners around the alps},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic nested tracking graphs. <em>TVCG</em>,
<em>26</em>(1), 249–258. (<a
href="https://doi.org/10.1109/TVCG.2019.2934368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work describes an approach for the interactive visual analysis of large-scale simulations, where numerous superlevel set components and their evolution are of primary interest. The approach first derives, at simulation runtime, a specialized Cinema database that consists of images of component groups, and topological abstractions. This database is processed by a novel graph operation-based nested tracking graph algorithm (GO-NTG) that dynamically computes NTGs for component groups based on size, overlap, persistence, and level thresholds. The resulting NTGs are in turn used in a feature-centered visual analytics framework to query specific database elements and update feature parameters, facilitating flexible post hoc analysis.},
  archive      = {J_TVCG},
  author       = {Jonas Lukasczyk and Christoph Garth and Gunther H. Weber and Tim Biedert and Ross Maciejewski and Heike Leitte},
  doi          = {10.1109/TVCG.2019.2934368},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {249-258},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic nested tracking graphs},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ProtoSteer: Steering deep sequence model with prototypes.
<em>TVCG</em>, <em>26</em>(1), 238–248. (<a
href="https://doi.org/10.1109/TVCG.2019.2934267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.},
  archive      = {J_TVCG},
  author       = {Yao Ming and Panpan Xu and Furui Cheng and Huamin Qu and Liu Ren},
  doi          = {10.1109/TVCG.2019.2934267},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {238-248},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ProtoSteer: Steering deep sequence model with prototypes},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Facetto: Combining unsupervised and supervised learning for
hierarchical phenotype analysis in multi-channel image data.
<em>TVCG</em>, <em>26</em>(1), 227–237. (<a
href="https://doi.org/10.1109/TVCG.2019.2934547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facetto is a scalable visual analytics application that is used to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of human tumors and tissues. Such images represent the cutting edge of digital histology and promise to revolutionize how diseases such as cancer are studied, diagnosed, and treated. Highly multiplexed tissue images are complex, comprising 109 or more pixels, 60-plus channels, and millions of individual cells. This makes manual analysis challenging and error-prone. Existing automated approaches are also inadequate, in large part, because they are unable to effectively exploit the deep knowledge of human tissue biology available to anatomic pathologists. To overcome these challenges, Facetto enables a semi-automated analysis of cell types and states. It integrates unsupervised and supervised learning into the image and feature exploration process and offers tools for analytical provenance. Experts can cluster the data to discover new types of cancer and immune cells and use clustering results to train a convolutional neural network that classifies new cells accordingly. Likewise, the output of classifiers can be clustered to discover aggregate patterns and phenotype subsets. We also introduce a new hierarchical approach to keep track of analysis steps and data subsets created by users; this assists in the identification of cell types. Users can build phenotype trees and interact with the resulting hierarchical structures of both high-dimensional feature and image spaces. We report on use-cases in which domain scientists explore various large-scale fluorescence imaging datasets. We demonstrate how Facetto assists users in steering the clustering and classification process, inspecting analysis results, and gaining new scientific insights into cancer biology.},
  archive      = {J_TVCG},
  author       = {Robert Krueger and Johanna Beyer and Won-Dong Jang and Nam Wook Kim and Artem Sokolov and Peter K. Sorger and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2019.2934547},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {227-237},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Facetto: Combining unsupervised and supervised learning for hierarchical phenotype analysis in multi-channel image data},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GenerativeMap: Visualization and exploration of dynamic
density maps via generative learning model. <em>TVCG</em>,
<em>26</em>(1), 216–226. (<a
href="https://doi.org/10.1109/TVCG.2019.2934806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density map is widely used for data sampling, time-varying detection, ensemble representation, etc. The visualization of dynamic evolution is a challenging task when exploring spatiotemporal data. Many approaches have been provided to explore the variation of data patterns over time, which commonly need multiple parameters and preprocessing works. Image generation is a well-known topic in deep learning, and a variety of generating models have been promoted in recent years. In this paper, we introduce a general pipeline called GenerativeMap to extract dynamics of density maps by generating interpolation information. First, a trained generative model comprises an important part of our approach, which can generate nonlinear and natural results by implementing a few parameters. Second, a visual presentation is proposed to show the density change, which is combined with the level of detail and blue noise sampling for a better visual effect. Third, for dynamic visualization of large-scale density maps, we extend this approach to show the evolution in regions of interest, which costs less to overcome the drawback of the learning-based generative model. We demonstrate our method on different types of cases, and we evaluate and compare the approach from multiple aspects. The results help identify the effectiveness of our approach and confirm its applicability in different scenarios.},
  archive      = {J_TVCG},
  author       = {Chen Chen and Changbo Wang and Xue Bai and Peiying Zhang and Chenhui Li},
  doi          = {10.1109/TVCG.2019.2934806},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {216-226},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GenerativeMap: Visualization and exploration of dynamic density maps via generative learning model},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TSR-TVD: Temporal super-resolution for time-varying data
analysis and visualization. <em>TVCG</em>, <em>26</em>(1), 205–215. (<a
href="https://doi.org/10.1109/TVCG.2019.2934255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present TSR-TVD, a novel deep learning framework that generates temporal super-resolution (TSR) of time-varying data (TVD) using adversarial learning. TSR-TVD is the first work that applies the recurrent generative network (RGN), a combination of the recurrent neural network (RNN) and generative adversarial network (GAN), to generate temporal high-resolution volume sequences from low-resolution ones. The design of TSR-TVD includes a generator and a discriminator. The generator takes a pair of volumes as input and outputs the synthesized intermediate volume sequence through forward and backward predictions. The discriminator takes the synthesized intermediate volumes as input and produces a score indicating the realness of the volumes. Our method handles multivariate data as well where the trained network from one variable is applied to generate TSR for another variable. To demonstrate the effectiveness of TSR-TVD, we show quantitative and qualitative results with several time-varying multivariate data sets and compare our method against standard linear interpolation and solutions solely based on RNN or CNN.},
  archive      = {J_TVCG},
  author       = {Jun Han and Chaoli Wang},
  doi          = {10.1109/TVCG.2019.2934255},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {205-215},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TSR-TVD: Temporal super-resolution for time-varying data analysis and visualization},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LassoNet: Deep lasso-selection of 3D point clouds.
<em>TVCG</em>, <em>26</em>(1), 195–204. (<a
href="https://doi.org/10.1109/TVCG.2019.2934332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website : https://LassoNet.github.io},
  archive      = {J_TVCG},
  author       = {Zhutian Chen and Wei Zeng and Zhiguang Yang and Lingyun Yu and Chi-Wing Fu and Huamin Qu},
  doi          = {10.1109/TVCG.2019.2934332},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {195-204},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LassoNet: Deep lasso-selection of 3D point clouds},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GUIRO: User-guided matrix reordering. <em>TVCG</em>,
<em>26</em>(1), 184–194. (<a
href="https://doi.org/10.1109/TVCG.2019.2934300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices-similar to node-link diagrams-are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: “Which matrix reordering algorithm should I choose for my dataset at hand?” To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.},
  archive      = {J_TVCG},
  author       = {Michael Behrisch and Tobias Schreck and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2019.2934300},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {184-194},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GUIRO: User-guided matrix reordering},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward localized topological data structures: Querying the
forest for the tree. <em>TVCG</em>, <em>26</em>(1), 173–183. (<a
href="https://doi.org/10.1109/TVCG.2019.2934257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological approaches to data analysis can answer complex questions about the number, connectivity, and scale of intrinsic features in scalar data. However, the global nature of many topological structures makes their computation challenging at scale, and thus often limits the size of data that can be processed. One key quality to achieving scalability and performance on modern architectures is data locality, i.e., a process operates on data that resides in a nearby memory system, avoiding frequent jumps in data access patterns. From this perspective, topological computations are particularly challenging because the implied data structures represent features that can span the entire data set, often requiring a global traversal phase that limits their scalability. Traditionally, expensive preprocessing is considered an acceptable trade-off as it accelerates all subsequent queries. Most published use cases, however, explore only a fraction of all possible queries, most often those returning small, local features. In these cases, much of the global information is not utilized, yet computing it dominates the overall response time. We address this challenge for merge trees, one of the most commonly used topological structures. In particular, we propose an alternative representation, the merge forest, a collection of local trees corresponding to regions in a domain decomposition. Local trees are connected by a bridge set that allows us to recover any necessary global information at query time. The resulting system couples (i) a preprocessing that scales linearly in practice with (ii) fast runtime queries that provide the same functionality as traditional queries of a global merge tree. We test the scalability of our approach on a shared-memory parallel computer and demonstrate how data structure locality enables the analysis of large data with an order of magnitude performance improvement over the status quo. Furthermore, a merge forest reduces the memory overhead compared to a global merge tree and enables the processing of data sets that are an order of magnitude larger than possible with previous algorithms.},
  archive      = {J_TVCG},
  author       = {Pavol Klacansky and Attila Gyulassy and Peer-Timo Bremer and Valerio Pascucci},
  doi          = {10.1109/TVCG.2019.2934257},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {173-183},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Toward localized topological data structures: Querying the forest for the tree},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effect of data transformations on scalar field
topological analysis of high-order FEM solutions. <em>TVCG</em>,
<em>26</em>(1), 162–172. (<a
href="https://doi.org/10.1109/TVCG.2019.2934338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-order finite element methods (HO-FEM) are gaining popularity in the simulation community due to their success in solving complex flow dynamics. There is an increasing need to analyze the data produced as output by these simulations. Simultaneously, topological analysis tools are emerging as powerful methods for investigating simulation data. However, most of the current approaches to topological analysis have had limited application to HO-FEM simulation data for two reasons. First, the current topological tools are designed for linear data (polynomial degree one), but the polynomial degree of the data output by these simulations is typically higher (routinely up to polynomial degree six). Second, the simulation data and derived quantities of the simulation data have discontinuities at element boundaries, and these discontinuities do not match the input requirements for the topological tools. One solution to both issues is to transform the high-order data to achieve low-order, continuous inputs for topological analysis. Nevertheless, there has been little work evaluating the possible transformation choices and their downstream effect on the topological analysis. We perform an empirical study to evaluate two commonly used data transformation methodologies along with the recently introduced L-SIAC filter for processing high-order simulation data. Our results show diverse behaviors are possible. We offer some guidance about how best to consider a pipeline of topological analysis of HO-FEM simulations with the currently available implementations of topological analysis.},
  archive      = {J_TVCG},
  author       = {Ashok Jallepalli and Joshua A. Levine and Robert M. Kirby},
  doi          = {10.1109/TVCG.2019.2934338},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {162-172},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of data transformations on scalar field topological analysis of high-order FEM solutions},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Progressive wasserstein barycenters of persistence diagrams.
<em>TVCG</em>, <em>26</em>(1), 151–161. (<a
href="https://doi.org/10.1109/TVCG.2019.2934256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an efficient algorithm for the progressive approximation of Wasserstein barycenters of persistence diagrams, with applications to the visual analysis of ensemble data. Given a set of scalar fields, our approach enables the computation of a persistence diagram which is representative of the set, and which visually conveys the number, data ranges and saliences of the main features of interest found in the set. Such representative diagrams are obtained by computing explicitly the discrete Wasserstein barycenter of the set of persistence diagrams, a notoriously computationally intensive task. In particular, we revisit efficient algorithms for Wasserstein distance approximation [12,51] to extend previous work on barycenter estimation [94]. We present a new fast algorithm, which progressively approximates the barycenter by iteratively increasing the computation accuracy as well as the number of persistent features in the output diagram. Such a progressivity drastically improves convergence in practice and allows to design an interruptible algorithm, capable of respecting computation time constraints. This enables the approximation of Wasserstein barycenters within interactive times. We present an application to ensemble clustering where we revisit the k-means algorithm to exploit our barycenters and compute, within execution time constraints, meaningful clusters of ensemble data along with their barycenter diagram. Extensive experiments on synthetic and real-life data sets report that our algorithm converges to barycenters that are qualitatively meaningful with regard to the applications, and quantitatively comparable to previous techniques, while offering an order of magnitude speedup when run until convergence (without time constraint). Our algorithm can be trivially parallelized to provide additional speedups in practice on standard workstations. We provide a lightweight C++ implementation of our approach that can be used to reproduce our results.},
  archive      = {J_TVCG},
  author       = {Jules Vidal and Joseph Budin and Julien Tierny},
  doi          = {10.1109/TVCG.2019.2934256},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {151-161},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Progressive wasserstein barycenters of persistence diagrams},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-throughput feature extraction for measuring attributes
of deforming open-cell foams. <em>TVCG</em>, <em>26</em>(1), 140–150.
(<a href="https://doi.org/10.1109/TVCG.2019.2934620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metallic open-cell foams are promising structural materials with applications in multifunctional systems such as biomedical implants, energy absorbers in impact, noise mitigation, and batteries. There is a high demand for means to understand and correlate the design space of material performance metrics to the material structure in terms of attributes such as density, ligament and node properties, void sizes, and alignments. Currently, X-ray Computed Tomography (CT) scans of these materials are segmented either manually or with skeletonization approaches that may not accurately model the variety of shapes present in nodes and ligaments, especially irregularities that arise from manufacturing, image artifacts, or deterioration due to compression. In this paper, we present a new workflow for analysis of open-cell foams that combines a new density measurement to identify nodal structures, and topological approaches to identify ligament structures between them. Additionally, we provide automated measurement of foam properties. We demonstrate stable extraction of features and time-tracking in an image sequence of a foam being compressed. Our approach allows researchers to study larger and more complex foams than could previously be segmented only manually, and enables the high-throughput analysis needed to predict future foam performance.},
  archive      = {J_TVCG},
  author       = {Steve Petruzza and Attila Gyulassy and Samuel Leventhal and John J. Baglino and Michael Czabaj and Ashley D. Spear and Valerio Pascucci},
  doi          = {10.1109/TVCG.2019.2934620},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {140-150},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {High-throughput feature extraction for measuring attributes of deforming open-cell foams},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Why authors don’t visualize uncertainty. <em>TVCG</em>,
<em>26</em>(1), 130–139. (<a
href="https://doi.org/10.1109/TVCG.2019.2934287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clear presentation of uncertainty is an exception rather than rule in media articles, data-driven reports, and consumer applications, despite proposed techniques for communicating sources of uncertainty in data. This work considers, Why do so many visualization authors choose not to visualize uncertainty? I contribute a detailed characterization of practices, associations, and attitudes related to uncertainty communication among visualization authors, derived from the results of surveying 90 authors who regularly create visualizations for others as part of their work, and interviewing thirteen influential visualization designers. My results highlight challenges that authors face and expose assumptions and inconsistencies in beliefs about the role of uncertainty in visualization. In particular, a clear contradiction arises between authors&#39; acknowledgment of the value of depicting uncertainty and the norm of omitting direct depiction of uncertainty. To help explain this contradiction, I present a rhetorical model of uncertainty omission in visualization-based communication. I also adapt a formal statistical model of how viewers judge the strength of a signal in a visualization to visualization-based communication, to argue that uncertainty communication necessarily reduces degrees of freedom in viewers&#39; statistical inferences. I conclude with recommendations for how visualization research on uncertainty communication could better serve practitioners&#39; current needs and values while deepening understanding of assumptions that reinforce uncertainty omission.},
  archive      = {J_TVCG},
  author       = {Jessica Hullman},
  doi          = {10.1109/TVCG.2019.2934287},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {130-139},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Why authors don&#39;t visualize uncertainty},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). What is interaction for data visualization? <em>TVCG</em>,
<em>26</em>(1), 119–129. (<a
href="https://doi.org/10.1109/TVCG.2019.2934283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction is fundamental to data visualization, but what “interaction” means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community - including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.},
  archive      = {J_TVCG},
  author       = {Evanthia Dimara and Charles Perin},
  doi          = {10.1109/TVCG.2019.2934283},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {119-129},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {What is interaction for data visualization?},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design by immersion: A transdisciplinary approach to
problem-driven visualizations. <em>TVCG</em>, <em>26</em>(1), 109–118.
(<a href="https://doi.org/10.1109/TVCG.2019.2934790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While previous work exists on how to conduct and disseminate insights from problem-driven visualization projects and design studies, the literature does not address how to accomplish these goals in transdisciplinary teams in ways that advance all disciplines involved. In this paper we introduce and define a new methodological paradigm we call design by immersion, which provides an alternative perspective on problem-driven visualization work. Design by immersion embeds transdisciplinary experiences at the center of the visualization process by having visualization researchers participate in the work of the target domain (or domain experts participate in visualization research). Based on our own combined experiences of working on cross-disciplinary, problem-driven visualization projects, we present six case studies that expose the opportunities that design by immersion enables, including (1) exploring new domain-inspired visualization design spaces, (2) enriching domain understanding through personal experiences, and (3) building strong transdisciplinary relationships. Furthermore, we illustrate how the process of design by immersion opens up a diverse set of design activities that can be combined in different ways depending on the type of collaboration, project, and goals. Finally, we discuss the challenges and potential pitfalls of design by immersion.},
  archive      = {J_TVCG},
  author       = {Kyle Wm. Hall and Adam J. Bradley and Uta Hinrichs and Samuel Huron and Jo Wood and Christopher Collins and Sheelagh Carpendale},
  doi          = {10.1109/TVCG.2019.2934790},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {109-118},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design by immersion: A transdisciplinary approach to problem-driven visualizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data by proxy — material traces as autographic
visualizations. <em>TVCG</em>, <em>26</em>(1), 98–108. (<a
href="https://doi.org/10.1109/TVCG.2019.2934788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information visualization limits itself, per definition, to the domain of symbolic information. This paper discusses arguments why the field should also consider forms of data that are not symbolically encoded, including physical traces and material indicators. Continuing a provocation presented by Pat Hanrahan in his 2004 IEEE Vis capstone address, this paper compares physical traces to visualizations and describes the techniques and visual practices for producing, revealing, and interpreting them. By contrasting information visualization with a speculative counter model of autographic visualization, this paper examines the design principles for material data. Autographic visualization addresses limitations of information visualization, such as the inability to directly reflect the material circumstances of data generation. The comparison between the two models allows probing the epistemic assumptions behind information visualization and uncovers linkages with the rich history of scientific visualization and trace reading. The paper begins by discussing the gap between data visualizations and their corresponding phenomena and proceeds by investigating how material visualizations can bridge this gap. It contextualizes autographic visualization with paradigms such as data physicalization and indexical visualization and grounds it in the broader theoretical literature of semiotics, science and technology studies (STS), and the history of scientific representation. The main section of the paper proposes a foundational design vocabulary for autographic visualization and offers examples of how citizen scientists already use autographic principles in their displays, which seem to violate the canonical principles of information visualization but succeed at fulfilling other rhetorical purposes in evidence construction. The paper concludes with a discussion of the limitations of autographic visualization, a roadmap for the empirical investigation of trace perception, and thoughts about how information visualization and autographic visualization techniques can contribute to each other.},
  archive      = {J_TVCG},
  author       = {Dietmar Offenhuber},
  doi          = {10.1109/TVCG.2019.2934788},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {98-108},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data by proxy — material traces as autographic visualizations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Criteria for rigor in visualization design study.
<em>TVCG</em>, <em>26</em>(1), 87–97. (<a
href="https://doi.org/10.1109/TVCG.2019.2934539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are INFORMED, REFLEXIVE, ABUNDANT, PLAUSIBLE, RESONANT, and TRANSPARENT. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline.},
  archive      = {J_TVCG},
  author       = {Miriah Meyer and Jason Dykes},
  doi          = {10.1109/TVCG.2019.2934539},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {87-97},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Criteria for rigor in visualization design study},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VASABI: Hierarchical user profiles for interactive visual
user behaviour analytics. <em>TVCG</em>, <em>26</em>(1), 77–86. (<a
href="https://doi.org/10.1109/TVCG.2019.2934609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User behaviour analytics (UBA) systems offer sophisticated models that capture users&#39; behaviour over time with an aim to identify fraudulent activities that do not match their profiles. Motivated by the challenges in the interpretation of UBA models, this paper presents a visual analytics approach to help analysts gain a comprehensive understanding of user behaviour at multiple levels, namely individual and group level. We take a user-centred approach to design a visual analytics framework supporting the analysis of collections of users and the numerous sessions of activities they conduct within digital applications. The framework is centred around the concept of hierarchical user profiles that are built based on features derived from sessions, as well as on user tasks extracted using a topic modelling approach to summarise and stratify user behaviour. We externalise a series of analysis goals and tasks, and evaluate our methods through use cases conducted with experts. We observe that with the aid of interactive visual hierarchical user profiles, analysts are able to conduct exploratory and investigative analysis effectively, and able to understand the characteristics of user behaviour to make informed decisions whilst evaluating suspicious users and activities.},
  archive      = {J_TVCG},
  author       = {Phong H. Nguyen and Rafael Henkin and Siming Chen and Natalia Andrienko and Gennady Andrienko and Olivier Thonnard and Cagatay Turkay},
  doi          = {10.1109/TVCG.2019.2934609},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {77-86},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VASABI: Hierarchical user profiles for interactive visual user behaviour analytics},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding the role of alternatives in data analysis
practices. <em>TVCG</em>, <em>26</em>(1), 66–76. (<a
href="https://doi.org/10.1109/TVCG.2019.2934593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data workers are people who perform data analysis activities as a part of their daily work but do not formally identify as data scientists. They come from various domains and often need to explore diverse sets of hypotheses and theories, a variety of data sources, algorithms, methods, tools, and visual designs. Taken together, we call these alternatives. To better understand and characterize the role of alternatives in their analyses, we conducted semi-structured interviews with 12 data workers with different types of expertise. We conducted four types of analyses to understand 1) why data workers explore alternatives; 2) the different notions of alternatives and how they fit into the sensemaking process; 3) the high-level processes around alternatives; and 4) their strategies to generate, explore, and manage those alternatives. We find that participants&#39; diverse levels of domain and computational expertise, experience with different tools, and collaboration within their broader context play an important role in how they explore these alternatives. These findings call out the need for more attention towards a deeper understanding of alternatives and the need for better tools to facilitate the exploration, interpretation, and management of alternatives. Drawing upon these analyses and findings, we present a framework based on participants&#39; 1) degree of attention, 2) abstraction level, and 3) analytic processes. We show how this framework can help understand how data workers consider such alternatives in their analyses and how tool designers might create tools to better support them.},
  archive      = {J_TVCG},
  author       = {Jiali Liu and Nadia Boukhelifa and James R. Eagan},
  doi          = {10.1109/TVCG.2019.2934593},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {66-76},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding the role of alternatives in data analysis practices},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The what-if tool: Interactive probing of machine learning
models. <em>TVCG</em>, <em>26</em>(1), 56–65. (<a
href="https://doi.org/10.1109/TVCG.2019.2934619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.},
  archive      = {J_TVCG},
  author       = {James Wexler and Mahima Pushkarna and Tolga Bolukbasi and Martin Wattenberg and Fernanda Viégas and Jimbo Wilson},
  doi          = {10.1109/TVCG.2019.2934619},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {56-65},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The what-if tool: Interactive probing of machine learning models},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supporting analysis of dimensionality reduction results with
contrastive learning. <em>TVCG</em>, <em>26</em>(1), 45–55. (<a
href="https://doi.org/10.1109/TVCG.2019.2934251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides a good first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additional analysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g., density-based clustering methods) to identify clusters, effective methods for understanding a cluster&#39;s characteristics are still lacking. A cluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforward task when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlights the essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastive principal component analysis (cPCA). Our method, called ccPCA (contrasting clusters in PCA), can calculate each feature&#39;s relative contribution to the contrast between one cluster and other clusters. With ccPCA, we have created an interactive system including a scalable visualization of clusters&#39; feature contributions. We demonstrate the effectiveness of our method and system with case studies using several publicly available datasets.},
  archive      = {J_TVCG},
  author       = {Takanori Fujiwara and Oh-Hyun Kwon and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2019.2934251},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {45-55},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Supporting analysis of dimensionality reduction results with contrastive learning},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NNVA: Neural network assisted visual analysis of yeast cell
polarization simulation. <em>TVCG</em>, <em>26</em>(1), 34–44. (<a
href="https://doi.org/10.1109/TVCG.2019.2934591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex computational models are often designed to simulate real-world physical phenomena in many scientific disciplines. However, these simulation models tend to be computationally very expensive and involve a large number of simulation input parameters, which need to be analyzed and properly calibrated before the models can be applied for real scientific studies. We propose a visual analysis system to facilitate interactive exploratory analysis of high-dimensional input parameter space for a complex yeast cell polarization simulation. The proposed system can assist the computational biologists, who designed the simulation model, to visually calibrate the input parameters by modifying the parameter values and immediately visualizing the predicted simulation outcome without having the need to run the original expensive simulation for every instance. Our proposed visual analysis system is driven by a trained neural network-based surrogate model as the backend analysis framework. In this work, we demonstrate the advantage of using neural networks as surrogate models for visual analysis by incorporating some of the recent advances in the field of uncertainty quantification, interpretability and explainability of neural network-based models. We utilize the trained network to perform interactive parameter sensitivity analysis of the original simulation as well as recommend optimal parameter configurations using the activation maximization framework of neural networks. We also facilitate detail analysis of the trained network to extract useful insights about the simulation model, learned by the network, during the training process. We performed two case studies, and discovered multiple new parameter configurations, which can trigger high cell polarization results in the original simulation model. We evaluated our results by comparing with the original simulation model outcomes as well as the findings from previous parameter analysis performed by our experts.},
  archive      = {J_TVCG},
  author       = {Subhashis Hazarika and Haoyu Li and Ko-Chih Wang and Han-Wei Shen and Ching-Shan Chou},
  doi          = {10.1109/TVCG.2019.2934591},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {34-44},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NNVA: Neural network assisted visual analysis of yeast cell polarization simulation},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). InSituNet: Deep image synthesis for parameter space
exploration of ensemble simulations. <em>TVCG</em>, <em>26</em>(1),
23–33. (<a href="https://doi.org/10.1109/TVCG.2019.2934312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose InSituNet, a deep learning based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. In situ visualization, generating visualizations at simulation time, is becoming prevalent in handling large-scale simulations because of the I/O and storage constraints. However, in situ visualization approaches limit the flexibility of post-hoc exploration because the raw simulation data are no longer available. Although multiple image-based approaches have been proposed to mitigate this limitation, those approaches lack the ability to explore the simulation parameters. Our approach allows flexible exploration of parameter space for large-scale ensemble simulations by taking advantage of the recent advances in deep learning. Specifically, we design InSituNet as a convolutional regression model to learn the mapping from the simulation and visualization parameters to the visualization results. With the trained model, users can generate new images for different simulation parameters under various visualization settings, which enables in-depth analysis of the underlying ensemble simulations. We demonstrate the effectiveness of InSituNet in combustion, cosmology, and ocean simulations through quantitative and qualitative evaluations.},
  archive      = {J_TVCG},
  author       = {Wenbin He and Junpeng Wang and Hanqi Guo and Ko-Chih Wang and Han-Wei Shen and Mukund Raj and Youssef S. G. Nashed and Tom Peterka},
  doi          = {10.1109/TVCG.2019.2934312},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {23-33},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InSituNet: Deep image synthesis for parameter space exploration of ensemble simulations},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data changes everything: Challenges and opportunities in
data visualization design handoff. <em>TVCG</em>, <em>26</em>(1), 12–22.
(<a href="https://doi.org/10.1109/TVCG.2019.2934538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.},
  archive      = {J_TVCG},
  author       = {Jagoda Walny and Christian Frisson and Mieka West and Doris Kosminsky and Søren Knudsen and Sheelagh Carpendale and Wesley Willett},
  doi          = {10.1109/TVCG.2019.2934538},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {12-22},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data changes everything: Challenges and opportunities in data visualization design handoff},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FlowSense: A natural language interface for visual data
exploration within a dataflow system. <em>TVCG</em>, <em>26</em>(1),
1–11. (<a href="https://doi.org/10.1109/TVCG.2019.2934668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.},
  archive      = {J_TVCG},
  author       = {Bowen Yu and Cláudio T. Silva},
  doi          = {10.1109/TVCG.2019.2934668},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FlowSense: A natural language interface for visual data exploration within a dataflow system},
  volume       = {26},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
