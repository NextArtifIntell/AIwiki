<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds---76">TCDS - 76</h2>
<ul>
<li><details>
<summary>
(2020a). IEEE computational intelligence society information.
<em>TCDS</em>, <em>12</em>(4), C3. (<a
href="https://doi.org/10.1109/TCDS.2020.3040831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2020.3040831},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding decisions in collective risk social dilemma
games using reinforcement learning. <em>TCDS</em>, <em>12</em>(4),
824–840. (<a href="https://doi.org/10.1109/TCDS.2020.3008890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior research has used reinforcement-learning models to investigate human decisions in choice games. However, research has not investigated how reinforcement-learning models expectancy valence learning (EVL) and prospect valence learning (PVL) would explain human decisions in applied judgment games where people face a collective risk social dilemma (CRSD) against societal problems such as climate change. In CRSD game, a group of players invested some part of their private incomes to a public fund over several rounds with the goal of collectively reaching a climate target, failing which climate change would occur with a certain probability making players lose their remaining incomes. In this article, we propose EVL and PVL models in the CRSD game and calibrate model parameters to aggregate and individual human decisions across four between-subjects information conditions, where half of the players in each condition possessed lesser wealth (poor) compared to the other half (rich). Results showed that model calibration to individual decisions provided a more accurate account compared to the calibration to aggregate decisions and the EVL model was better fit compared to the PVL model across most conditions. Both models outperformed the symmetric Nash model across all conditions. Overall, moderate recency, loss aversion, and exploration drove people’s decisions. We present the implications of our model results for situations involving a CRSD.},
  archive      = {J_TCDS},
  author       = {Medha Kumar and Varun Dutt},
  doi          = {10.1109/TCDS.2020.3008890},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {824-840},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Understanding decisions in collective risk social dilemma games using reinforcement learning},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extended interaction with a BCI video game changes
resting-state brain activity. <em>TCDS</em>, <em>12</em>(4), 809–823.
(<a href="https://doi.org/10.1109/TCDS.2020.2985102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video games are a widespread leisure activity and essential for a substantial field of research. In several kinds of research, video games show positive effects on cognition. Video games’ ability to change the brain in a way that improves cognition is already evident in the research world. The underlying brain dynamics assessed by coherence (Coh) and partial-directed coherence (PDC) can shed light on the effect of video game playing. Here, resting-state brain dynamics have been analyzed before and after four weeks of video game playing. Fifteen participants took part in this article, which ran for one consecutive month. Participants played a gem-swapping game with a brain–computer interface (BCI) paradigm for five days per week for approximately 90 min for four continuous weeks. Significant ( $p&amp;lt; 0.05$ ) changes in information flow and connectivity measures for Coh and PDC were found in the fronto-central, fronto-parietal, and centro-parietal networks due to extended interaction with BCI. The results suggest that BCI is a potential facilitator of such resting-state network changes and may help to develop new strategies for improving cognition, but we also cannot deny the possible effects of such an effort on the disruption of a player’s sense of engagement and increased mental fatigue.},
  archive      = {J_TCDS},
  author       = {Avinash Kumar Singh and Yu-Kai Wang and Jung-Tai King and Chin-Teng Lin},
  doi          = {10.1109/TCDS.2020.2985102},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {809-823},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Extended interaction with a BCI video game changes resting-state brain activity},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolving generalized modulatory learning: Unifying
neuromodulation and synaptic plasticity. <em>TCDS</em>, <em>12</em>(4),
797–808. (<a href="https://doi.org/10.1109/TCDS.2019.2960766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromodulation and neuroplasticity work together to help organisms learn to cope in their environment. Experiments demonstrate that simple forms of neuromodulation aid in the learning process. In those studies, neuromodulation was used as a multiplier to scale the learning rate. However, more complex interactions have not been investigated. Our contributions are twofold: 1) we evolve a subnetwork that produces a modulatory signal that 2) is incorporated into the synaptic plasticity rule in nonlinear ways. In our experiments, we compute synaptic updates using a neural network and include the modulatory signal as one of the inputs. This allows evolution to combine the synaptic activity with modulation in highly nonlinear ways to arrive at weight updates. We show that organisms that evolve with this added complexity outperform simpler multiplicative neuromodulation, suggesting that gains might be won by investigating a broader class of interactions between neuromodulation and synaptic plasticity.},
  archive      = {J_TCDS},
  author       = {Lin Wang and Junteng Zheng and Jeff Orchard},
  doi          = {10.1109/TCDS.2019.2960766},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {797-808},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Evolving generalized modulatory learning: Unifying neuromodulation and synaptic plasticity},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward improving engagement in neural rehabilitation:
Attention enhancement based on brain–computer interface and audiovisual
feedback. <em>TCDS</em>, <em>12</em>(4), 787–796. (<a
href="https://doi.org/10.1109/TCDS.2019.2959055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both motor and cognitive function rehabilitation benefits can be improved significantly by patients&#39; active participation. To this goal, an attention enhancement system based on the brain-computer interface (BCI) and audiovisual feedback is proposed. First, an interactive position-tracking riding game is designed to increase the training challenge and neural engagement. Subjects were asked to drive one of the avatars to keep up with another by adjusting their riding speed and attention. Second, the subject&#39;s electroencephalogram (EEG)-based attention level is divided into three regions (low, moderate, and high) by using the theta-to-beta ratio (TBR). According to the subject&#39;s attention states, different speed adjustment strategies are adopted to adjust the tracking challenge and improve the subject&#39;s attention. Besides, if the subject&#39;s attention focused on the training is moderate or low, an auditory feedback will be given to remind the subject to pay more attention to the training. The contrast experimental results show that subjects&#39; performance indicated by overall attention level and average muscle activation can be improved significantly by using the attention enhancement system, which validates the feasibility of the proposed system for improving the neural and motor engagement.},
  archive      = {J_TCDS},
  author       = {Jiaxing Wang and Weiqun Wang and Zeng-Guang Hou},
  doi          = {10.1109/TCDS.2019.2959055},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {787-796},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Toward improving engagement in neural rehabilitation: Attention enhancement based on Brain–Computer interface and audiovisual feedback},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A biologically inspired behavior control for the unexpected
uncertainty with motivated developmental network. <em>TCDS</em>,
<em>12</em>(4), 774–786. (<a
href="https://doi.org/10.1109/TCDS.2019.2953944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the movement of the mobile robot, except the common scenario, sometimes, the robot has to face the unexpected uncertainty in the environment. Traditional methods to address this problem generally use the task-specific method from the engineering perspective, lacking flexibility in the changing environment and being difficult to respond to the environmental challenges. Because of the function of the brain’s neuromodulatory system, human beings have ability to respond to the ever-changing environment quickly. To simulate the working mechanism of the human brain in responding to the unexpected uncertainty in the environment, this article presents a motivated developmental network (MDN) to offer a control configuration for an artificial agent to face the unexpected uncertainty in the environment, through introducing the acetylcholine/norepinephrine (ACh/NE) systems to the MDN. Taking the regulation role of ACh/NE in serotonin and dopamine (DA) into account, a novel learning rate for the hidden layer neurons of the MDN is proposed. Moreover, a novel composite mechanism is presented to decide the moving direction of the agent. Under the modulation of DA, serotonin, ACh, and NE, the agent can perform specific functions effectively, e.g., chase a target and elude the obstacle, especially the sudden obstacle. Goal-directed pursuing behavior in three simulation cases illustrates the effect of the presented neural modulatory systems, for instance, dealing with the unexpected uncertainty, realizing the attentional effort, reinforcement learning, etc. To the best of our knowledge, this article is the first endeavor to address the unexpected uncertainty with the MDN.},
  archive      = {J_TCDS},
  author       = {Dongshu Wang and Wenjie Si and Yong Luo},
  doi          = {10.1109/TCDS.2019.2953944},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {774-786},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A biologically inspired behavior control for the unexpected uncertainty with motivated developmental network},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forming the concept of direction developmentally.
<em>TCDS</em>, <em>12</em>(4), 759–773. (<a
href="https://doi.org/10.1109/TCDS.2019.2955816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of direction is critical for a robot to establish its abilities in spatial perception. This article addresses the issue of how a robot developmentally forms the concept of direction. We propose a novel framework, based on the related mechanisms of humans, in which motion cues are employed instead of other commonly used sensing means like vision or audition. Using motion cues is actually one of the most important ways for humans to acquire concepts. This leads to advantages in two ways: 1) models based on motion cues are usually more convenient for a robot&#39;s control of motion and 2) multimodal perceptual cues can complement each other and result in improved robustness. The framework behind our methodology lies in developmentally forming the concept of direction in two successive phases: 1) the robot acquires the perceptual representation of its motion by motor babbling and 2) the corresponding higher level features are gradually captured, based on which the conceptual representation is obtained and then the concept of direction is formed. The proposed framework and the corresponding models are evaluated with a PKU-HR5.0II humanoid robot. The results show that the robot can successfully form the concept of direction in a manner similar to that of humans.},
  archive      = {J_TCDS},
  author       = {Dingsheng Luo and Mengxi Nie and Yaoyao Wei and Fan Hu and Xihong Wu},
  doi          = {10.1109/TCDS.2019.2955816},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {759-773},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Forming the concept of direction developmentally},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble hierarchical extreme learning machine for speech
dereverberation. <em>TCDS</em>, <em>12</em>(4), 744–758. (<a
href="https://doi.org/10.1109/TCDS.2019.2953620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven deep learning solutions with gradient-based neural architecture, have proven useful in overcoming some limitations of traditional signal processing techniques. However, a large number of reverberant-anechoic training utterance pairs covering as many environmental conditions as possible is required to achieve robust dereverberation performance in unseen testing conditions. In this article, we propose to address the data requirement issue while preserving the advantages of deep neural structures leveraging upon hierarchical extreme learning machines (HELMs), which are not gradient-based neural architectures. In particular, an ensemble HELM learning framework is established to effectively recover anechoic speech from a reverberant one based on spectral mapping. In addition to the ensemble learning framework, we further derive two novel HELM models, namely, highway HELM [HELM(Hwy)] and residual HELM [HELM(Res)], both incorporating low-level features to enrich the information for spectral mapping. We evaluated the proposed ensemble learning framework using simulated and measured impulse responses by employing Texas Instrument and Massachusetts Institute of Technology (TIMIT), Mandarin hearing in noise test (MHINT), and reverberant voice enhancement and recognition benchmark (REVERB) corpora. The experimental results show that the proposed framework outperforms both traditional methods and a recently proposed integrated deep and ensemble learning algorithm in terms of standardized objective and subjective evaluations under matched and mismatched testing conditions for simulated and measured impulse responses.},
  archive      = {J_TCDS},
  author       = {Tassadaq Hussain and Sabato Marco Siniscalchi and Hsiao-Lan Sharon Wang and Yu Tsao and Valerio Mario Salerno and Wen-Hung Liao},
  doi          = {10.1109/TCDS.2019.2953620},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {744-758},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Ensemble hierarchical extreme learning machine for speech dereverberation},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling brain diverse and complex hemodynamic response
patterns via deep recurrent autoencoder. <em>TCDS</em>, <em>12</em>(4),
733–743. (<a href="https://doi.org/10.1109/TCDS.2019.2949195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, a variety of task-based functional MRI (tfMRI) data analysis approaches have been developed, including the general linear model (GLM), sparse representations, and independent component analysis (ICA). However, these methods are mainly shallow models and are limited in faithfully modeling the complex, diverse, and concurrent spatial-temporal functional brain activities. Recently, recurrent neural networks (RNNs) have demonstrated great superiority in modeling temporal dependency of signals, while autoencoder models have been proven to be effective in automatically estimating the optimal representations of the original data. These characteristics of RNNs and autoencoders naturally meet the requirement of modeling hemodynamic response patterns in tfMRI data. Thus, we propose a novel unsupervised framework of deep recurrent autoencoder (DRAE) for modeling hemodynamic response patterns in this article. The basic idea of the DRAE model is to combine the deep RNN and the autoencoder to automatically characterize the meaningful functional brain networks and corresponding diverse and complex hemodynamic response patterns simultaneously. The experimental results demonstrate the superiority of the proposed DRAE model in automatically estimating the diverse and complex hemodynamic response patterns.},
  archive      = {J_TCDS},
  author       = {Yan Cui and Shijie Zhao and Yaowu Chen and Junwei Han and Lei Guo and Li Xie and Tianming Liu},
  doi          = {10.1109/TCDS.2019.2949195},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {733-743},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Modeling brain diverse and complex hemodynamic response patterns via deep recurrent autoencoder},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational modeling of embodied visual perspective
taking. <em>TCDS</em>, <em>12</em>(4), 723–732. (<a
href="https://doi.org/10.1109/TCDS.2019.2949861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are inherently social beings that benefit from their perceptional capability to embody another point of view, typically referred to as perspective taking. Perspective taking is an essential feature in our daily interactions and is pivotal for human development. However, much remains unknown about the precise mechanisms that underlie perspective taking. Here, we show that formalizing perspective taking in a computational model can detail the embodied mechanisms employed by humans in perspective taking. The model’s main building block is a set of action primitives that are passed through a forward model. The model employs a process that selects a subset of action primitives to be passed through the forward model to reduce the response time. The model demonstrates the results that mimic those captured by human data, including response times differences caused by the angular disparity between the perspective-taker and the other agent, the impact of task-irrelevant body posture variations in perspective taking, and differences in the perspective taking strategy between individuals. Our results provide support for the hypothesis that perspective taking is a mental simulation of the physical movements that are required to match another person’s visual viewpoint. Furthermore, the model provides several testable predictions, including the prediction that forced early responses lead to an egocentric bias and that a selection process introduces dependencies between two consecutive trials. Our results indicate potential links between perspective taking and other essential perceptional and cognitive mechanisms, such as active vision and autobiographical memories.},
  archive      = {J_TCDS},
  author       = {Tobias Fischer and Yiannis Demiris},
  doi          = {10.1109/TCDS.2019.2949861},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {723-732},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Computational modeling of embodied visual perspective taking},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Epileptic signal classification with deep EEG features by
stacked CNNs. <em>TCDS</em>, <em>12</em>(4), 709–722. (<a
href="https://doi.org/10.1109/TCDS.2019.2936441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scalp electroencephalogram (EEG)-based epileptic seizure/nonseizure detection has been comprehensively studied, and fruitful achievements have been reported in the past. Yet, few investigations have been paid to the preictal stage detection, which is practically more crucial to epileptics in taking precautions before seizure onset. In this article, a novel epileptic preictal state classification and seizure detection algorithm based on deep features learned by stacked convolutional neural networks (SCNNs) is developed. The mean amplitude of sub-band spectrum map (MAS) obtained from the average sub-band spectra of multichannel EEGs is adopted for representation. The probability feature vectors by stacked convolutional neural networks (CNNs) are extracted in the softmax layer of CNNs, where an adaptive and discriminative feature weighting fusion (AWF) is developed for performance enhancement. Following the deep extraction layer, the effective kernel extreme learning machine (KELM) is adopted for feature learning and epileptic classification. Experiments on the benchmark CHB-MIT database and a real recorded epileptic database are conducted for performance demonstration. Comparisons to many state-of-the-art epileptic classification methods are provided to show the superiority of the proposed SCNN+AWF algorithm.},
  archive      = {J_TCDS},
  author       = {Jiuwen Cao and Jiahua Zhu and Wenbin Hu and Anton Kummert},
  doi          = {10.1109/TCDS.2019.2936441},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {709-722},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Epileptic signal classification with deep EEG features by stacked CNNs},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multiple-attribute decision-making approach to
reinforcement learning. <em>TCDS</em>, <em>12</em>(4), 695–708. (<a
href="https://doi.org/10.1109/TCDS.2019.2924724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the reinforcement learning (RL) system, one important issue is the tradeoff problem between exploration and exploitation. In this paper, we studied this dilemma and proposed a new approach to solving this problem by multiple-attribute decision making (MADM). The applicability of the proposed method is extended by transfer learning. The method decomposes a task into several subtasks and uses the policies of subtasks trained by RL. The proposed visual MADM method (V-MADM) is based on the state-action values of each subtask to select the action with maximal one. Meanwhile, this paper proposes a transfer learning method using a decay function with decreasing probability such that the prior experiences of the subtasks can be utilized to accelerate the learning rate. Finally, the experiment of robot confrontation and Maze walker is performed to evaluate the learning performance of the proposed method. The experimental results show that fewer training cost is needed to obtain a more effective learning performance.},
  archive      = {J_TCDS},
  author       = {Haobin Shi and Meng Xu},
  doi          = {10.1109/TCDS.2019.2924724},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {695-708},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A multiple-attribute decision-making approach to reinforcement learning},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of methods for mitigating within-task luminance
change for eyewear-based cognitive load measurement. <em>TCDS</em>,
<em>12</em>(4), 681–694. (<a
href="https://doi.org/10.1109/TCDS.2018.2876348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye activity-based within-task cognitive load measurement (CLM) is currently not feasible in everyday situations. One important issue to be addressed to move such CLM beyond controlled laboratory environments is determining practical methods for mitigating the pupillary light reflex (PLR) effect in CLM. In this paper, four approaches to dealing with the PLR effect within a modified verbal digit span task are investigated: ignore the PLR, exclude PLR data, compensate for PLR and use PLR features for measurement. During experimental work, cognitive load and the PLR were induced with a modified verbal digit span task and changes in brightness of a large monitor, respectively. The “exclude PLR,” “compensate for PLR,” and “use PLR features” methods were found to improve classification performance by up to 18.5% relative to the “ignore PLR” method, which yielded the worst classification accuracy of 58% using an average pupil diameter feature. Features derived from the transient properties of the PLR response associated with cognitive load were found to yield the superior classification accuracy of 70%, which is an improvement compared with previously published approaches which treated the PLR responses as interference. The findings from this paper suggest that the PLR cannot be easily ignored or normalized, and clearly demonstrate the importance of PLR-aware feature extraction for the design of future eyewear-based always-on CLM in conditions that are more realistic than a darkened, controlled laboratory.},
  archive      = {J_TCDS},
  author       = {Hoe Kin Wong and Julien Epps and Siyuan Chen},
  doi          = {10.1109/TCDS.2018.2876348},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {681-694},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A comparison of methods for mitigating within-task luminance change for eyewear-based cognitive load measurement},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE computational intelligence society information.
<em>TCDS</em>, <em>12</em>(3), C3. (<a
href="https://doi.org/10.1109/TCDS.2020.3019675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2020.3019675},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot manipulation in open environments: New perspectives.
<em>TCDS</em>, <em>12</em>(3), 669–675. (<a
href="https://doi.org/10.1109/TCDS.2019.2921098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of performing everyday manipulation tasks robustly in open environments is currently beyond the capabilities of artificially intelligent robots; humans are required. The difficulty arises from the high variability in open environments; it is not feasible to program for, or train for, every variation. This correspondence paper presents the case for a new approach to the problem, based on three mutually dependent ideas: 1) highly transferable manipulation skills; 2) choice of representation: a scene can be modeled in several different ways; and 3) top-down processes by which the robot&#39;s task can influence the bottom-up processes interpreting a scene. The approach we advocate is supported by evidence from what we know about humans, and also the approach is implicitly taken by human designers in designing representations for robots. We present brief results of an implementation of these ideas in robot vision, and give some guidelines for how the key ideas can be implemented more generally in practical robot systems.},
  archive      = {J_TCDS},
  author       = {Frank Guerin and Paulo Ferreira},
  doi          = {10.1109/TCDS.2019.2921098},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {669-675},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robot manipulation in open environments: New perspectives},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Models of cross-situational and crossmodal word learning in
task-oriented scenarios. <em>TCDS</em>, <em>12</em>(3), 658–668. (<a
href="https://doi.org/10.1109/TCDS.2020.2995045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two related but different cross-situational and crossmodal models of incremental word learning. Model 1 is a Bayesian approach for co-learning object-word mappings and referential intention which allows for incremental learning from only a few situations where the display of referents to the learning system is systematically varied. We demonstrate the robustness of the model with respect to sensory noise, including errors in the visual (object recognition) and auditory (recognition of words) systems. The model is then integrated with a cognitive robotic architecture in order to realize cross-situational word learning on a robot. A different approach to word learning is demonstrated with Model 2, an information-theoretic model for the object- and action-word learning from modality rich input data based on pointwise mutual information. The approach is inspired by insights from language development and learning where the caregiver/teacher typically shows objects and performs actions to the infant while naming what the teacher is doing. We demonstrate the word learning capabilities of the model, feeding it with crossmodal input data from two German multimodal corpora which comprise visual scenes of performed actions and related utterances.},
  archive      = {J_TCDS},
  author       = {Brigitte Krenn and Sepideh Sadeghi and Friedrich Neubarth and Stephanie Gross and Martin Trapp and Matthias Scheutz},
  doi          = {10.1109/TCDS.2020.2995045},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {658-668},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Models of cross-situational and crossmodal word learning in task-oriented scenarios},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A method for napping time recommendation using electrical
brain activity. <em>TCDS</em>, <em>12</em>(3), 645–657. (<a
href="https://doi.org/10.1109/TCDS.2020.2991176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Napping in the workplace has become popular. Knowing how to nap for brain benefits is important. We designed a nap experiment to investigate how napping after different sleep stages impacts procedural memory and sleepiness. In total, 45 nonhabitual nappers were randomly assigned to the Wake group (no napping), N2 group (napping and being woken after enough N2 sleep), and slow-wave sleep (SWS) group (napping and being woken after the end of the first cycle of slow-wave sleep). The results show that the N2 group produces benefits in procedural memory consolidation and sleepiness reduction. In contrast, the SWS group had a lower behavioral performance than the N2 group and their sleepiness. The Wake group had lower performance and higher sleepiness score than the other groups. The results suggest that the ideal napping time is 10-20 min of N2 sleep. Considering that people&#39;s sleep-onset time might be different, we developed a napping time suggestion system using a single-channel electroencephalogram signal. The testing results show that the difference between a 10-min nap of N2 sleep calculated by our system and by an expert is only 0.45 min on average, which demonstrates the feasibility of waking people up at the right time.},
  archive      = {J_TCDS},
  author       = {Sheng-Fu Liang and Yu-Hsuan Shih and Yu-Han Hu and Chih-En Kuo},
  doi          = {10.1109/TCDS.2020.2991176},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {645-657},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A method for napping time recommendation using electrical brain activity},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multitier reinforcement learning model for a cooperative
multiagent system. <em>TCDS</em>, <em>12</em>(3), 636–644. (<a
href="https://doi.org/10.1109/TCDS.2020.2970487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiagent cooperative systems with value-based reinforcement learning, agents learn how to complete the task by an optimal policy learned through value-policy improvement iterations. But how to design a policy that avoids cooperation dilemmas and comes to a common consensus between agents is an important issue. A method that improves the coordination ability of agents in cooperative systems by assessing the cooperative tendency and increases the collective payoff by candidate policy is proposed in this article. The method learns the cooperative rules by recording the cooperation probabilities for agents in a multitier reinforcement learning model. The candidate action sets are selected through the candidate policy which considers the payoff of the coalition. Then, the optimal strategy is selected through the Nash bargaining solution (NBS) from these candidate action sets. The method is tested using two cooperative tasks. The results show that the proposed algorithm, which addresses the instability and ambiguity in a win or learning fast policy hill-climbing (WoLF-PHC) and requires significantly less memory space than the NBS, is more stable and more efficient than other methods.},
  archive      = {J_TCDS},
  author       = {Haobin Shi and Liangjing Zhai and Haibo Wu and Maxwell Hwang and Kao-Shing Hwang and Hsuan-Pei Hsu},
  doi          = {10.1109/TCDS.2020.2970487},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {636-644},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A multitier reinforcement learning model for a cooperative multiagent system},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploration of subjective color perceptual-ability by
EEG-induced type-2 fuzzy classifiers. <em>TCDS</em>, <em>12</em>(3),
618–635. (<a href="https://doi.org/10.1109/TCDS.2019.2959138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perceptual-ability informally refers to the ability of a person to recognize a stimulus. This article deals with color perceptual-ability measurement of subjects using brain response to basic color (red, green, and blue) stimuli. It also attempts to determine the subjective ability to recognize the base colors in the presence of noise tolerance of the base colors, referred to as recognition tolerance. Because of intrasession and intersession variations in subjective brain signal features for a given color stimulus, there exists uncertainty in perceptual-ability. In addition, small variations in the color stimulus result in wide variations in brain signal features, introducing uncertainty in the perceptual-ability of the subject. Type-2 fuzzy logic has been employed to handle the uncertainty in color perceptual-ability measurements due to: 1) variations in brain signal features for a given color and 2) the presence of colored noise on the base colors. Because of the limited power of uncertainty management of the interval type-2 fuzzy sets and high computational overhead of its general type-2 counterpart, we develop a semi-general type-2 fuzzy classifier to recognize the base color. It is important to note that the proposed technique transforms a vertical slice-based general type-2 fuzzy set (GT2FS) into an equivalent interval type-2 counterpart to reduce the computational overhead, without losing the contributions of the secondary memberships. The proposed Semi-GT2FSs-induced classifier yields superior performance in classification accuracy with respect to existing type-1, type-2, and other well-known classifiers. The brain-understanding of a perceived base or noisy base colors is also obtained by exact low-resolution electromagnetic topographic analysis (e-LORETA) software. This is used as the reference for our experimental results of the semi-general type-2 classifier in color perceptual-ability detection. Statistical tests undertaken confirm the superiority of the proposed classifier over its competitors. The proposed technique is expected to have interesting applications in identifying people with excellent color perceptual-ability for chemical, pharmaceutical, and textile industries.},
  archive      = {J_TCDS},
  author       = {Mousumi Laha and Amit Konar and Pratyusha Rakshit and Atulya K. Nagar},
  doi          = {10.1109/TCDS.2019.2959138},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {618-635},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Exploration of subjective color perceptual-ability by EEG-induced type-2 fuzzy classifiers},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reducing redundancy of musculoskeletal robot with convex
hull vertexes selection. <em>TCDS</em>, <em>12</em>(3), 601–617. (<a
href="https://doi.org/10.1109/TCDS.2019.2953642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been a long-term dream of roboticist to create a robot that has similar behavior and appearance to human. With highly redundant joints and actuators, the musculoskeletal system of human can fulfill movement and operation with high precision, flexibility, and reliability. However, the redundancy of the musculoskeletal system also brings great difficulties to the control and fabrication of a human-like robot. Therefore, we propose an algorithm based on the convex hull theory to effectively reduce the number of redundant muscles. Specifically, taking the strength limitation and state-dependent characteristics of muscles into consideration, by analyzing the torque contribution of muscles for the discrete motion of joints, each muscle can be represented by a feature vector, and the convex hulls are formed by these vectors in high-dimensional space. By applying the proposed algorithm, the muscles corresponding to the vertexes of the convex hull are reserved while the redundant muscles within the convex hull are deleted, and a simplified model can be obtained. Sets of motion experiments demonstrate that the simplified model can achieve high-precision movement as a complete model. Based on the simplified model, a hardware platform of a robotic arm is constructed, which consists of six degrees of freedom and 11 pneumatic muscles.},
  archive      = {J_TCDS},
  author       = {Shanlin Zhong and Jiahao Chen and Xingyu Niu and Hang Fu and Hong Qiao},
  doi          = {10.1109/TCDS.2019.2953642},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {601-617},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Reducing redundancy of musculoskeletal robot with convex hull vertexes selection},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time fall detection using uncalibrated fisheye cameras.
<em>TCDS</em>, <em>12</em>(3), 588–600. (<a
href="https://doi.org/10.1109/TCDS.2019.2948786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we describe an approach for the problem of fall detection among the several other everyday activities in indoor environment, using three uncalibrated fisheye cameras. The proposed methodology requires the input segmented silhouettes from the three simultaneously acquired frames, and it is capable of detecting fall events in every location of the imaged environment. The presented algorithm uses the model of fisheye image formation that is based on the spherical projection followed by central projection. Under this model, vertical structures are imaged as straight lines passing through the center of field of view, by a camera with approximately vertical optical axis. The main advantages of this article are the simplicity of the detecting rule, the speed of execution, and the utilization of heterogeneous omnidirectional cameras that allows simultaneous imaging along any direction. The proposed algorithm is designed and parameterized using an extensive data set of synthetic frames. The results from the real videos are presented using the frame statistics and the event-based statistics. The proposed algorithm correctly detects the fall events within standing or walking, as well as other nonfalling activities.},
  archive      = {J_TCDS},
  author       = {Konstantina N. Kottari and Konstantinos K. Delibasis and Ilias G. Maglogiannis},
  doi          = {10.1109/TCDS.2019.2948786},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {588-600},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Real-time fall detection using uncalibrated fisheye cameras},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A circadian rhythms learning network for resisting cognitive
periodic noises of time-varying dynamic system and applications to
robots. <em>TCDS</em>, <em>12</em>(3), 575–587. (<a
href="https://doi.org/10.1109/TCDS.2019.2948066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying dynamic system contaminated by cognitive noises is universal in the fields of engineering and science. In this article, a circadian rhythms learning network (CRLN) is proposed and investigated for disposing the noise disturbed time-varying dynamic system. To do so, a vector-error function is first defined. Second, a neural dynamic model is formulated. Third, a co-state matrix is integrated into the model, of which the states are thPe linear combination of the previous periodic states and errors, which can effectively suppress periodic noises. Theoretical analysis and mathematical derivation prove the global exponential convergence performance of the proposed CRLN model. Finally, a practical noise disturbed time-varying dynamic system example with four different noises illustrates the accuracy and efficacy of the proposed CRLN model. Comparisons with traditional zeroing neural network further verify the advantages of the proposed CRLN model.},
  archive      = {J_TCDS},
  author       = {Zhijun Zhang and Xianzhi Deng and Lingdong Kong and Shuai Li},
  doi          = {10.1109/TCDS.2019.2948066},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {575-587},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A circadian rhythms learning network for resisting cognitive periodic noises of time-varying dynamic system and applications to robots},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path planning in multiple-AUV systems for difficult target
traveling missions: A hybrid metaheuristic approach. <em>TCDS</em>,
<em>12</em>(3), 561–574. (<a
href="https://doi.org/10.1109/TCDS.2019.2944945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple autonomous underwater vehicles (AUVs) are popular for challenging submarine missions. In this article, we focus on the multi-AUV path planning for a common class of missions that need to traverse lots of mission targets in large and complex environments. Given that AUVs are often launched from a movable surface vehicle, e.g., a ship, a multi-AUV target traveling problem is formulated with a requirement of surface point location. Thereafter, a hybrid metaheuristic approach is developed by sequentially performing a cube-based environment modeling, cost map building, voyage planning, and detailed trajectory planning. Specifically, the shortest path faster algorithm (SPFA) is adopted to build a cost map among targets and candidate surface points, and the A* search is utilized for trajectory planning. The optimality of both SPFA and A* can indeed be guaranteed. Thus, voyage planning becomes critical and an algorithm namely DE-C-ACO is proposed by combining the ant colony optimization (ACO) and the differential evolution (DE) with a cluster-based adjustment strategy, i.e., DE-C. DE-C and ACO evolve in parallel for surface point location and voyage generation, respectively. Experiments based on realistic bathymetries are conducted and the results validate the effectiveness and efficiency of the proposed DE-C-ACO.},
  archive      = {J_TCDS},
  author       = {Xue Yu and Wei-Neng Chen and Xiao-Min Hu and Tianlong Gu and Huaqiang Yuan and Yuren Zhou and Jun Zhang},
  doi          = {10.1109/TCDS.2019.2944945},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {561-574},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Path planning in multiple-AUV systems for difficult target traveling missions: A hybrid metaheuristic approach},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted ensemble of deep convolution neural networks for
single-trial character detection in devanagari-script-based p300
speller. <em>TCDS</em>, <em>12</em>(3), 551–560. (<a
href="https://doi.org/10.1109/TCDS.2019.2942437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing Devanagari-script-input-based P300 speller (DS-P3S) performs better mostly with 3-15 trials. This leads to poor information transfer rate (ITR) and a major concern in its real-time adaptation. In DS-P3S, the display paradigm is a matrix of 8×8 size which has 28 more characters than the 6×6 English paradigm. The increased number of characters leads to user-related issues such as a crowding effect, double flashing, adjacency distraction, task difficulty, and fatigue which increases the false detection rate. To tackle this, we propose an efficient single-trial character detection approach for DS-P3S using weighted ensemble of deep convolution neural networks (WE-DCNNs). The weighted strategy is constructed based on measured ensemble diversity to counter the instability by the individual classifier. Additionally, to reduce the false detection rate arising from a single trial, a new channel dropout-based character detection approach is introduced first time in this article. The ITR of 55.45 b/min and an average P300 classification accuracy of 92.64% achieved are comparatively higher than existing methods of DS-P3S. The significant reduction in tradeoff between bias and variance for the different subjects affirms the ease of applicability of the proposed model with just a single trial.},
  archive      = {J_TCDS},
  author       = {Ghanahshyam B. Kshirsagar and Narendra D. Londhe},
  doi          = {10.1109/TCDS.2019.2942437},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {551-560},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Weighted ensemble of deep convolution neural networks for single-trial character detection in devanagari-script-based p300 speller},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sentiment cognition from words shortlisted by fuzzy entropy.
<em>TCDS</em>, <em>12</em>(3), 541–550. (<a
href="https://doi.org/10.1109/TCDS.2019.2937796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis (SA) is the process of evaluating human emotions, opinions, and reviews expressed in text to detect the writer&#39;s mental outlook toward a particular event, topic, product, service, etc., and assign a relevant sentiment. In SA, to highlight the correct words which contribute toward sentiment cognition is very difficult. Simulating this task of shortlisting of words by human observers is challenging due to complexity of human mind&#39;s processing. The use of fuzzy entropy is proposed in this article as an innovative step to tap sentiment quotients of online movie reviews. We have proposed a novel approach of shortlisting of words that help in sentiment cognition using a combination of fuzzy entropy, k -means clustering, and sentiment lexicon SentiWordNet. We have addressed this challenging task of simulating the human cognition of words by developing a model that recognizes sentiment based on fuzzy scores derived from SentiWordNet in an automatic manner. Experiments on two benchmark movie review data sets-IMDB and polarity data sets by Pang and Lee-with training by long short-term memory neural networks, yields high accuracy for our approach as compared to other state-of-the-art-methods of SA.},
  archive      = {J_TCDS},
  author       = {Srishti Vashishtha and Seba Susan},
  doi          = {10.1109/TCDS.2019.2937796},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {541-550},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Sentiment cognition from words shortlisted by fuzzy entropy},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An associative-memory-based reconfigurable memristive
neuromorphic system with synchronous weight training. <em>TCDS</em>,
<em>12</em>(3), 529–540. (<a
href="https://doi.org/10.1109/TCDS.2019.2932179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memristive neuromorphic systems are emerging potential hardware platforms to implement artificial neural networks. Combining features of memristive neuromorphic systems with associative memory, this article proposes an associative-memory-based reconfigurable memristive neuromorphic system. In the proposed system, there are two neural networks: 1) the neural network for computing acceleration and 2) the neural network mimicking associative memory. Then, a case study of the system is presented, which includes an associative memory network to realize apple recognition and a computing acceleration network for iris classification. The associative memory network depends on associative learning to achieve the recognition function. In addition, during the corresponding forgetting process, the connections of the related synaptic circuits are cut off and sent to a synaptic circuit block, realizing variable circuit topology. Further, the synaptic circuits in the block are applied to construct the iris classification network, accomplishing the reconfiguration of the proposed system. The circuit structure of this classification network matches backpropagation (BP) algorithm well. Meanwhile, the network reaches a relatively high classification accuracy after training. In an iteration of the training, all the synaptic circuits that need to change can adjust weights synchronously, which improves training speed.},
  archive      = {J_TCDS},
  author       = {Le Yang and Zhigang Zeng and Yi Huang},
  doi          = {10.1109/TCDS.2019.2932179},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {529-540},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An associative-memory-based reconfigurable memristive neuromorphic system with synchronous weight training},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized energy-aware co-planning of motion and
communication strategies for networked mobile robots. <em>TCDS</em>,
<em>12</em>(3), 519–528. (<a
href="https://doi.org/10.1109/TCDS.2019.2932751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a decentralized planning scheme is proposed to determine simultaneously communication and motion strategies for a team of mobile robots. These robots accomplish a collection of target visiting tasks in a complex environment with optimal energy consumption and guaranteed end-to-end connectivity. Information generated during the team deployment is transmitted to an operation center via a multihop wireless network whose channels are modeled by stochastic variables. For each announced task, mobile robots adopt different roles depending on the task&#39;s nature and the team&#39;s current configuration; then, each robot determines its communication and motion policies by solving a convex optimization problem. Avoiding inter-robot collisions and obstacles is also taken into account. The suggested approach leads to the efficient use of available robots and their energy resources compared to the rival methods in the literature. Effectiveness of the proposed algorithm is illustrated by computer simulations.},
  archive      = {J_TCDS},
  author       = {Shirin Rahmanpour and Reza Mahboobi Esfanjani},
  doi          = {10.1109/TCDS.2019.2932751},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {519-528},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Decentralized energy-aware co-planning of motion and communication strategies for networked mobile robots},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scene-specific multiple cues integration for multiperson
tracking. <em>TCDS</em>, <em>12</em>(3), 511–518. (<a
href="https://doi.org/10.1109/TCDS.2019.2928338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust multiperson tracking requires the correct associations of online detection responses with existing trajectories. In this paper, we propose to integrate multiple cues to resolve the ambiguities in data association for multiperson tracking. Unlike most existing algorithms which integrate multiple cues in the same manner for different scenes, we learn scene-specific parameters to integrate multiple cues for different scenes, as the discriminative power of each cue may vary in different scenes. The scene-specific integration parameters are learned offline by supervised learning method. Min-cost multicommodity flow is employed to solve the data association task. The edge cost of the multicommodity network, which is crucial for the data association, is determined by integrating the multiple cues extracted from the detection response based on the learned scene-specific integration parameters. The experimental results on public multiperson tracking data set demonstrate the effectiveness of the proposed scene-specific integration method.},
  archive      = {J_TCDS},
  author       = {Yanmei Dong and Mingtao Pei and Xiaofeng Liu and Meng Zhao},
  doi          = {10.1109/TCDS.2019.2928338},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {511-518},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Scene-specific multiple cues integration for multiperson tracking},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling-tree model: Efficient implementation of distributed
bayesian inference in neural networks. <em>TCDS</em>, <em>12</em>(3),
497–510. (<a href="https://doi.org/10.1109/TCDS.2019.2927808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experimental observations from neuroscience have suggested that the cognitive process of human brain is realized as probabilistic reasoning and further modeled as Bayesian inference. However, it remains unclear how Bayesian inference could be implemented by network of neurons in the brain. Here a novel implementation of neural circuit, named the sampling-tree model, is proposed to fulfill this aim. By using a deep tree structure to implement sampling with simple and stackable basic neural network motifs for any given Bayesian networks, one can perform local inference while guaranteeing the accuracy of global inference. We show that these task-independent motifs can be used in parallel for fast inference without intensive iteration and scale-limitation. As a result, this model utilizes the structure benefit of neuronal system, i.e., neuronal abundance and multihierarchy, to perform fast inference in an extendable way.},
  archive      = {J_TCDS},
  author       = {Zhaofei Yu and Feng Chen and Jian K. Liu},
  doi          = {10.1109/TCDS.2019.2927808},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {497-510},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Sampling-tree model: Efficient implementation of distributed bayesian inference in neural networks},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Affective EEG-based person identification using the deep
learning approach. <em>TCDS</em>, <em>12</em>(3), 486–496. (<a
href="https://doi.org/10.1109/TCDS.2019.2924648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) is another method for performing person identification (PI). Due to the nature of the EEG signals, EEG-based PI is typically done while a person is performing a mental task such as motor control. However, few studies used EEG-based PI while the person is in different mental states (affective EEG). The aim of this paper is to improve the performance of affective EEG-based PI using a deep learning (DL) approach. We proposed a cascade of DL using a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs). CNNs are used to handle the spatial information from the EEG while RNNs extract the temporal information. We evaluated two types of RNNs, namely long short-term memory (LSTM) and gated recurrent unit (GRU). The proposed method is evaluated on the state-of-the-art affective data set DEAP. The results indicate that CNN-GRU and CNN-LSTM can perform PI from different affective states and reach up to 99.90%-100% mean correct recognition rate. This significantly outperformed a support vector machine baseline system that used power spectral density features. Notably, the 100% mean CRR came from 32 subjects in DEAP data set. Even after the reduction of the number of EEG electrodes from 32 to 5 for more practical applications, the model could still maintain an optimal result obtained from the frontal region, reaching up to 99.17%. Amongst the two DL models, we found that CNN-GRU and CNN-LSTM performed similarly while CNN-GRU expended faster training time. In conclusion, the studied DL approaches overcame the influence of affective states in EEG-Based PI reported in the previous works.},
  archive      = {J_TCDS},
  author       = {Theerawit Wilaiprasitporn and Apiwat Ditthapron and Karis Matchaparn and Tanaboon Tongbuasirilai and Nannapas Banluesombatkul and Ekapol Chuangsuwanich},
  doi          = {10.1109/TCDS.2019.2924648},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {486-496},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Affective EEG-based person identification using the deep learning approach},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Policy sharing using aggregation trees for <span
class="math inline"><em>Q</em></span> -learning in a continuous state
and action spaces. <em>TCDS</em>, <em>12</em>(3), 474–485. (<a
href="https://doi.org/10.1109/TCDS.2019.2926477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = { $Q$ -learning is a generic approach that uses a finite discrete state and an action domain to estimate action values using tabular or function approximation methods. An intelligent agent eventually learns policies from continuous sensory inputs and encodes these environmental inputs onto a discrete state space. The application of $Q$ -learning in a continuous state/action domain is the subject of many studies. This paper uses a tree structure to approximate a $Q$ -function using in a continuous state domain. The agent selects a discretized action with a maximum $Q$ -value and this discretized action is then extended to a continuous action using an action bias function. Reinforcement learning is difficult for a single agent when the state space is huge. This proposed architecture is also applied to a multiagent system, wherein an individual agent transfers its useful $Q$ -values to other agents to accelerate the learning process. Policy is shared between agents by grafting the branches of trees in which $Q$ -values are stored to other trees. The results for simulation show that the proposed architecture performs better than tabular $Q$ -learning and significantly accelerates the learning process because all agents use the sharing mechanisms to cooperate with each other.},
  archive      = {J_TCDS},
  author       = {Yu-Jen Chen and Wei-Cheng Jiang and Ming-Yi Ju and Kao-Shing Hwang},
  doi          = {10.1109/TCDS.2019.2926477},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {474-485},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Policy sharing using aggregation trees for ${Q}$ -learning in a continuous state and action spaces},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep residual network with adaptive learning framework for
fingerprint liveness detection. <em>TCDS</em>, <em>12</em>(3), 461–473.
(<a href="https://doi.org/10.1109/TCDS.2019.2920364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, fingerprint recognition technology has aroused wide attention in the society, especially in the application of identity authentication with a smartphone as a carrier. However, the disadvantage of these devices is that the identification sensors are vulnerable to spoofing attacks from artificial replicas made from clay, gelatin, silicon, etc. To resolve it, a feasible anti-deception countermeasure, called fingerprint liveness detection (FLD), has been proposed. Different from most shallow feature methods, the deep convolutional neural network (DCNN)-based FLD methods have been widely explored with the properties of fast operation, few parameters, and end-to-end feature self-learning. Meanwhile, DCNN faces a pair of contradictory problems, on the one hand, the training accuracy will keep rising with the increasement of multilayer perceptron (MLP), finally tends to a stable value. Continue to increase the number of MLP, results will decline. Much research, on the other hand, shows that the number of MLP is the foundation for realizing a high performance detection. Hereby, we apply deep residual network (DRN) to FLD for the first time to solve the contradiction mentioned in this paper. Next, to eliminate the interference of invalid regions of given images, a region-of-interest (ROI) extraction algorithm is put forward. Afterward, to avoid the parameters learned plunging into local optimization, adaptive learning-based DRNs (ALDRNs), which automatically adjust the learning rate if those monitoring parameters (verification accuracy) are stable, are explored. Finally, we propose a novel texture enhancement based on the local gradient pattern (LGP) method to improve the generalization of a model classifier as well. Experimental results on three benchmark data sets: LivDet 2011, 2013, and 2015, show that our results outperform the state-of-the-art FLD methods.},
  archive      = {J_TCDS},
  author       = {Chengsheng Yuan and Zhihua Xia and Xingming Sun and Q. M. Jonathan Wu},
  doi          = {10.1109/TCDS.2019.2920364},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {461-473},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep residual network with adaptive learning framework for fingerprint liveness detection},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Four-dimensional modeling of fMRI data via spatio–temporal
convolutional neural networks (ST-CNNs). <em>TCDS</em>, <em>12</em>(3),
451–460. (<a href="https://doi.org/10.1109/TCDS.2019.2916916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the human brain functional mechanism has been enabled for investigation by the functional magnetic resonance imaging (fMRI) technology, simultaneous modeling of both the spatial and temporal patterns of brain functional networks from 4-D fMRI data has been a fundamental but still challenging research topic for neuroimaging and medical image analysis fields. Currently, general linear model (GLM), independent component analysis (ICA), sparse dictionary learning, and recently deep learning models, are major methods for fMRI data analysis in either spatial or temporal domains, but there are few joint spatial–temporal methods proposed, as far as we know. As a result, the 4-D nature of fMRI data has not been effectively investigated due to this methodological gap. The recent success of deep learning applications for functional brain decoding and encoding greatly inspired us in this paper to propose a novel framework called spatio–temporal convolutional neural network (ST-CNN) to extract both spatial and temporal characteristics from targeted networks jointly and automatically identify of functional networks. The identification of default mode network (DMN) from fMRI data was used for evaluation of the proposed framework. Results show that only training the framework on one fMRI data set is sufficiently generalizable to identify the DMN from different data sets of different cognitive tasks and resting state. Further investigation of the results shows that the joint-learning scheme can capture the intrinsic relationship between the spatial and temporal characteristics of DMN and thus it ensures the accurate identification of DMN from independent data sets. The ST-CNN model brings new tools and insights for fMRI analysis in cognitive and clinical neuroscience studies.},
  archive      = {J_TCDS},
  author       = {Yu Zhao and Xiang Li and Heng Huang and Wei Zhang and Shijie Zhao and Milad Makkie and Mo Zhang and Quanzheng Li and Tianming Liu},
  doi          = {10.1109/TCDS.2019.2916916},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {451-460},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Four-dimensional modeling of fMRI data via Spatio–Temporal convolutional neural networks (ST-CNNs)},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive question-posing system for robot-assisted
reminiscence from personal photographs. <em>TCDS</em>, <em>12</em>(3),
439–450. (<a href="https://doi.org/10.1109/TCDS.2019.2917030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reminiscence is a lifelong activity that happens throughout our lifespan. While memories can serve as topics in people&#39;s everyday conversations, recalling the past can also help us build self-esteem and increase our level of happiness. In this paper, we aim to develop a robot companion that helps people to recollect their memories from personal photographs. We focus on how a robot can associate concepts relevant to the content in the photographs and evoke people&#39;s memories by asking questions that are both relatable and engaging. To understand the content of a picture, we applied deep learning techniques in order to recognize events, objects, and scenes in it. Then, these observations and any user utterances are considered in a Markov random field (MRF)-based algorithm that contains common sense knowledge of a number of events, with loopy belief propagation being used to infer possible associated concepts and topics. Afterward, the robot poses appropriate questions about the selected topics, guiding the user to reminisce through conversation. Our results show that the proposed system can pose related and appropriate questions to interact with the user, and has the potential guide the user to recall the past in an organized way.},
  archive      = {J_TCDS},
  author       = {Yi-Luen Wu and Edwinn Gamborino and Li-Chen Fu},
  doi          = {10.1109/TCDS.2019.2917030},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {439-450},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Interactive question-posing system for robot-assisted reminiscence from personal photographs},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization of output spike train encoding for a spiking
neuron based on its spatio–temporal input pattern. <em>TCDS</em>,
<em>12</em>(3), 427–438. (<a
href="https://doi.org/10.1109/TCDS.2019.2909355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common learning task for a spiking neuron is to map a spatio-temporal input pattern to a target output spike train. There is no prescribed method for selection of the target output spike train. However, the precise spiking pattern of the target output spike train (output encoding) can affect the learning performance of the spiking neuron. Therefore, systematic methods of finding the optimum spiking pattern for a target output spike train that can be learned by spiking neurons are needed. Here, a method is proposed to adaptively adjust an initial suboptimal output encoding during different learning epochs to find the optimal output encoding. A time varying value of a local event called a spike trace is used to calculate the amount of a required adjustment. The remote supervised method (ReSuMe) learning algorithm is used to train the weights, and the proposed method is used for finding optimized output encoding (optimized desired spikes). Experimental results show that optimizing the output encoding during the learning phase increases the accuracy. The proposed method was applied to find optimized output encoding in classification tasks and the results revealed improvements up to 16.5% in accuracy compared to when using the non-adapted method. It also increases the accuracy in a classification task from 90% to 100%.},
  archive      = {J_TCDS},
  author       = {Aboozar Taherkhani and Georgina Cosma and Thomas Martin McGinnity},
  doi          = {10.1109/TCDS.2019.2909355},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {427-438},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Optimization of output spike train encoding for a spiking neuron based on its Spatio–Temporal input pattern},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive image-based visual servoing for hovering control of
quad-rotor. <em>TCDS</em>, <em>12</em>(3), 417–426. (<a
href="https://doi.org/10.1109/TCDS.2019.2908923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based visual servoing (IBVS) achieves precise positioning and motion control for a relatively stationary target by visual feedback, but problems persist with convergence and stability. Appropriate servoing gains for the IBVS are critical to the convergence and stability, but this control gain is heuristically a constant for most IBVS applications. This paper proposes an integrated method that allows adaptive adjustment of the servoing gain by reinforcement learning (RL) for IBVS control. The proposed method learns a policy to determine the value of the servoing gain on the fly. To ensure rapid convergence for the RL, truncating Q-learning (TQL) with faster convergence is used as learning algorithm, which uses truncated temporal differences (TDs) to update the TD. A nonuniform state space partitioning as a state encoder for RL allows more efficient policy. A strategy that uses the Metropolis derived from the simulated annealing is introduced for selecting the action, in order to balance exploration and exploitation so as to accelerate the learning speed. The integrated IBVS control system is tested using experiments involving a quad-rotor helicopter hovering control. The results of simulation and experiment show that the integrated IBVS method increases stability and ensures more rapid convergence than other methods.},
  archive      = {J_TCDS},
  author       = {Haobin Shi and Lin Shi and Gang Sun and Kao-Shing Hwang},
  doi          = {10.1109/TCDS.2019.2908923},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {417-426},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adaptive image-based visual servoing for hovering control of quad-rotor},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A computational model for child inferences of word meanings
via syntactic categories for different ages and languages.
<em>TCDS</em>, <em>12</em>(3), 401–416. (<a
href="https://doi.org/10.1109/TCDS.2018.2883048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children exploit their morphosyntactic knowledge in order to infer the meanings of words. A recent behavioral study has reported developmental changes in word learning from three to five years of age, with respect to a child&#39;s native language. To understand the computational basis of this phenomenon, we propose a model based on a hidden Markov model (HMM). The HMM acquires syntactic categories of given words as its hidden states, which are associated with observed features. Then, the model infers the syntactic category of a new word, which facilitates the selection of an appropriate visual feature. We hypothesize that using this model with different numbers of categories can replicate the manner in which children of different ages learn words. We perform simulation experiments in three native language environments (English, Japanese, and Chinese), which demonstrate that the model produces similar performances as the children in each environment. Allowing a larger number of categories means that the model can acquire a sufficient number of obvious categories, which results in the successful inference of visual features for novel words. In addition, cross-linguistic differences originating from the acquisition of language-specific syntactic categories are identified, i.e., the syntactic categories learned from English and Chinese corpora are relatively reliant on word orders, whereas the Japanese-trained model exploits morphological cues to infer the syntactic categories.},
  archive      = {J_TCDS},
  author       = {Yuji Kawai and Yuji Oshima and Yuki Sasamoto and Yukie Nagai and Minoru Asada},
  doi          = {10.1109/TCDS.2018.2883048},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {401-416},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A computational model for child inferences of word meanings via syntactic categories for different ages and languages},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The role of criticality of gene regulatory networks in
morphogenesis. <em>TCDS</em>, <em>12</em>(3), 390–400. (<a
href="https://doi.org/10.1109/TCDS.2018.2876090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene regulatory network (GRN)-based morphogenetic models have recently gained an increasing attention. However, the relationship between microscopic properties of intracellular GRNs and macroscopic properties of morphogenetic systems has not been fully understood yet. Here, we propose a theoretical morphogenetic model representing an aggregation of cells, and reveal the relationship between criticality of GRNs and morphogenetic pattern formation. In our model, the positions of the cells are determined by spring-mass-damper kinetics. Each cell has an identical Kauffman&#39;s NK random Boolean network (RBN) as its GRN. We varied the properties of GRNs from ordered, through critical, to chaotic by adjusting node in-degree K. We randomly assigned four cell fates to the attractors of RBNs for cellular behaviors. By comparing diverse morphologies generated in our morphogenetic systems, we investigated what the role of the criticality of GRNs is in forming morphologies. We found that nontrivial spatial patterns were generated most frequently when GRNs were at criticality. Our finding indicates that the criticality of GRNs facilitates the formation of nontrivial morphologies in GRN-based morphogenetic systems.},
  archive      = {J_TCDS},
  author       = {Hyobin Kim and Hiroki Sayama},
  doi          = {10.1109/TCDS.2018.2876090},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {390-400},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The role of criticality of gene regulatory networks in morphogenesis},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emergent jaw predominance in vocal development through
stochastic optimization. <em>TCDS</em>, <em>12</em>(3), 378–389. (<a
href="https://doi.org/10.1109/TCDS.2017.2704912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infant vocal babbling strongly relies on jaw oscillations, especially at the stage of canonical babbling, which underlies the syllabic structure of world languages. In this paper, we propose, model and analyze an hypothesis to explain this predominance of the jaw in early babbling. This hypothesis states that general stochastic optimization principles, when applied to learning sensorimotor control, automatically generate ordered babbling stages with a predominant exploration of jaw movements in early stages. The reason is that those movements impact the auditory effects more than other articulators. In previous computational models, such general principles were shown to selectively freeze and free degrees of freedom in a model reproducing the proximo-distal development observed in infant arm reaching. The contribution of this paper is to show how, using the same methods, we are able to explain such patterns in vocal development. We present three experiments. The two first ones show that the recruitment order of articulators emerging from stochastic optimization depends on the target sound to be achieved but that on average the jaw is largely chosen as the first recruited articulator. The third experiment analyses in more detail how the emerging recruitment order is shaped by the dynamics of the optimization process.},
  archive      = {J_TCDS},
  author       = {Clément Moulin-Frier and Jules Brochard and Freek Stulp and Pierre-Yves Oudeyer},
  doi          = {10.1109/TCDS.2017.2704912},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {378-389},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Emergent jaw predominance in vocal development through stochastic optimization},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). IEEE computational intelligence society information.
<em>TCDS</em>, <em>12</em>(2), C3. (<a
href="https://doi.org/10.1109/TCDS.2020.2991558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2020.2991558},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A concealed information test system based on functional
brain connectivity and signal entropy of audio–visual ERP.
<em>TCDS</em>, <em>12</em>(2), 361–370. (<a
href="https://doi.org/10.1109/TCDS.2020.2991359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deception is a human behavior and its cognitive process and mechanism involve complex neuronal activities of the brain. In this article, we develop a simple and feasible concealed information test (CIT) method which is based on the audio-visual event-related potentials (ERPs) and its spatial and temporal features. The main purpose of this article is to extend a pattern recognition method with functional network parameters and global feature entropy of the EEG signals from the whole brain. At the same time, a novel quantum neural network (QNN) classifier was developed to distinguish the guilty and innocent conditions. Functional connectivity can provide extra information of interdependence between different brain regions from the spatial dimension, and entropy can reflect the complexity of the whole brain from the temporal dimension. 20 subjects participated in the CIT experiment and 30 channel ERPs were recorded. A high accuracy of 87.67% was got in recognizing the concealed information, which was higher than 85.43% for basic features, demonstrated the effectiveness of this article. Future studies should further clarify the connectivity difference and further improve the accuracy of the QNN classifier for CIT.},
  archive      = {J_TCDS},
  author       = {Wenwen Chang and Hong Wang and Zhiguo Lu and Chong Liu},
  doi          = {10.1109/TCDS.2020.2991359},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {361-370},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A concealed information test system based on functional brain connectivity and signal entropy of Audio–Visual ERP},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intraindividual completion time modulates the prediction
error negativity in a virtual 3-d object selection task. <em>TCDS</em>,
<em>12</em>(2), 354–360. (<a
href="https://doi.org/10.1109/TCDS.2020.2991301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prediction error negativity (PEN) can be observed in the human electroencephalogram when there is a mismatch between the predicted and the perceived changes in the environment. Our previous study using a virtual object selection task demonstrated an impact of the level of avatar realism on the PEN, reflecting a mismatch between visual and proprioceptive feedback about the object selection. To investigate the role of temporal integration of different sensory information on the PEN, this article investigated the impact of task completion times on the PEN amplitude, using the same virtual object selection task. Trials from each participant were divided into slow trials and fast trials based on the task completion time, and their associated PEN amplitudes were separately aggregated and analyzed. The result shows that PEN amplitudes are significantly more pronounced in slow trials than in fast trials. This finding suggests that task completion times modulate the PEN amplitude—a long task completion time allowed for a better integration of information from both visual and proprioceptive systems as the basis to detect a mismatch between the expected hand trajectory during a reaching motion and the perceived visual feedback in the virtual environment.},
  archive      = {J_TCDS},
  author       = {Avinash Kumar Singh and Hsiang-Ting Chen and Klaus Gramann and Chin-Teng Lin},
  doi          = {10.1109/TCDS.2020.2991301},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {354-360},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Intraindividual completion time modulates the prediction error negativity in a virtual 3-D object selection task},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain adaptation for EEG emotion recognition based on
latent representation similarity. <em>TCDS</em>, <em>12</em>(2),
344–353. (<a href="https://doi.org/10.1109/TCDS.2019.2949306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition has many potential applications in the real world. Among the many emotion recognition methods, electroencephalogram (EEG) shows advantage in reliability and accuracy. However, the individual differences of EEG limit the generalization of emotion classifiers across subjects. Moreover, due to the nonstationary characteristic of EEG, the signals of one subject change over time, which is a challenge to acquire models that could work across sessions. In this article, we propose a novel domain adaptation method to generalize the emotion recognition models across subjects and sessions. We use neural networks to implement the emotion recognition models, which are optimized by minimizing the classification error on the source while making the source and the target similar in their latent representations. Considering the functional differences of the network layers, we use adversarial training to adapt the marginal distributions in the early layers and perform association reinforcement to adapt the conditional distributions in the last layers. In this way, we approximately adapt the joint distributions by simultaneously adapting marginal distributions and conditional distributions. The method is compared with multiple representatives and recent domain adaptation algorithms on benchmark SEED and DEAP for recognizing three and four affective states, respectively. The experimental results show that the proposed method reaches and outperforms the state of the arts.},
  archive      = {J_TCDS},
  author       = {Jinpeng Li and Shuang Qiu and Changde Du and Yixin Wang and Huiguang He},
  doi          = {10.1109/TCDS.2019.2949306},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {344-353},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Domain adaptation for EEG emotion recognition based on latent representation similarity},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perceptual modeling of tinnitus pitch and loudness.
<em>TCDS</em>, <em>12</em>(2), 332–343. (<a
href="https://doi.org/10.1109/TCDS.2020.2964841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tinnitus is the phantom perception of sound, experienced by 10%-15% of the global population. Computational models have been used to investigate the mechanisms underlying the generation of tinnitus-related activity. However, existing computational models have rarely benchmarked the modeled perception of a phantom sound against recorded data relating to a person&#39;s perception of tinnitus characteristics, such as pitch or loudness. This article details the development of two perceptual models of tinnitus. The models are validated using empirical data from people with tinnitus and the models&#39; performance is compared with the existing perceptual models of tinnitus pitch. The first model extends existing perceptual models of tinnitus, while the second model utilizes an entirely novel approach to modeling tinnitus perception using a linear mixed effects (LMEs) model. The LME model is also used to model the perceived loudness of the phantom sound which has not been considered in previous models. The LME model creates an accurate model of tinnitus pitch and loudness and shows that both tinnitus-related activity and individual perception of sound are factors in the perception of the phantom sound that characterizes tinnitus.},
  archive      = {J_TCDS},
  author       = {Richard Gault and Thomas Martin McGinnity and Sonya Coleman},
  doi          = {10.1109/TCDS.2020.2964841},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {332-343},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Perceptual modeling of tinnitus pitch and loudness},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression-based continuous driving fatigue estimation:
Toward practical implementation. <em>TCDS</em>, <em>12</em>(2), 323–331.
(<a href="https://doi.org/10.1109/TCDS.2019.2929858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental fatigue in drivers is one of the leading causes that give rise to traffic accidents. Electroencephalography (EEG)-based driving fatigue studies showed promising performance in fatigue monitoring. However, complex methodologies are not suitable for practical implementation. In our simulation-based setup that retained the constraints of real driving, we took a step closer to fatigue estimation in a practical scenario. We adopted a preprocessing pipeline with low computational complexity, which can be easily and practically implemented in real time. Moreover, regression-based continuous fatigue estimation was achieved using power spectral features in conjunction with time as the fatigue label. We sought to compare three regression models and three time windows to demonstrate their effects on the performance of fatigue estimation. Dynamic time warping was proposed as a new measure for evaluating the performance of fatigue estimation. The results derived from the validation of the proposed framework on 19 subjects showed that our proposed framework was promising toward practical implementation. Fatigue estimation by the support vector regression with radial basis function kernel and 5-s window length achieved the best performance. We also provided a comprehensive analysis on the spatial distribution of channels and frequency bands mostly contributing to fatigue estimation, which can inform the feature and channel reduction for real-time fatigue monitoring in practical driving. After reducing the number of electrodes by 75%, the proposed framework retained comparable performance in fatigue estimation. This paper demonstrates the feasibility and adaptability of our proposed framework in practical implementation of mental fatigue estimation.},
  archive      = {J_TCDS},
  author       = {Rohit Bose and Hongtao Wang and Andrei Dragomir and Nitish V. Thakor and Anastasios Bezerianos and Junhua Li},
  doi          = {10.1109/TCDS.2019.2929858},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {323-331},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Regression-based continuous driving fatigue estimation: Toward practical implementation},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Facial expression recognition via deep action units graph
network based on psychological mechanism. <em>TCDS</em>, <em>12</em>(2),
311–322. (<a href="https://doi.org/10.1109/TCDS.2019.2917711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is currently a very attractive research field in cognitive psychology and artificial intelligence. In this paper, an innovative FER algorithm called deep action units graph network (DAUGN) is proposed based on psychological mechanism. First, a segmentation method is designed to divide the face into small key areas, which are then converted into corresponding AU-related facial expression regions. Second, the local appearance features of these critical regions are extracted for further action units (AUs) analysis. Then, an AUs facial graph is constructed to represent expressions by taking the AU-related regions as vertices and the distances between each two landmarks as edges. Finally, the adjacency matrices of facial graph are put into a graph-based convolutional neural network to combine the local-appearance and global-geometry information, which greatly improving the performance of FER. Experiments and comparisons on CK+, MMI, and SFEW data sets reveal that the DAUGN achieves more competitive results than several other popular approaches.},
  archive      = {J_TCDS},
  author       = {Yang Liu and Xingming Zhang and Yubei Lin and Haoxiang Wang},
  doi          = {10.1109/TCDS.2019.2917711},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {311-322},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Facial expression recognition via deep action units graph network based on psychological mechanism},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactions with reconfigurable modular robots enhance
spatial reasoning performance. <em>TCDS</em>, <em>12</em>(2), 300–310.
(<a href="https://doi.org/10.1109/TCDS.2019.2914162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable modular robots (RMRobots) can change their shape and functionality (e.g., locomotion styles) to fit different environments, and have been widely investigated in applications, such as exploration and inspection. In this paper, we present a new application of RMRobots for improving human spatial ability which plays a significant role in developing an individual&#39;s performance and achievement in science, technology, engineering, and mathematics (STEM). Two user studies are conducted, and the results show that: 1) the task performance of interacting with RMRobots has a significant positive relationship with mental rotation, a widely used measure of spatial ability; and 2) interacting with RMRobots can effectively improve the performance on a task related to spatial reasoning skills according to behavioral data and electroencephalograph (EEG) indices. Our presented study broadens RMRobot research in the area of human-robot interaction.},
  archive      = {J_TCDS},
  author       = {Minjing Yu and Yong-Jin Liu and Yulin Zhang and Guozhen Zhao and Chun Yu and Yuanchun Shi},
  doi          = {10.1109/TCDS.2019.2914162},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {300-310},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Interactions with reconfigurable modular robots enhance spatial reasoning performance},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applying a psychotherapeutic theory to the modeling of
affective intelligent agents. <em>TCDS</em>, <em>12</em>(2), 285–299.
(<a href="https://doi.org/10.1109/TCDS.2019.2911643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the application of a well-known psychotherapeutic model, the Ellis&#39;s ABC theory, to the modeling of intelligent agents. The final aim is to be able to simulate human realistic behavior in situations in which varied reactions are possible and are determined by several factors such as personality, affective state, prior experiences, and regulation capabilities. The proposed framework, called ABC-EBDI, is the result of integrating the ABC theory into a BDI framework. Other affective theories have also been taken into account to support the modeling of personality, mood, and affect regulation. The distinguishing feature of the proposed framework is the classification of the agent&#39;s cognitive-affective process as either rational or irrational. This process will lead to functional emotions and adaptive conduct, in the first case, and dysfunctional emotions and maladaptive conduct in the second. While the ABC-EBDI framework may be used in different application domains due to its characteristics it can be particularly useful in mental health application. A prototype has been implemented and applied to simulate a bad news scenario. A first evaluation with specialists has been carried out with promising results.},
  archive      = {J_TCDS},
  author       = {Yanet Sánchez and Teresa Coma and Antonio Aguelo and Eva Cerezo},
  doi          = {10.1109/TCDS.2019.2911643},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {285-299},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Applying a psychotherapeutic theory to the modeling of affective intelligent agents},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Concrete action representation model: From neuroscience to
robotics. <em>TCDS</em>, <em>12</em>(2), 272–284. (<a
href="https://doi.org/10.1109/TCDS.2019.2896300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can robotics benefit from neuroscience to build a unified framework that computes actions for both locomotion and manipulation tasks? Inspired by the hierarchical neural control of movement from cortex to spinal cord, we propose a model that generates a concrete action representation in robotics. The action program is composed of four basic modules: 1) pattern selection; 2) spatial coordination; 3) temporal coordination; and 4) sensory motor adaptation. The first and the fourth are considered for behavior initiation. The model is implemented on a humanoid robot to generate rhythmic and nonrhythmic movements. The robot is able to perform tasks like perturbation recovery, and drawing based on different motor programs generated by the same model. Unifying motor control in robotics through a hierarchical structure increases the capacity to gain an accurate and deep understanding of transfer of motor skills between different tasks.},
  archive      = {J_TCDS},
  author       = {John Nassour and Tran Duy Hoa and Payam Atoofi and Fred Hamker},
  doi          = {10.1109/TCDS.2019.2896300},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {272-284},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Concrete action representation model: From neuroscience to robotics},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal turn-taking: Motivations, methodological
challenges, and novel approaches. <em>TCDS</em>, <em>12</em>(2),
260–271. (<a href="https://doi.org/10.1109/TCDS.2019.2892991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we note that despite being a multimodal phenomenon, turn-taking has still been investigated mostly as being unimodal. Based on theoretical positions emphasizing that communication is organized jointly by interaction partners, we identify the challenge of assessing human sequential behavior: 1) spread across different modalities and 2) co-constructed with a partner. By analyzing a corpus of mother-child dyads with cross recurrence quantification analysis and frequent pattern mining, we offer novel steps toward understanding multimodal turn-taking.},
  archive      = {J_TCDS},
  author       = {Katharina J. Rohlfing and Giuseppe Leonardi and Iris Nomikou and Joanna Rączaszek-Leonardi and Eyke Hüllermeier},
  doi          = {10.1109/TCDS.2019.2892991},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {260-271},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal turn-taking: Motivations, methodological challenges, and novel approaches},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised vision-based detection of the active speaker
as support for socially aware language acquisition. <em>TCDS</em>,
<em>12</em>(2), 250–259. (<a
href="https://doi.org/10.1109/TCDS.2019.2927941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a self-supervised method for visual detection of the active speaker in a multiperson spoken interaction scenario. Active speaker detection is a fundamental prerequisite for any artificial cognitive system attempting to acquire language in social settings. The proposed method is intended to complement the acoustic detection of the active speaker, thus improving the system robustness in noisy conditions. The method can detect an arbitrary number of possibly overlapping active speakers based exclusively on visual information about their face. Furthermore, the method does not rely on external annotations, thus complying with cognitive development. Instead, the method uses information from the auditory modality to support learning in the visual domain. This paper reports an extensive evaluation of the proposed method using a large multiperson face-to-face interaction data set. The results show good performance in a speaker dependent setting. However, in a speaker independent setting the proposed method yields a significantly lower performance. We believe that the proposed method represents an essential component of any artificial cognitive system or robotic platform engaging in social interactions.},
  archive      = {J_TCDS},
  author       = {Kalin Stefanov and Jonas Beskow and Giampiero Salvi},
  doi          = {10.1109/TCDS.2019.2927941},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {250-259},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Self-supervised vision-based detection of the active speaker as support for socially aware language acquisition},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint attention in hearing parent–deaf child and hearing
parent–hearing child dyads. <em>TCDS</em>, <em>12</em>(2), 243–249. (<a
href="https://doi.org/10.1109/TCDS.2018.2877658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we characterize establishment of joint attention in hearing parent-deaf child dyads and hearing parent-hearing child dyads. Deaf children were candidates for cochlear implantation who had not yet been implanted and who had no exposure to formal manual communication (e.g., American Sign Language). Because many parents whose deaf children go through early cochlear implant surgery do not themselves know a visual language, these dyads do not share a formal communication system based in a common sensory modality prior to the child&#39;s implantation. Joint attention episodes were identified during free play between hearing parents and their hearing children (N = 4) and hearing parents and their deaf children (N = 4). Attentional episode types included successful parent-initiated joint attention, unsuccessful parent-initiated joint attention, passive attention, successful child-initiated joint attention, and unsuccessful child-initiated joint attention. Group differences emerged in both successful and unsuccessful parent-initiated attempts at joint attention, parent passive attention, and successful child-initiated attempts at joint attention based on proportion of time spent in each. These findings highlight joint attention as an indicator of early communicative efficacy in parent-child interaction for different child populations. We discuss the active role parents and children play in communication, regardless of their hearing status.},
  archive      = {J_TCDS},
  author       = {Heather Bortfeld and John S. Oghalai},
  doi          = {10.1109/TCDS.2018.2877658},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {243-249},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Joint attention in hearing Parent–Deaf child and hearing Parent–Hearing child dyads},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adults use cross-situational statistics for word learning in
a conservative way. <em>TCDS</em>, <em>12</em>(2), 232–242. (<a
href="https://doi.org/10.1109/TCDS.2018.2870161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examined how much adults rely on cross-situational information in word learning by comparing their gaze behavior in a word learning task with models of four learning strategies. We manipulated the input type of situations (consecutive versus interleaved) and the co-occurrence frequencies between words and objects so that adult learners could infer correct word-object mappings based on cross-situational information. There are two key findings. First, an exposure-by-exposure analysis of gaze behavior during the word learning procedure revealed that most participants collected sufficient cross-situational information before they developed a preference for one particular word-object mapping, with consecutive as well as interleaved situations. Second, a classification approach in which individual gaze behavior was attributed to different word learning strategies showed that participants relied mostly on a conservative cross-situational learning (XSL) strategy, compared to Associative XSL, Propose-but-Verify, and Random strategies. Adults relied on Conservative XSL when presented with consecutive and interleaved situations, but they shifted toward Associative XSL when presented with interleaved situations.},
  archive      = {J_TCDS},
  author       = {Suzanne Aussems and Paul Vogt},
  doi          = {10.1109/TCDS.2018.2870161},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {232-242},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adults use cross-situational statistics for word learning in a conservative way},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). When object color is a red herring: Extraneous perceptual
information hinders word learning via referent selection. <em>TCDS</em>,
<em>12</em>(2), 222–231. (<a
href="https://doi.org/10.1109/TCDS.2019.2894507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning words from ambiguous naming events is difficult. In such situations, children struggle with not attending to task irrelevant information when learning object names. This paper reduces the problem space of learning names for object categories by holding color constant between the target and other extraneous objects. We examine how this influences two types of word learning (retention and generalization) in both 30-month-old children (Experiment 1) and the iCub humanoid robot (Experiment 2). Overall, all children and iCub performed well on the retention trials, but they were only able to generalize the novel names to new exemplars of the target categories if the objects were originally encountered in sets with objects of the same colors, not if the objects were originally encountered in sets with objects of different colors. These data demonstrate that less information presented during the learning phase narrows the problem space and leads to better word learning success for both children and iCub. Findings are discussed in terms of cognitive load and desirable difficulties.},
  archive      = {J_TCDS},
  author       = {Jessica S. Horst and Katherine E. Twomey and Anthony F. Morse and Rosie Nurse and Angelo Cangelosi},
  doi          = {10.1109/TCDS.2019.2894507},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {222-231},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {When object color is a red herring: Extraneous perceptual information hinders word learning via referent selection},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Beyond the self: Using grounded affordances to interpret and
describe others’ actions. <em>TCDS</em>, <em>12</em>(2), 209–221. (<a
href="https://doi.org/10.1109/TCDS.2018.2882140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a developmental approach that allows a robot to interpret and describe the actions of human agents by reusing previous experience. The robot first learns the association between words and object affordances by manipulating the objects in its environment. It then uses this information to learn a mapping between its own actions and those performed by a human in a shared environment. It finally fuses the information from these two models to interpret and describe human actions in light of its own experience. In our experiments, we show that the model can be used flexibly to do inference on different aspects of the scene. We can predict the effects of an action on the basis of object properties. We can revise the belief that a certain action occurred, given the observed effects of the human action. In an early action recognition fashion, we can anticipate the effects when the action has only been partially observed. By estimating the probability of words given the evidence and feeding them into a predefined grammar, we can generate relevant descriptions of the scene. We believe that this is a step toward providing robots with the fundamental skills to engage in social collaboration with humans.},
  archive      = {J_TCDS},
  author       = {Giovanni Saponaro and Lorenzo Jamone and Alexandre Bernardino and Giampiero Salvi},
  doi          = {10.1109/TCDS.2018.2882140},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {209-221},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Beyond the self: Using grounded affordances to interpret and describe others’ actions},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Social reinforcement in artificial prelinguistic
development: A study using intrinsically motivated exploration
architectures. <em>TCDS</em>, <em>12</em>(2), 198–208. (<a
href="https://doi.org/10.1109/TCDS.2018.2883249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an intrinsically motivated sensorimotor exploration architecture which considers social reinforcement and motor constraint awareness. The main objective is to study the influence of social interactions during artificial early prelinguistic development. We argue that this architecture contributes to explain development from voiceless to sequence of vowels vocalizations. A cognitive developmental perspective is considered emphasizing embodied cognition and sensorimotor exploratory behaviors. For a new-born agent, motor constraints are unknown. However, the agent is endowed with a somatosensory system that indicates if a motor configuration was reached or not. This information is used to model and predict constraint violations. Furthermore, the architecture considers imitative behaviors that constrain the search space during exploration. Interaction occurs when the learner sensory production is similar to a sensory unit relevant to communication. In that case, the instructor perceives this similitude and reformulates with the relevant sensory unit. When the learner perceives an utterance by the instructor, it attempts to imitate it. Two systems are considered for experimentation: 1) a toy example and 2) a simulated vocal tract. In general, our results suggest that constraint awareness and social reinforcement contribute to achieve less redundant exploration, lower exploration and evaluation errors, and a clearer picture of developmental transitions.},
  archive      = {J_TCDS},
  author       = {Juan M. Acevedo-Valle and Verena V. Hafner and Cecilio Angulo},
  doi          = {10.1109/TCDS.2018.2883249},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {198-208},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Social reinforcement in artificial prelinguistic development: A study using intrinsically motivated exploration architectures},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The subject–object asymmetry revisited: Experimental and
computational approaches to the role of information structure in
children’s argument omissions. <em>TCDS</em>, <em>12</em>(2), 189–197.
(<a href="https://doi.org/10.1109/TCDS.2019.2938924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In two studies, we investigated the relation between information structure and argument omission in German child language in order to quantify to what extent the subject-object hypothesis (i.e., subjects are omitted more often than objects) is influenced by discourse pragmatics. Twenty four children took part in an elicited production study in which they produced transitive subject-verb-object and object-verb-subject sentences. Both constructions are instances of a topic-comment information structure. The results showed that 3;6 year-old children omitted subjects and objects alike when the arguments assumed topics status and were placed in utterance-initial position. In a second study, we then assessed whether a model of language learning implemented with a recency bias (resulting in learning from the end of utterances) would produce similar omission rates of initial arguments. The model was found to be sensitive to the frequency with which both word orders occurred in the input: initial objects were omitted more often than initial subjects, the pattern found in German caregiver speech. The results suggest that argument omission is heavily influenced by information structure and that a subject-object asymmetry per se does not exist.},
  archive      = {J_TCDS},
  author       = {Eileen Graf and Anna Theakston and Daniel Freudenthal and Elena Lieven},
  doi          = {10.1109/TCDS.2019.2938924},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {189-197},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The Subject–Object asymmetry revisited: Experimental and computational approaches to the role of information structure in children’s argument omissions},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Teach your robot your language! Trainable neural parser for
modeling human sentence processing: Examples for 15 languages.
<em>TCDS</em>, <em>12</em>(2), 179–188. (<a
href="https://doi.org/10.1109/TCDS.2019.2957006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a recurrent neural network (RNN) that performs thematic role assignment and can be used for human–robot interaction (HRI). The RNN is trained to map sentence structures to meanings (e.g., predicates). Previously, we have shown that the model is able to generalize on English and French corpora. In this article, we investigate its ability to adapt to various languages originating from Asia or Europe. We show that it can successfully learn to parse sentences related to home scenarios in 15 languages, namely English, German, French, Spanish, Catalan, Basque, Portuguese, Italian, Bulgarian, Turkish, Persian, Hindi, Marathi, Malay, and Mandarin Chinese. Moreover, in the corpora, we have deliberately included variable complex sentences in order to explore the flexibility of the predicate-like output representations. This demonstrates that: 1) the learning principle of our model is not limited to a particular language (or particular sentence structures), but more generic in nature and 2) it can deal with various kind of representations (not only predicates), which enables users to adapt it to their own needs. As the model is inspired from neuroscience and language acquisition theories, this generic and language-independent aspect makes it a good candidate for modeling human sentence processing. Finally, we discuss the potential implementation of the model in a grounded robotic architecture.},
  archive      = {J_TCDS},
  author       = {Xavier Hinaut and Johannes Twiefel},
  doi          = {10.1109/TCDS.2019.2957006},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {179-188},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Teach your robot your language! trainable neural parser for modeling human sentence processing: Examples for 15 languages},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating image-based and knowledge-based representation
learning. <em>TCDS</em>, <em>12</em>(2), 169–178. (<a
href="https://doi.org/10.1109/TCDS.2019.2906685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of brain areas is involved in language understanding and generation, accounting for the scope of language that can refer to many real-world matters. In this paper, we investigate how regularities among real-world entities impact emergent language representations. Specifically, we consider knowledge bases, which represent entities and their relations as structured triples, and image representations, which are obtained via deep convolutional networks. We combine these sources of information to learn representations of an image-based knowledge representation learning (IKRL) model. An attention mechanism lets more informative images contribute more to the image-based representations. Evaluation results show that the model outperforms all baselines on the tasks of knowledge graph (KG) completion and triple classification. In analyzing the learned models, we found that the structure-based and image-based representations integrate different aspects of the entities and the attention mechanism provides robustness during learning.},
  archive      = {J_TCDS},
  author       = {Ruobing Xie and Stefan Heinrich and Zhiyuan Liu and Cornelius Weber and Yuan Yao and Stefan Wermter and Maosong Sun},
  doi          = {10.1109/TCDS.2019.2906685},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {169-178},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Integrating image-based and knowledge-based representation learning},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neurocomputational models capture the effect of learned
labels on infants’ object and category representations. <em>TCDS</em>,
<em>12</em>(2), 160–168. (<a
href="https://doi.org/10.1109/TCDS.2018.2882920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effect of labels on nonlinguistic representations is the focus of substantial theoretical debate in the developmental literature. A recent empirical study demonstrated that ten-month-old infants respond differently to objects for which they know a label relative to unlabeled objects. One account of these results is that infants&#39; label representations are incorporated into their object representations, such that when the object is seen without its label, a novelty response is elicited. These data are compatible with two recent theories of integrated label-object representations, one of which assumes labels are features of object representations, and one which assumes labels are represented separately, but become closely associated across learning. Here, we implement both of these accounts in an auto-encoder neurocomputational model. Simulation data support an account in which labels are features of objects, with the same representational status as the objects&#39; visual and haptic characteristics. Then, we use our model to make predictions about the effect of labels on infants&#39; broader category representations. Overall, we show that the generally accepted link between internal representations and looking times may be more complex than previously thought.},
  archive      = {J_TCDS},
  author       = {Arthur Capelier-Mourguy and Katherine E. Twomey and Gert Westermann},
  doi          = {10.1109/TCDS.2018.2882920},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {160-168},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Neurocomputational models capture the effect of learned labels on infants’ object and category representations},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying the role of vocabulary knowledge in predicting
future word learning. <em>TCDS</em>, <em>12</em>(2), 148–159. (<a
href="https://doi.org/10.1109/TCDS.2019.2928023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can we predict the words a child is going to learn next given information about the words that a child knows now? Do different representations of a child&#39;s vocabulary knowledge affect our ability to predict the acquisition of lexical items for individual children? Past research has often focused on population statistics of vocabulary growth rather than prediction of words an individual child is likely to learn next. We consider a neural network approach to predict vocabulary acquisition. Specifically, we investigate how best to represent the child&#39;s current vocabulary in order to accurately predict future learning. The models we consider are based on qualitatively different sources of information: descriptive information about the child, the specific words a child knows, and representations that aim to capture the child&#39;s aggregate lexical knowledge. Using longitudinal vocabulary data from children aged 15-36 months, we construct neural network models to predict which words are likely to be learned by a particular child in the coming month. Many models based on child-specific vocabulary information outperform models with child information only, suggesting that the words a child knows influence prediction of future language learning. These models provide an understanding of the role of current vocabulary knowledge on future lexical growth.},
  archive      = {J_TCDS},
  author       = {Nicole M. Beckage and Michael C. Mozer and Eliana Colunga},
  doi          = {10.1109/TCDS.2019.2928023},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {148-159},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Quantifying the role of vocabulary knowledge in predicting future word learning},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An epigenetic approach to semantic categories.
<em>TCDS</em>, <em>12</em>(2), 139–147. (<a
href="https://doi.org/10.1109/TCDS.2018.2833387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is argued that early language learning in children emerges from five primary knowledge structures: 1) space; 2) objects; 3) actions; 4) number; and 5) events. These structures constitute the basis for the semantic domains that are used to form categories that represent the meanings of early words. The domains are naturally modeled in conceptual spaces that are based on geometric notions rather than on symbolic representations. It is shown how these semantics domains can be used to generate an epigenetic model of language acquisition. The four first primary knowledge systems are used for prepositions, nouns, verbs, and quantifiers, respectively, while events form the meanings of declarative sentences. This semantic model leads to direct recommendations for how the model can be implemented in robots and other artificial systems.},
  archive      = {J_TCDS},
  author       = {Peter Gärdenfors},
  doi          = {10.1109/TCDS.2018.2833387},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {139-147},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An epigenetic approach to semantic categories},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial special issue on multidisciplinary
perspectives on mechanisms of language learning. <em>TCDS</em>,
<em>12</em>(2), 134–138. (<a
href="https://doi.org/10.1109/TCDS.2020.2991470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans excel at learning from other humans [item 1) in the Appendix). Language facilitates such learning and plays a crucial role. On the one hand, it coordinates our interactions and cooperative behavior [item 2) in the Appendix). On the other hand, language and communication allow to directly incorporate novel knowledge gathered from social interaction or from reading [item 3) in the Appendix). It has been a long-standing goal of artificial intelligence to leverage such communicative abilities [item 4) in the Appendix) for robots and smart software agents which would, at first, simplify our interactions with machines through a more human-like way of coordinating between humans and robots. But furthermore, this would allow us to easily teach these machines, increasing their abilities and skills further which would allow them to become real partners and companions.},
  archive      = {J_TCDS},
  author       = {Malte Schilling and Katharina J. Rohlfing and Paul Vogt and Chen Yu and Michael Spranger},
  doi          = {10.1109/TCDS.2020.2991470},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {134-138},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on multidisciplinary perspectives on mechanisms of language learning},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). IEEE computational intelligence society information.
<em>TCDS</em>, <em>12</em>(1), C3. (<a
href="https://doi.org/10.1109/TCDS.2020.2977436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2020.2977436},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A brain-inspired visual fear responses model for UAV
emergent obstacle dodging. <em>TCDS</em>, <em>12</em>(1), 124–132. (<a
href="https://doi.org/10.1109/TCDS.2019.2939024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dodging emergent dangers is an innate cognitive ability for animals, which helps them to survive in the natural environment. The retina-superior colliculus (SC)-pulvinar- amygdala-periaqueductal gray pathway is responsible for the visual fear responses, and it is able to quickly detect the looming obstacles for innate dodging. Inspired by the mechanism of the visual fear responses pathway, we propose a brain-inspired emergent obstacle dodging method to model the functions of the related brain regions. This method first detects the moving direction and speed of the salient point of moving objects (retina). Then, we detect the looming obstacles (SC). Third, we modulate attention to the most dangerous area (pulvinar). Fourth, if the degree of danger exceeds the threshold (amygdala), the unmanned ariel vehicle (UAV) moves back to dodge it (periaqueductal gray). Two types of experiments are conducted to validate the effectiveness of the proposed model. In a simulated scene, we simulate the process of mice&#39;s fear responses by putting looming dark lights shining on them. In a natural scene, we apply the proposed model to the UAV emergent obstacles dodging. Compared to the stereo vision model, the proposed model is not only more biologically realistic from the mechanisms perspective, but also more accurate and faster for computation.},
  archive      = {J_TCDS},
  author       = {Feifei Zhao and Qingqun Kong and Yi Zeng and Bo Xu},
  doi          = {10.1109/TCDS.2019.2939024},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {124-132},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A brain-inspired visual fear responses model for UAV emergent obstacle dodging},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Usage-based learning in human interaction with an adaptive
virtual assistant. <em>TCDS</em>, <em>12</em>(1), 109–123. (<a
href="https://doi.org/10.1109/TCDS.2019.2927399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today users can interact with popular virtual assistants such as Siri to accomplish their tasks on a digital environment. In these systems, links between natural language requests and their concrete realizations are specified at the conception phase. A more adaptive approach would be to allow the user to provide natural language instructions or demonstrations when a task is unknown by the assistant. An adaptive solution should allow the virtual assistant to operate a much larger digital environment composed of multiple application domains and providers and better match user needs. We have previously developed robotic systems, inspired by human language developmental studies, that provide such a usage-based adaptive capacity. Here, we extend this approach to human interaction with a virtual assistant that can first learn the mapping between verbal commands and basic action semantics of a specific domain. Then, it can learn higher level mapping by combining previously learned procedural knowledge in interaction with the user. The flexibility of the system is demonstrated as the virtual assistant can learn actions in new domains (e-mail, Wikipedia, etc.), and then can learn how e-mail and Wikipedia basic procedures can be combined to form hybrid procedural knowledge.},
  archive      = {J_TCDS},
  author       = {Clément Delgrange and Jean-Michel Dussoux and Peter Ford Dominey},
  doi          = {10.1109/TCDS.2019.2927399},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {109-123},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Usage-based learning in human interaction with an adaptive virtual assistant},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Memory mechanisms for discriminative visual tracking
algorithms with deep neural networks. <em>TCDS</em>, <em>12</em>(1),
98–108. (<a href="https://doi.org/10.1109/TCDS.2019.2900506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-neural-networks-based online visual tracking methods have achieved state-of-the-art results. One of the core components of these methods is the memory pool, in which a number of samples consisting of image patches and the corresponding labels are stored to update the online tracking network. Hence, the mechanism of updating the stored samples determines the performance of the tracking method. In this paper, a novel memory mechanism is proposed to control the writing and reading accesses of the memory pool using credit assignment network H, which learns features of the target object. This memory mechanism comprises the writing and reading mechanisms. In the writing mechanism, network H produces credits for the current tracked object and the samples in the memory pool. This ensures that the reliable samples are written into the memory pool and the unreliable samples are replaced if the memory pool is full. In the reading mechanism, network H assigns an importance score to each sample selected to update the online tracking network. The state-of-the-art tracking methods with and without the proposed memory mechanism are evaluated on the CVPR2013 and OTB100 benchmarks. The experimental results demonstrated that the proposed memory mechanism improves tracking performance significantly.},
  archive      = {J_TCDS},
  author       = {Lituan Wang and Lei Zhang and Jianyong Wang and Zhang Yi},
  doi          = {10.1109/TCDS.2019.2900506},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {98-108},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Memory mechanisms for discriminative visual tracking algorithms with deep neural networks},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic relational object tracking. <em>TCDS</em>,
<em>12</em>(1), 84–97. (<a
href="https://doi.org/10.1109/TCDS.2019.2915763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the topic of semantic world modeling by conjoining probabilistic reasoning and object anchoring. The proposed approach uses a so-called bottom-up object anchoring method that relies on rich continuous attribute values measured from perceptual sensor data. A novel anchoring matching function learns to maintain object entities in space and time and is validated using a large set of trained humanly annotated ground truth data of real-world objects. For more complex scenarios, a high-level probabilistic object tracker has been integrated with the anchoring framework and handles the tracking of occluded objects via reasoning about the state of unobserved objects. We demonstrate the performance of our integrated approach through scenarios such as the shell game scenario, where we illustrate how anchored objects are retained by preserving relations through probabilistic reasoning.},
  archive      = {J_TCDS},
  author       = {Andreas Persson and Pedro Zuidberg Dos Martires and Luc De Raedt and Amy Loutfi},
  doi          = {10.1109/TCDS.2019.2915763},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {84-97},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Semantic relational object tracking},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-shot classification based on multitask mixed attribute
relations and attribute-specific features. <em>TCDS</em>,
<em>12</em>(1), 73–83. (<a
href="https://doi.org/10.1109/TCDS.2019.2902250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot classification is a hot topic in computer vision and pattern recognition. Most zero-shot classification methods are based on the intermediate level representation of attributes to achieve knowledge transfer from the training classes to the unseen test classes. Recently, multitask learning (MTL) has been shown as one of state-of-the-art approaches for attribute learning and zero-shot classification. Aiming at the attribute relation learning, features shared by attributes learning and attribute heterogeneity, we propose a zero-shot classification based on multitask mixed attribute relations and attribute-specific features. First, considering the relationship between attribute-attribute and attribute-features, a second-order attribute relation and attribute-specific features learning model is constructed from training samples based on MTL. Second, second-order attribute relation is extended to high-order attribute relation and multiple attribute classifiers are learned. Finally, zero-shot classification is completed based on the maximum posterior probability. Experimental results on AWA and PubFig data sets show that the proposed method can yield more accurate attribute prediction and zero-shot classification compared with several multitask attribute learning methods.},
  archive      = {J_TCDS},
  author       = {Ping Gong and Xuesong Wang and Yuhu Cheng and Z. Jane Wang and Qiang Yu},
  doi          = {10.1109/TCDS.2019.2902250},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {73-83},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Zero-shot classification based on multitask mixed attribute relations and attribute-specific features},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selective perception as a mechanism to adapt agents to the
environment: An evolutionary approach. <em>TCDS</em>, <em>12</em>(1),
64–72. (<a href="https://doi.org/10.1109/TCDS.2019.2896306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advancement of machine learning makes it possible to consider large amounts of data to learn from. Learning agents may get data ranging on real intervals directly from the environment they interact with, in a process usually time expensive. To improve learning and manage these data, approximated models and memory mechanisms are adopted. In most of the implementations of reinforcement learning facing this type of data, approximation is obtained by neural networks and the process of drawing information from data is mediated by a shortterm memory that stores the previous experiences for additional relearning, to speed-up the learning process, mimicking what is done by people. In this paper, we are proposing a novel computational approach able to selectively filter the information, or cognitive load, for the agent&#39;s short-term memory, thus emulating the attention mechanism characteristic of human perception. In this work, we use genetic algorithms in order to evolve the most efficient attention filter mechanism that would be able to provide the agent with an optimal perception for a specific environment by discriminating which experiences are valuable for the learning process. This approach can evolve a filter which can able to provide an optimal cognitive load of the experiences entering in the agent&#39;s short-term memory of a limited capacity. The evolved sampling dynamics can also lead to the emergence of intrinsically motivated curiosity.},
  archive      = {J_TCDS},
  author       = {Mirza Ramicic and Andrea Bonarini},
  doi          = {10.1109/TCDS.2019.2896306},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {64-72},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Selective perception as a mechanism to adapt agents to the environment: An evolutionary approach},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepFeat: A bottom-up and top-down saliency model based on
deep features of convolutional neural networks. <em>TCDS</em>,
<em>12</em>(1), 54–63. (<a
href="https://doi.org/10.1109/TCDS.2019.2894561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep feature-based saliency model (DeepFeat) is developed to leverage understanding of the prediction of human fixations. Conventional saliency models often predict the human visual attention relying on few image cues. Although such models predict fixations on a variety of image complexities, their approaches are limited to the incorporated features. In this paper, we aim to utilize the deep features of convolutional neural networks by combining bottom-up (BU) and top-down (TD) saliency maps. The proposed framework is applied on deep features of three popular deep convolutional neural networks (DCNNs). We exploit four evaluation metrics to evaluate the correspondence between the proposed saliency model and the ground-truth fixations over two datasets. The results demonstrate that the deep features of pretrained DCNNs over the ImageNet dataset are strong predictors of the human fixations. The incorporation of BU and TD saliency maps outperforms the individual BU or TD implementations. Moreover, in comparison to nine saliency models, including four state-of-the-art and five conventional saliency models, our proposed DeepFeat model outperforms the conventional saliency models over all four evaluation metrics.},
  archive      = {J_TCDS},
  author       = {Ali Mahdi and Jun Qin and Garth Crosby},
  doi          = {10.1109/TCDS.2019.2894561},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {54-63},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {DeepFeat: A bottom-up and top-down saliency model based on deep features of convolutional neural networks},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robotic-assisted rehabilitation trainer improves balance
function in stroke survivors. <em>TCDS</em>, <em>12</em>(1), 43–53. (<a
href="https://doi.org/10.1109/TCDS.2018.2883653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nerve injury after stroke leads to disorders of locomotion and a declining balance function, which increases the risk of falling. Restriction of pelvic motions can hinder successful rehabilitation, hence a robotic-assisted rehabilitation trainer (RART) is proposed to assist patients in controlling the pelvic motions via force field. The mechanical design, kinetic framework, and the intention-based controller of the RART are introduced in this paper. Based on an intention-based control strategy, the robot generates force field that affects the pelvic motions (vertical and lateral motion, rotation, and obliquity) via a compliant pelvic brace while the patient is walking on ground. A control experiment with 16 hemiplegic patients is carried out to examine the effects on recovery of the balance function. Clinical evaluations and gait analysis are performed before and after the treatment. The experimental results show that the proposed control method is effective in motion recognition, and significant improvements of the balance function, gait speed, stride, stride frequency, Fugl-Meyer assessment, peak knee, and hip flexion angle during swing phase for their affected side are observed in the comparisons within the RART group. These preliminary results demonstrate that the proposed robot with force field and visual feedback may be effective in improving the balance function.},
  archive      = {J_TCDS},
  author       = {Jiancheng Ji and Tao Song and Shuai Guo and Fengfeng Xi and Hua Wu},
  doi          = {10.1109/TCDS.2018.2883653},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {43-53},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robotic-assisted rehabilitation trainer improves balance function in stroke survivors},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Abnormal event detection from videos using a two-stream
recurrent variational autoencoder. <em>TCDS</em>, <em>12</em>(1), 30–42.
(<a href="https://doi.org/10.1109/TCDS.2018.2883368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the massive deployment of distributed video surveillance systems, the automatic detection of abnormal events in video streams has become an urgent need. An abnormal event can be considered as a deviation from the regular scene; however, the distribution of normal and abnormal events is severely imbalanced, since the abnormal events do not frequently occur. To make use of a large number of video surveillance videos of regular scenes, we propose a semi-supervised learning scheme, which only uses the data that contains the ordinary scenes. The proposed model has a two-stream structure that is composed of the appearance and motion streams. For each stream, a recurrent variational autoencoder can model the probabilistic distribution of the normal data in a semi-supervised learning scheme. The appearance and motion features from the two streams can provide complementary information to describe this probabilistic distribution. Comprehensive experiments validate the effectiveness of our proposed scheme on several public benchmark data sets which include the Avenue, the Ped1, the Ped2, the Subway-entry, and the Subway-exit.},
  archive      = {J_TCDS},
  author       = {Shiyang Yan and Jeremy S. Smith and Wenjin Lu and Bailing Zhang},
  doi          = {10.1109/TCDS.2018.2883368},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {30-42},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Abnormal event detection from videos using a two-stream recurrent variational autoencoder},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative manipulation for a mobile dual-arm robot using
sequences of dynamic movement primitives. <em>TCDS</em>, <em>12</em>(1),
18–29. (<a href="https://doi.org/10.1109/TCDS.2018.2868921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to extend promising robot applications in human daily lives, robots need to perform dextrous manipulation tasks, particularly for a mobile dual-arm robot. This paper propose a novel control strategy, which consists of a first trial process and a learning phase, to enable a mobile dual-arm robot to complete a grasp-and-place task which can be decomposed into movement sequences, such as reaching, grasping, and cooperative manipulation of a grasped object. Under the guidance of vision system, the robot with physical constraints successfully fulfills the task by tracking trajectories generated by redundancy resolution online using a neural-dynamic optimization. Then a reinforcement learning algorithm called the policy improvement with path integrals for sequences of dynamic movement primitives is applied to learn and adjust the recorded trajectories. Experimental results of the developed mobile dual-arm robot verified that the proposed strategy is able to successfully and optimally complete a grasp-and-place task.},
  archive      = {J_TCDS},
  author       = {Ting Zhao and Mingdi Deng and Zhijun Li and Yingbai Hu},
  doi          = {10.1109/TCDS.2018.2868921},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {18-29},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cooperative manipulation for a mobile dual-arm robot using sequences of dynamic movement primitives},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can the evidence for explanatory reasoning be explained
away? <em>TCDS</em>, <em>12</em>(1), 12–17. (<a
href="https://doi.org/10.1109/TCDS.2018.2861832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent evidence appears to show a close connection between explanation and belief revision, specifically, the revision of graded beliefs. Insofar as this is also evidence of violations of Bayesian norms of reasoning, the question arises whether we are facing a new bias here, on a par with previously discovered biases in probabilistic reasoning. We consider an apparently successful attempt by Costello and Watts to explain away a number of known such biases in terms of sampling error, which makes those biases look entirely innocuous and compatible with the descriptive adequacy of Bayesian psychology in any but the most uninteresting way. Specifically, we query whether this attempt can be extended to neutralize the aforementioned evidence allegedly showing that explanatory considerations influence our reasoning in ways inconsistent with Bayesian prescriptions.},
  archive      = {J_TCDS},
  author       = {Igor Douven},
  doi          = {10.1109/TCDS.2018.2861832},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {12-17},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Can the evidence for explanatory reasoning be explained away?},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A furcated visual collision avoidance system for an
autonomous microrobot. <em>TCDS</em>, <em>12</em>(1), 1–11. (<a
href="https://doi.org/10.1109/TCDS.2018.2858742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a secondary reactive collision avoidance system for microclass of robots based on a novel approach known as the furcated luminance-difference processing (FLDP) inspired by the lobula giant movement detector, a wide-field visual neuron located in the lobula layer of a locust nervous system. This paper addresses some of the major collision avoidance challenges: obstacle proximity and direction estimation, and operation in GPS-denied environment with irregular lighting. Additionally, it has proven effective in detecting edges independent of background color, size, and contour. The FLDP executes a series of image enhancement and edge detection algorithms to estimate collision threat-level which further determines whether the robot&#39;s field of view must be dissected where each section&#39;s response is compared against the others to generate a simple collision-free maneuver. Ultimately, the computation load and the performance of the model are assessed against an eclectic set of offline as well as real-time real-world collision scenarios validating the proposed model&#39;s asserted capability to avoid obstacles at more than 670 mm prior to collision, moving at 1.2 ms -1 with a successful avoidance rate of 90% processing at 120 Hz on a simple single-core microcontroller, sufficient to conclude the system&#39;s feasibility for real-time real-world applications that possess fail-safe collision avoidance system.},
  archive      = {J_TCDS},
  author       = {Hamid Isakhani and Nabil Aouf and Odysseas Kechagias-Stamatis and James F. Whidborne},
  doi          = {10.1109/TCDS.2018.2858742},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A furcated visual collision avoidance system for an autonomous microrobot},
  volume       = {12},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
