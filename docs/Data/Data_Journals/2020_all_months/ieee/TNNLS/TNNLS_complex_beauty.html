<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TNNLS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tnnls---474">TNNLS - 474</h2>
<ul>
<li><details>
<summary>
(2020a). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(12), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.3036317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.3036317},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image-based model parameter optimization using
model-assisted generative adversarial networks. <em>TNNLS</em>,
<em>31</em>(12), 5645–5650. (<a
href="https://doi.org/10.1109/TNNLS.2020.2969327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and demonstrate the use of a model-assisted generative adversarial network (GAN) to produce fake images that accurately match true images through the variation of the parameters of the model that describes the features of the images. The generator learns the model parameter values that produce fake images that best match the true images. Two case studies show excellent agreement between the generated best match parameters and the true parameters. The best match model parameter values can be used to retune the default simulation to minimize any bias when applying image recognition techniques to fake and true images. In the case of a real-world experiment, the true images are experimental data with unknown true model parameter values, and the fake images are produced by a simulation that takes the model parameters as input. The model-assisted GAN uses a convolutional neural network to emulate the simulation for all parameter values that, when trained, can be used as a conditional generator for fast fake-image production.},
  archive      = {J_TNNLS},
  author       = {Saúl Alonso-Monsalve and Leigh H. Whitehead},
  doi          = {10.1109/TNNLS.2020.2969327},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5645-5650},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image-based model parameter optimization using model-assisted generative adversarial networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter selection for linear support vector regression.
<em>TNNLS</em>, <em>31</em>(12), 5639–5644. (<a
href="https://doi.org/10.1109/TNNLS.2020.2967637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In linear support vector regression (SVR), the regularization and error sensitivity parameters are used to avoid overfitting the training data. A proper selection of parameters is very essential for obtaining a good model, but the search process may be complicated and time-consuming. In an earlier work by Chu et al. (2015), an effective parameter-selection procedure by using warm-start techniques to solve a sequence of optimization problems has been proposed for linear classification. We extend their techniques to linear SVR, but address some new and challenging issues. In particular, linear classification involves only the regularization parameter, but linear SVR has an extra error sensitivity parameter. We investigate the effective range of each parameter and the sequence in checking the two parameters. Based on this work, an effective tool for the selection of parameters for linear SVR has been available for public use.},
  archive      = {J_TNNLS},
  author       = {Jui-Yang Hsia and Chih-Jen Lin},
  doi          = {10.1109/TNNLS.2020.2967637},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5639-5644},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter selection for linear support vector regression},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transferable linear discriminant analysis. <em>TNNLS</em>,
<em>31</em>(12), 5630–5638. (<a
href="https://doi.org/10.1109/TNNLS.2020.2966746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) has been widely used as the technique of feature exaction. However, LDA may be invalid to address the data from different domains. The reasons are as follows: 1) the distribution discrepancy of data may disturb the linear transformation matrix so that it cannot extract the most discriminative feature and 2) the original design of LDA does not consider the unlabeled data so that the unlabeled data cannot take part in the training process for further improving the performance of LDA. To address these problems, in this brief, we propose a novel transferable LDA (TLDA) method to extend LDA into the scenario in which the data have different probability distributions. The whole learning process of TLDA is driven by the philosophy that the data from the same subspace have a low-rank structure. The matrix rank in TLDA is the key learning criterion to conduct local and global linear transformations for restoring the low-rank structure of data from different distributions and enlarging the distances among different subspaces. In doing so, the variations of distribution discrepancy within the same subspace can be reduced, i.e., data can be aligned well and the maximally separated structure can be achieved for the data from different subspaces. A simple projected subgradient-based method is proposed to optimize the objective of TLDA, and a strict theory proof is provided to guarantee a quick convergence. The experimental evaluation on public data sets demonstrates that our TLDA can achieve better classification performance and outperform the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Na Han and Jigang Wu and Xiaozhao Fang and Jie Wen and Shanhua Zhan and Shengli Xie and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.2966746},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5630-5638},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transferable linear discriminant analysis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Revisiting l2,1-norm robustness with vector outlier
regularization. <em>TNNLS</em>, <em>31</em>(12), 5624–5629. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world applications, data usually contain outliers. One popular approach is to use the L 2,1 -norm function as a robust loss/error function. However, the robustness of the L 2,1 -norm function is not well understood so far. In this brief, we propose a new vector outlier regularization (VOR) framework to understand and analyze the robustness of the L 2,1 -norm function. Our VOR function defines a data point to be the outlier if it is outside a threshold with respect to a theoretical prediction, and regularizes it, i.e., pull it back to the threshold line. Thus, in the VOR function, how far an outlier lies away from its theoretical predicted value does not affect the final regularization and analysis results. One important aspect of the VOR function is that it has an equivalent continuous formulation, based on which we can prove that the L 2,1 -norm function is the limiting case of the proposed VOR function. Based on this theoretical result, we thus provide a new and intuitive explanation for the robustness property of the L 2,1 -norm function. As an example, we use the VOR function to matrix factorization and propose a VOR principal component analysis (PCA) (VORPCA). We show some benefits of VORPCA on data reconstruction and clustering tasks.},
  archive      = {J_TNNLS},
  author       = {Bo Jiang and Chris Ding},
  doi          = {10.1109/TNNLS.2020.2964297},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5624-5629},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Revisiting l2,1-norm robustness with vector outlier regularization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Generalized convolution spectral mixture for multitask
gaussian processes. <em>TNNLS</em>, <em>31</em>(12), 5613–5623. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask Gaussian processes (MTGPs) are a powerful approach for modeling dependencies between multiple related tasks or functions for joint regression. Current kernels for MTGPs cannot fully model nonlinear task correlations and other types of dependencies. In this article, we address this limitation. We focus on spectral mixture (SM) kernels and propose an enhancement of this type of kernels, called multitask generalized convolution SM (MT-GCSM) kernel. The MT-GCSM kernel can model nonlinear task correlations and dependence between components, including time and phase delay dependence. Each task in MT-GCSM has its GCSM kernel with its number of convolution structures, and dependencies between all components from different tasks are considered. Another constraint of current kernels for MTGPs is that components from different tasks are aligned. Here, we lift this constraint by using inner and outer full cross convolution between a base component and the reversed complex conjugate of another base component. Extensive experiments on two synthetic and three real-life data sets illustrate the difference between MT-GCSM and previous SM kernels as well as the practical effectiveness of MT-GCSM.},
  archive      = {J_TNNLS},
  author       = {Kai Chen and Twan van Laarhoven and Perry Groot and Jinsong Chen and Elena Marchiori},
  doi          = {10.1109/TNNLS.2020.2980779},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5613-5623},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized convolution spectral mixture for multitask gaussian processes},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A universal approximation result for difference of
log-sum-exp neural networks. <em>TNNLS</em>, <em>31</em>(12), 5603–5612.
(<a href="https://doi.org/10.1109/TNNLS.2020.2975051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that a neural network whose output is obtained as the difference of the outputs of two feedforward networks with exponential activation function in the hidden layer and logarithmic activation function in the output node, referred to as log-sum-exp (LSE) network, is a smooth universal approximator of continuous functions over convex, compact sets. By using a logarithmic transform, this class of network maps to a family of subtraction-free ratios of generalized posynomials (GPOS), which we also show to be universal approximators of positive functions over log-convex, compact subsets of the positive orthant. The main advantage of difference-LSE networks with respect to classical feedforward neural networks is that, after a standard training phase, they provide surrogate models for a design that possesses a specific difference-of-convex-functions form, which makes them optimizable via relatively efficient numerical methods. In particular, by adapting an existing difference-of-convex algorithm to these models, we obtain an algorithm for performing an effective optimization-based design. We illustrate the proposed approach by applying it to the data-driven design of a diet for a patient with type-2 diabetes and to a nonconvex optimization problem.},
  archive      = {J_TNNLS},
  author       = {Giuseppe C. Calafiore and Stephane Gaubert and Corrado Possieri},
  doi          = {10.1109/TNNLS.2020.2975051},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5603-5612},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A universal approximation result for difference of log-sum-exp neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Heterogeneous domain adaptation: An unsupervised approach.
<em>TNNLS</em>, <em>31</em>(12), 5588–5602. (<a
href="https://doi.org/10.1109/TNNLS.2020.2973293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation leverages the knowledge in one domain-the source domain-to improve learning efficiency in another domain-the target domain. Existing heterogeneous domain adaptation research is relatively well-progressed but only in situations where the target domain contains at least a few labeled instances. In contrast, heterogeneous domain adaptation with an unlabeled target domain has not been well-studied. To contribute to the research in this emerging field, this article presents: 1) an unsupervised knowledge transfer theorem that guarantees the correctness of transferring knowledge and 2) a principal angle-based metric to measure the distance between two pairs of domains: one pair comprises the original source and target domains and the other pair comprises two homogeneous representations of two domains. The theorem and the metric have been implemented in an innovative transfer model, called a Grassmann-linear monotonic maps-geodesic flow kernel (GLG), which is specifically designed for heterogeneous unsupervised domain adaptation (HeUDA). The linear monotonic maps (LMMs) meet the conditions of the theorem and are used to construct homogeneous representations of the heterogeneous domains. The metric shows the extent to which the homogeneous representations have preserved the information in the original source and target domains. By minimizing the proposed metric, the GLG model learns the homogeneous representations of heterogeneous domains and transfers knowledge through these learned representations via a geodesic flow kernel (GFK). To evaluate the model, five public data sets were reorganized into ten HeUDA tasks across three applications: cancer detection, the credit assessment, and text classification. The experiments demonstrate that the proposed model delivers superior performance over the existing baselines.},
  archive      = {J_TNNLS},
  author       = {Feng Liu and Guangquan Zhang and Jie Lu},
  doi          = {10.1109/TNNLS.2020.2973293},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5588-5602},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous domain adaptation: An unsupervised approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pairwise constraint propagation with dual adversarial
manifold regularization. <em>TNNLS</em>, <em>31</em>(12), 5575–5587. (<a
href="https://doi.org/10.1109/TNNLS.2020.2970195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise constraints (PCs) composed of must-links (MLs) and cannot-links (CLs) are widely used in many semisupervised tasks. Due to the limited number of PCs, pairwise constraint propagation (PCP) has been proposed to augment them. However, the existing PCP algorithms only adopt a single matrix to contain all the information, which overlooks the differences between the two types of links such that the discriminability of the propagated PCs is compromised. To this end, this article proposes a novel PCP model via dual adversarial manifold regularization to fully explore the potential of the limited initial PCs. Specifically, we propagate MLs and CLs with two separated variables, called similarity and dissimilarity matrices, under the guidance of the graph structure constructed from data samples. At the same time, the adversarial relationship between the two matrices is taken into consideration. The proposed model is formulated as a nonnegative constrained minimization problem, which can be efficiently solved with convergence theoretically guaranteed. We conduct extensive experiments to evaluate the proposed model, including propagation effectiveness and applications on constrained clustering and metric learning, all of which validate the superior performance of our model to state-of-the-art PCP models.},
  archive      = {J_TNNLS},
  author       = {Yuheng Jia and Hui Liu and Junhui Hou and Sam Kwong},
  doi          = {10.1109/TNNLS.2020.2970195},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5575-5587},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pairwise constraint propagation with dual adversarial manifold regularization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MBA: Mini-batch AUC optimization. <em>TNNLS</em>,
<em>31</em>(12), 5561–5574. (<a
href="https://doi.org/10.1109/TNNLS.2020.2969527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Area under the receiver operating characteristics curve (AUC) is an important metric for a wide range of machine-learning problems, and scalable methods for optimizing AUC have recently been proposed. However, handling very large data sets remains an open challenge for this problem. This article proposes a novel approach to AUC maximization based on sampling mini-batches of positive/negative instance pairs and computing U-statistics to approximate a global risk minimization problem. The resulting algorithm is simple, fast, and learning-rate free. We show that the number of samples required for good performance is independent of the number of pairs available, which is a quadratic function of the positive and negative instances. Extensive experiments show the practical utility of the proposed method.},
  archive      = {J_TNNLS},
  author       = {San Gultekin and Avishek Saha and Adwait Ratnaparkhi and John Paisley},
  doi          = {10.1109/TNNLS.2020.2969527},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5561-5574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MBA: Mini-batch AUC optimization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From discriminant to complete: Reinforcement searching-agent
learning for weakly supervised object detection. <em>TNNLS</em>,
<em>31</em>(12), 5549–5560. (<a
href="https://doi.org/10.1109/TNNLS.2020.2969483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD) is an interesting yet challenging task in the computer vision community. The core is to discover the image regions that contain the complete object instances under the image-level supervision. Existing works usually solve this problem via a proposal selection strategy, which selects the most discriminative box regions from the weakly labeled training images. However, these regions usually only contain the discriminative object parts rather than the complete object instances. To address this problem, this article proposes to learn a searching-agent to gradually mine desirable object regions under a region searching paradigm, where we formulate the searching process as a Markov decision process and learn the searching-agent under a deep reinforcement learning framework. To learn such a searching-agent under the weak supervision, we extract the pseudo-complete object regions and the corresponding local discriminative object parts and introduce the obtained pseudo-target-part training pairs into the reinforcement learning process of the search-agent. This learning strategy has twofold advantages: 1) it can mimic the searching process to reveal complete object regions from a certain discriminative part of the object under the weak supervision and 2) it will not suffer from the learning difficulty arise from the long-action sequence that happens when searching from the entire image range. Comprehensive experiments on benchmark data sets demonstrate that by integrating the learned searching-agent with the existing WSOD method, we can achieve better performance than the other state-of-the-art and baseline methods.},
  archive      = {J_TNNLS},
  author       = {Dingwen Zhang and Junwei Han and Long Zhao and Tao Zhao},
  doi          = {10.1109/TNNLS.2020.2969483},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5549-5560},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {From discriminant to complete: Reinforcement searching-agent learning for weakly supervised object detection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An online event-triggered near-optimal controller for nash
solution in interconnected system. <em>TNNLS</em>, <em>31</em>(12),
5534–5548. (<a
href="https://doi.org/10.1109/TNNLS.2020.2969249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a real-time event-triggered near-optimal controller for the nonlinear discrete-time interconnected system. The interconnected system has a number of subsystems/agents, which pose a nonzero-sum game scenario. The control inputs/policies based on proposed event-based controller methodology attain a Nash equilibrium fulfilling the desired goal of the system. The near-optimal control policies are generated online only at events using actor-critic neural network architecture whose weights are updated too at the same instants. The approach ensures stability as the event-triggering condition for agents is derived using Lyapunov stability analysis. The lower bound on interevent time, boundedness of closed-loop parameters, and optimality of the proposed controller are also guaranteed. The efficacy of the proposed approach has been validated on a practical heating, ventilation, and air-conditioning system for achieving the desired temperature set in four zones of a building. The control update instants are minimized to as low as 27\% for the desired temperature set.},
  archive      = {J_TNNLS},
  author       = {Narendra Kumar Dhar and Nishchal Kumar Verma and Laxmidhar Behera},
  doi          = {10.1109/TNNLS.2020.2969249},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5534-5548},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An online event-triggered near-optimal controller for nash solution in interconnected system},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive optimal control for stochastic multiplayer
differential games using on-policy and off-policy reinforcement
learning. <em>TNNLS</em>, <em>31</em>(12), 5522–5533. (<a
href="https://doi.org/10.1109/TNNLS.2020.2969215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control-theoretic differential games have been used to solve optimal control problems in multiplayer systems. Most existing studies on differential games either assume deterministic dynamics or dynamics corrupted with additive noise. In realistic environments, multidimensional environmental uncertainties often modulate system dynamics in a more complicated fashion. In this article, we study stochastic multiplayer differential games, where the players&#39; dynamics are modulated by randomly time-varying parameters. We first formulate two differential games for systems of general uncertain linear dynamics, including the two-player zero-sum and multiplayer nonzero-sum games. We then show that optimal control policies, which constitute the Nash equilibrium solutions, can be derived from the corresponding Hamiltonian functions. Stability is proven using the Lyapunov type of analysis. In order to solve the stochastic differential games online, we integrate reinforcement learning (RL) and an effective uncertainty sampling method called the multivariate probabilistic collocation method (MPCM). Two learning algorithms, including the on-policy integral RL (IRL) and off-policy IRL, are designed for the formulated games, respectively. We show that the proposed learning algorithms can effectively find the Nash equilibrium solutions for the stochastic multiplayer differential games.},
  archive      = {J_TNNLS},
  author       = {Mushuang Liu and Yan Wan and Frank L. Lewis and Victor G. Lopez},
  doi          = {10.1109/TNNLS.2020.2969215},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5522-5533},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive optimal control for stochastic multiplayer differential games using on-policy and off-policy reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep subspace clustering. <em>TNNLS</em>, <em>31</em>(12),
5509–5521. (<a
href="https://doi.org/10.1109/TNNLS.2020.2968848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a deep extension of sparse subspace clustering, termed deep subspace clustering with L1-norm (DSC-L1). Regularized by the unit sphere distribution assumption for the learned deep features, DSC-L1 can infer a new data affinity matrix by simultaneously satisfying the sparsity principle of SSC and the nonlinearity given by neural networks. One of the appealing advantages brought by DSC-L1 is that when original real-world data do not meet the class-specific linear subspace distribution assumption, DSC-L1 can employ neural networks to make the assumption valid with its nonlinear transformations. Moreover, we prove that our neural network could sufficiently approximate the minimizer under mild conditions. To the best of our knowledge, this could be one of the first deep-learning-based subspace clustering methods. Extensive experiments are conducted on four real-world data sets to show that the proposed method is significantly superior to 17 existing methods for subspace clustering on handcrafted features and raw data.},
  archive      = {J_TNNLS},
  author       = {Xi Peng and Jiashi Feng and Joey Tianyi Zhou and Yingjie Lei and Shuicheng Yan},
  doi          = {10.1109/TNNLS.2020.2968848},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5509-5521},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep subspace clustering},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid-learning algorithm for online dynamic state
estimation in multimachine power systems. <em>TNNLS</em>,
<em>31</em>(12), 5497–5508. (<a
href="https://doi.org/10.1109/TNNLS.2020.2968486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing penetration of distributed generators in the smart grids, having knowledge of rapid real-time electromechanical dynamic states has become crucial to system stability control. Conventional Supervisory Control and Data Acquisition (SCADA)-based dynamic state estimation (DSE) techniques are limited by the slow sampling rates, while the emerging phasor measurement units (PMUs) technology enables rapid real-time measurements at network nodes. Using generator bus terminal voltages, we propose a hybrid-learning DSE (HL-DSE) algorithm to estimate the synchronous machine rotor angle and speed in real time. The HL-DSE takes the power system model into account and trains neuroestimators with real-time data in an online manner. Compared with traditional DSE methods, the HL-DSE overcomes limitations by using a data-driven approach in conjunction with the physical power system model. The time efficiency, accuracy, convergence, and robustness of the proposed algorithm are tested under noises and fault conditions in both small- and large-scale test systems. Simulation results show that the proposed HL-DSE is much more computationally efficient than widely used Kalman filter (KF)-based methods while maintaining comparable accuracy and robustness. In particular, HL-DSE is over 100 times faster than square-root unscented KF (SR-UKF) and 80 times faster than extended KF (EKF). The advantages and challenges of the HL-DSE are also discussed.},
  archive      = {J_TNNLS},
  author       = {Guanyu Tian and Qun Zhou and Rahul Birari and Junjian Qi and Zhihua Qu},
  doi          = {10.1109/TNNLS.2020.2968486},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5497-5508},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid-learning algorithm for online dynamic state estimation in multimachine power systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization of coupled time-delay neural networks with
mode-dependent average dwell time switching. <em>TNNLS</em>,
<em>31</em>(12), 5483–5496. (<a
href="https://doi.org/10.1109/TNNLS.2020.2968342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature, the effects of switching with average dwell time (ADT), Markovian switching, and intermittent coupling on stability and synchronization of dynamic systems have been extensively investigated. However, all of them are considered separately because it seems that the three kinds of switching are different from each other. This article proposes a new concept to unify these switchings and considers global exponential synchronization almost surely (GES a.s.) in an array of neural networks (NNs) with mixed delays (including time-varying delay and unbounded distributed delay), switching topology, and stochastic perturbations. A general switching mechanism with transition probability (TP) and mode-dependent ADT (MDADT) (i.e., TP-based MDADT switching in this article) is introduced. By designing a multiple Lyapunov-Krasovskii functional and developing a set of new analytical techniques, sufficient conditions are obtained to ensure that the coupled NNs with the general switching topology achieve GES a.s., even in the case that there are both synchronizing and nonsynchronizing modes. Our results have removed the restrictive condition that the increment coefficients of the multiple Lyapunov-Krasovskii functional at switching instants are larger than one. As applications, the coupled NNs with Markovian switching topology and intermittent coupling are employed. Numerical examples are provided to demonstrate the effectiveness and the merits of the theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Xinsong Yang and Yang Liu and Jinde Cao and Leszek Rutkowski},
  doi          = {10.1109/TNNLS.2020.2968342},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5483-5496},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of coupled time-delay neural networks with mode-dependent average dwell time switching},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning deep gradient descent optimization for image
deconvolution. <em>TNNLS</em>, <em>31</em>(12), 5468–5482. (<a
href="https://doi.org/10.1109/TNNLS.2020.2968289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an integral component of blind image deblurring, non-blind deconvolution removes image blur with a given blur kernel, which is essential but difficult due to the ill-posed nature of the inverse problem. The predominant approach is based on optimization subject to regularization functions that are either manually designed or learned from examples. Existing learning-based methods have shown superior restoration quality but are not practical enough due to their restricted and static model design. They solely focus on learning a prior and require to know the noise level for deconvolution. We address the gap between the optimization- and learning-based approaches by learning a universal gradient descent optimizer. We propose a recurrent gradient descent network (RGDN) by systematically incorporating deep neural networks into a fully parameterized gradient descent scheme. A hyperparameter-free update unit shared across steps is used to generate the updates from the current estimates based on a convolutional neural network. By training on diverse examples, the RGDN learns an implicit image prior and a universal update rule through recursive supervision. The learned optimizer can be repeatedly used to improve the quality of diverse degenerated observations. The proposed method possesses strong interpretability and high generalization. Extensive experiments on synthetic benchmarks and challenging real-world images demonstrate that the proposed deep optimization method is effective and robust to produce favorable results as well as practical for real-world image deblurring applications.},
  archive      = {J_TNNLS},
  author       = {Dong Gong and Zhen Zhang and Qinfeng Shi and Anton van den Hengel and Chunhua Shen and Yanning Zhang},
  doi          = {10.1109/TNNLS.2020.2968289},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5468-5482},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning deep gradient descent optimization for image deconvolution},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic finite-time h∞ state estimation for discrete-time
semi-markovian jump neural networks with time-varying delays.
<em>TNNLS</em>, <em>31</em>(12), 5456–5467. (<a
href="https://doi.org/10.1109/TNNLS.2020.2968074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the finite-time H ∞ state estimation problem is addressed for a class of discrete-time neural networks with semi-Markovian jump parameters and time-varying delays. The focus is mainly on the design of a state estimator such that the constructed error system is stochastically finite-time bounded with a prescribed H ∞ performance level via finite-time Lyapunov stability theory. By constructing a delay-product-type Lyapunov functional, in which the information of time-varying delays and characteristics of activation functions are fully taken into account, and using the Jensen summation inequality, the free weighting matrix approach, and the extended reciprocally convex matrix inequality, some sufficient conditions are established in terms of linear matrix inequalities to ensure the existence of the state estimator. Finally, numerical examples with simulation results are provided to illustrate the effectiveness of our proposed results.},
  archive      = {J_TNNLS},
  author       = {Wen-Juan Lin and Yong He and Chuan-Ke Zhang and Min Wu},
  doi          = {10.1109/TNNLS.2020.2968074},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5456-5467},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastic finite-time h∞ state estimation for discrete-time semi-markovian jump neural networks with time-varying delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safe intermittent reinforcement learning with static and
dynamic event generators. <em>TNNLS</em>, <em>31</em>(12), 5441–5455.
(<a href="https://doi.org/10.1109/TNNLS.2020.2967871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an intermittent framework for safe reinforcement learning (RL) algorithms. First, we develop a barrier function-based system transformation to impose state constraints while converting the original problem to an unconstrained optimization problem. Second, based on optimal derived policies, two types of intermittent feedback RL algorithms are presented, namely, a static and a dynamic one. We finally leverage an actor/critic structure to solve the problem online while guaranteeing optimality, stability, and safety. Simulation results show the efficacy of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Yongliang Yang and Kyriakos G. Vamvoudakis and Hamidreza Modares and Yixin Yin and Donald C. Wunsch},
  doi          = {10.1109/TNNLS.2020.2967871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5441-5455},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Safe intermittent reinforcement learning with static and dynamic event generators},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble stochastic configuration networks for estimating
prediction intervals: A simultaneous robust training algorithm and its
application. <em>TNNLS</em>, <em>31</em>(12), 5426–5440. (<a
href="https://doi.org/10.1109/TNNLS.2020.2967816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining accurate point prediction of industrial processes&#39; key variables is challenging due to the outliers and noise that are common in industrial data. Hence the prediction intervals (PIs) have been widely adopted to quantify the uncertainty related to the point prediction. In order to improve the prediction accuracy and quantify the level of uncertainty associated with the point prediction, this article estimates the PIs by using ensemble stochastic configuration networks (SCNs) and bootstrap method. The estimated PIs can guarantee both the modeling stability and computational efficiency. To encourage the cooperation among the base SCNs and improve the robustness of the ensemble SCNs when the training data are contaminated with noise and outliers, a simultaneous robust training method of the ensemble SCNs is developed based on the Bayesian ridge regression and M-estimate. Moreover, the hyperparameters of the assumed distributions over noise and output weights of the ensemble SCNs are estimated by the expectation-maximization (EM) algorithm, which can result in the optimal PIs and better prediction accuracy. Finally, the performance of the proposed approach is evaluated on three benchmark data sets and a real-world data set collected from a refinery. The experimental results demonstrate that the proposed approach exhibits better performance in terms of the quality of PIs, prediction accuracy, and robustness.},
  archive      = {J_TNNLS},
  author       = {Jun Lu and Jinliang Ding and Xuewu Dai and Tianyou Chai},
  doi          = {10.1109/TNNLS.2020.2967816},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5426-5440},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ensemble stochastic configuration networks for estimating prediction intervals: A simultaneous robust training algorithm and its application},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-modal attention with semantic consistence for
image–text matching. <em>TNNLS</em>, <em>31</em>(12), 5412–5425. (<a
href="https://doi.org/10.1109/TNNLS.2020.2967597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image-text matching refers to measuring the visual-semantic similarity between an image and a sentence. Recently, the fine-grained matching methods that explore the local alignment between the image regions and the sentence words have shown advance in inferring the image-text correspondence by aggregating pairwise region-word similarity. However, the local alignment is hard to achieve as some important image regions may be inaccurately detected or even missing. Meanwhile, some words with high-level semantics cannot be strictly corresponding to a single-image region. To tackle these problems, we address the importance of exploiting the global semantic consistence between image regions and sentence words as complementary for the local alignment. In this article, we propose a novel hybrid matching approach named Cross-modal Attention with Semantic Consistency (CASC) for image-text matching. The proposed CASC is a joint framework that performs cross-modal attention for local alignment and multilabel prediction for global semantic consistence. It directly extracts semantic labels from available sentence corpus without additional labor cost, which further provides a global similarity constraint for the aggregated region-word similarity obtained by the local alignment. Extensive experiments on Flickr30k and Microsoft COCO (MSCOCO) data sets demonstrate the effectiveness of the proposed CASC on preserving global semantic consistence along with the local alignment and further show its superior image-text matching performance compared with more than 15 state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Xing Xu and Tan Wang and Yang Yang and Lin Zuo and Fumin Shen and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2020.2967597},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5412-5425},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-modal attention with semantic consistence for Image–Text matching},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank tensor train coefficient array estimation for
tensor-on-tensor regression. <em>TNNLS</em>, <em>31</em>(12), 5402–5411.
(<a href="https://doi.org/10.1109/TNNLS.2020.2967022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tensor-on-tensor regression can predict a tensor from a tensor, which generalizes most previous multilinear regression approaches, including methods to predict a scalar from a tensor, and a tensor from a scalar. However, the coefficient array could be much higher dimensional due to both high-order predictors and responses in this generalized way. Compared with the current low CANDECOMP/PARAFAC (CP) rank approximation-based method, the low tensor train (TT) approximation can further improve the stability and efficiency of the high or even ultrahigh-dimensional coefficient array estimation. In the proposed low TT rank coefficient array estimation for tensor-on-tensor regression, we adopt a TT rounding procedure to obtain adaptive ranks, instead of selecting ranks by experience. Besides, an l 2 constraint is imposed to avoid overfitting. The hierarchical alternating least square is used to solve the optimization problem. Numerical experiments on a synthetic data set and two real-life data sets demonstrate that the proposed method outperforms the state-of-the-art methods in terms of prediction accuracy with comparable computational complexity, and the proposed method is more computationally efficient when the data are high dimensional with small size in each mode.},
  archive      = {J_TNNLS},
  author       = {Yipeng Liu and Jiani Liu and Ce Zhu},
  doi          = {10.1109/TNNLS.2020.2967022},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5402-5411},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-rank tensor train coefficient array estimation for tensor-on-tensor regression},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive tracking control of state constraint systems based
on differential neural networks: A barrier lyapunov function approach.
<em>TNNLS</em>, <em>31</em>(12), 5390–5401. (<a
href="https://doi.org/10.1109/TNNLS.2020.2966914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to investigate the trajectory tracking problem of systems with uncertain models and state restrictions using differential neural networks (DNNs). The adaptive control design considers the design of a nonparametric identifier based on a class of continuous artificial neural networks (ANNs). The design of adaptive controllers used the estimated weights on the identifier structure yielding a compensating structure and a linear correction element on the tracking error. The stability of both the identification and tracking errors, considering the DNN, uses a barrier Lyapunov function (BLF) that grow to infinity whenever its arguments approach some finite limits for the state satisfying some predefined ellipsoid bounds. The analysis guarantees the semi-globally uniformly ultimately bounded (SGUUB) solution for the tracking error, which implies the achievement of an invariant set. The suggested controller produces closed-loop bounded signals. This article also presents the comparison between the tracking states forced by the adaptive controller estimated with the DNN based on BLF and quadratic Lyapunov functions as well. The effectiveness of the proposal is demonstrated with a numerical example and an implementation in a real plant (mass-spring system). This comparison confirmed the superiority of the suggested controller based on the BLF using the estimates of the upper bounds for the system states.},
  archive      = {J_TNNLS},
  author       = {Rita Q. Fuentes-Aguilar and Isaac Chairez},
  doi          = {10.1109/TNNLS.2020.2966914},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5390-5401},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive tracking control of state constraint systems based on differential neural networks: A barrier lyapunov function approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Beyond expectation: Deep joint mean and quantile regression
for spatiotemporal problems. <em>TNNLS</em>, <em>31</em>(12), 5377–5389.
(<a href="https://doi.org/10.1109/TNNLS.2020.2966745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal problems are ubiquitous and of vital importance in many research fields. Despite the potential already demonstrated by deep learning methods in modeling spatiotemporal data, typical approaches tend to focus solely on conditional expectations of the output variables being modeled. In this article, we propose a multioutput multiquantile deep learning approach for jointly modeling several conditional quantiles together with the conditional expectation as a way to provide a more complete “picture” of the predictive density in spatiotemporal problems. Using two large-scale data sets from the transportation domain, we empirically demonstrate that, by approaching the quantile regression problem from a multitask learning perspective, it is possible to solve the embarrassing quantile crossings problem while simultaneously significantly outperforming state-of-the-art quantile regression methods. Moreover, we show that jointly modeling the mean and several conditional quantiles not only provides a rich description about the predictive density that can capture heteroscedastic properties at a neglectable computational overhead but also leads to improved predictions of the conditional expectation due to the extra information and the regularization effect induced by the added quantiles.},
  archive      = {J_TNNLS},
  author       = {Filipe Rodrigues and Francisco C. Pereira},
  doi          = {10.1109/TNNLS.2020.2966745},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5377-5389},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Beyond expectation: Deep joint mean and quantile regression for spatiotemporal problems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple convolutional recurrent neural networks for fault
identification and performance degradation evaluation of high-speed
train bogie. <em>TNNLS</em>, <em>31</em>(12), 5363–5376. (<a
href="https://doi.org/10.1109/TNNLS.2020.2966744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important part of high-speed train (HST), the mechanical performance of bogies imposes a direct impact on the safety and reliability of HST. It is a fact that, regardless of the potential mechanical performance degradation status, most existing fault diagnosis methods focus only on the identification of bogie fault types. However, for application scenarios such as auxiliary maintenance, identifying the performance degradation of bogie is critical in determining a particular maintenance strategy. In this article, by considering the intrinsic link between fault type and performance degradation of bogie, a novel multiple convolutional recurrent neural network (M-CRNN) that consists of two CRNN frameworks is proposed for simultaneous diagnosis of fault type and performance degradation state. Specifically, the CRNN framework 1 is designed to detect the fault types of the bogie. Meanwhile, CRNN framework 2, which is formed by CRNN Framework 1 and an RNN module, is adopted to further extract the features of fault performance degradation. It is worth highlighting that M-CRNN extends the structure of traditional neural networks and makes full use of the temporal correlation of performance degradation and model fault types. The effectiveness of the proposed M-CRNN algorithm is tested via the HST model CRH380A at different running speeds, including 160, 200, and 220 km/h. The overall accuracy of M-CRNN, i.e., the product of the accuracies for identifying the fault types and evaluating the fault performance degradation, is beyond 94.6\% in all cases. This clearly demonstrates the potential applicability of the proposed method for multiple fault diagnosis tasks of HST bogie system.},
  archive      = {J_TNNLS},
  author       = {Na Qin and Kaiwei Liang and Deqing Huang and Lei Ma and Andrew H. Kemp},
  doi          = {10.1109/TNNLS.2020.2966744},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5363-5376},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiple convolutional recurrent neural networks for fault identification and performance degradation evaluation of high-speed train bogie},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Why ResNet works? Residuals generalize. <em>TNNLS</em>,
<em>31</em>(12), 5349–5362. (<a
href="https://doi.org/10.1109/TNNLS.2020.2966319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residual connections significantly boost the performance of deep neural networks. However, few theoretical results address the influence of residuals on the hypothesis complexity and the generalization ability of deep neural networks. This article studies the influence of residual connections on the hypothesis complexity of the neural network in terms of the covering number of its hypothesis space. We first present an upper bound of the covering number of networks with residual connections. This bound shares a similar structure with that of neural networks without residual connections. This result suggests that moving a weight matrix or nonlinear activation from the bone to a vine would not increase the hypothesis space. Afterward, an O(1 / √N) margin-based multiclass generalization bound is obtained for ResNet, as an exemplary case of any deep neural network with residual connections. Generalization guarantees for similar state-of-the-art neural network architectures, such as DenseNet and ResNeXt, are straightforward. According to the obtained generalization bound, we should introduce regularization terms to control the magnitude of the norms of weight matrices not to increase too much, in practice, to ensure a good generalization ability, which justifies the technique of weight decay.},
  archive      = {J_TNNLS},
  author       = {Fengxiang He and Tongliang Liu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2020.2966319},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5349-5362},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Why ResNet works? residuals generalize},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and comprehensive analysis of a noise-tolerant ZNN
model with limited-time convergence for time-dependent nonlinear
minimization. <em>TNNLS</em>, <em>31</em>(12), 5339–5348. (<a
href="https://doi.org/10.1109/TNNLS.2020.2966294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zeroing neural network (ZNN) is a powerful tool to address the mathematical and optimization problems broadly arisen in the science and engineering areas. The convergence and robustness are always co-pursued in ZNN. However, there exists no related work on the ZNN for time-dependent nonlinear minimization that achieves simultaneously limited-time convergence and inherently noise suppression. In this article, for the purpose of satisfying such two requirements, a limited-time robust neural network (LTRNN) is devised and presented to solve time-dependent nonlinear minimization under various external disturbances. Different from the previous ZNN model for this problem either with limited-time convergence or with noise suppression, the proposed LTRNN model simultaneously possesses such two characteristics. Besides, rigorous theoretical analyses are given to prove the superior performance of the LTRNN model when adopted to solve time-dependent nonlinear minimization under external disturbances. Comparative results also substantiate the effectiveness and advantages of LTRNN via solving a time-dependent nonlinear minimization problem.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Jianhua Dai and Rongbo Lu and Shuai Li and Jichun Li and Shoujin Wang},
  doi          = {10.1109/TNNLS.2020.2966294},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5339-5348},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design and comprehensive analysis of a noise-tolerant ZNN model with limited-time convergence for time-dependent nonlinear minimization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid deep learning-gaussian process network for pedestrian
lane detection in unstructured scenes. <em>TNNLS</em>, <em>31</em>(12),
5324–5338. (<a
href="https://doi.org/10.1109/TNNLS.2020.2966246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian lane detection is an important task in many assistive and autonomous navigation systems. This article presents a new approach for pedestrian lane detection in unstructured environments, where the pedestrian lanes can have arbitrary surfaces with no painted markers. In this approach, a hybrid deep learning-Gaussian process (DL-GP) network is proposed to segment a scene image into lane and background regions. The network combines a compact convolutional encoder-decoder net and a powerful nonparametric hierarchical GP classifier. The resulting network with a smaller number of trainable parameters helps mitigate the overfitting problem while maintaining the modeling power. In addition to the segmentation output for each test image, the network also generates a map of uncertainty-a measure that is negatively correlated with the confidence level with which we can trust the segmentation. This measure is important for pedestrian lane-detection applications, since its prediction affects the safety of its users. We also introduce a new data set of 5000 images for training and evaluating the pedestrian lane-detection algorithms. This data set is expected to facilitate research in pedestrian lane detection, especially the application of DL in this area. Evaluated on this data set, the proposed network shows significant performance improvements compared with several existing methods.},
  archive      = {J_TNNLS},
  author       = {Thi Nhat Anh Nguyen and Son Lam Phung and Abdesselam Bouzerdoum},
  doi          = {10.1109/TNNLS.2020.2966246},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5324-5338},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid deep learning-gaussian process network for pedestrian lane detection in unstructured scenes},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed complementary binary quantization for joint hash
table learning. <em>TNNLS</em>, <em>31</em>(12), 5312–5323. (<a
href="https://doi.org/10.1109/TNNLS.2020.2965992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building multiple hash tables serves as a very successful technique for gigantic data indexing, which can simultaneously guarantee both the search accuracy and efficiency. However, most of existing multitable indexing solutions, without informative hash codes and strong table complementarity, largely suffer from the table redundancy. To address the problem, we propose a complementary binary quantization (CBQ) method for jointly learning multiple tables and the corresponding informative hash functions in a centralized way. Based on CBQ, we further design a distributed learning algorithm (D-CBQ) to accelerate the training over the large-scale distributed data set. The proposed (D-)CBQ exploits the power of prototype-based incomplete binary coding to well align the data distributions in the original space and the Hamming space and further utilizes the nature of multi-index search to jointly reduce the quantization loss. (D-)CBQ possesses several attractive properties, including the extensibility for generating long hash codes in the product space and the scalability with linear training time. Extensive experiments on two popular large-scale tasks, including the Euclidean and semantic nearest neighbor search, demonstrate that the proposed (D-)CBQ enjoys efficient computation, informative binary quantization, and strong table complementarity, which together help significantly outperform the state of the arts, with up to 57.76\% performance gains relatively.},
  archive      = {J_TNNLS},
  author       = {Xianglong Liu and Qiang Fu and Deqing Wang and Xiao Bai and Xinyu Wu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2020.2965992},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5312-5323},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed complementary binary quantization for joint hash table learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised AER object recognition based on multiscale
spatio-temporal features and spiking neurons. <em>TNNLS</em>,
<em>31</em>(12), 5300–5311. (<a
href="https://doi.org/10.1109/TNNLS.2020.2966058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an unsupervised address event representation (AER) object recognition approach. The proposed approach consists of a novel multiscale spatio-temporal feature (MuST) representation of input AER events and a spiking neural network (SNN) using spike-timing-dependent plasticity (STDP) for object recognition with MuST. MuST extracts the features contained in both the spatial and temporal information of AER event flow, and forms an informative and compact feature spike representation. We show not only how MuST exploits spikes to convey information more effectively, but also how it benefits the recognition using SNN. The recognition process is performed in an unsupervised manner, which does not need to specify the desired status of every single neuron of SNN, and thus can be flexibly applied in real-world recognition tasks. The experiments are performed on five AER datasets including a new one named GESTURE-DVS. Extensive experimental results show the effectiveness and advantages of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Qianhui Liu and Gang Pan and Haibo Ruan and Dong Xing and Qi Xu and Huajin Tang},
  doi          = {10.1109/TNNLS.2020.2966058},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5300-5311},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised AER object recognition based on multiscale spatio-temporal features and spiking neurons},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Circular complex-valued GMDH-type neural network for
real-valued classification problems. <em>TNNLS</em>, <em>31</em>(12),
5285–5299. (<a
href="https://doi.org/10.1109/TNNLS.2020.2966031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, applications of complex-valued neural networks (CVNNs) to real-valued classification problems have attracted significant attention. However, most existing CVNNs are black-box models with poor explanation performance. This study extends the real-valued group method of data handling (RGMDH)-type neural network to the complex field and constructs a circular complex-valued group method of data handling (C-CGMDH)-type neural network, which is a white-box model. First, a complex least squares method is proposed for parameter estimation. Second, a new complex-valued symmetric regularity criterion is constructed with a logarithmic function to represent explicitly the magnitude and phase of the actual and predicted complex output to evaluate and select the middle candidate models. Furthermore, the property of this new complex-valued external criterion is proven to be similar to that of the real external criterion. Before training this model, a circular transformation is used to transform the real-valued input features to the complex field. Twenty-five real-valued classification data sets from the UCI Machine Learning Repository are used to conduct the experiments. The results show that both RGMDH and C-CGMDH models can select the most important features from the complete feature space through a self-organizing modeling process. Compared with RGMDH, the C-CGMDH model converges faster and selects fewer features. Furthermore, its classification performance is statistically significantly better than the benchmark complex-valued and real-valued models. Regarding time complexity, the C-CGMDH model is comparable with other models in dealing with the data sets that have few features. Finally, we demonstrate that the GMDH-type neural network can be interpretable.},
  archive      = {J_TNNLS},
  author       = {Jin Xiao and Yanlin Jia and Xiaoyi Jiang and Shouyang Wang},
  doi          = {10.1109/TNNLS.2020.2966031},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5285-5299},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Circular complex-valued GMDH-type neural network for real-valued classification problems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An accelerated finite-time convergent neural network for
visual servoing of a flexible surgical endoscope with physical and RCM
constraints. <em>TNNLS</em>, <em>31</em>(12), 5272–5284. (<a
href="https://doi.org/10.1109/TNNLS.2020.2965553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article designs and analyzes a recurrent neural network (RNN) for the visual servoing of a flexible surgical endoscope. The flexible surgical endoscope is based on a commercially available UR5 robot with a flexible endoscope attached as an end-effector. Most of the existing visual servo control frameworks of the robotic endoscopes or robot arms have not considered either the physical limits of the robot or the remote center of motion (RCM) constraints (i.e., the fulcrum effect). To tackle this issue, this article first conducts the kinematic modeling of the flexible robotic endoscope to achieve automation by visual servo control. The kinematic modeling results in a quadratic programming (QP) framework with physical limits and RCM constraints involved, making the UR5 robot applicable to surgical field. To solve the QP problem and accomplish the visual task, an RNN activated by a sign-bi-power activation function (AF) is proposed. The motivation of using the sign-bi-power AF is to enable the RNN to exhibit an accelerated finite-time convergence, which is more preferred in time-critical applications. Theoretically, the finite-time convergence of the RNN is rigorously proved using the Lyapunov theory. Compared with the previous AFs applied to the RNN, theoretical analysis shows that the RNN activated by the sign-bi-power AF delivers an accelerated convergence speed. Comparative validations are performed, showing that the proposed finite-time convergent neural network is effective to achieve visual servoing of the flexible endoscope with physical limits and RCM constraints handled simultaneously.},
  archive      = {J_TNNLS},
  author       = {Weibing Li and Philip Wai Yan Chiu and Zheng Li},
  doi          = {10.1109/TNNLS.2020.2965553},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5272-5284},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An accelerated finite-time convergent neural network for visual servoing of a flexible surgical endoscope with physical and RCM constraints},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A brain-inspired framework for evolutionary artificial
general intelligence. <em>TNNLS</em>, <em>31</em>(12), 5257–5271. (<a
href="https://doi.org/10.1109/TNNLS.2020.2965567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the medical field to agriculture, from energy to transportation, every industry is going through a revolution by embracing artificial intelligence (AI); nevertheless, AI is still in its infancy. Inspired by the evolution of the human brain, this article demonstrates a novel method and framework to synthesize an artificial brain with cognitive abilities by taking advantage of the same process responsible for the growth of the biological brain called “neuroembryogenesis.” This framework shares some of the key behavioral aspects of the biological brain, such as spiking neurons, neuroplasticity, neuronal pruning, and excitatory and inhibitory interactions between neurons, together making it capable of learning and memorizing. One of the highlights of the proposed design is its potential to incrementally improve itself over generations based on system performance, using genetic algorithms. A proof of concept at the end of this article demonstrates how a simplified implementation of the human visual cortex using the proposed framework is capable of character recognition. Our framework is open source, and the code is shared with the scientific community at http://www.feagi.org.},
  archive      = {J_TNNLS},
  author       = {Mohammad Nadji-Tehrani and Ali Eslami},
  doi          = {10.1109/TNNLS.2020.2965567},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5257-5271},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A brain-inspired framework for evolutionary artificial general intelligence},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal elevator group control via deep asynchronous
actor–critic learning. <em>TNNLS</em>, <em>31</em>(12), 5245–5256. (<a
href="https://doi.org/10.1109/TNNLS.2020.2965208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new deep reinforcement learning (RL) method, called asynchronous advantage actor-critic (A3C) method, is developed to solve the optimal control problem of elevator group control systems (EGCSs). The main contribution of this article is that the optimal control law of EGCSs is designed via a new deep RL method, such that the elevator system sends passengers to the desired destination floors as soon as possible. Deep convolutional and recurrent neural networks, which can update themselves during applications, are designed to dispatch elevators. Then, the structure of the A3C method is developed, and the training phase for the learning optimal law is discussed. Finally, simulation results illustrate that the developed method effectively reduces the average waiting time in a complex building environment. Comparisons with traditional algorithms further verify the effectiveness of the developed method.},
  archive      = {J_TNNLS},
  author       = {Qinglai Wei and Lingxiao Wang and Yu Liu and Marios M. Polycarpou},
  doi          = {10.1109/TNNLS.2020.2965208},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5245-5256},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal elevator group control via deep asynchronous Actor–Critic learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recent advances on dynamical behaviors of coupled neural
networks with and without reaction–diffusion terms. <em>TNNLS</em>,
<em>31</em>(12), 5231–5244. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the dynamical behaviors of coupled neural networks (CNNs) with and without reaction-diffusion terms have been widely researched due to their successful applications in different fields. This article introduces some important and interesting results on this topic. First, synchronization, passivity, and stability analysis results for various CNNs with and without reaction-diffusion terms are summarized, including the results for impulsive, time-varying, time-invariant, uncertain, fuzzy, and stochastic network models. In addition, some control methods, such as sampled-data control, pinning control, impulsive control, state feedback control, and adaptive control, have been used to realize the desired dynamical behaviors in CNNs with and without reaction-diffusion terms. In this article, these methods are summarized. Finally, some challenging and interesting problems deserving of further investigation are discussed.},
  archive      = {J_TNNLS},
  author       = {Jin-Liang Wang and Shui-Han Qiu and Wei-Zhong Chen and Huai-Ning Wu and Tingwen Huang},
  doi          = {10.1109/TNNLS.2020.2964843},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5231-5244},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recent advances on dynamical behaviors of coupled neural networks with and without Reaction–Diffusion terms},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning salient and discriminative descriptor for palmprint
feature extraction and identification. <em>TNNLS</em>, <em>31</em>(12),
5219–5230. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint recognition has been widely applied in security and, particularly, authentication. In the past decade, various palmprint recognition methods have been proposed and achieved promising recognition performance. However, most of these methods require rich a priori knowledge and cannot adapt well to different palmprint recognition scenarios, including contact-based, contactless, and multispectral palmprint recognition. This problem limits the application and popularization of palmprint recognition. In this article, motivated by the least square regression, we propose a salient and discriminative descriptor learning method (SDDLM) for general scenario palmprint recognition. Different from the conventional palmprint feature extraction methods, the SDDLM jointly learns noise and salient information from the pixels of palmprint images, simultaneously. The learned noise enforces the projection matrix to learn salient and discriminative features from each palmprint sample. Thus, the SDDLM can be adaptive to multiscenarios. Experiments were conducted on the IITD, CASIA, GPDS, PolyU near infrared (NIR), noisy IITD, and noisy GPDS palmprint databases, and palm vein and dorsal hand vein databases. It can be seen from the experimental results that the proposed SDDLM consistently outperformed the classical palmprint recognition methods and state-of-the-art methods for palmprint recognition.},
  archive      = {J_TNNLS},
  author       = {Shuping Zhao and Bob Zhang},
  doi          = {10.1109/TNNLS.2020.2964799},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5219-5230},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning salient and discriminative descriptor for palmprint feature extraction and identification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subspace distribution adaptation frameworks for domain
adaptation. <em>TNNLS</em>, <em>31</em>(12), 5204–5218. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation tries to adapt a model trained from a source domain to a different but related target domain. Currently, prevailing methods for domain adaptation rely on either instance reweighting or feature transformation. Unfortunately, instance reweighting has difficulty in estimating the sample weights as the dimension increases, whereas feature transformation sometimes fails to make the transformed source and target distributions similar when the cross-domain discrepancy is large. In order to overcome the shortcomings of both methodologies, in this article, we model the unsupervised domain adaptation problem under the generalized covariate shift assumption and adapt the source distribution to the target distribution in a subspace by applying a distribution adaptation function. Accordingly, we propose two frameworks: Bregman-divergence-embedded structural risk minimization (BSRM) and joint structural risk minimization (JSRM). In the proposed frameworks, the subspace distribution adaptation function and the target prediction model are jointly learned. Under certain instantiations, convex optimization problems are derived from both frameworks. Experimental results on the synthetic and real-world text and image data sets show that the proposed methods outperform the state-of-the-art domain adaptation techniques with statistical significance.},
  archive      = {J_TNNLS},
  author       = {Sentao Chen and Le Han and Xiaolan Liu and Zongyao He and Xiaowei Yang},
  doi          = {10.1109/TNNLS.2020.2964790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5204-5218},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Subspace distribution adaptation frameworks for domain adaptation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiclass oblique random forests with dual-incremental
learning capacity. <em>TNNLS</em>, <em>31</em>(12), 5192–5203. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oblique random forests (ObRFs) have attracted increasing attention recently. Their popularity is mainly driven by learning oblique hyperplanes instead of expensively searching for axis-aligned hyperplanes in the standard random forest. However, most existing methods are trained in an off-line mode, which assumes that the training data are given as a batch. Efficient dual-incremental learning (DIL) strategies for ObRF have rarely been explored when new inputs from the existing classes or unseen classes come. The goal of this article is to provide an ObRF with DIL capacity to perform classification on-the-fly. First, we propose a batch multiclass ObRF (ObRF-BM) algorithm by using a broad learning system and a multi-to-binary method to obtain an optimal oblique hyperplane in a higher dimensional space and then separate the samples into two supervised clusters at each node, which provides the basis for the following incremental learning strategy. Then, the DIL strategy for ObRF-BM, termed ObRF-DIL, is developed by analytically updating the parameters of all nodes on the classification route of the increment of input samples and the increment of input classes so that the ObRF-BM model can be effectively updated without laborious retraining from scratch. Experimental results using several public data sets demonstrate the superiority of the proposed approach in comparison with several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zheng Chai and Chunhui Zhao},
  doi          = {10.1109/TNNLS.2020.2964737},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5192-5203},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiclass oblique random forests with dual-incremental learning capacity},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Entropy and confidence-based undersampling boosting random
forests for imbalanced problems. <em>TNNLS</em>, <em>31</em>(12),
5178–5191. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel entropy and confidence-based undersampling boosting (ECUBoost) framework to solve imbalanced problems. The boosting-based ensemble is combined with a new undersampling method to improve the generalization performance. To avoid losing informative samples during the data preprocessing of the boosting-based ensemble, both confidence and entropy are used in ECUBoost as benchmarks to ensure the validity and structural distribution of the majority samples during the undersampling. Furthermore, different from other iterative dynamic resampling methods, ECUBoost based on confidence can be applied to algorithms without iterations such as decision trees. Meanwhile, random forests are used as base classifiers in ECUBoost. Furthermore, experimental results on both artificial data sets and KEEL data sets prove the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Zhe Wang and Chenjie Cao and Yujin Zhu},
  doi          = {10.1109/TNNLS.2020.2964585},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5178-5191},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Entropy and confidence-based undersampling boosting random forests for imbalanced problems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural network backstepping control of
fractional-order nonlinear systems with actuator faults. <em>TNNLS</em>,
<em>31</em>(12), 5166–5177. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backstepping control for fractional-order nonlinear systems (FONSs) requires the analytic calculation of fractional derivatives of certain complicated stabilizing functions, which becomes prohibitive as the order of the system increases. This article aims to facilitate the adaptive neural network (NN) backstepping control design for FONSs with actuator faults whose parameters and patterns are fully unknown. A fractional filtering approach, which obviates the requirement of analytic fractional differentiation, is used to generate command signals together with their fractional derivatives. Compensated tracking errors that can eliminate approximation errors of command signals are generated by fractional filters. The proposed adaptive NN command filtered backstepping control (ANNCFBC) approach, together with fractional adaptive laws, guarantees not only the boundedness of all involved variables but also the convergence of both the tracking error and the compensated tracking error to a sufficiently small region. Finally, simulation studies are given to indicate the effectiveness of the proposed control method.},
  archive      = {J_TNNLS},
  author       = {Heng Liu and Yongping Pan and Jinde Cao and Hongxing Wang and Yan Zhou},
  doi          = {10.1109/TNNLS.2020.2964044},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5166-5177},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network backstepping control of fractional-order nonlinear systems with actuator faults},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bi-stream pose-guided region ensemble network for fingertip
localization from stereo images. <em>TNNLS</em>, <em>31</em>(12),
5153–5165. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human-computer interaction, it is important to accurately estimate the hand pose, especially fingertips. However, traditional approaches to fingertip localization mainly rely on depth images and thus suffer considerably from noise and missing values. Instead of depth images, stereo images can also provide 3-D information of hands. There are nevertheless limitations on the dataset size, global viewpoints, hand articulations, and hand shapes in publicly available stereo-based hand pose datasets. To mitigate these limitations and promote further research on hand pose estimation from stereo images, we build a new large-scale binocular hand pose dataset called THU-Bi-Hand, offering a new perspective for fingertip localization. In the THU-Bi-Hand dataset, there are 447k pairs of stereo images of different hand shapes from ten subjects with accurate 3-D location annotations of the wrist and five fingertips. Captured with minimal restriction on the range of hand motion, the dataset covers a large global viewpoint space and hand articulation space. To better present the performance of fingertip localization on THU-Bi-Hand, we propose a novel scheme termed bi-stream pose-guided region ensemble network (Bi-Pose-REN). It extracts more representative feature regions around joints in the feature maps under the guidance of the previously estimated pose. The feature regions are integrated hierarchically according to the topology of hand joints to regress a refined hand pose. Bi-Pose-REN and several existing methods are evaluated on THU-Bi-Hand so that benchmarks are provided for further research. Experimental results show that our Bi-Pose-REN has achieved the best performance on THU-Bi-Hand.},
  archive      = {J_TNNLS},
  author       = {Guijin Wang and Cairong Zhang and Xinghao Chen and Xiangyang Ji and Jing-Hao Xue and Hang Wang},
  doi          = {10.1109/TNNLS.2020.2964037},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5153-5165},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bi-stream pose-guided region ensemble network for fingertip localization from stereo images},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monostability and multistability for almost-periodic
solutions of fractional-order neural networks with unsaturating
piecewise linear activation functions. <em>TNNLS</em>, <em>31</em>(12),
5138–5152. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the unsaturating activation function is unbounded, more complex dynamics may exist in neural networks with this kind of activation function. In this article, monostability and multistability results of almost-periodic solutions are developed for fractional-order neural networks with unsaturating piecewise linear activation functions. Some globally Mittag-Leffler attractive sets are given, and the existence of globally Mittag-Leffler stable almost-periodic solution is demonstrated by using Ascoli–Arzela theorem. In particular, some sufficient conditions are provided to ascertain the multistability of almost-periodic solutions based on locally positively invariant set. It shows that there exists an almost-periodic solution in each positively invariant set, and all trajectories converge to this periodic trajectory in that rectangular area. Two illustrative examples are provided to demonstrate the effectiveness of the proposed sufficient criteria.},
  archive      = {J_TNNLS},
  author       = {Peng Wan and Dihua Sun and Min Zhao and Hang Zhao},
  doi          = {10.1109/TNNLS.2020.2964030},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5138-5152},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Monostability and multistability for almost-periodic solutions of fractional-order neural networks with unsaturating piecewise linear activation functions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel connectivity-preserving control design for
rendezvous problem of networked uncertain nonlinear systems.
<em>TNNLS</em>, <em>31</em>(12), 5127–5137. (<a
href="https://doi.org/10.1109/TNNLS.2020.2964017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel and robust connectivity-preserving rendezvous control design for a group of uncertain nonlinear multiagent systems with communication constraint where each agent has a limited sensing range. The control design can work under the assumption that the communication network is initially connected and is characterized by two distinguishing features. First, a new potential function is provided not only to maintain the existing and newly added links by the hysteresis rule but also to overcome the difficulty imposed by the nonlinear terms from system dynamics. Second, by constructing a series of lemmas, a connectivity-preserving stabilizing control law is presented to solve the robust stabilization problem with connectivity preservation for a time-varying nonlinear system, which is a special case of the augmented system with both dynamic and static uncertainties obtained via internal model design. After further incorporating the adaptive control technique, regardless of uncertain parameters and external disturbances in the multiple nonlinear subsystems, the leader-following rendezvous with connectivity preservation problem is finally solved by a distributed connectivity-preserving controller with parameter update law.},
  archive      = {J_TNNLS},
  author       = {Yi Dong and Shengyuan Xu},
  doi          = {10.1109/TNNLS.2020.2964017},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5127-5137},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel connectivity-preserving control design for rendezvous problem of networked uncertain nonlinear systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RNN for perturbed manipulability optimization of
manipulators based on a distributed scheme: A game-theoretic
perspective. <em>TNNLS</em>, <em>31</em>(12), 5116–5126. (<a
href="https://doi.org/10.1109/TNNLS.2020.2963998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to leverage the unique advantages of redundant manipulators, avoiding the singularity during motion planning and control should be considered as a fundamental issue to handle. In this article, a distributed scheme is proposed to improve the manipulability of redundant manipulators in a group. To this end, the manipulability index is incorporated into the cooperative control of multiple manipulators in a distributed network, which is used to guide manipulators to adjust to the optimal spatial position. Moreover, from the perspective of game theory, this article formulates the problem into a Nash equilibrium. Then, a neural network with anti-noise ability is constructed to seek and approximate the optimal strategy profile of the Nash equilibrium problem with time-varying parameters. Theoretical analyses show that the neural network model has the superior global convergence and noise immunity. Finally, simulation results demonstrate that the neural network is effective in real-time cooperative motion generation of multiple redundant manipulators under perturbations in distributed networks.},
  archive      = {J_TNNLS},
  author       = {Jiazheng Zhang and Long Jin and Long Cheng},
  doi          = {10.1109/TNNLS.2020.2963998},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5116-5126},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RNN for perturbed manipulability optimization of manipulators based on a distributed scheme: A game-theoretic perspective},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MASK-RL: Multiagent video object segmentation framework
through reinforcement learning. <em>TNNLS</em>, <em>31</em>(12),
5103–5115. (<a
href="https://doi.org/10.1109/TNNLS.2019.2963282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating human-provided location priors into video object segmentation has been shown to be an effective strategy to enhance performance, but their application at large scale is unfeasible. Gamification can help reduce the annotation burden, but it still requires user involvement. We propose a video object segmentation framework that leverages the combined advantages of user feedback for segmentation and gamification strategy by simulating multiple game players through a reinforcement learning (RL) model that reproduces human ability to pinpoint moving objects and using the simulated feedback to drive the decisions of a fully convolutional deep segmentation network. Experimental results on the DAVIS-17 benchmark show that: 1) including user-provided prior, even if not precise, yields high performance; 2) our RL agent replicates satisfactorily the same variability of humans in identifying spatiotemporal salient objects; and 3) employing artificially generated priors in an unsupervised video object segmentation model reaches state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Giuseppe Vecchio and Simone Palazzo and Daniela Giordano and Francesco Rundo and Concetto Spampinato},
  doi          = {10.1109/TNNLS.2019.2963282},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5103-5115},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MASK-RL: Multiagent video object segmentation framework through reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization of delayed neural networks via
integral-based event-triggered scheme. <em>TNNLS</em>, <em>31</em>(12),
5092–5102. (<a
href="https://doi.org/10.1109/TNNLS.2019.2963146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the event-triggered synchronization of delayed neural networks (NNs). A novel integral-based event-triggered scheme (IETS) is proposed where the integral of the system states, and past triggered data over a period of time are used. With the proposed IETS, the integral event-triggered synchronization problem becomes a distributed delay problem. Using the Bessel-Legendre inequalities, sufficient conditions for the existence of a controller that ensures asymptotic synchronization are provided in the form of linear matrix inequalities (LMIs). Illustrative examples are used to demonstrate the advantages of the proposed IETS method over other event-triggered scheme (ETS) methods. Moreover, this IETS method is applied to the image encryption and decryption. A novel encryption algorithm is proposed to enhance the quality of the encryption process.},
  archive      = {J_TNNLS},
  author       = {Liruo Zhang and Sing Kiong Nguang and Deqiang Ouyang and Shen Yan},
  doi          = {10.1109/TNNLS.2019.2963146},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5092-5102},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of delayed neural networks via integral-based event-triggered scheme},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PID controller-based stochastic optimization acceleration
for deep neural networks. <em>TNNLS</em>, <em>31</em>(12), 5079–5091.
(<a href="https://doi.org/10.1109/TNNLS.2019.2963066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are widely used and demonstrated their power in many applications, such as computer vision and pattern recognition. However, the training of these networks can be time consuming. Such a problem could be alleviated by using efficient optimizers. As one of the most commonly used optimizers, stochastic gradient descent-momentum (SGD-M) uses past and present gradients for parameter updates. However, in the process of network training, SGD-M may encounter some drawbacks, such as the overshoot phenomenon. This problem would slow the training convergence. To alleviate this problem and accelerate the convergence of DNN optimization, we propose a proportional-integral-derivative (PID) approach. Specifically, we investigate the intrinsic relationships between the PID-based controller and SGD-M first. We further propose a PID-based optimization algorithm to update the network parameters, where the past, current, and change of gradients are exploited. Consequently, our proposed PID-based optimization alleviates the overshoot problem suffered by SGD-M. When tested on popular DNN architectures, it also obtains up to 50\% acceleration with competitive accuracy. Extensive experiments about computer vision and natural language processing demonstrate the effectiveness of our method on benchmark data sets, including CIFAR10, CIFAR100, Tiny-ImageNet, and PTB. We have released the code at https://github.com/tensorboy/PIDOptimizer .},
  archive      = {J_TNNLS},
  author       = {Haoqian Wang and Yi Luo and Wangpeng An and Qingyun Sun and Jun Xu and Lei Zhang},
  doi          = {10.1109/TNNLS.2019.2963066},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5079-5091},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PID controller-based stochastic optimization acceleration for deep neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning multiple parameters for kernel collaborative
representation classification. <em>TNNLS</em>, <em>31</em>(12),
5068–5078. (<a
href="https://doi.org/10.1109/TNNLS.2019.2962878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the problem of automatically learning multiple parameters for kernel collaborative representation classification (KCRC) is considered. We investigate the KCRC and measure its generalization error via leave-one-out cross-validation (LOO-CV). By taking advantage of the specific properties of KCRC, a closed-form expression is derived for the outputs of LOO-CV. Then, a simple classification rule that provides probabilistic outputs is adopted, and thereby, an effective loss function that is an explicit function with respect to the parameters is proposed as the generalization error. The gradients of the loss function are calculated, and the parameters are learned by minimizing the loss function using a gradient-based optimization algorithm. Furthermore, the proposed approach makes it possible to solve the multiple kernel/feature learning problems of KCRC effectively. Experiment results on six data sets taken from different scenes demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Jianjun Liu and Zebin Wu and Liang Xiao and Hong Yan},
  doi          = {10.1109/TNNLS.2019.2962878},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5068-5078},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning multiple parameters for kernel collaborative representation classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DACH: Domain adaptation without domain information.
<em>TNNLS</em>, <em>31</em>(12), 5055–5067. (<a
href="https://doi.org/10.1109/TNNLS.2019.2962817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is becoming increasingly important for learning systems in recent years, especially with the growing diversification of data domains in real-world applications, such as the genetic data from various sequencing platforms and video feeds from multiple surveillance cameras. Traditional domain adaptation approaches target to design transformations for each individual domain so that the twisted data from different domains follow an almost identical distribution. In many applications, however, the data from diversified domains are simply dumped to an archive even without clear domain labels. In this article, we discuss the possibility of learning domain adaptations even when the data does not contain domain labels. Our solution is based on our new model, named domain adaption using cross-domain homomorphism (DACH in short), to identify intrinsic homomorphism hidden in mixed data from all domains. DACH is generally compatible with existing deep learning frameworks, enabling the generation of nonlinear features from the original data domains. Our theoretical analysis not only shows the universality of the homomorphism, but also proves the convergence of DACH for significant homomorphism structures over the data domains is preserved. Empirical studies on real-world data sets validate the effectiveness of DACH on merging multiple data domains for joint machine learning tasks and the scalability of our algorithm to domain dimensionality.},
  archive      = {J_TNNLS},
  author       = {Ruichu Cai and Jiahao Li and Zhenjie Zhang and Xiaoyan Yang and Zhifeng Hao},
  doi          = {10.1109/TNNLS.2019.2962817},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5055-5067},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DACH: Domain adaptation without domain information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Weakly supervised complets ranking for deep image quality
modeling. <em>TNNLS</em>, <em>31</em>(12), 5041–5054. (<a
href="https://doi.org/10.1109/TNNLS.2019.2962548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the competitive prediction performance, recent deep image quality models suffer from the following limitations. First, it is deficiently effective to interpret and quantify the region-level quality, which contributes to global features during deep architecture training. Second, human visual perception is sensitive to compositional features (i.e., the sophisticated spatial configurations among regions), but explicitly incorporating them into a deep model is challenging. Third, the state-of-the-art deep quality models typically use rectangular image patches as inputs, but there is no evidence that these rectangles can reflect arbitrarily shaped objects, such as beaches and jungles. By defining the complet, which is a set of image segments collaboratively characterizing the spatial/geometric distribution of multiple visual elements, we propose a novel quality-modeling framework that involves two key modules: a complet ranking algorithm and a spatially-aware dual aggregation network (SDA-Net). Specifically, to describe the region-level quality features, we build complets to characterize the high-order spatial interactions among the arbitrarily shaped segments in each image. To obtain complets that are highly descriptive to image compositions, a weakly supervised complet ranking algorithm is designed by quantifying the quality of each complet. The algorithm seamlessly encodes three factors: the image-level quality discrimination, weakly supervised constraint, and complet geometry of each image. Based on the top-ranking complets, a novel multi-column convolutional neural network (CNN) called SDA-Net is designed, which supports input segments with arbitrary shapes. The key is a dual-aggregation mechanism that fuses the intracomplet deep features and the intercomplet deep features under a unified framework. Thorough experimental validations on a series of benchmark data sets demonstrated the superiority of our method.},
  archive      = {J_TNNLS},
  author       = {Luming Zhang and Mingliang Xu and Jianwei Yin and Chao Zhang and Ling Shao},
  doi          = {10.1109/TNNLS.2019.2962548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5041-5054},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weakly supervised complets ranking for deep image quality modeling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(11), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.3030510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.3030510},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Fixed-time leader–follower consensus of networked nonlinear
systems via event/self-triggered control. <em>TNNLS</em>,
<em>31</em>(11), 5029–5037. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief addresses the fixed-time event/self-triggered leader-follower consensus problems for networked multi-agent systems subject to nonlinear dynamics. First, we present an event-triggered control strategy to achieve the fixed-time consensus, and a new measurement error is designed to avoid Zeno behavior. Then, two new self-triggered control strategies are presented to avoid continuous triggering condition monitoring. Moreover, under the proposed self-triggered control strategies, a strictly positive minimal triggering interval of each follower is given to exclude Zeno behavior. Compared with the existing fixed-time event-triggered results, we propose two new self-triggered control strategies, and the nonlinear term is more general. Finally, the performances of the consensus tracking algorithms are illustrated by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Jian Liu and Yanling Zhang and Yao Yu and Changyin Sun},
  doi          = {10.1109/TNNLS.2019.2957069},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5029-5037},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time Leader–Follower consensus of networked nonlinear systems via Event/Self-triggered control},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiagent containment control with nonconvex states
constraints, nonuniform time delays, and switching directed networks.
<em>TNNLS</em>, <em>31</em>(11), 5021–5028. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief studies the constrained containment control problem of continuous-time multiagent systems with nonconvex states constraints, nonuniform time delays, and switching directed networks. A distributed containment control algorithm consisting of nonlinear projection, nonlinear constraints, and communication delays is proposed. Based on the model transformation technique, convex analysis method, and the Lyapunov function, we prove that all agents finally converge into the convex hull formed by the given stationary points with arbitrarily bounded communication delays and arbitrarily switching networks as long as each agent jointly has at least one path from the given points to itself. Finally, a numerical example is included to show the effectiveness of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Yi Huang and Mengmeng Duan and Lipo Mo},
  doi          = {10.1109/TNNLS.2019.2955678},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5021-5028},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiagent containment control with nonconvex states constraints, nonuniform time delays, and switching directed networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing graph-based semisupervised learning via
knowledge-aware data embedding. <em>TNNLS</em>, <em>31</em>(11),
5014–5020. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised learning (SSL) is a family of classification methods conceived to reduce the amount of required labeled information in the training phase. Graph-based methods are among the most popular semisupervised strategies: the nearest neighbor graph is built in such a way that the manifold of the data is captured and the labeled information is propagated to target samples along the structure of the manifold. Research in graph-based SSL has mainly focused on two aspects: 1) the construction of the k-nearest neighbors graph and/or 2) the propagation algorithm providing the classification. Differently from the previous literature, in this article, we focus on the data representation with the aim of incorporating semisupervision earlier in the process. To this end, we propose an algorithm that learns a new knowledge-aware data embedding via an ensemble of semisupervised autoencoders to enhance a graph-based semisupervised classification. The experiments carried out on different classification tasks demonstrate the benefit of our approach.},
  archive      = {J_TNNLS},
  author       = {Dino Ienco and Ruggero G. Pensa},
  doi          = {10.1109/TNNLS.2019.2955565},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5014-5020},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhancing graph-based semisupervised learning via knowledge-aware data embedding},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Six-DOF spacecraft optimal trajectory planning and real-time
attitude control: A deep neural network-based approach. <em>TNNLS</em>,
<em>31</em>(11), 5005–5013. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief presents an integrated trajectory planning and attitude control framework for six-degree-of-freedom (6-DOF) hypersonic vehicle (HV) reentry flight. The proposed framework utilizes a bilevel structure incorporating desensitized trajectory optimization and deep neural network (DNN)-based control. In the upper level, a trajectory data set containing optimal system control and state trajectories is generated, while in the lower level control system, DNNs are constructed and trained using the pregenerated trajectory ensemble in order to represent the functional relationship between the optimized system states and controls. These well-trained networks are then used to produce optimal feedback actions online. A detailed simulation analysis was performed to validate the real-time applicability and the optimality of the designed bilevel framework. Moreover, a comparative analysis was also carried out between the proposed DNN-driven controller and other optimization-based techniques existing in related works. Our results verify the reliability of using the proposed bilevel design for the control of HV reentry flight in real time.},
  archive      = {J_TNNLS},
  author       = {Runqi Chai and Antonios Tsourdos and Al Savvaris and Senchun Chai and Yuanqing Xia and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2019.2955400},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5005-5013},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Six-DOF spacecraft optimal trajectory planning and real-time attitude control: A deep neural network-based approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural cryptography based on complex-valued neural network.
<em>TNNLS</em>, <em>31</em>(11), 4999–5004. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural cryptography is a public key exchange algorithm based on the principle of neural network synchronization. By using the learning algorithm of a neural network, the two neural networks update their own weight through exchanging output from each other. Once the synchronization is completed, the weights of the two neural networks are the same. The weights of the neural network can be used for the secret key. However, all the existing works are based on the real-valued neural network model. There are seldom works studying the neural cryptography based on a complex-valued neural network model. In this technical note, a neural cryptography based on the complex-valued tree parity machine network (CVTPM) is proposed. The input, output, and weights of CVTPM are a complex value, which can be considered as an extension of TPM. There are two advantages of the CVTPM: 1) the security of CVTPM is higher than that of TPM with the same hidden units, input neurons, and synaptic depths and 2) the two parties with the CVTPM can exchange two group keys in one neural synchronization process. A series of numerical simulation experiments is provided to verify our results.},
  archive      = {J_TNNLS},
  author       = {Tao Dong and Tingwen Huang},
  doi          = {10.1109/TNNLS.2019.2955165},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4999-5004},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural cryptography based on complex-valued neural network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An exact reformulation of feature-vector-based
radial-basis-function networks for graph-based observations.
<em>TNNLS</em>, <em>31</em>(11), 4990–4998. (<a
href="https://doi.org/10.1109/TNNLS.2019.2953919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radial basis function (RBF) networks are traditionally defined for sets of vector-based observations. In this brief, we reformulate such networks so that they can be applied to adjacency-matrix representations of weighted, directed graphs that represent the relationships between object pairs. We restate the sum-of-squares objective function so that it is purely dependent on entries from the adjacency matrix. From this objective function, we derive a gradient descent update for the network weights. We also derive a gradient update that simulates the repositioning of the radial basis prototypes and changes in the radial basis prototype parameters. An important property of our radial basis function networks is that they are guaranteed to yield the same responses as conventional radial basis networks trained on a corresponding vector realization of the relationships encoded by the adjacency matrix. Such a vector realization only needs to provably exist for this property to hold, which occurs whenever the relationships correspond to distances from some arbitrary metric applied to a latent set of vectors. We, therefore, completely avoid needing to actually construct vectorial realizations via multidimensional scaling, which ensures that the underlying relationships are totally preserved.},
  archive      = {J_TNNLS},
  author       = {Isaac J. Sledge and José C. Príncipe},
  doi          = {10.1109/TNNLS.2019.2953919},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4990-4998},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An exact reformulation of feature-vector-based radial-basis-function networks for graph-based observations},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonfragile finite-time synchronization for coupled neural
networks with impulsive approach. <em>TNNLS</em>, <em>31</em>(11),
4980–4989. (<a
href="https://doi.org/10.1109/TNNLS.2020.3001196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of the average stochastic finite-time synchronization (ASFTS) for a set of coupled neural networks (NNs) with energy-bounded noises. Due to the channel capacity constraint, the impulsive approach is introduced so as to cut down the communication times among the leader NNs and the follower NNs. Then, a nonfragile controller is designed to improve the robustness of the controller with randomly occurred uncertainty. The sufficient conditions that guarantee the ASFTS of the coupled NNs and the leader NNs are achieved. The boundary of the synchronization error is also obtained by constructing the monotonic increasing functions. Finally, the controller gains are given based on the derived conditions, and their effectiveness is illustrated by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Hongxia Rao and Yuru Guo and Yong Xu and Chang Liu and Renquan Lu},
  doi          = {10.1109/TNNLS.2020.3001196},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4980-4989},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonfragile finite-time synchronization for coupled neural networks with impulsive approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new concept of multiple neural networks structure using
convex combination. <em>TNNLS</em>, <em>31</em>(11), 4968–4979. (<a
href="https://doi.org/10.1109/TNNLS.2019.2962020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new concept of convex-combined multiple neural networks (NNs) structure is proposed. This new approach uses the collective information from multiple NNs to train the model. Based on both theoretical and experimental analyses, the new approach is shown to achieve faster training convergence with a similar or even better test accuracy than a conventional NN structure. Two experiments are conducted to demonstrate the performance of our new structure: the first one is a semantic frame parsing task for spoken language understanding (SLU) on the Airline Travel Information System (ATIS) data set and the other is a handwritten digit recognition task on the Mixed National Institute of Standards and Technology (MNIST) data set. We test this new structure using both the recurrent NN and convolutional NNs through these two tasks. The results of both experiments demonstrate a 4×-8× faster training speed with better or similar performance by using this new concept.},
  archive      = {J_TNNLS},
  author       = {Yu Wang and Yue Deng and Yilin Shen and Hongxia Jin},
  doi          = {10.1109/TNNLS.2019.2962020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4968-4979},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new concept of multiple neural networks structure using convex combination},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic and finite-time cluster synchronization of
coupled fractional-order neural networks with time delay.
<em>TNNLS</em>, <em>31</em>(11), 4956–4967. (<a
href="https://doi.org/10.1109/TNNLS.2019.2962006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is devoted to the cluster synchronization issue of coupled fractional-order neural networks. By introducing the stability theory of fractional-order differential systems and the framework of Filippov regularization, some sufficient conditions are derived for ascertaining the asymptotic and finite-time cluster synchronization of coupled fractional-order neural networks, respectively. In addition, the upper bound of the settling time for finite-time cluster synchronization is estimated. Compared with the existing works, the results herein are applicable for fractional-order systems, which could be regarded as an extension of integer-order ones. A numerical example with different cases is presented to illustrate the validity of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Peng Liu and Zhigang Zeng and Jun Wang},
  doi          = {10.1109/TNNLS.2019.2962006},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4956-4967},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymptotic and finite-time cluster synchronization of coupled fractional-order neural networks with time delay},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiglobal consensus of a class of heterogeneous multi-agent
systems with saturation. <em>TNNLS</em>, <em>31</em>(11), 4946–4955. (<a
href="https://doi.org/10.1109/TNNLS.2019.2959804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the leader-following consensus of a class of multi-agent systems (MASs) subjected to saturation. Unlike previous literature, the followers are with heterogeneous dynamics. To solve this problem, we employ the low-gain feedback technique and the parameterized algebraic Riccati equations to design the controllers. For the fixed and switching network topologies, sufficient conditions are put in place to guarantee the semiglobal stability of the consensus error system. Numerical results are also provided to validate the effectiveness of the control design.},
  archive      = {J_TNNLS},
  author       = {Kexin Liu and Haibo Gu and Wei Wang and Jinhu Lü},
  doi          = {10.1109/TNNLS.2019.2959804},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4946-4955},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semiglobal consensus of a class of heterogeneous multi-agent systems with saturation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reducing estimation bias via triplet-average deep
deterministic policy gradient. <em>TNNLS</em>, <em>31</em>(11),
4933–4945. (<a
href="https://doi.org/10.1109/TNNLS.2019.2959129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The overestimation caused by function approximation is a well-known property in Q-learning algorithms, especially in single-critic models, which leads to poor performance in practical tasks. However, the opposite property, underestimation, which often occurs in Q-learning methods with double critics, has been largely left untouched. In this article, we investigate the underestimation phenomenon in the recent twin delay deep deterministic actor-critic algorithm and theoretically demonstrate its existence. We also observe that this underestimation bias does indeed hurt performance in various experiments. Considering the opposite properties of single-critic and double-critic methods, we propose a novel triplet-average deep deterministic policy gradient algorithm that takes the weighted action value of three target critics to reduce the estimation bias. Given the connection between estimation bias and approximation error, we suggest averaging previous target values to reduce per-update error and further improve performance. Extensive empirical results over various continuous control tasks in OpenAI gym show that our approach outperforms the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Dongming Wu and Xingping Dong and Jianbing Shen and Steven C. H. Hoi},
  doi          = {10.1109/TNNLS.2019.2959129},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4933-4945},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reducing estimation bias via triplet-average deep deterministic policy gradient},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite-time tracking control for a class of uncertain
strict-feedback nonlinear systems with state constraints: A smooth
control approach. <em>TNNLS</em>, <em>31</em>(11), 4920–4932. (<a
href="https://doi.org/10.1109/TNNLS.2019.2959016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the finite-time tracking control problem for a class of strict-feedback nonlinear systems involving state constraints, unknown nonlinearities, and nonvanishing disturbances. Unlike the literature that mainly focuses on a C 0 finite-time controller, in this article, a novel C 1 smooth finite-time adaptive neural network (NN) controller is proposed by employing a smooth switch between the fractional and cubic form state feedback. The proposed controller not only avoids the singularity but also makes it possible to implement the dynamic surface control (DSC) technique. By applying the adaptive NN control technique, together with barrier Lyapunov functions (BLFs) and a generalized first-order filter including both linear and fractional terms, the desired fast finite-time control performance of the closed-loop nonlinear systems can be guaranteed, and meanwhile, the state constraints are never violated. Under the proposed control scheme, the tracking control problems of nonlinear systems with output constraint and full-state constraints are, respectively, discussed. It is explicitly shown that all the internal error signals are driven to converge into small regions in a finite time. Finally, the effectiveness of the control scheme is also confirmed by the applications to the control of a second-order nonlinear system and an uncertain ship autopilot.},
  archive      = {J_TNNLS},
  author       = {Bing Cui and Yuanqing Xia and Kun Liu and Ganghui Shen},
  doi          = {10.1109/TNNLS.2019.2959016},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4920-4932},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time tracking control for a class of uncertain strict-feedback nonlinear systems with state constraints: A smooth control approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized event-triggered online adaptive control of
unknown large-scale systems over wireless communication networks.
<em>TNNLS</em>, <em>31</em>(11), 4907–4919. (<a
href="https://doi.org/10.1109/TNNLS.2019.2959005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel online decentralized event-triggered control scheme is proposed for a class of nonlinear interconnected large-scale systems subject to unknown internal system dynamics and interconnected terms. First, by designing a neural network-based identifier, the unknown internal dynamics of the interconnected systems is reconstructed. Then, the adaptive critic design method is used to learn the approximate optimal control policies in the context of event-triggered mechanism. Specifically, the event-based control processes of different subsystems are independent, asynchronous, and decentralized. That is, the decentralized event-triggering conditions and the controllers only rely on the local state information of the corresponding subsystems, which avoids the transmissions of the state information between the subsystems over the wireless communication networks. Then, with the help of Lyapunov&#39;s theorem, the states of the developed closed-loop control system and the critic weight estimation errors are proved to be uniformly ultimately bounded. Finally, the effectiveness and applicability of the event-based control method are verified by an illustrative numerical example and a practical example.},
  archive      = {J_TNNLS},
  author       = {Hanguang Su and Huaguang Zhang and Xiaodong Liang and Chong Liu},
  doi          = {10.1109/TNNLS.2019.2959005},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4907-4919},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized event-triggered online adaptive control of unknown large-scale systems over wireless communication networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-level game-based distributed optimal fault-tolerant
control for nonlinear interconnected systems. <em>TNNLS</em>,
<em>31</em>(11), 4892–4906. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the distributed optimal fault-tolerant control (FTC) issue by using the two-level game approach for a class of nonlinear interconnected systems, in which each subsystem couples with its neighbors through not only the states but also the inputs. At the first level, the FTC problem for each subsystem is formulated as a zero-sum differential game, in which the controller and the fault are regarded as two players with opposite interests. At the second level, the whole interconnected system is formulated as a graphical game, in which each subsystem is a player to achieve the global Nash equilibrium for the overall system. The rigorous proof of the stability of the interconnected system is given by means of the cyclic-small-gain theorem, and the relationship between the local optimality and the global optimality is analyzed. Moreover, based on the adaptive dynamic programming (ADP) technology, a distributed optimal FTC learning scheme is proposed, in which a group of critic neural networks (NNs) are established to approximate the cost functions. Finally, an example is taken to illustrate the efficiency and applicability of the obtained theoretical results.},
  archive      = {J_TNNLS},
  author       = {Yuhang Xu and Bin Jiang and Hao Yang},
  doi          = {10.1109/TNNLS.2019.2958948},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4892-4906},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two-level game-based distributed optimal fault-tolerant control for nonlinear interconnected systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate kernel selection via matrix approximation.
<em>TNNLS</em>, <em>31</em>(11), 4881–4891. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel selection is of fundamental importance for the generalization of kernel methods. This article proposes an approximate approach for kernel selection by exploiting the approximability of kernel selection and the computational virtue of kernel matrix approximation. We define approximate consistency to measure the approximability of the kernel selection problem. Based on the analysis of approximate consistency, we solve the theoretical problem of whether, under what conditions, and at what speed, the approximate criterion is close to the accurate one, establishing the foundations of approximate kernel selection. We introduce two selection criteria based on error estimation and prove the approximate consistency of the multilevel circulant matrix (MCM) approximation and Nyström approximation under these criteria. Under the theoretical guarantees of the approximate consistency, we design approximate algorithms for kernel selection, which exploits the computational advantages of the MCM and Nyström approximations to conduct kernel selection in a linear or quasi-linear complexity. We experimentally validate the theoretical results for the approximate consistency and evaluate the effectiveness of the proposed kernel selection algorithms.},
  archive      = {J_TNNLS},
  author       = {Lizhong Ding and Shizhong Liao and Yong Liu and Li Liu and Fan Zhu and Yazhou Yao and Ling Shao and Xin Gao},
  doi          = {10.1109/TNNLS.2019.2958922},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4881-4891},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Approximate kernel selection via matrix approximation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hopfield neural network flow: A geometric viewpoint.
<em>TNNLS</em>, <em>31</em>(11), 4869–4880. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide gradient flow interpretations for the continuous-time continuous-state Hopfield neural network (HNN). The ordinary and stochastic differential equations associated with the HNN were introduced in the literature as analog optimizers and were reported to exhibit good performance in numerical experiments. In this work, we point out that the deterministic HNN can be transcribed into Amari&#39;s natural gradient descent, and thereby uncover the explicit relation between the underlying Riemannian metric and the activation functions. By exploiting an equivalence between the natural gradient descent and the mirror descent, we show how the choice of activation function governs the geometry of the HNN dynamics. For the stochastic HNN, we show that the so-called “diffusion machine,” while not a gradient flow itself, induces a gradient flow when lifted in the space of probability measures. We characterize this infinite-dimensional flow as the gradient descent of certain free energy with respect to a Wasserstein metric that depends on the geodesic distance on the ground manifold. Furthermore, we demonstrate how this gradient flow interpretation can be used for fast computation via recently developed proximal algorithms.},
  archive      = {J_TNNLS},
  author       = {Abhishek Halder and Kenneth F. Caluya and Bertrand Travacca and Scott J. Moura},
  doi          = {10.1109/TNNLS.2019.2958556},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4869-4880},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hopfield neural network flow: A geometric viewpoint},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep clustering with sample-assignment invariance prior.
<em>TNNLS</em>, <em>31</em>(11), 4857–4868. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most popular clustering methods map raw image data into a projection space in which the clustering assignment is obtained with the vanilla k-means approach. In this article, we discovered a novel prior, namely, there exists a common invariance when assigning an image sample to clusters using different metrics. In short, different distance metrics will lead to similar soft clustering assignments on the manifold. Based on such a novel prior, we propose a novel clustering method by minimizing the discrepancy between pairwise sample assignments for each data point. To the best of our knowledge, this could be the first work to reveal the sample-assignment invariance prior based on the idea of treating labels as ideal representations. Furthermore, the proposed method is one of the first end-to-end clustering approaches, which jointly learns clustering assignment and representation. Extensive experimental results show that the proposed method is remarkably superior to 16 state-of-the-art clustering methods on five image data sets in terms of four evaluation metrics.},
  archive      = {J_TNNLS},
  author       = {Xi Peng and Hongyuan Zhu and Jiashi Feng and Chunhua Shen and Haixian Zhang and Joey Tianyi Zhou},
  doi          = {10.1109/TNNLS.2019.2958324},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4857-4868},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep clustering with sample-assignment invariance prior},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative transfer feature and label consistency for
cross-domain image classification. <em>TNNLS</em>, <em>31</em>(11),
4842–4856. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual domain adaptation aims to seek an effective transferable model for unlabeled target images by benefiting from the well-labeled source images following different distributions. Many recent efforts focus on extracting domain-invariant image representations via exploring target pseudo labels, predicted by the source classifier, to further mitigate the conditional distribution shift across domains. However, two essential factors are overlooked by most existing methods: 1) the learned transferable features should be not only domain invariant but also category discriminative; and 2) the target pseudo label is a two-edged sword to cross-domain alignment. In other words, the wrongly predicted target labels may hinder the class-wise domain matching. In this article, to address these two issues simultaneously, we propose a discriminative transfer feature and label consistency (DTLC) approach for visual domain adaptation problems, which can naturally unify cross-domain alignment with discriminative information preserved and label consistency of source and target data into one framework. To be specific, DTLC first incorporates class discriminative information by penalizing the maximum distance of data pair in the same class and the minimum distance of data pair sharing the different labels for each data into the distribution alignment of both domains. The target pseudo labels are then refined based on the label consistency within the domains. Thus, the transfer feature learning and coarse-to-fine target labels would be coupled to benefit each other in an iterative way. Comprehensive experiments on several visual cross-domain benchmarks verify that DTLC can gain remarkable margins over state-of-the-art (SOTA) nondeep visual domain adaptation methods and even be comparable to competitive deep domain adaptation ones.},
  archive      = {J_TNNLS},
  author       = {Shuang Li and Chi Harold Liu and Limin Su and Binhui Xie and Zhengming Ding and C. L. Philip Chen and Dapeng Wu},
  doi          = {10.1109/TNNLS.2019.2958152},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4842-4856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative transfer feature and label consistency for cross-domain image classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative fault-tolerant output regulation for multiagent
systems by distributed learning control approach. <em>TNNLS</em>,
<em>31</em>(11), 4831–4841. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new distributed learning control approach is proposed to address the cooperative fault-tolerant output regulation problem for linear multiagent systems with actuator faults. First, a distributed estimation algorithm with an online learning mechanism is presented to identify the system matrix of the exosystem and to estimate the state of the exosystem. In particular, an auxiliary variable is introduced in the distributed estimation algorithm to construct a data matrix, which is used to learn the system matrix of the exosystem for each subsystem. In addition, by resetting the state of the estimator and by using the identified matrix to update the estimator, all subsystems can reconstruct the state of the exosystem at an initial period of time, which is used for the neighbor subsystem to learn the system matrix of the exosystem. Based on the designed estimator, a novel distributed fault-tolerant controller is developed. Compared with the existing cooperative output regulation results, the system matrix of the exosystem considered in this article is unknown for all subsystems. Finally, a simulation example is provided to show the effectiveness of the obtained new design techniques.},
  archive      = {J_TNNLS},
  author       = {Chao Deng and Wei-Wei Che and Peng Shi},
  doi          = {10.1109/TNNLS.2019.2958151},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4831-4841},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cooperative fault-tolerant output regulation for multiagent systems by distributed learning control approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive bipartite event-triggered output consensus of
heterogeneous linear multiagent systems under fixed and switching
topologies. <em>TNNLS</em>, <em>31</em>(11), 4816–4830. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the adaptive bipartite event-triggered output consensus issue for heterogeneous linear multiagent systems. We consider both cooperative interaction and antagonistic interaction between neighbor agents in both fixed and switching topologies. An adaptive bipartite compensator consisting of time-varying coupling weights and dynamic event-triggered mechanism is first proposed to estimate the leader&#39;s state in a fully distributed manner. Different from the existing methods, the proposed compensator has three advantages: 1) it does not depend on any global information of the network graph; 2) it avoids the continuous communication between neighbor agents; and 3) it is applicable for the signed communication topology. Assume that the system states are unmeasurable, and we thus design the state observer. Based on the devised compensator and observer, the distributed control law is developed such that the bipartite event-triggered output consensus problem can be achieved. Moreover, we extend the results in fixed topology to switching topology, which is more challenging in that state estimation is updated in two cases: 1) the interaction graph is switched or 2) the event-triggered mechanism is satisfied. It is proven that no agent exhibits Zeno behavior in both fixed and switching interaction topologies. Finally, two examples are provided to illustrate the feasibility of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Huaguang Zhang and Yuliang Cai and Yingchun Wang and Hanguang Su},
  doi          = {10.1109/TNNLS.2019.2958107},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4816-4830},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive bipartite event-triggered output consensus of heterogeneous linear multiagent systems under fixed and switching topologies},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequence classification restricted boltzmann machines with
gated units. <em>TNNLS</em>, <em>31</em>(11), 4806–4815. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the classification of sequential data, dynamic Bayesian networks and recurrent neural networks (RNNs) are the preferred models. While the former can explicitly model the temporal dependences between the variables, and the latter have the capability of learning representations. The recurrent temporal restricted Boltzmann machine (RTRBM) is a model that combines these two features. However, learning and inference in RTRBMs can be difficult because of the exponential nature of its gradient computations when maximizing log likelihoods. In this article, first, we address this intractability by optimizing a conditional rather than a joint probability distribution when performing sequence classification. This results in the “sequence classification restricted Boltzmann machine” (SCRBM). Second, we introduce gated SCRBMs (gSCRBMs), which use an information processing gate, as an integration of SCRBMs with long short-term memory (LSTM) models. In the experiments reported in this article, we evaluate the proposed models on optical character recognition, chunking, and multiresident activity recognition in smart homes. The experimental results show that gSCRBMs achieve the performance comparable to that of the state of the art in all three tasks. gSCRBMs require far fewer parameters in comparison with other recurrent networks with memory gates, in particular, LSTMs and gated recurrent units (GRUs).},
  archive      = {J_TNNLS},
  author       = {Son N. Tran and Artur d’Avila Garcez and Tillman Weyde and Jie Yin and Qing Zhang and Mohan Karunanithi},
  doi          = {10.1109/TNNLS.2019.2958103},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4806-4815},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sequence classification restricted boltzmann machines with gated units},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Relaxed asymmetric deep hashing learning: Point-to-angle
matching. <em>TNNLS</em>, <em>31</em>(11), 4791–4805. (<a
href="https://doi.org/10.1109/TNNLS.2019.2958061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the powerful capability of the data representation, deep learning has achieved a remarkable performance in supervised hash function learning. However, most of the existing hashing methods focus on point-to-point matching that is too strict and unnecessary. In this article, we propose a novel deep supervised hashing method by relaxing the matching between each pair of instances to a point-to-angle way. Specifically, an inner product is introduced to asymmetrically measure the similarity and dissimilarity between the real-valued output and the binary code. Different from existing methods that strictly enforce each element in the real-valued output to be either +1 or -1, we only encourage the output to be close to its corresponding semantic-related binary code under the cross-angle. This asymmetric product not only projects both the real-valued output and the binary code into the same Hamming space but also relaxes the output with wider choices. To further exploit the semantic affinity, we propose a novel Hamming-distance-based triplet loss, efficiently making a ranking for the positive and negative pairs. An algorithm is then designed to alternatively achieve optimal deep features and binary codes. Experiments on four real-world data sets demonstrate the effectiveness and superiority of our approach to the state of the art.},
  archive      = {J_TNNLS},
  author       = {Jinxing Li and Bob Zhang and Guangming Lu and Jane You and Yong Xu and Feng Wu and David Zhang},
  doi          = {10.1109/TNNLS.2019.2958061},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4791-4805},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Relaxed asymmetric deep hashing learning: Point-to-angle matching},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A stochastic quasi-newton method for large-scale nonconvex
optimization with applications. <em>TNNLS</em>, <em>31</em>(11),
4776–4790. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the positive definiteness and avoiding ill conditioning of the Hessian update in the stochastic Broyden-Fletcher-Goldfarb-Shanno (BFGS) method are significant in solving nonconvex problems. This article proposes a novel stochastic version of a damped and regularized BFGS method for addressing the above problems. While the proposed regularized strategy helps to prevent the BFGS matrix from being close to singularity, the new damped parameter further ensures the positivity of the product of correction pairs. To alleviate the computational cost of the stochastic limited memory BFGS (LBFGS) updates and to improve its robustness, the curvature information is updated using the averaged iterate at spaced intervals. The effectiveness of the proposed method is evaluated through the logistic regression and Bayesian logistic regression problems in machine learning. Numerical experiments are conducted by using both synthetic data set and several real data sets. The results show that the proposed method generally outperforms the stochastic damped LBFGS (SdLBFGS) method. In particular, for problems with small sample sizes, our method has shown superior performance and is capable of mitigating ill-conditioned problems. Furthermore, our method is more robust to the variations of the batch size and memory size than the SdLBFGS method.},
  archive      = {J_TNNLS},
  author       = {Huiming Chen and Ho-Chun Wu and Shing-Chow Chan and Wong-Hing Lam},
  doi          = {10.1109/TNNLS.2019.2957843},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4776-4790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A stochastic quasi-newton method for large-scale nonconvex optimization with applications},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural networks multiobjective learning with spherical
representation of weights. <em>TNNLS</em>, <em>31</em>(11), 4761–4775.
(<a href="https://doi.org/10.1109/TNNLS.2019.2957730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel representation of artificial neural networks (ANNs) that is based on a projection of weights into a new spherical space defined by a radius r and a vector of angles Θ. This spherical representation of ANNs further simplifies the multiobjective learning problem, which is usually treated as a constrained optimization problem that requires great computational effort to maintain the constraints. With the proposed spherical representation, the constrained optimization problem becomes unconstrained, which simplifies the formulation and computational effort required. In addition, it also allows the use of any nonlinear optimization method for the multiobjective learning of ANNs. Results presented in this article show that the proposed spherical representation of weights yields more accurate estimates of the Pareto set than the classical multiobjective approach. Regarding the final solution selected from the Pareto set, our approach was effective and outperformed some state-of-the-art methods on several data sets.},
  archive      = {J_TNNLS},
  author       = {Honovan P. Rocha and Marcelo A. Costa and Antônio P. Braga},
  doi          = {10.1109/TNNLS.2019.2957730},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4761-4775},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural networks multiobjective learning with spherical representation of weights},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral images super-resolution via learning
high-order coupled tensor ring representation. <em>TNNLS</em>,
<em>31</em>(11), 4747–4760. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) super-resolution is a hot topic in remote sensing and computer vision. Recently, tensor analysis has been proven to be an efficient technology for HSI image processing. However, the existing tensor-based methods of HSI super-resolution are not able to capture the high-order correlations in HSI. In this article, we propose to learn a high-order coupled tensor ring (TR) representation for HSI super-resolution. The proposed method first tensorizes the HSI to be estimated into a high-order tensor in which multiscale spatial structures and the original spectral structure are represented. Then, a coupled TR representation model is proposed to fuse the low-resolution HSI (LR-HSI) and high-resolution multispectral image (HR-MSI). In the proposed model, some latent core tensors in TR of the LR-HSI and the HR-MSI are shared, and we use the relationship between the spectral core tensors to reconstruct the HSI. In addition, the graph-Laplacian regularization is introduced to the spectral core tensors to preserve the spectral information. To enhance the robustness of the proposed model, Frobenius norm regularizations are introduced to the other core tensors. Experimental results on both synthetic and real data sets show that the proposed method achieves the state-of-the-art super-resolution performance.},
  archive      = {J_TNNLS},
  author       = {Yang Xu and Zebin Wu and Jocelyn Chanussot and Zhihui Wei},
  doi          = {10.1109/TNNLS.2019.2957527},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4747-4760},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral images super-resolution via learning high-order coupled tensor ring representation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep supervised learning framework for data-driven soft
sensor modeling of industrial processes. <em>TNNLS</em>,
<em>31</em>(11), 4737–4746. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been recently introduced for soft sensors in industrial processes. However, most of the existing deep networks, such as stacked autoencoder, are pretrained in a layerwise unsupervised way to learn feature representations for the raw input data itself. For soft sensors, it is necessary to extract quality-relevant features for quality prediction. Thus, a deep layerwise supervised pretraining framework is proposed for quality-relevant feature extraction and soft sensor modeling in this article, which is based on stacked supervised encoder-decoder (SSED). In SSED, hierarchical quality-relevant features are successively learned by a number of supervised encoder-decoder (SED) models. For each SED, the features from the previous hidden layer are served as new inputs to generate the high-level features that are learned with the constraint of predicting the quality data as good as possible at the output layer of this SED. With this new structure, the SED can learn quality-relevant features that can largely improve the prediction performance. By stacking multiple SEDs, hierarchical quality-relevant features can be progressively learned, and irrelevant information is gradually reduced by deep SSED network. The effectiveness of the proposed model is demonstrated on a numerical example and an industrial process of the debutanizer column.},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Yuan and Yongjie Gu and Yalin Wang and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TNNLS.2019.2957366},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4737-4746},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deep supervised learning framework for data-driven soft sensor modeling of industrial processes},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). State-feedback filtering for delayed discrete-time
complex-valued neural networks. <em>TNNLS</em>, <em>31</em>(11),
4726–4736. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores a new filtering problem for the class of delayed discrete-time complex-valued neural networks (CVNNs) via state-feedback control design. The novelty of this article comes from the consideration of the newly developed complex-valued reciprocal convex matrix inequality as well as the complex-valued Jensen-based summation inequalities (JSIs). By employing an appropriate Lyapunov-Krasovskii functional (LKF) and by using newly proposed complex-valued inequalities, attention is concentrated on the design of a state-feedback filter such that the associated filtering error system is asymptotically stable with prescribed filter and control gain matrices. The proposed theoretical results are presented in terms of complex-valued linear matrix inequalities (LMIs) that can be solved numerically by using the YALMIP toolbox in MATLAB software. Additionally, one numerical example is given to confirm the validity of the resulting sufficient conditions with the availability of the suitable control and filter design.},
  archive      = {J_TNNLS},
  author       = {Soundararajan Ganesan and Nagamani Gnaneswaran},
  doi          = {10.1109/TNNLS.2019.2957304},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4726-4736},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {State-feedback filtering for delayed discrete-time complex-valued neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning-based nearly optimal control for
constrained-input partially unknown systems using differentiator.
<em>TNNLS</em>, <em>31</em>(11), 4713–4725. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a synchronous reinforcement-learning-based algorithm is developed for input-constrained partially unknown systems. The proposed control also alleviates the need for an initial stabilizing control. A first-order robust exact differentiator is employed to approximate unknown drift dynamics. Critic, actor, and disturbance neural networks (NNs) are established to approximate the value function, the control policy, and the disturbance policy, respectively. The Hamilton-Jacobi-Isaacs equation is solved by applying the value function approximation technique. The stability of the closed-loop system can be ensured. The state and weight errors of the three NNs are all uniformly ultimately bounded. Finally, the simulation results are provided to verify the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Xinxin Guo and Weisheng Yan and Rongxin Cui},
  doi          = {10.1109/TNNLS.2019.2957287},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4713-4725},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based nearly optimal control for constrained-input partially unknown systems using differentiator},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coupled attribute learning for heterogeneous face
recognition. <em>TNNLS</em>, <em>31</em>(11), 4699–4712. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous face recognition (HFR) is a challenging problem in face recognition and subject to large textural and spatial structure differences of face images. Different from conventional face recognition in homogeneous environments, there exist many face images taken from different sources (including different sensors or different mechanisms) in reality. In addition, limited training samples of cross-modality pairs make HFR more challenging due to the complex generation procedure of these images. Despite the great progress that has been achieved in recent years, existing works mainly focus on HFR from only cross-modality image matching. However, it is more practical to obtain both facial images and semantic descriptions about facial attributes in real-world situations, in which the semantic description clues are nearly always obtained during the process of image generation. Motivated by human cognitive mechanisms, we naturally utilize the explicit invariant semantic description, i.e., face attributes, to help address the gap among face images of different modalities. Existing facial attributes-related face recognition methods primarily regard attributes as the high-level features used to enhance recognition performance, ignoring the inherent relationship between face attributes and identities. In this article, we propose novel coupled attribute learning for the HFR (CAL-HFR) method without labeling the attributes manually. Deep convolutional networks are employed to directly map face images in heterogeneous scenarios to a compact common space where distances are taken as dissimilarities of pairs. Coupled attribute guided triplet loss (CAGTL) is designed to train an end-to-end HFR network that can effectively eliminate defects of incorrectly estimated attributes. Extensive experiments on multiple heterogeneous scenarios demonstrate that the proposed method achieves superior performance compared with that of state-of-the-art methods. Furthermore, we make publicly available our generated pairwise annotated heterogeneous facial attribute database for evaluation and promoting related research.},
  archive      = {J_TNNLS},
  author       = {Decheng Liu and Xinbo Gao and Nannan Wang and Jie Li and Chunlei Peng},
  doi          = {10.1109/TNNLS.2019.2957285},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4699-4712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coupled attribute learning for heterogeneous face recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural machine translation with GRU-gated attention model.
<em>TNNLS</em>, <em>31</em>(11), 4688–4698. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural machine translation (NMT) heavily relies on context vectors generated by an attention network to predict target words. In practice, we observe that the context vectors for different target words are quite similar to one another and translations with such nondiscriminatory context vectors tend to be degenerative. We ascribe this similarity to the invariant source representations that lack dynamics across decoding steps. In this article, we propose a novel gated recurrent unit (GRU)-gated attention model (GAtt) for NMT. By updating the source representations with the previous decoder state via a GRU, GAtt enables translation-sensitive source representations that then contribute to discriminative context vectors. We further propose a variant of GAtt by swapping the input order of the source representations and the previous decoder state to the GRU. Experiments on the NIST Chinese-English, WMT14 English-German, and WMT17 English-German translation tasks show that the two GAtt models achieve significant improvements over the vanilla attention-based NMT. Further analyses on the attention weights and context vectors demonstrate the effectiveness of GAtt in enhancing the discriminating capacity of representations and handling the challenging issue of overtranslation.},
  archive      = {J_TNNLS},
  author       = {Biao Zhang and Deyi Xiong and Jun Xie and Jinsong Su},
  doi          = {10.1109/TNNLS.2019.2957276},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4688-4698},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural machine translation with GRU-gated attention model},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LogDet metric-based domain adaptation. <em>TNNLS</em>,
<em>31</em>(11), 4673–4687. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation has proven to be successful in dealing with the case where training and test samples are drawn from two kinds of distributions, respectively. Recently, the second-order statistics alignment has gained significant attention in the field of domain adaptation due to its superior simplicity and effectiveness. However, researchers have encountered major difficulties with optimization, as it is difficult to find an explicit expression for the gradient. Moreover, the used transformation employed here does not perform dimensionality reduction. Accordingly, in this article, we prove that there exits some scaled LogDet metric that is more effective for the second-order statistics alignment than the Frobenius norm, and hence, we consider it for second-order statistics alignment. First, we introduce the two homologous transformations, which can help to reduce dimensionality and excavate transferable knowledge from the relevant domain. Second, we provide an explicit gradient expression, which is an important ingredient for optimization. We further extend the LogDet model from single-source domain setting to multisource domain setting by applying the weighted Karcher mean to the LogDet metric. Experiments on both synthetic and realistic domain adaptation tasks demonstrate that the proposed approaches are effective when compared with state-of-the-art ones.},
  archive      = {J_TNNLS},
  author       = {Youfa Liu and Bo Du and Weiping Tu and Mingming Gong and Yuhong Guo and Dacheng Tao},
  doi          = {10.1109/TNNLS.2019.2957229},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4673-4687},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LogDet metric-based domain adaptation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Few-shot learning with geometric constraints.
<em>TNNLS</em>, <em>31</em>(11), 4660–4672. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the problem of few-shot learning for classification. We assume a network trained for base categories with a large number of training examples, and we aim to add novel categories to it that have only a few, e.g., one or five, training examples. This is a challenging scenario because: 1) high performance is required in both the base and novel categories; and 2) training the network for the new categories with a few training examples can contaminate the feature space trained well for the base categories. To address these challenges, we propose two geometric constraints to fine-tune the network with a few training examples. The first constraint enables features of the novel categories to cluster near the category weights, and the second maintains the weights of the novel categories far from the weights of the base categories. By applying the proposed constraints, we extract discriminative features for the novel categories while preserving the feature space learned for the base categories. Using public data sets for few-shot learning that are subsets of ImageNet, we demonstrate that the proposed method outperforms prevalent methods by a large margin.},
  archive      = {J_TNNLS},
  author       = {Hong-Gyu Jung and Seong-Whan Lee},
  doi          = {10.1109/TNNLS.2019.2957187},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4660-4672},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Few-shot learning with geometric constraints},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating minibatch stochastic gradient descent using
typicality sampling. <em>TNNLS</em>, <em>31</em>(11), 4649–4659. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning, especially deep neural networks, has developed rapidly in fields, including computer vision, speech recognition, and reinforcement learning. Although minibatch stochastic gradient descent (SGD) is one of the most popular stochastic optimization methods for training deep networks, it shows a slow convergence rate due to the large noise in the gradient approximation. In this article, we attempt to remedy this problem by building a more efficient batch selection method based on typicality sampling, which reduces the error of gradient estimation in conventional minibatch SGD. We analyze the convergence rate of the resulting typical batch SGD algorithm and compare the convergence properties between the minibatch SGD and the algorithm. Experimental results demonstrate that our batch selection scheme works well and more complex minibatch SGD variants can benefit from the proposed batch selection strategy.},
  archive      = {J_TNNLS},
  author       = {Xinyu Peng and Li Li and Fei-Yue Wang},
  doi          = {10.1109/TNNLS.2019.2957003},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4649-4659},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accelerating minibatch stochastic gradient descent using typicality sampling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stroke sequence-dependent deep convolutional neural network
for online handwritten chinese character recognition. <em>TNNLS</em>,
<em>31</em>(11), 4637–4648. (<a
href="https://doi.org/10.1109/TNNLS.2019.2956965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel model, called stroke sequence-dependent deep convolutional neural network (SSDCNN), which uses the stroke sequence information and eight-directional features of Chinese characters for online handwritten Chinese character recognition (OLHCCR). SSDCNN learns the representation of OLHCCs by incorporating the natural sequence information of the strokes. Furthermore, it naturally incorporates the eight-directional features. First, SSDCNN inputs the stroke sequence and transforms it into stacks of feature maps following the writing order of the strokes. Second, the fixed-length, stroke sequence-dependent representations of OLHCC are derived through convolutional, residual, and max-pooling operations. Third, the stroke sequence-dependent representation is combined with the eight-directional features via a number of fully connected neural network layers. Finally, the Chinese characters are recognized using a softmax classifier. The SSDCNN is trained in two stages: 1) the whole architecture is pretrained using the training data until the performance converges to an acceptable degree. 2) The stroke sequence-dependent representation is combined with the eight-directional features by a fully connected neural network and a softmax layer for further training. The model was experimentally evaluated on the OLHCCR competition tasks of International Conference on Document Analysis and Recognition (ICDAR) 2013. The recognition error was a maximum 58.28\% lower in SSDCNN than in a model using the eight-directional features alone (5.13\% versus 2.14\%). Owing to its high accuracy (97.86\%), the proposed SSDCNN reduced the recognition error by approximately 18.0\% as compared with that of the winning system in the ICDAR 2013 competition. SSDCNN integrated with an adaptation mechanism, called the SSDCNN+Adapt model, and reached a new state-of-the-art (SOTA) standard with an accuracy of 97.94\%. The SSDCNN exploits the stroke sequence information to learn high-quality OLHCC representations. Moreover, the learned representation and the classical eight-directional features complement each other within the SSDCNN architecture.},
  archive      = {J_TNNLS},
  author       = {Xin Liu and Baotian Hu and Qingcai Chen and Xiangping Wu and Jinghan You},
  doi          = {10.1109/TNNLS.2019.2956965},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4637-4648},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stroke sequence-dependent deep convolutional neural network for online handwritten chinese character recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor networks for latent variable analysis: Novel
algorithms for tensor train approximation. <em>TNNLS</em>,
<em>31</em>(11), 4622–4636. (<a
href="https://doi.org/10.1109/TNNLS.2019.2956926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decompositions of tensors into factor matrices, which interact through a core tensor, have found numerous applications in signal processing and machine learning. A more general tensor model that represents data as an ordered network of subtensors of order-2 or order-3 has, so far, not been widely considered in these fields, although this so-called tensor network (TN) decomposition has been long studied in quantum physics and scientific computing. In this article, we present novel algorithms and applications of TN decompositions, with a particular focus on the tensor train (TT) decomposition and its variants. The novel algorithms developed for the TT decomposition update, in an alternating way, one or several core tensors at each iteration and exhibit enhanced mathematical tractability and scalability for large-scale data tensors. For rigor, the cases of the given ranks, given approximation error, and the given error bound are all considered. The proposed algorithms provide well-balanced TT-decompositions and are tested in the classic paradigms of blind source separation from a single mixture, denoising, and feature extraction, achieving superior performance over the widely used truncated algorithms for TT decomposition.},
  archive      = {J_TNNLS},
  author       = {Anh-Huy Phan and Andrzej Cichocki and André Uschmajew and Petr Tichavský and George Luta and Danilo P. Mandic},
  doi          = {10.1109/TNNLS.2019.2956926},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4622-4636},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tensor networks for latent variable analysis: Novel algorithms for tensor train approximation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust cumulative crowdsourcing framework using new
incentive payment function and joint aggregation model. <em>TNNLS</em>,
<em>31</em>(11), 4610–4621. (<a
href="https://doi.org/10.1109/TNNLS.2019.2956523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, crowdsourcing has gained tremendous attention in the machine learning community due to the increasing demand for labeled data. However, the labels collected by crowdsourcing are usually unreliable and noisy. This issue is mainly caused by: 1) nonflexible data collection mechanisms; 2) nonincentive payment functions; and 3) inexpert crowd workers. We propose a new robust crowdsourcing framework as a comprehensive solution for all these challenging problems. Our unified framework consists of three novel components. First, we introduce a new flexible data collection mechanism based on the cumulative voting system, allowing crowd workers to express their confidence for each option in multi-choice questions. Second, we design a novel payment function regarding the settings of our data collection mechanism. The payment function is theoretically proved to be incentive-compatible, encouraging crowd workers to disclose truthfully their beliefs to get the maximum payment. Third, we propose efficient aggregation models, which are compatible with both single-option and multi-option crowd labels. We define a new aggregation model, called simplex constrained majority voting (SCMV), and enhance it by using the probabilistic generative model. Furthermore, fast optimization algorithms are derived for the proposed aggregation models. Experimental results indicate higher quality for the crowd labels collected by our proposed mechanism without increasing the cost. Our aggregation models also outperform the state-of-the-art models on multiple crowdsourcing data sets in terms of accuracy and convergence speed.},
  archive      = {J_TNNLS},
  author       = {Kamran Ghasedi Dizaji and Hongchang Gao and Yanhua Yang and Heng Huang and Cheng Deng},
  doi          = {10.1109/TNNLS.2019.2956523},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4610-4621},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust cumulative crowdsourcing framework using new incentive payment function and joint aggregation model},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter distribution balanced CNNs. <em>TNNLS</em>,
<em>31</em>(11), 4600–4609. (<a
href="https://doi.org/10.1109/TNNLS.2019.2956390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN) is the primary technique that has greatly promoted the development of computer vision technologies. However, there is little research on how to allocate parameters in different convolution layers when designing CNNs. We research mainly on revealing the relationship between CNN parameter distribution, i.e., the allocation of parameters in convolution layers, and the discriminative performance of CNN. Unlike previous works, we do not append more elements into the network, such as more convolution layers or denser short connections. We focus on enhancing the discriminative performance of CNN through varying its parameter distribution under strict size constraint. We propose an energy function to represent the CNN parameter distribution, which establishes the connection between the allocation of parameters and the discriminative performance of CNN. Extensive experiments with shallow CNNs on three public image classification data sets demonstrate that the CNN parameter distribution with a higher energy value will promote the model to obtain better performance. According to the motivated observation, the problem of finding the optimal parameter distribution can be transformed into an optimization problem of finding the biggest energy value. We present a simple yet effective guideline that uses balanced parameter distribution to design CNNs. Extensive experiments on ImageNet with three popular backbones, i.e., AlexNet, ResNet34, and ResNet101, demonstrate that the proposed guideline can make consistent improvements upon different baselines under strict size constraint.},
  archive      = {J_TNNLS},
  author       = {Lixin Liao and Yao Zhao and Shikui Wei and Yunchao Wei and Jingdong Wang},
  doi          = {10.1109/TNNLS.2019.2956390},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4600-4609},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter distribution balanced CNNs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey of computational intelligence techniques for wind
power uncertainty quantification in smart grids. <em>TNNLS</em>,
<em>31</em>(11), 4582–4599. (<a
href="https://doi.org/10.1109/TNNLS.2019.2956195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high penetration level of renewable energy is thought to be one of the basic characteristics of future smart grids. Wind power, as one of the most increasing renewable energy, has brought a large number of uncertainties into the power systems. These uncertainties would require system operators to change their traditional ways of decision-making. This article provides a comprehensive survey of computational intelligence techniques for wind power uncertainty quantification in smart grids. First, prediction intervals (PIs) are introduced as a means to quantify the uncertainties in wind power forecasts. Various PI evaluation indices, including the latest trends in comprehensive evaluation techniques, are compared. Furthermore, computational intelligence-based PI construction methods are summarized and classified into traditional methods (parametric) and direct PI construction methods (nonparametric). In the second part of this article, methods of incorporating wind power forecast uncertainties into power system decision-making processes are investigated. Three techniques, namely, stochastic models, fuzzy logic models, and robust optimization, and different power system applications using these techniques are reviewed. Finally, future research directions, such as spatiotemporal and hierarchical forecasting, deep learning-based methods, and integration of predictive uncertainty estimates into the decision-making process, are discussed. This survey can benefit the readers by providing a complete technical summary of wind power uncertainty quantification and decision-making in smart grids.},
  archive      = {J_TNNLS},
  author       = {Hao Quan and Abbas Khosravi and Dazhi Yang and Dipti Srinivasan},
  doi          = {10.1109/TNNLS.2019.2956195},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4582-4599},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of computational intelligence techniques for wind power uncertainty quantification in smart grids},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced sparsity prior model for low-rank tensor
completion. <em>TNNLS</em>, <em>31</em>(11), 4567–4581. (<a
href="https://doi.org/10.1109/TNNLS.2019.2956153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional tensor completion (TC) methods generally assume that the sparsity of tensor-valued data lies in the global subspace. The so-called global sparsity prior is measured by the tensor nuclear norm. Such assumption is not reliable in recovering low-rank (LR) tensor data, especially when considerable elements of data are missing. To mitigate this weakness, this article presents an enhanced sparsity prior model for LRTC using both local and global sparsity information in a latent LR tensor. In specific, we adopt a doubly weighted strategy for nuclear norm along each mode to characterize global sparsity prior of tensor. Different from traditional tensor-based local sparsity description, the proposed factor gradient sparsity prior in the Tucker decomposition model describes the underlying subspace local smoothness in real-world tensor objects, which simultaneously characterizes local piecewise structure over all dimensions. Moreover, there is no need to minimize the rank of a tensor for the proposed local sparsity prior. Extensive experiments on synthetic data, real-world hyperspectral images, and face modeling data demonstrate that the proposed model outperforms state-of-the-art techniques in terms of prediction capability and efficiency.},
  archive      = {J_TNNLS},
  author       = {Jize Xue and Yongqiang Zhao and Wenzhi Liao and Jonathan Cheung-Wai Chan and Seong G. Kong},
  doi          = {10.1109/TNNLS.2019.2956153},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4567-4581},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhanced sparsity prior model for low-rank tensor completion},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Walk-steered convolution for graph classification.
<em>TNNLS</em>, <em>31</em>(11), 4553–4566. (<a
href="https://doi.org/10.1109/TNNLS.2019.2956095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph classification is a fundamental but challenging issue for numerous real-world applications. Despite recent great progress in image/video classification, convolutional neural networks (CNNs) cannot yet cater to graphs well because of graphical non-Euclidean topology. In this article, we propose a walk-steered convolutional (WSC) network to assemble the essential success of standard CNNs, as well as the powerful representation ability of random walk. Instead of deterministic neighbor searching used in previous graphical CNNs, we construct multiscale walk fields (a.k.a. local receptive fields) with random walk paths to depict subgraph structures and advocate graph scalability. To express the internal variations of a walk field, Gaussian mixture models are introduced to encode the principal components of walk paths therein. As an analogy to a standard convolution kernel on image, Gaussian models implicitly coordinate those unordered vertices/nodes and edges in a local receptive field after projecting to the gradient space of Gaussian parameters. We further stack graph coarsening upon Gaussian encoding by using dynamic clustering, such that high-level semantics of graph can be well learned like the conventional pooling on image. The experimental results on several public data sets demonstrate the superiority of our proposed WSC method over many state of the arts for graph classification.},
  archive      = {J_TNNLS},
  author       = {Jiatao Jiang and Chunyan Xu and Zhen Cui and Tong Zhang and Wenming Zheng and Jian Yang},
  doi          = {10.1109/TNNLS.2019.2956095},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4553-4566},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Walk-steered convolution for graph classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust triple-matrix-recovery-based auto-weighted label
propagation for classification. <em>TNNLS</em>, <em>31</em>(11),
4538–4552. (<a
href="https://doi.org/10.1109/TNNLS.2019.2956015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph-based semisupervised label propagation (LP) algorithm has delivered impressive classification results. However, the estimated soft labels typically contain mixed signs and noise, which cause inaccurate predictions due to the lack of suitable constraints. Moreover, the available methods typically calculate the weights and estimate the labels in the original input space, which typically contains noise and corruption. Thus, the encoded similarities and manifold smoothness may be inaccurate for label estimation. In this article, we present effective schemes for resolving these issues and propose a novel and robust semisupervised classification algorithm, namely the triple matrix recovery-based robust auto-weighted label propagation framework (ALP-TMR). Our ALP-TMR introduces a TMR mechanism to remove noise or mixed signs from the estimated soft labels and improve the robustness to noise and outliers in the steps of assigning weights and predicting the labels simultaneously. Our method can jointly recover the underlying clean data, clean labels, and clean weighting spaces by decomposing the original data, predicted soft labels, or weights into a clean part plus an error part by fitting noise. In addition, ALP-TMR integrates the auto-weighting process by minimizing the reconstruction errors over the recovered clean data and clean soft labels, which can encode the weights more accurately to improve both data representation and classification. By classifying samples in the recovered clean label and weight spaces, one can potentially improve the label prediction results. Extensive simulations verified the effectiveness of our ALP-TMR.},
  archive      = {J_TNNLS},
  author       = {Huan Zhang and Zhao Zhang and Mingbo Zhao and Qiaolin Ye and Min Zhang and Meng Wang},
  doi          = {10.1109/TNNLS.2019.2956015},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4538-4552},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust triple-matrix-recovery-based auto-weighted label propagation for classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotical feedback set stabilization of probabilistic
boolean control networks. <em>TNNLS</em>, <em>31</em>(11), 4524–4537.
(<a href="https://doi.org/10.1109/TNNLS.2019.2955974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the asymptotical feedback set stabilization in distribution of probabilistic Boolean control networks (PBCNs). We prove that a PBCN is asymptotically feedback stabilizable to a given subset if and only if (iff) it constitutes asymptotically feedback stabilizable to the largest control-invariant subset (LCIS) contained in this subset. We proposed an algorithm to calculate the LCIS contained in any given subset with the necessary and sufficient condition for asymptotical set stabilizability in terms of obtaining the reachability matrix. In addition, we propose a method to design stabilizing feedback based on a state-space partition. Finally, the results were applied to solve asymptotical feedback output tracking and asymptotical feedback synchronization of PBCNs. Examples were detailed to demonstrate the feasibility of the proposed method and results.},
  archive      = {J_TNNLS},
  author       = {Rongpei Zhou and Yuqian Guo and Yuhu Wu and Weihua Gui},
  doi          = {10.1109/TNNLS.2019.2955974},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4524-4537},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymptotical feedback set stabilization of probabilistic boolean control networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A learning-based solution for an adversarial repeated game
in cyber–physical power systems. <em>TNNLS</em>, <em>31</em>(11),
4512–4523. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapidly expanding complexity of the cyber-physical power systems, the probability of a system malfunctioning and failing is increasing. Most of the existing works combining smart grid (SG) security and game theory fail to replicate the adversarial events in the simulated environment close to the real-life events. In this article, a repeated game is formulated to mimic the real-life interactions between the adversaries of the modern electric power system. The optimal action strategies for different environment settings are analyzed. The advantage of the repeated game is that the players can generate actions independent of the previous actions&#39; history. The solution of the game is designed based on the reinforcement learning algorithm, which ensures the desired outcome in favor of the players. The outcome in favor of a player means achieving higher mixed strategy payoff compared to the other player. Different from the existing game-theoretic approaches, both the attacker and the defender participate actively in the game and learn the sequence of actions applying to the power transmission lines. In this game, we consider several factors (e.g., attack and defense costs, allocated budgets, and the players&#39; strengths) that could affect the outcome of the game. These considerations make the game close to real-life events. To evaluate the game outcome, both players&#39; utilities are compared, and they reflect how much power is lost due to the attacks and how much power is saved due to the defenses. The players&#39; favorable outcome is achieved for different attack and defense strengths (probabilities). The IEEE 39 bus system is used here as the test benchmark. Learned attack and defense strategies are applied in a simulated power system environment (PowerWorld) to illustrate the postattack effects on the system.},
  archive      = {J_TNNLS},
  author       = {Shuva Paul and Zhen Ni and Chaoxu Mu},
  doi          = {10.1109/TNNLS.2019.2955857},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4512-4523},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A learning-based solution for an adversarial repeated game in Cyber–Physical power systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DiffGrad: An optimization method for convolutional neural
networks. <em>TNNLS</em>, <em>31</em>(11), 4500–4511. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) is one of the core techniques behind the success of deep neural networks. The gradient provides information on the direction in which a function has the steepest rate of change. The main problem with basic SGD is to change by equal-sized steps for all parameters, irrespective of the gradient behavior. Hence, an efficient way of deep network optimization is to have adaptive step sizes for each parameter. Recently, several attempts have been made to improve gradient descent methods such as AdaGrad, AdaDelta, RMSProp, and adaptive moment estimation (Adam). These methods rely on the square roots of exponential moving averages of squared past gradients. Thus, these methods do not take advantage of local change in gradients. In this article, a novel optimizer is proposed based on the difference between the present and the immediate past gradient (i.e., diffGrad). In the proposed diffGrad optimization technique, the step size is adjusted for each parameter in such a way that it should have a larger step size for faster gradient changing parameters and a lower step size for lower gradient changing parameters. The convergence analysis is done using the regret bound approach of the online learning framework. In this article, thorough analysis is made over three synthetic complex nonconvex functions. The image categorization experiments are also conducted over the CIFAR10 and CIFAR100 data sets to observe the performance of diffGrad with respect to the state-of-the-art optimizers such as SGDM, AdaGrad, AdaDelta, RMSProp, AMSGrad, and Adam. The residual unit (ResNet)-based convolutional neural network (CNN) architecture is used in the experiments. The experiments show that diffGrad outperforms other optimizers. Also, we show that diffGrad performs uniformly well for training CNN using different activation functions. The source code is made publicly available at https://github.com/shivram1987/diffGrad.},
  archive      = {J_TNNLS},
  author       = {Shiv Ram Dubey and Soumendu Chakraborty and Swalpa Kumar Roy and Snehasis Mukherjee and Satish Kumar Singh and Bidyut Baran Chaudhuri},
  doi          = {10.1109/TNNLS.2019.2955777},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4500-4511},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DiffGrad: An optimization method for convolutional neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A path-integral-based reinforcement learning algorithm for
path following of an autoassembly mobile robot. <em>TNNLS</em>,
<em>31</em>(11), 4487–4499. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) combined with deep neural networks has led to a number of great achievements for robot control in virtual computer environments, where sufficient data can be obtained without any difficulty to train various models. However, thus far, only few and relatively simple tasks have been accomplished for practical robots, which is mainly caused by the following two reasons. First, training with real robots, especially with dynamic systems, is too complicated to be fully and accurately represented in simulations. Second, it is very costly to obtain training data from real systems. To address these two problems effectively, in this article, a path-integral-based RL algorithm is proposed for the task of path following of an autoassembly mobile robot, wherein three kernel techniques are introduced. First, a generalized path-integral-control approach is proposed to obtain the numerical solution of a stochastic dynamical system, wherein the calculation of the gradient and kinematics inverse is avoided to ensure fast and reliable training convergence. Second, a novel parameterization method using Lyapunov techniques is introduced into the RL algorithm to ensure good performance of the system when directly transferring simulation results into practical systems. Third, the optimal parameters for all discrete initial states are first learned offline and then tuned online to improve the generalization and real-time performance. In addition to the optimization control for the mobile robot, the proposed method also possesses general applicability for a class of nonlinear systems such as crane systems. Simulation and experimental results are included and analyzed to illustrate the superior performance of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Wei Zhu and Xian Guo and Yongchun Fang and Xueyou Zhang},
  doi          = {10.1109/TNNLS.2019.2955699},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4487-4499},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A path-integral-based reinforcement learning algorithm for path following of an autoassembly mobile robot},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning in text streams: Discovery and disambiguation of
entity and relation instances. <em>TNNLS</em>, <em>31</em>(11),
4475–4486. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a scenario where an artificial agent is reading a stream of text composed of a set of narrations, and it is informed about the identity of some of the individuals that are mentioned in the text portion that is currently being read. The agent is expected to learn to follow the narrations, thus disambiguating mentions and discovering new individuals. We focus on the case in which individuals are entities and relations and propose an end-to-end trainable memory network that learns to discover and disambiguate them in an online manner, performing one-shot learning and dealing with a small number of sparse supervisions. Our system builds a not-given-in-advance knowledge base, and it improves its skills while reading the unsupervised text. The model deals with abrupt changes in the narration, considering their effects when resolving coreferences. We showcase the strong disambiguation and discovery skills of our model on a corpus of Wikipedia documents and on a newly introduced data set that we make publicly available.},
  archive      = {J_TNNLS},
  author       = {Marco Maggini and Giuseppe Marra and Stefano Melacci and Andrea Zugarini},
  doi          = {10.1109/TNNLS.2019.2955597},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4475-4486},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning in text streams: Discovery and disambiguation of entity and relation instances},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient group recommendation model with
multiattention-based neural networks. <em>TNNLS</em>, <em>31</em>(11),
4461–4474. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group recommendation research has recently received much attention in a recommender system community. Currently, several deep-learning-based methods are used in group recommendation to learn preferences of groups on items and predict the next ones in which groups may be interested. However, their recommendation effectiveness is disappointing. To address this challenge, this article proposes a novel model called a multiattention-based group recommendation model (MAGRM). It well utilizes multiattention-based deep neural network structures to achieve accurate group recommendation. We train its two closely related modules: vector representation for group features and preference learning for groups on items. The former is proposed to learn to accurately represent each group&#39;s deep semantic features. It integrates four aspects of subfeatures: group co-occurrence, group description, and external and internal social features. In particular, we employ multiattention networks to learn to capture internal social features for groups. The latter employs a neural attention mechanism to depict preference interactions between each group and its members and then combines group and item features to accurately learn group preferences on items. Through extensive experiments on two real-world databases, we show that MAGRM remarkably outperforms the state-of-the-art methods in solving a group recommendation problem.},
  archive      = {J_TNNLS},
  author       = {Zhenhua Huang and Xin Xu and Honghao Zhu and MengChu Zhou},
  doi          = {10.1109/TNNLS.2019.2955567},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4461-4474},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient group recommendation model with multiattention-based neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural networks finite-time optimal control for a
class of nonlinear systems. <em>TNNLS</em>, <em>31</em>(11), 4451–4460.
(<a href="https://doi.org/10.1109/TNNLS.2019.2955438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the finite-time optimal control problem for a class of nonlinear systems whose powers are positive odd rational numbers. First of all, a finite-time controller, which is capable of ensuring the semiglobal practical finite-time stability for the closed-loop systems, is developed using the adaptive neural networks (NNs) control method, adding one power integrator technique and backstepping scheme. Second, the corresponding design parameters are optimized, and the finite-time optimal control property is obtained by means of minimizing the well-defined and designed cost function. Finally, a numerical simulation example is given to further validate the feasibility and effectiveness of the proposed optimal control strategy.},
  archive      = {J_TNNLS},
  author       = {Yongming Li and Tingting Yang and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2019.2955438},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4451-4460},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural networks finite-time optimal control for a class of nonlinear systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-triggered synchronization for neutral-type
semi-markovian neural networks with partial mode-dependent time-varying
delays. <em>TNNLS</em>, <em>31</em>(11), 4437–4450. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the event-triggered stochastic synchronization problem for neutral-type semi-Markovian jump (SMJ) neural networks with partial mode-dependent additive time-varying delays (ATDs), where the SMJ parameters in two ATDs are considered to be not completely the same as the one in the connection weight matrices of the systems. Different from the weak infinitesimal operator of multi-Markov processes, a new one for the double semi-Markovian processes (SMPs) is first proposed. To reduce the conservative of the stability criteria, a generalized reciprocally convex combination inequality (RCCI) is established by the virtue of an interesting technique. Then, based on an eligible stochastic Lyapunov-Krasovski functional, three novel stability criteria for the studied systems are derived by employing the new RCCI and combining with a well-designed event-triggered control scheme. Finally, three numerical examples and one practical engineering example are presented to show the validity of our methods.},
  archive      = {J_TNNLS},
  author       = {Haiyang Zhang and Zhipeng Qiu and Jinde Cao and Mahmoud Abdel-Aty and Lianglin Xiong},
  doi          = {10.1109/TNNLS.2019.2955287},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4437-4450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered synchronization for neutral-type semi-markovian neural networks with partial mode-dependent time-varying delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust structured graph clustering. <em>TNNLS</em>,
<em>31</em>(11), 4424–4436. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based clustering methods have achieved remarkable performance by partitioning the data samples into disjoint groups with the similarity graph that characterizes the sample relations. Nevertheless, their learning scheme still suffers from two important problems: 1) the similarity graph directly constructed from the raw features may be unreliable as real-world data always involves adverse noises, outliers, and irrelevant information and 2) most graph-based clustering methods adopt two-step learning strategy that separates the similarity graph construction and clustering into two independent processes. Under such circumstance, the generated graph is unstructured and fixed. It may suffer from a low-quality clustering structure and thus lead to suboptimal clustering performance. To alleviate these limitations, in this article we propose a robust structured graph clustering (RSGC) model. We formulate a novel learning framework to simultaneously learn a robust structured similarity graph and perform clustering. Specifically, the structured graph with proper probabilistic neighborhood assignment is adaptively learned on a robust latent representation that resists the noises and outliers. Furthermore, an explicit rank constraint is imposed on the Laplacian matrix to structurize the graph such that the number of the connected components is exactly equal to the ground-truth cluster number. To solve the challenging objective formulation, we propose to first transform it into an equivalent one that can be tackled more easily. An iterative solution based on the augmented Lagrangian multiplier is then derived to solve the model. In RSGC, the discrete cluster labels can be directly obtained by partitioning the learned similarity graph without reliance on label discretization strategy as most graph-based clustering methods. Experiments on both synthetic and real data sets demonstrate the superiority of the proposed method compared with the state-of-the-art clustering techniques.},
  archive      = {J_TNNLS},
  author       = {Dan Shi and Lei Zhu and Yikun Li and Jingjing Li and Xiushan Nie},
  doi          = {10.1109/TNNLS.2019.2955209},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4424-4436},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust structured graph clustering},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). When gaussian process meets big data: A review of scalable
GPs. <em>TNNLS</em>, <em>31</em>(11), 4405–4423. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
  archive      = {J_TNNLS},
  author       = {Haitao Liu and Yew-Soon Ong and Xiaobo Shen and Jianfei Cai},
  doi          = {10.1109/TNNLS.2019.2957109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4405-4423},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {When gaussian process meets big data: A review of scalable GPs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(10), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.3022551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.3022551},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic gradient descent for nonconvex learning without
bounded gradient assumptions. <em>TNNLS</em>, <em>31</em>(10),
4394–4400. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) is a popular and efficient method with wide applications in training deep neural nets and other nonconvex models. While the behavior of SGD is well understood in the convex learning setting, the existing theoretical results for SGD applied to nonconvex objective functions are far from mature. For example, existing results require to impose a nontrivial assumption on the uniform boundedness of gradients for all iterates encountered in the learning process, which is hard to verify in practical implementations. In this article, we establish a rigorous theoretical foundation for SGD in nonconvex learning by showing that this boundedness assumption can be removed without affecting convergence rates, and relaxing the standard smoothness assumption to Hölder continuity of gradients. In particular, we establish sufficient conditions for almost sure convergence as well as optimal convergence rates for SGD applied to both general nonconvex and gradient-dominated objective functions. A linear convergence is further derived in the case with zero variances.},
  archive      = {J_TNNLS},
  author       = {Yunwen Lei and Ting Hu and Guiying Li and Ke Tang},
  doi          = {10.1109/TNNLS.2019.2952219},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4394-4400},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastic gradient descent for nonconvex learning without bounded gradient assumptions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LS-SVR as a bayesian RBF network. <em>TNNLS</em>,
<em>31</em>(10), 4389–4393. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show theoretical similarities between the least squares support vector regression (LS-SVR) model with a radial basis functions (RBFs) kernel and maximum a posteriori (MAP) inference on Bayesian RBF networks with a specific Gaussian prior on the regression weights. Although previous articles have pointed out similar expressions between those learning approaches, we explicitly and formally state the existing correspondences. We empirically demonstrate our result by performing computational experiments with standard regression benchmarks. Our findings open a range of possibilities to improve LS-SVR by borrowing strength from well-established developments in Bayesian methodology.},
  archive      = {J_TNNLS},
  author       = {Diego P. P. Mesquita and Luis A. Freitas and João P. P. Gomes and César L. C. Mattos},
  doi          = {10.1109/TNNLS.2019.2952000},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4389-4393},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LS-SVR as a bayesian RBF network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dynamic event-triggered approach to recursive filtering
for complex networks with switching topologies subject to random sensor
failures. <em>TNNLS</em>, <em>31</em>(10), 4381–4388. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the recursive filtering issue for a class of nonlinear complex networks (CNs) with switching topologies, random sensor failures and dynamic event-triggered mechanisms. A Markov chain is utilized to characterize the switching behavior of the network topology. The phenomenon of sensor failures occurs in a random way governed by a set of stochastic variables obeying certain probability distributions. In order to save communication cost, a dynamic event-triggered transmission protocol is introduced into the transmission channel from the sensors to the recursive filters. The objective of the addressed problem is to design a set of dynamic event-triggered filters for the underlying CN with a certain guaranteed upper bound (on the filtering error covariance) that is then locally minimized. By employing the induction method, an upper bound is first obtained on the filtering error covariance and subsequently minimized by properly designing the filter parameters. Finally, a simulation example is provided to demonstrate the effectiveness of the proposed filtering scheme.},
  archive      = {J_TNNLS},
  author       = {Qi Li and Zidong Wang and Nan Li and Weiguo Sheng},
  doi          = {10.1109/TNNLS.2019.2951948},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4381-4388},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A dynamic event-triggered approach to recursive filtering for complex networks with switching topologies subject to random sensor failures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Qualitative measurements of policy discrepancy for
return-based deep q-network. <em>TNNLS</em>, <em>31</em>(10), 4374–4380.
(<a href="https://doi.org/10.1109/TNNLS.2019.2948892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep Q-network (DQN) and return-based reinforcement learning are two promising algorithms proposed in recent years. The DQN brings advances to complex sequential decision problems, while return-based algorithms have advantages in making use of sample trajectories. In this brief, we propose a general framework to combine the DQN and most of the return-based reinforcement learning algorithms, named R-DQN. We show that the performance of the traditional DQN can be significantly improved by introducing return-based algorithms. In order to further improve the R-DQN, we design a strategy with two measurements to qualitatively measure the policy discrepancy. We conduct experiments on several representative tasks from the OpenAI Gym and Atari games. The state-of-the-art performance achieved by our method with this proposed strategy validates its effectiveness.},
  archive      = {J_TNNLS},
  author       = {Wenjia Meng and Qian Zheng and Long Yang and Pengfei Li and Gang Pan},
  doi          = {10.1109/TNNLS.2019.2948892},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4374-4380},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Qualitative measurements of policy discrepancy for return-based deep Q-network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asynchronous distributed learning from constraints.
<em>TNNLS</em>, <em>31</em>(10), 4367–4373. (<a
href="https://doi.org/10.1109/TNNLS.2019.2947740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, the extension of the framework of Learning from Constraints (LfC) to a distributed setting where multiple parties, connected over the network, contribute to the learning process is studied. LfC relies on the generic notion of “constraint” to inject knowledge into the learning problem, and, due to its generality, it deals with possibly nonconvex constraints, enforced either in a hard or soft way. Motivated by recent progresses in the field of distributed and constrained nonconvex optimization, we apply the (distributed) asynchronous method of multipliers (ASYMM) to LfC. The study shows that such a method allows us to support scenarios where selected constraints (i.e., knowledge), data, and outcomes of the learning process can be locally stored in each computational node without being shared with the rest of the network, opening the road to further investigations into privacy-preserving LfC. Constraints act as a bridge between what is shared over the net and what is private to each node, and no central authority is required. We demonstrate the applicability of these ideas in two distributed real-world settings in the context of digit recognition and document classification.},
  archive      = {J_TNNLS},
  author       = {Francesco Farina and Stefano Melacci and Andrea Garulli and Antonio Giannitrapani},
  doi          = {10.1109/TNNLS.2019.2947740},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4367-4373},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asynchronous distributed learning from constraints},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deterministic annealing neural network algorithm for the
minimum concave cost transportation problem. <em>TNNLS</em>,
<em>31</em>(10), 4354–4366. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a deterministic annealing neural network algorithm is proposed to solve the minimum concave cost transportation problem. Specifically, the algorithm is derived from two neural network models and Lagrange-barrier functions. The Lagrange function is used to handle linear equality constraints, and the barrier function is used to force the solution to move to the global or near-global optimal solution. In both neural network models, two descent directions are constructed, and an iterative procedure for the optimization of the neural network is proposed. As a result, two corresponding Lyapunov functions are naturally obtained from these two descent directions. Furthermore, the proposed neural network models are proved to be completely stable and converge to the stable equilibrium state, therefore, the proposed algorithm converges. At last, the computer simulations on several test problems are made, and the results indicate that the proposed algorithm always generates global or near-global optimal solutions.},
  archive      = {J_TNNLS},
  author       = {Zhengtian Wu and Hamid Reza Karimi and Chuangyin Dang},
  doi          = {10.1109/TNNLS.2019.2955137},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4354-4366},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deterministic annealing neural network algorithm for the minimum concave cost transportation problem},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural-network-based adaptive resilient dynamic surface
control against unknown deception attacks of uncertain nonlinear
time-delay cyberphysical systems. <em>TNNLS</em>, <em>31</em>(10),
4341–4353. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A neural-network-based dynamic surface design strategy against sensor and actuator deception attacks is presented to design a delay-independent adaptive resilient control scheme of uncertain nonlinear time-delay cyberphysical systems in the lower triangular form. It is assumed that all nonlinearities, time-varying delays, and sensor and actuator attacks are unknown. In the concerned problem, since the state information measured by sensors is compromised by additional attack signals, the exact state variables are not available for feedback. Thus, a memoryless adaptive resilient control design using compromised state variables is developed by employing the neural-network-based function approximation technique and designing the attack compensator. The resulting control scheme ensures the robust stabilization in the presence of unknown deception attacks and time-varying delays. It is shown from the Lyapunov stability analysis that all closed-loop signals are uniformly ultimately bounded and the stabilization errors converge to an adjustable neighborhood of the origin.},
  archive      = {J_TNNLS},
  author       = {Sung Jin Yoo},
  doi          = {10.1109/TNNLS.2019.2955132},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4341-4353},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive resilient dynamic surface control against unknown deception attacks of uncertain nonlinear time-delay cyberphysical systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning-based optimal stabilization for
unknown nonlinear systems subject to inputs with uncertain constraints.
<em>TNNLS</em>, <em>31</em>(10), 4330–4340. (<a
href="https://doi.org/10.1109/TNNLS.2019.2954983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel reinforcement learning strategy that addresses an optimal stabilizing problem for unknown nonlinear systems subject to uncertain input constraints. The control algorithm is composed of two parts, i.e., online learning optimal control for the nominal system and feedforward neural networks (NNs) compensation for handling uncertain input constraints, which are considered as the saturation nonlinearities. Integrating the input-output data and recurrent NN, a Luenberger observer is established to approximate the unknown system dynamics. For nominal systems without input constraints, the online learning optimal control policy is derived by solving Hamilton-Jacobi-Bellman equation via a critic NN alone. By transforming the uncertain input constraints to saturation nonlinearities, the uncertain input constraints can be compensated by employing a feedforward NN compensator. The convergence of the closed-loop system is guaranteed to be uniformly ultimately bounded by using the Lyapunov stability analysis. Finally, the effectiveness of the developed stabilization scheme is illustrated by simulation studies.},
  archive      = {J_TNNLS},
  author       = {Bo Zhao and Derong Liu and Chaomin Luo},
  doi          = {10.1109/TNNLS.2019.2954983},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4330-4340},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based optimal stabilization for unknown nonlinear systems subject to inputs with uncertain constraints},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive hashing with sparse matrix factorization.
<em>TNNLS</em>, <em>31</em>(10), 4318–4329. (<a
href="https://doi.org/10.1109/TNNLS.2019.2954856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing offers a desirable and effective solution for efficiently retrieving the nearest neighbors from large-scale data because of its low storage and computation costs. One of the most appealing techniques for hashing learning is matrix factorization. However, most hashing methods focus only on building the mapping relationships between the Euclidean and Hamming spaces and, unfortunately, underestimate the naturally sparse structures of the data. In addition, parameter tuning is always a challenging and head-scratching problem for sparse hashing learning. To address these problems, in this article, we propose a novel hashing method termed adaptively sparse matrix factorization hashing (SMFH), which exploits sparse matrix factorization to explore the parsimonious structures of the data. Moreover, SMFH adopts an orthogonal transformation to minimize the quantization loss while deriving the binary codes. The most distinguished property of SMFH is that it is adaptive and parameter-free, that is, SMFH can automatically generate sparse representations and does not require human involvement to tune the regularization parameters for the sparse models. Empirical studies on four publicly available benchmark data sets show that the proposed method can achieve promising performance and is competitive with a variety of state-of-the-art hashing methods.},
  archive      = {J_TNNLS},
  author       = {Huawen Liu and Xuelong Li and Shichao Zhang and Qi Tian},
  doi          = {10.1109/TNNLS.2019.2954856},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4318-4329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive hashing with sparse matrix factorization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative local sparse representation by robust
adaptive dictionary pair learning. <em>TNNLS</em>, <em>31</em>(10),
4303–4317. (<a
href="https://doi.org/10.1109/TNNLS.2019.2954545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a structured robust adaptive dictionary pair learning (RA-DPL) framework for the discriminative sparse representation (SR) learning. To achieve powerful representation ability of the available samples, the setting of RA-DPL seamlessly integrates the robust projective DPL, locality-adaptive SRs, and discriminative coding coefficients learning into a unified learning framework. Specifically, RA-DPL improves existing projective DPL in four perspectives. First, it applies a sparse l 2,1 -norm-based metric to encode the reconstruction error to deliver the robust projective dictionary pairs, and the l 2,1 -norm has the potential to minimize the error. Second, it imposes the robust l 2,1 -norm clearly on the analysis dictionary to ensure the sparse property of the coding coefficients rather than using the costly l 0 /l 1 -norm. As such, the robustness of the data representation and the efficiency of the learning process are jointly considered to guarantee the efficacy of our RA-DPL. Third, RA-DPL conceives a structured reconstruction weight learning paradigm to preserve the local structures of the coding coefficients within each class clearly in an adaptive manner, which encourages to produce the locality preserving representations. Fourth, it also considers improving the discriminating ability of coding coefficients and dictionary by incorporating a discriminating function, which can ensure high intraclass compactness and interclass separation in the code space. Extensive experiments show that our RA-DPL can obtain superior performance over other state of the arts.},
  archive      = {J_TNNLS},
  author       = {Yulin Sun and Zhao Zhang and Weiming Jiang and Zheng Zhang and Li Zhang and Shuicheng Yan and Meng Wang},
  doi          = {10.1109/TNNLS.2019.2954545},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4303-4317},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative local sparse representation by robust adaptive dictionary pair learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SRSC: Selective, robust, and supervised constrained feature
representation for image classification. <em>TNNLS</em>,
<em>31</em>(10), 4290–4302. (<a
href="https://doi.org/10.1109/TNNLS.2019.2953675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation learning, an emerging topic in recent years, has achieved great progress. Powerful learned features can lead to excellent classification accuracy. In this article, a selective and robust feature representation framework with a supervised constraint (SRSC) is presented. SRSC seeks a selective, robust, and discriminative subspace by transforming the original feature space into the category space. Particularly, we add a selective constraint to the transformation matrix (or classifier parameter) that can select discriminative dimensions of the input samples. Moreover, a supervised regularization is tailored to further enhance the discriminability of the subspace. To relax the hard zero-one label matrix in the category space, an additional error term is also incorporated into the framework, which can lead to a more robust transformation matrix. SRSC is formulated as a constrained least square learning (feature transforming) problem. For the SRSC problem, an inexact augmented Lagrange multiplier method (ALM) is utilized to solve it. Extensive experiments on several benchmark data sets adequately demonstrate the effectiveness and superiority of the proposed method. The proposed SRSC approach has achieved better performances than the compared counterpart methods.},
  archive      = {J_TNNLS},
  author       = {Guo-Sen Xie and Zheng Zhang and Li Liu and Fan Zhu and Xu-Yao Zhang and Ling Shao and Xuelong Li},
  doi          = {10.1109/TNNLS.2019.2953675},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4290-4302},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SRSC: Selective, robust, and supervised constrained feature representation for image classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An event-triggering approach to recursive filtering for
complex networks with state saturations and random coupling strengths.
<em>TNNLS</em>, <em>31</em>(10), 4279–4289. (<a
href="https://doi.org/10.1109/TNNLS.2019.2953649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the recursive filtering problem is investigated for a class of time-varying complex networks with state saturations and random coupling strengths under an event-triggering transmission mechanism. The coupled strengths among nodes are characterized by a set of random variables obeying the uniform distribution. The event-triggering scheme is employed to mitigate the network data transmission burden. The purpose of the problem addressed is to design a recursive filter such that in the presence of the state saturations, event-triggering communication mechanism, and random coupling strengths, certain locally optimized upper bound is guaranteed on the filtering error covariance. By using the stochastic analysis technique, an upper bound on the filtering error covariance is first derived via the solution to a set of matrix difference equations. Next, the obtained upper bound is minimized by properly parameterizing the filter parameters. Subsequently, the boundedness issue of the filtering error covariance is studied. Finally, two numerical simulation examples are provided to illustrate the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Hongyu Gao and Hongli Dong and Zidong Wang and Fei Han},
  doi          = {10.1109/TNNLS.2019.2953649},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4279-4289},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An event-triggering approach to recursive filtering for complex networks with state saturations and random coupling strengths},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continual learning of recurrent neural networks by locally
aligning distributed representations. <em>TNNLS</em>, <em>31</em>(10),
4267–4278. (<a
href="https://doi.org/10.1109/TNNLS.2019.2953622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal models based on recurrent neural networks have proven to be quite powerful in a wide variety of applications, including language modeling and speech processing. However, training these models often relies on backpropagation through time (BPTT), which entails unfolding the network over many time steps, making the process of conducting credit assignment considerably more challenging. Furthermore, the nature of backpropagation itself does not permit the use of nondifferentiable activation functions and is inherently sequential, making parallelization of the underlying training process difficult. Here, we propose the parallel temporal neural coding network (P-TNCN), a biologically inspired model trained by the learning algorithm we call local representation alignment. It aims to resolve the difficulties and problems that plague recurrent networks trained by BPTT. The architecture requires neither unrolling in time nor the derivatives of its internal activation functions. We compare our model and learning procedure with other BPTT alternatives (which also tend to be computationally expensive), including real-time recurrent learning, echo state networks, and unbiased online recurrent optimization. We show that it outperforms these on-sequence modeling benchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing NotMNIST, and Penn Treebank. Notably, our approach can, in some instances, outperform full BPTT as well as variants such as sparse attentive backtracking. Significantly, the hidden unit correction phase of P-TNCN allows it to adapt to new data sets even if its synaptic weights are held fixed (zero-shot adaptation) and facilitates retention of prior generative knowledge when faced with a task sequence. We present results that show the P-TNCN&#39;s ability to conduct zero-shot adaptation and online continual sequence modeling.},
  archive      = {J_TNNLS},
  author       = {Alexander Ororbia and Ankur Mali and C. Lee Giles and Daniel Kifer},
  doi          = {10.1109/TNNLS.2019.2953622},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4267-4278},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continual learning of recurrent neural networks by locally aligning distributed representations},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized event-triggered adaptive control of
discrete-time nonzero-sum games over wireless sensor-actuator networks
with input constraints. <em>TNNLS</em>, <em>31</em>(10), 4254–4266. (<a
href="https://doi.org/10.1109/TNNLS.2019.2953613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies an event-triggered communication and adaptive dynamic programming (ADP) co-design control method for the multiplayer nonzero-sum (NZS) games of a class of nonlinear discrete-time wireless sensor-actuator network (WSAN) systems subject to input constraints. By virtue of the ADP algorithm, the critic and actor networks are established to attain the approximate Nash equilibrium point solution in the context of the constrained control mechanism. Simultaneously, as the sensors and actuators are physically distributed, a decentralized event-triggered communication protocol is presented, accompanied by a dead-zone operation which avoids the unnecessary events. By predefining the triggering thresholds and compensation values, a novel adaptive triggering condition is derived to guarantee the stability of the event-based closed-loop control system. Then resorting to the Lyapunov theory, the system states and the critic/actor network weight estimation errors are proven to be ultimately bounded. Moreover, an explicit analysis on the nontriviality of the interevent times is also provided. Finally, two numerical examples are conducted to validate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Hanguang Su and Huaguang Zhang and He Jiang and Yinlei Wen},
  doi          = {10.1109/TNNLS.2019.2953613},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4254-4266},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized event-triggered adaptive control of discrete-time nonzero-sum games over wireless sensor-actuator networks with input constraints},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The cubic dynamic uncertain causality graph: A methodology
for temporal process modeling and diagnostic logic inference.
<em>TNNLS</em>, <em>31</em>(10), 4239–4253. (<a
href="https://doi.org/10.1109/TNNLS.2019.2953177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the demand for dynamic and highly reliable real-time fault diagnosis for complex systems, we extend the dynamic uncertain causality graph (DUCG) by proposing novel temporal causality modeling and reasoning methods. A new methodology, the Cubic DUCG, is therefore developed. It exploits an efficient scheme for compactly representing and accurately reasoning about the dynamic causalities in the system fault-spreading process. The Cubic DUCG is characterized by: 1) continuous generation of a causality graph that allows for causal connections penetrating among any number of time slices and discards the restrictive assumptions (about the underlying graph structure) upon which the existing research commonly relies; 2) a modeling scheme of complex causalities that includes dynamic negative feedback loops in a natural and intuitive manner; 3) a rigorous and reliable inference algorithm based on complete causalities that reflect real-time fault situations rather than on the cumulative aggregation of static time slices; and 4) some solutions to causality simplification and reduction, graphical transformation, and logical reasoning, for the sake of reducing the reasoning complexity. A series of fault diagnosis experiments on a nuclear power plant simulator verifies the accuracy, robustness, and efficiency of the proposed methodology.},
  archive      = {J_TNNLS},
  author       = {Chunling Dong and Qin Zhang},
  doi          = {10.1109/TNNLS.2019.2953177},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4239-4253},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The cubic dynamic uncertain causality graph: A methodology for temporal process modeling and diagnostic logic inference},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Communication-efficient federated deep learning with
layerwise asynchronous model update and temporally weighted aggregation.
<em>TNNLS</em>, <em>31</em>(10), 4229–4238. (<a
href="https://doi.org/10.1109/TNNLS.2019.2953131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning obtains a central model on the server by aggregating models trained locally on clients. As a result, federated learning does not require clients to upload their data to the server, thereby preserving the data privacy of the clients. One challenge in federated learning is to reduce the client-server communication since the end devices typically have very limited communication bandwidth. This article presents an enhanced federated learning technique by proposing an asynchronous learning strategy on the clients and a temporally weighted aggregation of the local models on the server. In the asynchronous learning strategy, different layers of the deep neural networks (DNNs) are categorized into shallow and deep layers, and the parameters of the deep layers are updated less frequently than those of the shallow layers. Furthermore, a temporally weighted aggregation strategy is introduced on the server to make use of the previously trained local models, thereby enhancing the accuracy and convergence of the central model. The proposed algorithm is empirically on two data sets with different DNNs. Our results demonstrate that the proposed asynchronous federated deep learning outperforms the baseline algorithm both in terms of communication cost and model accuracy.},
  archive      = {J_TNNLS},
  author       = {Yang Chen and Xiaoyan Sun and Yaochu Jin},
  doi          = {10.1109/TNNLS.2019.2953131},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4229-4238},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Communication-efficient federated deep learning with layerwise asynchronous model update and temporally weighted aggregation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive deep belief network with sparse restricted
boltzmann machines. <em>TNNLS</em>, <em>31</em>(10), 4217–4228. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep belief network (DBN) is an efficient learning model for unknown data representation, especially nonlinear systems. However, it is extremely hard to design a satisfactory DBN with a robust structure because of traditional dense representation. In addition, backpropagation algorithm-based fine-tuning tends to yield poor performance since its ease of being trapped into local optima. In this article, we propose a novel DBN model based on adaptive sparse restricted Boltzmann machines (AS-RBM) and partial least square (PLS) regression fine-tuning, abbreviated as ARP-DBN, to obtain a more robust and accurate model than the existing ones. First, the adaptive learning step size is designed to accelerate an RBM training process, and two regularization terms are introduced into such a process to realize sparse representation. Second, initial weight derived from AS-RBM is further optimized via layer-by-layer PLS modeling starting from the output layer to input one. Third, we present the convergence and stability analysis of the proposed method. Finally, our approach is tested on Mackey-Glass time-series prediction, 2-D function approximation, and unknown system identification. Simulation results demonstrate that it has higher learning accuracy and faster learning speed. It can be used to build a more robust model than the existing ones.},
  archive      = {J_TNNLS},
  author       = {Gongming Wang and Junfei Qiao and Jing Bi and Qing-Shan Jia and MengChu Zhou},
  doi          = {10.1109/TNNLS.2019.2952864},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4217-4228},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An adaptive deep belief network with sparse restricted boltzmann machines},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model for <span
class="math inline"><em>R</em>(<em>t</em>)</span> elements and <span
class="math inline"><em>R</em>(<em>t</em>)</span> -based
spike-timing-dependent plasticity with basic circuit examples.
<em>TNNLS</em>, <em>31</em>(10), 4206–4216. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike-timing-dependent plasticity (STDP) is a fundamental synaptic learning rule observed in biology that leads to numerous behavioral and cognitive outcomes. Emulating STDP in electronic spiking neural networks with high-density memristive synapses is, therefore, of significant interest. While one popular method involves pulse-shaping the spiking neuron output voltages, an alternative approach is outlined in this article. The proposed STDP implementation uses time-varying dynamic resistance [R(t)] elements to achieve local synaptic learning from spike-pair STDP, spike triplet STDP, and firing rates. The R(t) elements are connected to each neuron circuit, thereby maintaining synaptic density and leveraging voltage division as a means of altering synaptic weight (memristor voltage). Example R(t) elements with their corresponding behaviors are demonstrated through simulation. A three-input-two-output network using single-memristor synaptic connections and R(t) elements is also simulated. Network-level effects, such as nonspecific synaptic plasticity, are discussed. Finally, spatiotemporal pattern recognition (STPR) using R(t) elements is demonstrated in simulation.},
  archive      = {J_TNNLS},
  author       = {Robert C. Ivans and Sumedha Gandharava Dahl and Kurtis D. Cantley},
  doi          = {10.1109/TNNLS.2019.2952768},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4206-4216},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A model for $R(t)$ elements and $R(t)$ -based spike-timing-dependent plasticity with basic circuit examples},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-triggered adaptive neural network control for
nonstrict-feedback nonlinear time-delay systems with unknown control
directions. <em>TNNLS</em>, <em>31</em>(10), 4196–4205. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the event-triggered-based adaptive neural network control problem is studied for a class of nonlinear time-delay systems with nonstrict-feedback structures and unknown control directions. First, a compensation system is introduced to handle the input delay and an observer is also designed to estimate the unmeasurable states. Then, by employing the neural networks and the variable separation approach, the adaptive backstepping method is applied to control the nonlinear systems with nonstrict-feedback structures. By codesigning the adaptive controller and the triggering mechanism, the input-to-state stability (ISS) assumption with respect to the measurement error is removed. Finally, it is shown that the proposed event-triggered adaptive controller can ensure the semiglobal boundedness of all the states in the closed-loop systems.},
  archive      = {J_TNNLS},
  author       = {Jiali Ma and Shengyuan Xu and Qian Ma and Zhengqiang Zhang},
  doi          = {10.1109/TNNLS.2019.2952709},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4196-4205},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive neural network control for nonstrict-feedback nonlinear time-delay systems with unknown control directions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leader–follower bipartite output synchronization on signed
digraphs under adversarial factors via data-based reinforcement
learning. <em>TNNLS</em>, <em>31</em>(10), 4185–4195. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimal solution to the leader-follower bipartite output synchronization problem is proposed for heterogeneous multiagent systems (MASs) over signed digraphs in the presence of adversarial inputs in this article. For the MASs, the dynamics and dimensions of the followers are different. Distributed observers are first designed to estimate the leader&#39;s two-way state and output over signed digraphs. Then, the leader-follower bipartite output synchronization problem on signed graphs is translated into a conventional output distributed leader-follower problem over nonnegative graphs after the state transformation by using the information of followers and observers. The effect of adversarial inputs in sensors or actuators of agents is mitigated by designing the resilient H ∞ controller. A data-based reinforcement learning (RL) algorithm is proposed to obtain the optimal control law, which implies that the dynamics of the followers is not required. Finally, a simulation example is given to verify the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Qing Li and Lina Xia and Ruizhuo Song and Jian Liu},
  doi          = {10.1109/TNNLS.2019.2952611},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4185-4195},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Leader–Follower bipartite output synchronization on signed digraphs under adversarial factors via data-based reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate tensor completion via adaptive low-rank
representation. <em>TNNLS</em>, <em>31</em>(10), 4170–4184. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representation-based approaches that assume low-rank tensors and exploit their low-rank structure with appropriate prior models have underpinned much of the recent progress in tensor completion. However, real tensor data only approximately comply with the low-rank requirement in most cases, viz., the tensor consists of low-rank (e.g., principle part) as well as non-low-rank (e.g., details) structures, which limit the completion accuracy of these approaches. To address this problem, we propose an adaptive low-rank representation model for tensor completion that represents low-rank and non-low-rank structures of a latent tensor separately in a Bayesian framework. Specifically, we reformulate the CANDECOMP/PARAFAC (CP) tensor rank and develop a sparsity-induced prior for the low-rank structure that can be used to determine tensor rank automatically. Then, the non-low-rank structure is modeled using a mixture of Gaussians prior that is shown to be sufficiently flexible and powerful to inform the completion process for a variety of real tensor data. With these two priors, we develop a Bayesian minimum mean-squared error estimate framework for inference. The developed framework can capture the important distinctions between low-rank and non-low-rank structures, thereby enabling more accurate model, and ultimately, completion. For various applications, compared with the state-of-the-art methods, the proposed model yields more accurate completion results.},
  archive      = {J_TNNLS},
  author       = {Lei Zhang and Wei Wei and Qinfeng Shi and Chunhua Shen and Anton van den Hengel and Yanning Zhang},
  doi          = {10.1109/TNNLS.2019.2952427},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4170-4184},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accurate tensor completion via adaptive low-rank representation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relaxed stability criteria for neural networks with
time-varying delay using extended secondary delay partitioning and
equivalent reciprocal convex combination techniques. <em>TNNLS</em>,
<em>31</em>(10), 4157–4169. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates global asymptotic stability for neural networks (NNs) with time-varying delay, which is differentiable and uniformly bounded, and the delay derivative exists and is upper-bounded. First, we propose the extended secondary delay partitioning technique to construct the novel Lyapunov-Krasovskii functional, where both single-integral and double-integral state variables are considered, while the single-integral ones are only solved by the traditional secondary delay partitioning. Second, a novel free-weight matrix equality (FWME) is presented to resolve the reciprocal convex combination problem equivalently and directly without Schur complement, which eliminates the need of positive definite matrices, and is less conservative and restrictive compared with various improved reciprocal convex inequalities. Furthermore, by the present extended secondary delay partitioning, equivalent reciprocal convex combination technique, and Bessel-Legendre inequality, two different relaxed sufficient conditions ensuring global asymptotic stability for NNs are obtained, for time-varying delays, respectively, with unknown and known lower bounds of the delay derivative. Finally, two examples are given to illustrate the superiority and effectiveness of the presented method.},
  archive      = {J_TNNLS},
  author       = {Shenquan Wang and Wenchengyu Ji and Yulian Jiang and Derong Liu},
  doi          = {10.1109/TNNLS.2019.2952410},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4157-4169},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Relaxed stability criteria for neural networks with time-varying delay using extended secondary delay partitioning and equivalent reciprocal convex combination techniques},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CHIP: Channel-wise disentangled interpretation of deep
convolutional neural networks. <em>TNNLS</em>, <em>31</em>(10),
4143–4156. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of deep convolutional neural networks (DCNNs), in addition to achieving high accuracy, it becomes increasingly important to explain how DCNNs make their decisions. In this article, we propose a CHannel-wise disentangled InterPretation (CHIP) model for visual interpretations of DCNN predictions. The proposed model distills the class-discriminative importance of channels in DCNN by utilizing sparse regularization. We first introduce network perturbation to learn the CHIP model. The proposed model is capable to not only distill the global perspective knowledge from networks but also present class-discriminative visual interpretations for the predictions of networks. It is noteworthy that the CHIP model is able to interpret different layers of networks without retraining. By combining the distilled interpretation knowledge at different layers, we further propose the Refined CHIP visual interpretation that is both high-resolution and class-discriminative. Based on qualitative and quantitative experiments on different data sets and networks, the proposed model provides promising visual interpretations for network predictions in an image classification task compared with the existing visual interpretation methods. The proposed model also outperforms the related approaches in the ILSVRC 2015 weakly supervised localization task.},
  archive      = {J_TNNLS},
  author       = {Xinrui Cui and Dan Wang and Z. Jane Wang},
  doi          = {10.1109/TNNLS.2019.2952322},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4143-4156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CHIP: Channel-wise disentangled interpretation of deep convolutional neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mixed <span
class="math inline"><em>H</em><sub>2</sub>/<em>H</em><sub>∞</sub></span>
state estimation for discrete-time switched complex networks with random
coupling strengths through redundant channels. <em>TNNLS</em>,
<em>31</em>(10), 4130–4142. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the mixed H 2 /H ∞ state estimation problem for a class of discrete-time switched complex networks with random coupling strengths through redundant communication channels. A sequence of random variables satisfying certain probability distributions is employed to describe the stochasticity of the coupling strengths. A redundant-channel-based data transmission mechanism is adopted to enhance the reliability of the transmission channel from the sensor to the estimator. The purpose of the addressed problem is to design a state estimator for each node, such that the error dynamics achieves both the stochastic stability (with probability 1) and the prespecified mixed H 2 /H ∞ performance requirement. By using the switched system theory, an extensive stochastic analysis is carried out to derive the sufficient conditions ensuring the stochastic stability as well as the mixed H 2 /H ∞ performance index. The desired state estimator is also parameterized by resorting to the solutions to certain convex optimization problems. A numerical example is provided to illustrate the validity of the proposed estimation scheme.},
  archive      = {J_TNNLS},
  author       = {Yun Chen and Zidong Wang and Licheng Wang and Weiguo Sheng},
  doi          = {10.1109/TNNLS.2019.2952249},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4130-4142},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mixed $H_2/H_\infty$ state estimation for discrete-time switched complex networks with random coupling strengths through redundant channels},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditional generative denoising autoencoder.
<em>TNNLS</em>, <em>31</em>(10), 4117–4129. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a generative denoising autoencoder model that has an embedded data classifier in its architecture in order to take advantage of class-based discriminating features and produce better data samples. The proposed model is a conditional generative model and is sampled with a Markov chain Monte Carlo (MCMC) process according to a label that denotes the desired (or undesired) class or classes. In this sense, any chosen predefined class or characteristic may have a positive or negative effect on the image generation process, meaning that it can be instructed to be present or absent from the generated sample. We argue that allowing discriminative information in the form of feature detectors to be present in the latent representation of the autoencoder can be generally beneficial. This technique is an alternative approach to variational autoencoders (VAEs) that enforce a prior on the latent distribution. We further claim that supervised learning may be generally able to serve unsupervised learning through an interaction between the two paradigms. However, the extreme majority of research done on the interaction of the two learning regimes has the goal of using unsupervised learning to improve supervised learning. In this article, we explore the two learning paradigms&#39; interaction in the opposite direction.},
  archive      = {J_TNNLS},
  author       = {Savvas Karatsiolis and Christos N. Schizas},
  doi          = {10.1109/TNNLS.2019.2952203},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4117-4129},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conditional generative denoising autoencoder},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Event-triggered exponential synchronization for
complex-valued memristive neural networks with time-varying delays.
<em>TNNLS</em>, <em>31</em>(10), 4104–4116. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article solves the event-triggered exponential synchronization problem for a class of complex-valued memristive neural networks with time-varying delays. The drive-response complex-valued memristive neural networks are translated into two real-valued memristive neural networks through the method of separating the complex-valued memristive neural networks into real and imaginary parts. In order to reduce the information exchange frequency between the sensor and the controller, a novel event-triggered mechanism with the event-triggering functions is introduced in wireless communication networks. Some sufficient conditions are established to achieve the event-triggered exponential synchronization for drive-response complex-valued memristive neural networks with time-varying delays. In addition, to guarantee that the Zeno behavior cannot occur, a positive lower bound for the interevent times is explicitly derived. Finally, numerical simulations are provided to illustrate the effectiveness and superiority of the obtained theoretical results.},
  archive      = {J_TNNLS},
  author       = {Xiaofan Li and Wenbing Zhang and Jian-An Fang and Huiyuan Li},
  doi          = {10.1109/TNNLS.2019.2952186},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4104-4116},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered exponential synchronization for complex-valued memristive neural networks with time-varying delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An active repetitive learning control method for lateral
suspension systems of high-speed trains. <em>TNNLS</em>,
<em>31</em>(10), 4094–4103. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel perspective to improve the ride quality of high-speed trains (HSTs), namely, by virtue of the periodicity of lateral dynamics to suppress the lateral vibration of HST. To resolve the contradiction between the complex HST model and the effective controller design, a simplified three-degrees-of-freedom (3-DOF) quarter-vehicle model is first employed for controller design, while a 17-DOF full-vehicle model is built for efficiency verification, where periodic and random track irregularities are considered, respectively. An active repetitive learning control (RLC) method is proposed to achieve the periodic tracking control, where the learning convergence is proved rigorously in a Lyapunov way. The configuration of RLC-based lateral suspensions is economical in the sense that only four actuators and six sensors are needed. It is verified by simulation that, compared with the dynamic matrix controller, the proposed RLC controller has greatly reduced the lateral vibration of a vehicle body, especially the lateral acceleration in the frequency range of (0, 3] Hz to which human body is strongly sensitive.},
  archive      = {J_TNNLS},
  author       = {Deqing Huang and Chunrong Chen and Tengfei Huang and Duo Zhao and Qichao Tang},
  doi          = {10.1109/TNNLS.2019.2952175},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4094-4103},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An active repetitive learning control method for lateral suspension systems of high-speed trains},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural output-feedback controller design of
switched nonlower triangular nonlinear systems with time delays.
<em>TNNLS</em>, <em>31</em>(10), 4084–4093. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the issue of adaptive neural output-feedback controller design for a class of uncertain switched time-delay nonlinear systems with nonlower triangular structure. The prominent contribution of this article is that the delay-dependent stability criterion of nonswitched nonlinear systems is successfully extended to that of switched nonlower triangular nonlinear systems. The design algorithm is listed as follows. First, a switched state observer is designed such that the error dynamic system can be generated. Second, neural networks, adaptive backstepping technique, and variable separation method are, respectively, applied to construct a common controller for all subsystems, in which the Lyapunov-Krasovskii functionals are deliberately constructed such that the average dwell-time scheme can be employed to guarantee the stability and performance of the closed-loop system, despite the existence of time delays. Third, the stability analysis process confirms in detail that all the variables of the closed-loop system are semiglobally uniformly ultimately bounded. Finally, simulation study is given to show the validity of the proposed control approach.},
  archive      = {J_TNNLS},
  author       = {Ben Niu and Ding Wang and Ming Liu and Xinmin Song and Huanqing Wang and Peiyong Duan},
  doi          = {10.1109/TNNLS.2019.2952108},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4084-4093},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural output-feedback controller design of switched nonlower triangular nonlinear systems with time delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information losses in neural classifiers from sampling.
<em>TNNLS</em>, <em>31</em>(10), 4073–4083. (<a
href="https://doi.org/10.1109/TNNLS.2019.2952029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the subject of information losses arising from the finite data sets used in the training of neural classifiers. It proves a relationship between such losses as the product of the expected total variation of the estimated neural model with the information about the feature space contained in the hidden representation of that model. It then bounds this expected total variation as a function of the size of randomly sampled data sets in a fairly general setting, and without bringing in any additional dependence on model complexity. It ultimately obtains bounds on information losses that are less sensitive to input compression and in general much smaller than existing bounds. This article then uses these bounds to explain some recent experimental findings of information compression in neural networks that cannot be explained by previous work. Finally, this article shows that not only are these bounds much smaller than existing ones, but they also correspond well with experiments.},
  archive      = {J_TNNLS},
  author       = {Brandon Foggo and Nanpeng Yu and Jie Shi and Yuanqi Gao},
  doi          = {10.1109/TNNLS.2019.2952029},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4073-4083},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Information losses in neural classifiers from sampling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quasi-synchronization and bifurcation results on
fractional-order quaternion-valued neural networks. <em>TNNLS</em>,
<em>31</em>(10), 4063–4072. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the quasi-synchronization and Hopf bifurcation issues are investigated for the fractional-order quaternion-valued neural networks (QVNNs) with time delay in the presence of parameter mismatches. On the basis of noncommutativity property of quaternion multiplication results, the quaternion network has been split as four real-valued networks. A synchronization theorem for fractional-order QVNNs is derived by employing suitable Lyapunov functional candidate; furthermore, the bifurcation behavior of the hub-structured fractional-order QVNNs with time delay has been investigated. Finally, two numerical examples are provided to demonstrate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Udhayakumar Kandasamy and Xiaodi Li and Rakkiyappan Rajan},
  doi          = {10.1109/TNNLS.2019.2951846},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4063-4072},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quasi-synchronization and bifurcation results on fractional-order quaternion-valued neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A network framework for small-sample learning.
<em>TNNLS</em>, <em>31</em>(10), 4049–4062. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-sample learning involves training a neural network on a small-sample data set. An expansion of the training set is a common way to improve the performance of neural networks in small-sample learning tasks. However, improper constraints in expanding training data will reduce the performance of the neural networks. In this article, we present certain conditions for incorporation of additional training data. According to these conditions, we propose a neural network framework for self-training using self-generated data called small-sample learning network (SSLN). The SSLN consists of two parts: the expression learning network and the sample recall generative network, both of which are constructed based on restricted Boltzmann machine (RBM). We show that this SSLN can converge as well as the RBM. Moreover, the experiment results on MNIST Digit, SVHN, CIFAR10, and STL-10 data sets reveal the superiority of the SSLN over other models.},
  archive      = {J_TNNLS},
  author       = {Dongbo Liu and Zhenan He and Dongdong Chen and Jiancheng Lv},
  doi          = {10.1109/TNNLS.2019.2951803},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4049-4062},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A network framework for small-sample learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realizing data features by deep nets. <em>TNNLS</em>,
<em>31</em>(10), 4036–4048. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the power of deep neural networks (deep nets) in realizing data features. Based on refined covering number estimates, we find that, to realize data features such as the locality, rotation invariance, and manifold structure, deep nets essentially improve the performances of shallow neural networks (shallow nets) without requiring additional capacity costs. Conversely, to realize some data features, such as the smoothness, we show that deep nets perform similar as shallow nets, provided the depth is not extremely large. Both sides show the advantages and limitations of deep nets in realizing data features and demonstrate that deep nets are not always better than shallow nets.},
  archive      = {J_TNNLS},
  author       = {Zheng-Chu Guo and Lei Shi and Shao-Bo Lin},
  doi          = {10.1109/TNNLS.2019.2951788},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4036-4048},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Realizing data features by deep nets},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence analysis of saturated iterative learning control
systems with locally lipschitz nonlinearities. <em>TNNLS</em>,
<em>31</em>(10), 4025–4035. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the robust trajectory tracking problem of iterative learning control (ILC) for uncertain nonlinear systems is considered, and the effects from locally Lipschitz nonlinearities, input saturations, and nonzero system relative degrees are treated. A saturated ILC algorithm is given, with the convergence analysis exploited using a composite energy function-based approach. It is shown that the tracking error can be guaranteed to converge both pointwisely and uniformly. Furthermore, the input updating signal can be ensured to eventually satisfy the input saturation requirements with increasing iterations. Two examples are given to demonstrate the validity of saturated ILC for systems with the relative degree of one, input saturation, and locally Lipschitz nonlinearity.},
  archive      = {J_TNNLS},
  author       = {Jingyao Zhang and Deyuan Meng},
  doi          = {10.1109/TNNLS.2019.2951752},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4025-4035},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convergence analysis of saturated iterative learning control systems with locally lipschitz nonlinearities},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural-network-based adaptive event-triggered control for
spacecraft attitude tracking. <em>TNNLS</em>, <em>31</em>(10),
4015–4024. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of attitude tracking control for spacecraft with limited communication rate is addressed in this article. To reduce the communication burden, an adaptive event-triggered control scheme is proposed. In the control scheme, only the sampling states at the event-triggering instants are sent to the control module, which can considerably decrease the data transmission rate. To address the inertia uncertainties and external disturbances, a radial basis function neural network (NN) is introduced. The bound of the uncertainties and disturbances is estimated for the proposed control scheme, which can simplify the NN and reduce the computation. Since the event-triggered error signal is discontinuous due to the event-triggered mechanism, the closed-loop system is formulated as an impulsive dynamical system to obtain the stability properties of the system. Finally, simulation results are given to demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Weixing Liu and Yunhai Geng and Baolin Wu and Danwei Wang},
  doi          = {10.1109/TNNLS.2019.2951732},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4015-4024},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive event-triggered control for spacecraft attitude tracking},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based event-triggered tracking control of
underactuated surface vessels with minimum learning parameters.
<em>TNNLS</em>, <em>31</em>(10), 4001–4014. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the model-based event-triggered control (ETC) for the tracking activity of the underactuated surface vessel (USV). Following this ideology, the continuous acquisition of states is no longer needed, and the communication traffic is reduced in the channel of sensor to controller. The control laws are fabricated in the frame of an adaptive model, which is renewed with the states of the original system whenever the triggering condition is violated. In the scheme, both internal and external uncertainties are approximated by the neural networks (NNs). To decrease the computing complexity, the minimum learning parameters (MLPs) are involved both in the adaptive model and the derived controller. The adaptive laws of only two MLPs are devised, and their updating only happens at triggering instants. Using the MLPs, an adaptive triggering condition is further derived. To avoid the “Zeno” phenomenon in small tracking errors, a dead-zone operator is designed for the triggering condition. Furthermore, we incorporate the dynamic surface control (DSC) into the controller design, such that the jumping of virtual control laws at triggering instants is smoothed and the problem of “complexity explosion” is circumvented. Through the techniques of the impulsive dynamic system and the direct Lyapunov function, the parameter setting for the DSC is derived to guarantee the semiglobal uniformly ultimate boundedness (SGUUB) of all the error signals in the closed-loop system. Finally, the effectiveness of the proposed scheme is validated through the simulation.},
  archive      = {J_TNNLS},
  author       = {Yingjie Deng and Xianku Zhang and Namkyun Im and Guoqing Zhang and Qiang Zhang},
  doi          = {10.1109/TNNLS.2019.2951709},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4001-4014},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-based event-triggered tracking control of underactuated surface vessels with minimum learning parameters},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Property-constrained dual learning for video summarization.
<em>TNNLS</em>, <em>31</em>(10), 3989–4000. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization is the technique to condense large-scale videos into summaries composed of key-frames or key-shots so that the viewers can browse the video content efficiently. Recently, supervised approaches have achieved great success by taking advantages of recurrent neural networks (RNNs). Most of them focus on generating summaries by maximizing the overlap between the generated summary and the ground truth. However, they neglect the most critical principle, i.e., whether the viewer can infer the original video content from the summary. As a result, existing approaches cannot preserve the summary quality well and usually demand large amounts of training data to reduce overfitting. In our view, video summarization has two tasks, i.e., generating summaries from videos and inferring the original content from summaries. Motivated by this, we propose a dual learning framework by integrating the summary generation (primal task) and video reconstruction (dual task) together, which targets to reward the summary generator under the assistance of the video reconstructor. Moreover, to provide more guidance to the summary generator, two property models are developed to measure the representativeness and diversity of the generated summary. Practically, experiments on four popular data sets (SumMe, TVsum, OVP, and YouTube) have demonstrated that our approach, with compact RNNs as the summary generator, using less training data, and even in the unsupervised setting, can get comparable performance with those supervised ones adopting more complex summary generators and trained on more annotated data.},
  archive      = {J_TNNLS},
  author       = {Bin Zhao and Xuelong Li and Xiaoqiang Lu},
  doi          = {10.1109/TNNLS.2019.2951680},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3989-4000},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Property-constrained dual learning for video summarization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GrAMME: Semisupervised learning using multilayered graph
attention models. <em>TNNLS</em>, <em>31</em>(10), 3977–3988. (<a
href="https://doi.org/10.1109/TNNLS.2019.2948797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data analysis pipelines are becoming increasingly complex due to the presence of multiview information sources. While graphs are effective in modeling complex relationships, in many scenarios, a single graph is rarely sufficient to succinctly represent all interactions, and hence, multilayered graphs have become popular. Though this leads to richer representations, extending solutions from the single-graph case is not straightforward. Consequently, there is a strong need for novel solutions to solve classical problems, such as node classification, in the multilayered case. In this article, we consider the problem of semisupervised learning with multilayered graphs. Though deep network embeddings, e.g., DeepWalk, are widely adopted for community discovery, we argue that feature learning with random node attributes, using graph neural networks, can be more effective. To this end, we propose to use attention models for effective feature learning and develop two novel architectures, GrAMME-SG and GrAMME-Fusion, that exploit the interlayer dependences for building multilayered graph embeddings. Using empirical studies on several benchmark data sets, we evaluate the proposed approaches and demonstrate significant performance improvements in comparison with the state-of-the-art network embedding strategies. The results also show that using simple random features is an effective choice, even in cases where explicit node attributes are not available.},
  archive      = {J_TNNLS},
  author       = {Uday Shankar Shanthamallu and Jayaraman J. Thiagarajan and Huan Song and Andreas Spanias},
  doi          = {10.1109/TNNLS.2019.2948797},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3977-3988},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GrAMME: Semisupervised learning using multilayered graph attention models},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample balancing for deep learning-based visual recognition.
<em>TNNLS</em>, <em>31</em>(10), 3962–3976. (<a
href="https://doi.org/10.1109/TNNLS.2019.2947789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample balancing includes sample selection and sample reweighting. Sample selection aims to remove some bad samples that may lead to bad local optima. Sample reweighting aims to assign optimal weights to samples to improve performance. In this article, we integrate a sample selection method based on self-paced learning into deep learning frameworks and study the influence of different sample selection strategies on training deep networks. In addition, most of the existing sample reweighting methods mainly take per-class sample number as a metric, which does not fully consider sample qualities. To improve the performance, we propose a novel metric based on the multiview semantic encoders to reweight the samples more appropriately. Then, we propose an optimization mechanism to embed sample weights into loss functions of deep networks, which can be trained in end-to-end manners. We conduct experiments on the CIFAR data set and the ImageNet data set. The experimental results demonstrate that our proposed sample balancing method can improve the performances of deep learning methods in several visual recognition tasks.},
  archive      = {J_TNNLS},
  author       = {Xin Chen and Jian Weng and Weiqi Luo and Wei Lu and Huimin Wu and Jiaming Xu and Qi Tian},
  doi          = {10.1109/TNNLS.2019.2947789},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3962-3976},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sample balancing for deep learning-based visual recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online topology learning by a gaussian membership-based
self-organizing incremental neural network. <em>TNNLS</em>,
<em>31</em>(10), 3947–3961. (<a
href="https://doi.org/10.1109/TNNLS.2019.2947658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to extract useful information from data streams, incremental learning has been introduced in more and more data mining algorithms. For instance, a self-organizing incremental neural network (SOINN) has been proposed to extract a topological structure that consists of one or more neural networks to closely reflect the data distribution of data streams. However, SOINN has the tradeoff between deleting previously learned nodes and inserting new nodes, i.e., the stability-plasticity dilemma. Therefore, it is not guaranteed that the topological structure obtained by the SOINN will closely represent data distribution. For solving the stability-plasticity dilemma, we propose a Gaussian membership-based SOINN (Gm-SOINN). Unlike other SOINN-based methods that allow only one node to be identified as a “winner” (the nearest node), the Gm-SOINN uses a Gaussian membership to indicate to which degree the node is a winner. Hence, the Gm-SOINN avoids the topological structure that cannot represent the data distribution because previously learned nodes overly deleted or noisy nodes inserted. In addition, an evolving Gaussian mixture model is integrated into the Gm-SOINN to estimate the density distribution of nodes, thereby avoiding the wrong connection between two nodes. Experiments involving both artificial and real-world data sets indicate that our proposed Gm-SOINN achieves better performance than other topology learning methods.},
  archive      = {J_TNNLS},
  author       = {Hang Yu and Jie Lu and Guangquan Zhang},
  doi          = {10.1109/TNNLS.2019.2947658},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3947-3961},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online topology learning by a gaussian membership-based self-organizing incremental neural network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Major–minor long short-term memory for word-level language
model. <em>TNNLS</em>, <em>31</em>(10), 3932–3946. (<a
href="https://doi.org/10.1109/TNNLS.2019.2947563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language model (LM) plays an important role in natural language processing (NLP) systems, such as machine translation, speech recognition, learning token embeddings, natural language generation, and text classification. Recently, the multilayer long short-term memory (LSTM) models have been demonstrated to achieve promising performance on word-level language modeling. For each LSTM layer, larger hidden size usually means more diverse semantic features, which enables the LM to perform better. However, we have observed that when a certain LSTM layer reaches a sufficiently large scale, the promotion of overall effect will slow down, as its hidden size increases. In this article, we analyze that an important factor leading to this phenomenon is the high correlation between the newly extended hidden states and the original hidden states, which hinders diverse feature expression of the LSTM. As a result, when the scale is large enough, simply lengthening the LSTM hidden states will cost tremendous extra parameters but has little effect. We propose a simple yet effective improvement on each LSTM layer consisting of a large-scale Major LSTM and a small-scale Minor LSTM to break the high correlation between the two parts of hidden states, which we call Major–Minor LSTMs (MMLSTMs). In experiments, we demonstrate the LM with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) data sets and outperforms the baseline by 3.3 points in perplexity on WikiText-103 data set without increasing model parameter counts.},
  archive      = {J_TNNLS},
  author       = {Kai Shuang and Rui Li and Mengyu Gu and Jonathan Loo and Sen Su},
  doi          = {10.1109/TNNLS.2019.2947563},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3932-3946},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Major–Minor long short-term memory for word-level language model},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pulsewidth modulation-based algorithm for spike phase
encoding and decoding of time-dependent analog data. <em>TNNLS</em>,
<em>31</em>(10), 3920–3931. (<a
href="https://doi.org/10.1109/TNNLS.2019.2947380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new spike encoding and decoding algorithm for analog data. The algorithm uses the pulsewidth modulation principles to achieve a high reconstruction accuracy of the signal, along with a high level of data compression. Two benchmark data sets are used to illustrate the method: stock index time series and human voice data. Applications of the method for spiking neural network (SNN) modeling and neuromorphic implementations are discussed. The proposed method would allow the development of new applications of SNNs as regression techniques for predictive time-series modeling.},
  archive      = {J_TNNLS},
  author       = {Ander Arriandiaga and Eva Portillo and Josafath Israel Espinosa-Ramos and Nikola K. Kasabov},
  doi          = {10.1109/TNNLS.2019.2947380},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3920-3931},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pulsewidth modulation-based algorithm for spike phase encoding and decoding of time-dependent analog data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiclass probabilistic classification vector machine.
<em>TNNLS</em>, <em>31</em>(10), 3906–3919. (<a
href="https://doi.org/10.1109/TNNLS.2019.2947309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probabilistic classification vector machine (PCVM) synthesizes the advantages of both the support vector machine and the relevant vector machine, delivering a sparse Bayesian solution to classification problems. However, the PCVM is currently only applicable to binary cases. Extending the PCVM to multiclass cases via heuristic voting strategies such as one-vs-rest or one-vs-one often results in a dilemma where classifiers make contradictory predictions, and those strategies might lose the benefits of probabilistic outputs. To overcome this problem, we extend the PCVM and propose a multiclass PCVM (mPCVM). Two learning algorithms, i.e., one top-down algorithm and one bottom-up algorithm, have been implemented in the mPCVM. The top-down algorithm obtains the maximum a posteriori (MAP) point estimates of the parameters based on an expectation-maximization algorithm, and the bottom-up algorithm is an incremental paradigm by maximizing the marginal likelihood. The superior performance of the mPCVMs, especially when the investigated problem has a large number of classes, is extensively evaluated on the synthetic and benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Shengfei Lyu and Xing Tian and Yang Li and Bingbing Jiang and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2019.2947309},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3906-3919},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiclass probabilistic classification vector machine},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic analysis of facial expressions based on deep
covariance trajectories. <em>TNNLS</em>, <em>31</em>(10), 3892–3905. (<a
href="https://doi.org/10.1109/TNNLS.2019.2947244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new approach for facial expression recognition (FER) using deep covariance descriptors. The solution is based on the idea of encoding local and global deep convolutional neural network (DCNN) features extracted from still images, in compact local and global covariance descriptors. The space geometry of the covariance matrices is that of symmetric positive definite (SPD) matrices. By conducting the classification of static facial expressions using a support vector machine (SVM) with a valid Gaussian kernel on the SPD manifold, we show that deep covariance descriptors are more effective than the standard classification with fully connected layers and softmax. Besides, we propose a completely new and original solution to model the temporal dynamic of facial expressions as deep trajectories on the SPD manifold. As an extension of the classification pipeline of covariance descriptors, we apply SVM with valid positive definite kernels derived from global alignment for deep covariance trajectories classification. By performing extensive experiments on the Oulu-CASIA, CK+, static facial expression in the wild (SFEW), and acted facial expressions in the wild (AFEW) data sets, we show that both the proposed static and dynamic approaches achieve the state-of-the-art performance for FER outperforming many recent approaches.},
  archive      = {J_TNNLS},
  author       = {Naima Otberdout and Anis Kacem and Mohamed Daoudi and Lahoucine Ballihi and Stefano Berretti},
  doi          = {10.1109/TNNLS.2019.2947244},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3892-3905},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic analysis of facial expressions based on deep covariance trajectories},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximum correntropy criterion-based robust semisupervised
concept factorization for image representation. <em>TNNLS</em>,
<em>31</em>(10), 3877–3891. (<a
href="https://doi.org/10.1109/TNNLS.2019.2947156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept factorization (CF) has shown its great advantage for both clustering and data representation and is particularly useful for image representation. Compared with nonnegative matrix factorization (NMF), CF can be applied to data containing negative values. However, the performance of CF method and its extensions will degenerate a lot due to the negative effects of outliers, and CF is an unsupervised method that cannot incorporate label information. In this article, we propose a novel CF method, with a novel model built based on the maximum correntropy criterion (MCC). In order to capture the local geometry information of data, our method integrates the robust adaptive embedding and CF into a unified framework. The label information is utilized in the adaptive learning process. Furthermore, an iterative strategy based on the accelerated block coordinate update is proposed. The convergence property of the proposed method is analyzed to ensure that the algorithm converges to a reliable solution. The experimental results on four real-world image data sets show that the new method can almost always filter out the negative effects of the outliers and outperform several state-of-the-art image representation methods.},
  archive      = {J_TNNLS},
  author       = {Nan Zhou and Badong Chen and Yuanhua Du and Tao Jiang and Jun Liu and Yangyang Xu},
  doi          = {10.1109/TNNLS.2019.2947156},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3877-3891},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Maximum correntropy criterion-based robust semisupervised concept factorization for image representation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel separating hyperplane classification framework to
unify nearest-class-model methods for high-dimensional data.
<em>TNNLS</em>, <em>31</em>(10), 3866–3876. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we establish a novel separating hyperplane classification (SHC) framework to unify three nearest-class-model methods for high-dimensional data: the nearest subspace method (NSM), the nearest convex hull method (NCHM), and the nearest convex cone method (NCCM). Nearest-class-model methods are an important paradigm for the classification of high-dimensional data. We first introduce the three nearest-class-model methods and then conduct dual analysis for theoretically investigating them, to understand deeply their underlying classification mechanisms. A new theorem for the dual analysis of NCCM is proposed in this article by discovering the relationship between a convex cone and its polar cone. We then establish the new SHC framework to unify the nearest-class-model methods based on the theoretical results. One important application of this new SHC framework is to help explain empirical classification results: why one class model has a better performance than others on certain data sets. Finally, we propose a new nearest-class-model method, the soft NCCM, under the novel SHC framework to solve the overlapping class model problem. For illustrative purposes, we empirically demonstrate the significance of our SHC framework and the soft NCCM through two types of typical real-world high-dimensional data: the spectroscopic data and the face image data.},
  archive      = {J_TNNLS},
  author       = {Rui Zhu and Ziyu Wang and Naoya Sogi and Kazuhiro Fukui and Jing-Hao Xue},
  doi          = {10.1109/TNNLS.2019.2946967},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3866-3876},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel separating hyperplane classification framework to unify nearest-class-model methods for high-dimensional data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A self-learning immune co-evolutionary network for multiple
escaping targets search with random observable conditions.
<em>TNNLS</em>, <em>31</em>(10), 3853–3865. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The search for multiple escaping targets is a significant issue of cooperative control in multi-agent systems since targets consciously seek to avoid being captured. Moreover, the assumption of continuous observations in existing works is not always suitable due to the limit of measuring equipment and uncertain movement of targets. Therefore, the problem with searching for escaping targets, which can be more aptly labeled “multiple escaping-targets search with random observation conditions” (MESROC), is difficult to address by conventional methods. Inspired by machine learning and the immune response mechanism of human bodies, a self-learning immune co-evolutionary network (SLICEN) is proposed. The SLICEN consists mainly of an immune cellular network (ICN) and an immune learning algorithm (ILA). The ICN provides feasible solutions to MESROC. Different kinds of network models are introduced to work as an ICN, such as convolutional neural networks, extreme learning machines, and support vector machines. The ILA evaluates the performance of feasible solutions and selects the optimal ones to further strengthen ICN reversely. Solutions are repeatedly improved through the co-evolution of ICN and ILA. An essential distinction to conventional machine learning approaches is that SLICEN works well without training samples. Simulations and comparisons demonstrate that patterns of advanced cooperative behavior among searchers function properly. SLICEN is an efficient method for solving MESROC.},
  archive      = {J_TNNLS},
  author       = {Chenwei Zhao and Feng Li and Kuangrong Hao and Lihong Ren and Tong Wang},
  doi          = {10.1109/TNNLS.2019.2946913},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3853-3865},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A self-learning immune co-evolutionary network for multiple escaping targets search with random observable conditions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subject-independent brain–computer interfaces based on deep
convolutional neural networks. <em>TNNLS</em>, <em>31</em>(10),
3839–3852. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a brain-computer interface (BCI) system, a calibration procedure is required for each individual user before he/she can use the BCI. This procedure requires approximately 20-30 min to collect enough data to build a reliable decoder. It is, therefore, an interesting topic to build a calibration-free, or subject-independent, BCI. In this article, we construct a large motor imagery (MI)-based electroencephalography (EEG) database and propose a subject-independent framework based on deep convolutional neural networks (CNNs). The database is composed of 54 subjects performing the left- and right-hand MI on two different days, resulting in 21 600 trials for the MI task. In our framework, we formulated the discriminative feature representation as a combination of the spectral-spatial input embedding the diversity of the EEG signals, as well as a feature representation learned from the CNN through a fusion technique that integrates a variety of discriminative brain signal patterns. To generate spectral-spatial inputs, we first consider the discriminative frequency bands in an information-theoretic observation model that measures the power of the features in two classes. From discriminative frequency bands, spectral-spatial inputs that include the unique characteristics of brain signal patterns are generated and then transformed into a covariance matrix as the input to the CNN. In the process of feature representations, spectral-spatial inputs are individually trained through the CNN and then combined by a concatenation fusion technique. In this article, we demonstrate that the classification accuracy of our subject-independent (or calibration-free) model outperforms that of subject-dependent models using various methods [common spatial pattern (CSP), common spatiospectral pattern (CSSP), filter bank CSP (FBCSP), and Bayesian spatio-spectral filter optimization (BSSFO)].},
  archive      = {J_TNNLS},
  author       = {O-Yeon Kwon and Min-Ho Lee and Cuntai Guan and Seong-Whan Lee},
  doi          = {10.1109/TNNLS.2019.2946869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3839-3852},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Subject-independent Brain–Computer interfaces based on deep convolutional neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressing deep neural networks with sparse matrix
factorization. <em>TNNLS</em>, <em>31</em>(10), 3828–3838. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep neural networks (DNNs) are usually overparameterized and composed of a large number of learnable parameters. One of a few effective solutions attempts to compress DNN models via learning sparse weights and connections. In this article, we follow this line of research and present an alternative framework of learning sparse DNNs, with the assistance of matrix factorization. We provide an underlying principle for substituting the original parameter matrices with the multiplications of highly sparse ones, which constitutes the theoretical basis of our method. Experimental results demonstrate that our method substantially outperforms previous states of the arts for compressing various DNNs, giving rich empirical evidence in support of its effectiveness. It is also worth mentioning that, unlike many other works that focus on feedforward networks like multi-layer perceptrons and convolutional neural networks only, we also evaluate our method on a series of recurrent networks in practice.},
  archive      = {J_TNNLS},
  author       = {Kailun Wu and Yiwen Guo and Changshui Zhang},
  doi          = {10.1109/TNNLS.2019.2946636},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3828-3838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Compressing deep neural networks with sparse matrix factorization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Deep learning method based on gated recurrent unit and
variational mode decomposition for short-term wind power interval
prediction. <em>TNNLS</em>, <em>31</em>(10), 3814–3827. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind power interval prediction (WPIP) plays an increasingly important role in evaluations of the uncertainty of wind power and becomes necessary for managing and planning power systems. However, the intermittent and fluctuating characteristics of wind power mean that high-quality prediction intervals (PIs) production is a challenging problem. In this article, we propose a novel hybrid model for the WPIP based on the gated recurrent unit (GRU) neural networks and variational mode decomposition (VMD). In the hybrid model, VMD is employed to decompose complex wind power data into simplified modes. Basic GRU prediction models, comprising a GRU input layer, multiple fully connected layers, and a rank-ordered terminal layer, are then trained for each mode to produce PIs, which are combined to obtain final PIs. In addition, an adaptive optimization method based on constructed intervals (CIs) is proposed to build high-quality training labels for supervised learning with the hybrid model. Several numerical experiments were implemented to validate the effectiveness of the proposed method. The results indicate that the proposed method performs better than the traditional interval prediction models with much higher quality PIs, and it requires less training time.},
  archive      = {J_TNNLS},
  author       = {Ruoheng Wang and Chaoshun Li and Wenlong Fu and Geng Tang},
  doi          = {10.1109/TNNLS.2019.2946414},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3814-3827},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning method based on gated recurrent unit and variational mode decomposition for short-term wind power interval prediction},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured optimal graph-based clustering with flexible
embedding. <em>TNNLS</em>, <em>31</em>(10), 3801–3813. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, the duality of high-dimensional data is widespread. The coclustering method has been widely used because they can exploit the co-occurring structure between samples and features. In fact, most of the existing coclustering methods cluster the graphs in the original data matrix. However, these methods fail to output an affinity graph with an explicit cluster structure and still call for the postprocessing step to obtain the final clustering results. In addition, these methods are difficult to find a good projection direction to complete the clustering task on high-dimensional data. In this article, we modify the flexible manifold embedding theory and embed it into the bipartite spectral graph partition. Then, we propose a new method called structured optimal graph-based clustering with flexible embedding (SOGFE). The SOGFE method can learn an affinity graph with an optimal and explicit clustering structure and does not require any postprocessing step. Additionally, the SOGFE method can learn a suitable projection direction to map high-dimensional data to a low-dimensional subspace. We perform extensive experiments on two synthetic data sets and seven benchmark data sets. The experimental results verify the superiority, robustness, and good projection direction selection ability of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Pengzhen Ren and Yun Xiao and Xiaojun Chang and Mahesh Prakash and Feiping Nie and Xin Wang and Xiaojiang Chen},
  doi          = {10.1109/TNNLS.2019.2946329},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3801-3813},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structured optimal graph-based clustering with flexible embedding},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). State-saturated recursive filter design for stochastic
time-varying nonlinear complex networks under deception attacks.
<em>TNNLS</em>, <em>31</em>(10), 3788–3800. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article tackles the recursive filtering problem for a class of stochastic nonlinear time-varying complex networks (CNs) suffering from both the state saturations and the deception attacks. The nonlinear inner coupling and the state saturations are taken into account to characterize the nonlinear nature of CNs. From the defender&#39;s perspective, the randomly occurring deception attack is governed by a set of Bernoulli binary distributed white sequence with a given probability. The objective of the addressed problem is to design a state-saturated recursive filter such that, in the simultaneous presence of the state saturations and the randomly occurring deception attacks, a certain upper bound is guaranteed on the filtering error covariance, and such an upper bound is then minimized at each time instant. By employing the induction method, an upper bound on the filtering error variance is first constructed in terms of the solutions to a set of matrix difference equations. Subsequently, the filter parameters are appropriately designed to minimize such an upper bound. Finally, a numerical simulation example is provided to demonstrate the feasibility and usefulness of the proposed filtering scheme.},
  archive      = {J_TNNLS},
  author       = {Bo Shen and Zidong Wang and Dong Wang and Qi Li},
  doi          = {10.1109/TNNLS.2019.2946290},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3788-3800},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {State-saturated recursive filter design for stochastic time-varying nonlinear complex networks under deception attacks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Observer-based impulsive synchronization for neural networks
with uncertain exchanging information. <em>TNNLS</em>, <em>31</em>(10),
3777–3787. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates synchronization for a group of discrete-time neural networks (NNs) with the uncertain exchanging information, which is caused by the uncertain connection weights among the NNs nodes, and they are transformed into a norm-bounded uncertain Laplacian matrix. Distributed impulsive observers, which possess the advantage of reducing the communication load among NNs nodes, are designed to observe the NNs state. The impulsive controller is proposed to improve the efficiency of the controller. An impulsive augmented error system (IAES) is obtained based on the matrix Kronecker product. A sufficient condition is established to ensure synchronization of the group of NNs by proving the stability of the IAES. An iterative algorithm is given to obtain a suboptimal allowed interval of the impulsive signal, and the corresponding gains of the observer and the controller are derived. The developed result is illustrated by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Hongxia Rao and Fen Liu and Hui Peng and Yong Xu and Renquan Lu},
  doi          = {10.1109/TNNLS.2019.2946151},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {3777-3787},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based impulsive synchronization for neural networks with uncertain exchanging information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(9), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.3016809},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble pruning based on objection maximization with a
general distributed framework. <em>TNNLS</em>, <em>31</em>(9),
3766–3774. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble pruning, selecting a subset of individual learners from an original ensemble, alleviates the deficiencies of ensemble learning on the cost of time and space. Accuracy and diversity serve as two crucial factors, while they usually conflict with each other. To balance both of them, we formalize the ensemble pruning problem as an objection maximization problem based on information entropy. Then we propose an ensemble pruning method, including a centralized version and a distributed version, in which the latter is to speed up the former. Finally, we extract a general distributed framework for ensemble pruning, which can be widely suitable for most of the existing ensemble pruning methods and achieve less time-consuming without much accuracy degradation. Experimental results validate the efficiency of our framework and methods, particularly concerning a remarkable improvement of the execution speed, accompanied by gratifying accuracy performance.},
  archive      = {J_TNNLS},
  author       = {Yijun Bian and Yijun Wang and Yaqiang Yao and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2019.2945116},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3766-3774},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ensemble pruning based on objection maximization with a general distributed framework},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep adaptive input normalization for time series
forecasting. <em>TNNLS</em>, <em>31</em>(9), 3760–3765. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) models can be used to tackle time series analysis tasks with great success. However, the performance of DL models can degenerate rapidly if the data are not appropriately normalized. This issue is even more apparent when DL is used for financial time series forecasting tasks, where the nonstationary and multimodal nature of the data pose significant challenges and severely affect the performance of DL models. In this brief, a simple, yet effective, neural layer that is capable of adaptively normalizing the input time series, while taking into account the distribution of the data, is proposed. The proposed layer is trained in an end-to-end fashion using backpropagation and leads to significant performance improvements compared to other evaluated normalization schemes. The proposed method differs from traditional normalization methods since it learns how to perform normalization for a given task instead of using a fixed normalization scheme. At the same time, it can be directly applied to any new time series without requiring retraining. The effectiveness of the proposed method is demonstrated using a large-scale limit order book data set, as well as a load forecasting data set.},
  archive      = {J_TNNLS},
  author       = {Nikolaos Passalis and Anastasios Tefas and Juho Kanniainen and Moncef Gabbouj and Alexandros Iosifidis},
  doi          = {10.1109/TNNLS.2019.2944933},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3760-3765},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep adaptive input normalization for time series forecasting},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive robust low-rank 2-d reconstruction with steerable
sparsity. <em>TNNLS</em>, <em>31</em>(9), 3754–3759. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image reconstruction methods frequently improve their robustness by using various nonsquared loss functions, which are still potentially sensitive to the outliers. More specifically, when certain samples in data sets encounter severe contamination, these methods cannot identify and filter out the ill ones, and thus lead to the functional degeneration of the associated models. To address this issue, we propose a general framework, named robust and sparse weight learning (RSWL), to compute the adaptive weights based on an objective for robustness and sparsity. More importantly, the degree of the sparsity is steerable, such that only k well-reserved samples are activated during the optimization of our model. As a result, the severely polluted or damaged samples are eliminated, and the robustness is ensured. The framework is further leveraged against a 2-D image reconstruction task. Theoretical analysis and extensive experiments are presented to demonstrate the superiority of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Rui Zhang and Han Zhang and Xuelong Li and Feiping Nie},
  doi          = {10.1109/TNNLS.2019.2944650},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3754-3759},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive robust low-rank 2-D reconstruction with steerable sparsity},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial-neurons-based passivity-guaranteed state estimation
for neural networks with randomly occurring time delays. <em>TNNLS</em>,
<em>31</em>(9), 3747–3753. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, the partial-neurons-based passivity-guaranteed state estimation (SE) problem is examined for a class of discrete-time artificial neural networks with randomly occurring time delays. The measurement outputs available utilized for the SE are allowed to be available only at a fraction of neurons in the networks. A Bernoulli-distributed random variable is employed to characterize the random nature of the occurrence of time delays. By resorting to the Lyapunov-Krasovskii functional method as well as the stochastic analysis technique, sufficient criteria are provided for the existence of the desired state estimators ensuring the estimation error dynamics to achieve the asymptotic stability in the mean square with a guaranteed passivity performance level. In addition, the parameterization of the estimator gain is acquired by solving a convex optimization problem. Finally, the validity of the obtained theoretical results is illustrated via a numerical simulation example.},
  archive      = {J_TNNLS},
  author       = {Jiahui Li and Hongli Dong and Zidong Wang and Xianye Bu},
  doi          = {10.1109/TNNLS.2019.2944552},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3747-3753},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Partial-neurons-based passivity-guaranteed state estimation for neural networks with randomly occurring time delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implementing any nonlinear quantum neuron. <em>TNNLS</em>,
<em>31</em>(9), 3741–3746. (<a
href="https://doi.org/10.1109/TNNLS.2019.2938899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of artificial neural networks (ANNs) to adapt to input data and perform generalizations is intimately connected to the use of nonlinear activation and propagation functions. Quantum versions of ANN have been proposed to take advantage of the possible supremacy of quantum over classical computing. To date, all proposals faced the difficulty of implementing nonlinear activation functions since quantum operators are linear. This brief presents an architecture to simulate the computation of an arbitrary nonlinear function as a quantum circuit. This computation is performed on the phase of an adequately designed quantum state, and quantum phase estimation recovers the result, given a fixed precision, in a circuit with linear complexity in function of ANN input size.},
  archive      = {J_TNNLS},
  author       = {Fernando M. de Paula Neto and Teresa B. Ludermir and Wilson R. de Oliveira and Adenilton J. da Silva},
  doi          = {10.1109/TNNLS.2019.2938899},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3741-3746},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Implementing any nonlinear quantum neuron},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Teacher–student curriculum learning. <em>TNNLS</em>,
<em>31</em>(9), 3732–3740. (<a
href="https://doi.org/10.1109/TNNLS.2019.2934906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task, and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e., where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student&#39;s performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with long short-term memory (LSTM) and navigation in Minecraft. Our automatically ordered curriculum of submazes enabled to solve a Minecraft maze that could not be solved at all when training directly on that maze, and the learning was an order of magnitude faster than a uniform sampling of those submazes.},
  archive      = {J_TNNLS},
  author       = {Tambet Matiisen and Avital Oliver and Taco Cohen and John Schulman},
  doi          = {10.1109/TNNLS.2019.2934906},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3732-3740},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Teacher–Student curriculum learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A wide-deep-sequence model-based quality prediction method
in industrial process analysis. <em>TNNLS</em>, <em>31</em>(9),
3721–3731. (<a
href="https://doi.org/10.1109/TNNLS.2020.3001602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.},
  archive      = {J_TNNLS},
  author       = {Lei Ren and Zihao Meng and Xiaokang Wang and Renquan Lu and Laurence T. Yang},
  doi          = {10.1109/TNNLS.2020.3001602},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3721-3731},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A wide-deep-sequence model-based quality prediction method in industrial process analysis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). XFlow: Cross-modal deep neural networks for audiovisual
classification. <em>TNNLS</em>, <em>31</em>(9), 3711–3720. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there have been numerous developments toward solving multimodal tasks, aiming to learn a stronger representation than through a single modality. Certain aspects of the data can be particularly useful in this case-for example, correlations in the space or time domain across modalities-but should be wisely exploited in order to benefit from their full predictive potential. We propose two deep learning architectures with multimodal cross connections that allow for dataflow between several feature extractors (XFlow). Our models derive more interpretable features and achieve better performances than models that do not exchange representations, usefully exploiting correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. This article improves on the existing multimodal deep learning algorithms in two essential ways: 1) it presents a novel method for performing cross modality (before features are learned from individual modalities) and 2) extends the previously proposed cross connections that only transfer information between the streams that process compatible data. Illustrating some of the representations learned by the connections, we analyze their contribution to the increase in discrimination ability and reveal their compatibility with a lip-reading network intermediate representation. We provide the research community with Digits, a new data set consisting of three data types extracted from videos of people saying the digits 0-9. Results show that both cross-modal architectures outperform their baselines (by up to 11.5\%) when evaluated on the AVletters, CUAVE, and Digits data sets, achieving the state-of-the-art results.},
  archive      = {J_TNNLS},
  author       = {Cătălina Cangea and Petar Veličković and Pietro Liò},
  doi          = {10.1109/TNNLS.2019.2945992},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3711-3720},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {XFlow: Cross-modal deep neural networks for audiovisual classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural network-based adaptive control for spacecraft under
actuator failures and input saturations. <em>TNNLS</em>, <em>31</em>(9),
3696–3710. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop attitude tracking control methods for spacecraft as rigid bodies against model uncertainties, external disturbances, subsystem faults/failures, and limited resources. A new intelligent control algorithm is proposed using approximations based on radial basis function neural networks (RBFNNs) and adopting the tunable parameter-based variable structure (TPVS) control techniques. By choosing different adaptation parameters elaborately, a series of control strategies are constructed to handle the challenging effects due to actuator faults/failures and input saturations. With the help of the Lyapunov theory, we show that our proposed methods guarantee both finite-time convergence and fault-tolerance capability of the closed-loop systems. Finally, benefits of the proposed control methods are illustrated through five numerical examples.},
  archive      = {J_TNNLS},
  author       = {Ning Zhou and Yu Kawano and Ming Cao},
  doi          = {10.1109/TNNLS.2019.2945920},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3696-3710},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based adaptive control for spacecraft under actuator failures and input saturations},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MLNE: Multi-label network embedding. <em>TNNLS</em>,
<em>31</em>(9), 3682–3695. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding aims to preserve topological structures of a network using low-dimensional vectors and has shown to be effective for driving a myriad of graph mining tasks (e.g., link prediction or classification) free of the stressful feature extraction procedure. Many methods have been proposed to integrate node content and/or label information, with nodes sharing similar content/labels being close to each other in the learned latent space. To date, existing methods either consider networked instances with a single label or consider a set of labels as a whole for node representation learning. Therefore, they cannot handle network of instances containing multiple labels (i.e. multi-labels), which are ubiquitous in describing complex concepts of instances. In this article, we formulate a new multi-label network embedding (MLNE) problem to learn feature representation for networked multi-label instances. We argue that the key to MLNE learning is to aggregate node topology structures, node content, and multi-label correlations. We propose a two-layer network embedding framework to couple information for effective learning. To capture higher order label correlations, we use labels to form a high-level label-label network over a low-level node-node network, in which the label network interacts with the node network through multi-labeling relations. The low-level node-node network can be enhanced by latent label-specific features from high-level label network with well-captured high-order correlations between labels. To enable the multi-label informed network embedding, we force both node and label representations being optimized under the same low-dimensional latent space by a unified training objective. Experiments on real-world data sets demonstrate that MLNE achieves better performance compared with methods with or without considering label information.},
  archive      = {J_TNNLS},
  author       = {Min Shi and Yufei Tang and Xingquan Zhu},
  doi          = {10.1109/TNNLS.2019.2945869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3682-3695},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MLNE: Multi-label network embedding},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonstationary discrete convolution kernel for multimodal
process monitoring. <em>TNNLS</em>, <em>31</em>(9), 3670–3681. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven process monitoring has benefited from the development and application of kernel transformations, especially when various types of nonlinearity exist in the data. However, when dealing with the multimodality behavior that is frequently observed in the process operations, the most widely used radial basis function (RBF) kernel has limitations in describing process data collected from multiple normal operating modes. In this article, we highlight this limitation via a synthesized example. In order to account for the multimodality behavior and improve the fault detection performance accordingly, we propose a novel nonstationary discrete convolution kernel, which derives from the convolution kernel structure, as an alternative to the RBF kernel. By assuming the training samples to be the support of the discrete convolution, this new kernel can properly address these training samples from different operating modes with diverse properties and, therefore, can improve the data description and fault detection performance. Its performance is compared with RBF kernels under a standard kernel principal component analysis framework and with other methods proposed for multimode process monitoring via numerical examples. Moreover, a benchmark data set collected from a pilot-scale multiphase flow facility is used to demonstrate the advantages of the new kernel when applied to an experimental data set.},
  archive      = {J_TNNLS},
  author       = {Ruomu Tan and James R. Ottewill and Nina F. Thornhill},
  doi          = {10.1109/TNNLS.2019.2945847},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3670-3681},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonstationary discrete convolution kernel for multimodal process monitoring},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design principles for central pattern generators with preset
rhythms. <em>TNNLS</em>, <em>31</em>(9), 3658–3669. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the design of synthetic central pattern generators (CPGs). Biological CPGs are neural circuits that determine a variety of rhythmic activities, including locomotion, in animals. A synthetic CPG is a network of dynamical elements (here called cells) properly coupled by various synapses to emulate rhythms produced by a biological CPG. We focus on CPGs for locomotion of quadrupeds and present our design approach, based on the principles of nonlinear dynamics, bifurcation theory, and parameter optimization. This approach lets us design the synthetic CPG with a set of desired rhythms and switch between them as the parameter representing the control actions from the brain is varied. The developed four-cell CPG can produce four distinct gaits: walk, trot, gallop, and bound, similar to the mouse locomotion. The robustness and adaptability of the network design principles are verified using different cell and synapse models.},
  archive      = {J_TNNLS},
  author       = {Matteo Lodi and Andrey L. Shilnikov and Marco Storace},
  doi          = {10.1109/TNNLS.2019.2945637},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3658-3669},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design principles for central pattern generators with preset rhythms},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An event-driven categorization model for AER image sensors
using multispike encoding and learning. <em>TNNLS</em>, <em>31</em>(9),
3649–3657. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a systematic computational model to explore brain-based computation for object recognition. The model extracts temporal features embedded in address-event representation (AER) data and discriminates different objects by using spiking neural networks (SNNs). We use multispike encoding to extract temporal features contained in the AER data. These temporal patterns are then learned through the tempotron learning rule. The presented model is consistently implemented in a temporal learning framework, where the precise timing of spikes is considered in the feature-encoding and learning process. A noise-reduction method is also proposed by calculating the correlation of an event with the surrounding spatial neighborhood based on the recently proposed time-surface technique. The model evaluated on wide spectrum data sets (MNIST, N-MNIST, MNIST-DVS, AER Posture, and Poker Card) demonstrates its superior recognition performance, especially for the events with noise.},
  archive      = {J_TNNLS},
  author       = {Rong Xiao and Huajin Tang and Yuhao Ma and Rui Yan and Garrick Orchard},
  doi          = {10.1109/TNNLS.2019.2945630},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3649-3657},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An event-driven categorization model for AER image sensors using multispike encoding and learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-media semantic correlation learning based on deep hash
network and semantic expansion for social network cross-media search.
<em>TNNLS</em>, <em>31</em>(9), 3634–3648. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-media search from large-scale social network big data has become increasingly valuable in our daily life because it can support querying different data modalities. Deep hash networks have shown high potential in achieving efficient and effective cross-media search performance. However, due to the fact that social network data often exhibit text sparsity, diversity, and noise characteristics, the search performance of existing methods often degrades when dealing with this data. In order to address this problem, this article proposes a novel end-to-end cross-media semantic correlation learning model based on a deep hash network and semantic expansion for social network cross-media search (DHNS). The approach combines deep network feature learning and hash-code quantization learning for multimodal data into a unified optimization architecture, which successfully preserves both intramedia similarity and intermedia correlation, by minimizing both cross-media correlation loss and binary hash quantization loss. In addition, our approach realizes semantic relationship expansion by constructing the image-word relation graph and mining the potential semantic relationship between images and words, and obtaining the semantic embedding based on both internal graph deep walk and an external knowledge base. Experimental results demonstrate that DHNS yields better cross-media search performance on standard benchmarks.},
  archive      = {J_TNNLS},
  author       = {Meiyu Liang and Junping Du and Congxian Yang and Zhe Xue and Haisheng Li and Feifei Kou and Yue Geng},
  doi          = {10.1109/TNNLS.2019.2945567},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3634-3648},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-media semantic correlation learning based on deep hash network and semantic expansion for social network cross-media search},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Laplacian-uniform mixture-driven iterative robust coding
with applications to face recognition against dense errors.
<em>TNNLS</em>, <em>31</em>(9), 3620–3633. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers due to occlusion, pixel corruption, and so on pose serious challenges to face recognition despite the recent progress brought by sparse representation. In this article, we show that robust statistics implemented by the state-of-the-art methods are insufficient for robustness against dense gross errors. By modeling the distribution of coding residuals with a Laplacian-uniform mixture, we obtain a sparse representation that is significantly more robust than the previous methods. The nonconvex error term of the implemented objective function is nondifferentiable at zero and cannot be properly addressed by the usual iteratively reweighted least-squares formulation. We show that an iterative robust coding algorithm can be derived by local linear approximation of the nonconvex error term, which is both effective and efficient. With iteratively reweighted l 1 minimization of the error term, the proposed algorithm is capable of handling the sparsity assumption of the coding errors more appropriately than the previous methods. Notably, it has the distinct property of addressing error detection and error correction cooperatively in the robust coding process. The proposed method demonstrates significantly improved robustness for face recognition against dense gross errors, either contiguous or discontiguous, as verified by extensive experiments.},
  archive      = {J_TNNLS},
  author       = {Huicheng Zheng and Dajun Lin and Lina Lian and Jiayu Dong and Peipei Zhang},
  doi          = {10.1109/TNNLS.2019.2945372},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3620-3633},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Laplacian-uniform mixture-driven iterative robust coding with applications to face recognition against dense errors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development and analysis of neural networks realized in the
presence of granular data. <em>TNNLS</em>, <em>31</em>(9), 3606–3619.
(<a href="https://doi.org/10.1109/TNNLS.2019.2945307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a design and evaluation framework of granular neural networks realized in the presence of information granules. Neural networks realized in this manner are able to process both nonnumerical data, such as information granules as well as numerical data. Information granules are meaningful and semantically sound entities formed by organizing existing knowledge and available experimental data. The directional nature of mapping between the input and output data needs to be considered when building information granules. The development of neural networks advocated in this article is realized as a two-phase process. First, a collection of information granules is formed through granulation of numeric data in the input and output spaces. Second, neural networks are constructed on the basis of information granules rather than original (numeric) data. The proposed method leads to the construction of neural networks in a completely new way. In comparison with traditional (numeric) neural networks, the networks developed in the presence of granular data require shorter learning time. They also produce the results (outputs) that are information granules rather than numeric entities. The quality of granular outputs generated by our neural networks is evaluated in terms of the coverage and specificity criteria that are pertinent to the characterization of the information granules.},
  archive      = {J_TNNLS},
  author       = {Xiubin Zhu and Witold Pedrycz and Zhiwu Li},
  doi          = {10.1109/TNNLS.2019.2945307},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3606-3619},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Development and analysis of neural networks realized in the presence of granular data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Singular values for ReLU layers. <em>TNNLS</em>,
<em>31</em>(9), 3594–3605. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their prevalence in neural networks, we still lack a thorough theoretical characterization of rectified linear unit (ReLU) layers. This article aims to further our understanding of ReLU layers by studying how the activation function ReLU interacts with the linear component of the layer and what role this interaction plays in the success of the neural network in achieving its intended task. To this end, we introduce two new tools: ReLU singular values of operators and the Gaussian mean width of operators. By presenting, on the one hand, theoretical justifications, results, and interpretations of these two concepts and, on the other hand, numerical experiments and results of the ReLU singular values and the Gaussian mean width being applied to trained neural networks, we hope to give a comprehensive, singular-value-centric view of ReLU layers. We find that ReLU singular values and the Gaussian mean width do not only enable theoretical insights but also provide one with metrics that seem promising for practical applications. In particular, these measures can be used to distinguish correctly and incorrectly classified data as it traverses the network. We conclude by introducing two tools based on our findings: double layers and harmonic pruning.},
  archive      = {J_TNNLS},
  author       = {Sören Dittmer and Emily J. King and Peter Maass},
  doi          = {10.1109/TNNLS.2019.2945113},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3594-3605},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Singular values for ReLU layers},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometric matrix completion with deep conditional random
fields. <em>TNNLS</em>, <em>31</em>(9), 3579–3593. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of completing high-dimensional matrices from a limited set of observations arises in many big data applications, especially recommender systems. The existing matrix completion models generally follow either a memory- or a model-based approach, whereas geometric matrix completion (GMC) models combine the best from both approaches. Existing deep-learning-based geometric models yield good performance, but, in order to operate, they require a fixed structure graph capturing the relationships among the users and items. This graph is typically constructed by evaluating a pre-defined similarity metric on the available observations or by using side information, e.g., user profiles. In contrast, Markov-random-fields-based models do not require a fixed structure graph but rely on handcrafted features to make predictions. When no side information is available and the number of available observations becomes very low, existing solutions are pushed to their limits. In this article, we propose a GMC approach that addresses these challenges. We consider matrix completion as a structured prediction problem in a conditional random field (CRF), which is characterized by a maximum a posteriori (MAP) inference, and we propose a deep model that predicts the missing entries by solving the MAP inference problem. The proposed model simultaneously learns the similarities among matrix entries, computes the CRF potentials, and solves the inference problem. Its training is performed in an end-to-end manner, with a method to supervise the learning of entry similarities. Comprehensive experiments demonstrate the superior performance of the proposed model compared to various state-of-the-art models on popular benchmark data sets and underline its superior capacity to deal with highly incomplete matrices.},
  archive      = {J_TNNLS},
  author       = {Duc Minh Nguyen and Robert Calderbank and Nikos Deligiannis},
  doi          = {10.1109/TNNLS.2019.2945111},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3579-3593},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Geometric matrix completion with deep conditional random fields},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning tracking control for robotic
manipulator with kernel-based dynamic model. <em>TNNLS</em>,
<em>31</em>(9), 3570–3578. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) is an efficient learning approach to solving control problems for a robot by interacting with the environment to acquire the optimal control policy. However, there are many challenges for RL to execute continuous control tasks. In this article, without the need to know and learn the dynamic model of a robotic manipulator, a kernel-based dynamic model for RL is proposed. In addition, a new tuple is formed through kernel function sampling to describe a robotic RL control problem. In this algorithm, a reward function is defined according to the features of tracking control in order to speed up the learning process, and then an RL tracking controller with a kernel-based transition dynamic model is proposed. Finally, a critic system is presented to evaluate the policy whether it is good or bad to the RL control tasks. The simulation results illustrate that the proposed method can fulfill the robotic tracking tasks effectively and achieve similar and even better tracking performance with much smaller inputs of force/torque compared with other learning algorithms, demonstrating the effectiveness and efficiency of the proposed RL algorithm.},
  archive      = {J_TNNLS},
  author       = {Yazhou Hu and Wenxue Wang and Hao Liu and Lianqing Liu},
  doi          = {10.1109/TNNLS.2019.2945019},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3570-3578},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning tracking control for robotic manipulator with kernel-based dynamic model},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complex-valued discrete-time neural dynamics for perturbed
time-dependent complex quadratic programming with applications.
<em>TNNLS</em>, <em>31</em>(9), 3555–3569. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been reported that some specially designed recurrent neural networks and their related neural dynamics are efficient for solving quadratic programming (QP) problems in the real domain. A complex-valued QP problem is generated if its variable vector is composed of the magnitude and phase information, which is often depicted in a time-dependent form. Given the important role that complex-valued problems play in cybernetics and engineering, computational models with high accuracy and strong robustness are urgently needed, especially for time-dependent problems. However, the research on the online solution of time-dependent complex-valued problems has been much less investigated compared to time-dependent real-valued problems. In this article, to solve the online time-dependent complex-valued QP problems subject to linear constraints, two new discrete-time neural dynamics models, which can achieve global convergence performance in the presence of perturbations with the provided theoretical analyses, are proposed and investigated. In addition, the second proposed model is developed to eliminate the operation of explicit matrix inversion by introducing the quasi-Newton Broyden-Fletcher-Goldfarb-Shanno (BFGS) method. Moreover, computer simulation results and applications in robotics and filters are provided to illustrate the feasibility and superiority of the proposed models in comparison with the existing solutions.},
  archive      = {J_TNNLS},
  author       = {Yimeng Qi and Long Jin and Yaonan Wang and Lin Xiao and Jiliang Zhang},
  doi          = {10.1109/TNNLS.2019.2944992},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3555-3569},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Complex-valued discrete-time neural dynamics for perturbed time-dependent complex quadratic programming with applications},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collocating clothes with generative adversarial networks
cosupervised by categories and attributes: A multidiscriminator
framework. <em>TNNLS</em>, <em>31</em>(9), 3540–3554. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The choice of which clothes to wear affects how one is perceived, as well as constitutes an expression of one’s personal style. Based on the recent advances in image-to-image translation by the conditional generative adversarial network (cGAN), we propose a new framework with a multidiscriminator by incorporating different types of conditional information into the discriminator of cGAN for clothing matches. In contrast with most extant frameworks under cGAN, with one generator and one discriminator, the proposed framework investigates the potential of utilizing conditional information delivered by multidiscriminators to guide the generator. Under this framework, we propose an Attribute-GAN with two discriminators and a category-attribute GAN (CA-GAN) with three discriminators. In order to evaluate the performance of our proposed models, we built a large-scale data set that consists of 19 081 pairs of collocation clothing images with 90 manually labeled attributes. Experimental results demonstrate that with supervision of the additional attribute discriminator or category discriminator, the quality of the generated clothing images by GANs is consistently improved in comparison with the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Linlin Liu and Haijun Zhang and Xiaofei Xu and Zhao Zhang and Shuicheng Yan},
  doi          = {10.1109/TNNLS.2019.2944979},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3540-3554},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collocating clothes with generative adversarial networks cosupervised by categories and attributes: A multidiscriminator framework},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Bayes imbalance impact index: A measure of class imbalanced
data set for classification problem. <em>TNNLS</em>, <em>31</em>(9),
3525–3539. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies of imbalanced data classification have shown that the imbalance ratio (IR) is not the only cause of performance loss in a classifier, as other data factors, such as small disjuncts, noise, and overlapping, can also make the problem difficult. The relationship between the IR and other data factors has been demonstrated, but to the best of our knowledge, there is no measurement of the extent to which class imbalance influences the classification performance of imbalanced data. In addition, it is also unknown which data factor serves as the main barrier for classification in a data set. In this article, we focus on the Bayes optimal classifier and examine the influence of class imbalance from a theoretical perspective. We propose an instance measure called the Individual Bayes Imbalance Impact Index (IBI 3 ) and a data measure called the Bayes Imbalance Impact Index (BI 3 ). IBI 3 and BI 3 reflect the extent of influence using only the imbalance factor, in terms of each minority class sample and the whole data set, respectively. Therefore, IBI 3 can be used as an instance complexity measure of imbalance and BI 3 as a criterion to demonstrate the degree to which imbalance deteriorates the classification of a data set. We can, therefore, use BI 3 to access whether it is worth using imbalance recovery methods, such as sampling or cost-sensitive methods, to recover the performance loss of a classifier. The experiments show that IBI 3 is highly consistent with the increase of the prediction score obtained by the imbalance recovery methods and that BI 3 is highly consistent with the improvement in the F1 score obtained by the imbalance recovery methods on both synthetic and real benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Yang Lu and Yiu-Ming Cheung and Yuan Yan Tang},
  doi          = {10.1109/TNNLS.2019.2944962},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3525-3539},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bayes imbalance impact index: A measure of class imbalanced data set for classification problem},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sketch kernel ridge regression using circulant matrix:
Algorithm and theory. <em>TNNLS</em>, <em>31</em>(9), 3512–3524. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel ridge regression (KRR) is a powerful method for nonparametric regression. The time and space complexity of computing the KRR estimate directly are $\mathcal {O}(n^{3})$ and $\mathcal {O}(n^{2})$ , respectively, which are prohibitive for large-scale data sets, where $n$ is the number of data. In this article, we propose a novel random sketch technique based on the circulant matrix that achieves savings in storage space and accelerates the solution of the KRR approximation. The circulant matrix has the following advantages: It can save time complexity by using the fast Fourier transform (FFT) to compute the product of matrix and vector, its space complexity is linear, and the circulant matrix, whose entries in the first column are independent of each other and obey the Gaussian distribution, is almost as effective as the i.i.d. Gaussian random matrix for approximating KRR. Combining the characteristics of the circulant matrix and our careful design, theoretical analysis and experimental results demonstrate that our proposed sketch method, making the estimate kernel methods scalable and practical for large-scale data problems, outperforms the state-of-the-art KRR estimates in time complexity while retaining similar accuracies. Meanwhile, our sketch method provides the theoretical bound that keeps the optimal convergence rate for approximating KRR.},
  archive      = {J_TNNLS},
  author       = {Rong Yin and Yong Liu and Weiping Wang and Dan Meng},
  doi          = {10.1109/TNNLS.2019.2944959},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3512-3524},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sketch kernel ridge regression using circulant matrix: Algorithm and theory},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural output feedback compensation control for
intermittent actuator faults using command-filtered backstepping.
<em>TNNLS</em>, <em>31</em>(9), 3497–3511. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively compensating unknown intermittent actuator faults in uncertain decentralized nonlinear systems is a very difficult problem, and very few results have been obtained. In this article, to address this issue, an adaptive neural output feedback compensation control scheme based on command-filtered backstepping is developed. First, we design a bank of observers to estimate the system states and utilize neural networks with random hidden nodes to approximate the unknown functions of these observers. Second, a smooth projection algorithm is used to online update estimated parameters in the controllers such that the possible ceaseless increase in the estimated parameters caused by intermittent actuator faults can be eliminated. Due to the presence of intermittent jumps of unknown parameters, a modified Lyapunov function is developed to analyze the system stability. It is proved that the boundedness of all closed-loop system signals is ensured and the ultimate bound of the tracking error depends on design parameters, adjustable jumping amplitude of Lyapunov function, and minimum fault time interval. Third, by analyzing the system transient performance, the peaking phenomenon at the starting instant of the system operation can be removed, and a root mean square type of bound is established to illustrate that the transient tracking error performance is tunable by design parameters. Finally, simulations studies are done to illustrate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Yongqiang Nai and Qingyu Yang and Zhiqiang Zhang},
  doi          = {10.1109/TNNLS.2019.2944897},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3497-3511},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural output feedback compensation control for intermittent actuator faults using command-filtered backstepping},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MVStream: Multiview data stream clustering. <em>TNNLS</em>,
<em>31</em>(9), 3482–3496. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a new problem of data stream clustering, namely, multiview data stream (MVStream) clustering. Although many data stream clustering algorithms have been developed, they are restricted to the single-view streaming data, and clustering MVStreams still remains largely unsolved. In addition to the many issues encountered by the conventional single-view data stream clustering, such as capturing cluster evolution and discovering clusters of arbitrary shapes under the limited computational resources, the main challenge of MVStream clustering lies in integrating information from multiple views in a streaming manner and abstracting summary statistics from the integrated features simultaneously. In this article, we propose a novel MVStream clustering algorithm for the first time. The main idea is to design a multiview support vector domain description (MVSVDD) model, by which the information from multiple insufficient views can be integrated, and the outputting support vectors (SVs) are utilized to abstract the summary statistics of the historical multiview data objects. Based on the MVSVDD model, a new multiview cluster labeling method is designed, whereby clusters of arbitrary shapes can be discovered for each view. By tracking the cluster labels of SVs in each view, the cluster evolution associated with concept drift can be captured. Since the SVs occupy only a small portion of data objects, the proposed MVStream algorithm is quite efficient with the limited computational resources. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Ling Huang and Chang-Dong Wang and Hong-Yang Chao and Philip S. Yu},
  doi          = {10.1109/TNNLS.2019.2944851},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3482-3496},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MVStream: Multiview data stream clustering},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural quantized control of MIMO nonlinear systems
under actuation faults and time-varying output constraints.
<em>TNNLS</em>, <em>31</em>(9), 3471–3481. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a neural network (NN)-based robust adaptive fault-tolerant control (FTC) algorithm is proposed for a class of multi-input multi-output (MIMO) strict-feedback nonlinear systems with input quantization and actuation faults as well as asymmetric yet time-varying output constraints. By introducing a key nonlinear decomposition for quantized input, the developed control scheme does not require the detailed information of quantization parameters. By imposing a reasonable condition on the gain matrix under actuation faults, together with the inherent approximation capability of NN, the difficulty of FTC design caused by anomaly actuation can be handled gracefully, and the normally used yet rigorous assumption on control gain matrix in most existing results is significantly relaxed. Furthermore, a brand new barrier function is constructed to handle the asymmetric yet time-varying output constraints such that the analysis and design are extremely simplified compared with the traditional barrier Lyapunov function (BLF)-based methods. NNs are used to approximate the unknown nonlinear continuous functions. The stability of the closed-loop system is analyzed by using the Lyapunov method and is verified through a simulation example.},
  archive      = {J_TNNLS},
  author       = {Kai Zhao and Jiawei Chen},
  doi          = {10.1109/TNNLS.2019.2944690},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3471-3481},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural quantized control of MIMO nonlinear systems under actuation faults and time-varying output constraints},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extracting relational explanations from deep neural
networks: A survey from a neural-symbolic perspective. <em>TNNLS</em>,
<em>31</em>(9), 3456–3470. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The term “explainable AI” refers to the goal of producing artificially intelligent agents that are capable of providing explanations for their decisions. Some models (e.g., rule-based systems) are designed to be explainable, while others are less explicit “black boxes” for which their reasoning remains a mystery. One example of the latter is the neural network, and over the past few decades, researchers in the field of neural-symbolic integration (NSI) have sought to extract relational knowledge from such networks. Extraction from deep neural networks, however, has remained a challenge until recent years in which many methods of extracting distinct, salient features from input or hidden feature spaces of deep neural networks have been proposed. Furthermore, methods of identifying relationships between these features have also emerged. This article presents examples of old and new developments in extracting relational explanations in order to argue that the latter have analogies in the former and, as such, can be described in terms of long-established taxonomies and frameworks presented in early neural-symbolic literature. We also outline potential future research directions that come to light from this refreshed perspective.},
  archive      = {J_TNNLS},
  author       = {Joseph Townsend and Thomas Chaton and João M. Monteiro},
  doi          = {10.1109/TNNLS.2019.2944672},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3456-3470},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Extracting relational explanations from deep neural networks: A survey from a neural-symbolic perspective},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiview uncorrelated locality preserving projection.
<em>TNNLS</em>, <em>31</em>(9), 3442–3455. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical Correlation Analysis (CCA) is a popular multiview dimension reduction method, which aims to maximize the correlation between two views to find the common subspace shared by these two views. However, it can only deal with two-view data, while the number of views frequently exceeds two in many real applications. To handle data with more than two views, in the previous studies, either the pairwise correlation or the high-order correlation was employed. These two types of correlation define the relation of multiview data from different viewpoints, and both have special effects for view consistency. To obtain flexible view consistency, in this article, we propose multiview uncorrelated locality preserving projection (MULPP), which considers two types of correlation simultaneously. The MULPP also considers the complementary property of different views by preserving the local structures of all the views. To obtain multiple projections and minimize the redundancy of low-dimensional features, for each view, the MULPP makes the features extracted by different projections uncorrelated. The MULPP is solved by an iteration algorithm, and the convergence of the algorithm is proven. The experiments on Multiple Feature, Coil-100, 3Sources, and NUS-WIDE data sets demonstrate the effectiveness of MULPP.},
  archive      = {J_TNNLS},
  author       = {Jun Yin and Shiliang Sun},
  doi          = {10.1109/TNNLS.2019.2944664},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3442-3455},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview uncorrelated locality preserving projection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-weighted clustering with adaptive neighbors.
<em>TNNLS</em>, <em>31</em>(9), 3428–3441. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many modern clustering models can be divided into two separated steps, i.e., constructing a similarity graph (SG) upon samples and partitioning each sample into the corresponding cluster based on SG. Therefore, learning a reasonable SG has become a hot issue in the clustering field. Many previous works that focus on constructing better SG have been proposed. However, most of them follow an ideal assumption that the importance of different features is equal, which is not adapted in practical applications. To alleviate this problem, this article proposes a self-weighted clustering with adaptive neighbors (SWCAN) model that can assign weights for different features, learn an SG, and partition samples into clusters simultaneously. In experiments, we observe that the SWCAN can assign weights for different features reasonably and outperform than comparison clustering models on synthetic and practical data sets.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Danyang Wu and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2019.2944565},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3428-3441},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-weighted clustering with adaptive neighbors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An exponential-type anti-noise varying-gain network for
solving disturbed time-varying inversion systems. <em>TNNLS</em>,
<em>31</em>(9), 3414–3427. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the disturbed time-varying inversion problem, an exponential-type anti-noise varying-gain network (EAVGN) is proposed and analyzed. To do so, a vector-based error function is first defined. By using the varying-gain neural dynamic design method, an EAVGN model is then formulated. Furthermore, the differentiation error and the model-implementation error are considered into the model, and the perturbed EAVGN model is obtained. For better illustrations, comparisons between the EAVGN and the conventional fixed-parameter recurrent neural network (FP-RNN) are conducted to illustrate the advantages of the proposed EAVGN. Mathematical proof demonstrates that the proposed EAVGN has much better anti-noise properties than FP-RNN. On one hand, the residual error of EAVGN can be reduced to zero in any case, but that of FP-RNN is large and cannot be convergent, in particular when the bound of Frobenius norm of the exact solution is large or the noise is large. On the other hand, the bound of the residual error of EAVGN is always smaller than that of FP-RNN. Simulation results verify that when different types of noises exist, the proposed EAVGN owns better anti-noise property compared with the state-of-the-art methods. In addition, a practical application is presented to illustrate the implementation process and the practical benefits of the EAVGN.},
  archive      = {J_TNNLS},
  author       = {Zhijun Zhang and Tao Chen and Min Wang and Lunan Zheng},
  doi          = {10.1109/TNNLS.2019.2944485},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3414-3427},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An exponential-type anti-noise varying-gain network for solving disturbed time-varying inversion systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust and communication-efficient federated learning from
non-i.i.d. data. <em>TNNLS</em>, <em>31</em>(9), 3400–3413. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows multiple parties to jointly train a deep learning model on their combined data, without any of the participants having to reveal their local data to a centralized server. This form of privacy-preserving collaborative learning, however, comes at the cost of a significant communication overhead during training. To address this problem, several compression methods have been proposed in the distributed training literature that can reduce the amount of required communication by up to three orders of magnitude. These existing methods, however, are only of limited utility in the federated learning setting, as they either only compress the upstream communication from the clients to the server (leaving the downstream communication uncompressed) or only perform well under idealized conditions, such as i.i.d. distribution of the client data, which typically cannot be found in federated learning. In this article, we propose sparse ternary compression (STC), a new compression framework that is specifically designed to meet the requirements of the federated learning environment. STC extends the existing compression technique of top-k gradient sparsification with a novel mechanism to enable downstream compression as well as ternarization and optimal Golomb encoding of the weight updates. Our experiments on four different learning tasks demonstrate that STC distinctively outperforms federated averaging in common federated learning scenarios. These results advocate for a paradigm shift in federated optimization toward high-frequency low-bitwidth communication, in particular in the bandwidth-constrained learning environments.},
  archive      = {J_TNNLS},
  author       = {Felix Sattler and Simon Wiedemann and Klaus-Robert Müller and Wojciech Samek},
  doi          = {10.1109/TNNLS.2019.2944481},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3400-3413},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust and communication-efficient federated learning from non-i.i.d. data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neuroadaptive control design for pure-feedback nonlinear
systems: A one-step design approach. <em>TNNLS</em>, <em>31</em>(9),
3389–3399. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a one-step control design approach for pure-feedback nonlinear systems in the presence of unmatched and nonvanishing external disturbances. Different from the commonly utilized backstepping design, the proposed method, integrated with the dynamic surface control (DSC) technique, only involves one-step design with one single Lyapunov function in the whole control synthesis, which derives the actual control and the intermediate controls simultaneously in a collective way, avoiding the repetitive design procedures and multiple Lyapunov functions, yet circumventing the issue of “explosion of complexity.” Furthermore, with this method, the increase in system order does not increase the design and analysis complexity. Numerical simulation examples confirm and validate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Shuyan Zhou and Yongduan Song},
  doi          = {10.1109/TNNLS.2019.2944459},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3389-3399},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive control design for pure-feedback nonlinear systems: A one-step design approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guide subspace learning for unsupervised domain adaptation.
<em>TNNLS</em>, <em>31</em>(9), 3374–3388. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prevailing problem in many machine learning tasks is that the training (i.e., source domain) and test data (i.e., target domain) have different distribution [i.e., non-independent identical distribution (i.i.d.)]. Unsupervised domain adaptation (UDA) was proposed to learn the unlabeled target data by leveraging the labeled source data. In this article, we propose a guide subspace learning (GSL) method for UDA, in which an invariant, discriminative, and domain-agnostic subspace is learned by three guidance terms through a two-stage progressive training strategy. First, the subspace-guided term reduces the discrepancy between the domains by moving the source closer to the target subspace. Second, the data-guided term uses the coupled projections to map both domains to a unified subspace, where each target sample can be represented by the source samples with a low-rank coefficient matrix that can preserve the global structure of data. In this way, the data from both domains can be well interlaced and the domain-invariant features can be obtained. Third, for improving the discrimination of the subspaces, the label-guided term is constructed for prediction based on source labels and pseudo-target labels. To further improve the model tolerance to label noise, a label relaxation matrix is introduced. For the solver, a two-stage learning strategy with teacher teaches and student feedbacks mode is proposed to obtain the discriminative domain-agnostic subspace. In addition, for handling nonlinear domain shift, a nonlinear GSL (NGSL) framework is formulated with kernel embedding, such that the unified subspace is imposed with nonlinearity. Experiments on various cross-domain visual benchmark databases show that our methods outperform many state-of-the-art UDA methods. The source code is available at https://github.com/Fjr9516/GSL .},
  archive      = {J_TNNLS},
  author       = {Lei Zhang and Jingru Fu and Shanshan Wang and David Zhang and Zhaoyang Dong and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2019.2944455},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3374-3388},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guide subspace learning for unsupervised domain adaptation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Projected neural network for a class of non-lipschitz
optimization problems with linear constraints. <em>TNNLS</em>,
<em>31</em>(9), 3361–3373. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider a class of nonsmooth, nonconvex, and non-Lipschitz optimization problems, which have wide applications in sparse optimization. We generalize the Clarke stationary point and define a kind of generalized stationary point of the problems with a stronger optimal capability. Based on the smoothing method, we propose a projected neural network for solving this kind of optimization problem. Under the condition that the level set of objective function in the feasible region is bounded, we prove that the solution of the proposed neural network is globally existent and bounded. The uniqueness of the solution of the proposed network is also analyzed. When the feasible region is bounded, any accumulation point of the proposed neural network is a generalized stationary point of the optimization model. Based on some suitable conditions, any solution of the proposed neural network is asymptotic convergent to one stationary point. In particular, we give some deep analysis on the proposed network for solving a special class of the non-Lipschitz optimization problem, which indicates a lower bound property and the unify identification for the nonzero elements of all accumulation points. Finally, some numerical results are presented to show the efficiency of the proposed neural network for solving some kinds of sparse optimization models.},
  archive      = {J_TNNLS},
  author       = {Wenjing Li and Wei Bian and Xiaoping Xue},
  doi          = {10.1109/TNNLS.2019.2944388},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3361-3373},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Projected neural network for a class of non-lipschitz optimization problems with linear constraints},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive control of noncanonical neural-network nonlinear
systems with unknown input dead-zone characteristics. <em>TNNLS</em>,
<em>31</em>(9), 3346–3360. (<a
href="https://doi.org/10.1109/TNNLS.2019.2943637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the available results on adaptive control of uncertain nonlinear systems with input dead-zone characteristics are for canonical nonlinear systems whose relative degrees are explicit and for which a Lyapunov-based backstepping design is directly applicable. However, those results cannot be applied to noncanonical form nonlinear systems whose relative degrees are implicit and for which a Lyapunov-based backstepping design may not be applicable. This article solves the adaptive control problem of a class of noncanonical neural-network nonlinear systems with unknown input dead-zones. A complete solution framework is developed, using a new gradient-based design which is applicable to noncanonical nonlinear systems with input dead-zones. Signal boundedness of the closed-loop system and the desired tracking performance are ensured with the developed control schemes. Their effectiveness is illustrated by an application example of speed control of dc motors. This article can be readily extended to handle general parametrizable noncanonical nonlinear systems with unknown dynamics and input dead-zones, to solve such an open problem.},
  archive      = {J_TNNLS},
  author       = {Guanyu Lai and Gang Tao and Yun Zhang and Zhi Liu},
  doi          = {10.1109/TNNLS.2019.2943637},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3346-3360},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive control of noncanonical neural-network nonlinear systems with unknown input dead-zone characteristics},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secure communication based on quantized synchronization of
chaotic neural networks under an event-triggered strategy.
<em>TNNLS</em>, <em>31</em>(9), 3334–3345. (<a
href="https://doi.org/10.1109/TNNLS.2019.2943548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a secure communication scheme based on the quantized synchronization of master-slave neural networks under an event-triggered strategy. First, a dynamic event-triggered strategy is proposed based on a quantized output feedback, for which a quantized output feedback controller is formed. Second, theoretical criteria are derived to ensure the bounded synchronization of master-slave neural networks. With these criteria, an explicit upper bound is given for the synchronization error. Sufficient conditions are also provided on the existence of quantized output feedback controllers. A Chua&#39;s circuit is chosen to illustrate the effectiveness of our theoretical results. Third, a secure communication scheme is presented based on the synchronization of master-slave neural networks by combining the basic principle of cryptology. Then, a secure image communication is studied to verify the feasibility and security performance of the proposed secure communication scheme. The impact of the quantization level and the event-triggered control (ETC) on image decryption is investigated through experiments.},
  archive      = {J_TNNLS},
  author       = {Wangli He and Tinghui Luo and Yang Tang and Wenli Du and Yu-Chu Tian and Feng Qian},
  doi          = {10.1109/TNNLS.2019.2943548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3334-3345},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Secure communication based on quantized synchronization of chaotic neural networks under an event-triggered strategy},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AlphaSeq: Sequence discovery with deep reinforcement
learning. <em>TNNLS</em>, <em>31</em>(9), 3319–3333. (<a
href="https://doi.org/10.1109/TNNLS.2019.2942951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequences play an important role in many applications and systems. Discovering sequences with desired properties has long been an interesting intellectual pursuit. This article puts forth a new paradigm, AlphaSeq, to discover desired sequences algorithmically using deep reinforcement learning (DRL) techniques. AlphaSeq treats the sequence discovery problem as an episodic symbol-filling game, in which a player fills symbols in the vacant positions of a sequence set sequentially during an episode of the game. Each episode ends with a completely filled sequence set, upon which a reward is given based on the desirability of the sequence set. AlphaSeq models the game as a Markov decision process (MDP) and adapts the DRL framework of AlphaGo to solve the MDP. Sequences discovered improve progressively as AlphaSeq, starting as a novice, and learns to become an expert game player through many episodes of game playing. Compared with traditional sequence construction by mathematical tools, AlphaSeq is particularly suitable for problems with complex objectives intractable to mathematical analysis. We demonstrate the searching capabilities of AlphaSeq in two applications: 1) AlphaSeq successfully rediscovers a set of ideal complementary codes that can zero-force all potential interferences in multi-carrier code-division multiple access (CDMA) systems and 2) AlphaSeq discovers new sequences that triple the signal-to-interference ratio-benchmarked against the well-known Legendre sequence-of a mismatched filter (MMF) estimator in pulse compression radar systems.},
  archive      = {J_TNNLS},
  author       = {Yulin Shao and Soung Chang Liew and Taotao Wang},
  doi          = {10.1109/TNNLS.2019.2942951},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3319-3333},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AlphaSeq: Sequence discovery with deep reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter optimization and learning in a spiking neural
network for UAV obstacle avoidance targeting neuromorphic processors.
<em>TNNLS</em>, <em>31</em>(9), 3305–3318. (<a
href="https://doi.org/10.1109/TNNLS.2019.2941506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lobula giant movement detector (LGMD) is an identified neuron of the locust that detects looming objects and triggers the insect&#39;s escape responses. Understanding the neural principles and network structure that leads to these fast and robust responses can facilitate the design of efficient obstacle avoidance strategies for robotic applications. Here, we present a neuromorphic spiking neural network model of the LGMD driven by the output of a neuromorphic dynamic vision sensor (DVS), which incorporates spiking frequency adaptation and synaptic plasticity mechanisms, and which can be mapped onto existing neuromorphic processor chips. However, as the model has a wide range of parameters and the mixed-signal analog-digital circuits used to implement the model are affected by variability and noise, it is necessary to optimize the parameters to produce robust and reliable responses. Here, we propose to use differential evolution (DE) and Bayesian optimization (BO) techniques to optimize the parameter space and investigate the use of self-adaptive DE (SADE) to ameliorate the difficulties of finding appropriate input parameters for the DE technique. We quantify the performance of the methods proposed with a comprehensive comparison of different optimizers applied to the model and demonstrate the validity of the approach proposed using recordings made from a DVS sensor mounted on an unmanned aerial vehicle (UAV).},
  archive      = {J_TNNLS},
  author       = {Llewyn Salt and David Howard and Giacomo Indiveri and Yulia Sandamirskaya},
  doi          = {10.1109/TNNLS.2019.2941506},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3305-3318},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter optimization and learning in a spiking neural network for UAV obstacle avoidance targeting neuromorphic processors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the working principle of the hopfield neural networks and
its equivalence to the GADIA in optimization. <em>TNNLS</em>,
<em>31</em>(9), 3294–3304. (<a
href="https://doi.org/10.1109/TNNLS.2019.2940920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hopfield neural networks (HNNs) are one of the most well-known and widely used kinds of neural networks in optimization. In this article, the author focuses on building a deeper understanding of the working principle of the HNN during an optimization process. Our investigations yield several novel results giving some important insights into the working principle of both continuous and discrete HNNs. This article shows that what the traditional HNN actually does as energy function decreases is to divide the neurons into two classes in such a way that the sum of biased class volumes is minimized (or maximized) regardless of the types of the optimization problems. Introducing neuron-specific class labels, the author concludes that the traditional discrete HNN is actually a special case of the greedy asynchronous distributed interference avoidance algorithm (GADIA) [17] of Babadi and Tarokh for the 2-class optimization problems. The computer results confirm the findings.},
  archive      = {J_TNNLS},
  author       = {Zekeriya Uykan},
  doi          = {10.1109/TNNLS.2019.2940920},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3294-3304},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the working principle of the hopfield neural networks and its equivalence to the GADIA in optimization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Region stabilization of switched neural networks with
multiple modes and multiple equilibria: A pole assignment method.
<em>TNNLS</em>, <em>31</em>(9), 3280–3293. (<a
href="https://doi.org/10.1109/TNNLS.2019.2940466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates region stabilization issue of switched neural networks (SNNs) with multiple modes (MMs) and multiple equilibria (ME) via a pole assignment method. In such an SNN, every neuron is observed with more than one mode and unstable equilibrium point. First, SNNs with MMs and ME are modeled in terms of switched systems with unstable subsystems and ME. Second, a necessary and sufficient condition and a sufficient condition are, respectively, proposed for arbitrary switching paths pole assignment and arbitrary periodic/quasi-periodic switching paths (PSPs/QSPs) asymptotically region stabilizing pole assignment of switched linear time-invariant (LTI) systems with ME. It is shown that to stabilize a switched LTI system, some/all poles of all/some linear subsystems can be assigned to suitable locations of the right-half side of the complex plane. Third, based on the obtained pole assignment results, an asymptotical-region-stabilizing-control law observed as distributed state feedback controllers of MMs, asymptotical-region-stabilizing PSPs/QSPs, and a corresponding algorithm are all designed for asymptotical region stabilization of switched linear/nonlinear neural networks with MMs and ME. Finally, a numeral example is given to illustrate the effectiveness and practicality of the new results.},
  archive      = {J_TNNLS},
  author       = {Liying Zhu and Jianbin Qiu and Hamid Reza Karimi},
  doi          = {10.1109/TNNLS.2019.2940466},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3280-3293},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Region stabilization of switched neural networks with multiple modes and multiple equilibria: A pole assignment method},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Supervised dimensionality reduction methods via recursive
regression. <em>TNNLS</em>, <em>31</em>(9), 3269–3279. (<a
href="https://doi.org/10.1109/TNNLS.2019.2940088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the recursive problems of both orthogonal linear discriminant analysis (OLDA) and orthogonal least squares regression (OLSR) are investigated. Different from other works, the associated recursive problems are addressed via a novel recursive regression method, which achieves the dimensionality reduction in the orthogonal complement space heuristically. As for the OLDA, an efficient method is developed to obtain the associated optimal subspace, which is closely related to the orthonormal basis of the optimal solution to the ridge regression. As for the OLSR, the scalable subspace is introduced to build up an original OLSR with optimal scaling (OS). Through further relaxing the proposed problem into a convex parameterized orthogonal quadratic problem, an effective approach is derived, such that not only the optimal subspace can be achieved but also the OS could be obtained automatically. Accordingly, two supervised dimensionality reduction methods are proposed via obtaining the heuristic solutions to the recursive problems of the OLDA and the OLSR.},
  archive      = {J_TNNLS},
  author       = {Yun Liu and Rui Zhang and Feiping Nie and Xuelong Li and Chris Ding},
  doi          = {10.1109/TNNLS.2019.2940088},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3269-3279},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supervised dimensionality reduction methods via recursive regression},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exact passive-aggressive algorithms for ordinal regression
using interval labels. <em>TNNLS</em>, <em>31</em>(9), 3259–3268. (<a
href="https://doi.org/10.1109/TNNLS.2019.2939861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose exact passive-aggressive (PA) online algorithms for ordinal regression. The proposed algorithms can be used even when we have interval labels instead of actual labels for example. The proposed algorithms solve a convex optimization problem at every trial. We find an exact solution to those optimization problems to determine the updated parameters. We propose a support class algorithm (SCA) that finds the active constraints using the Karush-Kuhn-Tucker (KKT) conditions of the optimization problems. These active constraints form a support set, which determines the set of thresholds that need to be updated. We derive update rules for PA, PA-I, and PA-II. We show that the proposed algorithms maintain the ordering of the thresholds after every trial. We provide the mistake bounds of the proposed algorithms in both ideal and general settings. We also show experimentally that the proposed algorithms successfully learn accurate classifiers using interval labels as well as exact labels. The proposed algorithms also do well compared to other approaches.},
  archive      = {J_TNNLS},
  author       = {Naresh Manwani and Mohit Chandra},
  doi          = {10.1109/TNNLS.2019.2939861},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3259-3268},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exact passive-aggressive algorithms for ordinal regression using interval labels},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised graph regularized deep NMF with
bi-orthogonal constraints for data representation. <em>TNNLS</em>,
<em>31</em>(9), 3245–3258. (<a
href="https://doi.org/10.1109/TNNLS.2019.2939637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised non-negative matrix factorization (NMF) exploits the strengths of NMF in effectively learning local information contained in data and is also able to achieve effective learning when only a small fraction of data is labeled. NMF is particularly useful for dimensionality reduction of high-dimensional data. However, the mapping between the low-dimensional representation, learned by semi-supervised NMF, and the original high-dimensional data contains complex hierarchical and structural information, which is hard to extract by using only single-layer clustering methods. Therefore, in this article, we propose a new deep learning method, called semi-supervised graph regularized deep NMF with bi-orthogonal constraints (SGDNMF). SGDNMF learns a representation from the hidden layers of a deep network for clustering, which contains varied and unknown attributes. Bi-orthogonal constraints on two factor matrices are introduced into our SGDNMF model, which can make the solution unique and improve clustering performance. This improves the effect of dimensionality reduction because it only requires a small fraction of data to be labeled. In addition, SGDNMF incorporates dual-hypergraph Laplacian regularization, which can reinforce high-order relationships in both data and feature spaces and fully retain the intrinsic geometric structure of the original data. This article presents the details of the SGDNMF algorithm, including the objective function and the iterative updating rules. Empirical experiments on four different data sets demonstrate state-of-the-art performance of SGDNMF in comparison with six other prominent algorithms.},
  archive      = {J_TNNLS},
  author       = {Yang Meng and Ronghua Shang and Fanhua Shang and Licheng Jiao and Shuyuan Yang and Rustam Stolkin},
  doi          = {10.1109/TNNLS.2019.2939637},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3245-3258},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semi-supervised graph regularized deep NMF with bi-orthogonal constraints for data representation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust distance measure for similarity-based
classification on the SPD manifold. <em>TNNLS</em>, <em>31</em>(9),
3230–3244. (<a
href="https://doi.org/10.1109/TNNLS.2019.2939177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The symmetric positive definite (SPD) matrices, forming a Riemannian manifold, are commonly used as visual representations. The non-Euclidean geometry of the manifold often makes developing learning algorithms (e.g., classifiers) difficult and complicated. The concept of similarity-based learning has been shown to be effective to address various problems on SPD manifolds. This is mainly because the similarity-based algorithms are agnostic to the geometry and purely work based on the notion of similarities/distances. However, existing similarity-based models on SPD manifolds opt for holistic representations, ignoring characteristics of information captured by SPD matrices. To circumvent this limitation, we propose a novel SPD distance measure for the similarity-based algorithm. Specifically, we introduce the concept of point-to-set transformation, which enables us to learn multiple lower dimensional and discriminative SPD manifolds from a higher dimensional one. For lower dimensional SPD manifolds obtained by the point-to-set transformation, we propose a tailored set-to-set distance measure by making use of the family of alpha-beta divergences. We further propose to learn the point-to-set transformation and the set-to-set distance measure jointly, yielding a powerful similarity-based algorithm on SPD manifolds. Our thorough evaluations on several visual recognition tasks (e.g., action classification and face recognition) suggest that our algorithm comfortably outperforms various state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Zhi Gao and Yuwei Wu and Mehrtash Harandi and Yunde Jia},
  doi          = {10.1109/TNNLS.2019.2939177},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3230-3244},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A robust distance measure for similarity-based classification on the SPD manifold},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatically design convolutional neural networks by
optimization with submodularity and supermodularity. <em>TNNLS</em>,
<em>31</em>(9), 3215–3229. (<a
href="https://doi.org/10.1109/TNNLS.2019.2939157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The architecture of convolutional neural networks (CNNs) is a key factor of influencing their performance. Although deep CNNs perform well in many difficult problems, how to intelligently design the architecture is still a challenging problem. Focusing on two practical architectural design problems: to maximize the accuracy with a given forward running time and to minimize the forward running time with a given accuracy requirement, we innovatively utilize prior knowledge to convert architecture optimization problems into submodular optimization problems. We propose efficient Greedy algorithms to solve them and give theoretical bounds of our algorithms. Specifically, we employ the techniques on some public data sets and compare our algorithms with some other hyperparameter optimization methods. Experiments show our algorithms&#39; efficiency.},
  archive      = {J_TNNLS},
  author       = {Wenzheng Hu and Junqi Jin and Tie-Yan Liu and Changshui Zhang},
  doi          = {10.1109/TNNLS.2019.2939157},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3215-3229},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatically design convolutional neural networks by optimization with submodularity and supermodularity},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). General 7-instant DCZNN model solving future different-level
system of nonlinear inequality and linear equation. <em>TNNLS</em>,
<em>31</em>(9), 3204–3214. (<a
href="https://doi.org/10.1109/TNNLS.2019.2938866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel and challenging problem called future different-level system of nonlinear inequality and linear equation (FDLSNILE) is proposed and investigated. To solve FDLSNILE, the corresponding continuous different-level system of nonlinear inequality and linear equation (CDLSNILE) is first analyzed, and then, a continuous combined zeroing neural network (CCZNN) model for solving CDLSNILE is proposed. To obtain a discrete combined zeroing neural network (DCZNN) model for solving FDLSNILE, a high-precision general 7-instant Zhang et al. discretization (ZeaD) formula for the first-order time derivative approximation is proposed. Furthermore, by applying the general 7-instant ZeaD formula to discretize the CCZNN model, a general 7-instant DCZNN (7IDCZNN) model is thus proposed for solving FDLSNILE. For comparison, by using three conventional ZeaD formulas, three conventional DCZNN models are also developed. Meanwhile, theoretical analyses and results guarantee the efficacy and superiority of the general 7IDCZNN model compared with the other three conventional DCZNN models for solving FDLSNILE. Finally, several comparative numerical experiments, including the motion control of a 5-link redundant manipulator, are provided to substantiate the efficacy and superiority of the general 7-instant ZeaD formula and the corresponding 7IDCZNN model.},
  archive      = {J_TNNLS},
  author       = {Min Yang and Yunong Zhang and Haifeng Hu and Binbin Qiu},
  doi          = {10.1109/TNNLS.2019.2938866},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3204-3214},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {General 7-instant DCZNN model solving future different-level system of nonlinear inequality and linear equation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and clustering positive vectors via nonparametric
mixture models of liouville distributions. <em>TNNLS</em>,
<em>31</em>(9), 3193–3203. (<a
href="https://doi.org/10.1109/TNNLS.2019.2938830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an effective mixture model-based approach to modeling and clustering positive data vectors. Our mixture model is based on the inverted Beta-Liouville (IBL) distribution which is extracted from the family of Liouville distributions. To cope with the problem of determining the appropriate number of clusters in our approach, a nonparametric Bayesian framework is used to extend the IBL mixture to an infinite mixture model in which the number of clusters is assumed to be infinite initially and will be inferred automatically during the learning process. To optimize the proposed model, we propose a convergence-guaranteed learning algorithm based on the averaged collapsed variational Bayes inference that can effectively learn model parameters with closed-form solutions. The effectiveness of the proposed infinite IBL mixture model for modeling and clustering positive vectors is validated through both synthetic and real-world data sets.},
  archive      = {J_TNNLS},
  author       = {Wentao Fan and Nizar Bouguila},
  doi          = {10.1109/TNNLS.2019.2938830},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3193-3203},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modeling and clustering positive vectors via nonparametric mixture models of liouville distributions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Harnessing side information for classification under label
noise. <em>TNNLS</em>, <em>31</em>(9), 3178–3192. (<a
href="https://doi.org/10.1109/TNNLS.2019.2938782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled. Such noisy labels will mislead the classifier training and severely decrease the classification performance. Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empirical risk minimization. However, they are only suitable for binary classification and also require strong prior knowledge. Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem. We denote our proposed method as “label noise handling via side information” (LNSI). Specifically, the observed label matrix is decomposed as the sum of two parts, in which the first part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix. The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been sufficiently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems. Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classification error of LNSI is upper bounded. The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets confirm the superiority of LNSI to state-of-the-art approaches on label noise handling.},
  archive      = {J_TNNLS},
  author       = {Yang Wei and Chen Gong and Shuo Chen and Tongliang Liu and Jian Yang and Dacheng Tao},
  doi          = {10.1109/TNNLS.2019.2938782},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3178-3192},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Harnessing side information for classification under label noise},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exponential state estimation for stochastically disturbed
discrete-time memristive neural networks: Multiobjective approach.
<em>TNNLS</em>, <em>31</em>(9), 3168–3177. (<a
href="https://doi.org/10.1109/TNNLS.2019.2938774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state estimation of the discrete-time memristive model is studied in this article. By applying the stochastic analysis technique, sufficient formulas are established to ensure the exponentially mean-square stability of the error model. Moreover, the derived control gain matrix can be calculated via the linear matrix inequality (LMI). It should be mentioned that, by extending the derived conclusion to a multiobjective optimization problem, the maximum bound of the active function and the minimum bound of the disturbance attenuation are derived. The corresponding simulation figures are provided in the end.},
  archive      = {J_TNNLS},
  author       = {Ruoxia Li and Xingbao Gao and Jinde Cao},
  doi          = {10.1109/TNNLS.2019.2938774},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3168-3177},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential state estimation for stochastically disturbed discrete-time memristive neural networks: Multiobjective approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Memory augmented deep recurrent neural network for video
question answering. <em>TNNLS</em>, <em>31</em>(9), 3159–3167. (<a
href="https://doi.org/10.1109/TNNLS.2019.2938015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video question answering (VideoQA) is a very important but challenging multimedia task, which automatically analyzes questions and videos and generates accurate answers. However, research on VideoQA is still in its infancy. In this article, we propose a novel memory augmented deep recurrent neural network (MA-DRNN) model for VideoQA, which features a new method for encoding videos and questions, and memory augmentation using the emerging differentiable neural computer (DNC). Specifically, we encode textual (questions) information before visual (videos) information, which leads to better visual-textual representations. Moreover, we leverage DNC (with an external memory) for storing and retrieving useful information in questions and videos, and modeling the long-term visual-textual dependence. To evaluate the proposed model, we conducted extensive experiments using the VTW data set and MSVD-QA data set, which are both Widely used large-scale video data sets for language-level understanding. The experimental results have well validated the proposed model and showed that it outperforms the state-of-the-art in terms of various accuracy-related metrics.},
  archive      = {J_TNNLS},
  author       = {Chengxiang Yin and Jian Tang and Zhiyuan Xu and Yanzhi Wang},
  doi          = {10.1109/TNNLS.2019.2938015},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3159-3167},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memory augmented deep recurrent neural network for video question answering},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-batch reference learning for deep retrieval.
<em>TNNLS</em>, <em>31</em>(9), 3145–3158. (<a
href="https://doi.org/10.1109/TNNLS.2019.2936876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning effective representations that exhibit semantic content is crucial to image retrieval applications. Recent advances in deep learning have made significant improvements in performance on a number of visual recognition tasks. Studies have also revealed that visual features extracted from a deep network learned on a large-scale image data set (e.g., ImageNet) for classification are generic and perform well on new recognition tasks in different domains. Nevertheless, when applied to image retrieval, such deep representations do not attain performance as impressive as used for classification. This is mainly because the deep features are optimized for classification rather than for the desired retrieval task. We introduce the cross-batch reference (CBR), a novel training mechanism that enables the optimization of deep networks with a retrieval criterion. With the CBR, the networks leverage both the samples in a single minibatch and the samples in the others for weight updates, enhancing the stochastic gradient descent (SGD) training by enabling interbatch information passing. This interbatch communication is implemented as a cross-batch retrieval process in which the networks are trained to maximize the mean average precision (mAP) that is a popular performance measure in retrieval. Maximizing the cross-batch mAP is equivalent to centralizing the samples relevant to each other in the feature space and separating the samples irrelevant to each other. The learned features can discriminate between relevant and irrelevant samples and thus are suitable for retrieval. To circumvent the discrete, nondifferentiable mAP maximization, we derive an approximate, differentiable lower bound that can be easily optimized in deep networks. Furthermore, the mAP loss can be used alone or with a classification loss. Experiments on several data sets demonstrate that our CBR learning provides favorable performance, validating its effectiveness.},
  archive      = {J_TNNLS},
  author       = {Huei-Fang Yang and Kevin Lin and Ting-Yen Chen and Chu-Song Chen},
  doi          = {10.1109/TNNLS.2019.2936876},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3145-3158},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-batch reference learning for deep retrieval},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(8), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.3009709},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised anomaly detection with LSTM neural networks.
<em>TNNLS</em>, <em>31</em>(8), 3127–3141. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate anomaly detection in an unsupervised framework and introduce long short-term memory (LSTM) neural network-based algorithms. In particular, given variable length data sequences, we first pass these sequences through our LSTM-based structure and obtain fixed-length sequences. We then find a decision function for our anomaly detectors based on the one-class support vector machines (OC-SVMs) and support vector data description (SVDD) algorithms. As the first time in the literature, we jointly train and optimize the parameters of the LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective gradient and quadratic programming-based training methods. To apply the gradient-based training method, we modify the original objective criteria of the OC-SVM and SVDD algorithms, where we prove the convergence of the modified objective criteria to the original criteria. We also provide extensions of our unsupervised formulation to the semisupervised and fully supervised frameworks. Thus, we obtain anomaly detection algorithms that can process variable length data sequences while providing high performance, especially for time series data. Our approach is generic so that we also apply this approach to the gated recurrent unit (GRU) architecture by directly replacing our LSTM-based structure with the GRU-based structure. In our experiments, we illustrate significant performance gains achieved by our algorithms with respect to the conventional methods.},
  archive      = {J_TNNLS},
  author       = {Tolga Ergen and Suleyman Serdar Kozat},
  doi          = {10.1109/TNNLS.2019.2935975},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3127-3141},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised anomaly detection with LSTM neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-efficient LSTM networks for online learning.
<em>TNNLS</em>, <em>31</em>(8), 3114–3126. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate variable-length data regression in an online setting and introduce an energy-efficient regression structure build on long short-term memory (LSTM) networks. For this structure, we also introduce highly effective online training algorithms. We first provide a generic LSTM-based regression structure for variable-length input sequences. To reduce the complexity of this structure, we then replace the regular multiplication operations with an energy-efficient operator, i.e., the ef-operator. To further reduce the complexity, we apply factorizations to the weight matrices in the LSTM network so that the total number of parameters to be trained is significantly reduced. We then introduce online training algorithms based on the stochastic gradient descent (SGD) and exponentiated gradient (EG) algorithms to learn the parameters of the introduced network. Thus, we obtain highly efficient and effective online learning algorithms based on the LSTM network. Thanks to our generic approach, we also provide and simulate an energy-efficient gated recurrent unit (GRU) network in our experiments. Through an extensive set of experiments, we illustrate significant performance gains and complexity reductions achieved by the introduced algorithms with respect to the conventional methods.},
  archive      = {J_TNNLS},
  author       = {Tolga Ergen and Ali H. Mirza and Suleyman Serdar Kozat},
  doi          = {10.1109/TNNLS.2019.2935796},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3114-3126},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Energy-efficient LSTM networks for online learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probability density rank-based quantization for convex
universal learning machines. <em>TNNLS</em>, <em>31</em>(8), 3100–3113.
(<a href="https://doi.org/10.1109/TNNLS.2019.2935502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributions of input data are very important for learning machines, such as the convex universal learning machines (CULMs). The CULMs are a family of universal learning machines with convex optimization. However, the computational complexity is a crucial problem in CULMs, because the dimension of the nonlinear mapping layer (the hidden layer) of the CULMs is usually rather large in complex system modeling. In this article, we propose an efficient quantization method called Probability density Rank-based Quantization (PRQ) to decrease the computational complexity of CULMs. The PRQ ranks the data according to the estimated probability densities and then selects a subset whose elements are equally spaced in the ranked data sequence. We apply the PRQ to kernel ridge regression (KRR) and random Fourier feature recursive least squares (RFF-RLS), which are two typical algorithms of CULMs. The proposed method not only keeps the similarity of data distribution between the code book and data set but also reduces the computational cost by using the kd-tree. Meanwhile, for a given data set, the method yields deterministic quantization results, and it can also exclude the outliers and avoid too many borders in the code book. This brings great convenience to practical applications of the CULMs. The proposed PRQ is evaluated on several real-world benchmark data sets. Experimental results show satisfactory performance of PRQ compared with some state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zhengda Qin and Badong Chen and Yuantao Gu and Nanning Zheng and Jose C. Principe},
  doi          = {10.1109/TNNLS.2019.2935502},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3100-3113},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probability density rank-based quantization for convex universal learning machines},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient implementation of second-order stochastic
approximation algorithms in high-dimensional problems. <em>TNNLS</em>,
<em>31</em>(8), 3087–3099. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic approximation (SA) algorithms have been widely applied in minimization problems when the loss functions and/or the gradient information are only accessible through noisy evaluations. Stochastic gradient (SG) descent-a first-order algorithm and a workhorse of much machine learning-is perhaps the most famous form of SA. Among all SA algorithms, the second-order simultaneous perturbation stochastic approximation (2SPSA) and the second-order stochastic gradient (2SG) are particularly efficient in handling highdimensional problems, covering both gradient-free and gradientbased scenarios. However, due to the necessary matrix operations, the per-iteration floating-point-operations (FLOPs) cost of the standard 2SPSA/2SG is O(p 3 ), where p is the dimension of the underlying parameter. Note that the O(p 3 ) FLOPs cost is distinct from the classical SPSA-based per-iteration O(1) cost in terms of the number of noisy function evaluations. In this work, we propose a technique to efficiently implement the 2SPSA/2SG algorithms via the symmetric indefinite matrix factorization and show that the FLOPs cost is reduced from O(p 3 ) to O(p 2 ). The formal almost sure convergence and rate of convergence for the newly proposed approach are directly inherited from the standard 2SPSA/2SG. The improvement in efficiency and numerical stability is demonstrated in two numerical studies.},
  archive      = {J_TNNLS},
  author       = {Jingyi Zhu and Long Wang and James C. Spall},
  doi          = {10.1109/TNNLS.2019.2935455},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3087-3099},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient implementation of second-order stochastic approximation algorithms in high-dimensional problems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised domain adaptation with adversarial residual
transform networks. <em>TNNLS</em>, <em>31</em>(8), 3073–3086. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) is widely used in learning problems lacking labels. Recent studies show that deep adversarial DA models can make markable improvements in performance, which include symmetric and asymmetric architectures. However, the former has poor generalization ability, whereas the latter is very hard to train. In this article, we propose a novel adversarial DA method named adversarial residual transform networks (ARTNs) to improve the generalization ability, which directly transforms the source features into the space of target features. In this model, residual connections are used to share features and adversarial loss is reconstructed, thus making the model more generalized and easier to train. Moreover, a special regularization term is added to the loss function to alleviate a vanishing gradient problem, which enables its training process stable. A series of experiments based on Amazon review data set, digits data sets, and Office-31 image data sets are conducted to show that the proposed ARTN can be comparable with the methods of the state of the art.},
  archive      = {J_TNNLS},
  author       = {Guanyu Cai and Yuqin Wang and Lianghua He and MengChu Zhou},
  doi          = {10.1109/TNNLS.2019.2935384},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3073-3086},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised domain adaptation with adversarial residual transform networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-atlas segmentation of anatomical brain structures
using hierarchical hypergraph learning. <em>TNNLS</em>, <em>31</em>(8),
3061–3072. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of anatomical brain structures is crucial for many neuroimaging applications, e.g., early brain development studies and the study of imaging biomarkers of neurodegenerative diseases. Although multi-atlas segmentation (MAS) has achieved many successes in the medical imaging area, this approach encounters limitations in segmenting anatomical structures associated with poor image contrast. To address this issue, we propose a new MAS method that uses a hypergraph learning framework to model the complex subject-within and subject-to-atlas image voxel relationships and propagate the label on the atlas image to the target subject image. To alleviate the low-image contrast issue, we propose two strategies equipped with our hypergraph learning framework. First, we use a hierarchical strategy that exploits high-level context features for hypergraph construction. Because the context features are computed on the tentatively estimated probability maps, we can ultimately turn the hypergraph learning into a hierarchical model. Second, instead of only propagating the labels from the atlas images to the target subject image, we use a dynamic label propagation strategy that can gradually use increasing reliably identified labels from the subject image to aid in predicting the labels on the difficult-to-label subject image voxels. Compared with the state-of-the-art label fusion methods, our results show that the hierarchical hypergraph learning framework can substantially improve the robustness and accuracy in the segmentation of anatomical brain structures with low image contrast from magnetic resonance (MR) images.},
  archive      = {J_TNNLS},
  author       = {Pei Dong and Yanrong Guo and Yue Gao and Peipeng Liang and Yonghong Shi and Guorong Wu},
  doi          = {10.1109/TNNLS.2019.2935184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3061-3072},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-atlas segmentation of anatomical brain structures using hierarchical hypergraph learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Graph edge convolutional neural networks for skeleton-based
action recognition. <em>TNNLS</em>, <em>31</em>(8), 3047–3060. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body joints, directly obtained from a pose estimation model, have proven effective for action recognition. Existing works focus on analyzing the dynamics of human joints. However, except joints, humans also explore motions of limbs for understanding actions. Given this observation, we investigate the dynamics of human limbs for skeleton-based action recognition. Specifically, we represent an edge in a graph of a human skeleton by integrating its spatial neighboring edges (for encoding the cooperation between different limbs) and its temporal neighboring edges (for achieving the consistency of movements in an action). Based on this new edge representation, we devise a graph edge convolutional neural network (CNN). Considering the complementarity between graph node convolution and edge convolution, we further construct two hybrid networks by introducing different shared intermediate layers to integrate graph node and edge CNNs. Our contributions are twofold, graph edge convolution and hybrid networks for integrating the proposed edge convolution and the conventional node convolution. Experimental results on the Kinetics and NTU-RGB+D data sets demonstrate that our graph edge convolution is effective at capturing the characteristics of actions and that our graph edge CNN significantly outperforms the existing state-of-the-art skeleton-based action recognition methods.},
  archive      = {J_TNNLS},
  author       = {Xikun Zhang and Chang Xu and Xinmei Tian and Dacheng Tao},
  doi          = {10.1109/TNNLS.2019.2935173},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3047-3060},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph edge convolutional neural networks for skeleton-based action recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Siamese dilated inception hashing with intra-group
correlation enhancement for image retrieval. <em>TNNLS</em>,
<em>31</em>(8), 3032–3046. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For large-scale image retrieval, hashing has been extensively explored in approximate nearest neighbor search methods due to its low storage and high computational efficiency. With the development of deep learning, deep hashing methods have made great progress in image retrieval. Most existing deep hashing methods cannot fully consider the intra-group correlation of hash codes, which leads to the correlation decrease problem of similar hash codes and ultimately affects the retrieval results. In this article, we propose an end-to-end siamese dilated inception hashing (SDIH) method that takes full advantage of multi-scale contextual information and category-level semantics to enhance the intra-group correlation of hash codes for hash codes learning. First, a novel siamese inception dilated network architecture is presented to generate hash codes with the intra-group correlation enhancement by exploiting multi-scale contextual information and category-level semantics simultaneously. Second, we propose a new regularized term, which can force the continuous values to approximate discrete values in hash codes learning and eventually reduces the discrepancy between the Hamming distance and the Euclidean distance. Finally, experimental results in five public data sets demonstrate that SDIH can outperform other state-of-the-art hashing algorithms.},
  archive      = {J_TNNLS},
  author       = {Xiaoqiang Lu and Yaxiong Chen and Xuelong Li},
  doi          = {10.1109/TNNLS.2019.2935118},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3032-3046},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Siamese dilated inception hashing with intra-group correlation enhancement for image retrieval},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted broad learning system and its application in
nonlinear industrial process modeling. <em>TNNLS</em>, <em>31</em>(8),
3017–3031. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning system (BLS) is a novel neural network with effective and efficient learning ability. BLS has attracted increasing attention from many scholars owing to its excellent performance. This article proposes a weighted BLS (WBLS) based on BLS to tackle the noise and outliers in an industrial process. WBLS provides a unified framework for easily using different methods of calculating the weighted penalty factor. Using the weighted penalty factor to constrain the contribution of each sample to modeling, the normal and abnormal samples were allocated higher and lower weights to increase and decrease their contributions, respectively. Hence, the WBLS can eliminate the bad effect of noise and outliers on the modeling. The weighted ridge regression algorithm is used to compute the algorithm solution. Weighted incremental learning algorithms are also developed using the weighted penalty factor to tackle the noise and outliers in the additional samples and quickly increase nodes or samples without retraining. The proposed weighted incremental learning algorithms provide a unified framework for using different methods of computing weights. We test the feasibility of the proposed algorithms on some public data sets and a real-world application. Experiment results show that our method has better generalization and robustness.},
  archive      = {J_TNNLS},
  author       = {Fei Chu and Tao Liang and C. L. Philip Chen and Xuesong Wang and Xiaoping Ma},
  doi          = {10.1109/TNNLS.2019.2935033},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3017-3031},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weighted broad learning system and its application in nonlinear industrial process modeling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse manifold-regularized neural networks for polarimetric
SAR terrain classification. <em>TNNLS</em>, <em>31</em>(8), 3007–3016.
(<a href="https://doi.org/10.1109/TNNLS.2019.2935027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new deep neural network based on sparse filtering and manifold regularization (DSMR) is proposed for feature extraction and classification of polarimetric synthetic aperture radar (PolSAR) data. DSMR uses a novel deep neural network (DNN) to automatically learn features from raw SAR data. During preprocessing, the spatial information between pixels on PolSAR images is exploited to weight each data sample. Then, in the pretraining and fine-tuning, DSMR uses the population sparsity and the lifetime sparsity (dual sparsity) to learn the global features and preserves the local structure of data by neighborhood-based manifold regularization. The dual sparsity only needs to tune a few parameters, and the manifold regularization cuts down the number of training samples. Experimental results on synthesized and real PolSAR data sets from different SAR systems show that DSMR can improve classification accuracy compared with conventional DNNs, even for data sets with a large angle of incidence.},
  archive      = {J_TNNLS},
  author       = {Hongying Liu and Fanhua Shang and Shuyuan Yang and Maoguo Gong and Tianwen Zhu and Licheng Jiao},
  doi          = {10.1109/TNNLS.2019.2935027},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3007-3016},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse manifold-regularized neural networks for polarimetric SAR terrain classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A semismooth newton algorithm for high-dimensional nonconvex
sparse learning. <em>TNNLS</em>, <em>31</em>(8), 2993–3006. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The smoothly clipped absolute deviation (SCAD) and the minimax concave penalty (MCP)-penalized regression models are two important and widely used nonconvex sparse learning tools that can handle variable selection and parameter estimation simultaneously and thus have potential applications in various fields, such as mining biological data in high-throughput biomedical studies. Theoretically, these two models enjoy the oracle property even in the high-dimensional settings, where the number of predictors p may be much larger than the number of observations n. However, numerically, it is quite challenging to develop fast and stable algorithms due to their nonconvexity and nonsmoothness. In this article, we develop a fast algorithm for SCAD- and MCP-penalized learning problems. First, we show that the global minimizers of both models are roots of the nonsmooth equations. Then, a semismooth Newton (SSN) algorithm is employed to solve the equations. We prove that the SSN algorithm converges locally and superlinearly to the Karush-Kuhn- Tucker (KKT) points. The computational complexity analysis shows that the cost of the SSN algorithm per iteration is O(np). Combined with the warm-start technique, the SSN algorithm can be very efficient and accurate. Simulation studies and a real data example suggest that our SSN algorithm, with comparable solution accuracy with the coordinate descent (CD) and the difference of convex (DC) proximal Newton algorithms, is more computationally efficient.},
  archive      = {J_TNNLS},
  author       = {Yueyong Shi and Jian Huang and Yuling Jiao and Qinglong Yang},
  doi          = {10.1109/TNNLS.2019.2935001},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2993-3006},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A semismooth newton algorithm for high-dimensional nonconvex sparse learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New varying-parameter ZNN models with finite-time
convergence and noise suppression for time-varying matrix moore–penrose
inversion. <em>TNNLS</em>, <em>31</em>(8), 2980–2992. (<a
href="https://doi.org/10.1109/TNNLS.2019.2934734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to solve the Moore-Penrose inverse of time-varying full-rank matrices in the presence of various noises in real time. For this purpose, two varying-parameter zeroing neural networks (VPZNNs) are proposed. Specifically, VPZNN-R and VPZNN-L models, which are based on a new design formula, are designed to solve the right and left Moore-Penrose inversion problems of time-varying full-rank matrices, respectively. The two VPZNN models are activated by two novel varying-parameter nonlinear activation functions. Detailed theoretical derivations are presented to show the desired finite-time convergence and outstanding robustness of the proposed VPZNN models under various kinds of noises. In addition, existing neural models, such as the original ZNN (OZNN) and the integration-enhanced ZNN (IEZNN), are compared with the VPZNN models. Simulation observations verify the advantages of the VPZNN models over the OZNN and IEZNN models in terms of convergence and robustness. The potential of the VPZNN models for robotic applications is then illustrated by an example of robot path tracking.},
  archive      = {J_TNNLS},
  author       = {Zhiguo Tan and Weibing Li and Lin Xiao and Yueming Hu},
  doi          = {10.1109/TNNLS.2019.2934734},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2980-2992},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New varying-parameter ZNN models with finite-time convergence and noise suppression for time-varying matrix Moore–Penrose inversion},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A double-variational bayesian framework in random fourier
features for indefinite kernels. <em>TNNLS</em>, <em>31</em>(8),
2965–2979. (<a
href="https://doi.org/10.1109/TNNLS.2019.2934729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Fourier features (RFFs) have been successfully employed to kernel approximation in large-scale situations. The rationale behind RFF relies on Bochner&#39;s theorem, but the condition is too strict and excludes many widely used kernels, e.g., dot-product kernels (violates the shift-invariant condition) and indefinite kernels [violates the positive definite (PD) condition]. In this article, we present a unified RFF framework for indefinite kernel approximation in the reproducing kernel Kreĭn spaces (RKKSs). Besides, our model is also suited to approximate a dot-product kernel on the unit sphere, as it can be transformed into a shift-invariant but indefinite kernel. By the Kolmogorov decomposition scheme, an indefinite kernel in RKKS can be decomposed into the difference of two unknown PD kernels. The spectral distribution of each underlying PD kernel can be formulated as a nonparametric Bayesian Gaussian mixtures model. Based on this, we propose a double-infinite Gaussian mixture model in RFF by placing the Dirichlet process prior. It takes full advantage of high flexibility on the number of components and has the capability of approximating indefinite kernels on a wide scale. In model inference, we develop a non-conjugate variational algorithm with a sub-sampling scheme for the posterior inference. It allows for the non-conjugate case in our model and is quite efficient due to the sub-sampling strategy. Experimental results on several large classification data sets demonstrate the effectiveness of our nonparametric Bayesian model for indefinite kernel approximation when compared to other representative random feature-based methods.},
  archive      = {J_TNNLS},
  author       = {Fanghui Liu and Xiaolin Huang and Lei Shi and Jie Yang and Johan A. K. Suykens},
  doi          = {10.1109/TNNLS.2019.2934729},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2965-2979},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A double-variational bayesian framework in random fourier features for indefinite kernels},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Synchronization and consensus in networks of linear
fractional-order multi-agent systems via sampled-data control.
<em>TNNLS</em>, <em>31</em>(8), 2955–2964. (<a
href="https://doi.org/10.1109/TNNLS.2019.2934648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses synchronization and consensus problems in networks of linear fractional-order multi-agent systems (LFOMAS) via sampled-data control. First, under very mild assumptions, the necessary and sufficient conditions are obtained for achieving synchronization in networks of LFOMAS. Second, the results of synchronization are applied to solve some consensus problems in networks of LFOMAS. In the obtained results, the coupling matrix does not have to be a Laplacian matrix, its off-diagonal elements do not have to be nonnegative, and its row-sum can be nonzero. Finally, the validity of the theoretical results is verified by three simulation examples.},
  archive      = {J_TNNLS},
  author       = {Jiejie Chen and Boshan Chen and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2019.2934648},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2955-2964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization and consensus in networks of linear fractional-order multi-agent systems via sampled-data control},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Barrier function-based adaptive control for uncertain
strict-feedback systems within predefined neural network approximation
sets. <em>TNNLS</em>, <em>31</em>(8), 2942–2954. (<a
href="https://doi.org/10.1109/TNNLS.2019.2934403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a globally stable adaptive control strategy for uncertain strict-feedback systems is proposed within predefined neural network (NN) approximation sets, despite the presence of unknown system nonlinearities. In contrast to the conventional adaptive NN control results in the literature, a primary benefit of the developed approach is that the barrier Lyapunov function is employed to predefine the compact set for maintaining the validity of NN approximation at each step, thus accomplishing the global boundedness of all the closed-loop signals. Simulation results are performed to clarify the effectiveness of the proposed methodology.},
  archive      = {J_TNNLS},
  author       = {Yong-Hua Liu and Chun-Yi Su and Hongyi Li and Renquan Lu},
  doi          = {10.1109/TNNLS.2019.2934403},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2942-2954},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Barrier function-based adaptive control for uncertain strict-feedback systems within predefined neural network approximation sets},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable distributed filtering for a class of discrete-time
complex networks over time-varying topology. <em>TNNLS</em>,
<em>31</em>(8), 2930–2941. (<a
href="https://doi.org/10.1109/TNNLS.2019.2934131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the distributed filtering problem for a class of discrete complex networks over time-varying topology described by a sequence of variables. In the developed scalable filtering algorithm, only the local information and the information from the neighboring nodes are used. As such, the proposed filter can be implemented in a truly distributed manner at each node, and it is no longer necessary to have a certain center node collecting information from all the nodes. The aim of the addressed filtering problem is to design a time-varying filter for each node such that an upper bound of the filtering error covariance is ensured and the desired filter gain is then calculated by minimizing the obtained upper bound. The filter is established by solving two sets of recursive matrix equations, and thus, the algorithm is suitable for online application. Sufficient conditions are provided under which the filtering error is exponentially bounded in mean square. The monotonicity of the filtering error with respect to the coupling strength is discussed as well. Finally, an illustrative example is presented to demonstrate the feasibility and effectiveness of our distributed filtering strategy.},
  archive      = {J_TNNLS},
  author       = {Yang Liu and Zidong Wang and Donghua Zhou},
  doi          = {10.1109/TNNLS.2019.2934131},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2930-2941},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scalable distributed filtering for a class of discrete-time complex networks over time-varying topology},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolutionary compression of deep neural networks for
biomedical image segmentation. <em>TNNLS</em>, <em>31</em>(8),
2916–2929. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical image segmentation is lately dominated by deep neural networks (DNNs) due to their surpassing expert-level performance. However, the existing DNN models for biomedical image segmentation are generally highly parameterized, which severely impede their deployment on real-time platforms and portable devices. To tackle this difficulty, we propose an evolutionary compression method (ECDNN) to automatically discover efficient DNN architectures for biomedical image segmentation. Different from the existing studies, ECDNN can optimize network loss and number of parameters simultaneously during the evolution, and search for a set of Pareto-optimal solutions in a single run, which is useful for quantifying the tradeoff in satisfying different objectives, and flexible for compressing DNN when preference information is uncertain. In particular, a set of novel genetic operators is proposed for automatically identifying less important filters over the whole network. Moreover, a pruning operator is designed for eliminating convolutional filters from layers involved in feature map concatenation, which is commonly adopted in DNN architectures for capturing multi-level features from biomedical images. Experiments carried out on compressing DNN for retinal vessel and neuronal membrane segmentation tasks show that ECDNN can not only improve the performance without any retraining but also discover efficient network architectures that well maintain the performance. The superiority of the proposed method is further validated by comparison with the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yao Zhou and Gary G. Yen and Zhang Yi},
  doi          = {10.1109/TNNLS.2019.2933879},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2916-2929},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Evolutionary compression of deep neural networks for biomedical image segmentation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Homophily preserving community detection. <em>TNNLS</em>,
<em>31</em>(8), 2903–2915. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental problem in social network analysis, community detection has recently attracted wide attention, accompanied by the output of numerous community detection methods. However, most existing methods are developed by only exploiting link topology, without taking node homophily (i.e., node similarity) into consideration. Thus, much useful information that can be utilized to improve the quality of detected communities is ignored. To overcome this limitation, we propose a new community detection approach based on nonnegative matrix factorization (NMF), namely, homophily preserving NMF (HPNMF), which models not only link topology but also node homophily of networks. As such, HPNMF is able to better reflect the inherent properties of community structure. In order to capture node homophily from scratch, we provide three similarity measurements that naturally reveal the association relationships between nodes. We further present an efficient learning algorithm with convergence guarantee to solve the proposed model. Finally, extensive experiments are conducted, and the results demonstrate that HPNMF has strong ability to outperform the state-of-the-art baseline methods.},
  archive      = {J_TNNLS},
  author       = {Fanghua Ye and Chuan Chen and Zhiyuan Wen and Zibin Zheng and Wuhui Chen and Yuren Zhou},
  doi          = {10.1109/TNNLS.2019.2933850},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2903-2915},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Homophily preserving community detection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SRGC-nets: Sparse repeated group convolutional neural
networks. <em>TNNLS</em>, <em>31</em>(8), 2889–2902. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group convolution is widely used in many mobile networks to remove the filter&#39;s redundancy from the channel extent. In order to further reduce the redundancy of group convolution, this article proposes a novel repeated group convolutional (RGC) kernel, which has M primary groups, and each primary group includes N tiny groups. In every primary group, the same convolutional kernel is repeated in all the tiny groups. The RGC filter is the first kernel to remove the redundancy from group extent. Based on RGC, a sparse RGC (SRGC) kernel is also introduced in this article, and its corresponding network is called SRGC neural networks (SRGC-Net). The SRGC kernel is the summation of RGC kernel and pointwise group convolutional (PGC) kernel. The number of PGC&#39;s groups is M. Accordingly, in each primary group, besides the center locations in all channels, the values of parameters located in other N -1 tiny groups are all zero. Therefore, SRGC can significantly reduce the parameters. Moreover, it can also effectively retrieve spatial and channel-difference features by utilizing RGC and PGC to preserve the richness of produced features. Comparative experiments were performed on the benchmark classification data sets. Compared with the traditional popular networks, SRGC-Nets can perform better with timely reducing the model size and computational complexity. Furthermore, it can also achieve better performances than other latest state-of-the-art mobile networks on most of the databases and effectively decrease the test and training runtime.},
  archive      = {J_TNNLS},
  author       = {Yao Lu and Guangming Lu and Rui Lin and Jinxing Li and David Zhang},
  doi          = {10.1109/TNNLS.2019.2933665},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2889-2902},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SRGC-nets: Sparse repeated group convolutional neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonpooling convolutional neural network forecasting for
seasonal time series with trends. <em>TNNLS</em>, <em>31</em>(8),
2879–2888. (<a
href="https://doi.org/10.1109/TNNLS.2019.2934110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on a problem important to automatic machine learning: the automatic processing of a nonpreprocessed time series. The convolutional neural network (CNN) is one of the most popular neural network (NN) algorithms for pattern recognition. Seasonal time series with trends are the most common data sets used in forecasting. Both the convolutional layer and the pooling layer of a CNN can be used to extract important features and patterns that reflect the seasonality, trends, and time lag correlation coefficients in the data. The ability to identify such features and patterns makes CNN a good candidate algorithm for analyzing seasonal time-series data with trends. This article reports our experimental findings using a fully connected NN (FNN), a nonpooling CNN (NPCNN), and a CNN to study both simulated and real time-series data with seasonality and trends. We found that convolutional layers tend to improve the performance, while pooling layers tend to introduce too many negative effects. Therefore, we recommend using an NPCNN when processing seasonal time-series data with trends. Moreover, we suggest using the Adam optimizer and selecting either a rectified linear unit (ReLU) function or a linear activation function. Using an NN to analyze seasonal time series with trends has become popular in the NN community. This article provides an approach for building a network that fits time-series data with seasonality and trends automatically.},
  archive      = {J_TNNLS},
  author       = {Shuai Liu and Hong Ji and Morgan C. Wang},
  doi          = {10.1109/TNNLS.2019.2934110},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2879-2888},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonpooling convolutional neural network forecasting for seasonal time series with trends},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the dynamics of classification measures for imbalanced
and streaming data. <em>TNNLS</em>, <em>31</em>(8), 2868–2878. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As each imbalanced classification problem comes with its own set of challenges, the measure used to evaluate classifiers must be individually selected. To help researchers make this decision in an informed manner, experimental and theoretical investigations compare general properties of measures. However, existing studies do not analyze changes in measure behavior imposed by different imbalance ratios. Moreover, several characteristics of imbalanced data streams, such as the effect of dynamically changing class proportions, have not been thoroughly investigated from the perspective of different metrics. In this paper, we study measure dynamics by analyzing changes of measure values, distributions, and gradients with diverging class proportions. For this purpose, we visualize measure probability mass functions and gradients. In addition, we put forward a histogram-based normalization method that provides a unified, probabilistic interpretation of any measure over data sets with different class distributions. The results of analyzing eight popular classification measures show that the effect class proportions have on each measure is different and should be taken into account when evaluating classifiers. Apart from highlighting imbalance-related properties of each measure, our study shows a direct connection between class ratio changes and certain types of concept drift, which could be influential in designing new types of classifiers and drift detectors for imbalanced data streams.},
  archive      = {J_TNNLS},
  author       = {Dariusz Brzezinski and Jerzy Stefanowski and Robert Susmaga and Izabela Szczech},
  doi          = {10.1109/TNNLS.2019.2899061},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2868-2878},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the dynamics of classification measures for imbalanced and streaming data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural architectures for highly imbalanced data in
bioinformatics. <em>TNNLS</em>, <em>31</em>(8), 2857–2867. (<a
href="https://doi.org/10.1109/TNNLS.2019.2914471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the postgenome era, many problems in bioinformatics have arisen due to the generation of large amounts of imbalanced data. In particular, the computational classification of precursor microRNA (pre-miRNA) involves a high imbalance in the classes. For this task, a classifier is trained to identify RNA sequences having the highest chance of being miRNA precursors. The big issue is that well-known pre-miRNAs are usually just a few in comparison to the hundreds of thousands of candidate sequences in a genome, which results in highly imbalanced data. This imbalance has a strong influence on most standard classifiers and, if not properly addressed, the classifier is not able to work properly in a real-life scenario. This work provides a comparative assessment of recent deep neural architectures for dealing with the large imbalanced data issue in the classification of pre-miRNAs. We present and analyze recent architectures in a benchmark framework with genomes of animals and plants, with increasing imbalance ratios up to 1:2000. We also propose a new graphical way for comparing classifiers performance in the context of high-class imbalance. The comparative results obtained show that, at a very high imbalance, deep belief neural networks can provide the best performance.},
  archive      = {J_TNNLS},
  author       = {Leandro A. Bugnon and Cristian Yones and Diego H. Milone and Georgina Stegmayer},
  doi          = {10.1109/TNNLS.2019.2914471},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2857-2867},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep neural architectures for highly imbalanced data in bioinformatics},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse supervised representation-based classifier for
uncontrolled and imbalanced classification. <em>TNNLS</em>,
<em>31</em>(8), 2847–2856. (<a
href="https://doi.org/10.1109/TNNLS.2018.2884444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse representation-based classification (SRC) has been utilized in many applications and is an effective algorithm in machine learning. However, the performance of SRC highly depends on the data distribution. Some existing works proved that SRC could not obtain satisfactory results on uncontrolled data sets. Except the uncontrolled data sets, SRC cannot deal with imbalanced classification either. In this paper, we proposed a model named sparse supervised representation classifier (SSRC) to solve the above-mentioned issues. The SSRC involves the class label information during the test sample representation phase to deal with the uncontrolled data sets. In SSRC, each class has the opportunity to linearly represent the test sample in its subspace, which can decrease the influences of the uncontrolled data distribution. In order to classify imbalanced data sets, a class weight learning model is proposed and added to SSRC. Each class weight is learned from its corresponding training samples. The experimental results based on the AR face database (uncontrolled) and 15 KEEL data sets (imbalanced) with an imbalanced rate ranging from 1.48 to 61.18 prove SSRC can effectively classify uncontrolled and imbalanced data sets.},
  archive      = {J_TNNLS},
  author       = {Ting Shu and Bob Zhang and Yuan Yan Tang},
  doi          = {10.1109/TNNLS.2018.2884444},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2847-2856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse supervised representation-based classifier for uncontrolled and imbalanced classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-paced balance learning for clinical skin disease
recognition. <em>TNNLS</em>, <em>31</em>(8), 2832–2846. (<a
href="https://doi.org/10.1109/TNNLS.2019.2917524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a challenging problem in many classification tasks. It induces biased classification results for minority classes that contain less training samples than others. Most existing approaches aim to remedy the imbalanced number of instances among categories by resampling the majority and minority classes accordingly. However, the imbalanced level of difficulty of recognizing different categories is also crucial, especially for distinguishing samples with many classes. For example, in the task of clinical skin disease recognition, several rare diseases have a small number of training samples, but they are easy to diagnose because of their distinct visual properties. On the other hand, some common skin diseases, e.g., eczema, are hard to recognize due to the lack of special symptoms. To address this problem, we propose a self-paced balance learning (SPBL) algorithm in this paper. Specifically, we introduce a comprehensive metric termed the complexity of image category that is a combination of both sample number and recognition difficulty. First, the complexity is initialized using the model of the first pace, where the pace indicates one iteration in the self-paced learning paradigm. We then assign each class a penalty weight that is larger for more complex categories and smaller for easier ones, after which the curriculum is reconstructed by rearranging the training samples. Consequently, the model can iteratively learn discriminative representations via balancing the complexity in each pace. Experimental results on the SD-198 and SD-260 benchmark data sets demonstrate that the proposed SPBL algorithm performs favorably against the state-of-the-art methods. We also demonstrate the effectiveness of the SPBL algorithm&#39;s generalization capacity on various tasks, such as indoor scene image recognition and object classification.},
  archive      = {J_TNNLS},
  author       = {Jufeng Yang and Xiaoping Wu and Jie Liang and Xiaoxiao Sun and Ming-Ming Cheng and Paul L. Rosin and Liang Wang},
  doi          = {10.1109/TNNLS.2019.2917524},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2832-2846},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-paced balance learning for clinical skin disease recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Radial-based oversampling for multiclass imbalanced data
classification. <em>TNNLS</em>, <em>31</em>(8), 2818–2831. (<a
href="https://doi.org/10.1109/TNNLS.2019.2913673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from imbalanced data is among the most popular topics in the contemporary machine learning. However, the vast majority of attention in this field is given to binary problems, while their much more difficult multiclass counterparts are relatively unexplored. Handling data sets with multiple skewed classes poses various challenges and calls for a better understanding of the relationship among classes. In this paper, we propose multiclass radial-based oversampling (MC-RBO), a novel data-sampling algorithm dedicated to multiclass problems. The main novelty of our method lies in using potential functions for generating artificial instances. We take into account information coming from all of the classes, contrary to existing multiclass oversampling approaches that use only minority class characteristics. The process of artificial instance generation is guided by exploring areas where the value of the mutual class distribution is very small. This way, we ensure a smart oversampling procedure that can cope with difficult data distributions and alleviate the shortcomings of existing methods. The usefulness of the MC-RBO algorithm is evaluated on the basis of extensive experimental study and backed-up with a thorough statistical analysis. Obtained results show that by taking into account information coming from all of the classes and conducting a smart oversampling, we can significantly improve the process of learning from multiclass imbalanced data.},
  archive      = {J_TNNLS},
  author       = {Bartosz Krawczyk and Michał Koziarski and Michał Woźniak},
  doi          = {10.1109/TNNLS.2019.2913673},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2818-2831},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Radial-based oversampling for multiclass imbalanced data classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iterative privileged learning. <em>TNNLS</em>,
<em>31</em>(8), 2805–2817. (<a
href="https://doi.org/10.1109/TNNLS.2018.2889906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Xue Li and Bo Du and Yipeng Zhang and Chang Xu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2018.2889906},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2805-2817},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative privileged learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast matrix factorization with nonuniform weights on missing
data. <em>TNNLS</em>, <em>31</em>(8), 2791–2804. (<a
href="https://doi.org/10.1109/TNNLS.2018.2890117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization (MF) has been widely used to discover the low-rank structure and to predict the missing entries of data matrix. In many real-world learning systems, the data matrix can be very high dimensional but sparse. This poses an imbalanced learning problem since the scale of missing entries is usually much larger than that of the observed entries, but they cannot be ignored due to the valuable negative signal. For efficiency concern, existing work typically applies a uniform weight on missing entries to allow a fast learning algorithm. However, this simplification will decrease modeling fidelity, resulting in suboptimal performance for downstream applications. In this paper, we weight the missing data nonuniformly, and more generically, we allow any weighting strategy on the missing data. To address the efficiency challenge, we propose a fast learning method, for which the time complexity is determined by the number of observed entries in the data matrix rather than the matrix size. The key idea is twofold: 1) we apply truncated singular value decomposition on the weight matrix to get a more compact representation of the weights and 2) we learn MF parameters with elementwise alternating least squares (eALS) and memorize the key intermediate variables to avoid repeating computations that are unnecessary. We conduct extensive experiments on two recommendation benchmarks, demonstrating the correctness, efficiency, and effectiveness of our fast eALS method.},
  archive      = {J_TNNLS},
  author       = {Xiangnan He and Jinhui Tang and Xiaoyu Du and Richang Hong and Tongwei Ren and Tat-Seng Chua},
  doi          = {10.1109/TNNLS.2018.2890117},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2791-2804},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast matrix factorization with nonuniform weights on missing data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative fast hierarchical learning for multiclass
image classification. <em>TNNLS</em>, <em>31</em>(8), 2779–2790. (<a
href="https://doi.org/10.1109/TNNLS.2019.2948881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a discriminative fast hierarchical learning algorithm is developed for supporting multiclass image classification, where a visual tree is seamlessly integrated with multitask learning to achieve fast training of the tree classifier hierarchically (i.e., a set of structural node classifiers over the visual tree). By partitioning a large number of categories hierarchically in a coarse-to-fine fashion, a visual tree is first constructed and further used to handle data imbalance and identify the interrelated learning tasks automatically (e.g., the tasks for learning the node classifiers for the sibling child nodes under the same parent node are strongly interrelated), and a multitask SVM classifier is trained for each nonleaf node to achieve more effective separation of its sibling child nodes at the next level of the visual tree. Both the internode visual similarities and the interlevel visual correlations are utilized to train more discriminative multitask SVM classifiers and control the interlevel error propagation effectively, and a stochastic gradient descent (SGD) algorithm is developed for learning such multitask SVM classifiers with higher efficiency. Our experimental results have demonstrated that our fast hierarchical learning algorithm can achieve very competitive results on both the classification accuracy rates and the computational efficiency.},
  archive      = {J_TNNLS},
  author       = {Yu Zheng and Jianping Fan and Ji Zhang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2019.2948881},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2779-2790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative fast hierarchical learning for multiclass image classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Adaptive chunk-based dynamic weighted majority for
imbalanced data streams with concept drift. <em>TNNLS</em>,
<em>31</em>(8), 2764–2778. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most challenging problems in the field of online learning is concept drift, which deeply influences the classification stability of streaming data. If the data stream is imbalanced, it is even more difficult to detect concept drifts and make an online learner adapt to them. Ensemble algorithms have been found effective for the classification of streaming data with concept drift, whereby an individual classifier is built for each incoming data chunk and its associated weight is adjusted to manage the drift. However, it is difficult to adjust the weights to achieve a balance between the stability and adaptability of the ensemble classifiers. In addition, when the data stream is imbalanced, the use of a size-fixed chunk to build a single classifier can create further problems; the data chunk may contain too few or even no minority class samples (i.e., only majority class samples). A classifier built on such a chunk is unstable in the ensemble. In this article, we propose a chunk-based incremental learning method called adaptive chunk-based dynamic weighted majority (ACDWM) to deal with imbalanced streaming data containing concept drift. ACDWM utilizes an ensemble framework by dynamically weighting the individual classifiers according to their classification performance on the current data chunk. The chunk size is adaptively selected by statistical hypothesis tests to access whether the classifier built on the current data chunk is sufficiently stable. ACDWM has four advantages compared with the existing methods as follows: 1) it can maintain stability when processing nondrifted streams and rapidly adapt to the new concept; 2) it is entirely incremental, i.e., no previous data need to be stored; 3) it stores a limited number of classifiers to ensure high efficiency; and 4) it adaptively selects the chunk size in the concept drift environment. Experiments on both synthetic and real data sets containing concept drift show that ACDWM outperforms both state-of-the-art chunk-based and online methods.},
  archive      = {J_TNNLS},
  author       = {Yang Lu and Yiu-Ming Cheung and Yuan Yan Tang},
  doi          = {10.1109/TNNLS.2019.2951814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2764-2778},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive chunk-based dynamic weighted majority for imbalanced data streams with concept drift},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep least squares fisher discriminant analysis.
<em>TNNLS</em>, <em>31</em>(8), 2752–2763. (<a
href="https://doi.org/10.1109/TNNLS.2019.2906302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While being one of the first and most elegant tools for dimensionality reduction, Fisher linear discriminant analysis (FLDA) is not currently considered among the top methods for feature extraction or classification. In this paper, we will review two recent approaches to FLDA, namely, least squares Fisher discriminant analysis (LSFDA) and regularized kernel FDA (RKFDA) and propose deep FDA (DFDA), a straightforward nonlinear extension of LSFDA that takes advantage of the recent advances on deep neural networks. We will compare the performance of RKFDA and DFDA on a large number of two-class and multiclass problems, many of them involving class-imbalanced data sets and some having quite large sample sizes; we will use, for this, the areas under the receiver operating characteristics (ROCs) curve of the classifiers considered. As we shall see, the classification performance of both methods is often very similar and particularly good on imbalanced problems, but building DFDA models is considerably much faster than doing so for RKFDA, particularly in problems with quite large sample sizes.},
  archive      = {J_TNNLS},
  author       = {David Díaz-Vico and José R. Dorronsoro},
  doi          = {10.1109/TNNLS.2019.2906302},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2752-2763},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep least squares fisher discriminant analysis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Siamese neural networks for user identity linkage through
web browsing. <em>TNNLS</em>, <em>31</em>(8), 2741–2751. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linking online identities of users among countless heterogeneous network services on the Internet can provide an explicit digital representation of users, which can benefit both research and industry. In recent years, user identity linkage (UIL) through the Internet has become an emerging task with great potential and many challenges. Existing works mainly focus on online social networks that consider inconsistent profiles, content, and networks as features or use sparse location-based data sets to link the online behaviors of a real person. To extend the UIL problem to a general scenario, we try to link the web-browsing behaviors of users, which can help to distinguish specific users from others, such as children or malicious users. More specifically, we propose a Siamese neural network (NN) architecture-based UIL (SAUIL) model that learns and compares the highest-level feature representation of input web-browsing behaviors with deep NNs. Although the number of matching and nonmatching pairs for the UIL problem is highly imbalanced, previous studies have not considered imbalanced UIL data sets. Therefore, we further address the imbalanced learning issue by proposing cost-sensitive SAUIL (C-SAUIL) model, which assumes higher costs for misclassifying the minority class. In the experiments, the proposed model is robust and exhibits a good performance on very large, real-world data sets collected from different regions with distinct characteristics.},
  archive      = {J_TNNLS},
  author       = {Yuanyuan Qiao and Yuewei Wu and Fan Duo and Wenhui Lin and Jie Yang},
  doi          = {10.1109/TNNLS.2019.2929575},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2741-2751},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Siamese neural networks for user identity linkage through web browsing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RecSys-DAN: Discriminative adversarial networks for
cross-domain recommender systems. <em>TNNLS</em>, <em>31</em>(8),
2731–2740. (<a
href="https://doi.org/10.1109/TNNLS.2019.2907430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sparsity and data imbalance are practical and challenging issues in cross-domain recommender systems (RSs). This paper addresses those problems by leveraging the concepts which derive from representation learning, adversarial learning, and transfer learning (particularly, domain adaptation). Although various transfer learning methods have shown promising performance in this context, our proposed novel method RecSys-DAN focuses on alleviating the cross-domain and within-domain data sparsity and data imbalance and learns transferable latent representations for users, items, and their interactions. Different from the existing approaches, the proposed method transfers the latent representations from a source domain to a target domain in an adversarial way. The mapping functions in the target domain are learned by playing a min–max game with an adversarial loss, aiming to generate domain indistinguishable representations for a discriminator. Four neural architectural instances of ResSys-DAN are proposed and explored. Empirical results on real-world Amazon data show that, even without using labeled data (i.e., ratings) in the target domain, RecSys-DAN achieves competitive performance as compared to the state-of-the-art supervised methods. More importantly, RecSys-DAN is highly flexible to both unimodal and multimodal scenarios, and thus it is more robust to the cold-start recommendation which is difficult for the previous methods.},
  archive      = {J_TNNLS},
  author       = {Cheng Wang and Mathias Niepert and Hui Li},
  doi          = {10.1109/TNNLS.2019.2907430},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2731-2740},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RecSys-DAN: Discriminative adversarial networks for cross-domain recommender systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Objective video quality assessment combining transfer
learning with CNN. <em>TNNLS</em>, <em>31</em>(8), 2716–2730. (<a
href="https://doi.org/10.1109/TNNLS.2018.2890310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, video quality assessment (VQA) is essential to video compression technology applied to video transmission and storage. However, small-scale video quality databases with imbalanced samples and low-level feature representations for distorted videos impede the development of VQA methods. In this paper, we propose a full-reference (FR) VQA metric integrating transfer learning with a convolutional neural network (CNN). First, we imitate the feature-based transfer learning framework to transfer the distorted images as the related domain, which enriches the distorted samples. Second, to extract high-level spatiotemporal features of the distorted videos, a six-layer CNN with the acknowledged learning ability is pretrained and finetuned by the common features of the distorted image blocks (IBs) and video blocks (VBs), respectively. Notably, the labels of the distorted IBs and VBs are predicted by the classic FR metrics. Finally, based on saliency maps and the entropy function, we conduct a pooling stage to obtain the quality scores of the distorted videos by weighting the block-level scores predicted by the trained CNN. In particular, we introduce a preprocessing and a postprocessing to reduce the impact of inaccurate labels predicted by the FR-VQA metric. Due to feature learning in the proposed framework, two kinds of experimental schemes including train-test iterative procedures on one database and tests on one database with training other databases are carried out. The experimental results demonstrate that the proposed method has high expansibility and is on a par with some state-of-the-art VQA metrics on two widely used VQA databases with various compression distortions.},
  archive      = {J_TNNLS},
  author       = {Yu Zhang and Xinbo Gao and Lihuo He and Wen Lu and Ran He},
  doi          = {10.1109/TNNLS.2018.2890310},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2716-2730},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Objective video quality assessment combining transfer learning with CNN},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning multi-level density maps for crowd counting.
<em>TNNLS</em>, <em>31</em>(8), 2705–2715. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People in crowd scenes often exhibit the characteristic of imbalanced distribution. On the one hand, people size varies largely due to the camera perspective. People far away from the camera look smaller and are likely to occlude each other, whereas people near to the camera look larger and are relatively sparse. On the other hand, the number of people also varies greatly in the same or different scenes. This article aims to develop a novel model that can accurately estimate the crowd count from a given scene with imbalanced people distribution. To this end, we have proposed an effective multi-level convolutional neural network (MLCNN) architecture that first adaptively learns multi-level density maps and then fuses them to predict the final output. Density map of each level focuses on dealing with people of certain sizes. As a result, the fusion of multi-level density maps is able to tackle the large variation in people size. In addition, we introduce a new loss function named balanced loss (BL) to impose relatively BL feedback during training, which helps further improve the performance of the proposed network. Furthermore, we introduce a new data set including 1111 images with a total of 49 061 head annotations. MLCNN is easy to train with only one end-to-end training stage. Experimental results demonstrate that our MLCNN achieves state-of-the-art performance. In particular, our MLCNN reaches a mean absolute error (MAE) of 242.4 on the UCF_CC_50 data set, which is 37.2 lower than the second-best result.},
  archive      = {J_TNNLS},
  author       = {Xiaoheng Jiang and Li Zhang and Pei Lv and Yibo Guo and Ruijie Zhu and Yafei Li and Yanwei Pang and Xi Li and Bing Zhou and Mingliang Xu},
  doi          = {10.1109/TNNLS.2019.2933920},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2705-2715},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning multi-level density maps for crowd counting},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning deep landmarks for imbalanced classification.
<em>TNNLS</em>, <em>31</em>(8), 2691–2704. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a deep imbalanced learning framework called learning DEep Landmarks in laTent spAce (DELTA). Our work is inspired by the shallow imbalanced learning approaches to rebalance imbalanced samples before feeding them to train a discriminative classifier. Our DELTA advances existing works by introducing the new concept of rebalancing samples in a deeply transformed latent space, where latent points exhibit several desired properties including compactness and separability. In general, DELTA simultaneously conducts feature learning, sample rebalancing, and discriminative learning in a joint, end-to-end framework. The framework is readily integrated with other sophisticated learning concepts including latent points oversampling and ensemble learning. More importantly, DELTA offers the possibility to conduct imbalanced learning with the assistancy of structured feature extractor. We verify the effectiveness of DELTA not only on several benchmark data sets but also on more challenging real-world tasks including click-through-rate (CTR) prediction, multi-class cell type classification, and sentiment analysis with sequential inputs.},
  archive      = {J_TNNLS},
  author       = {Feng Bao and Yue Deng and Youyong Kong and Zhiquan Ren and Jinli Suo and Qionghai Dai},
  doi          = {10.1109/TNNLS.2019.2927647},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2691-2704},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning deep landmarks for imbalanced classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial special issue on recent advances in theory,
methodology, and applications of imbalanced learning. <em>TNNLS</em>,
<em>31</em>(8), 2688–2690. (<a
href="https://doi.org/10.1109/TNNLS.2020.3003494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced learning is a challenging task in machine learning, faced by practitioners, and intensively investigated by researchers from a wide range of communities. However, as pointed out in the book titled “ Imbalanced Learning: Foundations, Algorithms, and Applications ” and collectively authored by experts in the field, many if not most of the approaches to imbalanced learning are heuristic and ad hoc in nature, hence leaving many questions unanswered. To fill this gap, the aim of this Special Issue is to collect recent research works that focus on the theory, methodology, and applications of imbalanced learning. After carefully reviewing a large number of submissions, we selected 15 works to be included in this Special Issue. These works can be roughly categorized into three types: deep-learning-based methods (6), methods based on other machine-learning paradigms (7), and empirical comparative studies (2).},
  archive      = {J_TNNLS},
  author       = {Jing-Hao Xue and Zhanyu Ma and Manuel Roveri and Nathalie Japkowicz},
  doi          = {10.1109/TNNLS.2020.3003494},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {2688-2690},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on recent advances in theory, methodology, and applications of imbalanced learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(7), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.3003490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.3003490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing for measure of performance in max-margin parsing.
<em>TNNLS</em>, <em>31</em>(7), 2680–2684. (<a
href="https://doi.org/10.1109/TNNLS.2019.2934225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many learning tasks in the field of natural language processing including sequence tagging, sequence segmentation, and syntactic parsing have been successfully approached by means of structured prediction methods. An appealing property of the corresponding training algorithms is their ability to integrate the loss function of interest into the optimization process improving the final results according to the chosen measure of performance. Here, we focus on the task of constituency parsing and show how to optimize the model for the F 1 -score in the max-margin framework of a structural support vector machine (SVM). For reasons of computational efficiency, it is a common approach to binarize the corresponding grammar before training. Unfortunately, this introduces a bias during the training procedure as the corresponding loss function is evaluated on the binary representation, while the resulting performance is measured on the original unbinarized trees. Here, we address this problem by extending the inference procedure presented by Bauer et al. Specifically, we propose an algorithmic modification that allows evaluating the loss on the unbinarized trees. The new approach properly models the loss function of interest resulting in better prediction accuracy and still benefits from the computational efficiency due to binarized representation. The presented idea can be easily transferred to other structured loss functions.},
  archive      = {J_TNNLS},
  author       = {Alexander Bauer and Shinichi Nakajima and Nico Görnitz and Klaus-Robert Müller},
  doi          = {10.1109/TNNLS.2019.2934225},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2680-2684},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimizing for measure of performance in max-margin parsing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring duality in visual question-driven top-down
saliency. <em>TNNLS</em>, <em>31</em>(7), 2672–2679. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Top-down, goal-driven visual saliency exerts a huge influence on the human visual system for performing visual tasks. Text generations, like visual question answering (VQA) and visual question generation (VQG), have intrinsic connections with top-down saliency, which is usually involved in both VQA and VQG processes in an unsupervised manner. However, it is shown that the regions that humans choose to look at to answer questions are very different from the unsupervised attention models. In this brief, we aim to explore the intrinsic relationship between top-down saliency and text generations, and to figure out whether an accurate saliency response benefits text generation. To this end, we propose a dual supervised network with dynamic parameter prediction. Dual-supervision explicitly exploits the probabilistic correlation between the primal task top-down saliency detection and the dual task text generation, while dynamic parameter prediction encodes the given text (i.e., question or answer) into the fully convolutional network. Extensive experiments show the proposed top-down saliency method achieves the best correlation with human attention among various baselines. In addition, the proposed model can be guided by either questions or answers, and output the counterpart. Furthermore, we show that combining human-like visual question-saliency improves the performance of both answer and question generations.},
  archive      = {J_TNNLS},
  author       = {Shengfeng He and Chu Han and Guoqiang Han and Jing Qin},
  doi          = {10.1109/TNNLS.2019.2933439},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2672-2679},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploring duality in visual question-driven top-down saliency},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep model compression and inference speedup of sum–product
networks on tensor trains. <em>TNNLS</em>, <em>31</em>(7), 2665–2671.
(<a href="https://doi.org/10.1109/TNNLS.2019.2928379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sum-product networks (SPNs) constitute an emerging class of neural networks with clear probabilistic semantics and superior inference speed over other graphical models. This brief reveals an important connection between SPNs and tensor trains (TTs), leading to a new canonical form which we call tensor SPNs (tSPNs). Specifically, we demonstrate the intimate relationship between a valid SPN and a TT. For the first time, through mapping an SPN onto a tSPN and employing specially customized optimization techniques, we demonstrate improvements up to a factor of 100 on both model compression and inference speedup for various data sets with negligible loss in accuracy.},
  archive      = {J_TNNLS},
  author       = {Ching-Yun Ko and Cong Chen and Zhuolun He and Yuke Zhang and Kim Batselier and Ngai Wong},
  doi          = {10.1109/TNNLS.2019.2928379},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2665-2671},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep model compression and inference speedup of Sum–Product networks on tensor trains},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mask dynamic routing to combined model of deep capsule
network and u-net. <em>TNNLS</em>, <em>31</em>(7), 2653–2664. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capsule network is a novel architecture to encode feature attributes and spatial relationships of an image. By using the dynamic routing (DR) algorithm, a capsule network (CapsNet) model can be trained. However, the original three-layer CapsNet with the DR algorithm performs poorly on complex data sets, such as FashionMNIST, CIFAR-10, and CIFAR-100. This deficiency limits the wider application of capsule networks. In this article, we propose a deep capsule network model combined with a U-Net preprocessing module (DCN-UN). Local connection and weight-sharing strategies are adopted from convolutional neural networks to design a convolutional capsule layer in the DCN-UN model. This allows considerably reducing the number of parameters in the network model. Moreover, a greedy strategy is incorporated into the design of a mask DR (MDR) algorithm to improve the performance of network models. DCN-UN requires up to five times fewer parameters compared with the original CapsNet and other CapsNet-based models. The performance improvement achieved by the DCN-UN model with the MDR algorithm over the original CapsNet model with the DR algorithm is approximately 12\% and 17\% on the CIFAR-10 and CIFAR-100 data sets, respectively. The experimental results confirm that the proposed DCN-UN model allows preserving advantages of image reconstruction and equivariance mechanism in capsule networks. Moreover, an efficient initialization method is explored to enhance training stability and avoid gradient explosion.},
  archive      = {J_TNNLS},
  author       = {Junying Chen and Zhan Liu},
  doi          = {10.1109/TNNLS.2020.2984686},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2653-2664},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mask dynamic routing to combined model of deep capsule network and U-net},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Backpropagation with <span
class="math inline"><em>N</em></span> -d vector-valued neurons using
arbitrary bilinear products. <em>TNNLS</em>, <em>31</em>(7), 2638–2652.
(<a href="https://doi.org/10.1109/TNNLS.2019.2933882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector-valued neural learning has emerged as a promising direction in deep learning recently. Traditionally, training data for neural networks (NNs) are formulated as a vector of scalars; however, its performance may not be optimal since associations among adjacent scalars are not modeled. In this article, we propose a new vector neural architecture called the Arbitrary BIlinear Product NN (ABIPNN), which processes information as vectors in each neuron, and the feedforward projections are defined using arbitrary bilinear products. Such bilinear products can include circular convolution, 7-D vector product, skew circular convolution, reversed-time circular convolution, or other new products that are not seen in the previous work. As a proof-of-concept, we apply our proposed network to multispectral image denoising and singing voice separation. Experimental results show that ABIPNN obtains substantial improvements when compared to conventional NNs, suggesting that associations are learned during training.},
  archive      = {J_TNNLS},
  author       = {Zhe-Cheng Fan and Tak-Shing T. Chan and Yi-Hsuan Yang and Jyh-Shing Roger Jang},
  doi          = {10.1109/TNNLS.2019.2933882},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2638-2652},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Backpropagation with $N$ -D vector-valued neurons using arbitrary bilinear products},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural probabilistic graphical model for face sketch
synthesis. <em>TNNLS</em>, <em>31</em>(7), 2623–2637. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network learning for face sketch synthesis from photos has attracted substantial attention due to its favorable synthesis performance. However, most existing deep-learning-based face sketch synthesis models stacked only by multiple convolutional layers without structured regression often lose the common facial structures, limiting their flexibility in a wide range of practical applications, including intelligent security and digital entertainment. In this article, we introduce a neural network to a probabilistic graphical model and propose a novel face sketch synthesis framework based on the neural probabilistic graphical model (NPGM) composed of a specific structure and a common structure. In the specific structure, we investigate a neural network for mapping the direct relationship between training photos and sketches, yielding the specific information and characteristic features of a test photo. In the common structure, the fidelity between the sketch pixels generated by the specific structure and their candidates selected from the training data are considered, ensuring the preservation of the common facial structure. Experimental results on the Chinese University of Hong Kong face sketch database demonstrate, both qualitatively and quantitatively, that the proposed NPGM-based face sketch synthesis approach can more effectively capture specific features and recover common structures compared with the state-of-the-art methods. Extensive experiments in practical applications further illustrate that the proposed method achieves superior performance.},
  archive      = {J_TNNLS},
  author       = {Mingjin Zhang and Nannan Wang and Yunsong Li and Xinbo Gao},
  doi          = {10.1109/TNNLS.2019.2933590},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2623-2637},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural probabilistic graphical model for face sketch synthesis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep one-class neural network for anomalous event
detection in complex scenes. <em>TNNLS</em>, <em>31</em>(7), 2609–2622.
(<a href="https://doi.org/10.1109/TNNLS.2019.2933554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to build a generic deep one-class (DeepOC) model to solve one-class classification problems for anomaly detection, such as anomalous event detection in complex scenes? The characteristics of existing one-class labels lead to a dilemma: it is hard to directly use a multiple classifier based on deep neural networks to solve one-class classification problems. Therefore, in this article, we propose a novel DeepOC neural network, termed as DeepOC, which can simultaneously learn compact feature representations and train a DeepOC classifier. Only with the given normal samples, we use the stacked convolutional encoder to generate their low-dimensional high-level features and train a one-class classifier to make these features as compact as possible. Meanwhile, for the sake of the correct mapping relation and the feature representations&#39; diversity, we utilize a decoder in order to reconstruct raw samples from these low-dimensional feature representations. This structure is gradually established using an adversarial mechanism during the training stage. This mechanism is the key to our model. It organically combines two seemingly contradictory components and allows them to take advantage of each other, thus making the model robust and effective. Unlike methods that use handcrafted features or those that are separated into two stages (extracting features and training classifiers), DeepOC is a one-stage model using reliable features that are automatically extracted by neural networks. Experiments on various benchmark data sets show that DeepOC is feasible and achieves the state-of-the-art anomaly detection results compared with a dozen existing methods.},
  archive      = {J_TNNLS},
  author       = {Peng Wu and Jing Liu and Fang Shen},
  doi          = {10.1109/TNNLS.2019.2933554},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2609-2622},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deep one-class neural network for anomalous event detection in complex scenes},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large margin partial label machine. <em>TNNLS</em>,
<em>31</em>(7), 2594–2608. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning (PLL) is a multi-class weakly supervised learning problem where each training instance is associated with a set of candidate labels but only one label is the ground truth. The main challenge of PLL is how to deal with the label ambiguities. Among various disambiguation techniques, large margin (LM)-based algorithms attract much attention due to their powerful discriminative performance. However, existing LM-based algorithms either neglect some potential candidate labels in constructing the margin or introduce auxiliary estimation of class capacities which is generally inaccurate. As a result, their generalization performances are deteriorated. To address the above-mentioned drawbacks, motivated by the optimistic superset loss, we propose an LM Partial LAbel machiNE (LM-PLANE) by extending multi-class support vector machines (SVM) to PLL. Compared with existing LM-based disambiguation algorithms, LM-PLANE considers the margin of all potential candidate labels without auxiliary estimation of class capacities. Furthermore, an efficient cutting plane (CP) method is developed to train LM-PLANE in the dual space. Theoretical insights into the effectiveness and convergence of our CP method are also presented. Extensive experiments on various PLL tasks demonstrate the superiority of LM-PLANE over existing LM based and other representative PLL algorithms in terms of classification accuracy.},
  archive      = {J_TNNLS},
  author       = {Jing Chai and Ivor W. Tsang and Weijie Chen},
  doi          = {10.1109/TNNLS.2019.2933530},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2594-2608},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Large margin partial label machine},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial examples: Opportunities and challenges.
<em>TNNLS</em>, <em>31</em>(7), 2578–2593. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have shown huge superiority over humans in image recognition, speech processing, autonomous vehicles, and medical diagnosis. However, recent studies indicate that DNNs are vulnerable to adversarial examples (AEs), which are designed by attackers to fool deep learning models. Different from real examples, AEs can mislead the model to predict incorrect outputs while hardly be distinguished by human eyes, therefore threaten security-critical deep-learning applications. In recent years, the generation and defense of AEs have become a research hotspot in the field of artificial intelligence (AI) security. This article reviews the latest research progress of AEs. First, we introduce the concept, cause, characteristics, and evaluation metrics of AEs, then give a survey on the state-of-the-art AE generation methods with the discussion of advantages and disadvantages. After that, we review the existing defenses and discuss their limitations. Finally, future research opportunities and challenges on AEs are prospected.},
  archive      = {J_TNNLS},
  author       = {Jiliang Zhang and Chen Li},
  doi          = {10.1109/TNNLS.2019.2933524},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2578-2593},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial examples: Opportunities and challenges},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Debiasing and distributed estimation for high-dimensional
quantile regression. <em>TNNLS</em>, <em>31</em>(7), 2569–2577. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed and parallel computing is becoming more important with the availability of extremely large data sets. In this article, we consider this problem for high-dimensional linear quantile regression. We work under the assumption that the coefficients in the regression model are sparse; therefore, a LASSO penalty is naturally used for estimation. We first extend the debiasing procedure, which is previously proposed for smooth parametric regression models to quantile regression. The technical challenges include dealing with the nondifferentiability of the loss function and the estimation of the unknown conditional density. In this article, the main objective is to derive a divide-and-conquer estimation approach using the debiased estimator which is useful under the big data setting. The effectiveness of distributed estimation is demonstrated using some numerical examples.},
  archive      = {J_TNNLS},
  author       = {Weihua Zhao and Fode Zhang and Heng Lian},
  doi          = {10.1109/TNNLS.2019.2933467},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2569-2577},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Debiasing and distributed estimation for high-dimensional quantile regression},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The strength of nesterov’s extrapolation in the individual
convergence of nonsmooth optimization. <em>TNNLS</em>, <em>31</em>(7),
2557–2568. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extrapolation strategy raised by Nesterov, which can accelerate the convergence rate of gradient descent methods by orders of magnitude when dealing with smooth convex objective, has led to tremendous success in training machine learning tasks. In this article, the convergence of individual iterates of projected subgradient (PSG) methods for nonsmooth convex optimization problems is theoretically studied based on Nesterov&#39;s extrapolation, which we name individual convergence. We prove that Nesterov&#39;s extrapolation has the strength to make the individual convergence of PSG optimal for nonsmooth problems. In light of this consideration, a direct modification of the subgradient evaluation suffices to achieve optimal individual convergence for strongly convex problems, which can be regarded as making an interesting step toward the open question about stochastic gradient descent (SGD) posed by Shamir. Furthermore, we give an extension of the derived algorithms to solve regularized learning tasks with nonsmooth losses in stochastic settings. Compared with other state-of-the-art nonsmooth methods, the derived algorithms can serve as an alternative to the basic SGD especially in coping with machine learning problems, where an individual output is needed to guarantee the regularization structure while keeping an optimal rate of convergence. Typically, our method is applicable as an efficient tool for solving large-scale l 1 -regularized hinge-loss learning problems. Several comparison experiments demonstrate that our individual output not only achieves an optimal convergence rate but also guarantees better sparsity than the averaged solution.},
  archive      = {J_TNNLS},
  author       = {Wei Tao and Zhisong Pan and Gaowei Wu and Qing Tao},
  doi          = {10.1109/TNNLS.2019.2933452},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2557-2568},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The strength of nesterov’s extrapolation in the individual convergence of nonsmooth optimization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Pay attention to them: Deep reinforcement learning-based
cascade object detection. <em>TNNLS</em>, <em>31</em>(7), 2544–2556. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel and effective approach, namely pay attention to them (PAT), to general object detection, which integrates the bottom-up single-shot convolutional neural networks (CNNs) and a top-down operating strategy. PAT starts by routinely applying a CNN regression detector to the entire input image. It then conducts refinement, which locates a sub-region that probably contains relevant objects through an intelligent agent built with an attentional mechanism and zooms it in to launch the detector again. This refining step is repeated in a cascaded way, where all the bounding boxes produced are scaled according to the original resolution and the sub-marginal and overlapping parts are wiped out to generate the final output. Due to such progressive processing, PAT improves the detection accuracy, especially for the objects of small sizes. Extensive experiments are conducted on the Pascal VOC and MS COCO benchmarks, and the results show that PAT is able to improve the representative baseline detectors, i.e., single shot multibox detector, YOLOv2, and Faster regions with CNN features, with remarkable accuracy gains [about 2\%-5\% mean Average Precision (mAP)], which demonstrates its competency.},
  archive      = {J_TNNLS},
  author       = {Songtao Liu and Di Huang and Yunhong Wang},
  doi          = {10.1109/TNNLS.2019.2933451},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2544-2556},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pay attention to them: Deep reinforcement learning-based cascade object detection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural network finite-time control for multi-input
and multi-output nonlinear systems with positive powers of odd rational
numbers. <em>TNNLS</em>, <em>31</em>(7), 2532–2543. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive neural network (NN) finite-time output tracking control problem for a class of multi-input and multi-output (MIMO) uncertain nonlinear systems whose powers are positive odd rational numbers. Such designs adopt NNs to approximate unknown continuous system functions, and a controller is constructed by combining backstepping design and adding a power integrator technique. By constructing new iterative Lyapunov functions and using finite-time stability theory, the closed-loop stability has been achieved, which further verifies that the entire system possesses semiglobal practical finite-time stability (SGPFS), and the tracking errors converge to a small neighborhood of the origin within finite time. Finally, a simulation example is given to elaborate the effectiveness and superiority of the developed.},
  archive      = {J_TNNLS},
  author       = {Yongming Li and Kewen Li and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2019.2933409},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2532-2543},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network finite-time control for multi-input and multi-output nonlinear systems with positive powers of odd rational numbers},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Benchmarking neural networks for quantum computations.
<em>TNNLS</em>, <em>31</em>(7), 2522–2531. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power of quantum computers is still somewhat speculative. Although they are certainly faster than classical ones at some tasks, the class of problems they can efficiently solve has not been mapped definitively onto known classical complexity theory. This means that we do not know for which calculations there will be a “quantum advantage,” once an algorithm is found. One way to answer the question is to find those algorithms, but finding truly quantum algorithms turns out to be very difficult. In previous work, over the past three decades, we have pursued the idea of using techniques of machine learning to develop algorithms for quantum computing. Here, we compare the performance of standard real- and complex-valued classical neural networks with that of one of our models for a quantum neural network, on both classical problems and on an archetypal quantum problem: the computation of an entanglement witness. The quantum network is shown to need far fewer epochs and a much smaller network to achieve comparable or better results.},
  archive      = {J_TNNLS},
  author       = {Nam H. Nguyen and E. C. Behrman and Mohamed A. Moustafa and J. E. Steck},
  doi          = {10.1109/TNNLS.2019.2933394},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2522-2531},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Benchmarking neural networks for quantum computations},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised non-negative matrix factorization with
dissimilarity and similarity regularization. <em>TNNLS</em>,
<em>31</em>(7), 2510–2521. (<a
href="https://doi.org/10.1109/TNNLS.2019.2933223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a semi-supervised non-negative matrix factorization (NMF) model by means of elegantly modeling the label information. The proposed model is capable of generating discriminable low-dimensional representations to improve clustering performance. Specifically, a pair of complementary regularizers, i.e., similarity and dissimilarity regularizers, is incorporated into the conventional NMF to guide the factorization. And, they impose restrictions on both the similarity and dissimilarity of the low-dimensional representations of data samples with labels as well as a small number of unlabeled ones. The proposed model is formulated as a well-posed constrained optimization problem and further solved with an efficient alternating iterative algorithm. Moreover, we theoretically prove that the proposed algorithm can converge to a limiting point that meets the Karush-Kuhn-Tucker conditions. Extensive experiments as well as comprehensive analysis demonstrate that the proposed model outperforms the state-of-the-art NMF methods to a large extent over five benchmark data sets, i.e., the clustering accuracy increases to 82.2\% from 57.0\%.},
  archive      = {J_TNNLS},
  author       = {Yuheng Jia and Sam Kwong and Junhui Hou and Wenhui Wu},
  doi          = {10.1109/TNNLS.2019.2933223},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2510-2521},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semi-supervised non-negative matrix factorization with dissimilarity and similarity regularization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning semisupervised multilabel fully convolutional
network for hierarchical object parsing. <em>TNNLS</em>, <em>31</em>(7),
2500–2509. (<a
href="https://doi.org/10.1109/TNNLS.2019.2931183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a semisupervised multilabel fully convolutional network (FCN) for hierarchical object parsing of images. We consider each object part (e.g., eye and head) as a class label and learn to assign every image pixel to multiple coherent part labels. Different from previous methods that consider part labels as independent classes, our method explicitly models the internal relationships between object parts, e.g., that a pixel highly scored for eyes should be highly scored for heads as well. Such relationships directly reflect the structure of the semantic space and thus should be respected while learning the deep representation. We achieve this objective by introducing a multilabel softmax loss function over both labeled and unlabeled images and regularizing it with two pairwise ranking constraints. The first constraint is based on a manifold assumption that image pixels being visually and spatially close to each other should be collaboratively classified as the same part label. The other constraint is used to enforce that no pixel receives significant scores from more than one label that are semantically conflicting with each other. The proposed loss function is differentiable with respect to network parameters and hence can be optimized by standard stochastic gradient methods. We evaluate the proposed method on two public image data sets for hierarchical object parsing and compare it with the alternative parsing methods. Extensive comparisons showed that our method can achieve state-of-the-art performance while using 50\% less labeled training samples than the alternatives.},
  archive      = {J_TNNLS},
  author       = {Xiaobai Liu and Qian Xu and Grayson Adkins and Eric Medwedeff and Liang Lin and Shuicheng Yan},
  doi          = {10.1109/TNNLS.2019.2931183},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2500-2509},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning semisupervised multilabel fully convolutional network for hierarchical object parsing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A generic improvement to deep residual networks based on
gradient flow. <em>TNNLS</em>, <em>31</em>(7), 2490–2499. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preactivation ResNets consistently outperforms the original postactivation ResNets on the CIFAR10/100 classification benchmark. However, these results surprisingly do not carry over to the standard ImageNet benchmark. First, we theoretically analyze this incongruity in terms of how the two variants differ in handling the propagation of gradients. Although identity shortcuts are critical in both variants for improving optimization and performance, we show that postactivation variants enable early layers to receive a diverse dynamic composition of gradients from effectively deeper paths in comparison to preactivation variants, enabling the network to make maximal use of its representational capacity. Second, we show that downsampling projections (while only a few in number) have a significantly detrimental effect on performance. We show that by simply replacing downsampling projections with identitylike dense-reshape shortcuts, the classification results of standard residual architectures such as ResNets, ResNeXts, and SE-Nets improve by up to 1.2\% on ImageNet, without any increase in computational complexity (FLOPs).},
  archive      = {J_TNNLS},
  author       = {Venkataraman Santhanam and Larry S. Davis},
  doi          = {10.1109/TNNLS.2019.2929198},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2490-2499},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A generic improvement to deep residual networks based on gradient flow},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning for sequence-to-sequence models.
<em>TNNLS</em>, <em>31</em>(7), 2469–2489. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, sequence-to-sequence (seq2seq) models have gained a lot of popularity and provide state-of-the-art performance in a wide variety of tasks, such as machine translation, headline generation, text summarization, speech-to-text conversion, and image caption generation. The underlying framework for all these models is usually a deep neural network comprising an encoder and a decoder. Although simple encoder-decoder models produce competitive results, many researchers have proposed additional improvements over these seq2seq models, e.g., using an attention-based model over the input, pointer-generation models, and self-attention models. However, such seq2seq models suffer from two common problems: 1) exposure bias and 2) inconsistency between train/test measurement. Recently, a completely novel point of view has emerged in addressing these two problems in seq2seq models, leveraging methods from reinforcement learning (RL). In this survey, we consider seq2seq problems from the RL point of view and provide a formulation combining the power of RL methods in decision-making with seq2seq models that enable remembering long-term memories. We present some of the most recent frameworks that combine the concepts from RL and deep neural networks. Our work aims to provide insights into some of the problems that inherently arise with current approaches and how we can address them with better RL models. We also provide the source code for implementing most of the RL models discussed in this paper to support the complex task of abstractive text summarization and provide some targeted experiments for these RL models, both in terms of performance and training time.},
  archive      = {J_TNNLS},
  author       = {Yaser Keneshloo and Tian Shi and Naren Ramakrishnan and Chandan K. Reddy},
  doi          = {10.1109/TNNLS.2019.2929141},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2469-2489},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning for sequence-to-sequence models},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust student network learning. <em>TNNLS</em>,
<em>31</em>(7), 2455–2468. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks bring in impressive accuracy in various applications, but the success often relies on heavy network architectures. Taking well-trained heavy networks as teachers, classical teacher-student learning paradigm aims to learn a student network that is lightweight yet accurate. In this way, a portable student network with significantly fewer parameters can achieve considerable accuracy, which is comparable to that of a teacher network. However, beyond accuracy, the robustness of the learned student network against perturbation is also essential for practical uses. Existing teacher-student learning frameworks mainly focus on accuracy and compression ratios, but ignore the robustness. In this paper, we make the student network produce more confident predictions with the help of the teacher network, and analyze the lower bound of the perturbation that will destroy the confidence of the student network. Two important objectives regarding prediction scores and gradients of examples are developed to maximize this lower bound, to enhance the robustness of the student network without sacrificing the performance. Experiments on benchmark data sets demonstrate the efficiency of the proposed approach to learning robust student networks that have satisfying accuracy and compact sizes.},
  archive      = {J_TNNLS},
  author       = {Tianyu Guo and Chang Xu and Shiyi He and Boxin Shi and Chao Xu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2019.2929114},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2455-2468},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust student network learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective data-aware covariance estimator from compressed
data. <em>TNNLS</em>, <em>31</em>(7), 2441–2454. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating covariance matrix from massive high-dimensional and distributed data is significant for various real-world applications. In this paper, we propose a data-aware weighted sampling-based covariance matrix estimator, namely DACE, which can provide an unbiased covariance matrix estimation and attain more accurate estimation under the same compression ratio. Moreover, we extend our proposed DACE to tackle multiclass classification problems with theoretical justification and conduct extensive experiments on both synthetic and real-world data sets to demonstrate the superior performance of our DACE.},
  archive      = {J_TNNLS},
  author       = {Xixian Chen and Haiqin Yang and Shenglin Zhao and Michael R. Lyu and Irwin King},
  doi          = {10.1109/TNNLS.2019.2929106},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2441-2454},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effective data-aware covariance estimator from compressed data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Label-less learning for emotion cognition. <em>TNNLS</em>,
<em>31</em>(7), 2430–2440. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a label-less learning for emotion cognition (LLEC) to achieve the utilization of a large amount of unlabeled data. We first inspect the unlabeled data from two perspectives, i.e., the feature layer and the decision layer. By utilizing the similarity model and the entropy model, this paper presents a hybrid label-less learning that can automatically label data without human intervention. Then, we design an enhanced hybrid label-less learning to purify the automatic labeled data. To further improve the accuracy of emotion detection model and increase the utilization of unlabeled data, we apply enhanced hybrid label-less learning for multimodal unlabeled emotion data. Finally, we build a real-world test bed to evaluate the LLEC algorithm. The experimental results show that the LLEC algorithm can improve the accuracy of emotion detection significantly.},
  archive      = {J_TNNLS},
  author       = {Min Chen and Yixue Hao},
  doi          = {10.1109/TNNLS.2019.2929071},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2430-2440},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Label-less learning for emotion cognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survey on multi-output learning. <em>TNNLS</em>,
<em>31</em>(7), 2409–2429. (<a
href="https://doi.org/10.1109/TNNLS.2019.2945133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of multi-output learning is to simultaneously predict multiple outputs given an input. It is an important learning problem for decision-making since making decisions in the real world often involves multiple complex factors and criteria. In recent times, an increasing number of research studies have focused on ways to predict multiple outputs at once. Such efforts have transpired in different forms according to the particular multi-output learning problem under study. Classic cases of multi-output learning include multi-label learning, multi-dimensional learning, multi-target regression, and others. From our survey of the topic, we were struck by a lack in studies that generalize the different forms of multi-output learning into a common framework. This article fills that gap with a comprehensive review and analysis of the multi-output learning paradigm. In particular, we characterize the four Vs of multi-output learning, i.e., volume, velocity, variety, and veracity, and the ways in which the four Vs both benefit and bring challenges to multi-output learning by taking inspiration from big data. We analyze the life cycle of output labeling, present the main mathematical definitions of multi-output learning, and examine the field&#39;s key challenges and corresponding solutions as found in the literature. Several model evaluation metrics and popular data repositories are also discussed. Last but not least, we highlight some emerging challenges with multi-output learning from the perspective of the four Vs as potential research directions worthy of further studies.},
  archive      = {J_TNNLS},
  author       = {Donna Xu and Yaxin Shi and Ivor W. Tsang and Yew-Soon Ong and Chen Gong and Xiaobo Shen},
  doi          = {10.1109/TNNLS.2019.2945133},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2409-2429},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Survey on multi-output learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust deep co-saliency detection with group semantic and
pyramid attention. <em>TNNLS</em>, <em>31</em>(7), 2398–2408. (<a
href="https://doi.org/10.1109/TNNLS.2020.2967471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-level semantic knowledge in addition to low-level visual cues is essentially crucial for co-saliency detection. This article proposes a novel end-to-end deep learning approach for robust co-saliency detection by simultaneously learning high-level groupwise semantic representation as well as deep visual features of a given image group. The interimage interaction at the semantic level and the complementarity between the group semantics and visual features are exploited to boost the inferring capability of co-salient regions. Specifically, the proposed approach consists of a co-category learning branch and a co-saliency detection branch. While the former is proposed to learn a groupwise semantic vector using co-category association of an image group as supervision, the latter is to infer precise co-salient maps based on the ensemble of group-semantic knowledge and deep visual cues. The group-semantic vector is used to augment visual features at multiple scales and acts as a top-down semantic guidance for boosting the bottom-up inference of co-saliency. Moreover, we develop a pyramidal attention (PA) module that endows the network with the capability of concentrating on important image patches and suppressing distractions. The co-category learning and co-saliency detection branches are jointly optimized in a multitask learning manner, further improving the robustness of the approach. We construct a new large-scale co-saliency data set COCO-SEG to facilitate research of the co-saliency detection. Extensive experimental results on COCO-SEG and a widely used benchmark Cosal2015 have demonstrated the superiority of the proposed approach compared with state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zheng-Jun Zha and Chong Wang and Dong Liu and Hongtao Xie and Yongdong Zhang},
  doi          = {10.1109/TNNLS.2020.2967471},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2398-2408},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust deep co-saliency detection with group semantic and pyramid attention},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain adaptation with neural embedding matching.
<em>TNNLS</em>, <em>31</em>(7), 2387–2397. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to exploit the supervision knowledge in a source domain for learning prediction models in a target domain. In this article, we propose a novel representation learning-based domain adaptation method, i.e., neural embedding matching (NEM) method, to transfer information from the source domain to the target domain where labeled data is scarce. The proposed approach induces an intermediate common representation space for both domains with a neural network model while matching the embedding of data from the two domains in this common representation space. The embedding matching is based on the fundamental assumptions that a cross-domain pair of instances will be close to each other in the embedding space if they belong to the same class category, and the local geometry property of the data can be maintained in the embedding space. The assumptions are encoded via objectives of metric learning and graph embedding techniques to regularize and learn the semisupervised neural embedding model. We also provide a generalization bound analysis for the proposed domain adaptation method. Meanwhile, a progressive learning strategy is proposed and it improves the generalization ability of the neural network gradually. Experiments are conducted on a number of benchmark data sets and the results demonstrate that the proposed method outperforms several state-of-the-art domain adaptation methods and the progressive learning strategy is promising.},
  archive      = {J_TNNLS},
  author       = {Zengmao Wang and Bo Du and Yuhong Guo},
  doi          = {10.1109/TNNLS.2019.2935608},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2387-2397},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain adaptation with neural embedding matching},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive kernel value caching for SVM training.
<em>TNNLS</em>, <em>31</em>(7), 2376–2386. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machines (SVMs) can solve structured multioutput learning problems such as multilabel classification, multiclass classification, and vector regression. SVM training is expensive, especially for large and high-dimensional data sets. The bottleneck of the SVM training often lies in the kernel value computation. In many real-world problems, the same kernel values are used in many iterations during the training, which makes the caching of kernel values potentially useful. The majority of the existing studies simply adopt the least recently used (LRU) replacement strategy for caching kernel values. However, as we analyze in this article, the LRU strategy generally achieves high hit ratio near the final stage of the training but does not work well in the whole training process. Therefore, we propose a new caching strategy called EFU (less frequently used), which replaces the EFU kernel values that enhance least frequently used (LFU). Our experimental results show that EFU often has 20\% higher hit ratio than LRU in the training with the Gaussian kernel. To further optimize the strategy, we propose a caching strategy called hybrid caching for the SVM training (HCST), which has a novel mechanism to automatically adapt the better caching strategy in different stages of the training. We have integrated the caching strategy into ThunderSVM, a recent SVM library on many-core processors. Our experiments show that HCST adaptively achieves high hit ratios with little runtime overhead among different problems including multilabel classification, multiclass classification, and regression problems. Compared with other existing caching strategies, HCST achieves 20\% more reduction in training time on average.},
  archive      = {J_TNNLS},
  author       = {Qinbin Li and Zeyi Wen and Bingsheng He},
  doi          = {10.1109/TNNLS.2019.2944562},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2376-2386},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive kernel value caching for SVM training},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A probabilistic zero-shot learning method via latent
nonnegative prototype synthesis of unseen classes. <em>TNNLS</em>,
<em>31</em>(7), 2361–2375. (<a
href="https://doi.org/10.1109/TNNLS.2019.2955157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL), a type of structured multioutput learning, has attracted much attention due to its requirement of no training data for target classes. Conventional ZSL methods usually project visual features into semantic space and assign labels by finding their nearest prototypes. However, this type of nearest neighbor search (NNS)-based method often suffers from great performance degradation because of the nonuniform variances between different categories. In this article, we propose a probabilistic framework by taking covariance into account to deal with the above-mentioned problem. In this framework, we define a new latent space, which has two characteristics. The first is that the features in this space should gather within the classes and scatter between the classes, which is implemented by triplet learning; the second is that the prototypes of unseen classes are synthesized with nonnegative coefficients, which are generated by nonnegative matrix factorization (NMF) of relations between the seen classes and the unseen classes in attribute space. During training, the learned parameters are the projection model for triplet network and the nonnegative coefficients between the unseen classes and the seen classes. In the testing phase, visual features are projected into latent space and assigned with the labels that have the maximum probability among unseen classes for classic ZSL or within all classes for generalized ZSL. Extensive experiments are conducted on four popular data sets, and the results show that the proposed method can outperform the state-of-the-art methods in most circumstances.},
  archive      = {J_TNNLS},
  author       = {Haofeng Zhang and Huaqi Mao and Yang Long and Wankou Yang and Ling Shao},
  doi          = {10.1109/TNNLS.2019.2955157},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2361-2375},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A probabilistic zero-shot learning method via latent nonnegative prototype synthesis of unseen classes},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting web images for multi-output classification: From
category to subcategories. <em>TNNLS</em>, <em>31</em>(7), 2348–2360.
(<a href="https://doi.org/10.1109/TNNLS.2020.2966644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies present that dividing categories into subcategories contributes to better image classification. Existing image subcategorization works relying on expert knowledge and labeled images are both time-consuming and labor-intensive. In this article, we propose to select and subsequently classify images into categories and subcategories. Specifically, we first obtain a list of candidate subcategory labels from untagged corpora. Then, we purify these subcategory labels through calculating the relevance to the target category. To suppress the search error and noisy subcategory label-induced outlier images, we formulate outlier images removing and the optimal classification models learning as a unified problem to jointly learn multiple classifiers, where the classifier for a category is obtained by combining multiple subcategory classifiers. Compared with the existing subcategorization works, our approach eliminates the dependence on expert knowledge and labeled images. Extensive experiments on image categorization and subcategorization demonstrate the superiority of our proposed approach.},
  archive      = {J_TNNLS},
  author       = {Yazhou Yao and Fumin Shen and Guosen Xie and Li Liu and Fan Zhu and Jian Zhang and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2020.2966644},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2348-2360},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploiting web images for multi-output classification: From category to subcategories},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamically spatiotemporal regularized correlation tracking.
<em>TNNLS</em>, <em>31</em>(7), 2336–2347. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, due to the high performance, spatially regularized strategy has been widely applied to addressing the issue of boundary effects existed in correlation filter (CF)-based visual tracking. Specifically, it introduces a spatially regularized term to penalize the coefficients of the CFs to be learned depending on their spatial locations. However, the regularization weights are often formed as a fixed Gaussian function, and hence may cause the learned model degenerate due to the inflexible constraints on the ever-changing CFs to be learned over time during tracking. To address this issue, in this paper, we develop a dynamically spatiotemporal regularization model to constrain the CFs to be learned with the ever-changing regularization weights learned from two consecutive frames. The proposed method jointly learns the CFs along with the dynamically spatiotemporal constraint term, which can be efficiently solved in the Fourier domain by the alternative direction method. Extensive evaluations on the popular data sets OTB-100 and VOT-2016 demonstrate that the proposed tracker performs favorably against the baseline tracker and several recently proposed state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yuhui Zheng and Huihui Song and Kaihua Zhang and Jiaqing Fan and Xinyan Liu},
  doi          = {10.1109/TNNLS.2019.2929407},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2336-2347},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamically spatiotemporal regularized correlation tracking},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GANE: A generative adversarial network embedding.
<em>TNNLS</em>, <em>31</em>(7), 2325–2335. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding is capable of providing low-dimensional feature representations for various machine learning applications. Current work focuses on: 1) designing the embedding as an unsupervised learning task to explicitly preserve the structural connectivity in the network or 2) generating the embedding as a by-product during the supervised learning of a specific discriminative task in a deep neural network. In this paper, we aim to take advantage of these two lines of research in the view of multi-output learning. That is, we propose a generative adversarial network embedding (GANE) model to adapt the generative adversarial framework to achieve the network embedding learning during the specific machine learning tasks. GANE has a generator to generate link edges, and a discriminator to distinguish the generated link edges from real connections (edges) in the network. Wasserstein-1 distance is adopted to train the generator to gain better stability. GANE is further extended by utilizing the pairwise connectivity of vertices to preserve the structural information in the original network. Experiments with real-world network data sets demonstrate that our models constantly outperform state-of-the-art solutions with significant improvements for the tasks of link prediction, clustering, and network alignment.},
  archive      = {J_TNNLS},
  author       = {Huiting Hong and Xin Li and Mingzhong Wang},
  doi          = {10.1109/TNNLS.2019.2921841},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2325-2335},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GANE: A generative adversarial network embedding},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Does tail label help for large-scale multi-label learning?
<em>TNNLS</em>, <em>31</em>(7), 2315–2324. (<a
href="https://doi.org/10.1109/TNNLS.2019.2935143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multi-label learning (LMLL) annotates relevant labels for unseen data from a huge number of candidate labels. It is perceived that labels exhibit a long tail distribution in which a significant number of labels are tail labels. Most previous studies consider that the performance would benefit from incorporating tail labels. Nonetheless, it is not quantified how tail labels impact the performance. In this article, we disclose that whatever labels are randomly missing or misclassified, the impact of labels on commonly used LMLL evaluation metrics (Propensity Score Precision (PSP)@$k$ and Propensity Score nDCG (PSnDCG)@$k$ ) is directly related to the product of the label weights and the label frequencies. In particular, when labels share equal weights, tail labels impact much less than common labels due to the scarcity of relevant examples. Based on such observation, we propose to develop low-complexity LMLL methods with the goal of facilitating fast prediction time and compact model size by restraining less performance-influential labels. With the consideration that discarding labels may cause the loss of predictive capability, we further propose to preserve dominant model parameters for the less performance-influential labels. Experiments clearly justify that both the prediction time and the model size are significantly reduced without sacrificing much predictive performance.},
  archive      = {J_TNNLS},
  author       = {Tong Wei and Yu-Feng Li},
  doi          = {10.1109/TNNLS.2019.2935143},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2315-2324},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Does tail label help for large-scale multi-label learning?},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RoSeq: Robust sequence labeling. <em>TNNLS</em>,
<em>31</em>(7), 2304–2314. (<a
href="https://doi.org/10.1109/TNNLS.2019.2911236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we mainly investigate two issues for sequence labeling, namely, label imbalance and noisy data that are commonly seen in the scenario of named entity recognition (NER) and are largely ignored in the existing works. To address these two issues, a new method termed robust sequence labeling (RoSeq) is proposed. Specifically, to handle the label imbalance issue, we first incorporate label statistics in a novel conditional random field (CRF) loss. In addition, we design an additional loss to reduce the weights of overwhelming easy tokens for augmenting the CRF loss. To address the noisy training data, we adopt an adversarial training strategy to improve model generalization. In experiments, the proposed RoSeq achieves the state-of-the-art performances on CoNLL and English Twitter NER—88.07\% on CoNLL-2002 Dutch, 87.33\% on CoNLL-2002 Spanish, 52.94\% on WNUT-2016 Twitter, and 43.03\% on WNUT-2017 Twitter without using the additional data.},
  archive      = {J_TNNLS},
  author       = {Joey Tianyi Zhou and Hao Zhang and Di Jin and Xi Peng and Yang Xiao and Zhiguo Cao},
  doi          = {10.1109/TNNLS.2019.2911236},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2304-2314},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RoSeq: Robust sequence labeling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconstruction regularized deep metric learning for
multi-label image classification. <em>TNNLS</em>, <em>31</em>(7),
2294–2303. (<a
href="https://doi.org/10.1109/TNNLS.2019.2924023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel deep metric learning method to tackle the multi-label image classification problem. In order to better learn the correlations among images features, as well as labels, we attempt to explore a latent space, where images and labels are embedded via two unique deep neural networks, respectively. To capture the relationships between image features and labels, we aim to learn a two-way deep distance metric over the embedding space from two different views, i.e., the distance between one image and its labels is not only smaller than those distances between the image and its labels’ nearest neighbors but also smaller than the distances between the labels and other images corresponding to the labels’ nearest neighbors. Moreover, a reconstruction module for recovering correct labels is incorporated into the whole framework as a regularization term, such that the label embedding space is more representative. Our model can be trained in an end-to-end manner. Experimental results on publicly available image data sets corroborate the efficacy of our method compared with the state of the arts.},
  archive      = {J_TNNLS},
  author       = {Changsheng Li and Chong Liu and Lixin Duan and Peng Gao and Kai Zheng},
  doi          = {10.1109/TNNLS.2019.2924023},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2294-2303},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reconstruction regularized deep metric learning for multi-label image classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed selection of continuous features in multilabel
classification using mutual information. <em>TNNLS</em>, <em>31</em>(7),
2280–2293. (<a
href="https://doi.org/10.1109/TNNLS.2019.2944298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel learning is a challenging task demanding scalable methods for large-scale data. Feature selection has shown to improve multilabel accuracy while defying the curse of dimensionality of high-dimensional scattered data. However, the increasing complexity of multilabel feature selection, especially on continuous features, requires new approaches to manage data effectively and efficiently in distributed computing environments. This article proposes a distributed model for mutual information (MI) adaptation on continuous features and multiple labels on Apache Spark. Two approaches are presented based on MI maximization, and minimum redundancy and maximum relevance. The former selects the subset of features that maximize the MI between the features and the labels, whereas the latter additionally minimizes the redundancy between the features. Experiments compare the distributed multilabel feature selection methods on 10 data sets and 12 metrics. Results validated through statistical analysis indicate that our methods outperform reference methods for distributed feature selection for multilabel data, while MIM also reduces the runtime in orders of magnitude.},
  archive      = {J_TNNLS},
  author       = {Jorge González-López and Sebastián Ventura and Alberto Cano},
  doi          = {10.1109/TNNLS.2019.2944298},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2280-2293},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed selection of continuous features in multilabel classification using mutual information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning with interpretable structure from gated RNN.
<em>TNNLS</em>, <em>31</em>(7), 2267–2279. (<a
href="https://doi.org/10.1109/TNNLS.2020.2967051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretability of deep learning models has raised extended attention these years. It will be beneficial if we can learn an interpretable structure from deep learning models. In this article, we focus on recurrent neural networks (RNNs), especially gated RNNs whose inner mechanism is still not clearly understood. We find that finite-state automaton (FSA) that processes sequential data have a more interpretable inner mechanism according to the definition of interpretability and can be learned from RNNs as the interpretable structure. We propose two methods to learn FSA from RNN based on two different clustering methods. With the learned FSA and via experiments on artificial and real data sets, we find that FSA is more trustable than the RNN from which it learned, which gives FSA a chance to substitute RNNs in applications involving humans&#39; lives or dangerous facilities. Besides, we analyze how the number of gates affects the performance of RNN. Our result suggests that gate in RNN is important but the less the better, which could be a guidance to design other RNNs. Finally, we observe that the FSA learned from RNN gives semantic aggregated states, and its transition graph shows us a very interesting vision of how RNNs intrinsically handle text classification tasks.},
  archive      = {J_TNNLS},
  author       = {Bo-Jian Hou and Zhi-Hua Zhou},
  doi          = {10.1109/TNNLS.2020.2967051},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2267-2279},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning with interpretable structure from gated RNN},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Multioutput convolution spectral mixture for gaussian
processes. <em>TNNLS</em>, <em>31</em>(7), 2255–2266. (<a
href="https://doi.org/10.1109/TNNLS.2019.2946082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multioutput Gaussian processes (MOGPs) are an extension of Gaussian processes (GPs) for predicting multiple output variables (also called channels/tasks) simultaneously. In this article, we use the convolution theorem to design a new kernel for MOGPs by modeling cross-channel dependencies through cross convolution of time-and phase-delayed components in the spectral domain. The resulting kernel is called multioutput convolution spectral mixture (MOCSM) kernel. The results of extensive experiments on synthetic and real-life data sets demonstrate the advantages of the proposed kernel and its state-of-the-art performance. MOCSM enjoys the desirable property to reduce to the well-known spectral mixture (SM) kernel when a single channel is considered. A comparison with the recently introduced multioutput SM kernel reveals that this is not the case for the latter kernel, which contains quadratic terms that generate undesirable scale effects when the spectral densities of different channels are either very close or very far from each other in the frequency domain.},
  archive      = {J_TNNLS},
  author       = {Kai Chen and Twan van Laarhoven and Perry Groot and Jinsong Chen and Elena Marchiori},
  doi          = {10.1109/TNNLS.2019.2946082},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2255-2266},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multioutput convolution spectral mixture for gaussian processes},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Insights into multiple/single lower bound approximation for
extended variational inference in non-gaussian structured data modeling.
<em>TNNLS</em>, <em>31</em>(7), 2240–2254. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For most of the non-Gaussian statistical models, the data being modeled represent strongly structured properties, such as scalar data with bounded support (e.g., beta distribution), vector data with unit length (e.g., Dirichlet distribution), and vector data with positive elements (e.g., generalized inverted Dirichlet distribution). In practical implementations of non-Gaussian statistical models, it is infeasible to find an analytically tractable solution to estimating the posterior distributions of the parameters. Variational inference (VI) is a widely used framework in Bayesian estimation. Recently, an improved framework, namely, the extended VI (EVI), has been introduced and applied successfully to a number of non-Gaussian statistical models. EVI derives analytically tractable solutions by introducing lower bound approximations to the variational objective function. In this paper, we compare two approximation strategies, namely, the multiple lower bounds (MLBs) approximation and the single lower bound (SLB) approximation, which can be applied to carry out the EVI. For implementation, two different conditions, the weak and the strong conditions, are discussed. Convergence of the EVI depends on the selection of the lower bound, regardless of the choice of weak or strong condition. We also discuss the convergence properties to clarify the differences between MLB and SLB. Extensive comparisons are made based on some EVI-based non-Gaussian statistical models. Theoretical analysis is conducted to demonstrate the differences between the weak and strong conditions. Experimental results based on real data show advantages of the SLB approximation over the MLB approximation.},
  archive      = {J_TNNLS},
  author       = {Zhanyu Ma and Jiyang Xie and Yuping Lai and Jalil Taghia and Jing-Hao Xue and Jun Guo},
  doi          = {10.1109/TNNLS.2019.2899613},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2240-2254},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Insights into Multiple/Single lower bound approximation for extended variational inference in non-gaussian structured data modeling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial special issue on structured multi-output
learning: Modeling, algorithm, theory, and applications. <em>TNNLS</em>,
<em>31</em>(7), 2236–2239. (<a
href="https://doi.org/10.1109/TNNLS.2020.2981981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured multioutput learning is a topic in artificial intelligence that considers multiple structured outputs prediction for a given input. The output may involve structured objects in the form of sequence, string, tree, lattice, or graph and has values that are characterized by diverse data types, such as binary, nominal, ordinal, and real-valued variables. Such learning problems arise in a variety of real-world applications, ranging from document classification, computer emulation, sensor network analysis, concept-based information retrieval, and human action/causal induction to video analysis, image annotation/retrieval, gene function prediction, and brain science. As many complex real-world scenarios can be posed as a structured multioutput learning problem, their importance and popularity have been increasing steadily.},
  archive      = {J_TNNLS},
  author       = {Weiwei Liu and Xiaobo Shen and Yew-Soon Ong and Ivor W. Tsang and Chen Gong and Vladimir Pavlovic},
  doi          = {10.1109/TNNLS.2020.2981981},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2236-2239},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on structured multi-output learning: Modeling, algorithm, theory, and applications},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020g). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(6), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.2994918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.2994918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A limitation of gradient descent learning. <em>TNNLS</em>,
<em>31</em>(6), 2227–2232. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over decades, gradient descent has been applied to develop learning algorithm to train a neural network (NN). In this brief, a limitation of applying such algorithm to train an NN with persistent weight noise is revealed. Let V(w) be the performance measure of an ideal NN. V(w) is applied to develop the gradient descent learning (GDL). With weight noise, the desired performance measure (denoted as .7 (w)) is E[V ( w)ιw], where w is the noisy weight vector. Applying GDL to train an NN with weight noise, the actual learning objective is clearly not V(w) but another scalar function 1(w). For decades, there is a misconception that 1(w) _ .7(w), and hence, the actual model attained by the GDL is the desired model. However, we show that it might not: 1) with persistent additive weight noise, the actual model attained is the desired model as 1(w) _ .7(w); and 2) with persistent multiplicative weight noise, the actual model attained is unlikely the desired model as 1(w) _ .7(w). Accordingly, the properties of the models attained as compared with the desired models are analyzed and the learning curves are sketched. Simulation results on 1) a simple regression problem and 2) the MNIST handwritten digit recognition are presented to support our claims.},
  archive      = {J_TNNLS},
  author       = {John Sum and Chi-Sing Leung and Kevin Ho},
  doi          = {10.1109/TNNLS.2019.2927689},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2227-2232},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A limitation of gradient descent learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stress-testing memcomputing on hard combinatorial
optimization problems. <em>TNNLS</em>, <em>31</em>(6), 2222–2226. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memcomputing is a novel computing paradigm that employs time non-local dynamical systems to compute with and in memory. The digital version of these machines [digital memcomputing machines or (DMMs)] is scalable, and is particularly suited to solve combinatorial optimization problems. One of its possible realizations is by means of standard electronic circuits, with and without memory. Since these elements are non-quantum, they can be described by ordinary differential equations. Therefore, the circuit representation of DMMs can also be simulated efficiently on our traditional computers. We have indeed previously shown that these simulations only require time and memory resources that scale linearly with the problem size when applied to finding a good approximation to the optimum of hard instances of the maximum-satisfiability problem. The state-of-the-art algorithms, instead, require exponential resources for the same instances. However, in that work, we did not push the simulations to the limit of the processor used. Since linear scalability at smaller problem sizes cannot guarantee linear scalability at much larger sizes, we have extended these results in a stress-test up to $64\times 10^{6}$ variables (corresponding to about 1 billion literals), namely the largest case that we could fit on a single core of an Intel Xeon E5-2860 with 128 GB of dynamic random-access memory (DRAM). For this test, we have employed a commercial simulator, Falcon of MemComputing, Inc. We find that the simulations of DMMs still scale linearly in both time and memory up to these very large problem sizes versus the exponential requirements of the state-of-the-art solvers. These results further reinforce the advantages of the physics-based memcomputing approach compared with traditional ones.},
  archive      = {J_TNNLS},
  author       = {Forrest Sheldon and Pietro Cicotti and Fabio L. Traversa and Massimiliano Di Ventra},
  doi          = {10.1109/TNNLS.2019.2927480},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2222-2226},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stress-testing memcomputing on hard combinatorial optimization problems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A projection neural network for the generalized lasso.
<em>TNNLS</em>, <em>31</em>(6), 2217–2221. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized lasso (GLasso) is an extension of the lasso regression in which there is an l 1 penalty term (or regularization) of the linearly transformed coefficient vector. Finding the optimal solution of GLasso is not straightforward since the penalty term is not differentiable. This brief presents a novel one-layer neural network to solve the generalized lasso for a wide range of penalty transformation matrices. The proposed neural network is proven to be stable in the sense of Lyapunov and converges globally to the optimal solution of the GLasso. It is also shown that the proposed neural solution can solve many optimization problems, including sparse and weighted sparse representations, (weighted) total variation denoising, fused lasso signal approximator, and trend filtering. Disparate experiments on the above problems illustrate and confirm the excellent performance of the proposed neural network in comparison to other competing techniques.},
  archive      = {J_TNNLS},
  author       = {Majid Mohammadi},
  doi          = {10.1109/TNNLS.2019.2927282},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2217-2221},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A projection neural network for the generalized lasso},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved sliding mode control for finite-time
synchronization of nonidentical delayed recurrent neural networks.
<em>TNNLS</em>, <em>31</em>(6), 2209–2216. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief further explores the problem of finite-time synchronization of delayed recurrent neural networks with the mismatched parameters and neuron activation functions. An improved sliding mode control approach is presented for addressing the finite-time synchronization problem. First, by employing the drive-response concept and the synchronization error of drive-response systems, a novel integral sliding mode surface is constructed such that the synchronization error can converge to zero in finite time along the constructed integral sliding mode surface. Second, a suitable sliding mode controller is designed by relying on Lyapunov stability theory such that all system state trajectories can be driven onto the predefined sliding mode surface in finite time. Moreover, it is found that the presented control approach can be conveniently verified and does not need to solve any linear matrix inequality (LMI) to guarantee the finite-time synchronization of delayed recurrent neural networks. Finally, three numerical examples are exploited to demonstrate the effectiveness of the presented control approach.},
  archive      = {J_TNNLS},
  author       = {Jing-Jing Xiong and Guo-Bao Zhang and Jun-Xiao Wang and Tian-Hong Yan},
  doi          = {10.1109/TNNLS.2019.2927249},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2209-2216},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved sliding mode control for finite-time synchronization of nonidentical delayed recurrent neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On optimal time-varying feedback controllability for
probabilistic boolean control networks. <em>TNNLS</em>, <em>31</em>(6),
2202–2208. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief studies controllability for probabilistic Boolean control network (PBCN) with time-varying feedback control laws. The concept of feedback controllability with an arbitrary probability for PBCNs is formulated first, and a control problem to maximize the probability of time-varying feedback controllability is investigated afterward. By introducing semitensor product (STP) technique, an equivalent multistage decision problem is deduced, and then a novel optimization algorithm is proposed to obtain the maximum probability of controllability and the corresponding optimal feedback law simultaneously. The advantages of the time-varying optimal controller obtained by the proposed algorithm, compared to the time-invariant one, are illustrated by numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Mitsuru Toyoda and Yuhu Wu},
  doi          = {10.1109/TNNLS.2019.2927241},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2202-2208},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On optimal time-varying feedback controllability for probabilistic boolean control networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stream deep hashing with class-specific centers for
supervised image search. <em>TNNLS</em>, <em>31</em>(6), 2189–2201. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Cheng Deng and Erkun Yang and Tongliang Liu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2019.2929068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2189-2201},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two-stream deep hashing with class-specific centers for supervised image search},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor networks for latent variable analysis: Higher order
canonical polyadic decomposition. <em>TNNLS</em>, <em>31</em>(6),
2174–2188. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The canonical polyadic decomposition (CPD) is a convenient and intuitive tool for tensor factorization; however, for higher order tensors, it often exhibits high computational cost and permutation of tensor entries, and these undesirable effects grow exponentially with the tensor order. Prior compression of tensor in-hand can reduce the computational cost of CPD, but this is only applicable when the rank R of the decomposition does not exceed the tensor dimensions. To resolve these issues, we present a novel method for CPD of higher order tensors, which rests upon a simple tensor network of representative inter-connected core tensors of orders not higher than 3. For rigor, we develop an exact conversion scheme from the core tensors to the factor matrices in CPD and an iterative algorithm of low complexity to estimate these factor matrices for the inexact case. Comprehensive simulations over a variety of scenarios support the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Anh-Huy Phan and Andrzej Cichocki and Ivan Oseledets and Giuseppe G. Calvi and Salman Ahmadi-Asl and Danilo P. Mandic},
  doi          = {10.1109/TNNLS.2019.2929063},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2174-2188},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tensor networks for latent variable analysis: Higher order canonical polyadic decomposition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time object detection with reduced region proposal
network via multi-feature concatenation. <em>TNNLS</em>, <em>31</em>(6),
2164–2173. (<a
href="https://doi.org/10.1109/TNNLS.2019.2929059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, object detection became more and more important following the successful results from studies in deep learning. Two types of neural network architectures are used for object detection: one-stage and two-stage. In this paper, we analyze a widely used two-stage architecture called Faster R-CNN to improve the inference time and achieve real-time object detection without compromising on accuracy. To increase the computation efficiency, pruning is first adopted to reduce the weights in convolutional and fully connected (FC) layers. However, this reduces the accuracy of detection. To address this loss in accuracy, we propose a reduced region proposal network (RRPN) with dilated convolution and concatenation of multi-scale features. In the assisted multi-feature concatenation, we propose the intra-layer concatenation and proposal refinement to efficiently integrate the feature maps from different convolutional layers; this is then provided as an input to the RRPN. Using the proposed method, the network can find object bounding boxes more accurately, thus compensating for the loss arising from compression. Finally, we test the proposed architecture using ZF-Net and VGG16 as a backbone network on the image sets in PASCAL VOC 2007 or VOC 2012. The results show that we can compress the parameters of the ZF-Net-based network by 81.2\% and save 66\% of computation. The parameters of VGG16-based network are compressed by 73\% and save 77\% of computation. Consequently, the inference speed is improved from 27 to 40 frames/s for ZF-Net and 9 to 27 frames/s for VGG16. Despite significant compression rates, the accuracy of ZF-Net is increased from 2.2\% to 60.2\% mean average precision (mAP) and that of VGG16 is increased from 2.6\% to 69.1\% mAP.},
  archive      = {J_TNNLS},
  author       = {Kuan-Hung Shih and Ching-Te Chiu and Jiou-Ai Lin and Yen-Yu Bu},
  doi          = {10.1109/TNNLS.2019.2929059},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2164-2173},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Real-time object detection with reduced region proposal network via multi-feature concatenation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive weighted sparse principal component analysis for
robust unsupervised feature selection. <em>TNNLS</em>, <em>31</em>(6),
2153–2163. (<a
href="https://doi.org/10.1109/TNNLS.2019.2928755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current unsupervised feature selection methods cannot well select the effective features from the corrupted data. To this end, we propose a robust unsupervised feature selection method under the robust principal component analysis (PCA) reconstruction criterion, which is named the adaptive weighted sparse PCA (AW-SPCA). In the proposed method, both the regularization term and the reconstruction error term are constrained by the ℓ 2,1 -norm: the ℓ 2,1 -norm regularization term plays a role in the feature selection, while the ℓ 2,1 -norm reconstruction error term plays a role in the robust reconstruction. The proposed method is in a convex formulation, and the selected features by it can be used for robust reconstruction and clustering. Experimental results demonstrate that the proposed method can obtain better reconstruction and clustering performance, especially for the corrupted data.},
  archive      = {J_TNNLS},
  author       = {Shuangyan Yi and Zhenyu He and Xiao-Yuan Jing and Yi Li and Yiu-Ming Cheung and Feiping Nie},
  doi          = {10.1109/TNNLS.2019.2928755},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2153-2163},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive weighted sparse principal component analysis for robust unsupervised feature selection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural network prescribed performance bounded-
<span class="math inline"><em>H</em><sub>∞</sub></span> tracking control
for a class of stochastic nonlinear systems. <em>TNNLS</em>,
<em>31</em>(6), 2140–2152. (<a
href="https://doi.org/10.1109/TNNLS.2019.2928594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to give a design strategy on the prescribed performance H ∞ tracking control problem for a class of strict-feedback stochastic nonlinear systems based on the backstepping technique. Generally, by using the backstepping design method, the stochastic nonlinear systems can only be made to be bounded in probability and it is difficult to achieve the H ∞ performance criterion due to the positive constant term appeared in the stability analysis. Thus, a novel concept with regard to the bounded-H∞ performance is proposed in this paper to overcome the design difficulty. By using the new concept and the adaptive neural network technique as well as Gronwall inequality, an adaptive neural network prescribed performance bounded-H ∞ tracking controller is designed. Therein, neural networks are used to approximate the unknown packaged nonlinear functions. The assumption that the approximation errors of neural networks are square-integrable in some literature is eliminated. The designed controller guarantees that all the signals in the closed-loop stochastic nonlinear systems are bounded in probability, the tracking error is constrained into an adjustable neighborhood of the origin with the prescribed performance bounds, and the controlled system has a given H ∞ disturbance attenuation performance for external disturbances. Finally, the simulation results are provided to illustrate the effectiveness and feasibility of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Hui Liu and Xiaohua Li and Xiaoping Liu and Huanqing Wang},
  doi          = {10.1109/TNNLS.2019.2928594},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2140-2152},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network prescribed performance bounded- $H_{\infty}$ tracking control for a class of stochastic nonlinear systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Output feedback control for set stabilization of boolean
control networks. <em>TNNLS</em>, <em>31</em>(6), 2129–2139. (<a
href="https://doi.org/10.1109/TNNLS.2019.2928028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the output feedback set stabilization problem for Boolean control networks (BCNs) is investigated with the help of the semi-tensor product (STP) tool. The concept of output feedback control invariant (OFCI) subset is introduced, and novel methods are developed to obtain the OFCI subsets. Based on the OFCI subsets, a technique, named spanning tree method, is further introduced to calculate all possible output feedback set stabilizers. An example concerning lac operon for the bacterium Escherichia coli is given to illustrate the effectiveness of the proposed method. This technique can also be used to solve the state feedback (set) stabilization problem for BCNs. Compared with the existing results, our method can dramatically reduce the computational cost when designing all possible state feedback stabilizers for BCNs.},
  archive      = {J_TNNLS},
  author       = {Rongjian Liu and Jianquan Lu and Wei Xing Zheng and Jürgen Kurths},
  doi          = {10.1109/TNNLS.2019.2928028},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2129-2139},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Output feedback control for set stabilization of boolean control networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contrastive hebbian feedforward learning for neural
networks. <em>TNNLS</em>, <em>31</em>(6), 2118–2128. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the biological plausibility of both backpropagation (BP) and contrastive Hebbian learning (CHL) used in the Boltzmann machines. The main claim of this paper is that CHL is a general learning algorithm that can be used to steer feedforward networks toward desirable outcomes, and steer them away from undesirable outcomes without any need for the specialized feedback circuit of BP or the symmetric connections used by the Boltzmann machines. After adding perturbations during the learning phase to all the neurons in the network, multiple feedforward outcomes are classified into Hebbian and anti-Hebbian sets based on the network predictions. The algorithm is applied to networks when optimizing a loss objective where BP excels and is also applied to networks with stochastic binary outputs where BP cannot be easily applied. The power of the proposed algorithm lies in its simplicity where both learning and gradient estimation through stochastic binary activations are combined into a single local Hebbian rule. We will also show that both Hebbian and anti-Hebbian correlations are evaluated from the readily available signals that are fundamentally different from CHL used in the Boltzmann machines. We will demonstrate that the new learning paradigm where Hebbian/anti-Hebbian correlations are based on correct/incorrect predictions is a powerful concept that separates this paper from other biologically inspired learning algorithms.},
  archive      = {J_TNNLS},
  author       = {Noureddine Kermiche},
  doi          = {10.1109/TNNLS.2019.2927957},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2118-2128},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Contrastive hebbian feedforward learning for neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Distributed optimization for disturbed second-order
multiagent systems based on active antidisturbance control.
<em>TNNLS</em>, <em>31</em>(6), 2104–2117. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the distributed optimization problem is studied for second-order multiagent systems with both mismatched and matched disturbances. For this problem, a distributed active antidisturbance control framework is established, which consists of both disturbance estimation/compensation and distributed feedforward-feedback composite control design. In the first stage, some disturbance estimators are utilized to estimate various types of matched/mismatched disturbances for each agent. In the second stage, for each agent, based on the disturbance estimates, the information exchanges between the neighboring agents, and the gradient of the local cost function only accessible to itself, a kind of distributed composite controllers are proposed. Under these controllers, all the agents&#39; outputs asymptotically reach consensus to the minimizer of the global cost function, which is the sum of all the local cost functions. The closed-loop system convergence is proven based on a new Lyapunov function, convex analysis, and an input-to-state stability criterion. Simulations demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Xiangyu Wang and Shihua Li and Guodong Wang},
  doi          = {10.1109/TNNLS.2019.2951790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2104-2117},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed optimization for disturbed second-order multiagent systems based on active antidisturbance control},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pinning synchronization of directed coupled
reaction-diffusion neural networks with sampled-data communications.
<em>TNNLS</em>, <em>31</em>(6), 2092–2103. (<a
href="https://doi.org/10.1109/TNNLS.2019.2928039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the design of a pinning sampled-data control mechanism for the exponential synchronization of directed coupled reaction-diffusion neural networks (CRDNNs) with sampled-data communications (SDCs). A new Lyapunov-Krasovskii functional (LKF) with some sampled-instant-dependent terms is presented, which can fully utilize the actual sampling information. Then, an inequality is first proposed, which effectively relaxes the restrictions of the positive definiteness of the constructed LKF. Based on the LKF and the inequality, sufficient conditions are derived to exponentially synchronize the directed CRDNNs with SDCs. The desired pinning sampled-data control gain is precisely obtained by solving some linear matrix inequalities (LMIs). Moreover, a less conservative exponential synchronization criterion is also established for directed coupled neural networks with SDCs. Finally, simulation results are provided to verify the effectiveness and merits of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Deqiang Zeng and Ruimei Zhang and Ju H. Park and Zhilin Pu and Yajuan Liu},
  doi          = {10.1109/TNNLS.2019.2928039},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2092-2103},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pinning synchronization of directed coupled reaction-diffusion neural networks with sampled-data communications},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed finite-time fault-tolerant containment control
for multiple unmanned aerial vehicles. <em>TNNLS</em>, <em>31</em>(6),
2077–2091. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the distributed finite-time fault-tolerant containment control problem for multiple unmanned aerial vehicles (multi-UAVs) in the presence of actuator faults and input saturation. The distributed finite-time sliding-mode observer (SMO) is first developed to estimate the reference for each follower UAV. Then, based on the estimated knowledge, the distributed finite-time fault-tolerant controller is recursively designed to guide all follower UAVs into the convex hull spanned by the trajectories of leader UAVs with the help of a new set of error variables. Moreover, the unknown nonlinearities inherent in the multi-UAVs system, computational burden, and input saturation are simultaneously handled by utilizing neural network (NN), minimum parameter learning of NN (MPLNN), first-order sliding-mode differentiator (FOSMD) techniques, and a group of auxiliary systems. Furthermore, the graph theory and Lyapunov stability analysis methods are adopted to guarantee that all follower UAVs can converge to the convex hull spanned by the leader UAVs even in the event of actuator faults. Finally, extensive comparative simulations have been conducted to demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Ziquan Yu and Zhixiang Liu and Youmin Zhang and Yaohong Qu and Chun-Yi Su},
  doi          = {10.1109/TNNLS.2019.2927887},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2077-2091},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed finite-time fault-tolerant containment control for multiple unmanned aerial vehicles},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning-based automatic exploration for
navigation in unknown environment. <em>TNNLS</em>, <em>31</em>(6),
2064–2076. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.},
  archive      = {J_TNNLS},
  author       = {Haoran Li and Qichao Zhang and Dongbin Zhao},
  doi          = {10.1109/TNNLS.2019.2927869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2064-2076},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning-based automatic exploration for navigation in unknown environment},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Discrete deep hashing with ranking optimization for image
retrieval. <em>TNNLS</em>, <em>31</em>(6), 2052–2063. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For large-scale image retrieval task, a hashing technique has attracted extensive attention due to its efficient computing and applying. By using the hashing technique in image retrieval, it is crucial to generate discrete hash codes and preserve the neighborhood ranking information simultaneously. However, both related steps are treated independently in most of the existing deep hashing methods, which lead to the loss of key category-level information in the discretization process and the decrease in discriminative ranking relationship. In order to generate discrete hash codes with notable discriminative information, we integrate the discretization process and the ranking process into one architecture. Motivated by this idea, a novel ranking optimization discrete hashing (RODH) method is proposed, which directly generates discrete hash codes (e.g., +1/-1) from raw images by balancing the effective category-level information of discretization and the discrimination of ranking information. The proposed method integrates convolutional neural network, discrete hash function learning, and ranking function optimizing into a unified framework. Meanwhile, a novel loss function based on label information and mean average precision (MAP) is proposed to preserve the label consistency and optimize the ranking information of hash codes simultaneously. Experimental results on four benchmark data sets demonstrate that RODH can achieve superior performance over the state-of-the-art hashing methods.},
  archive      = {J_TNNLS},
  author       = {Xiaoqiang Lu and Yaxiong Chen and Xuelong Li},
  doi          = {10.1109/TNNLS.2019.2927868},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2052-2063},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discrete deep hashing with ranking optimization for image retrieval},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Error-based learning mechanism for fast online adaptation in
robot motor control. <em>TNNLS</em>, <em>31</em>(6), 2042–2051. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing state-of-the-art frequency adaptation mechanisms of central pattern generators (CPGs) for robot locomotion control typically rely on correlation-based learning. They do not account for the tracking error that may occur between the actual system motion and CPG output, leading to the loss of precision, unwanted movement, inefficient energy locomotion, and in the worst cases, motor collapse. To overcome this problem, we developed online error-based learning for frequency adaptation of CPGs. The learning mechanism used for error reduction is a novel modification of the dual learner (DL) called dual integral learner (DIL). Being able to reduce tracking and steady-state errors, it can also perform fast and stable learning, adapting the CPG frequency to match the performance of robotic systems. Control parameters of the DIL are more straightforward for complex systems (like walking robots), compared to traditional correlation-based learning, since they correspond to error reduction. Due to its embedded memory, the DIL can relearn quickly and recover spontaneously from the previously learned parameters. All these features are not covered by the existing frequency adaptation mechanisms. We integrated the DIL into a neural CPG-based motor control system for use on different legged robots with various morphologies for evaluation. The results show that: 1) the DIL does not require precise adjustment of its parameters to fit specific robots; and 2) the DIL can automatically and quickly adapt the CPG frequency to the robots such that the entire trajectory of the CPG can be precisely followed with very low tracking and steady-state errors. Consequently, the robots can perform the desired movements with more energy-efficient locomotion compared to the state-of-the-art correlation-based learning mechanism called frequency adaptation through fast dynamical coupling (AFDC). In the future, the proposed error-based learning mechanism for fast online adaptation in robot motor control can be used as a basis for trajectory optimization, universal controllers, and other studies concerning the change of intrinsic or extrinsic parameters.},
  archive      = {J_TNNLS},
  author       = {Mathias Thor and Poramate Manoonpong},
  doi          = {10.1109/TNNLS.2019.2927737},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2042-2051},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Error-based learning mechanism for fast online adaptation in robot motor control},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Precise measurement of position and attitude based on
convolutional neural network and visual correspondence relationship.
<em>TNNLS</em>, <em>31</em>(6), 2030–2041. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate measurement of position and attitude information is particularly important. Traditional measurement methods generally require high-precision measurement equipment for analysis, leading to high costs and limited applicability. Vision-based measurement schemes need to solve complex visual relationships. With the extensive development of neural networks in related fields, it has become possible to apply them to the object position and attitude. In this paper, we propose an object pose measurement scheme based on convolutional neural network and we have successfully implemented end-to-end position and attitude detection. Furthermore, to effectively expand the measurement range and reduce the number of training samples, we demonstrated the independence of objects in each dimension and proposed subadded training programs. At the same time, we generated generating image encoder to guarantee the detection performance of the training model in practical applications.},
  archive      = {J_TNNLS},
  author       = {Jiachen Yang and Jiabao Man and Meng Xi and Xinbo Gao and Wen Lu and Qinggang Meng},
  doi          = {10.1109/TNNLS.2019.2927719},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2030-2041},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Precise measurement of position and attitude based on convolutional neural network and visual correspondence relationship},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two projection neural networks with reduced model complexity
for nonlinear programming. <em>TNNLS</em>, <em>31</em>(6), 2020–2029.
(<a href="https://doi.org/10.1109/TNNLS.2019.2927639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent reports show that projection neural networks with a low-dimensional state space can enhance computation speed obviously. This paper proposes two projection neural networks with reduced model dimension and complexity (RDPNNs) for solving nonlinear programming (NP) problems. Compared with existing projection neural networks for solving NP, the proposed two RDPNNs have a low-dimensional state space and low model complexity. Under the condition that the Hessian matrix of the associated Lagrangian function is positive semi-definite and positive definite at each Karush-Kuhn-Tucker point, the proposed two RDPNNs are proven to be globally stable in the sense of Lyapunov and converge globally to a point satisfying the reduced optimality condition of NP. Therefore, the proposed two RDPNNs are theoretically guaranteed to solve convex NP problems and a class of nonconvex NP problems. Computed results show that the proposed two RDPNNs have a faster computation speed than the existing projection neural networks for solving NP problems.},
  archive      = {J_TNNLS},
  author       = {Youshen Xia and Jun Wang and Wenzhong Guo},
  doi          = {10.1109/TNNLS.2019.2927639},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2020-2029},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two projection neural networks with reduced model complexity for nonlinear programming},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning markov blankets from multiple interventional data
sets. <em>TNNLS</em>, <em>31</em>(6), 2005–2019. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning Markov blankets (MBs) plays an important role in many machine learning tasks, such as causal Bayesian network structure learning, feature selection, and domain adaptation. Since variables included in the MB of a target variable of interest have causal relationships with the target, the MB can serve as the basis of learning the global structure of a causal Bayesian network or as a reliable and robust feature set for classification, both within the same domain or across domains. In this article, we study the problem of learning the MB of a target variable from multiple interventional data sets. Data sets attained from interventional experiments contain richer causal information than passively observed data (observational data) for MB discovery. However, almost all existing MB discovery methods are designed for learning MBs from a single observational data set. To learn MBs from multiple interventional data sets, we face two challenges: 1) unknown intervention variables and 2) nonidentical data distributions. To address these challenges, we theoretically analyze: 1) under what conditions we can find the correct MB of a target variable and 2) under what conditions we can identify the causes of the target variable via discovering its MB. Based on the theoretical analysis, we propose a new algorithm for learning MBs from multiple interventional data sets, and we present the conditions/assumptions that assure the correctness of the algorithm. To the best of our knowledge, this article is the first to present the theoretical analyses about the conditions for MB discovery in multiple interventional data sets and the algorithm to find the MBs in relation to the conditions. Using benchmark Bayesian networks and real-world data sets, the experiments have validated the effectiveness and efficiency of the proposed algorithm in this article.},
  archive      = {J_TNNLS},
  author       = {Kui Yu and Lin Liu and Jiuyong Li},
  doi          = {10.1109/TNNLS.2019.2927636},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2005-2019},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning markov blankets from multiple interventional data sets},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impulsive consensus of nonlinear multi-agent systems via
edge event-triggered control. <em>TNNLS</em>, <em>31</em>(6), 1995–2004.
(<a href="https://doi.org/10.1109/TNNLS.2019.2927623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we mainly investigate two kinds of consensuses of multi-agent systems (MASs) with nonlinear dynamics based on impulsive control, event-triggered control, and sampled-data control. The two types of impulsive protocols are proposed for the case without and with leader agent. Edge event-triggered technique is presented, where for each communication link, occurrence of edge event can activate the mutual state sampling and controller update of the corresponding agents. The control approach combines the characteristics of impulsive control and edge event-triggered control and is defined as “impulsive edge event-triggered control.” It has good performance in robustness against disturbance and reduces the communication cost. The results with the aid of the Lyapunov function approach and stability theory of impulsive control show that if some sufficient conditions are satisfied, the consensus of MASs can be guaranteed and the rate of convergence can be exponentially estimated. Additionally, Zeno-behavior can be eliminated by using impulsive edge event-triggered control, which reduces the burden of event detectors. Finally, two simulations are provided to illustrate the effectiveness and performance of our theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Ziqiang Xu and Chuandong Li and Yiyan Han},
  doi          = {10.1109/TNNLS.2019.2927623},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1995-2004},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Impulsive consensus of nonlinear multi-agent systems via edge event-triggered control},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stubborn state estimation for delayed neural networks using
saturating output errors. <em>TNNLS</em>, <em>31</em>(6), 1982–1994. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the stubborn state estimation of delayed neural networks that subject to a general class of disturbances in measurements, including outliers and impulsive disturbances as its special cases. This class of disturbances may be unbounded, irregular, and assorted; therefore, they can hardly be suppressed by existing identification-based estimation approaches. In this paper, a stubborn state estimator is constructed by intentionally devising a saturation scheme on the injection of output estimation error. The embedded saturation can effectively resist the influences from these measurement disturbances by saturating them. Moreover, the saturation threshold in the designed scheme is not constant but governed by a dynamic equation with parameters to be designed. Benefiting from this adaptiveness, the estimator obtains more freedom in dealing with various disturbances. By combining a novel Lyapunov functional, the generalized sector condition and two latest integral inequalities, a delay-dependent criterion is derived in a less conservative way to check whether the estimation error system with this dynamic saturation is globally stable. A sufficient condition with two tuning scalars is further provided to codesign the gain of the state estimator and the evolution law of the saturation threshold. Finally, two numerical examples are used to illustrate the stubbornness of this state estimator in the presence of measurement outliers or impulsive disturbances.},
  archive      = {J_TNNLS},
  author       = {Chengda Lu and Min Wu and Yong He},
  doi          = {10.1109/TNNLS.2019.2927610},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1982-1994},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stubborn state estimation for delayed neural networks using saturating output errors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-based adaptive neural tracking control for
discrete-time stochastic nonlinear systems: A triggering threshold
compensation strategy. <em>TNNLS</em>, <em>31</em>(6), 1968–1981. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the event-triggered (ET) tracking control problem for a class of discrete-time strict-feedback nonlinear systems subject to both stochastic noises and limited controller-to-actuator communication capacities. The ET mechanism with fixed triggering threshold is designed to decide whether the current control signal should be transmitted to the actuator. A systematic framework is developed to construct a novel adaptive neural controller by directly applying the backstepping procedure to the underlying system. The proposed framework overcomes the noncausality problem, avoids the possible controller-related singularity problem, and gets rid of the neural approximation of the virtual control laws. Under the ET mechanism, the corresponding ET-based actuator is put forward by introducing an ET threshold compensation operator. Such a compensation operator (with an adjustable design parameter) is subtly designed based on a hyperbolic tangent function and a sign function. The threshold compensation error is analytically characterized in terms of a time-varying parameter, and the error bound is shown to be relatively small that is dependent on the adjustable design parameter. Compared with the traditional ET-based actuator without the compensation operator, the proposed ET-based actuator exhibits several distinguished features including: 1) improvement of the tracking accuracy (especially at the triggering instants); 2) further mitigation of the communication load; and 3) enlargement of the allowable range of the ET threshold. These features are illustrated by numerical and practical examples.},
  archive      = {J_TNNLS},
  author       = {Min Wang and Zidong Wang and Yun Chen and Weiguo Sheng},
  doi          = {10.1109/TNNLS.2019.2927595},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1968-1981},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based adaptive neural tracking control for discrete-time stochastic nonlinear systems: A triggering threshold compensation strategy},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance-constrained recursive state estimation for
time-varying complex networks with quantized measurements and uncertain
inner coupling. <em>TNNLS</em>, <em>31</em>(6), 1955–1967. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new recursive state estimation problem is discussed for a class of discrete time-varying stochastic complex networks with uncertain inner coupling and signal quantization under the error-variance constraints. The coupling strengths are allowed to be varying within certain intervals, and the measurement signals are subject to the quantization effects before being transmitted to the remote estimator. The focus of the conducted topic is on the design of a variance-constrained state estimation algorithm with the aim to ensure a locally minimized upper bound on the estimation error covariance at every sampling instant. Furthermore, the boundedness of the resulting estimation error is analyzed, and a sufficient criterion is established to ensure the desired exponential boundedness of the state estimation error in the mean square sense. Finally, some simulations are proposed with comparisons to illustrate the validity of the newly developed variance-constrained estimation method.},
  archive      = {J_TNNLS},
  author       = {Jun Hu and Zidong Wang and Guo-Ping Liu and Hongxu Zhang},
  doi          = {10.1109/TNNLS.2019.2927554},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1955-1967},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Variance-constrained recursive state estimation for time-varying complex networks with quantized measurements and uncertain inner coupling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal power management based on q-learning and
neuro-dynamic programming for plug-in hybrid electric vehicles.
<em>TNNLS</em>, <em>31</em>(6), 1942–1954. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy optimization for plug-in hybrid electric vehicles (PHEVs) is a challenging problem due to the system complexity and many physical and operational constraints in PHEVs. In this paper, we present a Q-learning-based in-vehicle learning system that is free of physical models and can robustly converge to an optimal energy control solution. The proposed machine learning algorithms combine neuro-dynamic programming (NDP) with future trip information to effectively estimate the expected future energy cost (expected cost-to-go) for a given vehicle state and control actions. The convergences of these learning algorithms were demonstrated on both fixed and randomly selected drive cycles. Based on the characteristics of these learning algorithms, we propose a two-stage deployment solution for PHEV power management applications. Furthermore, we introduce a new initialization strategy, which combines the optimal learning with a properly selected penalty function. This initialization scheme can reduce the learning convergence time by 70\%, which is a significant improvement for in-vehicle implementation efficiency. Finally, we develop a neural network (NN) for predicting battery state-of-charge (SoC), rendering the proposed power management controller completely free of physical models.},
  archive      = {J_TNNLS},
  author       = {Chang Liu and Yi Lu Murphey},
  doi          = {10.1109/TNNLS.2019.2927531},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1942-1954},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal power management based on Q-learning and neuro-dynamic programming for plug-in hybrid electric vehicles},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural quantized control for a class of MIMO
switched nonlinear systems with asymmetric actuator dead-zone.
<em>TNNLS</em>, <em>31</em>(6), 1927–1941. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concentrates on the adaptive state-feedback quantized control problem for a class of multiple-input-multiple-output (MIMO) switched nonlinear systems with unknown asymmetric actuator dead-zone. In this study, we employ different quantizers for different subsystem inputs. The main challenge of this study is to deal with the coupling between the quantizers and the dead-zone nonlinearities. To solve this problem, a novel approximation model for the coupling between quantizer and dead-zone is proposed. Then, the corresponding robust adaptive law is designed to eliminate this nonlinear term asymptotically. A direct neural control scheme is employed to reduce the number of adaptive laws significantly. The backstepping-based adaptive control scheme is also presented to guarantee the system performance. Finally, two simulation examples are presented to show the effectiveness of our control scheme.},
  archive      = {J_TNNLS},
  author       = {Kan Xie and Ziliang Lyu and Zhi Liu and Yun Zhang and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2019.2927507},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1927-1941},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural quantized control for a class of MIMO switched nonlinear systems with asymmetric actuator dead-zone},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multistability of almost periodic solution for memristive
cohen-grossberg neural networks with mixed delays. <em>TNNLS</em>,
<em>31</em>(6), 1914–1926. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the multistability analysis of almost periodic state solutions for memristive Cohen-Grossberg neural networks (MCGNNs) with both distributed delay and discrete delay. The activation function of the considered MCGNNs is generalized to be nonmonotonic and nonpiecewise linear. It is shown that the MCGNNs with n-neuron have (K + 1) n locally exponentially stable almost periodic solutions, where nature number K depends on the geometrical structure of the considered activation function. Compared with the previous related works, the number of almost periodic state solutions of the MCGNNs is extensively increased. The obtained conclusions in this paper are also capable of studying the multistability of equilibrium points or periodic solutions of the MCGNNs. Moreover, the enlarged attraction basins of attractors are estimated based on original partition. Some comparisons and convincing numerical examples are provided to substantiate the superiority and efficiency of obtained results.},
  archive      = {J_TNNLS},
  author       = {Sitian Qin and Qiang Ma and Jiqiang Feng and Chen Xu},
  doi          = {10.1109/TNNLS.2019.2927506},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1914-1926},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multistability of almost periodic solution for memristive cohen-grossberg neural networks with mixed delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A maximally split and relaxed ADMM for regularized extreme
learning machines. <em>TNNLS</em>, <em>31</em>(6), 1899–1913. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the salient features of the extreme learning machine (ELM) is its fast learning speed. However, in a big data environment, the ELM still suffers from an overly heavy computational load due to the high dimensionality and the large amount of data. Using the alternating direction method of multipliers (ADMM), a convex model fitting problem can be split into a set of concurrently executable subproblems, each with just a subset of model coefficients. By maximally splitting across the coefficients and incorporating a novel relaxation technique, a maximally split and relaxed ADMM (MS-RADMM), along with a scalarwise implementation, is developed for the regularized ELM (RELM). The convergence conditions and the convergence rate of the MS-RADMM are established, which exhibits linear convergence with a smaller convergence ratio than the unrelaxed maximally split ADMM. The optimal parameter values of the MS-RADMM are obtained and a fast parameter selection scheme is provided. Experiments on ten benchmark classification data sets are conducted, the results of which demonstrate the fast convergence and parallelism of the MS-RADMM. Complexity comparisons with the matrix-inversion-based method in terms of the numbers of multiplication and addition operations, the computation time and the number of memory cells are provided for performance evaluation of the MS-RADMM.},
  archive      = {J_TNNLS},
  author       = {Xiaoping Lai and Jiuwen Cao and Xiaofeng Huang and Tianlei Wang and Zhiping Lin},
  doi          = {10.1109/TNNLS.2019.2927385},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1899-1913},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A maximally split and relaxed ADMM for regularized extreme learning machines},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative memory for lifelong learning. <em>TNNLS</em>,
<em>31</em>(6), 1884–1898. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong learning is a crucial issue in advanced artificial intelligence. It requires the learning system to learn and accumulate knowledge from sequential tasks. The learning system needs to deal with increasingly more domains and tasks. We consider that the key to an effective and efficient lifelong learning system is the ability to memorize and recall the learned knowledge using neural networks. Following this idea, we propose Generative Memory (GM) as a novel memory module, and the resulting lifelong learning system is referred to as the GM Net (GMNet). To make the GMNet feasible, we propose a novel learning mechanism, referred to as P-invariant learning method. It replaces the memory of the real data by a memory of the data distribution, which makes it possible for the learning system to accurately and continuously accumulate the learned experiences. We demonstrate that GMNet achieves the state-of-the-art performance on lifelong learning tasks.},
  archive      = {J_TNNLS},
  author       = {Xin Su and Shangqi Guo and Tian Tan and Feng Chen},
  doi          = {10.1109/TNNLS.2019.2927369},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1884-1898},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generative memory for lifelong learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Incremental reinforcement learning in continuous spaces via
policy relaxation and importance weighting. <em>TNNLS</em>,
<em>31</em>(6), 1870–1883. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a systematic incremental learning method is presented for reinforcement learning in continuous spaces where the learning environment is dynamic. The goal is to adjust the previously learned policy in the original environment to a new one incrementally whenever the environment changes. To improve the adaptability to the ever-changing environment, we propose a two-step solution incorporated with the incremental learning procedure: policy relaxation and importance weighting. First, the behavior policy is relaxed to a random one in the initial learning episodes to encourage a proper exploration in the new environment. It alleviates the conflict between the new information and the existing knowledge for a better adaptation in the long term. Second, it is observed that episodes receiving higher returns are more in line with the new environment, and hence contain more new information. During parameter updating, we assign higher importance weights to the learning episodes that contain more new information, thus encouraging the previous optimal policy to be faster adapted to a new one that fits in the new environment. Empirical studies on continuous controlling tasks with varying configurations verify that the proposed method achieves a significantly faster adaptation to various dynamic environments than the baselines.},
  archive      = {J_TNNLS},
  author       = {Zhi Wang and Han-Xiong Li and Chunlin Chen},
  doi          = {10.1109/TNNLS.2019.2927320},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1870-1883},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incremental reinforcement learning in continuous spaces via policy relaxation and importance weighting},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Change detection in graph streams by learning graph
embeddings on constant-curvature manifolds. <em>TNNLS</em>,
<em>31</em>(6), 1856–1869. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The space of graphs is often characterized by a nontrivial geometry, which complicates learning and inference in practical applications. A common approach is to use embedding techniques to represent graphs as points in a conventional Euclidean space, but non-Euclidean spaces have often been shown to be better suited for embedding graphs. Among these, constant-curvature Riemannian manifolds (CCMs) offer embedding spaces suitable for studying the statistical properties of a graph distribution, as they provide ways to easily compute metric geodesic distances. In this paper, we focus on the problem of detecting changes in stationarity in a stream of attributed graphs. To this end, we introduce a novel change detection framework based on neural networks and CCMs, which takes into account the non-Euclidean nature of graphs. Our contribution in this paper is twofold. First, via a novel approach based on adversarial learning, we compute graph embeddings by training an autoencoder to represent graphs on CCMs. Second, we introduce two novel change detection tests operating on CCMs. We perform experiments on synthetic data, as well as two real-world application scenarios: the detection of epileptic seizures using functional connectivity brain networks and the detection of hostility between two subjects, using human skeletal graphs. Results show that the proposed methods are able to detect even small changes in a graph-generating process, consistently outperforming approaches based on Euclidean embeddings.},
  archive      = {J_TNNLS},
  author       = {Daniele Grattarola and Daniele Zambon and Lorenzo Livi and Cesare Alippi},
  doi          = {10.1109/TNNLS.2019.2927301},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1856-1869},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Change detection in graph streams by learning graph embeddings on constant-curvature manifolds},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep spiking neural network for video-based disguise face
recognition based on dynamic facial movements. <em>TNNLS</em>,
<em>31</em>(6), 1843–1855. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of social media and smart devices, the face as one of the key biometrics becomes vital for person identification. Among those face recognition algorithms, video-based face recognition methods could make use of both temporal and spatial information just as humans do to achieve better classification performance. However, they cannot identify individuals when certain key facial areas, such as eyes or nose, are disguised by heavy makeup or rubber/digital masks. To this end, we propose a novel deep spiking neural network architecture in this paper. It takes dynamic facial movements, the facial muscle changes induced by speaking or other activities, as the sole input. An event-driven continuous spike-timing-dependent plasticity learning rule with adaptive thresholding is applied to train the synaptic weights. The experiments on our proposed video-based disguise face database (MakeFace DB) demonstrate that the proposed learning method performs very well, i.e., it achieves from 95\% to 100\% correct classification rates under various realistic experimental scenarios.},
  archive      = {J_TNNLS},
  author       = {Daqi Liu and Nicola Bellotto and Shigang Yue},
  doi          = {10.1109/TNNLS.2019.2927274},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1843-1855},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep spiking neural network for video-based disguise face recognition based on dynamic facial movements},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graphical nash equilibria and replicator dynamics on complex
networks. <em>TNNLS</em>, <em>31</em>(6), 1831–1842. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise-interaction graphical games have been widely used in the study and design of strategic interaction in multiagent systems. With regard to this issue, one entitative problem is actually to understand how the interaction structure of agents affects the strategy configuration of Nash equilibria. This paper intends to study the effect of interaction networks on Nash equilibria in pairwise-interaction graphical games. We first show that interaction networks may induce new strategy equilibria in pairwise-interaction graphical games and then provide graphical conditions for the existence of these network-induced equilibria. Furthermore, to determine Nash equilibria of pairwise-interaction graphical games, a graphical replicator dynamics model is formulated, and its connection with graphical games is established. In detail, it is shown that every Nash equilibrium of the graphical games corresponds to a fixed point of the graphical replicator dynamics and that every asymptotically stable fixed point of the graphical replicator dynamics corresponds to a strict pure Nash equilibrium of the graphical games. The obtained results are applied in understanding coordination in complex networks and determination of structural conflicts in signed graphs. This work may provide new insights into understanding and designing strategy equilibria and dynamics in games on networks.},
  archive      = {J_TNNLS},
  author       = {Shaolin Tan and Yaonan Wang},
  doi          = {10.1109/TNNLS.2019.2927233},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1831-1842},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graphical nash equilibria and replicator dynamics on complex networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate policy-based accelerated deep reinforcement
learning. <em>TNNLS</em>, <em>31</em>(6), 1820–1830. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the deep reinforcement learning (DRL) algorithms have been developed rapidly and have achieved excellent performance in many challenging tasks. However, due to the complexity of network structure and a large amount of network parameters, the training of deep network is time-consuming, and consequently, the learning efficiency of DRL is limited. In this paper, aiming to speed up the learning process of DRL agent, we propose a novel approximate policy-based accelerated (APA) algorithm from the viewpoint of the error analysis of approximate policy iteration reinforcement learning algorithms. The proposed APA is proven to be convergent even with a more aggressive learning rate, making the DRL agent have a faster learning speed. Furthermore, to combine the accelerated algorithm with deep Q-network (DQN), Double DQN and deep deterministic policy gradient (DDPG), we proposed three novel DRL algorithms: APA-DQN, APA-Double DQN, and APA-DDPG, which demonstrates the adaptability of the accelerated algorithm with DRL algorithms. We have tested the proposed algorithms on both discrete-action and continuous-action tasks. Their superior performance demonstrates their great potential in the practical applications.},
  archive      = {J_TNNLS},
  author       = {Xuesong Wang and Yang Gu and Yuhu Cheng and Aiping Liu and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2019.2927227},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1820-1830},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Approximate policy-based accelerated deep reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of state-dependent switching laws for stability of
switched stochastic neural networks with time-delays. <em>TNNLS</em>,
<em>31</em>(6), 1808–1819. (<a
href="https://doi.org/10.1109/TNNLS.2019.2927161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the stability properties of switched stochastic neural networks (SSNNs) with time-varying delays whose subsystem is not necessarily stable. We introduce state-dependent switching (SDS) as a tool for stability analysis. Some SDS laws for asymptotic stability and p th moment exponentially stable are designed by employing Lyapunov-Krasovskii (L-K) functional and Lyapunov-Razumikhin (L-R) method, respectively. It is shown that the stability of SSNNs with time-varying delays composed of unstable subsystems can be achieved by using SDS law. The control gains in the designed SDS laws can be derived by solving the LMIs in derived stability criteria. Two numerical examples are provided to demonstrate the effectiveness of the proposed SDS laws.},
  archive      = {J_TNNLS},
  author       = {Dan Yang and Xiaodi Li and Shiji Song},
  doi          = {10.1109/TNNLS.2019.2927161},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1808-1819},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design of state-dependent switching laws for stability of switched stochastic neural networks with time-delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lightweight pyramid networks for image deraining.
<em>TNNLS</em>, <em>31</em>(6), 1794–1807. (<a
href="https://doi.org/10.1109/TNNLS.2019.2926481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep convolutional neural networks (CNNs) have found major success in image deraining, but at the expense of an enormous number of parameters. This limits their potential applications, e.g., in mobile devices. In this paper, we propose a lightweight pyramid networt (LPNet) for single-image deraining. Instead of designing a complex network structure, we use domain-specific knowledge to simplify the learning process. In particular, we find that by introducing the mature Gaussian-Laplacian image pyramid decomposition technology to the neural network, the learning problem at each pyramid level is greatly simplified and can be handled by a relatively shallow network with few parameters. We adopt recursive and residual network structures to build the proposed LPNet, which has less than 8K parameters while still achieving the state-of-the-art performance on rain removal. We also discuss the potential value of LPNet for other low- and high-level vision tasks.},
  archive      = {J_TNNLS},
  author       = {Xueyang Fu and Borong Liang and Yue Huang and Xinghao Ding and John Paisley},
  doi          = {10.1109/TNNLS.2019.2926481},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1794-1807},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Lightweight pyramid networks for image deraining},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A taxonomy for neural memory networks. <em>TNNLS</em>,
<em>31</em>(6), 1780–1793. (<a
href="https://doi.org/10.1109/TNNLS.2019.2926466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of neural memory networks have been developed, leading to the need for a systematic approach to analyze and compare their underlying memory structures. Thus, in this paper, we first create a framework for memory organization and then compare four popular dynamic models: vanilla recurrent neural network, long short-term memory, neural stack, and neural RAM. This analysis helps to open the dynamic neural networks&#39; black box from the memory usage prospective. Accordingly, a taxonomy for these networks and their variants is proposed and proved using a unifying architecture. With the taxonomy, both network architectures and learning tasks are classified into four classes, and a one-to-one mapping is built between them to help practitioners select the appropriate architecture. To exemplify each task type, four synthetic tasks with different memory requirements are selected. Moreover, we use some signal processing applications and two natural language processing applications to evaluate the methodology in a realistic setting.},
  archive      = {J_TNNLS},
  author       = {Ying Ma and Jose C. Principe},
  doi          = {10.1109/TNNLS.2019.2926466},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {1780-1793},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A taxonomy for neural memory networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020h). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(5), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.2987663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.2987663},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed training for multi-layer neural networks by
consensus. <em>TNNLS</em>, <em>31</em>(5), 1771–1778. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, there has been a growing interest in large-scale and privacy-concerned machine learning, especially in the situation where the data cannot be shared due to privacy protection or cannot be centralized due to computational limitations. Parallel computation has been proposed to circumvent these limitations, usually based on the master-slave and decentralized topologies, and the comparison study shows that a decentralized graph could avoid the possible communication jam on the central agent but incur extra communication cost. In this brief, a consensus algorithm is designed to allow all agents over the decentralized graph to converge to each other, and the distributed neural networks with enough consensus steps could have nearly the same performance as the centralized training model. Through the analysis of convergence, it is proved that all agents over an undirected graph could converge to the same optimal model even with only a single consensus step, and this can significantly reduce the communication cost. Simulation studies demonstrate that the proposed distributed training algorithm for multi-layer neural networks without data exchange could exhibit comparable or even better performance than the centralized training model.},
  archive      = {J_TNNLS},
  author       = {Bo Liu and Zhengtao Ding and Chen Lv},
  doi          = {10.1109/TNNLS.2019.2921926},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1771-1778},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed training for multi-layer neural networks by consensus},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Direct error-driven learning for deep neural networks with
applications to big data. <em>TNNLS</em>, <em>31</em>(5), 1763–1770. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, heterogeneity and noise in big data are shown to increase the generalization error for a traditional learning regime utilized for deep neural networks (deep NNs). To reduce this error, while overcoming the issue of vanishing gradients, a direct error-driven learning (EDL) scheme is proposed. First, to reduce the impact of heterogeneity and data noise, the concept of a neighborhood is introduced. Using this neighborhood, an approximation of generalization error is obtained and an overall error, comprised of learning and the approximate generalization errors, is defined. A novel NN weight-tuning law is obtained through a layer-wise performance measure enabling the direct use of overall error for learning. Additional constraints are introduced into the layer-wise performance measure to guide and improve the learning process in the presence of noisy dimensions. The proposed direct EDL scheme effectively addresses the issue of heterogeneity and noise while mitigating vanishing gradients and noisy dimensions. A comprehensive simulation study is presented where the proposed approach is shown to mitigate the vanishing gradient problem while improving generalization by 6\%.},
  archive      = {J_TNNLS},
  author       = {R. Krishnan and S. Jagannathan and V. A. Samaranayake},
  doi          = {10.1109/TNNLS.2019.2920964},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1763-1770},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Direct error-driven learning for deep neural networks with applications to big data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Finite-time consensus of second-order switched nonlinear
multi-agent systems. <em>TNNLS</em>, <em>31</em>(5), 1757–1762. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, the practical finite-time consensus (FTC) problem is investigated for the second-order heterogeneous switched nonlinear multi-agent systems (MASs), where the subsystems and the switching signal for each agent are different. Mainly due to that agents&#39; dynamics are switched and the unknown nonlinearities in the systems are more general, the practical FTC problem of the MASs is rather difficult to be solved by existing methods. As such, a new protocol design framework for the FTC problem is developed. Then, a novel adaptive protocol is proposed for the switched nonlinear MASs based on the developed design framework and the neural network method. The sufficient conditions for the practical FTC of nonlinear MASs under arbitrary switching are given. Finally, a numerical example is presented to demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Wencheng Zou and Peng Shi and Zhengrong Xiang and Yan Shi},
  doi          = {10.1109/TNNLS.2019.2920880},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1757-1762},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time consensus of second-order switched nonlinear multi-agent systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A semisupervised recurrent convolutional attention model for
human activity recognition. <em>TNNLS</em>, <em>31</em>(5), 1747–1756.
(<a href="https://doi.org/10.1109/TNNLS.2019.2927224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the success of deep learning methods in human activity recognition (HAR). The longstanding shortage of labeled activity data inherently calls for a plethora of semisupervised learning methods, and one of the most challenging and common issues with semisupervised learning is the imbalanced distribution of labeled data over classes. Although the problem has long existed in broad real-world HAR applications, it is rarely explored in the literature. In this paper, we propose a semisupervised deep model for imbalanced activity recognition from multimodal wearable sensory data. We aim to address not only the challenges of multimodal sensor data (e.g., interperson variability and interclass similarity) but also the limited labeled data and class-imbalance issues simultaneously. In particular, we propose a pattern-balanced semisupervised framework to extract and preserve diverse latent patterns of activities. Furthermore, we exploit the independence of multi-modalities of sensory data and attentively identify salient regions that are indicative of human activities from inputs by our recurrent convolutional attention networks. Our experimental results demonstrate that the proposed model achieves a competitive performance compared to a multitude of state-of-the-art methods, both semisupervised and supervised ones, with 10\% labeled training data. The results also show the robustness of our method over imbalanced, small training data sets.},
  archive      = {J_TNNLS},
  author       = {Kaixuan Chen and Lina Yao and Dalin Zhang and Xianzhi Wang and Xiaojun Chang and Feiping Nie},
  doi          = {10.1109/TNNLS.2019.2927224},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1747-1756},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A semisupervised recurrent convolutional attention model for human activity recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Disturbance observer-based neural network control of
cooperative multiple manipulators with input saturation. <em>TNNLS</em>,
<em>31</em>(5), 1735–1746. (<a
href="https://doi.org/10.1109/TNNLS.2019.2923241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the complex problems of internal forces and position control are studied simultaneously and a disturbance observer-based radial basis function neural network (RBFNN) control scheme is proposed to: 1) estimate the unknown parameters accurately; 2) approximate the disturbance experienced by the system due to input saturation; and 3) simultaneously improve the robustness of the system. More specifically, the proposed scheme utilizes disturbance observers, neural network (NN) collaborative control with an adaptive law, and full state feedback. Utilizing Lyapunov stability principles, it is shown that semiglobally uniformly bounded stability is guaranteed for all controlled signals of the closed-loop system. The effectiveness of the proposed controller as predicted by the theoretical analysis is verified by comparative experimental studies.},
  archive      = {J_TNNLS},
  author       = {Wei He and Yongkun Sun and Zichen Yan and Chenguang Yang and Zhijun Li and Okyay Kaynak},
  doi          = {10.1109/TNNLS.2019.2923241},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1735-1746},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disturbance observer-based neural network control of cooperative multiple manipulators with input saturation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative adaptive output regulation for lower triangular
nonlinear multi-agent systems subject to jointly connected switching
networks. <em>TNNLS</em>, <em>31</em>(5), 1724–1734. (<a
href="https://doi.org/10.1109/TNNLS.2019.2922174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cooperative global robust output regulation problem for multi-agent systems is a generalization of the leader-following consensus problem. The problem has been studied for various multi-agent systems over connected static networks and for some special classes of nonlinear multi-agent systems over jointly connected switching networks. In this paper, we further consider the same problem for a class of heterogeneous lower triangular nonlinear multi-agent systems over jointly connected switching networks. This class of systems is quite general in that it contains inverse dynamics, is of any order, and its subsystems can have different relative degrees. We will integrate the adaptive distributed observer and the distributed internal model approach to come up with a recursive approach to deal with our problem. We will also apply our approach to a leader-following consensus problem for a group of hyperchaotic Lorenz systems.},
  archive      = {J_TNNLS},
  author       = {Wei Liu and Jie Huang},
  doi          = {10.1109/TNNLS.2019.2922174},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1724-1734},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cooperative adaptive output regulation for lower triangular nonlinear multi-agent systems subject to jointly connected switching networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SVRG-MKL: A fast and scalable multiple kernel learning
solution for features combination in multi-class classification
problems. <em>TNNLS</em>, <em>31</em>(5), 1710–1723. (<a
href="https://doi.org/10.1109/TNNLS.2019.2922123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel strategy to combine a set of compact descriptors to leverage an associated recognition task. We formulate the problem from a multiple kernel learning (MKL) perspective and solve it following a stochastic variance reduced gradient (SVRG) approach to address its scalability, currently an open issue. MKL models are ideal candidates to jointly learn the optimal combination of features along with its associated predictor. However, they are unable to scale beyond a dozen thousand of samples due to high computational and memory requirements, which severely limits their applicability. We propose SVRG-MKL, an MKL solution with inherent scalability properties that can optimally combine multiple descriptors involving millions of samples. Our solution takes place directly in the primal to avoid Gram matrices computation and memory allocation, whereas the optimization is performed with a proposed algorithm of linear complexity and hence computationally efficient. Our proposition builds upon recent progress in SVRG with the distinction that each kernel is treated differently during optimization, which results in a faster convergence than applying off-the-shelf SVRG into MKL. Extensive experimental validation conducted on several benchmarking data sets confirms a higher accuracy and a significant speedup of our solution. Our technique can be extended to other MKL problems, including visual search and transfer learning, as well as other formulations, such as group-sensitive (GMKL) and localized MKL (LMKL) in convex settings.},
  archive      = {J_TNNLS},
  author       = {Mitchel Alioscha-Perez and Meshia Cédric Oveneke and Hichem Sahli},
  doi          = {10.1109/TNNLS.2019.2922123},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1710-1723},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SVRG-MKL: A fast and scalable multiple kernel learning solution for features combination in multi-class classification problems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy disaggregation via deep temporal dictionary learning.
<em>TNNLS</em>, <em>31</em>(5), 1696–1709. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel nonlinear dictionary learning (DL) model to address the energy disaggregation (ED) problem, i.e., decomposing the electricity signal of a home to its operating devices. First, ED is modeled as a new temporal DL problem where a set of dictionary atoms is learned to capture the most representative temporal features of electricity signals. The sparse codes corresponding to these atoms show the contribution of each device in the total electricity consumption. To learn powerful atoms, a novel deep temporal DL (DTDL) model is proposed that computes complex nonlinear dictionaries in the latent space of a long short-term memory autoencoder (LSTM-AE). While the LSTM-AE captures the deep temporal manifold of electricity signals, the DTDL model finds the most representative atoms inside this manifold. To simultaneously optimize the dictionary and the deep temporal manifold, a new optimization algorithm is proposed that alternates between finding the optimal LSTM-AE and the optimal dictionary. To the best of authors&#39; knowledge, DTDL is the only DL model that understands the deep temporal structures of the data. Experiments on the Reference ED Data Set show an outstanding performance compared with the recent state-of-the-art algorithms in terms of precision, recall, accuracy, and F-score.},
  archive      = {J_TNNLS},
  author       = {Mahdi Khodayar and Jianhui Wang and Zhaoyu Wang},
  doi          = {10.1109/TNNLS.2019.2921952},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1696-1709},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Energy disaggregation via deep temporal dictionary learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep class-wise hashing: Semantics-preserving hashing via
class-wise loss. <em>TNNLS</em>, <em>31</em>(5), 1681–1695. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep supervised hashing has emerged as an effective solution to large-scale semantic image retrieval problems in computer vision. Convolutional neural network-based hashing methods typically seek pairwise or triplet labels to conduct similarity-preserving learning. However, complex semantic concepts of visual contents are hard to capture by similar/dissimilar labels, which limits the retrieval performance. Generally, pairwise or triplet losses not only suffer from expensive training costs but also lack sufficient semantic information. In this paper, we propose a novel deep supervised hashing model to learn more compact class-level similarity-preserving binary codes. Our model is motivated by deep metric learning that directly takes semantic labels as supervised information in training and generates corresponding discriminant hashing code. Specifically, a novel cubic constraint loss function based on Gaussian distribution is proposed, which preserves semantic variations while penalizes the overlapping part of different classes in the embedding space. To address the discrete optimization problem introduced by binary codes, a two-step optimization strategy is proposed to provide efficient training and avoid the problem of gradient vanishing. Extensive experiments on five large-scale benchmark databases show that our model can achieve the state-of-the-art retrieval performance.},
  archive      = {J_TNNLS},
  author       = {Xuefei Zhe and Shifeng Chen and Hong Yan},
  doi          = {10.1109/TNNLS.2019.2921805},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1681-1695},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep class-wise hashing: Semantics-preserving hashing via class-wise loss},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient entropy-based causal discovery method for
linear structural equation models with IID noise variables.
<em>TNNLS</em>, <em>31</em>(5), 1667–1680. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of causal relationships from the observational data is an important task. To identify the unique causal structure belonging to a Markov equivalence class, a number of algorithms, such as the linear non-Gaussian acyclic model (LiNGAM), have been proposed. However, two challenges remain to be met: 1) these algorithms fail to work on the data which follow linear structural equation model with Gaussian noise and 2) they misjudge the causal direction when the data contain additional measurement errors. In this paper, we propose an entropy-based two-phase iterative algorithm for arbitrary distribution data with additional measurement errors under some mild assumptions. In the first phase of the algorithm, based on the property that entropy can measure the amount of information behind the data with arbitrary distribution, we design a general approach for the identification of exogenous variable on both Gaussian and non-Gaussian data, and we give the corresponding theoretical derivation. In the second phase, to eliminate the effects of measurement errors, we revise the value of the exogenous variable by removing its measurement error and further use the revised value to remove its effect on the remaining variables. Experimental results on real-world causal structures are presented to demonstrate the effectiveness and stability of our method. We also apply the proposed algorithm on the mobile-base-station data with measurement errors, and the results further prove the effectiveness of our algorithm.},
  archive      = {J_TNNLS},
  author       = {Feng Xie and Ruichu Cai and Yan Zeng and Jiantao Gao and Zhifeng Hao},
  doi          = {10.1109/TNNLS.2019.2921613},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1667-1680},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient entropy-based causal discovery method for linear structural equation models with IID noise variables},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge-driven deep unrolling for robust image layer
separation. <em>TNNLS</em>, <em>31</em>(5), 1653–1666. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image layer separation targets to decompose the observed image into two independent components in terms of different application demands. It is known that many vision and multimedia applications can be (re)formulated as a separation problem. Due to the fundamentally ill-posed natural of these separations, existing methods are inclined to investigate model priors on the separated components elaborately. Nevertheless, it is knotty to optimize the cost function with complicated model regularizations. Effectiveness is greatly conceded by the settled iteration mechanism, and the adaption cannot be guaranteed due to the poor data fitting. What is more, for a universal framework, the most taxing point is that one type of visual cue cannot be shared with different tasks. To partly overcome the weaknesses mentioned earlier, we delve into a generic optimization unrolling technique to incorporate deep architectures into iterations for adaptive image layer separation. First, we propose a general energy model with implicit priors, which is based on maximum a posterior, and employ the extensively accepted alternating direction method of multiplier to determine our elementary iteration mechanism. By unrolling with one general residual architecture prior and one task-specific prior, we attain a straightforward, flexible, and data-dependent image separation framework successfully. We apply our method to four different tasks, including single-image-rain streak removal, high-dynamic-range tone mapping, low-light image enhancement, and single-image reflection removal. Extensive experiments demonstrate that the proposed method is applicable to multiple tasks and outperforms the state of the arts by a large margin qualitatively and quantitatively.},
  archive      = {J_TNNLS},
  author       = {Risheng Liu and Zhiying Jiang and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TNNLS.2019.2921597},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1653-1666},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge-driven deep unrolling for robust image layer separation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cascade superpixel regularized gabor feature fusion for
hyperspectral image classification. <em>TNNLS</em>, <em>31</em>(5),
1638–1652. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3-D Gabor wavelet provides an effective way to obtain the spectral-spatial-fused features for hyperspectral image, which has shown advantageous performance for material classification and recognition. In this paper, instead of separately employing the Gabor magnitude and phase features, which, respectively, reflect the intensity and variation of surface materials in local area, a cascade superpixel regularized Gabor feature fusion (CSRGFF) approach has been proposed. First, the Gabor filters with particular orientation are utilized to obtain Gabor features (including magnitude and phase) from the original hyperspectral image. Second, a support vector machine (SVM)-based probability representation strategy is developed to fully exploit the decision information in SVM output, and the achieved confidence score can make the following fusion with Gabor phase more effective. Meanwhile, the quadrant bit coding and Hamming distance metric are applied to encode the Gabor phase features and measure sample similarity in sequence. Third, the carefully defined characteristics of two kinds of features are directly combined together without any weighting operation to describe the weight of samples belonging to each class. Finally, a series of superpixel graphs extracted from the raw hyperspectral image with different numbers of superpixels are employed to successively regularize the weighting cube from over-segmentation to under-segmentation, and the classification performance gradually improves with the decrease in the number of superpixels in the regularization procedure. Four widely used real hyperspectral images have been conducted, and the experimental results constantly demonstrate the superiority of our CSRGFF approach over several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Sen Jia and Zhijie Lin and Bin Deng and Jiasong Zhu and Qingquan Li},
  doi          = {10.1109/TNNLS.2019.2921564},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1638-1652},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cascade superpixel regularized gabor feature fusion for hyperspectral image classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimization of fraction function penalty in compressed
sensing. <em>TNNLS</em>, <em>31</em>(5), 1626–1637. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the minimization problem of a non-convex sparsity-promoting penalty function, i.e., fraction function, in compressed sensing. First, we discuss the equivalence of $\ell _{0}$ minimization and fraction function minimization. It is proved that the optimal solution to fraction function minimization solves $\ell _{0}$ minimization and the optimal solution to the regularization problem also solves fraction function minimization if the certain conditions are satisfied, which is similar to the regularization problem in a convex optimization theory. Second, we study the properties of the optimal solution to the regularization problem, including the first-order and second-order optimality conditions and the lower and upper bounds of the absolute value for its nonzero entries. Finally, we derive the closed-form representation of the optimal solution to the regularization problem and propose an iterative $FP$ thresholding algorithm to solve the regularization problem. We also provide a series of experiments to assess the performance of the $FP$ algorithm, and the experimental results show that the $FP$ algorithm performs well in sparse signal recovery with and without measurement noise.},
  archive      = {J_TNNLS},
  author       = {Haiyang Li and Qian Zhang and Angang Cui and Jigen Peng},
  doi          = {10.1109/TNNLS.2019.2921404},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1626-1637},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Minimization of fraction function penalty in compressed sensing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stabilization of mode-dependent impulsive hybrid systems
driven by DFA with mixed-mode effects. <em>TNNLS</em>, <em>31</em>(5),
1616–1625. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with mode-dependent impulsive hybrid systems driven by deterministic finite automaton (DFA) with mixed-mode effects. In the hybrid systems, a complex phenomenon called mixed mode, caused in time-varying delay switching systems, is considered explicitly. Furthermore, mode-dependent impulses, which can exist not only at the instants coinciding with mode switching but also at the instants when there is no system switching, are also taken into consideration. First, we establish a rigorous mathematical equation expression of this class of hybrid systems. Then, several criteria of stabilization of this class of hybrid systems are presented based on semi-tensor product (STP) techniques, multiple Lyapunov–Krasovskii functionals, as well as the average dwell time approach. Finally, an example is simulated to illustrate the effectiveness of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Junhui Zhang and Anni Li and Wei D. Lu and Jitao Sun},
  doi          = {10.1109/TNNLS.2019.2921020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1616-1625},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stabilization of mode-dependent impulsive hybrid systems driven by DFA with mixed-mode effects},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multidimensional gains for stochastic approximation.
<em>TNNLS</em>, <em>31</em>(5), 1602–1615. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with iterative Jacobian-based recursion technique for the root-finding problem of the vectorvalued function, whose evaluations are contaminated by noise. Instead of a scalar step size, we use an iterate-dependent matrix gain to effectively weigh the different elements associated with the noisy observations. The analytical development of the matrix gain is built on an iterative-dependent linear function interfered by additive zero-mean white noise, where the dimension of the function is M 1 and the dimension of the unknown variable is N 1. Necessary and sufficient conditions for M N algorithms are presented pertaining to algorithm stability and convergence of the estimate error covariance matrix. Two algorithms are proposed: one for the case where M N and the second one for the antithesis. The two algorithms assume full knowledge of the Jacobian. The recursive algorithms are proposed for generating the optimal iterative-dependent matrix gain. The proposed algorithms here aim for per-iteration minimization of the mean square estimate error. We show that the proposed algorithm satisfies the presented conditions for stability and convergence of the covariance. In addition, the convergence rate of the estimation error covariance is shown to be inversely proportional to the number of iterations. For the antithesis M &lt;; N, contraction of the error covariance is guaranteed. This underdetermined system of equations can be helpful in training neural networks. Numerical examples are presented to illustrate the performance capabilities of the proposed multidimensional gain while considering nonlinear functions.},
  archive      = {J_TNNLS},
  author       = {Samer S. Saab and Dong Shen},
  doi          = {10.1109/TNNLS.2019.2920930},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1602-1615},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multidimensional gains for stochastic approximation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Person reidentification via multi-feature fusion with
adaptive graph learning. <em>TNNLS</em>, <em>31</em>(5), 1592–1601. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of person reidentification (Re-ID) is to identify a given pedestrian from a network of nonoverlapping surveillance cameras. Most existing works follow the supervised learning paradigm which requires pairwise labeled training data for each pair of cameras. However, this limits their scalability to real-world applications where abundant unlabeled data are available. To address this issue, we propose a multi-feature fusion with adaptive graph learning model for unsupervised Re-ID. Our model aims to negotiate comprehensive assessment on the consistent graph structure of pedestrians with the help of special information of feature descriptors. Specifically, we incorporate multi-feature dictionary learning and adaptive multi-feature graph learning into a unified learning model such that the learned dictionaries are discriminative and the subsequent graph structure learning is accurate. An alternating optimization algorithm with proved convergence is developed to solve the final optimization objective. Extensive experiments on four benchmark data sets demonstrate the superiority and effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Runwu Zhou and Xiaojun Chang and Lei Shi and Yi-Dong Shen and Yi Yang and Feiping Nie},
  doi          = {10.1109/TNNLS.2019.2920905},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1592-1601},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Person reidentification via multi-feature fusion with adaptive graph learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fault diagnosis of complex processes using sparse kernel
local fisher discriminant analysis. <em>TNNLS</em>, <em>31</em>(5),
1581–1591. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an outstanding discriminant analysis technique, Fisher discriminant analysis (FDA) gained extensive attention in supervised dimensionality reduction and fault diagnosis fields. However, it typically ignores the multimodality within the measured data, which may cause infeasibility in practice. In addition, it generally incorporates all process variables without emphasizing the key faulty ones when modeling the complex process, thus leading to degraded fault classification capability and poor model interpretability. To ease the above two drawbacks of conventional FDA, this brief presents an advantageously sparse local FDA (SLFDA) model, it first preserves the within-class multimodality by introducing local weighting factors into scatter matrix. Then, the responsible faulty variables are identified automatically through the elastic net algorithm, and the current optimization problem is subsequently settled through the feasible gradient direction method. Since then, the local data structure characteristics are exploited from both the sample dimension and variable dimension so that the fault diagnosis performance and model interpretability are significantly enhanced. In addition, we naturally extend SLFDA model to nonlinear variant (i.e., sparse kernel local FDA) by the kernel trick, which is substantially more resistant to strong nonlinearity. The simulation studies on Tennessee Eastman (TE) benchmark process and real-world diesel engine working process both validate that the novel diagnosis strategy is more accurate and reliable than the existing state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Kai Zhong and Min Han and Tie Qiu and Bing Han},
  doi          = {10.1109/TNNLS.2019.2920903},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1581-1591},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fault diagnosis of complex processes using sparse kernel local fisher discriminant analysis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed synchronization control of nonaffine multiagent
systems with guaranteed performance. <em>TNNLS</em>, <em>31</em>(5),
1571–1580. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the synchronization control problem in the leader-follower format of a class of high-order nonaffine nonlinear multiagent systems under a directed communication protocol. A novel adaptive neural distributed synchronization scheme with guaranteed performance is proposed. The main contribution lies in the fact that both nonaffine agent dynamics, which basically makes most existing agent dynamics as special cases, and guaranteed synchronization performance are taken into account. The difficulty lies mainly in the nonaffine terms and coupling terms due to the interactions of agents. To overcome this challenge, an augmented quadratic Lyapunov function by incorporating the lower bounds of control gains is proposed. The problems resulting from the nonaffine dynamics and the coupling terms among agents are solved by incorporating the special property of radial basis function neural network into the derivative of the augmented quadratic Lyapunov function. The unknown nonaffine terms are addressed by using an indirected neural network approach. A nonlinear mapping is built to relate the local consensus error to a new one, which is subsequently stabilized via Lyapunov synthesis. As a result, the proposed approach can ensure the outputs of all follower agents to track the outputs of the leader, while the synchronization performance bounds can be quantified on both transient and steady-state stages. All other signals in the closed loop are ensured to be semiglobally, uniformly, and ultimately bounded. Finally, the effectiveness of the proposed controller is verified through a heterogeneous four-agent example.},
  archive      = {J_TNNLS},
  author       = {Wenchao Meng and Peter Xiaoping Liu and Qinmin Yang and Youxian Sun},
  doi          = {10.1109/TNNLS.2019.2920892},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1571-1580},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed synchronization control of nonaffine multiagent systems with guaranteed performance},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural network-based information transfer for dynamic
optimization. <em>TNNLS</em>, <em>31</em>(5), 1557–1570. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dynamic optimization problems (DOPs), as the environment changes through time, the optima also dynamically change. How to adapt to the dynamic environment and quickly find the optima in all environments is a challenging issue in solving DOPs. Usually, a new environment is strongly relevant to its previous environment. If we know how it changes from the previous environment to the new one, then we can transfer the information of the previous environment, e.g., past solutions, to get new promising information of the new environment, e.g., new high-quality solutions. Thus, in this paper, we propose a neural network (NN)-based information transfer method, named NNIT, to learn the transfer model of environment changes by NN and then use the learned model to reuse the past solutions. When the environment changes, NNIT first collects the solutions from both the previous environment and the new environment and then uses an NN to learn the transfer model from these solutions. After that, the NN is used to transfer the past solutions to new promising solutions for assisting the optimization in the new environment. The proposed NNIT can be incorporated into population-based evolutionary algorithms (EAs) to solve DOPs. Several typical state-of-the-art EAs for DOPs are selected for comprehensive study and evaluated using the widely used moving peaks benchmark. The experimental results show that the proposed NNIT is promising and can accelerate algorithm convergence.},
  archive      = {J_TNNLS},
  author       = {Xiao-Fang Liu and Zhi-Hui Zhan and Tian-Long Gu and Sam Kwong and Zhenyu Lu and Henry Been-Lirn Duh and Jun Zhang},
  doi          = {10.1109/TNNLS.2019.2920887},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1557-1570},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based information transfer for dynamic optimization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A training data set cleaning method by classification
ability ranking for the <span class="math inline"><em>k</em></span>
-nearest neighbor classifier. <em>TNNLS</em>, <em>31</em>(5), 1544–1556.
(<a href="https://doi.org/10.1109/TNNLS.2019.2920864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-nearest neighbor (KNN) rule is a successful technique in pattern classification due to its simplicity and effectiveness. As a supervised classifier, KNN classification performance usually suffers from low-quality samples in the training data set. Thus, training data set cleaning (TDC) methods are needed for enhancing the classification accuracy by cleaning out noisy, or even wrong, samples in the original training data set. In this paper, we propose a classification ability ranking (CAR)-based TDC method to improve the performance of a KNN classifier, namely CAR-based TDC method. The proposed classification ability function ranks a training sample in terms of its contribution to correctly classify other training samples as a KNN through the leave-one-out (LV1) strategy in the cleaning stage. The training sample that likely misclassifies the other samples during the KNN classifications according to the LV1 strategy is considered to have lower classification ability and will be cleaned out from the original training data set. Extensive experiments, based on ten real-world data sets, show that the proposed CAR-based TDC method can significantly reduce the classification error rates of KNN-based classifiers, while reducing computational complexity thanks to a smaller cleaned training data set.},
  archive      = {J_TNNLS},
  author       = {Yidi Wang and Zhibin Pan and Yiwei Pan},
  doi          = {10.1109/TNNLS.2019.2920864},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1544-1556},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A training data set cleaning method by classification ability ranking for the $k$ -nearest neighbor classifier},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral pansharpening with deep priors.
<em>TNNLS</em>, <em>31</em>(5), 1529–1543. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral (HS) image can describe subtle differences in the spectral signatures of materials, but it has low spatial resolution limited by the existing technical and budget constraints. In this paper, we propose a promising HS pansharpening method with deep priors (HPDP) to fuse a low-resolution (LR) HS image with a high-resolution (HR) panchromatic (PAN) image. Different from the existing methods, we redefine the spectral response function (SRF) based on the larger eigenvalue of structure tensor (ST) matrix for the first time that is more in line with the characteristics of HS imaging. Then, we introduce HFNet to capture deep residual mapping of high frequency across the upsampled HS image and the PAN image in a band-by-band manner. Specifically, the learned residual mapping of high frequency is injected into the structural transformed HS images, which are the extracted deep priors served as additional constraint in a Sylvester equation to estimate the final HR HS image. Comparative analyses validate that the proposed HPDP method presents the superior pansharpening performance by ensuring higher quality both in spatial and spectral domains for all types of data sets. In addition, the HFNet is trained in the high-frequency domain based on multispectral (MS) images, which overcomes the sensitivity of deep neural network (DNN) to data sets acquired by different sensors and the difficulty of insufficient training samples for HS pansharpening.},
  archive      = {J_TNNLS},
  author       = {Weiying Xie and Jie Lei and Yuhang Cui and Yunsong Li and Qian Du},
  doi          = {10.1109/TNNLS.2019.2920857},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1529-1543},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral pansharpening with deep priors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep inception-residual laplacian pyramid networks for
accurate single-image super-resolution. <em>TNNLS</em>, <em>31</em>(5),
1514–1528. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With exploiting contextual information over large image regions in an efficient way, the deep convolutional neural network has shown an impressive performance for single-image super-resolution (SR). In this paper, we propose a new deep convolutional network by cascading multiple well-designed inception-residual blocks within the deep Laplacian pyramid framework to progressively restore the missing high-frequency details in the low-resolution images. By optimizing our network structure, the trainable depth of our proposed network gains a significant improvement, which in turn improves super-resolving accuracy. However, the saturation and degradation of training accuracy remains a critical problem. With regard to this, we propose an effective two-stage training strategy, in which we first use the images downsampled from the ground-truth high-resolution (HR) images to pretrain the inception-residual blocks on each pyramid level with an extremely high learning rate enabled by gradient clipping, and then the original ground-truth HR images are used to fine-tune all the pretrained inception-residual blocks for obtaining our final SR models. Furthermore, we present a new loss function operating in both image space and local rank space to optimize our network for exploiting the contextual information among different output components. Extensive experiments on benchmark data sets validate that the proposed method outperforms the existing state-of-the-art SR methods in terms of the objective evaluation as well as the visual quality.},
  archive      = {J_TNNLS},
  author       = {Yongliang Tang and Weiguo Gong and Xi Chen and Weihong Li},
  doi          = {10.1109/TNNLS.2019.2920852},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1514-1528},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep inception-residual laplacian pyramid networks for accurate single-image super-resolution},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New criteria for stability of neutral-type neural networks
with multiple time delays. <em>TNNLS</em>, <em>31</em>(5), 1504–1513.
(<a href="https://doi.org/10.1109/TNNLS.2019.2920672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work studies stability problems for more general models of neutral-type neural systems where both neuron states and the time derivative of neuron states involve multiple delays. Some new sufficient criterion is presented, which guarantee the existence, uniqueness, and global asymptotic stability of equilibrium points of the considered neural network model. These obtained stability conditions, which can be applied to some larger classes of general neural network models, are based on the analysis of a new and improved suitable Lyapunov functional. The proposed conditions are independent of time delay parameters and can be easily justified by examining some certain relationships among the relevant neural network parameters. This paper also shows that the obtained stability criteria can be considered as the generalization of some previously reported corresponding stability conditions for neural networks, including multiple time delay parameters.},
  archive      = {J_TNNLS},
  author       = {Sabri Arik},
  doi          = {10.1109/TNNLS.2019.2920672},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1504-1513},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New criteria for stability of neutral-type neural networks with multiple time delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of domain of attraction for aperiodic
sampled-data switched delayed neural networks subject to actuator
saturation. <em>TNNLS</em>, <em>31</em>(5), 1489–1503. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, for the case of the asynchronous switching caused by that subsystem&#39;s switching occuring during a sampling interval, the domain of attraction estimation problem is investigated for aperiodic sampled-data switched delayed neural networks (ASDSDNNs) subject to actuator saturation. A parameters-dependent time-scheduled Lyapunov functional consisting of a novel looped-functional is constructed using segmentation technology and linear interpolation. By employing this novel functional and using an average dwell time (ADT) approach, exponential stability criteria are proposed for polytopic uncertain ASDSDNNs subject to actuator saturation. And a relationship between ADT and sampling period is revealed for ASDSDNNs. As a corollary, exponential stability criteria are proposed for nominal ASDSDNNs subject to actuator saturation. Furthermore, by describing the domain of attraction as a time-varying ellipsoid determined by the time-scheduled Lyapunov matrix, the proposed theoretical conditions are transformed into a linear matrix inequality (LMI)-based multi-objective optimization problem. The dynamic estimates of the domain of attraction for ASDSDNNs are solved. Numerical simulation examples are provided to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Guoliang Chen and Jian Sun and Jianwei Xia},
  doi          = {10.1109/TNNLS.2019.2920665},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1489-1503},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Estimation of domain of attraction for aperiodic sampled-data switched delayed neural networks subject to actuator saturation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel neural network for solving nonsmooth nonconvex
optimization problems. <em>TNNLS</em>, <em>31</em>(5), 1475–1488. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel recurrent neural network (RNN) is presented to deal with a kind of nonsmooth nonconvex optimization problem in which the objective function may be nonsmooth and nonconvex, and the constraints include linear equations and convex inequations. Under certain suitable assumptions, from an arbitrary initial state, each solution to the proposed RNN exists globally and is bounded, and it enters the feasible region within a limited time. Moreover, the solution to the RNN with an arbitrary initial state can converge to the critical point set of the optimization problem. In particular, the RNN does not need the following: 1) abounded feasible region; 2) the computation of an exact penalty parameter; or 3) the initial state being chosen from a given bounded set. Numerical experiments are provided to show the effectiveness and advantages of the RNN.},
  archive      = {J_TNNLS},
  author       = {Xin Yu and Lingzhen Wu and Chenhua Xu and Yue Hu and Chong Ma},
  doi          = {10.1109/TNNLS.2019.2920408},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1475-1488},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel neural network for solving nonsmooth nonconvex optimization problems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skip-connected covariance network for remote sensing scene
classification. <em>TNNLS</em>, <em>31</em>(5), 1461–1474. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel end-to-end learning model, called skip-connected covariance (SCCov) network, for remote sensing scene classification (RSSC). The innovative contribution of this paper is to embed two novel modules into the traditional convolutional neural network (CNN) model, i.e., skip connections and covariance pooling. The advantages of newly developed SCCov are twofold. First, by means of the skip connections, the multi-resolution feature maps produced by the CNN are combined together, which provides important benefits to address the presence of large-scale variance in RSSC data sets. Second, by using covariance pooling, we can fully exploit the second-order information contained in such multi-resolution feature maps. This allows the CNN to achieve more representative feature learning when dealing with RSSC problems. Experimental results, conducted using three large-scale benchmark data sets, demonstrate that our newly proposed SCCov network exhibits very competitive or superior classification performance when compared with the current state-of-the-art RSSC techniques, using a much lower amount of parameters. Specifically, our SCCov only needs 10\% of the parameters used by its counterparts.},
  archive      = {J_TNNLS},
  author       = {Nanjun He and Leyuan Fang and Shutao Li and Javier Plaza and Antonio Plaza},
  doi          = {10.1109/TNNLS.2019.2920374},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1461-1474},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Skip-connected covariance network for remote sensing scene classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite-horizon <span
class="math inline"><em>H</em><sub>∞</sub></span> state estimation for
periodic neural networks over fading channels. <em>TNNLS</em>,
<em>31</em>(5), 1450–1460. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finite-horizon H ∞ state estimator design for periodic neural networks over multiple fading channels is studied in this paper. To characterize the measurement signals transmitted through different channels experiencing channel fading, a multiple fading channels model is considered. For investigating the situation of correlated fading channels, a set of correlated random variables is introduced. Specifically, the channel coefficients are described by white noise processes and are assumed to be correlated. Two sufficient criteria are provided, by utilizing a stochastic analysis approach, to guarantee that the estimation error system is stochastically stable and achieves the prescribed H ∞ performance. Then, the parameters of the estimator are derived by solving recursive linear matrix inequalities. Finally, some simulation results are shown to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Xiao-Meng Li and Bin Zhang and Panshuo Li and Qi Zhou and Renquan Lu},
  doi          = {10.1109/TNNLS.2019.2920368},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1450-1460},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-horizon $H_{\infty}$ state estimation for periodic neural networks over fading channels},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal deep network embedding with integrated structure
and attribute information. <em>TNNLS</em>, <em>31</em>(5), 1437–1449.
(<a href="https://doi.org/10.1109/TNNLS.2019.2920267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding is the process of learning low-dimensional representations for nodes in a network while preserving node features. Existing studies only leverage network structure information and emphasize the preservation of structural features. However, nodes in real-world networks often have a rich set of attributes providing extra semantic information. It has been demonstrated that both structural and attribute features are important for network analysis tasks. To preserve both features, we investigate the problem of integrating structure and attribute information to perform network embedding and propose a multimodal deep network embedding (MDNE) method. MDNE captures the non-linear network structures and the complex interactions among structures and attributes using a deep model consisting of multiple layers of non-linear functions. Since structures and attributes are two different types of information, a multimodal learning method is adopted to pre-process them and help the model to better capture the correlations between node structure and attribute information. We define the loss function employing structural and attribute proximities to preserve the respective features, and the representations are obtained by minimizing the loss function. Results of extensive experiments on four real-world data sets show that the proposed method performs significantly better than baselines on a variety of tasks, which demonstrates the effectiveness and generality of our method.},
  archive      = {J_TNNLS},
  author       = {Conghui Zheng and Li Pan and Peng Wu},
  doi          = {10.1109/TNNLS.2019.2920267},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1437-1449},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multimodal deep network embedding with integrated structure and attribute information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online incremental classification resonance network and its
application to human–robot interaction. <em>TNNLS</em>, <em>31</em>(5),
1426–1436. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human-robot interaction (HRI), classification is one of the most important problems, and it is essential particularly when the robot recognizes the surroundings and chooses a reaction based on a certain situation. Each interaction is different since new people appear or the environment changes, and the robot should be able to adapt to different situations during a brief interaction. Thus, it is imperative that the classification is performed incrementally in real time. In this sense, we propose an online incremental classification resonance network (OICRN) that enables incremental class learning in multi-class classification with high performance online. In OICRN, a scale-preserving projection process is introduced to use the raw input vectors online without a normalization process in advance. The integrated network of the convolutional neural network (CNN) for feature extraction and the OICRN for classification is applied to a robotic system that learns human identities through HRIs. To demonstrate the effectiveness of our network, experiments are carried out on benchmark data sets and on a humanoid robot, Mybot, developed in the Robot Intelligence Technology Laboratory, KAIST.},
  archive      = {J_TNNLS},
  author       = {Ju-Youn Park and Jong-Hwan Kim},
  doi          = {10.1109/TNNLS.2019.2920158},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1426-1436},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online incremental classification resonance network and its application to Human–Robot interaction},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020i). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(4), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.2982920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.2982920},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual adversarial autoencoders for clustering.
<em>TNNLS</em>, <em>31</em>(4), 1417–1424. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a powerful approach for exploratory data analysis, unsupervised clustering is a fundamental task in computer vision and pattern recognition. Many clustering algorithms have been developed, but most of them perform unsatisfactorily on the data with complex structures. Recently, adversarial autoencoder (AE) (AAE) shows effectiveness on tackling such data by combining AE and adversarial training, but it cannot effectively extract classification information from the unlabeled data. In this brief, we propose dual AAE (Dual-AAE) which simultaneously maximizes the likelihood function and mutual information between observed examples and a subset of latent variables. By performing variational inference on the objective function of Dual-AAE, we derive a new reconstruction loss which can be optimized by training a pair of AEs. Moreover, to avoid mode collapse, we introduce the clustering regularization term for the category variable. Experiments on four benchmarks show that Dual-AAE achieves superior performance over state-of-the-art clustering methods. In addition, by adding a reject option, the clustering accuracy of Dual-AAE can reach that of supervised CNN algorithms. Dual-AAE can also be used for disentangling style and content of images without using supervised information.},
  archive      = {J_TNNLS},
  author       = {Pengfei Ge and Chuan-Xian Ren and Dao-Qing Dai and Jiashi Feng and Shuicheng Yan},
  doi          = {10.1109/TNNLS.2019.2919948},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1417-1424},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual adversarial autoencoders for clustering},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Riemannian curvature of deep neural networks.
<em>TNNLS</em>, <em>31</em>(4), 1410–1416. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze deep neural networks using the theory of Riemannian geometry and curvature. The objective is to gain insight into how Riemannian geometry can characterize and predict the trained behavior of neural networks. We define a method for calculating Riemann and Ricci curvature tensors, and Ricci scalar curvature values for a trained neural net, in such a way that the output classifier softmax values are related to the input transformations, through the curvature equations. We also measure these curvature tensors experimentally for different networks which are pretrained with stochastic gradient descent and offer a way of visualizing and understanding the measurements to gain insight into the effect curvature has on behavior the neural networks locally, and possibly predict their behavior for different transformations of the test data. We also analyze the effect of variation in depth of the neural networks as well as how it behaves for different choices of data set.},
  archive      = {J_TNNLS},
  author       = {Piyush Kaul and Brejesh Lall},
  doi          = {10.1109/TNNLS.2019.2919705},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1410-1416},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Riemannian curvature of deep neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive decentralized neural network tracking control for
uncertain interconnected nonlinear systems with input quantization and
time delay. <em>TNNLS</em>, <em>31</em>(4), 1401–1409. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the problem of adaptive decentralized tracking control for a class of interconnected nonlinear systems with input quantization, unknown function, and time-delay, where the time-delay and interconnection terms are supposed to be bounded by some completely unknown functions. An adaptive decentralized tracking controller is constructed via the backstepping method and neural network technique, where a sliding-mode differentiator is presented to estimate the derivative of the virtual control law and reduce the complexity of the control scheme. On the basis of Lyapunov analysis scheme and graph theory, all the signals of the closed-loop system are uniformly ultimately bounded. Finally, an application example of an inverted pendulum system is given to demonstrate the effectiveness of the developed methods.},
  archive      = {J_TNNLS},
  author       = {Haibin Sun and Linlin Hou and Guangdeng Zong and Xinghuo Yu},
  doi          = {10.1109/TNNLS.2019.2919697},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1401-1409},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive decentralized neural network tracking control for uncertain interconnected nonlinear systems with input quantization and time delay},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid classifier ensemble for imbalanced data.
<em>TNNLS</em>, <em>31</em>(4), 1387–1400. (<a
href="https://doi.org/10.1109/TNNLS.2019.2920246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class imbalance problem has become a leading challenge. Although conventional imbalance learning methods are proposed to tackle this problem, they have some limitations: 1) undersampling methods suffer from losing important information and 2) cost-sensitive methods are sensitive to outliers and noise. To address these issues, we propose a hybrid optimal ensemble classifier framework that combines density-based undersampling and cost-effective methods through exploring state-of-the-art solutions using multi-objective optimization algorithm. Specifically, we first develop a density-based undersampling method to select informative samples from the original training data with probability-based data transformation, which enables to obtain multiple subsets following a balanced distribution across classes. Second, we exploit the cost-sensitive classification method to address the incompleteness of information problem via modifying weights of misclassified minority samples rather than the majority ones. Finally, we introduce a multi-objective optimization procedure and utilize connections between samples to self-modify the classification result using an ensemble classifier framework. Extensive comparative experiments conducted on real-world data sets demonstrate that our method outperforms the majority of imbalance and ensemble classification approaches.},
  archive      = {J_TNNLS},
  author       = {Kaixiang Yang and Zhiwen Yu and Xin Wen and Wenming Cao and C. L. Philip Chen and Hau-San Wong and Jane You},
  doi          = {10.1109/TNNLS.2019.2920246},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1387-1400},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid classifier ensemble for imbalanced data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Composite neural learning-based nonsingular terminal sliding
mode control of MEMS gyroscopes. <em>TNNLS</em>, <em>31</em>(4),
1375–1386. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient driving control of MEMS gyroscopes is an attractive way to improve the precision without hardware redesign. This paper investigates the sliding mode control (SMC) for the dynamics of MEMS gyroscopes using neural networks (NNs). Considering the existence of the dynamics uncertainty, the composite neural learning is constructed to obtain higher tracking precision using the serial–parallel estimation model (SPEM). Furthermore, the nonsingular terminal SMC (NTSMC) is proposed to achieve finite-time convergence. To obtain the prescribed performance, a time-varying barrier Lyapunov function (BLF) is introduced to the control scheme. Through simulation tests, it is observed that under the BLF-based NTSMC with composite learning design, the tracking precision of MEMS gyroscopes is highly improved.},
  archive      = {J_TNNLS},
  author       = {Bin Xu and Rui Zhang and Shuai Li and Wei He and Zhongke Shi},
  doi          = {10.1109/TNNLS.2019.2919931},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1375-1386},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composite neural learning-based nonsingular terminal sliding mode control of MEMS gyroscopes},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolving local plasticity rules for synergistic learning in
echo state networks. <em>TNNLS</em>, <em>31</em>(4), 1363–1374. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing synaptic plasticity rules for optimizing the connections between neurons within the reservoir of echo state networks (ESNs) remain to be global in that the same type of plasticity rule with the same parameters is applied to all neurons. However, this is biologically implausible and practically inflexible for learning the structures in the input signals, thereby limiting the learning performance of ESNs. In this paper, we propose to use local plasticity rules that allow different neurons to use different types of plasticity rules and different parameters, which are achieved by optimizing the parameters of the local plasticity rules using the evolution strategy (ES) with covariance matrix adaptation (CMA-ES). We show that evolving neural plasticity will result in a synergistic learning of different plasticity rules, which plays an important role in improving the learning performance. Meanwhile, we show that the local plasticity rules can effectively alleviate synaptic interferences in learning the structure in sensory inputs. The proposed local plasticity rules are compared with a number of the state-of-the-art ESN models and the canonical ESN using a global plasticity rule on a set of widely used prediction and classification benchmark problems to demonstrate its competitive learning performance.},
  archive      = {J_TNNLS},
  author       = {Xinjie Wang and Yaochu Jin and Kuangrong Hao},
  doi          = {10.1109/TNNLS.2019.2919903},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1363-1374},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Evolving local plasticity rules for synergistic learning in echo state networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple kernel clustering with neighbor-kernel subspace
segmentation. <em>TNNLS</em>, <em>31</em>(4), 1351–1362. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel clustering (MKC) has been intensively studied during the last few decades. Even though they demonstrate promising clustering performance in various applications, existing MKC algorithms do not sufficiently consider the intrinsic neighborhood structure among base kernels, which could adversely affect the clustering performance. In this paper, we propose a simple yet effective neighbor-kernel-based MKC algorithm to address this issue. Specifically, we first define a neighbor kernel, which can be utilized to preserve the block diagonal structure and strengthen the robustness against noise and outliers among base kernels. After that, we linearly combine these base neighbor kernels to extract a consensus affinity matrix through an exact-rank-constrained subspace segmentation. The naturally possessed block diagonal structure of neighbor kernels better serves the subsequent subspace segmentation, and in turn, the extracted shared structure is further refined through subspace segmentation based on the combined neighbor kernels. In this manner, the above two learning processes can be seamlessly coupled and negotiate with each other to achieve better clustering. Furthermore, we carefully design an efficient iterative optimization algorithm with proven convergence to address the resultant optimization problem. As a by-product, we reveal an interesting insight into the exact-rank constraint in ridge regression by careful theoretical analysis: it back-projects the solution of the unconstrained counterpart to its principal components. Comprehensive experiments have been conducted on several benchmark data sets, and the results demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Sihang Zhou and Xinwang Liu and Miaomiao Li and En Zhu and Li Liu and Changwang Zhang and Jianping Yin},
  doi          = {10.1109/TNNLS.2019.2919900},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1351-1362},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiple kernel clustering with neighbor-kernel subspace segmentation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multilinear multitask learning by rank-product
regularization. <em>TNNLS</em>, <em>31</em>(4), 1336–1350. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilinear multitask learning (MLMTL) considers an MTL problem in which tasks are arranged by multiple indices. By exploiting the higher order correlations among the tasks, MLMTL is expected to improve the performance of traditional MTL, which only considers the first-order correlation across all tasks, e.g., low-rank structure of the coefficient matrix. The key to MLMTL is designing a rational regularization term to represent the latent correlation structure underlying the coefficient tensor instead of matrix. In this paper, we propose a new MLMTL model by employing the rank-product regularization term in the objective, which on one hand can automatically rectify the weights along all its tensor modes and on the other hand have an explicit physical meaning. By using this regularization, the intrinsic high-order correlations among tasks can be more precisely described, and thus, the overall performance of all tasks can be improved. To solve the resulted optimization model, we design an efficient algorithm by applying the alternating direction method of multipliers (ADMM). We also analyze the convergence and show that the proposed algorithm, with certain restriction, is asymptotically regular. Experiments on both synthetic and real data sets substantiate the superiority of the proposed method beyond the existing MLMTL methods in terms of accuracy and efficiency.},
  archive      = {J_TNNLS},
  author       = {Qian Zhao and Xiangyu Rui and Zhi Han and Deyu Meng},
  doi          = {10.1109/TNNLS.2019.2919774},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1336-1350},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilinear multitask learning by rank-product regularization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Redundancy and attention in convolutional LSTM for gesture
recognition. <em>TNNLS</em>, <em>31</em>(4), 1323–1335. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional long short-term memory (ConvLSTM) networks have been widely used for action/gesture recognition, and different attention mechanisms have also been embedded into ConvLSTM networks. This paper explores the redundancy of spatial convolutions and the effects of the attention mechanism in ConvLSTM, based on our previous gesture recognition architectures that combine the 3-D convolutional neural network (CNN) and ConvLSTM. Depthwise separable, group, and shuffle convolutions are used to replace the convolutional structures in ConvLSTM for the redundancy analysis. In addition, four ConvLSTM variants are derived for attention analysis: 1) by removing the convolutional structures of the three gates in ConvLSTM; 2) by applying the attention mechanism on the ConvLSTM input; and 3) by reconstructing the input and 4) output gates with the modified channelwise attention mechanism. Evaluation results demonstrate that the spatial convolutions in the three gates scarcely contribute to the spatiotemporal feature fusion and that the attention mechanisms embedded into the input and output gates cannot improve the feature fusion. In other words, ConvLSTM mainly contributes to the temporal fusion along with the recurrent steps to learn long-term spatiotemporal features when taking spatial or spatiotemporal features as input. A new LSTM variant is derived on this basis in which the convolutional structures are embedded only into the input-to-state transition of LSTM. The code of the LSTM variants is publicly available. 11 https://github.com/GuangmingZhu/ConvLSTMForGR.},
  archive      = {J_TNNLS},
  author       = {Guangming Zhu and Liang Zhang and Lu Yang and Lin Mei and Syed Afaq Ali Shah and Mohammed Bennamoun and Peiyi Shen},
  doi          = {10.1109/TNNLS.2019.2919764},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1323-1335},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Redundancy and attention in convolutional LSTM for gesture recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective evolutionary federated learning.
<em>TNNLS</em>, <em>31</em>(4), 1310–1322. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is an emerging technique used to prevent the leakage of private information. Unlike centralized learning that needs to collect data from users and store them collectively on a cloud server, federated learning makes it possible to learn a global model while the data are distributed on the users&#39; devices. However, compared with the traditional centralized approach, the federated setting consumes considerable communication resources of the clients, which is indispensable for updating global models and prevents this technique from being widely used. In this paper, we aim to optimize the structure of the neural network models in federated learning using a multi-objective evolutionary algorithm to simultaneously minimize the communication costs and the global model test errors. A scalable method for encoding network connectivity is adapted to federated learning to enhance the efficiency in evolving deep neural networks. Experimental results on both multilayer perceptrons and convolutional neural networks indicate that the proposed optimization method is able to find optimized neural network models that can not only significantly reduce communication costs but also improve the learning performance of federated learning compared with the standard fully connected neural networks.},
  archive      = {J_TNNLS},
  author       = {Hangyu Zhu and Yaochu Jin},
  doi          = {10.1109/TNNLS.2019.2919699},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1310-1322},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-objective evolutionary federated learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive global sliding-mode control for dynamic systems
using double hidden layer recurrent neural network structure.
<em>TNNLS</em>, <em>31</em>(4), 1297–1309. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a full-regulated neural network (NN) with a double hidden layer recurrent neural network (DHLRNN) structure is designed, and an adaptive global sliding-mode controller based on the DHLRNN is proposed for a class of dynamic systems. Theoretical guidance and adaptive adjustment mechanism are established to set up the base width and central vector of the Gaussian function in the DHLRNN structure, where six sets of parameters can be adaptively stabilized to their best values according to different inputs. The new DHLRNN can improve the accuracy and generalization ability of the network, reduce the number of network weights, and accelerate the network training speed due to the strong fitting and presentation ability of two-layer activation functions compared with a general NN with a single hidden layer. Since the neurons of input layer can receive signals which come back from the neurons of output layer in the output feedback neural structure, it can possess associative memory and rapid system convergence, achieving better approximation and superior dynamic capability. Simulation and experiment on an active power filter are carried out to indicate the excellent static and dynamic performances of the proposed DHLRNN-based adaptive global sliding-mode controller, verifying its best approximation performance and the most stable internal state compared with other schemes.},
  archive      = {J_TNNLS},
  author       = {Yundi Chu and Juntao Fei and Shixi Hou},
  doi          = {10.1109/TNNLS.2019.2919676},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1297-1309},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive global sliding-mode control for dynamic systems using double hidden layer recurrent neural network structure},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training spiking neural networks for cognitive tasks: A
versatile framework compatible with various temporal codes.
<em>TNNLS</em>, <em>31</em>(4), 1285–1296. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated the effectiveness of supervised learning in spiking neural networks (SNNs). A trainable SNN provides a valuable tool not only for engineering applications but also for theoretical neuroscience studies. Here, we propose a modified SpikeProp learning algorithm, which ensures better learning stability for SNNs and provides more diverse network structures and coding schemes. Specifically, we designed a spike gradient threshold rule to solve the well-known gradient exploding problem in SNN training. In addition, regulation rules on firing rates and connection weights are proposed to control the network activity during training. Based on these rules, biologically realistic features such as lateral connections, complex synaptic dynamics, and sparse activities are included in the network to facilitate neural computation. We demonstrate the versatility of this framework by implementing three well-known temporal codes for different types of cognitive tasks, namely, handwritten digit recognition, spatial coordinate transformation, and motor sequence generation. Several important features observed in experimental studies, such as selective activity, excitatory-inhibitory balance, and weak pairwise correlation, emerged in the trained model. This agreement between experimental and computational results further confirmed the importance of these features in neural function. This work provides a new framework, in which various neural behaviors can be modeled and the underlying computational mechanisms can be studied.},
  archive      = {J_TNNLS},
  author       = {Chaofei Hong and Xile Wei and Jiang Wang and Bin Deng and Haitao Yu and Yanqiu Che},
  doi          = {10.1109/TNNLS.2019.2919662},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1285-1296},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Training spiking neural networks for cognitive tasks: A versatile framework compatible with various temporal codes},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-triggered neural control of nonlinear systems with
rate-dependent hysteresis input based on a new filter. <em>TNNLS</em>,
<em>31</em>(4), 1270–1284. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In controlling nonlinear uncertain systems, compensating for rate-dependent hysteresis nonlinearity is an important, yet challenging problem in adaptive control. In fact, it can be illustrated through simulation examples that instability is observed when existing control methods in canceling hysteresis nonlinearities are applied to the networked control systems (NCSs). One control difficulty that obstructs these methods is the design conflict between the quantized networked control signal and the rate-dependent hysteresis characteristics. So far, there is still no solution to this problem. In this paper, we consider the event-triggered control for NCSs subject to actuator rate-dependent hysteresis and failures. A new second-order filter is proposed to overcome the design conflict and used for control design. With the incorporation of the filter, a novel adaptive control strategy is developed from a neural network technique and a modified backstepping recursive design. It is proved that all the control signals are semiglobally uniformly ultimately bounded and the tracking error will converge to a tunable residual around zero.},
  archive      = {J_TNNLS},
  author       = {Kaixin Lu and Zhi Liu and C. L. Philip Chen and Yun Zhang},
  doi          = {10.1109/TNNLS.2019.2919641},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1270-1284},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered neural control of nonlinear systems with rate-dependent hysteresis input based on a new filter},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Online model-free n-step HDP with stability analysis.
<em>TNNLS</em>, <em>31</em>(4), 1255–1269. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of a powerful temporal-difference (TD) with λ [TD(λ)] learning method, this paper presents a novel n-step adaptive dynamic programming (ADP) architecture that combines TD(λ) with regular TD learning for solving optimal control problems with reduced iterations. In contrast with a backward view learning of TD(λ) that is required an extra parameter named eligibility traces to update at the end of each episode (offline training), the new design in this paper has forward view learning, which is updated at each time step (online training) without needing the eligibility trace parameter in various applications without mathematical models. Therefore, the new design is called the online model-free n-step action-dependent (AD) heuristic dynamic programming [NSHDP(λ)]. NSHDP(λ) has three neural networks: the critic network (CN) with regular one-step TD [TD(0)], the CN with n-step TD learning [or TD(λ)], and the actor network (AN). Because the forward view learning does not require any extra eligibility traces associated with each state, the NSHDP(λ) architecture has low computational costs and is memory efficient. Furthermore, the stability is proven for NSHDP(λ) under certain conditions by using Lyapunov analysis to obtain the uniformly ultimately bounded (UUB) property. We compare the results with the performance of HDP and traditional action-dependent HDP(λ) [ADHDP(λ)] with different λ values. Moreover, a complex nonlinear system and 2-D maze problem are two simulation benchmarks in this paper, and the third one is an inverted pendulum simulation benchmark, which is presented in the supplemental material part of this paper. NSHDP(λ) performance is examined and compared with other ADP methods.},
  archive      = {J_TNNLS},
  author       = {Seaar Al-Dabooni and Donald C. Wunsch},
  doi          = {10.1109/TNNLS.2019.2919614},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1255-1269},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online model-free n-step HDP with stability analysis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Completely automated CNN architecture design based on
blocks. <em>TNNLS</em>, <em>31</em>(4), 1242–1254. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of convolutional neural networks (CNNs) highly relies on their architectures. In order to design a CNN with promising performance, extensive expertise in both CNNs and the investigated problem domain is required, which is not necessarily available to every interested user. To address this problem, we propose to automatically evolve CNN architectures by using a genetic algorithm (GA) based on ResNet and DenseNet blocks. The proposed algorithm is completely automatic in designing CNN architectures. In particular, neither preprocessing before it starts nor postprocessing in terms of CNNs is needed. Furthermore, the proposed algorithm does not require users with domain knowledge on CNNs, the investigated problem, or even GAs. The proposed algorithm is evaluated on the CIFAR10 and CIFAR100 benchmark data sets against 18 state-of-the-art peer competitors. Experimental results show that the proposed algorithm outperforms the state-of-the-art CNNs hand-crafted and the CNNs designed by automatic peer competitors in terms of the classification performance and achieves a competitive classification accuracy against semiautomatic peer competitors. In addition, the proposed algorithm consumes much less computational resource than most peer competitors in finding the best CNN architectures.},
  archive      = {J_TNNLS},
  author       = {Yanan Sun and Bing Xue and Mengjie Zhang and Gary G. Yen},
  doi          = {10.1109/TNNLS.2019.2919608},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1242-1254},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Completely automated CNN architecture design based on blocks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-based dissipative analysis for discrete time-delay
singular jump neural networks. <em>TNNLS</em>, <em>31</em>(4),
1232–1241. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the event-triggered dissipative filtering issue for discrete-time singular neural networks with time-varying delays and Markovian jump parameters. Via event-triggered communication technique, a singular jump neural network (SJNN) model of network-induced delays is first given, and sufficient criteria are then provided to guarantee that the resulting augmented SJNN is stochastically admissible and strictly stochastically dissipative (SASSD) with respect to (X ι , Y ι , Z ι , δ) by using slack matrix scheme. Furthermore, employing filter equivalent technique, codesigned filter gains, and event-triggered matrices are derived to make sure that the augmented SJNN model is SASSD with respect to (X ι , Y ι , Z ι , δ). An example is also given to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yingqi Zhang and Peng Shi and Ramesh K. Agarwal and Yan Shi},
  doi          = {10.1109/TNNLS.2019.2919585},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1232-1241},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based dissipative analysis for discrete time-delay singular jump neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Direct adaptive preassigned finite-time control with
time-delay and quantized input using neural network. <em>TNNLS</em>,
<em>31</em>(4), 1222–1231. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates an adaptive finite-time control (FTC) problem for a class of strict-feedback nonlinear systems with both time-delays and quantized input from a new point of view. First, a new concept, called preassigned finite-time performance function (PFTF), is defined. Then, another novel notion, called practically preassigned finite-time stability (PPFTS), is introduced. With PFTF and PPFTS in hand, a novel sufficient condition of the FTC is given by using the neural network (NN) control and direct adaptive backstepping technique, which is different from the existing results. In addition, a modified barrier function is first introduced in this work. Moreover, this work is first to focus on the FTC for the situation that the time-delay and quantized input simultaneously exist in the nonlinear systems. Finally, simulation results are carried out to illustrate the effectiveness of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Yang Liu and Xiaoping Liu and Yuanwei Jing and Xiangyong Chen and Jianlong Qiu},
  doi          = {10.1109/TNNLS.2019.2919577},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1222-1231},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Direct adaptive preassigned finite-time control with time-delay and quantized input using neural network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Projective synchronization of delayed neural networks with
mismatched parameters and impulsive effects. <em>TNNLS</em>,
<em>31</em>(4), 1211–1221. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the impulsive effects on projective synchronization between the parameter mismatched neural networks with mixed time-varying delays have been analyzed. Since complete synchronization is not possible due to the existence of parameter mismatch and projective factor, a drive has been taken to achieve the weak projective synchronization of different neural networks under impulsive control strategies. Through the use of matrix measure technique and the extended comparison principle based on the formula of variation of parameters for mixed time-varying delayed impulsive systems, sufficient criteria have been derived for exponential convergence of the networks under the effects of extensive range of impulse. Instead of upper or lower bound of the impulsive interval, the concept of the average impulsive interval is applied to estimate the number of impulsive points in an interval. The concept of calculus is applied for optimizing the synchronization error bounds which are obtained because of different ranges of impulse. Finally, the numerical simulations for various impulsive ranges for different cases are presented graphically to validate the efficiency of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Rakesh Kumar and Shreemoyee Sarkar and Subir Das and Jinde Cao},
  doi          = {10.1109/TNNLS.2019.2919560},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1211-1221},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Projective synchronization of delayed neural networks with mismatched parameters and impulsive effects},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance enhancement of learning tracking systems over
fading channels with multiplicative and additive randomness.
<em>TNNLS</em>, <em>31</em>(4), 1196–1210. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper applies learning control to repetitive systems over fading channels at both output and input sides to improve tracking performance without applying restrictive fading conditions. Both multiplicative and additive randomness of the fading channel are addressed, and the effects of fading communication on the data are carefully analyzed. A decreasing gain sequence and a moving-average operator are introduced to modify the generic learning control algorithm to reduce the fading effect and improve control system performance. Results reveal that the tracking error converges to zero in the mean-square sense as the iteration number increases. Illustrative simulations are presented to verify the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Dong Shen and Ganggui Qu},
  doi          = {10.1109/TNNLS.2019.2919510},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1196-1210},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Performance enhancement of learning tracking systems over fading channels with multiplicative and additive randomness},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Error bounds for piecewise smooth and switching regression.
<em>TNNLS</em>, <em>31</em>(4), 1183–1195. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with regression problems, in which the nonsmooth target is assumed to switch between different operating modes. Specifically, piecewise smooth (PWS) regression considers target functions switching deterministically via a partition of the input space, while switching regression considers arbitrary switching laws. This paper derives generalization error bounds in these two settings by following the approach based on Rademacher complexities. For PWS regression, our derivation involves a chaining argument and a decomposition of the covering numbers of PWS classes in terms of the ones of their component function classes and the capacity of the classifier partitioning the input space. This yields error bounds with a radical dependence on the number of modes. For switching regression, the decomposition can be performed directly at the level of the Rademacher complexities, which yields bounds with a linear dependence on the number of modes. By using once more chaining and decomposition at the level of covering numbers, we show how to recover a radical dependence. Examples of applications are given in particular for PWS and swichting regression with linear and kernel-based component functions.},
  archive      = {J_TNNLS},
  author       = {Fabien Lauer},
  doi          = {10.1109/TNNLS.2019.2919444},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1183-1195},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Error bounds for piecewise smooth and switching regression},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RBFNN-based data-driven predictive iterative learning
control for nonaffine nonlinear systems. <em>TNNLS</em>, <em>31</em>(4),
1170–1182. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel data-driven predictive iterative learning control (DDPILC) scheme based on a radial basis function neural network (RBFNN) is proposed for a class of repeatable nonaffine nonlinear discrete-time systems subjected to nonrepetitive external disturbances. First, by utilizing the dynamic linearization technique (DLT) with a newly introduced and unknown system parameter pseudopartial derivative (PPD) and designing a new RBFNN estimation algorithm along the iterative learning axis for addressing the unknown PPD and the unknown nonrepetitive external disturbances, a data-driven prediction model is established. It is theoretically shown that by constructing a composite energy function (CEF) with respect to the modeling error for the first time, the convergence of the modeling error via the proposed DLT-based RBFNN modeling method can be guaranteed, and the convergence speed is tunable. Then, a DDPILC with a disturbance compensation term is designed, and the convergence of the tracking control error is analyzed. Finally, simulations of a train operation system reveal that even if the train suffers from randomly varying load disturbances and nonlinear running resistance, the proposed scheme can make both the modeling error and the tracking control error decrease successively with increasing operation number.},
  archive      = {J_TNNLS},
  author       = {Qiongxia Yu and Zhongsheng Hou and Xuhui Bu and Qiongfang Yu},
  doi          = {10.1109/TNNLS.2019.2919441},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1170-1182},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RBFNN-based data-driven predictive iterative learning control for nonaffine nonlinear systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). An improved n-step value gradient learning adaptive dynamic
programming algorithm for online learning. <em>TNNLS</em>,
<em>31</em>(4), 1155–1169. (<a
href="https://doi.org/10.1109/TNNLS.2019.2919338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In problems with complex dynamics and challenging state spaces, the dual heuristic programming (DHP) algorithm has been shown theoretically and experimentally to perform well. This was recently extended by an approach called value gradient learning (VGL). VGL was inspired by a version of temporal difference (TD) learning that uses eligibility traces. The eligibility traces create an exponential decay of older observations with a decay parameter (λ). This approach is known as TD(λ), and its DHP extension is known as VGL(λ), where VGL(0) is identical to DHP. VGL has presented convergence and other desirable properties, but it is primarily useful for batch learning. Online learning requires an eligibility-trace-work-space matrix, which is not required for the batch learning version of VGL. Since online learning is desirable for many applications, it is important to remove this computational and memory impediment. This paper introduces a dual-critic version of VGL, called N-step VGL (NSVGL), that does not need the eligibility-trace-workspace matrix, thereby allowing online learning. Furthermore, this combination of critic networks allows an NSVGL algorithm to learn faster. The first critic is similar to DHP, which is adapted based on TD(0) learning, while the second critic is adapted based on a gradient of n-step TD(λ) learning. Both networks are combined to train an actor network. The combination of feedback signals from both critic networks provides an optimal decision faster than traditional adaptive dynamic programming (ADP) via mixing current information and event history. Convergence proofs are provided. Gradients of one-and n-step value functions are monotonically nondecreasing and converge to the optimum. Two simulation case studies are presented for NSVGL to show their superior performance.},
  archive      = {J_TNNLS},
  author       = {Seaar Al-Dabooni and Donald C. Wunsch},
  doi          = {10.1109/TNNLS.2019.2919338},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1155-1169},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An improved N-step value gradient learning adaptive dynamic programming algorithm for online learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task assignment for multivehicle systems based on
collaborative neurodynamic optimization. <em>TNNLS</em>, <em>31</em>(4),
1145–1154. (<a
href="https://doi.org/10.1109/TNNLS.2019.2918984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses task assignment (TA) for multivehicle systems. Multivehicle TA problems are formulated as a combinatorial optimization problem and further as a global optimization problem. To fulfill heterogeneous tasks, cooperation among heterogeneous vehicles is incorporated in the problem formulations. A collaborative neurodynamic optimization approach is developed for solving the TA problems. Experimental results on four types of TA problems are discussed to substantiate the efficacy of the approach.},
  archive      = {J_TNNLS},
  author       = {Jiasen Wang and Jun Wang and Hangjun Che},
  doi          = {10.1109/TNNLS.2019.2918984},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1145-1154},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Task assignment for multivehicle systems based on collaborative neurodynamic optimization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance evaluation of probabilistic methods based on
bootstrap and quantile regression to quantify PV power point forecast
uncertainty. <em>TNNLS</em>, <em>31</em>(4), 1134–1144. (<a
href="https://doi.org/10.1109/TNNLS.2019.2918795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents two probabilistic approaches based on bootstrap method and quantile regression (QR) method to estimate the uncertainty associated with solar photovoltaic (PV) power point forecasts. Solar PV output power forecasts are obtained using a hybrid intelligent model, which is composed of a data filtering technique based on wavelet transform (WT) and a soft computing model (SCM) based on radial basis function neural network (RBFNN) that is optimized by particle swarm optimization (PSO) algorithm. The point forecast capability of the proposed hybrid WT+RBFNN+PSO intelligent model is examined and compared with other hybrid models as well as individual SCM. The performance of the proposed bootstrap method in the form of probabilistic forecasts is compared with the QR method by generating different prediction intervals (PIs). Numerical tests using real data demonstrate that the point forecasts obtained from the proposed hybrid intelligent model can be effectively used to quantify PV power uncertainty. The performance of these two uncertainty quantification methods is assessed through reliability.},
  archive      = {J_TNNLS},
  author       = {Yuxin Wen and Donna AlHakeem and Paras Mandal and Shantanu Chakraborty and Yuan-Kang Wu and Tomonobu Senjyu and Sumit Paudyal and Tzu-Liang Tseng},
  doi          = {10.1109/TNNLS.2019.2918795},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1134-1144},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Performance evaluation of probabilistic methods based on bootstrap and quantile regression to quantify PV power point forecast uncertainty},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuously constructive deep neural networks.
<em>TNNLS</em>, <em>31</em>(4), 1124–1133. (<a
href="https://doi.org/10.1109/TNNLS.2019.2918225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, deep learning algorithms update the network weights, whereas the network architecture is chosen manually using a process of trial and error. In this paper, we propose two novel approaches that automatically update the network structure while also learning its weights. The novelty of our approach lies in our parameterization, where the depth, or additional complexity, is encapsulated continuously in the parameter space through control parameters that add additional complexity. We propose two methods. In tunnel networks, this selection is done at the level of a hidden unit, and in budding perceptrons, this is done at the level of a network layer; updating this control parameter introduces either another hidden unit or layer. We show the effectiveness of our methods on the synthetic two-spiral data and on three real data sets of MNIST, MIRFLICKR, and CIFAR, where we see that our proposed methods, with the same set of hyperparameters, can correctly adjust the network complexity to the task complexity.},
  archive      = {J_TNNLS},
  author       = {Ozan İrsoy and Ethem Alpaydın},
  doi          = {10.1109/TNNLS.2019.2918225},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1124-1133},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continuously constructive deep neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A probabilistic synapse with strained MTJs for spiking
neural networks. <em>TNNLS</em>, <em>31</em>(4), 1113–1123. (<a
href="https://doi.org/10.1109/TNNLS.2019.2917819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are of interest for applications for which conventional computing suffers from the nearly insurmountable memory-processor bottleneck. This paper presents a stochastic SNN architecture that is based on specialized logic-in-memory synaptic units to create a unique processing system that offers massively parallel processing power. Our proposed synaptic unit consists of strained magnetic tunnel junction (MTJ) devices and transistors. MTJs in our synapse are dual purpose, used as both random bit generators and as general-purpose memory. Our neurons are modeled as integrate-and-fire components with thresholding and refraction. Our circuit is implemented using CMOS 28-nm technology that is compatible with the MTJ technology. Our design shows that the required area for the proposed synapse is only 3.64 μm2/unit. When idle, the synapse consumes 675 pW. When firing, the energy required to propagate a spike is 8.87 fJ. We then demonstrate an SNN that learns (without supervision) and classifies handwritten digits of the MNIST database. Simulation results show that our network presents high classification efficiency even in the presence of fabrication variability.},
  archive      = {J_TNNLS},
  author       = {Samuel N. Pagliarini and Sudipta Bhuin and Mehmet Meric Isgenc and Ayan Kumar Biswas and Larry Pileggi},
  doi          = {10.1109/TNNLS.2019.2917819},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1113-1123},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A probabilistic synapse with strained MTJs for spiking neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning low-dimensional latent graph structures: A density
estimation approach. <em>TNNLS</em>, <em>31</em>(4), 1098–1112. (<a
href="https://doi.org/10.1109/TNNLS.2019.2917696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to automatically learn a latent graph structure in a low-dimensional space from high-dimensional, unsupervised data based on a unified density estimation framework for both feature extraction and feature selection, where the latent structure is considered as a compact and informative representation of the high-dimensional data. Based on this framework, two novel methods are proposed with very different but intuitive learning criteria from existing methods. The proposed feature extraction method can learn a set of embedded points in a low-dimensional space by naturally integrating the discriminative information of the input data with structure learning so that multiple disconnected embedding structures of data can be uncovered. The proposed feature selection method preserves the pairwise distances only on the optimal set of features and selects these features simultaneously. It not only obtains the optimal set of features but also learns both the structure and embeddings for visualization. Extensive experiments demonstrate that our proposed methods can achieve competitive quantitative (often better) results in terms of discriminant evaluation performance and are able to obtain the embeddings of smooth skeleton structures and select optimal features to unveil the correct graph structures of high-dimensional data sets.},
  archive      = {J_TNNLS},
  author       = {Li Wang and Ren-cang Li},
  doi          = {10.1109/TNNLS.2019.2917696},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1098-1112},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning low-dimensional latent graph structures: A density estimation approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Where computation and dynamics meet: Heteroclinic
network-based controllers in evolutionary robotics. <em>TNNLS</em>,
<em>31</em>(4), 1084–1097. (<a
href="https://doi.org/10.1109/TNNLS.2019.2917471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fields of artificial neural networks and robotics, complicated, often high-dimensional systems can be designed using evolutionary/other algorithms to successfully solve very complex tasks. However, dynamical analysis of the underlying controller can often be near impossible, due to the high dimension and nonlinearities in the system. In this paper, we propose a more restricted form of controller, such that the underlying dynamical systems are forced to contain a dynamical object called a heteroclinic network. Systems containing heteroclinic networks share some properties with finite-state machines (FSMs) but are not discrete: both space and time are still described with continuous variables. Thus, we suggest that the heteroclinic networks can provide a hybrid between continuous and discrete systems. We investigate this innovated architecture in a minimal categorical perception task. The similarity of the controller to an FSM allows us to describe some of the system&#39;s behaviors as transition between states. However, other, essential behavior involves subtle ongoing interaction between the controller and the environment that eludes description at this level.},
  archive      = {J_TNNLS},
  author       = {Matthew D. Egbert and Valerie Jeong and Claire M. Postlethwaite},
  doi          = {10.1109/TNNLS.2019.2917471},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1084-1097},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Where computation and dynamics meet: Heteroclinic network-based controllers in evolutionary robotics},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust multiview subspace learning with nonindependently and
nonidentically distributed complex noise. <em>TNNLS</em>,
<em>31</em>(4), 1070–1083. (<a
href="https://doi.org/10.1109/TNNLS.2019.2917328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview Subspace Learning (MSL), which aims at obtaining a low-dimensional latent subspace from multiview data, has been widely used in practical applications. Most recent MSL approaches, however, only assume a simple independent identically distributed (i.i.d.) Gaussian or Laplacian noise for all views of data, which largely underestimates the noise complexity in practical multiview data. Actually, in real cases, noises among different views generally have three specific characteristics. First, in each view, the data noise always has a complex configuration beyond a simple Gaussian or Laplacian distribution. Second, the noise distributions of different views of data are generally nonidentical and with evident distinctiveness. Third, noises among all views are nonindependent but obviously correlated. Based on such understandings, we elaborately construct a new MSL model by more faithfully and comprehensively considering all these noise characteristics. First, the noise in each view is modeled as a Dirichlet process (DP) Gaussian mixture model (DPGMM), which can fit a wider range of complex noise types than conventional Gaussian or Laplacian. Second, the DPGMM parameters in each view are different from one another, which encodes the “nonidentical” noise property. Third, the DPGMMs on all views share the same high-level priors by using the technique of hierarchical DP, which encodes the “nonindependent” noise property. All the aforementioned ideas are incorporated into an integrated graphics model which can be appropriately solved by the variational Bayes algorithm. The superiority of the proposed method is verified by experiments on 3-D reconstruction simulations, multiview face modeling, and background subtraction, as compared with the current state-of-the-art MSL methods.},
  archive      = {J_TNNLS},
  author       = {Zongsheng Yue and Hongwei Yong and Deyu Meng and Qian Zhao and Yee Leung and Lei Zhang},
  doi          = {10.1109/TNNLS.2019.2917328},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1070-1083},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust multiview subspace learning with nonindependently and nonidentically distributed complex noise},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020j). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(3), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.2973780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.2973780},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comments on “fractional extreme value adaptive training
method: Fractional steepest descent approach.” <em>TNNLS</em>,
<em>31</em>(3), 1066–1068. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this comment, we raise serious concerns over the derivation of the rate of convergence of fractional steepest descent algorithm in fractional adaptive learning approach presented in “Fractional Extreme Value Adaptive Training Method: Fractional Steepest Descent Approach.” We substantiate that the estimate of the rate of convergence is grandiloquent. We also draw attention toward a critical flaw in the design of the algorithm stymieing its applicability for broad adaptive learning problems. Our claims are based on analytical reasoning supported by experimental results.},
  archive      = {J_TNNLS},
  author       = {Abdul Wahab and Shujaat Khan},
  doi          = {10.1109/TNNLS.2019.2899219},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1066-1068},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Comments on “Fractional extreme value adaptive training method: Fractional steepest descent approach”},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust event-triggered control invariance of probabilistic
boolean control networks. <em>TNNLS</em>, <em>31</em>(3), 1060–1065. (<a
href="https://doi.org/10.1109/TNNLS.2019.2917753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, the robust control invariance problem of probabilistic Boolean control networks (PBCNs) is investigated by a class of event-triggered control (ETC), which is an intermittent control scheme in essential. By resorting to the semi-tensor product (STP) technique, a PBCN with ETC can be equivalently described in a form of an algebraic linear system. Based on which, a matrix testing condition is derived to judge whether the given set can be a robust ETC invariant set (RETCIS). Subsequently, a necessary and sufficient condition is developed for the existence of event-triggered controllers. Meanwhile, all feasible event-triggered controllers are designed for guaranteeing the given set to be an RETCIS. Finally, a biological example is employed to demonstrate the availability of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Lin Lin and Jinde Cao and Leszek Rutkowski},
  doi          = {10.1109/TNNLS.2019.2917753},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1060-1065},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust event-triggered control invariance of probabilistic boolean control networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Composite learning enhanced robot impedance control.
<em>TNNLS</em>, <em>31</em>(3), 1052–1059. (<a
href="https://doi.org/10.1109/TNNLS.2019.2912212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The desired impedance dynamics can be achieved for a robot if and only if an impedance error converges to zero or a small neighborhood of zero. Although the convergence of impedance errors is important, it is seldom obtained in the existing impedance controllers due to robots modeling uncertainties and external disturbances. This brief proposes two composite learning impedance controllers (CLICs) for robots with parameter uncertainties based on whether a factorization assumption is satisfied or not. In the proposed control designs, the convergence of impedance errors, reflected by the convergence of parameter estimation errors and some auxiliary errors, is achieved by using composite learning laws under a relaxed excitation condition. The theoretical results are proven based on the Lyapunov theory. The effectiveness and advantages of the proposed CLICs are validated by simulations on a parallel robot in three cases.},
  archive      = {J_TNNLS},
  author       = {Tairen Sun and Liang Peng and Long Cheng and Zeng-Guang Hou and Yongping Pan},
  doi          = {10.1109/TNNLS.2019.2912212},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1052-1059},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composite learning enhanced robot impedance control},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The robustness of outputs with respect to disturbances for
boolean control networks. <em>TNNLS</em>, <em>31</em>(3), 1046–1051. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we investigate the robustness of outputs with respect to disturbances for Boolean control networks (BCNs) by semi-tensor product (STP) of matrices. First, BCNs are converted into the corresponding algebraic forms by STP, then two sufficient conditions for the robustness are derived. Moreover, the corresponding permutation system and permutation graph are constructed. It is proven that if there exist controllers such that the outputs of permutation systems are robust with respect to disturbances, then there must also exist controllers such that the outputs of the corresponding original systems achieve robustness with respect to disturbances. One effective method is proposed to construct controllers to achieve robustness. Examples are also provided to illustrate the correctness of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Bowen Li and Yang Liu and Jungang Lou and Jianquan Lu and Jinde Cao},
  doi          = {10.1109/TNNLS.2019.2910193},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1046-1051},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The robustness of outputs with respect to disturbances for boolean control networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Consensus tracking control of switched stochastic nonlinear
multiagent systems via event-triggered strategy. <em>TNNLS</em>,
<em>31</em>(3), 1036–1045. (<a
href="https://doi.org/10.1109/TNNLS.2019.2917137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the consensus tracking problem is investigated for a class of continuous switched stochastic nonlinear multiagent systems with an event-triggered control strategy. For continuous stochastic multiagent systems via event-triggered protocols, it is rather difficult to avoid the Zeno behavior by the existing methods. Thus, we propose a new protocol design framework for the underlying systems. It is proven that follower agents can almost surely track the given leader signal with bounded errors and no agent exhibits the Zeno behavior by the given control scheme. Finally, two numerical examples are given to illustrate the effectiveness and advantages of the new design techniques.},
  archive      = {J_TNNLS},
  author       = {Wencheng Zou and Peng Shi and Zhengrong Xiang and Yan Shi},
  doi          = {10.1109/TNNLS.2019.2917137},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1036-1045},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consensus tracking control of switched stochastic nonlinear multiagent systems via event-triggered strategy},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constrained quaternion-variable convex optimization: A
quaternion-valued recurrent neural network approach. <em>TNNLS</em>,
<em>31</em>(3), 1022–1035. (<a
href="https://doi.org/10.1109/TNNLS.2019.2916597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a quaternion-valued one-layer recurrent neural network approach to resolve constrained convex function optimization problems with quaternion variables. Leveraging the novel generalized Hamilton-real (GHR) calculus, the quaternion gradient-based optimization techniques are proposed to derive the optimization algorithms in the quaternion field directly rather than the methods of decomposing the optimization problems into the complex domain or the real domain. Via chain rules and Lyapunov theorem, the rigorous analysis shows that the deliberately designed quaternion-valued one-layer recurrent neural network stabilizes the system dynamics while the states reach the feasible region in finite time and converges to the optimal solution of the considered constrained convex optimization problems finally. Numerical simulations verify the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Yang Liu and Yanling Zheng and Jianquan Lu and Jinde Cao and Leszek Rutkowski},
  doi          = {10.1109/TNNLS.2019.2916597},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1022-1035},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Constrained quaternion-variable convex optimization: A quaternion-valued recurrent neural network approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural networks-based distributed adaptive control of
nonlinear multiagent systems. <em>TNNLS</em>, <em>31</em>(3), 1010–1021.
(<a href="https://doi.org/10.1109/TNNLS.2019.2915376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cooperative control problem of nonlinear multiagent systems is studied in this paper. The followers in the communication network are subject to unmodeled dynamics. A fully distributed neural-networks-based adaptive control strategy is designed to guarantee that all the followers are asymptotically synchronized to the leader, and the synchronization errors are within a prescribed level, where some global information, such as minimum and maximum singular value of graph adjacency matrix, is not necessarily to be known. Based on the Lyapunov stability theory and algebraic graph theory, the stability analysis of the resulting closed-loop system is provided. Finally, an numerical example illustrates the effectiveness and potential of the proposed new design techniques.},
  archive      = {J_TNNLS},
  author       = {Qikun Shen and Peng Shi and Junwu Zhu and Shuoyu Wang and Yan Shi},
  doi          = {10.1109/TNNLS.2019.2915376},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1010-1021},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural networks-based distributed adaptive control of nonlinear multiagent systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global stabilization of fractional-order memristor-based
neural networks with time delay. <em>TNNLS</em>, <em>31</em>(3),
997–1009. (<a href="https://doi.org/10.1109/TNNLS.2019.2915353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the global stabilization of fractional-order memristor-based neural networks (FMNNs) with time delay. The voltage threshold type memristor model is considered, and the FMNNs are represented by fractional-order differential equations with discontinuous right-hand sides. Then, the problem is addressed based on fractional-order differential inclusions and set-valued maps, together with the aid of Lyapunov functions and the comparison principle. Two types of control laws (delayed state feedback control and coupling state feedback control) are designed. Accordingly, two types of stabilization criteria [algebraic form and linear matrix inequality (LMI) form] are established. There are two groups of adjustable parameters included in the delayed state feedback control, which can be selected flexibly to achieve the desired global asymptotic stabilization or global Mittag-Leffler stabilization. Since the existing LMI-based stability analysis techniques for fractional-order systems are not applicable to delayed fractional-order nonlinear systems, a fractional-order differential inequality is established to overcome this difficulty. Based on the coupling state feedback control, some LMI stabilization criteria are developed for the first time with the help of the newly established fractional-order differential inequality. The obtained LMI results provide new insights into the research of delayed fractional-order nonlinear systems. Finally, three numerical examples are presented to illustrate the effectiveness of the proposed theoretical results.},
  archive      = {J_TNNLS},
  author       = {Jia Jia and Xia Huang and Yuxia Li and Jinde Cao and Ahmed Alsaedi},
  doi          = {10.1109/TNNLS.2019.2915353},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {997-1009},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global stabilization of fractional-order memristor-based neural networks with time delay},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneous domain adaptation via nonlinear matrix
factorization. <em>TNNLS</em>, <em>31</em>(3), 984–996. (<a
href="https://doi.org/10.1109/TNNLS.2019.2913723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation (HDA) aims to solve the learning problems where the source- and the target-domain data are represented by heterogeneous types of features. The existing HDA approaches based on matrix completion or matrix factorization have proven to be effective to capture shareable information between heterogeneous domains. However, there are two limitations in the existing methods. First, a large number of corresponding data instances between the source domain and the target domain are required to bridge the gap between different domains for performing matrix completion. These corresponding data instances may be difficult to collect in real-world applications due to the limited size of data in the target domain. Second, most existing methods can only capture linear correlations between features and data instances while performing matrix completion for HDA. In this paper, we address these two issues by proposing a new matrix-factorization-based HDA method in a semisupervised manner, where only a few labeled data are required in the target domain without requiring any corresponding data instances between domains. Such labeled data are more practical to obtain compared with cross-domain corresponding data instances. Our proposed algorithm is based on matrix factorization in an approximated reproducing kernel Hilbert space (RKHS), where nonlinear correlations between features and data instances can be exploited to learn heterogeneous features for both the source and the target domains. Extensive experiments are conducted on cross-domain text classification and object recognition, and experimental results demonstrate the superiority of our proposed method compared with the state-of-the-art HDA approaches.},
  archive      = {J_TNNLS},
  author       = {Haoliang Li and Sinno Jialin Pan and Shiqi Wang and Alex C. Kot},
  doi          = {10.1109/TNNLS.2019.2913723},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {984-996},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous domain adaptation via nonlinear matrix factorization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural output-feedback decentralized control for
large-scale nonlinear systems with stochastic disturbances.
<em>TNNLS</em>, <em>31</em>(3), 972–983. (<a
href="https://doi.org/10.1109/TNNLS.2019.2912082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of adaptive neural output-feedback decentralized control for a class of strongly interconnected nonlinear systems suffering stochastic disturbances. An state observer is designed to approximate the unmeasurable state signals. Using the approximation capability of radial basis function neural networks (NNs) and employing classic adaptive control strategy, an observer-based adaptive backstepping decentralized controller is developed. In the control design process, NNs are applied to model the uncertain nonlinear functions, and adaptive control and backstepping are combined to construct the controller. The developed control scheme can guarantee that all signals in the closed-loop systems are semiglobally uniformly ultimately bounded in fourth-moment. The simulation results demonstrate the effectiveness of the presented control scheme.},
  archive      = {J_TNNLS},
  author       = {Huanqing Wang and Peter Xiaoping Liu and Jialei Bao and Xue-Jun Xie and Shuai Li},
  doi          = {10.1109/TNNLS.2019.2912082},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {972-983},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural output-feedback decentralized control for large-scale nonlinear systems with stochastic disturbances},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization of the networked system with continuous and
impulsive hybrid communications. <em>TNNLS</em>, <em>31</em>(3),
960–971. (<a href="https://doi.org/10.1109/TNNLS.2019.2911926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many networked systems display some kind of dynamics behaving in a style with both continuous and impulsive communications. The cooperation behaviors of these networked systems with continuous connected or impulsive connected or both connected topologies of communications are important to understand. This paper is devoted to the synchronization of the networked system with continuous and impulsive hybrid communications, where each topology of communication mode is not connected in every moment. Two kind of structures, i.e., fixed structure and switching structures, are taken into consideration. A general concept of directed spanning tree (DST) is proposed to describe the connectivity of the networked system with hybrid communication modes. The suitable Lyapunov functions are constructed to analyze the synchronization stability. It is showed that for fixed topology having a jointly DST, the networked system with continuous and impulsive hybrid communication modes will achieve asymptotic synchronization if the feedback gain matrix and the average impulsive interval are properly selected. The results are then extended to the switching case where the graph has a frequently jointly DST. Some simple examples are then given to illustrate the derived synchronization criteria.},
  archive      = {J_TNNLS},
  author       = {Wen Sun and Junxia Guan and Jinhu Lü and Zhigang Zheng and Xinghuo Yu and Shihua Chen},
  doi          = {10.1109/TNNLS.2019.2911926},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {960-971},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of the networked system with continuous and impulsive hybrid communications},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional neural networks as asymmetric volterra models
based on generalized orthonormal basis functions. <em>TNNLS</em>,
<em>31</em>(3), 950–959. (<a
href="https://doi.org/10.1109/TNNLS.2019.2911603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a convolutional neural network (CNN) approach to derive Volterra models of dynamical systems based on generalized orthonormal basis function (GOBF)-Volterra. The approach derives the parameters of the model through a CNN and the neural network&#39;s learned weights represent the poles of a system. Simulation results show that the parameters of the system can be exactly recovered when no noise is applied. Furthermore, when noise is present, the errors in the parameters are very small for both the linear and nonlinear cases. Finally, the approach is used to identify the model of a quadcopter using data from actual flight tests. Comparisons with previous works demonstrate that CNNs can be satisfactorily used for the identification of dynamical systems.},
  archive      = {J_TNNLS},
  author       = {Jeremias B. Machado and Sidney N. Givigi},
  doi          = {10.1109/TNNLS.2019.2911603},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {950-959},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional neural networks as asymmetric volterra models based on generalized orthonormal basis functions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cognitive action laws: The case of visual features.
<em>TNNLS</em>, <em>31</em>(3), 938–949. (<a
href="https://doi.org/10.1109/TNNLS.2019.2911174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a theory for understanding perceptual learning processes within the general framework of laws of nature. Artificial neural networks are regarded as systems whose connections are Lagrangian variables, namely, functions depending on time. They are used to minimize the cognitive action, an appropriate functional index that measures the agent interactions with the environment. The cognitive action contains a potential and a kinetic term that nicely resemble the classic formulation of regularization in machine learning. A special choice of the functional index, which leads to the fourth-order differential equations-Cognitive Action Laws (CAL) - exhibits a structure that mirrors classic formulation of machine learning. In particular, unlike the action of mechanics, the stationarity condition corresponds with the global minimum. Moreover, it is proven that typical asymptotic learning conditions on the weights can coexist with the initialization provided that the system dynamics is driven under a policy referred to as information overloading control. Finally, the theory is experimented for the problem of feature extraction in computer vision.},
  archive      = {J_TNNLS},
  author       = {Alessandro Betti and Marco Gori and Stefano Melacci},
  doi          = {10.1109/TNNLS.2019.2911174},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {938-949},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cognitive action laws: The case of visual features},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From whole to part: Reference-based representation for
clustering categorical data. <em>TNNLS</em>, <em>31</em>(3), 927–937.
(<a href="https://doi.org/10.1109/TNNLS.2019.2911118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dissimilarity measures play a crucial role in clustering and, are directly related to the performance of clustering algorithms. However, effectively measuring the dissimilarity is not easy, especially for categorical data. The main difficulty of the dissimilarity measurement for categorical data is that its representation lacks a clear space structure. Therefore, the space structure-based representation has been proposed to provide the categorical data with a clear linear representation space. This representation improves the clustering performance obviously but only applies to small data sets because its dimensionality increases rapidly with the size of the data set. In this paper, we investigate the possibility of reducing the dimensionality of the space structure-based representation while maintaining the same representation ability. A lightweight representation scheme is proposed by taking a set of representative objects as the reference system (called the reference set) to position other objects in the Euclidean space. Moreover, a preclustering-based strategy is designed to select an appropriate reference set quickly. Finally, the representation scheme together with the $k$ -means algorithm provides an efficient method to cluster the categorical data. The theoretical and the experimental analysis shows that the proposed method outperforms state-of-the-art methods in terms of both accuracy and efficiency.},
  archive      = {J_TNNLS},
  author       = {Qibin Zheng and Xingchun Diao and Jianjun Cao and Yi Liu and Hongmei Li and Junnan Yao and Chen Chang and Guojun Lv},
  doi          = {10.1109/TNNLS.2019.2911118},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {927-937},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {From whole to part: Reference-based representation for clustering categorical data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust and sparse linear discriminant analysis via an
alternating direction method of multipliers. <em>TNNLS</em>,
<em>31</em>(3), 915–926. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a robust linear discriminant analysis (RLDA) through Bhattacharyya error bound optimization. RLDA considers a nonconvex problem with the L 1 -norm operation that makes it less sensitive to outliers and noise than the L 2 -norm linear discriminant analysis (LDA). In addition, we extend our RLDA to a sparse model (RSLDA). Both RLDA and RSLDA can extract unbounded numbers of features and avoid the small sample size (SSS) problem, and an alternating direction method of multipliers (ADMM) is used to cope with the nonconvexity in the proposed formulations. Compared with the traditional LDA, our RLDA and RSLDA are more robust to outliers and noise, and RSLDA can obtain sparse discriminant directions. These findings are supported by experiments on artificial data sets as well as human face databases.},
  archive      = {J_TNNLS},
  author       = {Chun-Na Li and Yuan-Hai Shao and Wotao Yin and Ming-Zeng Liu},
  doi          = {10.1109/TNNLS.2019.2910991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {915-926},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust and sparse linear discriminant analysis via an alternating direction method of multipliers},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural network-based adaptive antiswing control of an
underactuated ship-mounted crane with roll motions and input dead zones.
<em>TNNLS</em>, <em>31</em>(3), 901–914. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a type of indispensable oceanic transportation tools, ship-mounted crane systems are widely employed to transport cargoes and containers on vessels due to their extraordinary flexibility. However, various working requirements and the oceanic environment may cause some uncertain and unfavorable factors for ship-mounted crane control. In particular, to accomplish different control tasks, some plant parameters (e.g., boom lengths, payload masses, and so on) frequently change; hence, most existing model-based controllers cannot ensure satisfactory control performance any longer. For example, inaccurate gravity compensation may result in positioning errors. Additionally, due to ship roll motions caused by sea waves, residual payload swing generally exists, which may result in safety risks in practice. To solve the above-mentioned issues, this paper designs a neural network-based adaptive control method that can provide effective control for both actuated and unactuated state variables based on the original nonlinear ship-mounted crane dynamics without any linearizing operations. In particular, the proposed update law availably compensates parameter/structure uncertainties for ship-mounted crane systems. Based on a 2-D sliding surface, the boom and rope can arrive at their preset positions in finite time, and the payload swing can be completely suppressed. Furthermore, the problem of nonlinear input dead zones is also taken into account. The stability of the equilibrium point of all state variables in ship-mounted crane systems is theoretically proven by a rigorous Lyapunov-based analysis. The hardware experimental results verify the practicability and robustness of the presented control approach.},
  archive      = {J_TNNLS},
  author       = {Tong Yang and Ning Sun and He Chen and Yongchun Fang},
  doi          = {10.1109/TNNLS.2019.2910580},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {901-914},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based adaptive antiswing control of an underactuated ship-mounted crane with roll motions and input dead zones},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A switched operation approach to sampled-data control
stabilization of fuzzy memristive neural networks with time-varying
delay. <em>TNNLS</em>, <em>31</em>(3), 891–900. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the issue of sampled-data stabilization for Takagi-Sugeno fuzzy memristive neural networks (FMNNs) with time-varying delay. First, the concerned FMNNs are transformed into the tractable fuzzy NNs based on the excitatory and inhibitory of memristive synaptic weights using a new convex combination technique. Meanwhile, a switched fuzzy sampled-data controller is employed for the first time to tackle stability problems related to FMNNs. Then, the novel stabilization criteria of the FMNNs are established using the fuzzy membership functions (FMFs)-dependent Lyapunov-Krasovskii functional. This sufficiently utilizes information from not only the delayed state and the actual sampling pattern but also the FMFs. Two simulation examples are presented to demonstrate the feasibility and validity of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Xin Wang and Ju H. Park and Shouming Zhong and Huilan Yang},
  doi          = {10.1109/TNNLS.2019.2910574},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {891-900},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A switched operation approach to sampled-data control stabilization of fuzzy memristive neural networks with time-varying delay},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bipartite differential neural network for unsupervised image
change detection. <em>TNNLS</em>, <em>31</em>(3), 876–890. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image change detection detects the regions of change in multiple images of the same scene taken at different times, which plays a crucial role in many applications. The two most popular image change detection techniques are as follows: pixel-based methods heavily rely on accurate image coregistration while object-based approaches can tolerate coregistration errors to some extent but are sensitive to image segmentation or classification errors. To address these issues, we propose an unsupervised image change detection approach based on a novel bipartite differential neural network (BDNN). The BDNN is a deep neural network with two input ends, which can extract the holistic features from the unchanged regions in the two input images, where two learnable change disguise maps (CDMs) are used to disguise the changed regions in the two input images, respectively, and thus demarcate the unchanged regions therein. The network parameters and CDMs will be learned by optimizing an objective function, which combines a loss function defined as the likelihood of the given input image pair over all possible input image pairs and two constraints imposed on CDMs. Compared with the pixel-based and object-based techniques, the BDNN is less sensitive to inaccurate image coregistration and does not involve image segmentation or classification. In fact, it can even skip over coregistration if the degree of transformation (due to the different view angles and/or positions of the camera) between the two input images is not that large. We compare the proposed approach with several state-of-the-art image change detection methods on various homogeneous and heterogeneous image pairs with and without coregistration. The results demonstrate the superiority of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Jia Liu and Maoguo Gong and A. K. Qin and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2019.2910571},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {876-890},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bipartite differential neural network for unsupervised image change detection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonsynaptic error backpropagation in long-term cognitive
networks. <em>TNNLS</em>, <em>31</em>(3), 865–875. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a neural cognitive mapping technique named long-term cognitive network (LTCN) that is able to memorize long-term dependencies between a sequence of input and output vectors, especially in those scenarios that require predicting the values of multiple dependent variables at the same time. The proposed technique is an extension of a recently proposed method named short-term cognitive network that aims at preserving the expert knowledge encoded in the weight matrix while optimizing the nonlinear mappings provided by the transfer function of each neuron. A nonsynaptic, backpropagation-based learning algorithm powered by stochastic gradient descent is put forward to iteratively optimize four parameters of the generalized sigmoid transfer function associated with each neuron. Numerical simulations over 35 multivariate regression and pattern completion data sets confirm that the proposed LTCN algorithm attains statistically significant performance differences with respect to other well-known state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Gonzalo Nápoles and Frank Vanhoenshoven and Rafael Falcon and Koen Vanhoof},
  doi          = {10.1109/TNNLS.2019.2910555},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {865-875},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonsynaptic error backpropagation in long-term cognitive networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trajectory tracking on uncertain complex networks via
NN-based inverse optimal pinning control. <em>TNNLS</em>,
<em>31</em>(3), 854–864. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new approach for trajectory tracking on uncertain complex networks is proposed. To achieve this goal, a neural controller is applied to a small fraction of nodes (pinned ones). Such controller is composed of an on-line identifier based on a recurrent high-order neural network, and an inverse optimal controller to track the desired trajectory; a complete stability analysis is also included. In order to verify the applicability and good performance of the proposed control scheme, a representative example is simulated, which consists of a complex network with each node described by a chaotic Lorenz oscillator.},
  archive      = {J_TNNLS},
  author       = {Carlos J. Vega and Oscar J. Suarez and Edgar N. Sanchez and Guanrong Chen and Santiago Elvira-Ceja and David I. Rodriguez},
  doi          = {10.1109/TNNLS.2019.2910504},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {854-864},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Trajectory tracking on uncertain complex networks via NN-based inverse optimal pinning control},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust visual system for small target motion detection
against cluttered moving backgrounds. <em>TNNLS</em>, <em>31</em>(3),
839–853. (<a href="https://doi.org/10.1109/TNNLS.2019.2910418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring small objects against cluttered moving backgrounds is a huge challenge to future robotic vision systems. As a source of inspiration, insects are quite apt at searching for mates and tracking prey, which always appear as small dim speckles in the visual field. The exquisite sensitivity of insects for small target motion, as revealed recently, is coming from a class of specific neurons called small target motion detectors (STMDs). Although a few STMD-based models have been proposed, these existing models only use motion information for small target detection and cannot discriminate small targets from small-target-like background features (named fake features). To address this problem, this paper proposes a novel visual system model (STMD+) for small target motion detection, which is composed of four subsystems-ommatidia, motion pathway, contrast pathway, and mushroom body. Compared with the existing STMD-based models, the additional contrast pathway extracts directional contrast from luminance signals to eliminate false positive background motion. The directional contrast and the extracted motion information by the motion pathway are integrated into the mushroom body for small target discrimination. Extensive experiments showed the significant and consistent improvements of the proposed visual system model over the existing STMD-based models against fake features.},
  archive      = {J_TNNLS},
  author       = {Hongxin Wang and Jigen Peng and Xuqiang Zheng and Shigang Yue},
  doi          = {10.1109/TNNLS.2019.2910418},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {839-853},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A robust visual system for small target motion detection against cluttered moving backgrounds},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Log-sum-exp neural networks and posynomial models for
convex and log-log-convex data. <em>TNNLS</em>, <em>31</em>(3), 827–838.
(<a href="https://doi.org/10.1109/TNNLS.2019.2910417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we show that a one-layer feedforward neural network with exponential activation functions in the inner layer and logarithmic activation in the output neuron is a universal approximator of convex functions. Such a network represents a family of scaled log-sum exponential functions, here named log-sum-exp (LSE T ). Under a suitable exponential transformation, the class of LSE T functions maps to a family of generalized posynomials GPOS T , which we similarly show to be universal approximators for log-log-convex functions. A key feature of an LSE T network is that, once it is trained on data, the resulting model is convex in the variables, which makes it readily amenable to efficient design based on convex optimization. Similarly, once a GPOS T model is trained on data, it yields a posynomial model that can be efficiently optimized with respect to its variables by using geometric programming (GP). The proposed methodology is illustrated by two numerical examples, in which, first, models are constructed from simulation data of the two physical processes (namely, the level of vibration in a vehicle suspension system, and the peak power generated by the combustion of propane), and then optimization-based design is performed on these models.},
  archive      = {J_TNNLS},
  author       = {Giuseppe C. Calafiore and Stephane Gaubert and Corrado Possieri},
  doi          = {10.1109/TNNLS.2019.2910417},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {827-838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Log-sum-exp neural networks and posynomial models for convex and log-log-convex data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recurrent neural networks with external addressable
long-term and working memory for learning long-term dependences.
<em>TNNLS</em>, <em>31</em>(3), 813–826. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning long-term dependences (LTDs) with recurrent neural networks (RNNs) is challenging due to their limited internal memories. In this paper, we propose a new external memory architecture for RNNs called an external addressable long-term and working memory (EALWM)-augmented RNN. This architecture has two distinct advantages over existing neural external memory architectures, namely the division of the external memory into two parts-long-term memory and working memory-with both addressable and the capability to learn LTDs without suffering from vanishing gradients with necessary assumptions. The experimental results on algorithm learning, language modeling, and question answering demonstrate that the proposed neural memory architecture is promising for practical applications.},
  archive      = {J_TNNLS},
  author       = {Zhibin Quan and Weili Zeng and Xuelian Li and Yandong Liu and Yunxiu Yu and Wankou Yang},
  doi          = {10.1109/TNNLS.2019.2910302},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {813-826},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recurrent neural networks with external addressable long-term and working memory for learning long-term dependences},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stability-based generalization analysis of distributed
learning algorithms for big data. <em>TNNLS</em>, <em>31</em>(3),
801–812. (<a href="https://doi.org/10.1109/TNNLS.2019.2910188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the efficient approaches to deal with big data, divide-and-conquer distributed algorithms, such as the distributed kernel regression, bootstrap, structured perception training algorithms, and so on, are proposed and broadly used in learning systems. Some learning theories have been built to analyze the feasibility, approximation, and convergence bounds of these distributed learning algorithms. However, less work has been studied on the stability of these distributed learning algorithms. In this paper, we discuss the generalization bounds of distributed learning algorithms from the view of algorithmic stability. First, we introduce a definition of uniform distributed stability for distributed algorithms and study the distributed algorithms&#39; generalization risk bounds. Then, we analyze the stability properties and generalization risk bounds of a kind of regularization-based distributed algorithms. Two generalization distributed risks obtained show that the generalization distributed risk bounds for the difference between their generalization distributed and empirical distributed/leave-one-computer-out risks are closely related to the size of samples n and the amount of working computers m as O(m/n 1/2 ) . Furthermore, the results in this paper indicate that, for a good generalization regularized distributed kernel algorithm, the regularization parameter λ should be adjusted with the change of the term m/n 1/2 . These theoretic discoveries provide the useful guidance when deploying the distributed algorithms on practical big data platforms. We explore our theoretic analyses through two simulation experiments. Finally, we discuss some problems about the sufficient amount of working computers, nonequivalence, and generalization for distributed learning. We show that the rules for the computation on one single computer may not always hold for distributed learning.},
  archive      = {J_TNNLS},
  author       = {Xinxing Wu and Junping Zhang and Fei-Yue Wang},
  doi          = {10.1109/TNNLS.2019.2910188},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {801-812},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability-based generalization analysis of distributed learning algorithms for big data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative fisher embedding dictionary learning
algorithm for object recognition. <em>TNNLS</em>, <em>31</em>(3),
786–800. (<a href="https://doi.org/10.1109/TNNLS.2019.2910146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both interclass variances and intraclass similarities are crucial for improving the classification performance of discriminative dictionary learning (DDL) algorithms. However, existing DDL methods often ignore the combination between the interclass and intraclass properties of dictionary atoms and coding coefficients. To address this problem, in this paper, we propose a discriminative Fisher embedding dictionary learning (DFEDL) algorithm that simultaneously establishes Fisher embedding models on learned atoms and coefficients. Specifically, we first construct a discriminative Fisher atom embedding model by exploring the Fisher criterion of the atoms, which encourages the atoms of the same class to reconstruct the corresponding training samples as much as possible. At the same time, a discriminative Fisher coefficient embedding model is formulated by imposing the Fisher criterion on the profiles (row vectors of the coding coefficient matrix) and coding coefficients, which forces the coding coefficient matrix to become a block-diagonal matrix. Since the profiles can indicate which training samples are represented by the corresponding atoms, the proposed two discriminative Fisher embedding models can alternatively and interactively promote the discriminative capabilities of the learned dictionary and coding coefficients. The extensive experimental results demonstrate that the proposed DFEDL algorithm achieves superior performance in comparison with some state-of-the-art dictionary learning algorithms on both hand-crafted and deep learning-based features.},
  archive      = {J_TNNLS},
  author       = {Zhengming Li and Zheng Zhang and Jie Qin and Zhao Zhang and Ling Shao},
  doi          = {10.1109/TNNLS.2019.2910146},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {786-800},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative fisher embedding dictionary learning algorithm for object recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compact and computationally efficient representation of deep
neural networks. <em>TNNLS</em>, <em>31</em>(3), 772–785. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the core of any inference procedure, deep neural networks are dot product operations, which are the component that requires the highest computational resources. For instance, deep neural networks, such as VGG-16, require up to 15-G operations in order to perform the dot products present in a single forward pass, which results in significant energy consumption and thus limits their use in resource-limited environments, e.g., on embedded devices or smartphones. One common approach to reduce the complexity of the inference is to prune and quantize the weight matrices of the neural network. Usually, this results in matrices whose entropy values are low, as measured relative to the empirical probability mass distribution of its elements. In order to efficiently exploit such matrices, one usually relies on, inter alia, sparse matrix representations. However, most of these common matrix storage formats make strong statistical assumptions about the distribution of the elements; therefore, cannot efficiently represent the entire set of matrices that exhibit low-entropy statistics (thus, the entire set of compressed neural network weight matrices). In this paper, we address this issue and present new efficient representations for matrices with low-entropy statistics. Alike sparse matrix data structures, these formats exploit the statistical properties of the data in order to reduce the size and execution complexity. Moreover, we show that the proposed data structures can not only be regarded as a generalization of sparse formats but are also more energy and time efficient under practically relevant assumptions. Finally, we test the storage requirements and execution performance of the proposed formats on compressed neural networks and compare them to dense and sparse representations. We experimentally show that we are able to attain up to ×42 compression ratios, ×5 speed ups, and ×90 energy savings when we lossless convert the state-of-the-art networks, such as AlexNet, VGG-16, ResNet152, and DenseNet, into the new data structures and benchmark their respective dot product.},
  archive      = {J_TNNLS},
  author       = {Simon Wiedemann and Klaus-Robert Müller and Wojciech Samek},
  doi          = {10.1109/TNNLS.2019.2910073},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {772-785},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Compact and computationally efficient representation of deep neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed dissipative state estimation for markov jump
genetic regulatory networks subject to round-robin scheduling.
<em>TNNLS</em>, <em>31</em>(3), 762–771. (<a
href="https://doi.org/10.1109/TNNLS.2019.2909747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed dissipative state estimation issue of Markov jump genetic regulatory networks subject to round-robin scheduling is investigated in this paper. The system parameters randomly change in the light of a Markov chain. Each node in sensor networks communicates with its neighboring nodes in view of the prescribed network topology graph. The round-robin scheduling is employed to arrange the transmission order to lessen the likelihood of the occurrence of data collisions. The main goal of the work is to design a compatible distributed estimator to assure that the distributed error system is strictly (Λ 1 , Λ 2 , Λ 3 )y-stochastically dissipative. By applying the Lyapunov stability theory and a modified matrix decoupling way, sufficient conditions are derived by solving some convex optimization problems. An illustrative example is given to verify the validity of the provided method.},
  archive      = {J_TNNLS},
  author       = {Hao Shen and Shicheng Huo and Huaicheng Yan and Ju H. Park and Victor Sreeram},
  doi          = {10.1109/TNNLS.2019.2909747},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {762-771},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed dissipative state estimation for markov jump genetic regulatory networks subject to round-robin scheduling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exactly robust kernel principal component analysis.
<em>TNNLS</em>, <em>31</em>(3), 749–761. (<a
href="https://doi.org/10.1109/TNNLS.2019.2909686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust principal component analysis (RPCA) can recover low-rank matrices when they are corrupted by sparse noises. In practice, many matrices are, however, of high rank and, hence, cannot be recovered by RPCA. We propose a novel method called robust kernel principal component analysis (RKPCA) to decompose a partially corrupted matrix as a sparse matrix plus a high- or full-rank matrix with low latent dimensionality. RKPCA can be applied to many problems such as noise removal and subspace clustering and is still the only unsupervised nonlinear method robust to sparse noises. Our theoretical analysis shows that, with high probability, RKPCA can provide high recovery accuracy. The optimization of RKPCA involves nonconvex and indifferentiable problems. We propose two nonconvex optimization algorithms for RKPCA. They are alternating direction method of multipliers with backtracking line search and proximal linearized minimization with adaptive step size (AdSS). Comparative studies in noise removal and robust subspace clustering corroborate the effectiveness and the superiority of RKPCA.},
  archive      = {J_TNNLS},
  author       = {Jicong Fan and Tommy W. S. Chow},
  doi          = {10.1109/TNNLS.2019.2909686},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {749-761},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exactly robust kernel principal component analysis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive deep modeling of users and items using side
information for recommendation. <em>TNNLS</em>, <em>31</em>(3), 737–748.
(<a href="https://doi.org/10.1109/TNNLS.2019.2909432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the existing recommender systems, matrix factorization (MF) is widely applied to model user preferences and item features by mapping the user-item ratings into a low-dimension latent vector space. However, MF has ignored the individual diversity where the user&#39;s preference for different unrated items is usually different. A fixed representation of user preference factor extracted by MF cannot model the individual diversity well, which leads to a repeated and inaccurate recommendation. To this end, we propose a novel latent factor model called adaptive deep latent factor model (ADLFM), which learns the preference factor of users adaptively in accordance with the specific items under consideration. We propose a novel user representation method that is derived from their rated item descriptions instead of original user-item ratings. Based on this, we further propose a deep neural networks framework with an attention factor to learn the adaptive representations of users. Extensive experiments on Amazon data sets demonstrate that ADLFM outperforms the state-of-the-art baselines greatly. Also, further experiments show that the attention factor indeed makes a great contribution to our method.},
  archive      = {J_TNNLS},
  author       = {Jiayu Han and Lei Zheng and Yuanbo Xu and Bangzuo Zhang and Fuzhen Zhuang and Philip S. Yu and Wanli Zuo},
  doi          = {10.1109/TNNLS.2019.2909432},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {737-748},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive deep modeling of users and items using side information for recommendation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). LABIN: Balanced min cut for large-scale data.
<em>TNNLS</em>, <em>31</em>(3), 725–736. (<a
href="https://doi.org/10.1109/TNNLS.2019.2909425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many spectral clustering algorithms have been proposed during the past decades, they are not scalable to large-scale data due to their high computational complexities. In this paper, we propose a novel spectral clustering method for large-scale data, namely, large-scale balanced min cut (LABIN). A new model is proposed to extend the self-balanced min-cut (SBMC) model with the anchor-based strategy and a fast spectral rotation with linear time complexity is proposed to solve the new model. Extensive experimental results show the superior performance of our proposed method in comparison with the state-of-the-art methods including SBMC.},
  archive      = {J_TNNLS},
  author       = {Xiaojun Chen and Renjie Chen and Qingyao Wu and Yixiang Fang and Feiping Nie and Joshua Zhexue Huang},
  doi          = {10.1109/TNNLS.2019.2909425},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {725-736},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LABIN: Balanced min cut for large-scale data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneous multilayer generalized operational perceptron.
<em>TNNLS</em>, <em>31</em>(3), 710–724. (<a
href="https://doi.org/10.1109/TNNLS.2019.2914082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional multilayer perceptron (MLP) using a McCulloch-Pitts neuron model is inherently limited to a set of neuronal activities, i.e., linear weighted sum followed by nonlinear thresholding step. Previously, generalized operational perceptron (GOP) was proposed to extend the conventional perceptron model by defining a diverse set of neuronal activities to imitate a generalized model of biological neurons. Together with GOP, a progressive operational perceptron (POP) algorithm was proposed to optimize a predefined template of multiple homogeneous layers in a layerwise manner. In this paper, we propose an efficient algorithm to learn a compact, fully heterogeneous multilayer network that allows each individual neuron, regardless of the layer, to have distinct characteristics. Based on the complexity of the problem, the proposed algorithm operates in a progressive manner on a neuronal level, searching for a compact topology, not only in terms of depth but also width, i.e., the number of neurons in each layer. The proposed algorithm is shown to outperform other related learning methods in extensive experiments on several classification problems.},
  archive      = {J_TNNLS},
  author       = {Dat Thanh Tran and Serkan Kiranyaz and Moncef Gabbouj and Alexandros Iosifidis},
  doi          = {10.1109/TNNLS.2019.2914082},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {710-724},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous multilayer generalized operational perceptron},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020k). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(2), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.2968826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.2968826},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stabilization of second-order memristive neural networks
with mixed time delays via nonreduced order. <em>TNNLS</em>,
<em>31</em>(2), 700–706. (<a
href="https://doi.org/10.1109/TNNLS.2019.2910125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we investigate a class of second-order memristive neural networks (SMNNs) with mixed time-varying delays. Based on nonsmooth analysis, the Lyapunov stability theory, and adaptive control theory, several new results ensuring global stabilization of the SMNNs are obtained. In addition, compared with the reduced-order method used in the existing research studies, we consider the global stabilization directly from the SMNNs themselves without the reduced-order method. Finally, we give some numerical simulations to show the effectiveness of the results.},
  archive      = {J_TNNLS},
  author       = {Guodong Zhang and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2019.2910125},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {700-706},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stabilization of second-order memristive neural networks with mixed time delays via nonreduced order},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <span class="math inline">𝒢</span> -softmax: Improving
intraclass compactness and interclass separability of features.
<em>TNNLS</em>, <em>31</em>(2), 685–699. (<a
href="https://doi.org/10.1109/TNNLS.2019.2909737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intraclass compactness and interclass separability are crucial indicators to measure the effectiveness of a model to produce discriminative features, where intraclass compactness indicates how close the features with the same label are to each other and interclass separability indicates how far away the features with different labels are. In this paper, we investigate intraclass compactness and interclass separability of features learned by convolutional networks and propose a Gaussian-based softmax (G-softmax) function that can effectively improve intraclass compactness and interclass separability. The proposed function is simple to implement and can easily replace the softmax function. We evaluate the proposed G-softmax function on classification data sets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) and on multilabel classification data sets (i.e., MS COCO and NUS-WIDE). The experimental results show that the proposed G-softmax function improves the state-of-the-art models across all evaluated data sets. In addition, the analysis of the intraclass compactness and interclass separability demonstrates the advantages of the proposed function over the softmax function, which is consistent with the performance improvement. More importantly, we observe that high intraclass compactness and interclass separability are linearly correlated with average precision on MS COCO and NUS-WIDE. This implies that the improvement of intraclass compactness and interclass separability would lead to the improvement of average precision.},
  archive      = {J_TNNLS},
  author       = {Yan Luo and Yongkang Wong and Mohan Kankanhalli and Qi Zhao},
  doi          = {10.1109/TNNLS.2019.2909737},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {685-699},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {$\mathcal{G}$ -softmax: Improving intraclass compactness and interclass separability of features},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stability analysis for delayed neural networks with an
improved general free-matrix-based integral inequality. <em>TNNLS</em>,
<em>31</em>(2), 675–684. (<a
href="https://doi.org/10.1109/TNNLS.2019.2909350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revisits the problem of stability analysis for neural networks with a time-varying delay. An improved general free-matrix-based (FMB) integral inequality is proposed with an undetermined number m. Compared with the conventional FMB ones, the improved inequality involves a much smaller number of free matrix variables. In particular, the improved FMB integral inequality is expressed in a concrete form for any value of m. By employing the new inequality with a properly constructed Lyapunov-Krasovskii functional, a new stability condition is derived for neural networks with a time-varying delay. Two commonly used numerical examples are given to show strong competitiveness of the proposed approach in both the conservatism and computation burdens.},
  archive      = {J_TNNLS},
  author       = {Jun Chen and Ju H. Park and Shengyuan Xu},
  doi          = {10.1109/TNNLS.2019.2909350},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {675-684},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability analysis for delayed neural networks with an improved general free-matrix-based integral inequality},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial pyramid-enhanced NetVLAD with weighted triplet loss
for place recognition. <em>TNNLS</em>, <em>31</em>(2), 661–674. (<a
href="https://doi.org/10.1109/TNNLS.2019.2908982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an end-to-end place recognition model based on a novel deep neural network. First, we propose to exploit the spatial pyramid structure of the images to enhance the vector of locally aggregated descriptors (VLAD) such that the enhanced VLAD features can reflect the structural information of the images. To encode this feature extraction into the deep learning method, we build a spatial pyramid-enhanced VLAD (SPE-VLAD) layer. Next, we impose weight constraints on the terms of the traditional triplet loss (T-loss) function such that the weighted T-loss (WT-loss) function avoids the suboptimal convergence of the learning process. The loss function can work well under weakly supervised scenarios in that it determines the semantically positive and negative samples of each query through not only the GPS tags but also the Euclidean distance between the image representations. The SPE-VLAD layer and the WT-loss layer are integrated with the VGG-16 network or ResNet-18 network to form a novel end-to-end deep neural network that can be easily trained via the standard backpropagation method. We conduct experiments on three benchmark data sets, and the results demonstrate that the proposed model defeats the state-of-the-art deep learning approaches applied to place recognition.},
  archive      = {J_TNNLS},
  author       = {Jun Yu and Chaoyang Zhu and Jian Zhang and Qingming Huang and Dacheng Tao},
  doi          = {10.1109/TNNLS.2019.2908982},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {661-674},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatial pyramid-enhanced NetVLAD with weighted triplet loss for place recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic neural network with complex exponential
activation functions in image recognition. <em>TNNLS</em>,
<em>31</em>(2), 651–660. (<a
href="https://doi.org/10.1109/TNNLS.2019.2908973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If the training data set in image recognition task is not very large, the feature extraction with a convolutional neural network is usually applied. Here, we focus on the nonparametric classification of extracted feature vectors using the probabilistic neural network (PNN). The latter is characterized by the high runtime and memory space complexity. We propose to overcome these drawbacks by replacing the exponential activation function in the Gaussian kernel to the complex exponential functions. Such complex nonlinearities make it possible to accurately approximate the unknown density function using the network with the number of neurons proportional to only cubic root of the database size. As a result, the proposed approach decreases the runtime and memory complexities of the PNN without losing its main advantages, namely, fast training and convergence to the Bayesian decision. In the experimental study, we describe a protocol for comparing recognition methods using the well-known visual object category data sets in the context of the small sample size problem. It has been experimentally shown that our approach rapidly obtains accurate decisions when compared to the known classifiers including the baseline PNN.},
  archive      = {J_TNNLS},
  author       = {Andrey V. Savchenko},
  doi          = {10.1109/TNNLS.2019.2908973},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {651-660},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic neural network with complex exponential activation functions in image recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Passivity analysis for quaternion-valued memristor-based
neural networks with time-varying delay. <em>TNNLS</em>, <em>31</em>(2),
639–650. (<a href="https://doi.org/10.1109/TNNLS.2019.2908755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the problem of global exponential passivity for quaternion-valued memristor-based neural networks (QVMNNs) with time-varying delay. The QVMNNs can be seen as a switched system due to the memristor parameters are switching according to the states of the network. This is the first time that the global exponential passivity of QVMNNs with time-varying delay is investigated. By means of a nondecomposition method and structuring novel Lyapunov functional in form of quaternion self-conjugate matrices, the delay-dependent passivity criteria are derived in the forms of quaternion-valued linear matrix inequalities (LMIs) as well as complex-valued LMIs. Furthermore, the asymptotical stability criteria can be obtained from the proposed passivity criteria. Finally, a numerical example is presented to illustrate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Ning Li and Wei Xing Zheng},
  doi          = {10.1109/TNNLS.2019.2908755},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {639-650},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Passivity analysis for quaternion-valued memristor-based neural networks with time-varying delay},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast semisupervised learning with bipartite graph for
large-scale data. <em>TNNLS</em>, <em>31</em>(2), 626–638. (<a
href="https://doi.org/10.1109/TNNLS.2019.2908504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the captured information in our real word is very scare and labeling sample is time cost and expensive, semisupervised learning (SSL) has an important application in computer vision and machine learning. Among SSL approaches, a graph-based SSL (GSSL) model has recently attracted much attention for high accuracy. However, for most traditional GSSL methods, the large-scale data bring higher computational complexity, which acquires a better computing platform. In order to dispose of these issues, we propose a novel approach, bipartite GSSL normalized (BGSSL-normalized) method, in this paper. This method consists of three parts. First, the bipartite graph between the original data and the anchor points is constructed, which is parameter-insensitive, scale-invariant, naturally sparse, and simple operation. Then, the label of the original data and anchors can be inferred through the graph. Besides, we extend our algorithm to handle out-of-sample for large-scale data by the inferred label of anchors, which not only retains good classification result but also saves a large amount of time. The computational complexity of BGSSL-normalized can be reduced to O(ndm +nm 2 ), which is a significant improvement compared with traditional GSSL methods that need O(n 2 d + n 3 ), where n, d, and m are the number of samples, features, and anchors, respectively. The experimental results on several publicly available data sets demonstrate that our approaches can achieve better classification accuracy with less time costs.},
  archive      = {J_TNNLS},
  author       = {Fang He and Feiping Nie and Rong Wang and Xuelong Li and Weimin Jia},
  doi          = {10.1109/TNNLS.2019.2908504},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {626-638},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast semisupervised learning with bipartite graph for large-scale data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distribution-free probability density forecast through deep
neural networks. <em>TNNLS</em>, <em>31</em>(2), 612–625. (<a
href="https://doi.org/10.1109/TNNLS.2019.2907305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probability density forecast offers the whole distributions of forecasting targets, which brings greater flexibility and practicability than the other probabilistic forecast models such as prediction interval (PI) and quantile forecast. However, existing density forecast models have introduced various constraints on forecasted distributions, which has limited their ability to approximate real distributions and may result in suboptimality. In this paper, a distribution-free density forecast model based on deep learning is proposed, in which the real cumulative density functions (CDFs) of forecasting target are approximated by a large-capacity positive-weighted deep neural network (NN). Benefiting from the universal approximation ability of NNs, the range of forecasted distributions has been proven to contain all the distributions with continuous CDFs, which is superior to existing models’ considering both width and accordance with reality. Three tests from different scenarios were implemented for evaluation, i.e., very-short-term wind power, wind speed, and day-ahead electricity price forecast, in which the proposed density forecast model has shown superior performance over the state of the art.},
  archive      = {J_TNNLS},
  author       = {Tianyu Hu and Qinglai Guo and Zhengshuo Li and Xinwei Shen and Hongbin Sun},
  doi          = {10.1109/TNNLS.2019.2907305},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {612-625},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distribution-free probability density forecast through deep neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Marginalized multiview ensemble clustering. <em>TNNLS</em>,
<em>31</em>(2), 600–611. (<a
href="https://doi.org/10.1109/TNNLS.2019.2906867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering (MVC), which aims to explore the underlying cluster structure shared by multiview data, has drawn more research efforts in recent years. To exploit the complementary information among multiple views, existing methods mainly learn a common latent subspace or develop a certain loss across different views, while ignoring the higher level information such as basic partitions (BPs) generated by the single-view clustering algorithm. In light of this, we propose a novel marginalized multiview ensemble clustering (M 2 VEC) method in this paper. Specifically, we solve MVC in an EC way, which generates BPs for each view individually and seeks for a consensus one. By this means, we naturally leverage the complementary information of multiview data upon the same partition space. In order to boost the robustness of our approach, the marginalized denoising process is adopted to mimic the data corruptions and noises, which provides robust partition-level representations for each view by training a single-layer autoencoder. A low-rank and sparse decomposition is seamlessly incorporated into the denoising process to explicitly capture the consistency information and meanwhile compensate the distinctness between heterogeneous features. Spectral consensus graph partitioning is also involved by our model to make M 2 VEC as a unified optimization framework. Moreover, a multilayer M 2 VEC is eventually delivered in a stacked fashion to encapsulate nonlinearity into partition-level representations for handling complex data. Experimental results on eight real-world data sets show the efficacy of our approach compared with several state-of-the-art multiview and EC methods. We also showcase our method performs well with partial multiview data.},
  archive      = {J_TNNLS},
  author       = {Zhiqiang Tao and Hongfu Liu and Sheng Li and Zhengming Ding and Yun Fu},
  doi          = {10.1109/TNNLS.2019.2906867},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {600-611},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Marginalized multiview ensemble clustering},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Eliminating the permutation ambiguity of convolutive blind
source separation by using coupled frequency bins. <em>TNNLS</em>,
<em>31</em>(2), 589–599. (<a
href="https://doi.org/10.1109/TNNLS.2019.2906833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind source separation (BSS) is a typical unsupervised learning method that extracts latent components from their observations. In the meanwhile, convolutive BSS (CBSS) is particularly challenging as the observations are the mixtures of latent components as well as their delayed versions. CBSS is usually solved in frequency domain since convolutive mixtures in time domain is just instantaneous mixtures in frequency domain, which allows to recover source frequency components independently of each frequency bin by running ordinary BSS, and then concatenate them to form the Fourier transformation of source signals. Because BSS has inherent permutation ambiguity, this category of CBSS methods suffers from a common drawback: it is very difficult to choose the frequency components belonging to a specific source as they are estimated from different frequency bins using BSS. This paper presents a tensor framework that can completely eliminate the permutation ambiguity. By combining each frequency bin with an anchor frequency bin that is chosen arbitrarily in advance, we establish a new virtual BSS model where the corresponding correlation matrices comply with a block tensor decomposition (BTD) model. The essential uniqueness of BTD and the sparse structure of coupled mixing parameters allow the estimation of the mixing matrices free of permutation ambiguity. Extensive simulation results confirmed that the proposed algorithm could achieve higher separation accuracy compared with the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Kan Xie and Guoxu Zhou and Junjie Yang and Zhaoshui He and Shengli Xie},
  doi          = {10.1109/TNNLS.2019.2906833},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {589-599},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Eliminating the permutation ambiguity of convolutive blind source separation by using coupled frequency bins},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward compact ConvNets via structure-sparsity regularized
filter pruning. <em>TNNLS</em>, <em>31</em>(2), 574–588. (<a
href="https://doi.org/10.1109/TNNLS.2019.2906563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of convolutional neural networks (CNNs) in computer vision applications has been accompanied by a significant increase of computation and memory costs, which prohibits their usage on resource-limited environments, such as mobile systems or embedded devices. To this end, the research of CNN compression has recently become emerging. In this paper, we propose a novel filter pruning scheme, termed structured sparsity regularization (SSR), to simultaneously speed up the computation and reduce the memory overhead of CNNs, which can be well supported by various off-the-shelf deep learning libraries. Concretely, the proposed scheme incorporates two different regularizers of structured sparsity into the original objective function of filter pruning, which fully coordinates the global output and local pruning operations to adaptively prune filters. We further propose an alternative updating with Lagrange multipliers (AULM) scheme to efficiently solve its optimization. AULM follows the principle of alternating direction method of multipliers (ADMM) and alternates between promoting the structured sparsity of CNNs and optimizing the recognition loss, which leads to a very efficient solver (2.5× to the most recent work that directly solves the group sparsity-based regularization). Moreover, by imposing the structured sparsity, the online inference is extremely memory-light since the number of filters and the output feature maps are simultaneously reduced. The proposed scheme has been deployed to a variety of state-of-the-art CNN structures, including LeNet, AlexNet, VGGNet, ResNet, and GoogLeNet, over different data sets. Quantitative results demonstrate that the proposed scheme achieves superior performance over the state-of-the-art methods. We further demonstrate the proposed compression scheme for the task of transfer learning, including domain adaptation and object detection, which also show exciting performance gains over the state-of-the-art filter pruning methods.},
  archive      = {J_TNNLS},
  author       = {Shaohui Lin and Rongrong Ji and Yuchao Li and Cheng Deng and Xuelong Li},
  doi          = {10.1109/TNNLS.2019.2906563},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {574-588},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward compact ConvNets via structure-sparsity regularized filter pruning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint and direct optimization for dictionary learning in
convolutional sparse representation. <em>TNNLS</em>, <em>31</em>(2),
559–573. (<a href="https://doi.org/10.1109/TNNLS.2019.2906074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional sparse coding (CSC) is a useful tool in many image and audio applications. Maximizing the performance of CSC requires that the dictionary used to store the features of signals can be learned from real data. The so-called convolutional dictionary learning (CDL) problem is formulated within a nonconvex, nonsmooth optimization framework. Most existing CDL solvers alternately update the coefficients and dictionary in an iterative manner. However, these approaches are prone to running redundant iterations, and their convergence properties are difficult to analyze. Moreover, most of those methods approximate the original nonconvex sparse inducing function using a convex regularizer to promote computational efficiency. This approach to approximation may result in nonsparse representations and, thereby, hinder the performance of the applications. In this paper, we deal with the nonconvex, nonsmooth constraints of the original CDL directly using the modified forward-backward splitting approach, in which the coefficients and dictionary are simultaneously updated in each iteration. We also propose a novel parameter adaption scheme to increase the speed of the algorithm used to obtain a usable dictionary and in so doing prove convergence. We also show that the proposed approach is applicable to parallel processing to reduce the computing time required by the algorithm to achieve convergence. The experimental results demonstrate that our method requires less time than the existing methods to achieve the convergence point while using a smaller final functional value. We also applied the dictionaries learned using the proposed and existing methods to an application involving signal separation. The dictionary learned using the proposed approach provides performance superior to that of comparable methods.},
  archive      = {J_TNNLS},
  author       = {Guan-Ju Peng},
  doi          = {10.1109/TNNLS.2019.2906074},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {559-573},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint and direct optimization for dictionary learning in convolutional sparse representation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive optimal control for a class of nonlinear systems:
The online policy iteration approach. <em>TNNLS</em>, <em>31</em>(2),
549–558. (<a href="https://doi.org/10.1109/TNNLS.2019.2905715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the online adaptive optimal controller design for a class of nonlinear systems through a novel policy iteration (PI) algorithm. By using the technique of neural network linear differential inclusion (LDI) to linearize the nonlinear terms in each iteration, the optimal law for controller design can be solved through the relevant algebraic Riccati equation (ARE) without using the system internal parameters. Based on PI approach, the adaptive optimal control algorithm is developed with the online linearization and the two-step iteration, i.e., policy evaluation and policy improvement. The convergence of the proposed PI algorithm is also proved. Finally, two numerical examples are given to illustrate the effectiveness and applicability of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Shuping He and Haiyang Fang and Maoguang Zhang and Fei Liu and Zhengtao Ding},
  doi          = {10.1109/TNNLS.2019.2905715},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {549-558},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive optimal control for a class of nonlinear systems: The online policy iteration approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Metacognitive octonion-valued neural networks as they relate
to time series analysis. <em>TNNLS</em>, <em>31</em>(2), 539–548. (<a
href="https://doi.org/10.1109/TNNLS.2019.2905643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a metacognitive octonion-valued neural network (Mc-OVNN) learning algorithm and its application to diverse time series prediction are presented. The Mc-OVNN is comprised of two components: the octonion-valued neural network that represents the cognitive component and the metacognitive component that serves to self-regulate the learning algorithm. At each epoch, the metacognitive component decides if, how, and when learning occurs. The algorithm deletes unneeded samples and only stores those that will be used. This decision is determined by the octonion magnitude and the seven phases. To evaluate the Mc-OVNN algorithm&#39;s performance, it is applied to five real-world forecasting problems: the power consumption of a home in Honolulu, HI, USA, Box and Jenkins J series, Euro to Algerian Dinar (DZ) real-time conversion rates, the Mackey-Glass equation, and Europe Brent oil price prediction in a time series. When comparing the Mc-OVNN to other relevant techniques, Mc-OVNN displays its capability for efficient time series prediction. The real-time evaluation of the proposed algorithm is presented using the power consumption of a home in Boumerdès, Algeria, as a case study.},
  archive      = {J_TNNLS},
  author       = {Lyes Saad Saoud and Reza Ghorbani},
  doi          = {10.1109/TNNLS.2019.2905643},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {539-548},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Metacognitive octonion-valued neural networks as they relate to time series analysis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Greedy projected gradient-newton method for sparse logistic
regression. <em>TNNLS</em>, <em>31</em>(2), 527–538. (<a
href="https://doi.org/10.1109/TNNLS.2019.2905261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse logistic regression (SLR), which is widely used for classification and feature selection in many fields, such as neural networks, deep learning, and bioinformatics, is the classical logistic regression model with sparsity constraints. In this paper, we perform theoretical analysis on the existence and uniqueness of the solution to the SLR, and we propose a greedy projected gradient-Newton (GPGN) method for solving the SLR. The GPGN method is a combination of the projected gradient method and the Newton method. The following characteristics show that the GPGN method achieves not only elegant theoretical results but also a remarkable numerical performance in solving the SLR: 1) the full iterative sequence generated by the GPGN method converges to a global/local minimizer of the SLR under weaker conditions; 2) the GPGN method has the properties of afinite identification for an optimal support set and local quadratic convergence; and 3) the GPGN method achieves higher accuracy and higher speed compared with a number of state-of-the-art solvers according to numerical experiments.},
  archive      = {J_TNNLS},
  author       = {Rui Wang and Naihua Xiu and Chao Zhang},
  doi          = {10.1109/TNNLS.2019.2905261},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {527-538},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Greedy projected gradient-newton method for sparse logistic regression},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The hierarchical continuous pursuit learning automation: A
novel scheme for environments with large numbers of actions.
<em>TNNLS</em>, <em>31</em>(2), 512–526. (<a
href="https://doi.org/10.1109/TNNLS.2019.2905162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the field of learning automata (LA) has made significant progress in the past four decades, the LA-based methods to tackle problems involving environments with a large number of actions is, in reality, relatively unresolved. The extension of the traditional LA to problems within this domain cannot be easily established when the number of actions is very large. This is because the dimensionality of the action probability vector is correspondingly large, and so, most components of the vector will soon have values that are smaller than the machine accuracy permits, implying that they will never be chosen. This paper presents a solution that extends the continuous pursuit paradigm to such large-actioned problem domains. The beauty of the solution is that it is hierarchical, where all the actions offered by the environment reside as leaves of the hierarchy. Furthermore, at every level, we merely require a two-action LA that automatically resolves the problem of dealing with arbitrarily small action probabilities. In addition, since all the LA invoke the pursuit paradigm, the best action at every level trickles up toward the root. Thus, by invoking the property of the “max” operator, in which the maximum of numerous maxima is the overall maximum, the hierarchy of LA converges to the optimal action. This paper describes the scheme and formally proves its E-optimal convergence. The results presented here can, rather trivially, be extended for the families of discretized and Bayesian pursuit LA too. This paper also reports extensive experimental results (including for environments having 128 and 256 actions) that demonstrate the power of the scheme and its computational advantages. As far as we know, there are no comparable pursuitbased results in the field of LA. In some cases, the hierarchical continuous pursuit automaton requires less than 18\% of the number of iterations than the benchmark L R-I scheme, which is, by all metrics, phenomenal.},
  archive      = {J_TNNLS},
  author       = {Anis Yazidi and Xuan Zhang and Lei Jiao and B. John Oommen},
  doi          = {10.1109/TNNLS.2019.2905162},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {512-526},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The hierarchical continuous pursuit learning automation: A novel scheme for environments with large numbers of actions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hidden bursting firings and bifurcation mechanisms in
memristive neuron model with threshold electromagnetic induction.
<em>TNNLS</em>, <em>31</em>(2), 502–511. (<a
href="https://doi.org/10.1109/TNNLS.2019.2905137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memristors can be employed to mimic biological neural synapses or to describe electromagnetic induction effects. To exhibit the threshold effect of electromagnetic induction, this paper presents a threshold flux-controlled memristor and examines its frequency-dependent pinched hysteresis loops. Using an electromagnetic induction current generated by the threshold memristor to replace the external current in 2-D Hindmarsh-Rose (HR) neuron model, a 3-D memristive HR (mHR) neuron model with global hidden oscillations is established and the corresponding numerical simulations are performed. It is found that due to no equilibrium point, the obtained mHR neuron model always operates in hidden bursting firing patterns, including coexisting hidden bursting firing patterns with bistability also. In addition, the model exhibits complex dynamics of the actual neuron electrical activities, which acts like the 3-D HR neuron model, indicating its feasibility. In particular, by constructing the fold and Hopf bifurcation sets of the fast-scale subsystem, the bifurcation mechanisms of hidden bursting firings are expounded. Finally, circuit experiments on hardware breadboards are deployed and the captured results well match with the numerical results, validating the physical mechanism of biological neuron and the reliability of electronic neuron.},
  archive      = {J_TNNLS},
  author       = {Han Bao and Aihuang Hu and Wenbo Liu and Bocheng Bao},
  doi          = {10.1109/TNNLS.2019.2905137},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {502-511},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hidden bursting firings and bifurcation mechanisms in memristive neuron model with threshold electromagnetic induction},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PCNN mechanism and its parameter settings. <em>TNNLS</em>,
<em>31</em>(2), 488–501. (<a
href="https://doi.org/10.1109/TNNLS.2019.2905113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pulse-coupled neural network (PCNN) model is a third-generation artificial neural network without training that uses the synchronous pulse bursts of neurons to process digital images, but the lack of in-depth theoretical research limits its extensive application. By analyzing the working mechanism of the PCNN, we present an expression for the fire-extinguishing time of neurons that fire in the second iteration and an expression for the firing time of neurons that extinguish in the second iteration. In addition, we find a phenomenon of the PCNN and name it mathematically coupled fire extinguishing. Based on the above analysis, we propose a new working mode for the PCNN, where the refiring of fire-extinguishing neurons is only allowed when all firing neurons are extinguished. We also work out the constraint conditions of the parameter settings under this mode. Furthermore, we analyze the relationship between the network parameters and mathematically coupled fire extinguishing, the coupling of neighboring neurons, and the convergence rate of the PCNN, respectively. In addition, we demonstrate the essential regularity of extinguished neuron in the PCNN and then propose an optimal parameter setting to achieve the best comprehensive performance of the PCNN.},
  archive      = {J_TNNLS},
  author       = {Xiangyu Deng and Chunman Yan and Yide Ma},
  doi          = {10.1109/TNNLS.2019.2905113},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {488-501},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PCNN mechanism and its parameter settings},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flow adversarial networks: Flowrate prediction for
gas–liquid multiphase flows across different domains. <em>TNNLS</em>,
<em>31</em>(2), 475–487. (<a
href="https://doi.org/10.1109/TNNLS.2019.2905082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The solution of how to accurately and timely predict the flowrate of gas–liquid mixtures is the key to help petroleum and other related industries to reduce costs, improve efficiency, and optimize management. Although numerous studies have been carried out over the past decades, the problem is still significantly challenging due to the complexity of multiphase flows. This paper attempts to seek new possibilities for multiphase flow measurement and novel application scenarios for state-of-the-art machine learning (ML) techniques. Convolutional neural networks (CNNs) are applied to predict the flowrate of multiphase flows for the first time and can achieve promising performance. In addition, considering the difference between data distributions of training and testing samples and its negative impact on prediction accuracy of the CNN models on testing samples, we propose flow adversarial networks (FANs) that can distill both domain-invariant and flowrate-discriminative features from the raw input. The method is evaluated on dynamic experimental data of different multiphase flows on different flow conditions and operating environments. The experimental results demonstrate that FANs can effectively prevent the accuracy degradation caused by the gap between training and testing samples and have better performance than state-of-the-art approaches in the flowrate prediction field.},
  archive      = {J_TNNLS},
  author       = {Delin Hu and Jinku Li and Yinyan Liu and Yi Li},
  doi          = {10.1109/TNNLS.2019.2905082},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {475-487},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Flow adversarial networks: Flowrate prediction for Gas–Liquid multiphase flows across different domains},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information transmitted from bioinspired neuron–astrocyte
network improves cortical spiking network’s pattern recognition
performance. <em>TNNLS</em>, <em>31</em>(2), 464–474. (<a
href="https://doi.org/10.1109/TNNLS.2019.2905003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We trained two spiking neural networks (SNNs), the cortical spiking network (CSN) and the cortical neuron–astrocyte network (CNAN), using a spike-based unsupervised method, on the MNIST and alpha-digit data sets and achieve an accuracy of 96.1\% and 77.35\%, respectively. We then connected CNAN to CSN by preserving maximum synchronization between them thanks to the concept of prolate spheroidal wave functions (PSWF). As a result, CSN receives additional information from CNAN without retraining. The important outcome is that CSN reaches 70.57\% correct classification rate on capital letters without being trained on them. The overall contribution of transfer is 87.47\%. We observed that for CSN the classifying neurons that relate to digits 0–9 of the alpha-digit data set are completely supported by the ones that relate to digits 0–9 of the MNIST data set. This means that CSN recognizes the similarity between the digits of the MNIST and alpha-digit data sets and classifies each digit of both data sets in the same class.},
  archive      = {J_TNNLS},
  author       = {Soheila Nazari and Masoud Amiri and Karim Faez and Marc M. Van Hulle},
  doi          = {10.1109/TNNLS.2019.2905003},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {464-474},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Information transmitted from bioinspired Neuron–Astrocyte network improves cortical spiking network’s pattern recognition performance},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local synchronization of interconnected boolean networks
with stochastic disturbances. <em>TNNLS</em>, <em>31</em>(2), 452–463.
(<a href="https://doi.org/10.1109/TNNLS.2019.2904978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the local synchronization problem for the interconnected Boolean networks (BNs) without and with stochastic disturbances. For the case without stochastic disturbances, first, the limit set and the transient period of the interconnected BNs are discussed by resorting to the properties of the reachable set for the global initial states set. Second, in terms of logical submatrices of a certain Boolean vector, a compact algebraic expression is presented for the limit set of the given initial states set. Based on it, several necessary and sufficient conditions are derived assuring the local synchronization of the interconnected BNs. Subsequently, an efficient algorithm is developed to calculate the largest domain of attraction. As for the interconnected BNs with stochastic disturbances, first, mutually independent two-valued random logical variables are introduced to describe the stochastic disturbances. Then, the corresponding local synchronization criteria are also established, and the algorithm to calculate the largest domain of attraction is designed. Finally, numerical examples are employed to illustrate the effectiveness of the obtained results/ algorithms.},
  archive      = {J_TNNLS},
  author       = {Hongwei Chen and Jinling Liang},
  doi          = {10.1109/TNNLS.2019.2904978},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {452-463},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local synchronization of interconnected boolean networks with stochastic disturbances},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Term selection for a class of separable nonlinear models.
<em>TNNLS</em>, <em>31</em>(2), 445–451. (<a
href="https://doi.org/10.1109/TNNLS.2019.2904952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the term selection problem for a class of separable nonlinear models. The strategy is a two-step process in which the nonlinear parameters of the model are first optimized by a variable projection method, and then the least absolute shrinkage and selection operator are adopted to obtain a sparse solution by picking out the critical terms automatically. This process may be repeated several times. The proposed algorithm is tested on parameter estimation problems for an exponential model and a neural network-based model. The numerical results show that the proposed algorithm can pick out the appropriate terms from the overparameterized model and the obtained parsimonious model performs better than other methods.},
  archive      = {J_TNNLS},
  author       = {Min Gan and Guang-Yong Chen and Long Chen and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2019.2904952},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {445-451},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Term selection for a class of separable nonlinear models},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint principal component and discriminant analysis for
dimensionality reduction. <em>TNNLS</em>, <em>31</em>(2), 433–444. (<a
href="https://doi.org/10.1109/TNNLS.2019.2904701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is the most widely used supervised dimensionality reduction approach. After removing the null space of the total scatter matrix S t via principal component analysis (PCA), the LDA algorithm can avoid the small sample size problem. Most existing supervised dimensionality reduction methods extract the principal component of data first, and then conduct LDA on it. However, “most variance” is very often the most important, but not always in PCA. Thus, this two-step strategy may not be able to obtain the most discriminant information for classification tasks. Different from traditional approaches which conduct PCA and LDA in sequence, we propose a novel method referred to as joint principal component and discriminant analysis (JPCDA) for dimensionality reduction. Using this method, we are able to not only avoid the small sample size problem but also extract discriminant information for classification tasks. An iterative optimization algorithm is proposed to solve the method. To validate the efficacy of the proposed method, we perform extensive experiments on several benchmark data sets in comparison with some state-of-the-art dimensionality reduction methods. A large number of experimental results illustrate that the proposed method has quite promising classification performance.},
  archive      = {J_TNNLS},
  author       = {Xiaowei Zhao and Jun Guo and Feiping Nie and Ling Chen and Zhihui Li and Huaxiang Zhang},
  doi          = {10.1109/TNNLS.2019.2904701},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {433-444},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint principal component and discriminant analysis for dimensionality reduction},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed fault-tolerant control of multiagent systems: An
adaptive learning approach. <em>TNNLS</em>, <em>31</em>(2), 420–432. (<a
href="https://doi.org/10.1109/TNNLS.2019.2904277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on developing a distributed leader-following fault-tolerant tracking control scheme for a class of high-order nonlinear uncertain multiagent systems. Neural network-based adaptive learning algorithms are developed to learn unknown fault functions, guaranteeing the system stability and cooperative tracking even in the presence of multiple simultaneous process and actuator faults in the distributed agents. The time-varying leader&#39;s command is only communicated to a small portion of follower agents through directed links, and each follower agent exchanges local measurement information only with its neighbors through a bidirectional but asymmetric topology. Adaptive fault-tolerant algorithms are developed for two cases, i.e., with full-state measurement and with only limited output measurement, respectively. Under certain assumptions, the closed-loop stability and asymptotic leader-follower tracking properties are rigorously established.},
  archive      = {J_TNNLS},
  author       = {Mohsen Khalili and Xiaodong Zhang and Yongcan Cao and Marios M. Polycarpou and Thomas Parisini},
  doi          = {10.1109/TNNLS.2019.2904277},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {420-432},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed fault-tolerant control of multiagent systems: An adaptive learning approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural network-based distributed cooperative learning
control for multiagent systems via event-triggered communication.
<em>TNNLS</em>, <em>31</em>(2), 407–419. (<a
href="https://doi.org/10.1109/TNNLS.2019.2904253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an event-based distributed cooperative learning (DCL) law is proposed for a group of adaptive neural control systems. The plants to be controlled have identical structures, but reference signals for each plant are different. During control process, each agent intermittently broadcasts its neural network (NN) weight estimation to its neighboring agents under an event-triggered condition that is only based on its own estimated NN weights. If communication topology is connected and undirected, the NN weights of all neural control systems can converge to a small neighborhood of their optimal values. The generalization ability of NNs is guaranteed in the event-triggered context, that is, the approximation domain of each NN is the union of all system trajectories. Furthermore, a strictly positive lower bound on the interevent intervals is also guaranteed to avoid the Zeno behavior. Finally, a numerical example is given to illustrate the effectiveness of the proposed learning law.},
  archive      = {J_TNNLS},
  author       = {Fei Gao and Weisheng Chen and Zhiwu Li and Jing Li and Bin Xu},
  doi          = {10.1109/TNNLS.2019.2904253},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {407-419},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based distributed cooperative learning control for multiagent systems via event-triggered communication},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <span class="math inline"><em>H</em><sub>∞</sub></span>
static output-feedback control design for discrete-time systems using
reinforcement learning. <em>TNNLS</em>, <em>31</em>(2), 396–406. (<a
href="https://doi.org/10.1109/TNNLS.2019.2901889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides necessary and sufficient conditions for the existence of the static output-feedback (OPFB) solution to the H ∞ control problem for linear discrete-time systems. It is shown that the solution of the static OPFB H ∞ control is a Nash equilibrium point. Furthermore, a Q-learning algorithm is developed to find the H ∞ OPFB solution online using data measured along the system trajectories and without knowing the system matrices. This is achieved by solving a game algebraic Riccati equation online and using the measured data. A simulation example shows the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Amir Parviz Valadbeigi and Ali Khaki Sedigh and F. L. Lewis},
  doi          = {10.1109/TNNLS.2019.2901889},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {396-406},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {$H_{\infty}$ static output-feedback control design for discrete-time systems using reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep decision tree transfer boosting. <em>TNNLS</em>,
<em>31</em>(2), 383–395. (<a
href="https://doi.org/10.1109/TNNLS.2019.2901273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance transfer approaches consider source and target data together during the training process, and borrow examples from the source domain to augment the training data, when there is limited or no label in the target domain. Among them, boosting-based transfer learning methods (e.g., TrAdaBoost) are most widely used. When dealing with more complex data, we may consider the more complex hypotheses (e.g., a decision tree with deeper layers). However, with the fixed and high complexity of the hypotheses, TrAdaBoost and its variants may face the overfitting problems. Even worse, in the transfer learning scenario, a decision tree with deep layers may overfit different distribution data in the source domain. In this paper, we propose a new instance transfer learning method, i.e., Deep Decision Tree Transfer Boosting (DTrBoost), whose weights are learned and assigned to base learners by minimizing the data-dependent learning bounds across both source and target domains in terms of the Rademacher complexities. This guarantees that we can learn decision trees with deep layers without overfitting. The theorem proof and experimental results indicate the effectiveness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Shuhui Jiang and Haiyi Mao and Zhengming Ding and Yun Fu},
  doi          = {10.1109/TNNLS.2019.2901273},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {383-395},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep decision tree transfer boosting},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A white-box equivalent neural network circuit model for SoC
estimation of electrochemical cells. <em>TNNLS</em>, <em>31</em>(2),
371–382. (<a href="https://doi.org/10.1109/TNNLS.2019.2901062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart grids, microgrids, and pure electric powertrains are the key technologies for achieving the expected goals concerning the restraint of CO 2 emissions and global warming. In this context, an effective use of electrochemical energy storage systems (ESSs) is mandatory. In particular, accurate state of charge (SoC) estimations are helpful for improving the ESS performances. To this aim, developing accurate models of electrochemical cells is necessary for implementing effective SoC estimators. Therefore, a novel neural network modeling technique is proposed in this paper. The main contribution consists in the development of a white-box neural design that provides helpful insights into the cell physics, together with a powerful nonlinear approximation capability, and a flexible system identification procedure. In order to do that, the system equations of a white-box equivalent circuit model (ECM) have been combined with computational intelligence techniques by approximating each circuit element with a dedicated neural network. The model performances have been analyzed in terms of model accuracy, SoC estimation effectiveness, and computational cost over two realistic data sets. Moreover, the proposed model has been compared with a white-box ECM and a gray-box neural network model. The results prove that the proposed modeling technique is able to provide useful improvements in the SoC estimation task with a competing computational cost.},
  archive      = {J_TNNLS},
  author       = {Massimiliano Luzi and Fabio Massimo Frattale Mascioli and Maurizio Paschero and Antonello Rizzi},
  doi          = {10.1109/TNNLS.2019.2901062},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {371-382},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A white-box equivalent neural network circuit model for SoC estimation of electrochemical cells},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selection and optimization of temporal spike encoding
methods for spiking neural networks. <em>TNNLS</em>, <em>31</em>(2),
358–370. (<a href="https://doi.org/10.1109/TNNLS.2019.2906158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) receive trains of spiking events as inputs. In order to design efficient SNN systems, real-valued signals must be optimally encoded into spike trains so that the task-relevant information is retained. This paper provides a systematic quantitative and qualitative analysis and guidelines for optimal temporal encoding. It proposes a methodology of a three-step encoding workflow: method selection by signal characteristics, parameter optimization by error metrics between original and reconstructed signals, and validation by comparison of the original signal and the encoded spike train. Four encoding methods are analyzed: one stimulus estimation [Ben&#39;s Spiker algorithm (BSA)] and three temporal contrast [threshold-based, step-forward (SW), and moving-window (MW)] encodings. A short theoretical analysis is provided, and the extended quantitative analysis is carried out applying four types of test signals: step-wise signal, smooth (sinusoid) signal with added noise, trended smooth signal, and event-like smooth signal. Various time-domain and frequency spectrum properties are explored, and a comparison is provided. BSA, the only method providing unipolar spikes, was shown to be ineffective for step-wise signals, but it can follow smoothly changing signals if filter coefficients are scaled appropriately. Producing bipolar (positive and negative) spike trains, SW encoding was most effective for all types of signals as it proved to be robust and easy to optimize. Signal-to-noise ratio (SNR) can be recommended as the error metric for parameter optimization. Currently, only a visual check is available for final validation.},
  archive      = {J_TNNLS},
  author       = {Balint Petro and Nikola Kasabov and Rita M. Kiss},
  doi          = {10.1109/TNNLS.2019.2906158},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {358-370},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Selection and optimization of temporal spike encoding methods for spiking neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020l). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>31</em>(1), C3. (<a
href="https://doi.org/10.1109/TNNLS.2019.2960955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2019.2960955},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noise robust projection rule for hyperbolic hopfield neural
networks. <em>TNNLS</em>, <em>31</em>(1), 352–356. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complex-valued Hopfield neural network (CHNN) is a multistate Hopfield model. Low noise tolerance is the main disadvantage of CHNNs. The hyperbolic Hopfield neural network (HHNN) is a noise robust multistate Hopfield model. In HHNNs employing the projection rule, noise tolerance rapidly worsened as the number of training patterns increased. This result was caused by the self-loops. The projection rule for CHNNs improves noise tolerance by removing the self-loops, however, that for HHNNs cannot remove them. In this brief, we extended the stability condition for the self-loops of HHNNs and modified the projection rule. Thus, the HHNNs had improved noise tolerance.},
  archive      = {J_TNNLS},
  author       = {Masaki Kobayashi},
  doi          = {10.1109/TNNLS.2019.2899914},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {352-356},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Noise robust projection rule for hyperbolic hopfield neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Augmenting recurrent neural networks resilience by dropout.
<em>TNNLS</em>, <em>31</em>(1), 345–351. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief discusses the simple idea that dropout regularization can be used to efficiently induce resiliency to missing inputs at prediction time in a generic neural network. We show how the approach can be effective on tasks where imputation strategies often fail, namely, involving recurrent neural networks and scenarios where whole sequences of input observations are missing. The experimental analysis provides an assessment of the accuracy-resiliency tradeoff in multiple recurrent models, including reservoir computing methods, and comprising real-world ambient intelligence and biomedical time series.},
  archive      = {J_TNNLS},
  author       = {Davide Bacciu and Francesco Crecchi},
  doi          = {10.1109/TNNLS.2019.2899744},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {345-351},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Augmenting recurrent neural networks resilience by dropout},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Associative memories with synaptic delays. <em>TNNLS</em>,
<em>31</em>(1), 331–344. (<a
href="https://doi.org/10.1109/TNNLS.2019.2921143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new concept of associative memories in which synaptic connections of the self-organizing neural network learn time delays between input sequence elements. Synaptic connections represent both the synaptic weights and expected delays between the network inputs. This property of synaptic connections facilitates recognition of time sequences and provides context-based associations between sequence elements. Characteristics of time delays are learned and are updated each time an input sequence is presented. There are no separate learning and testing modes typically used in other neural networks, as the network starts to predict the next input element as soon as there is no expected input signal. The network generates output signals useful for associative recall and prediction. These output signals depend on the presented input context and the knowledge stored in the graph. Such a mode of operation is preferred for the organization of episodic memories used to store the observed episodes and to recall them if a sufficient context is provided. The associative sequential recall is useful for the operation of working memory in a cognitive agent. Test results demonstrate that the network correctly recognizes the input sequences with variable delays and that it is more efficient than other recently developed sequential memory networks based on associative neurons.},
  archive      = {J_TNNLS},
  author       = {Janusz A. Starzyk and Łukasz Maciura and Adrian Horzyk},
  doi          = {10.1109/TNNLS.2019.2921143},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {331-344},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Associative memories with synaptic delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attribute-guided network for cross-modal zero-shot hashing.
<em>TNNLS</em>, <em>31</em>(1), 321–330. (<a
href="https://doi.org/10.1109/TNNLS.2019.2904991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot hashing (ZSH) aims at learning a hashing model that is trained only by instances from seen categories but can generate well to those of unseen categories. Typically, it is achieved by utilizing a semantic embedding space to transfer knowledge from seen domain to unseen domain. Existing efforts mainly focus on single-modal retrieval task, especially image-based image retrieval (IBIR). However, as a highlighted research topic in the field of hashing, cross-modal retrieval is more common in real-world applications. To address the cross-modal ZSH (CMZSH) retrieval task, we propose a novel attribute-guided network (AgNet), which can perform not only IBIR but also text-based image retrieval (TBIR). In particular, AgNet aligns different modal data into a semantically rich attribute space, which bridges the gap caused by modality heterogeneity and zero-shot setting. We also design an effective strategy that exploits the attribute to guide the generation of hash codes for image and text within the same network. Extensive experimental results on three benchmark data sets (AwA, SUN, and ImageNet) demonstrate the superiority of AgNet on both cross-modal and single-modal zero-shot image retrieval tasks.},
  archive      = {J_TNNLS},
  author       = {Zhong Ji and Yuxin Sun and Yunlong Yu and Yanwei Pang and Jungong Han},
  doi          = {10.1109/TNNLS.2019.2904991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {321-330},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attribute-guided network for cross-modal zero-shot hashing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel concept drift detection method for incremental
learning in nonstationary environments. <em>TNNLS</em>, <em>31</em>(1),
309–320. (<a href="https://doi.org/10.1109/TNNLS.2019.2900956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for concept drift detection, based on: 1) the development and continuous updating of online sequential extreme learning machines (OS-ELMs) and 2) the quantification of how much the updated models are modified by the newly collected data. The proposed method is verified on two synthetic case studies regarding different types of concept drift and is applied to two public real-world data sets and a real problem of predicting energy production from a wind plant. The results show the superiority of the proposed method with respect to alternative state-of-the-art concept drift detection methods. Furthermore, updating the prediction model when the concept drift has been detected is shown to allow improving the overall accuracy of the energy prediction model and, at the same time, minimizing the number of model updatings.},
  archive      = {J_TNNLS},
  author       = {Zhe Yang and Sameer Al-Dahidi and Piero Baraldi and Enrico Zio and Lorenzo Montelatici},
  doi          = {10.1109/TNNLS.2019.2900956},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {309-320},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel concept drift detection method for incremental learning in nonstationary environments},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semisupervised text classification by variational
autoencoder. <em>TNNLS</em>, <em>31</em>(1), 295–308. (<a
href="https://doi.org/10.1109/TNNLS.2019.2900734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised text classification has attracted much attention from the research community. In this paper, a novel model, the semisupervised sequential variational autoencoder (SSVAE), is proposed to tackle this problem. By treating the categorical label of unlabeled data as a discrete latent variable, the proposed model maximizes the variational evidence lower bound of the data likelihood, which implicitly derives the underlying label distribution for the unlabeled data. Analytical work indicates that the autoregressive nature of the sequential model is the crucial issue that renders the vanilla model ineffective. To remedy this, two types of decoders are investigated in the SSVAE model and verified. In addition, a reweighting approach is proposed to circumvent the credit assignment problem that occurs during the reconstruction procedure, which can further improve performance for sparse text data. Experimental results show that our method significantly improves the classification accuracy compared with other modern methods.},
  archive      = {J_TNNLS},
  author       = {Weidi Xu and Ying Tan},
  doi          = {10.1109/TNNLS.2019.2900734},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {295-308},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised text classification by variational autoencoder},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A conclusive analysis of the finite-time behavior of the
discretized pursuit learning automaton. <em>TNNLS</em>, <em>31</em>(1),
284–294. (<a href="https://doi.org/10.1109/TNNLS.2019.2900639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the finite-time analysis (FTA) of learning automata (LA), which is a topic for which very little work has been reported in the literature. This is as opposed to the asymptotic steady-state analysis for which there are, probably, scores of papers. As clarified later, unarguably, the FTA of Markov chains, in general, and of LA, in particular, is far more complex than the asymptotic steady-state analysis. Such an FTA provides rigid bounds for the time required for the LA to attain to a given convergence accuracy. We concentrate on the FTA of the Discretized Pursuit Automaton (DPA), which is probably one of the fastest and most accurate reported LA. Although such an analysis was carried out many years ago, we record that the previous work is flawed. More specifically, in all brevity, the flaw lies in the wrongly “derived” monotonic behavior of the LA after a certain number of iterations. Rather, we claim that the property should be invoked is the submartingale property. This renders the proof to be much more involved and deep. In this paper, we rectify the flaw and reestablish the FTA based on such a submartingale phenomenon. More importantly, from the derived analysis, we are able to discover and clarify, for the first time, the underlying dilemma between the DPA&#39;s exploitation and exploration properties. We also nontrivially confirm the existence of the optimal learning rate, which yields a better comprehension of the DPA itself.},
  archive      = {J_TNNLS},
  author       = {Xuan Zhang and Lei Jiao and B. John Oommen and Ole-Christoffer Granmo},
  doi          = {10.1109/TNNLS.2019.2900639},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {284-294},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A conclusive analysis of the finite-time behavior of the discretized pursuit learning automaton},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A time wave neural network framework for solving
time-dependent project scheduling problems. <em>TNNLS</em>,
<em>31</em>(1), 274–283. (<a
href="https://doi.org/10.1109/TNNLS.2019.2900544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the time-dependent project scheduling problem (TPSP). We propose a time wave neural network (TWNN) framework that is able to achieve the global optimal solution (viz., the optimal project schedule) of the TPSP, which is very difficult to obtain using conventional methods (e.g., Dijkstra&#39;s algorithm). The proposed TWNN is a time wave neuron-based neural network without a requirement for any training. In the design of a TWNN, the overall project network of the TPSP is viewed as a neural network, while each node is considered as a wave-based neuron. With this new perspective, the wave-based neuron is constructed based on seven parts: an input, a wave receiver, a neuron state, a time-window selector, a wave generator, a wave sender, and an output. The first three parts are used to receive the waves coming from the predecessor neurons, the fourth part is used to choose the optimal feasible time window, and the remaining three parts are utilized to generate waves for the successive neurons. The main idea of a TWNN is based on the following mechanism: a wave generated from a neuron (node) means that all previous arcs (subprojects) of this neuron have been completed. In particular, the global optimal project scheduling is obtained when a wave is generated by the final destination neuron. To evaluate the performance of a TWNN, the well-known project scheduling problem library data sets are modified and considered in a comparative analysis. Numerical examples are also utilized to demonstrate the robustness of the method.},
  archive      = {J_TNNLS},
  author       = {Wei Huang and Liang Gao},
  doi          = {10.1109/TNNLS.2019.2900544},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {274-283},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A time wave neural network framework for solving time-dependent project scheduling problems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning-based robust tracking control of quadrotor with
time-varying and coupling uncertainties. <em>TNNLS</em>, <em>31</em>(1),
259–273. (<a href="https://doi.org/10.1109/TNNLS.2019.2900510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a learning-based robust tracking control scheme is proposed for a quadrotor unmanned aerial vehicle system. The quadrotor dynamics are modeled including time-varying and coupling uncertainties. By designing position and attitude tracking error subsystems, the robust tracking control strategy is conducted by involving the approximately optimal control of associated nominal error subsystems. Furthermore, an improved weight updating rule is adopted, and neural networks are applied in the learning-based control scheme to get the approximately optimal control laws of the nominal error subsystems. The stability of tracking error subsystems with time-varying and coupling uncertainties is provided as the theoretical guarantee of learning-based robust tracking control scheme. Finally, considering the variable disturbances in the actual environment, three simulation cases are presented based on linear and nonlinear models of quadrotor with competitive results to demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Chaoxu Mu and Yong Zhang},
  doi          = {10.1109/TNNLS.2019.2900510},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {259-273},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning-based robust tracking control of quadrotor with time-varying and coupling uncertainties},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple instance learning for multiple diverse
hyperspectral target characterizations. <em>TNNLS</em>, <em>31</em>(1),
246–258. (<a href="https://doi.org/10.1109/TNNLS.2019.2900465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A practical hyperspectral target characterization task estimates a target signature from imprecisely labeled training data. The imprecisions arise from the characteristics of the real-world tasks. First, accurate pixel-level labels on training data are often unavailable. Second, the subpixel targets and occluded targets cause the training samples to contain mixed data and multiple target types. To address these imprecisions, this paper proposes a new hyperspectral target characterization method to produce diverse multiple hyperspectral target signatures under a multiple instance learning (MIL) framework. The proposed method uses only bag-level training samples and labels, which solves the problems arising from the mixed data and lack of pixel-level labels. Moreover, by formulating a multiple characterization MIL and including a diversity-promoting term, the proposed method can learn a set of diverse target signatures, which solves the problems arising from multiple target types in training samples. The experiments on hyperspectral target detections using the learned multiple target signatures over synthetic and real-world data show the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Ping Zhong and Zhiqiang Gong and Jiaxin Shan},
  doi          = {10.1109/TNNLS.2019.2900465},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {246-258},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiple instance learning for multiple diverse hyperspectral target characterizations},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attack detection and approximation in nonlinear networked
control systems using neural networks. <em>TNNLS</em>, <em>31</em>(1),
235–245. (<a href="https://doi.org/10.1109/TNNLS.2019.2900430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In networked control systems (NCS), a certain class of attacks on the communication network is known to raise traffic flows causing delays and packet losses to increase. This paper presents a novel neural network (NN)-based attack detection and estimation scheme that captures the abnormal traffic flow due to a class of attacks on the communication links within the feedback loop of an NCS. By modeling the unknown network flow as a nonlinear function at the bottleneck node and using a NN observer, the network attack detection residual is defined and utilized to determine the onset of an attack in the communication network when the residual exceeds a predefined threshold. Upon detection, another NN is used to estimate the flow injected by the attack. For the physical system, we develop an attack detection scheme by using an adaptive dynamic programming-based optimal event-triggered NN controller in the presence of network delays and packet losses. Attacks on the network as well as on the sensors of the physical system can be detected and estimated with the proposed scheme. The simulation results confirm theoretical conclusions.},
  archive      = {J_TNNLS},
  author       = {Haifeng Niu and Chandreyee Bhowmick and Sarangapani Jagannathan},
  doi          = {10.1109/TNNLS.2019.2900430},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {235-245},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attack detection and approximation in nonlinear networked control systems using neural networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Further results on adaptive stabilization of high-order
stochastic nonlinear systems subject to uncertainties. <em>TNNLS</em>,
<em>31</em>(1), 225–234. (<a
href="https://doi.org/10.1109/TNNLS.2019.2900339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the adaptive state-feedback control for a class of high-order stochastic nonlinear systems with uncertainties including time-varying delay, unknown control gain, and parameter perturbation. The commonly used growth assumptions on system nonlinearities are removed, and the adaptive control technique is combined with the sign function to deal with the unknown control gain. Then, with the help of the radial basis function neural network approximation approach and Lyapunov-Krasovskii functional, an adaptive state-feedback controller is obtained through the backstepping design procedure. It is verified that the constructed controller can render the closed-loop system semiglobally uniformly ultimately bounded. Finally, both the practical and numerical examples are presented to validate the effectiveness of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Huifang Min and Shengyuan Xu and Jason Gu and Baoyong Zhang and Zhengqiang Zhang},
  doi          = {10.1109/TNNLS.2019.2900339},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {225-234},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Further results on adaptive stabilization of high-order stochastic nonlinear systems subject to uncertainties},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HONN-based adaptive ILC for pure-feedback nonaffine
discrete-time systems with unknown control directions. <em>TNNLS</em>,
<em>31</em>(1), 212–224. (<a
href="https://doi.org/10.1109/TNNLS.2019.2900278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearly all adaptive control techniques require that the control directions of dynamical systems are known in advance. In this paper, for a class of pure-feedback nonaffine discrete-time systems with unknown control directions (UCDs), a high-order neural network (HONN)-based adaptive iterative learning control (ILC) approach is presented to address a repetitive tracking control issue. The implicit function theorem is adopted to cope with the difficulty resulting from the nonaffine structure of control input. Employing a discrete Nussbaum-type function in the neural network weight adaptation law to suit the UCD, an HONN is used to iteratively estimate the ideal control signals. In addition, a novel dead-zone method is developed in the HONN-based adaptive ILC algorithm to enhance its robustness against nonrepetitive desired trajectories and random uncertainties in iterative initial errors and external disturbance. Consequently, the system output, except at the initial n time instants, is demonstrated to asymptotically converge to an adjustable range of the desired trajectory along the iteration axis, while all of the system signals remain bounded during the entire ILC process. Two simulation examples show the feasibility of the adaptive ILC approach.},
  archive      = {J_TNNLS},
  author       = {Qing-Yuan Xu and Xiao-Dong Li},
  doi          = {10.1109/TNNLS.2019.2900278},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {212-224},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HONN-based adaptive ILC for pure-feedback nonaffine discrete-time systems with unknown control directions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The forbidden region self-organizing map neural network.
<em>TNNLS</em>, <em>31</em>(1), 201–211. (<a
href="https://doi.org/10.1109/TNNLS.2019.2900091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-organizing maps (SOMs) are aimed to learn a representation of the input distribution which faithfully describes the topological relations among the clusters of the distribution. For some data sets and applications, it is known beforehand that some regions of the input space cannot contain any samples. Those are known as forbidden regions. In these cases, any prototype which lies in a forbidden region is meaningless. However, previous self-organizing models do not address this problem. In this paper, we propose a new SOM model which is guaranteed to keep all prototypes out of a set of prespecified forbidden regions. Experimental results are reported, which show that our proposal outperforms the SOM both in terms of vector quantization error and quality of the learned topological maps.},
  archive      = {J_TNNLS},
  author       = {Antonio Díaz Ramos and Ezequiel López-Rubio and Esteban J. Palomo},
  doi          = {10.1109/TNNLS.2019.2900091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {201-211},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The forbidden region self-organizing map neural network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging coupled interaction for multimodal alzheimer’s
disease diagnosis. <em>TNNLS</em>, <em>31</em>(1), 186–200. (<a
href="https://doi.org/10.1109/TNNLS.2019.2900077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the population becomes older worldwide, accurate computer-aided diagnosis for Alzheimer&#39;s disease (AD) in the early stage has been regarded as a crucial step for neurodegeneration care in recent years. Since it extracts the low-level features from the neuroimaging data, previous methods regarded this computer-aided diagnosis as a classification problem that ignored latent featurewise relation. However, it is known that multiple brain regions in the human brain are anatomically and functionally interlinked according to the current neuroscience perspective. Thus, it is reasonable to assume that the extracted features from different brain regions are related to each other to some extent. Also, the complementary information between different neuroimaging modalities could benefit multimodal fusion. To this end, we consider leveraging the coupled interactions in the feature level and modality level for diagnosis in this paper. First, we propose capturing the feature-level coupled interaction using a coupled feature representation. Then, to model the modality-level coupled interaction, we present two novel methods: 1) the coupled boosting (CB) that models the correlation of pairwise coupled-diversity on both inconsistently and incorrectly classified samples between different modalities and 2) the coupled metric ensemble (CME) that learns an informative feature projection from different modalities by integrating the intrarelation and interrelation of training samples. We systematically evaluated our methods with the AD neuroimaging initiative data set. By comparison with the baseline learning-based methods and the state-of-the-art methods that are specially developed for AD/MCI (mild cognitive impairment) diagnosis, our methods achieved the best performance with accuracy of 95.0\% and 80.7\% (CB), 94.9\% and 79.9\% (CME) for AD/NC (normal control), and MCI/NC identification, respectively.},
  archive      = {J_TNNLS},
  author       = {Yinghuan Shi and Heung-Il Suk and Yang Gao and Seong-Whan Lee and Dinggang Shen},
  doi          = {10.1109/TNNLS.2019.2900077},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {186-200},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Leveraging coupled interaction for multimodal alzheimer’s disease diagnosis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new timing error cost function for binary time series
prediction. <em>TNNLS</em>, <em>31</em>(1), 174–185. (<a
href="https://doi.org/10.1109/TNNLS.2019.2900046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to make predictions is central to the artificial intelligence problem. While machine learning algorithms have difficulty in learning to predict events with hundreds of time-step dependencies, animals can learn event timing within tens of trials across a broad spectrum of time scales. This suggests strongly a need for new perspectives on the forecasting problem. This paper focuses on binary time series that can be predicted within some temporal precision. We demonstrate that the sum of squared errors (SSE) calculated at every time step is not appropriate for this problem. Next, we look at the advantages and shortcomings of using a dynamic time warping (DTW) cost function. Then, we propose the squared timing error (STE) that uses DTW on the event space and applies SSE on the timing error instead of at each time step. We evaluate all three cost functions on different types of timing errors, such as phase shift, warping, and missing events, on synthetic and real-world binary time series (heartbeats, finance, and music). The results show that STE provides more information about timing error, is differentiable, and can be computed online efficiently. Finally, we devise a gradient descent algorithm for STE on a simplified recurrent neural network. We then compare the performance of the STE-based algorithm to SSE- and logit-based gradient descent algorithms on the same network architecture. The results in real-world binary time series show that the STE algorithm generally outperforms all the other cost functions considered.},
  archive      = {J_TNNLS},
  author       = {François Rivest and Richard Kohar},
  doi          = {10.1109/TNNLS.2019.2900046},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {174-185},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new timing error cost function for binary time series prediction},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Set-membership estimation for complex networks subject to
linear and nonlinear bounded attacks. <em>TNNLS</em>, <em>31</em>(1),
163–173. (<a href="https://doi.org/10.1109/TNNLS.2019.2900045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the set-membership estimation problem for complex networks subject to unknown but bounded attacks. Adversaries are assumed to exist in the nonsecure communication channels from the nodes to the estimators. The transmitted measurements may be modified by an attack function with added noise that is determined by the adversary but unknown to the estimators. A novel set-membership estimation model against unknown but bounded attacks is presented. Two sufficient conditions are derived to guarantee the existence of the set-membership estimators for the cases that the attack functions are linear and nonlinear, respectively. Two strategies for the design of the set-membership estimator gains are presented. The effectiveness of the proposed estimator design method is verified by two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Haiyu Song and Peng Shi and Cheng-Chew Lim and Wen-An Zhang and Li Yu},
  doi          = {10.1109/TNNLS.2019.2900045},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {163-173},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Set-membership estimation for complex networks subject to linear and nonlinear bounded attacks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable digital neuromorphic architecture for large-scale
biophysically meaningful neural network with multi-compartment neurons.
<em>TNNLS</em>, <em>31</em>(1), 148–162. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicompartment emulation is an essential step to enhance the biological realism of neuromorphic systems and to further understand the computational power of neurons. In this paper, we present a hardware efficient, scalable, and real-time computing strategy for the implementation of large-scale biologically meaningful neural networks with one million multi-compartment neurons (CMNs). The hardware platform uses four Altera Stratix III field-programmable gate arrays, and both the cellular and the network levels are considered, which provides an efficient implementation of a large-scale spiking neural network with biophysically plausible dynamics. At the cellular level, a cost-efficient multi-CMN model is presented, which can reproduce the detailed neuronal dynamics with representative neuronal morphology. A set of efficient neuromorphic techniques for single-CMN implementation are presented with all the hardware cost of memory and multiplier resources removed and with hardware performance of computational speed enhanced by 56.59\% in comparison with the classical digital implementation method. At the network level, a scalable network-on-chip (NoC) architecture is proposed with a novel routing algorithm to enhance the NoC performance including throughput and computational latency, leading to higher computational efficiency and capability in comparison with state-of-the-art projects. The experimental results demonstrate that the proposed work can provide an efficient model and architecture for large-scale biologically meaningful networks, while the hardware synthesis results demonstrate low area utilization and high computational speed that supports the scalability of the approach.},
  archive      = {J_TNNLS},
  author       = {Shuangming Yang and Bin Deng and Jiang Wang and Huiyan Li and Meili Lu and Yanqiu Che and Xile Wei and Kenneth A. Loparo},
  doi          = {10.1109/TNNLS.2019.2899936},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {148-162},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scalable digital neuromorphic architecture for large-scale biophysically meaningful neural network with multi-compartment neurons},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-tuning neural predictive control scheme for
ultrabattery to emulate a virtual synchronous machine in autonomous
power systems. <em>TNNLS</em>, <em>31</em>(1), 136–147. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive neural predictive controller (ANPC) is proposed for an ultrabattery energy storage system (UBESS) to enable its operation as a virtual synchronous machine (VSM) in an autonomous wind-diesel power system. The proposed VSM emulates the inertial response and oscillation damping capability of a typical synchronous machine (employed in conventional power plants) by adaptively controlling the power electronic interface of the UBESS. The control objective is to support the network frequency while ensuring efficient/economic use of the UBESS energy. During the load-generation mismatch, ANPC continuously searches for optimal VSM parameters to minimize the actual frequency variations, their rate of change of frequency (ROCOF), and the power flow through the UBESS while maintaining the state of the charge (voltage) of the ultrabattery bank to tackle subsequent disturbances. Simulations confirm that the proposed self-tuning VSM achieves similar performance as that of other VSM control schemes while substantially reducing the power flow through the UBESS and, hence, uses significantly less energy per hertz improvement (in frequency). An index is used to evaluate the performance of the proposed scheme. In addition, the self-tuning VSM has a better dynamic response (quantified as a reduction in ROCOF and settling times) while attenuating the frequency excursions for all simulated cases.},
  archive      = {J_TNNLS},
  author       = {Abdul Saleem Mir and Nilanjan Senroy},
  doi          = {10.1109/TNNLS.2019.2899904},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {136-147},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-tuning neural predictive control scheme for ultrabattery to emulate a virtual synchronous machine in autonomous power systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cell-coupled long short-term memory with <span
class="math inline"><em>L</em></span> -skip fusion mechanism for mood
disorder detection through elicited audiovisual features.
<em>TNNLS</em>, <em>31</em>(1), 124–135. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In early stages, patients with bipolar disorder are often diagnosed as having unipolar depression in mood disorder diagnosis. Because the long-term monitoring is limited by the delayed detection of mood disorder, an accurate and one-time diagnosis is desirable to avoid delay in appropriate treatment due to misdiagnosis. In this paper, an elicitation-based approach is proposed for realizing a one-time diagnosis by using responses elicited from patients by having them watch six emotion-eliciting videos. After watching each video clip, the conversations, including patient facial expressions and speech responses, between the participant and the clinician conducting the interview were recorded. Next, the hierarchical spectral clustering algorithm was employed to adapt the facial expression and speech response features by using the extended Cohn-Kanade and eNTERFACE databases. A denoizing autoencoder was further applied to extract the bottleneck features of the adapted data. Then, the facial and speech bottleneck features were input into support vector machines to obtain speech emotion profiles (EPs) and the modulation spectrum (MS) of the facial action unit sequence for each elicited response. Finally, a cell-coupled long short-term memory (LSTM) network with an L-skip fusion mechanism was proposed to model the temporal information of all elicited responses and to loosely fuse the EPs and the MS for conducting mood disorder detection. The experimental results revealed that the cell-coupled LSTM with the L-skip fusion mechanism has promising advantages and efficacy for mood disorder detection.},
  archive      = {J_TNNLS},
  author       = {Ming-Hsiang Su and Chung-Hsien Wu and Kun-Yi Huang and Tsung-Hsien Yang},
  doi          = {10.1109/TNNLS.2019.2899884},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {124-135},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cell-coupled long short-term memory with $L$ -skip fusion mechanism for mood disorder detection through elicited audiovisual features},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic sleep staging employing convolutional neural
networks and cortical connectivity images. <em>TNNLS</em>,
<em>31</em>(1), 113–123. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding of the neuroscientific sleep mechanisms is associated with mental/cognitive and physical well-being and pathological conditions. A prerequisite for further analysis is the identification of the sleep macroarchitecture through manual sleep staging. Several computer-based approaches have been proposed to extract time and/or frequency-domain features with accuracy ranging from 80\% to 95\% compared with the golden standard of manual staging. However, their acceptability by the medical community is still suboptimal. Recently, utilizing deep learning methodologies increased the research interest in computer-assisted recognition of sleep stages. Aiming to enhance the arsenal of automatic sleep staging, we propose a novel classification framework based on convolutional neural networks. These receive as input synchronizations features derived from cortical interactions within various electroencephalographic rhythms (delta, theta, alpha, and beta) for specific cortical regions which are critical for the sleep deepening. These functional connectivity metrics are then processed as multidimensional images. We also propose to augment the small portion of sleep onset (N1 stage) through the Synthetic Minority Oversampling Technique in order to deal with the great difference in its duration when compared with the remaining sleep stages. Our results (99.85\%) indicate the flexibility of deep learning techniques to learn sleep-related neurophysiological patterns.},
  archive      = {J_TNNLS},
  author       = {Panteleimon Chriskos and Christos A. Frantzidis and Polyxeni T. Gkivogkli and Panagiotis D. Bamidis and Chrysoula Kourtidou-Papadeli},
  doi          = {10.1109/TNNLS.2019.2899781},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {113-123},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic sleep staging employing convolutional neural networks and cortical connectivity images},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reservoir computing universality with stochastic inputs.
<em>TNNLS</em>, <em>31</em>(1), 100–112. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The universal approximation properties with respect to L p -type criteria of three important families of reservoir computers with stochastic discrete-time semi-infinite inputs are shown. First, it is proven that linear reservoir systems with either polynomial or neural network readout maps are universal. More importantly, it is proven that the same property holds for two families with linear readouts, namely, trigonometric state-affine systems and echo state networks, which are the most widely used reservoir systems in applications. The linearity in the readouts is a key feature in supervised machine learning applications. It guarantees that these systems can be used in high-dimensional situations and in the presence of large data sets. The L p criteria used in this paper allow the formulation of universality results that do not necessarily impose almost sure uniform boundedness in the inputs or the fading memory property in the filter that needs to be approximated.},
  archive      = {J_TNNLS},
  author       = {Lukas Gonon and Juan-Pablo Ortega},
  doi          = {10.1109/TNNLS.2019.2899649},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {100-112},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reservoir computing universality with stochastic inputs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3-d learning-enhanced adaptive ILC for iteration-varying
formation tasks. <em>TNNLS</em>, <em>31</em>(1), 89–99. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the formation control problem of repetitive nonlinear homogeneous and asynchronous multiagent networks, where the early starting agent is designated as the parent, and the later starting agent with a small delayed time is designated as the child. Moreover, the desired formation reference is allowed to be different from iteration to iteration. A space-dimensional dynamic linearization method is presented to build the linear dynamic relationship between two parent-child agents in a networked system. Then, a 3-D learning-enhanced adaptive iterative learning control (3D-AILC) is proposed by utilizing the additional control information from previous time instants, iterative operations, and parent agents. In other words, the proposed method processes 3-D dynamics to strengthen its learnability, i.e., time dimension, iteration dimension, and space dimension. The desired formation signal is incorporated into the learning control law to compensate its iterative variation to achieve a fast and precise tracking performance. The proposed 3D-AILC is data based and does not use an explicit mechanistic model. The validity of the proposed approach is proven theoretically and tested through simulations as well. Moreover, the proposed method also works well with time-iteration-varying topologies and nonrepetitive uncertainties.},
  archive      = {J_TNNLS},
  author       = {Yu Hui and Ronghu Chi and Biao Huang and Zhongsheng Hou},
  doi          = {10.1109/TNNLS.2019.2899632},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {89-99},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {3-D learning-enhanced adaptive ILC for iteration-varying formation tasks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-triggered optimal control with performance guarantees
using adaptive dynamic programming. <em>TNNLS</em>, <em>31</em>(1),
76–88. (<a href="https://doi.org/10.1109/TNNLS.2019.2899594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of event-triggered optimal control (ETOC) for continuous-time nonlinear systems and proposes a novel event-triggering condition that enables designing ETOC methods directly based on the solution of the Hamilton-Jacobi-Bellman (HJB) equation. We provide formal performance guarantees by proving a predetermined upper bound. Moreover, we also prove the existence of a lower bound for interexecution time. For implementation purposes, an adaptive dynamic programming (ADP) method is developed to realize the ETOC using a critic neural network (NN) to approximate the value function of the HJB equation. Subsequently, we prove that semiglobal uniform ultimate boundedness can be guaranteed for states and NN weight errors with the ADP-based ETOC. Simulation results demonstrate the effectiveness of the developed ADP-based ETOC method.},
  archive      = {J_TNNLS},
  author       = {Biao Luo and Yin Yang and Derong Liu and Huai-Ning Wu},
  doi          = {10.1109/TNNLS.2019.2899594},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {76-88},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered optimal control with performance guarantees using adaptive dynamic programming},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural network learning controller design for a
class of nonlinear systems with time-varying state constraints.
<em>TNNLS</em>, <em>31</em>(1), 66–75. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies an adaptive neural network (NN) tracking control method for a class of uncertain nonlinear strict-feedback systems with time-varying full-state constraints. As we all know, the states are inevitably constrained in the actual systems because of the safety and performance factors. The main contributions of this paper are that: 1) in order to ensure that the states do not violate the asymmetric time-varying constraint regions, an adaptive NN controller is constructed by introducing the asymmetric time-varying barrier Lyapunov function (TVBLF) and 2) the amount of the learning parameters is reduced by introducing a TVBLF at each step of the backstepping. Based on the Lyapunov stability analysis, it can be proven that all the signals in the closed-loop system are the semiglobal ultimately uniformly bounded and the time-varying full-state constraints are never violated. Finally, a numerical simulation is given, and the effectiveness of this adaptive control method can be verified.},
  archive      = {J_TNNLS},
  author       = {Yan-Jun Liu and Lei Ma and Lei Liu and Shaocheng Tong and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2019.2899589},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {66-75},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network learning controller design for a class of nonlinear systems with time-varying state constraints},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GeCo: Classification restricted boltzmann machine hardware
for on-chip semisupervised learning and bayesian inference.
<em>TNNLS</em>, <em>31</em>(1), 53–65. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probabilistic Bayesian inference of real-time input data is becoming more popular, and the importance of semisupervised learning is growing. We present a classification restricted Boltzmann machine (ClassRBM)-based hardware accelerator with on-chip semisupervised learning and Bayesian inference capability. ClassRBM is a specific type of Markov network that can perform classification tasks and reconstruct its input data. ClassRBM has several advantages in terms of hardware implementation compared to other backpropagation-based neural networks. However, its accuracy is relatively low compared to backpropagation-based learning. To improve the accuracy of ClassRBM, we propose the multi-neuron-per-class (multi-NPC) voting scheme. We also reveal that the contrastive divergence (CD) algorithm, which is commonly used to train RBM, shows poor performance in this multi-NPC ClassRBM. As an alternative, we propose an asymmetric contrastive divergence (ACD) training algorithm that improves the accuracy of multi-NPC ClassRBM. With the ACD learning algorithm, ClassRBM operates in the form of a combination of Markov Chain training and Bayesian inference. The experimental results on a field-programmable gate array (FPGA) board for a Modified National Institute of Standards and Technology data set confirm that the inference accuracy of the proposed ACD algorithm is 5.82\% higher for a supervised learning case and 12.78\% higher for a 1\% labeled semisupervised learning case than the conventional CD algorithm. Also, the GeCo ver.2 hardware implemented on a Xilinx ZCU102 FPGA board was 349.04 times faster than the C simulation on CPU.},
  archive      = {J_TNNLS},
  author       = {Wooseok Yi and Junki Park and Jae-Joon Kim},
  doi          = {10.1109/TNNLS.2019.2899386},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {53-65},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GeCo: Classification restricted boltzmann machine hardware for on-chip semisupervised learning and bayesian inference},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified entropy-based distance metric for
ordinal-and-nominal-attribute data clustering. <em>TNNLS</em>,
<em>31</em>(1), 39–52. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal data are common in many data mining and machine learning tasks. Compared to nominal data, the possible values (also called categories interchangeably) of an ordinal attribute are naturally ordered. Nevertheless, since the data values are not quantitative, the distance between two categories of an ordinal attribute is generally not well defined, which surely has a serious impact on the result of the quantitative analysis if an inappropriate distance metric is utilized. From the practical perspective, ordinal-and-nominal-attribute categorical data, i.e., categorical data associated with a mixture of nominal and ordinal attributes, is common, but the distance metric for such data has yet to be well explored in the literature. In this paper, within the framework of clustering analysis, we therefore first propose an entropy-based distance metric for ordinal attributes, which exploits the underlying order information among categories of an ordinal attribute for the distance measurement. Then, we generalize this distance metric and propose a unified one accordingly, which is applicable to ordinal-and-nominal-attribute categorical data. Compared with the existing metrics proposed for categorical data, the proposed metric is simple to use and nonparametric. More importantly, it reasonably exploits the underlying order information of ordinal attributes and statistical information of nominal attributes for distance measurement. Extensive experiments show that the proposed metric outperforms the existing counterparts on both the real and benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Yiqun Zhang and Yiu-Ming Cheung and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2019.2899381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {39-52},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A unified entropy-based distance metric for ordinal-and-nominal-attribute data clustering},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatially arranged sparse recurrent neural networks for
energy efficient associative memory. <em>TNNLS</em>, <em>31</em>(1),
24–38. (<a href="https://doi.org/10.1109/TNNLS.2019.2899344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of hardware neural networks, including neuromorphic hardware, has been accelerated over the past few years. However, it is challenging to operate very large-scale neural networks with low-power hardware devices, partly due to signal transmissions through a massive number of interconnections. Our aim is to deal with the issue of communication cost from an algorithmic viewpoint and study learning algorithms for energy-efficient information processing. Here, we consider two approaches to finding spatially arranged sparse recurrent neural networks with the high cost-performance ratio for associative memory. In the first approach following classical methods, we focus on sparse modular network structures inspired by biological brain networks and examine their storage capacity under an iterative learning rule. We show that incorporating long-range intermodule connections into purely modular networks can enhance the cost-performance ratio. In the second approach, we formulate for the first time an optimization problem where the network sparsity is maximized under the constraints imposed by a pattern embedding condition. We show that there is a tradeoff between the interconnection cost and the computational performance in the optimized networks. We demonstrate that the optimized networks can achieve a better cost-performance ratio compared with those considered in the first approach. We show the effectiveness of the optimization approach mainly using binary patterns and apply it also to gray-scale image restoration. Our results suggest that the presented approaches are useful in seeking more sparse and less costly connectivity of neural networks for the enhancement of energy efficiency in hardware neural networks.},
  archive      = {J_TNNLS},
  author       = {Gouhei Tanaka and Ryosho Nakane and Tomoya Takeuchi and Toshiyuki Yamane and Daiju Nakano and Yasunao Katayama and Akira Hirose},
  doi          = {10.1109/TNNLS.2019.2899344},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {24-38},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatially arranged sparse recurrent neural networks for energy efficient associative memory},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neuromemristive circuits for edge computing: A review.
<em>TNNLS</em>, <em>31</em>(1), 4–23. (<a
href="https://doi.org/10.1109/TNNLS.2019.2899262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volume, veracity, variability, and velocity of data produced from the ever increasing network of sensors connected to Internet pose challenges for power management, scalability, and sustainability of cloud computing infrastructure. Increasing the data processing capability of edge computing devices at lower power requirements can reduce several overheads for cloud computing solutions. This paper provides the review of neuromorphic CMOS-memristive architectures that can be integrated into edge computing devices. We discuss why the neuromorphic architectures are useful for edge devices and show the advantages, drawbacks, and open problems in the field of neuromemristive circuits for edge computing.},
  archive      = {J_TNNLS},
  author       = {Olga Krestinskaya and Alex Pappachen James and Leon Ong Chua},
  doi          = {10.1109/TNNLS.2019.2899262},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {4-23},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuromemristive circuits for edge computing: A review},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial: Another successful year and looking forward to
2020. <em>TNNLS</em>, <em>31</em>(1), 2–3. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Happy New Year!” As you open this January issue of the IEEE Transactions on Neural Networks and Learning Systems (TNNLS), I hope everyone enjoyed a great holiday season and is excited for the new year of 2020.},
  archive      = {J_TNNLS},
  author       = {Haibo He},
  doi          = {10.1109/TNNLS.2019.2957621},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {2-3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Editorial: Another successful year and looking forward to 2020},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
