<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc---144">TC - 144</h2>
<ul>
<li><details>
<summary>
(2020). Analysis and efficient implementations of a class of
composited de bruijn sequences. <em>TC</em>, <em>69</em>(12), 1835–1848.
(<a href="https://doi.org/10.1109/TC.2020.2979460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A binary de Bruijn sequence is a sequence of period 2n in which every binary n-tuple occurs exactly once in each period. A de Bruijn sequence has good randomness properties, such as long period, ideal tuple distribution, and high linear complexity, and can be generated by a nonlinear feedback shift register (NLFSR). Finding an efficient NLFSR that can generate a de Bruijn sequence with a long period is a significant challenge. “Composited construction” is a technique for constructing a de Bruijn sequence of period 2 n+k by an NLFSR from a de Bruijn sequence of period 2 n through a composition operation repeatedly applying k times. The goal of this article is to further investigate the composited construction of de Bruijn sequences with efficient hardware implementations, and determine randomness properties such as linear complexity. Our contributions in this article are as follows. First, we present a generalized construction of composited de Bruijn sequences that is constructed by adding a combination of conjugate pairs of different lengths in the feedback function of the composited construction, which results in generating a class of de Bruijn sequences of size 2 k , whereas the original composited construction can generate only two sequences. Second, we investigate the linear complexity and the correlation property of the new class of de Bruijn sequences. We prove theoretically that the linear complexity of this class of de Bruijn sequences is optimal or close to optimal. Interestingly, we also prove that the linear complexities of all the sequences of this class are equal, which strengthens Etzion&#39;s conjecture (JCTA 1985, IEEE-IT 1999) about the number of de Bruijn sequences with equal linear complexity. This is the first known construction of de Bruijn sequences of an arbitrarily long period whose linear complexities are determined theoretically. Finally, we implement our construction in hardware to demonstrate its practicality. We synthesize our implementations for a 65 nm ASIC and a Xilinx Spartan FPGA and present hardware areas, and performances of de Bruijn sequences of periods in the range of 2 160 to 2 1056 . For instance, a class of de Bruijn sequences of period 2 160 (resp. 2 288 ) can be implemented with an area of 3.43 (resp. 6.71) kGEs in 65 nm ASIC, and 83 (resp. 229) slices in Spartan6 FPGA.},
  archive      = {J_TC},
  author       = {Kalikinkar Mandal and Bo Yang and Guang Gong and Mark Aagaard},
  doi          = {10.1109/TC.2020.2979460},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1835-1848},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Analysis and efficient implementations of a class of composited de bruijn sequences},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STFL-DDR: Improving the energy-efficiency of memory
interface. <em>TC</em>, <em>69</em>(12), 1823–1834. (<a
href="https://doi.org/10.1109/TC.2020.2978826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power dissipation is a significant problem limiting the performance of today&#39;s computer systems. One of the main contributors to power consumption in microprocessors is data movement in cache and memory interface. Several solutions such as low power interconnects, energy-aware data encoding, and low power signaling have been proposed to mitigate this problem. Almost all of these techniques result in a significant system performance degradation. This article examines the application of a novel technique, called STFL-DDR, for hybrid signaling on low-power DRAM interface. To keep the power consumption low, STFL-DDR employs a high-performance clock rate for transferring data on low power wires. To avoid any signal deterioration, STFL-DDR employs data encoding/decoding to prevent each wire from switching in any two consecutive cycles. STFL-DDR creates new opportunities for optimizing the energy-efficiency of DRAM systems. We compare the efficiency of STFL-DDR with the state-of-the-art methods by simulating a mix of 12 parallel benchmark applications on a muticore system. Our simulation results indicate that STFL can reduce the energy consumption of a contemporary DRAM interface by 17 percent as compared to an LPDDR baseline while achieving the throughput of a high-performance DRAM. Applying STFL to both last level cache and DRAM interface results in improving the system energy, energy-delay product, and performance by 8, 15, and 9 percent respectively. Compared with a high-performance memory interface, STFL improves the system energy and energy-delay product by 25 and 75 percent, while reaching 98 percent of the average performance of the high-performance system.},
  archive      = {J_TC},
  author       = {Payman Behnam and Mahdi Nazm Bojnordi},
  doi          = {10.1109/TC.2020.2978826},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1823-1834},
  shortjournal = {IEEE Trans. Comput.},
  title        = {STFL-DDR: Improving the energy-efficiency of memory interface},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving SSD read latency via coding. <em>TC</em>,
<em>69</em>(12), 1809–1822. (<a
href="https://doi.org/10.1109/TC.2020.2978823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the potential enhancement of the read access speed in high-performance solid-state drives (SSDs) by coding, given speed variations across the multiple flash interfaces and assuming occasional local memory failures. Our analysis is based on a queuing model that incorporates both read request failures and NAND element failures. The NAND element failure in the present context reflects various limitations on the memory element level such as bad blocks, dies or chips that cannot be corrected by error control coding (ECC) typically employed to protect pages read off the NAND cells. Our analysis provides a clear picture of the storage-overhead and read-latency trade-offs given read failures and NAND element failures. We investigate two different ways to mitigate the effect of NAND element failures using the notion of multi-class jobs with different priorities. A strong motivation for this work is to understand the reliability requirement of NAND chip components given an additional layer of failure protection, under the latency/storage-overhead constraints.},
  archive      = {J_TC},
  author       = {Hyegyeong Park and Jaekyun Moon},
  doi          = {10.1109/TC.2020.2978823},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1809-1822},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improving SSD read latency via coding},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LUTNet: Learning FPGA configurations for highly efficient
neural network inference. <em>TC</em>, <em>69</em>(12), 1795–1808. (<a
href="https://doi.org/10.1109/TC.2020.2978817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research has shown that deep neural networks contain significant redundancy, and thus that high classification accuracy can be achieved even when weights and activations are quantized down to binary values. Network binarization on FPGAs greatly increases area efficiency by replacing resource-hungry multipliers with lightweight XNOR gates. However, an FPGA&#39;s fundamental building block, the K-LUT, is capable of implementing far more than an XNOR: it can perform any K-input Boolean operation. Inspired by this observation, we propose LUTNet, an end-to-end hardware-software framework for the construction of area-efficient FPGA-based neural network accelerators using the native LUTs as inference operators. We describe the realization of both unrolled and tiled LUTNet architectures, with the latter facilitating smaller, less power-hungry deployment over the former while sacrificing area and energy efficiency along with throughput. For both varieties, we demonstrate that the exploitation of LUT flexibility allows for far heavier pruning than possible in prior works, resulting in significant area savings while achieving comparable accuracy. Against the state-of-the-art binarized neural network implementation, we achieve up to twice the area efficiency for several standard network models when inferencing popular datasets. We also demonstrate that even greater energy efficiency improvements are obtainable.},
  archive      = {J_TC},
  author       = {Erwei Wang and James J. Davis and Peter Y. K. Cheung and George A. Constantinides},
  doi          = {10.1109/TC.2020.2978817},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1795-1808},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LUTNet: Learning FPGA configurations for highly efficient neural network inference},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved basic block reordering. <em>TC</em>,
<em>69</em>(12), 1784–1794. (<a
href="https://doi.org/10.1109/TC.2020.2982888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basic block reordering is an important step for profile-guided binary optimization. The state-of-the-art goal for basic block reordering is to maximize the number of fall-through branches. However, we demonstrate that such orderings may impose suboptimal performance on instruction and I-TLB caches. We propose a new algorithm that relies on a model combining the effects of fall-through and caching behavior. As details of modern processor caching is quite complex and often unknown, we show how to use machine learning in selecting parameters that best trade off different caching effects to maximize binary performance. An extensive evaluation on a variety of applications, including Facebook production workloads, the open-source compilers Clang and GCC, and SPEC CPU benchmarks, indicate that the new method outperforms existing block reordering techniques, improving the resulting performance of applications with large code size. We have open sourced the code of the new algorithm as a part of a post-link binary optimization tool, BOLT.},
  archive      = {J_TC},
  author       = {Andy Newell and Sergey Pupyrev},
  doi          = {10.1109/TC.2020.2982888},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1784-1794},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improved basic block reordering},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tight interval inclusions with compensated algorithms.
<em>TC</em>, <em>69</em>(12), 1774–1783. (<a
href="https://doi.org/10.1109/TC.2019.2924005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compensated algorithms consist in computing the rounding errors of individual operations and then adding them later on to the computed result. This makes it possible to increase the accuracy of the computed result efficiently. Computing the rounding error of an individual operation is possible through the use of a so-called error-free transformation. In this article, we show that it is possible to use compensated algorithms for having tight interval inclusions. We study compensated algorithms for summation, dot product and polynomial evaluation. We prove that the use of directed rounding makes it possible to get narrow inclusions with compensated algorithms. This is due to the fact that error-free transformations are no more exact but still sufficiently accurate to improve the numerical quality of results.},
  archive      = {J_TC},
  author       = {Stef Graillat and Fabienne Jézéquel},
  doi          = {10.1109/TC.2019.2924005},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1774-1783},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tight interval inclusions with compensated algorithms},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New low-area designs for the AES forward, inverse and
combined s-boxes. <em>TC</em>, <em>69</em>(12), 1757–1773. (<a
href="https://doi.org/10.1109/TC.2019.2922601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The implementation of AES S-boxes is one of the most extensively studied areas of cryptography. In this paper, we propose three new hardware designs for the AES S-box that can serve in the forward, inverse and combined data paths. Each of these designs represents the smallest AES S-box ever proposed in its respective category. We achieve this goal by using new tower field representation over normal bases and optimizing each and every block inside the three proposed architectures. Our complexity analysis and ASIC synthesis results in the CMOS STM 65 nm, as well as the NanGate 15 nm technologies, show that our designs outperform their counterparts in terms of area and power.},
  archive      = {J_TC},
  author       = {Arash Reyhani-Masoleh and Mostafa Taha and Doaa Ashmawy},
  doi          = {10.1109/TC.2019.2922601},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1757-1773},
  shortjournal = {IEEE Trans. Comput.},
  title        = {New low-area designs for the AES forward, inverse and combined S-boxes},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Round-off error and exceptional behavior analysis of
explicit runge-kutta methods. <em>TC</em>, <em>69</em>(12), 1745–1756.
(<a href="https://doi.org/10.1109/TC.2019.2917902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical integration schemes are mandatory to understand complex behaviors of dynamical systems described by ordinary differential equations. Implementation of these numerical methods involve floating-point computations and propagation of round-off errors. This paper presents a new fine-grained analysis of round-off errors in explicit Runge-Kutta integration methods, taking into account exceptional behaviors, such as underflow and overflow. Linear stability properties play a central role in the proposed approach. For a large class of Runge-Kutta methods applied on linear problems, a tight bound of the round-off errors is provided. A simple test is defined and ensures the absence of underflow and a tighter round-off error bound. The absence of overflow is guaranteed as linear stability properties imply that (computed) solutions are non-increasing.},
  archive      = {J_TC},
  author       = {Sylvie Boldo and Florian Faissole and Alexandre Chapoutot},
  doi          = {10.1109/TC.2019.2917902},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1745-1756},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Round-off error and exceptional behavior analysis of explicit runge-kutta methods},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New results on non-normalized floating-point formats.
<em>TC</em>, <em>69</em>(12), 1733–1744. (<a
href="https://doi.org/10.1109/TC.2019.2929039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compulsory normalization of the represented numbers is a key requirement of the floating-point standard. This requirement contributes to fundamental characteristics of the standard, such as taking the most of the precision available, reproducibility and facilitation of comparison and other operations. However, it also imposes a high restriction in effectiveness of basic arithmetic operation implementation. In many embedded applications may be worth to sacrifice the benefits of normalization for gaining in implementation metrics. This paper analyzes and measures the effect of removing the normalization requirement in terms of precision and implementation savings for embedded applications. We propose several adder and multiplier architectures to deal with non-normalized floating-point numbers, and quantify the accuracy loss and the improvements in hardware implementation. Our experiments show that it is possible to reduce the area and power consumption up to 78 percent in ASIC and 50 percent in FPGA implementations with a reasonable accuracy loss.},
  archive      = {J_TC},
  author       = {Sonia González-Navarro and Javier Hormigo},
  doi          = {10.1109/TC.2019.2929039},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1733-1744},
  shortjournal = {IEEE Trans. Comput.},
  title        = {New results on non-normalized floating-point formats},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implementing the residue logarithmic number system using
interpolation and cotransformation. <em>TC</em>, <em>69</em>(12),
1719–1732. (<a href="https://doi.org/10.1109/TC.2019.2930514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Residue Logarithmic Number System (RLNS) offers fast multiplication and division, but poses challenges for implementing addition and subtraction because the underlying integer Residue Number System (RNS) has slow sign detection. The conventional Binary Logarithmic Number Systems (BLNS) has benefited from interpolation and cotransformation. We propose a dual-path ALU that speculates about the sign detection to adapt interpolation and cotransformation to the limitations of RLNS. Synthesis shows for the same precision and technology, the area of the proposed RLNS circuit is similar to BLNS and much smaller than prior RLNS methods. We also compare against Floating Point (FP).},
  archive      = {J_TC},
  author       = {Mark G. Arnold and Vassilis Paliouras and Ioannis Kouretas},
  doi          = {10.1109/TC.2019.2930514},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {1719-1732},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Implementing the residue logarithmic number system using interpolation and cotransformation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Elliptic curve cryptography point multiplication core for
hardware security module. <em>TC</em>, <em>69</em>(11), 1707–1718. (<a
href="https://doi.org/10.1109/TC.2020.3013266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s technology, a sheer number of Internet of Things applications use hardware security modules for secure communications. The widely used algorithms in security modules, for example, digital signatures and key agreement, are based upon elliptic curve cryptography (ECC). A core operation used in ECC is the point multiplication, which is computationally expensive for many Internet of things applications. In many IoT applications, such as intelligent transportation systems and distributed control systems, thousands of safety messages need to be signed and verified within a very short time-frame. Considerable research has been conducted in the design of a fast elliptic curve arithmetic on finite fields using residue number systems (RNS). In this article, we propose an RNS-based ECC core hardware for the two families of elliptic curves that are short Weierstraß and twisted Edwards curves. Specifically, we present RNS implementations for SECP256K1 and ED25519 standard curves. We propose an RNS hardware architecture supporting fast elliptic curve point-addition (ECPA), point-doubling (ECPD), and point-tripling (ECPT). We implemented different ECC point multiplication algorithms on the Xilinx FPGA platform. The test results confirm that the performance of our fully RNS ECC point multiplication is better than the fastest ECC point multiplication cores in the literature.},
  archive      = {J_TC},
  author       = {Mohamad Ali Mehrabi and Christophe Doche and Alireza Jolfaei},
  doi          = {10.1109/TC.2020.3013266},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1707-1718},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Elliptic curve cryptography point multiplication core for hardware security module},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A lightweight detection algorithm for collision-optimized
divide-and-conquer attacks. <em>TC</em>, <em>69</em>(11), 1694–1706. (<a
href="https://doi.org/10.1109/TC.2020.3002795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By introducing collision information into divide-and-conquer attacks, several existing works transform the original candidate space, which may be too large to enumerate, into a significantly smaller collision space, thus making key recovery possible. However, the inefficient collision detection algorithms and fault tolerance mechanisms make them time-consuming and their success rate low. Moreover, they may still leave very huge chain spaces that makes it difficult for key recovery. In this article, we exploit collision attack to optimize Template Attack (TA), and propose a Lightweight Collision Detection (LCD) algorithm. The proposed method exploits a jump detection mechanism to efficiently reduce the repetitive collision detections on chains with the same prefix sub-chains. We then introduce guessing theory to reorder the collision detection of the sub-keys according to their guessing lengths, and provide us with an evaluation tool. Finally, we design a highly efficient fault tolerance mechanism for our LCD to allow flexible thresholds adjustment, and further optimize sieving mechanism to efficiently extract the best chains with the largest number of collisions. Experimental results fully demonstrate LCD&#39;s superiority.},
  archive      = {J_TC},
  author       = {Changhai Ou and Siew-Kei Lam and Chengju Zhou and Guiyuan Jiang and Fan Zhang},
  doi          = {10.1109/TC.2020.3002795},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1694-1706},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A lightweight detection algorithm for collision-optimized divide-and-conquer attacks},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Side-channel analysis and countermeasure design on ARM-based
quantum-resistant SIKE. <em>TC</em>, <em>69</em>(11), 1681–1693. (<a
href="https://doi.org/10.1109/TC.2020.3020407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The implementations of post-quantum cryptographic algorithms have been newly explored, whereas, the protection against side-channel attacks shall be considered upfront, since it can have a non-negligible impact on security and performance. In this article, the security of supersingular isogeny key encapsulation (SIKE), a second-round candidate of NIST&#39;s on-going post-quantum standardization process, is thoroughly evaluated under side-channel analysis. First, the vulnerabilities of reference and optimized implementations of SIKE are thoroughly analyzed in terms of both horizontal and vertical side-channel leakage. After the optimized SIKE, which is based on Three-point Montgomery Differential Ladder algorithm, is proved to be constant-time and there is no horizontal leakage, a vertical vulnerability is analyzed based on the source code at the algorithmic level, and a theoretical differential power analysis (DPA) attack is proposed. In order to exploit this vulnerability, the differential electromagnetic attack (DEMA) is put into practice to extract the private key of SIKE based on a 32-bit ARM platform. To the best of our knowledge, this is the first practical side-channel attack at SIKE implemented on real ARM-based devices. Our experiments show that the DEMA needs only hundreds of electromagnetic traces to carry out the attack. More importantly, an efficient window-based countermeasure is proposed to eliminate the vertical leakage and prevent side-channel attacks with only a little overhead. The security of our countermeasure is carefully evaluated against most of well-known power analysis attacks. Through careful evaluation and comparison with other countermeasures, this method can lead to higher security at a very small cost in terms of time and memory.},
  archive      = {J_TC},
  author       = {Fan Zhang and Bolin Yang and Xiaofei Dong and Sylvain Guilley and Zhe Liu and Wei He and Fangguo Zhang and Kui Ren},
  doi          = {10.1109/TC.2020.3020407},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1681-1693},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Side-channel analysis and countermeasure design on ARM-based quantum-resistant SIKE},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hardware-based architecture-neutral framework for
real-time IoT workload forensics. <em>TC</em>, <em>69</em>(11),
1668–1680. (<a href="https://doi.org/10.1109/TC.2020.3000237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beneath the potential benefits of the rapidly growing Internet of Things (IoT) technology lurk security risks. In this article, we propose a hardware-based generic framework for IoT workload forensics, an infrastructural technique to securely monitor and ensure delivered IoT services in accordance with specifications and regulatory compliance. In particular, this technique identifies digital workloads being executed in real time through dynamic program behavior modeling based on architecture-level data, fulfilled by dedicated machine learning hardware, without the intervention of high-level software, e.g., the OS and/or the hypervisor. In contrast to the conventional software-based solutions, whose effectiveness may be undermined by software attacks, and which introduce significant runtime overhead, a hardware-based framework enables a secure, prompt and non-intrusive solution. The proposed framework was evaluated on Zedboard, a Zynq-7000 FPGA embedding an ARM Cortex-A9 core. Experimental results using Mibench workload benchmark reveal an average workload identification accuracy of 96.37 percent with insignificant area/power overhead.},
  archive      = {J_TC},
  author       = {Liwei Zhou and Yang Hu and Yiorgos Makris},
  doi          = {10.1109/TC.2020.3000237},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1668-1680},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A hardware-based architecture-neutral framework for real-time IoT workload forensics},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MTHAEL: Cross-architecture IoT malware detection based on
neural network advanced ensemble learning. <em>TC</em>, <em>69</em>(11),
1654–1667. (<a href="https://doi.org/10.1109/TC.2020.3015584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity, sophistication, and impact of malware evolve with industrial revolution and technology advancements. This article discusses and proposes a robust cross-architecture IoT malware threat hunting model based on advanced ensemble learning (MTHAEL). Our unique MTHAEL model using stacked ensemble of heterogeneous feature selection algorithms and state-of-the-art neural networks to learn different levels of semantic features demonstrates enhanced IoT malware detection than existing approaches. MTHAEL is the first of its kind that effectively optimizes recurrent neural network (RNN) and convolutional neural network (CNN) with high classification accuracy and consistently low computational overheads on different IoT architectures. Cross-architecture benchmarking is performed during the training with different architectures such as ARM, Intel80386, MIPS, and MIPS+Intel80386 individually. Two different hardware architectures were employed to analyze the architecture overhead, namely Raspberry Pi 4 (ARM-based architecture) and Core-i5 (Intel-based architecture). Our proposed MTHAEL is evaluated comprehensively with a large IoT cross-architecture dataset of 21,137 samples and has achieved 99.98 percent classification accuracy for ARM architecture samples, surpassing prior related works. Overall, MTHAEL has demonstrated practical suitability for cross-architecture IoT malware detection with low computational overheads requiring only 0.32 seconds to detect Any IoT malware.},
  archive      = {J_TC},
  author       = {Danish Vasan and Mamoun Alazab and Sitalakshmi Venkatraman and Junaid Akram and Zheng Qin},
  doi          = {10.1109/TC.2020.3015584},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1654-1667},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MTHAEL: Cross-architecture IoT malware detection based on neural network advanced ensemble learning},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Instruction sequence identification and disassembly using
power supply side-channel analysis. <em>TC</em>, <em>69</em>(11),
1639–1653. (<a href="https://doi.org/10.1109/TC.2020.3018092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded systems are prone to leak information via side-channels associated with their physical internal activity, such as power consumption, timing, and faults. Leaked information can be analyzed to extract sensitive data and devices should be assessed for such vulnerabilities. Side-channel power-supply leakage from embedded devices can also provide information regarding instruction-level activity for control code executed on these devices. Methods proposed to disassemble instruction-level activity via side-channel leakage have not addressed issues related to pipelined multi-clock-cycle architectures, nor have proven robustness or reliability. The problem of detecting malicious code modifications while not obstructing the sequence of instructions being executed needs to be addressed. In this article, instruction sequences being executed on a general-purpose pipelined computing platform are identified and instructions that make up these sequences are classified based on hardware utilization. Individual instruction classification results using a fine-grained classifier is also presented. A dynamic programming algorithm was applied to detect the boundaries of instructions in a sequence with a 100 percent accuracy. A unique aspect of this technique is the use of multiple power supply pin measurements to increase precision and accuracy. To demonstrate the robustness of this technique, power leakage data from ten target FPGAs programmed with a prototype of the pipelined architecture was analyzed and classification accuracies averaging 99 percent were achieved with instructions labeled based on hardware utilization. Individual instruction classification accuracies above 90 percent were achieved using a fine-grained classifier. Classification accuracies were also verified when a target FPGA was subjected to different controlled temperatures. The classification accuracies on discrete (ASIC) pipelined-architecture microcontrollers was 97 percent.},
  archive      = {J_TC},
  author       = {Deepak Krishnankutty and Zheng Li and Ryan Robucci and Nilanjan Banerjee and Chintan Patel},
  doi          = {10.1109/TC.2020.3018092},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1639-1653},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Instruction sequence identification and disassembly using power supply side-channel analysis},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SCAUL: Power side-channel analysis with unsupervised
learning. <em>TC</em>, <em>69</em>(11), 1626–1638. (<a
href="https://doi.org/10.1109/TC.2020.3013196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing power analysis techniques rely on strong adversary models with prior knowledge of the leakage or training data. We introduce side-channel analysis with unsupervised learning (SCAUL) that can recover the secret key without requiring prior knowledge or profiling (training). We employ an LSTM auto-encoder to extract features from power traces with high mutual information with the data-dependent samples of the measurements. We demonstrate that by replacing the raw measurements with the auto-encoder features in a classical DPA attack, the efficiency, in terms of required number of measurements for key recovery, improves by 10X. Further, we employ these features to identify a leakage model with sensitivity analysis and multi-layer perceptron (MLP) networks. SCAUL uses the auto-encoder features and the leakage model, obtained in an unsupervised approach, to find the correct key. On a lightweight implementation of AES on Artix-7 FPGA, we show that SCAUL is able to recover the correct key with 3,700 power measurements with random plaintexts, while a DPA attack requires at least 17,400 measurements. Using misaligned traces, with an uncertainty equal to 20 percent of the hardware clock cycle, SCAUL is able to recover the secret key with 12,300 measurements while the DPA attack fails to detect the key.},
  archive      = {J_TC},
  author       = {Keyvan Ramezanpour and Paul Ampadu and William Diehl},
  doi          = {10.1109/TC.2020.3013196},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1626-1638},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SCAUL: Power side-channel analysis with unsupervised learning},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 2.5D root of trust: Secure system-level integration of
untrusted chiplets. <em>TC</em>, <em>69</em>(11), 1611–1625. (<a
href="https://doi.org/10.1109/TC.2020.3020777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the first time, we leverage the 2.5D interposer technology to establish system-level security in the face of hardware- and software-centric adversaries. More specifically, we integrate chiplets (i.e., third-party hard intellectual property of complex functionality, like microprocessors) using a security-enforcing interposer. Such hardware organization provides a robust 2.5D root of trust for trustworthy, yet powerful and flexible, computation systems. The security paradigms for our scheme, employed firmly by design and construction, are: 1) stringent physical separation of trusted from untrusted components and 2) runtime monitoring. The system-level activities of all untrusted commodity chiplets are checked continuously against security policiesvia physically separated security features. Aside from the security promises, the good economics of outsourced supply chains are still maintained; the system vendor is free to procure chiplets from the open market, while only producing the interposer and assembling the 2.5D system oneself. We showcase our scheme using the Cortex-M0 core and the AHB-Lite bus by ARM, building a secure 64-core system with shared memories. We evaluate our scheme through hardware simulation, considering different threat scenarios. Finally, we devise a physical-design flow for 2.5D systems, based on commercial-grade design tools, to demonstrate and evaluate our 2.5D root of trust.},
  archive      = {J_TC},
  author       = {Mohammed Nabeel and Mohammed Ashraf and Satwik Patnaik and Vassos Soteriou and Ozgur Sinanoglu and Johann Knechtel},
  doi          = {10.1109/TC.2020.3020777},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1611-1625},
  shortjournal = {IEEE Trans. Comput.},
  title        = {2.5D root of trust: Secure system-level integration of untrusted chiplets},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling secure NVM-based in-memory neural network computing
by sparse fast gradient encryption. <em>TC</em>, <em>69</em>(11),
1596–1610. (<a href="https://doi.org/10.1109/TC.2020.3017870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network (NN) computing is energy-consuming on traditional computing systems, owing to the inherent memory wall bottleneck of the von Neumann architecture and the Moore&#39;s Law being approaching the end. Non-volatile memories (NVMs) have been demonstrated as promising alternatives for constructing computing-in-memory (CIM) systems to accelerate NN computing. However, NVM-based NN computing systems are vulnerable to the confidentiality attacks because the weight parameters persist in memory when the system is powered off, enabling an adversary with physical access to extract the well-trained NN models. The goal of this article is to find a solution for thwarting the confidentiality attacks. We define and model the weight encryption problem. Then we propose an effective framework, containing a sparse fast gradient encryption (SFGE) method and a runtime encryption scheduling (RES) scheme, to guarantee the confidentiality security of NN models with a negligible performance overhead. Moreover, we improve the SFGE method by incrementally generating the encryption keys. Additionally, we provide variants of the encryption method to better fit quantized models and various mapping strategies. The experiments demonstrate that only encrypting an extremely small proportion of the weights (e.g., 20 weights per layer in ResNet-101), the NN models can be strictly protected.},
  archive      = {J_TC},
  author       = {Yi Cai and Xiaoming Chen and Lu Tian and Yu Wang and Huazhong Yang},
  doi          = {10.1109/TC.2020.3017870},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1596-1610},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling secure NVM-based in-memory neural network computing by sparse fast gradient encryption},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding selective delay as a method for efficient
secure speculative execution. <em>TC</em>, <em>69</em>(11), 1584–1595.
(<a href="https://doi.org/10.1109/TC.2020.3014456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the introduction of Meltdown and Spectre, the research community has been tirelessly working on speculative side-channel attacks and on how to shield computer systems from them. To ensure that a system is protected not only from all the currently known attacks but also from future, yet to be discovered, attacks, the solutions developed need to be general in nature, covering a wide array of system components, while at the same time keeping the performance, energy, area, and implementation complexity costs at a minimum. One such solution is our own delay-on-miss, which efficiently protects the memory hierarchy by i) selectively delaying speculative load instructions and ii) utilizing value prediction as an invisible form of speculation. In this article we dive deeper into delay-on-miss, offering insights into why and how it affects the performance of the system. We also reevaluate value prediction as an invisible form of speculation. Specifically, we focus on the implications that delaying memory loads has in the memory level parallelism of the system and how this affects the value predictor and the overall performance of the system. We present new, updated results but more importantly, we also offer deeper insight into why delay-on-miss works so well and what this means for the future of secure speculative execution.},
  archive      = {J_TC},
  author       = {Christos Sakalis and Stefanos Kaxiras and Alberto Ros and Alexandra Jimborean and Magnus Själander},
  doi          = {10.1109/TC.2020.3014456},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1584-1595},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Understanding selective delay as a method for efficient secure speculative execution},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Built-in security computer: Deploying security-first
architecture using active security processor. <em>TC</em>,
<em>69</em>(11), 1571–1583. (<a
href="https://doi.org/10.1109/TC.2020.3011748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continually disclosed vulnerabilities reveal that traditional computer architecture lacks the consideration of security. This article proposes a security-first architecture, with an Active Security Processor (ASP) integrated to conventional computer architectures. To reduce the attack surface of ASP and improve the security of the whole system, the ASP is physically isolated from Computation Processor Units (CPU) with an asymmetric address space, which enables both ASP and CPU to run their operating system and applications independently in their own memory space. Furthermore, the ASP, which has the highest privilege (Super Root) of the whole system, possesses two advantageous features. First, the ASP can efficiently access all CPU resources and collect multi-dimensional information to monitor malicious behaviors, meanwhile, the CPU cannot access the ASP&#39;s private resources in any way. Second, instead of being scheduled by CPUs, the ASP can actively manage the security mechanisms employed in either CPUs or the ASP. Based on the security-first architecture, we introduce several typical security tasks running on ASP. With different considerations in terms of system overhead, complexity and performance, we also explore four typical system-level implementations for integrating the ASP to the security-first architecture. The first-generation ASP was designed and implemented based on the 40nm technology, and a security computer system was implemented based on it. Evaluations on this real hardware platform demonstrate that the security-first architecture can protect the system effectively with minor performance impacts on computing workloads.},
  archive      = {J_TC},
  author       = {Dan Meng and Rui Hou and Gang Shi and Bibo Tu and Aimin Yu and Ziyuan Zhu and Xiaoqi Jia and Yu Wen and Yun Yang},
  doi          = {10.1109/TC.2020.3011748},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1571-1583},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Built-in security computer: Deploying security-first architecture using active security processor},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OPTIMUS: A security-centric dynamic hardware partitioning
scheme for processors that prevent microarchitecture state attacks.
<em>TC</em>, <em>69</em>(11), 1558–1570. (<a
href="https://doi.org/10.1109/TC.2020.2996021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware virtualization allows multiple security-critical and ordinary (insecure) processes to co-execute on a processor. These processes temporally share hardware resources and endure numerous security threats on the microarchitecture state. State-of-the-art secure processor architectures, such as MI6 and IRONHIDE enable capabilities to execute security-critical processes in hardware isolated enclaves utilizing the strong isolation security primitive. The MI6 processor purges small state resources on each enclave entry/exit and statically partitions the last-level cache and DRAM regions to ensure strong isolation. IRONHIDE takes a spatial approach and creates two isolated clusters of cores in a multicore processor to ensure strong isolation for processes executing in the enclave cluster. Both architectures observe performance degradation due to static partitioning of shared hardware resources. OPTIMUS proposes a security-centric dynamic hardware resource partitioning scheme that operates entirely at runtime and ensures strong isolation. It enables deterministic resource allocations at the application level granularity, and limits the number of hardware reconfigurations to ensure bounded information leakage via the timing and termination channels. The dynamic hardware resource partitioning capability of OPTIMUS is shown to co-optimize performance and security for the MI6 and IRONHIDE architectures.},
  archive      = {J_TC},
  author       = {Hamza Omar and Brandon D&#39;Agostino and Omer Khan},
  doi          = {10.1109/TC.2020.2996021},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1558-1570},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OPTIMUS: A security-centric dynamic hardware partitioning scheme for processors that prevent microarchitecture state attacks},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editors’ introduction to the special issue on hardware
security. <em>TC</em>, <em>69</em>(11), 1556–1557. (<a
href="https://doi.org/10.1109/TC.2020.3021223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The twelve papers in this special section focus on hardware security. This topic is becoming a significant challenge in modern computing systems. Recently discovered hardware vulnerabilities, such as Spectre and Meltdown, are striking evidence that today’s computing systems are untenable without deliberate consideration of the security aspects at the design time. The papers address various topics related to hardware security: secure-by-design architectures, secure speculative execution, secure system integration of untrusted chiplets, malware detection, program analysis using power side channels, architecture support for forensics, and efficient implementations of security modules.},
  archive      = {J_TC},
  author       = {Amro Awad and Rujia Wang},
  doi          = {10.1109/TC.2020.3021223},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1556-1557},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editors&#39; introduction to the special issue on hardware security},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A power- and performance-aware software framework for
control system applications. <em>TC</em>, <em>69</em>(10), 1544–1555.
(<a href="https://doi.org/10.1109/TC.2020.2978468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes the development of a software architectural framework for implementing compute-aware control systems, where the term “compute-aware” describes controllers that can modify existing low-level computing platform power managers in response to the needs of the physical system controller. This level of interaction means that high-level decisions can be made as to when to operate the computing platform in a power-savings mode or a high-performance mode in response to situation awareness of the physical system. The framework is demonstrated experimentally on a mobile robot platform. In this example, a situation-aware governor is developed that adjusts the speed of the processor based on the physical performance of the robot as it traverses a path through obstacles. The results show that the situation-aware governor results in overall power savings of up to 38.9 percent with 1.3 percent degradation in performance compared to the static high-power strategy.},
  archive      = {J_TC},
  author       = {Michael Giardino and Eric Klawitter and Bonnie Ferri and Aldo Ferri},
  doi          = {10.1109/TC.2020.2978468},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1544-1555},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A power- and performance-aware software framework for control system applications},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pipelined hardware implementation of COPA, ELmD, and COLM.
<em>TC</em>, <em>69</em>(10), 1533–1543. (<a
href="https://doi.org/10.1109/TC.2020.2977031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authenticated encryption algorithms offer privacy, authentication, and data integrity, as well. In recent years, they have received special attention after the call for submissions of Competition for Authenticated Encryption: Security, Applicability, and Robustness (CAESAR) was published. The CAESAR goal is to generate a portfolio with recommendations of authenticated encryption algorithms for three different scenarios: Lightweight, high speed, and defense in deep. ELmD and COPA are two on-line authenticated encryption algorithms submitted to CAESAR; because of their similarities, they were merged as COLM during the third-round of CAESAR. COLM is a finalist in the use case 3 defense in depth. ELmD, COPA, and COLM are based on the ECB-mix-ECB structure, which is highly parallelizable and pipelineable. In this paper, we present optimized single-chip implementations of ELmD, COPA, and COLM using pipelining. For ELmD, we present implementations for eight combinations of its parameters set: For intermediate tags, fixed, variable tag length, and 10 and 6 AES rounds. COLM implementation is for variable tag length without intermediate tags. In the case of COPA, it does not have parameters set. The implementation results with a Xilinx Virtex 6 FPGA show that ELmD is the best option concerning area and speed for single-chip implementation. The area of COPA and COLM are 1.65 and 1.69 times ELmD&#39;s respectively. Regarding throughput, the range of our implementations goes from 33.34 Gbits/s for COLM to more than 35 Gbits/s for several versions of ELmD.},
  archive      = {J_TC},
  author       = {Lilian Bossuet and Cuauhtemoc Mancillas-López and Brisbane Ovilla-Martínez},
  doi          = {10.1109/TC.2020.2977031},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1533-1543},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Pipelined hardware implementation of COPA, ELmD, and COLM},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithmics of cost-driven computation offloading in the
edge-cloud environment. <em>TC</em>, <em>69</em>(10), 1519–1532. (<a
href="https://doi.org/10.1109/TC.2020.2976996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation offloading between the edge and the cloud is an effective way for deployed service to fully utilize the resources at both sides for its QoS improvement and overall cost reduction. Although the offloading problem has been intensively studied in the context of mobile computing, existing algorithms in most cases cannot be effectively migrated to the edge-cloud environment because their inter-partition communication costs are always deemed as symmetric, and their intra-partition communication costs are often ignored, which, though reasonable to the traditional case, are not valid to our settings anymore. In this article, we propose a new algorithmic approach to the offloading problem in the edge-cloud environment, where a heterogeneous model is advocated to incorporate the communication cost between co-resident tasks while considering the asymmetry of communication costs between non-coresident tasks. We prove the offloading problem with respect to this model is NP-hard, and thereby designing an efficient algorithm to obtain a sub-optimal solution. Additionally, we also show that in a homogeneous case when the intra-partition and inter-partition communication costs between any pair of interactive tasks are symmetric, an optimal offloading algorithm can be devised by transforming the problem into a classical min-cut problem. We implemented and evaluated the algorithms by offloading a PageRank-based application in a controlled edge-cloud setting. Our empirical results show that the proposed algorithm for the heterogeneous case is always efficient to find a better offloading scheme, compared with the selected existing algorithms, while for the homogeneous case, the proposed solution can efficiently achieve the optimal strategy.},
  archive      = {J_TC},
  author       = {Mingzhe Du and Yang Wang and Kejiang Ye and Chengzhong Xu},
  doi          = {10.1109/TC.2020.2976996},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1519-1532},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Algorithmics of cost-driven computation offloading in the edge-cloud environment},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PARMA: Parallelization-aware run-time management for
energy-efficient many-core systems. <em>TC</em>, <em>69</em>(10),
1507–1518. (<a href="https://doi.org/10.1109/TC.2020.2975787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance and energy efficiency considerations have shifted computing paradigms from single-core to many-core architectures. At the same time, traditional speedup models such as Amdahl&#39;s Law face challenges in the run-time reasoning for system performance and energy efficiency, because these models typically assume limited variations of the parallel fraction. Moreover, the parallel fraction, which varies dynamically in workloads, is generally unknown at run-time without application-level instrumentation. This article describes novel performance/energy trade-off models based on realistic architectural considerations, which describe the parallel fraction and speedup as functions of performance counter values available in modern processors, removing the need for application-level instrumentation. These are then used to develop a Parallelization-Aware Run-time Management (PARMA) approach. PARMA aims at controlling core allocations and operating voltage/frequency points for energy efficiency, according to the varying workload parallel fractions. The efficacy of our models and the PARMA approach is extensively validated using a number of PARSEC benchmark applications, involving two performance/energy trade-off metrics: energy-delay-product (EDP), typically used in high-performance applications and energy per instruction (EPI), suitable for energy-aware applications. Up to 48 and 68 percent improvements in EDP and EPI have been observed using the PARMA approach compared with parallelization-agnostic methods.},
  archive      = {J_TC},
  author       = {Mohammed A. Noaman Al-hayanni and Ashur Rafiev and Fei Xia and Rishad Shafik and Alexander Romanovsky and Alex Yakovlev},
  doi          = {10.1109/TC.2020.2975787},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1507-1518},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PARMA: Parallelization-aware run-time management for energy-efficient many-core systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LUT input reordering to reduce aging impact on FPGA LUTs.
<em>TC</em>, <em>69</em>(10), 1500–1506. (<a
href="https://doi.org/10.1109/TC.2020.2974955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a fine-grained FPGA aging mitigation method. Our method focuses on Look Up Tables (LUTs) on which Boolean functions are mapped. Based on our observations, for any configuration, even if it is carefully selected, a number of LUT transistors experience severe stress rates. Therefore, an algorithm is presented to select several alternative configurations for each LUT. Alternative configurations are obtained by LUT input reordering. These alternative configurations are rotationally loaded into the FPGA. Experimental results shows that our method achieves 263 and 14.1 percent Mean Time To Failure (MTTF) improvement for Hot Carrier Injection (HCI) and Bias Temperature Instability (BTI), respectively. Additionally, due to changing only local routings, our method imposes up to 1 percent performance overhead to the systems.},
  archive      = {J_TC},
  author       = {Mohammad Ebrahimi and Rezgar Sadeghi and Zainalabedin Navabi},
  doi          = {10.1109/TC.2020.2974955},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1500-1506},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LUT input reordering to reduce aging impact on FPGA LUTs},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Schedulability analysis of global scheduling for multicore
systems with shared caches. <em>TC</em>, <em>69</em>(10), 1487–1499. (<a
href="https://doi.org/10.1109/TC.2020.2974224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared caches in multicore processors introduce serious difficulties in providing guarantees on the real-time properties of embedded software due to the interaction and the resulting contention in the shared caches. To address this problem, we develop a new schedulability analysis for real-time multicore systems with shared caches, globally scheduled by Earliest Deadline First (EDF) and Fixed Priority (FP) algorithms. We construct an integer programming formulation, which can be transformed to an integer linear programming formulation, to calculate an upper bound on cache interference exhibited by a task within a given execution window. Using the integer programming formulation, an iterative algorithm is presented to obtain the upper bound on cache interference a task may exhibit during one job execution. The upper bound on cache interference is subsequently integrated into the schedulability analysis to derive a new schedulability condition. A range of experiments is performed to investigate how the schedulability is degraded by shared cache interference. We also evaluate the schedulability performance of EDF against FP scheduling over randomly generated tasksets. Our empirical evaluations show that EDF is better than FP scheduling in terms of the number of task sets deemed schedulable.},
  archive      = {J_TC},
  author       = {Jun Xiao and Sebastian Altmeyer and Andy D. Pimentel},
  doi          = {10.1109/TC.2020.2974224},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1487-1499},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Schedulability analysis of global scheduling for multicore systems with shared caches},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Separable binary convolutional neural network on embedded
systems. <em>TC</em>, <em>69</em>(10), 1474–1486. (<a
href="https://doi.org/10.1109/TC.2020.2973974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have witnessed the tremendous success of deep neural networks. However, this success comes with the considerable memory and computational costs which make it difficult to deploy these networks directly on resource-constrained embedded systems. To address this problem, we propose TaijiNet, a separable binary network, to reduce the storage and computational overhead while maintaining a comparable accuracy. Furthermore, we also introduce a strategy called partial binarized convolution which binarizes only unimportant kernels to efficiently balance network performance and accuracy. Our approach is evaluated on the CIFAR-10 and ImageNet datasets. The experimental results show that with the proposed TaijiNet, the separable binary versions of AlexNet and ResNet-18 can achieve 26× and 6.4× compression rates with comparable accuracy when comparing with the full-precision versions respectively. In addition, by adjusting the PCA threshold, the xnor version of Taiji-AlexNet improves accuracy by 4-8 percent comparing with other state-of-the-art methods.},
  archive      = {J_TC},
  author       = {Renping Liu and Xianzhang Chen and Duo Liu and Yingjian Ling and Weilue Wang and Yujuan Tan and Chunhua Xiao and Chaoshu Yang and Runyu Zhang and Liang Liang},
  doi          = {10.1109/TC.2020.2973974},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1474-1486},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Separable binary convolutional neural network on embedded systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling energy-efficient and reliable neural network via
neuron-level voltage scaling. <em>TC</em>, <em>69</em>(10), 1460–1473.
(<a href="https://doi.org/10.1109/TC.2020.2973150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the platforms of running deep neural networks (DNNs) move from large-scale data centers to handheld devices, power emerge as one of the most significant obstacles. Voltage scaling is a promising technique that enables power saving. Nevertheless, it raises reliability and performance concerns that may undesirably deteriorate NNs accuracy and performance. Consequently, an energy-efficient and reliable scheme is required for NNs to balance the above three aspects with satisfied user experience. To this end, we propose a neuron-level voltage scaling framework called NN-APP to model the impact of supply voltages on NNs from output accuracy (A), power (P), and performance (P) perspectives. We analyze the error propagation characteristics in NNs at both inter- and intra-network layers to precisely model the impact of voltage scaling on the final output accuracy at neuron-level. Furthermore, we combine a voltage clustering method and the multi-objective optimization to identify the optimal voltage islands and apply the same voltage to neurons with similar fault tolerance capability. We perform three case studies to demonstrate the efficacy of the proposed techniques.},
  archive      = {J_TC},
  author       = {Jing Wang and Xin Fu and Xu Wang and Shubo Liu and Lan Gao and Weigong Zhang},
  doi          = {10.1109/TC.2020.2973150},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1460-1473},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling energy-efficient and reliable neural network via neuron-level voltage scaling},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Laser-induced fault injection on smartphone bypassing the
secure boot-extended version. <em>TC</em>, <em>69</em>(10), 1449–1459.
(<a href="https://doi.org/10.1109/TC.2018.2860010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes the outcome of a laser attack study on an Android smartphone targeting specifically the secure boot sequence. Laser fault injection has become a classical attack path in the secure chip industry to investigate potential security mitigation. The implementation of such attacks on a recent mobile phone remains relatively unexplored and represents different challenges, both at hardware and software levels. In this paper, we show how the device is crafted to get a direct access to the silicon and explain the corresponding experimental setup. By inserting our own software into the boot sequence, it was possible to achieve a fine characterization of the die sensitivity to laser emissions. With the knowledge of potential perturbations, several attack scenarios were built, allowing to malevolently get the highest level of privilege within the mobile phone.},
  archive      = {J_TC},
  author       = {Aurélien Vasselle and Hugues Thiebeauld and Quentin Maouhoub and Adèle Morisset and Sébastien Ermeneux},
  doi          = {10.1109/TC.2018.2860010},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1449-1459},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Laser-induced fault injection on smartphone bypassing the secure boot-extended version},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lightweight ciphers and their side-channel resilience.
<em>TC</em>, <em>69</em>(10), 1434–1448. (<a
href="https://doi.org/10.1109/TC.2017.2757921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side-channel attacks represent a powerful category of attacks against cryptographic devices. Still, side-channel analysis for lightweight ciphers is much less investigated than for instance for AES. Although intuition may lead to the conclusion that lightweight ciphers are weaker in terms of side-channel resistance, that remains to be confirmed and quantified. In this paper, we consider various side-channel analysis metrics which should provide an insight on the resistance of lightweight ciphers against side-channel attacks. In particular, for the non-profiled scenario we use the theoretical confusion coefficient and empirical optimal distinguisher. Our study considers side-channel attacks on the first, the last, or both rounds simultaneously. Furthermore, we conduct a profiled side-channel analysis using various machine learning attacks to recover 4-bit and 8-bit intermediate states of the cipher. Our results show that the difference between AES and lightweight ciphers is smaller than one would expect, and even find scenarios in which lightweight ciphers may be more resistant. Interestingly, we observe that the studied 4-bit S-boxes have a different side-channel resilience, while the difference in the 8-bit ones is only theoretically present.},
  archive      = {J_TC},
  author       = {Annelie Heuser and Stjepan Picek and Sylvain Guilley and Nele Mentens},
  doi          = {10.1109/TC.2017.2757921},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1434-1448},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight ciphers and their side-channel resilience},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient software implementation of ring-LWE encryption on
IoT processors. <em>TC</em>, <em>69</em>(10), 1424–1433. (<a
href="https://doi.org/10.1109/TC.2017.2750146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded processors have been widely used for building up Internet of Things (IoT) platforms, in which the security issue is becoming critical. This paper studies efficient techniques of lattice-based cryptography on these processors and presents the first implementation of ring-LWE encryption on ARM NEON and MSP430 architectures. For ARM NEON architecture, we propose a vectorized version of Iterative Number Theoretic Transform (NTT) for high-speed computation of polynomial multiplication on ARM NEON platforms and a 32-bit variant of SAMS2 technique for fast reduction. For MSP430 architecture, we propose an optimized SWAMS2 reduction technique, which consists of five different basic operations, including Shifting, Swapping, Addition, and two Multiplication-Subtractions. Regarding of the sampling from the discrete Gaussian distribution, we adopt Knuth-Yao sampler, accompanied with optimized methods such as Look-Up Table (LUT) and byte-scanning. Subsequently, a full-fledged implementation of Ring-LWE is presented by both taking advantage of our proposed method and previous optimization techniques re-designed for desired platforms. Our ring-LWE implementation of encryption/decryption at a classical security level of 128 bits requires only 149:4k=32:8k clock cycles on ARM NEON, and 2126:3k=244:5k clock cycles on MSP430. These results are roughly 7 times faster than the fastest ECC implementation on desired platforms with same security level.},
  archive      = {J_TC},
  author       = {Zhe Liu and Reza Azarderakhsh and Howon Kim and Hwajeong Seo},
  doi          = {10.1109/TC.2017.2750146},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1424-1433},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient software implementation of ring-LWE encryption on IoT processors},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Techniques to reduce switching and leakage energy in
unrolled block ciphers. <em>TC</em>, <em>69</em>(10), 1414–1423. (<a
href="https://doi.org/10.1109/TC.2017.2747546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption of block ciphers is critical in resource constrained devices. Unrolling has been explored in literature as a technique to increase efficiency by eliminating energy spent in loop control elements such as registers and multiplexers. However these savings are minimal and are offset by the increase in glitching power that comes with unrolling. We propose an efficient latch-based glitch filter for unrolled designs that reduces energy per encryption by an order of magnitude over a straightforward implementation, and by 28-45 percent over the best existing glitch filtering schemes. We explore the optimal number of glitch filters that should be used in order to minimize total energy, and provide estimates of the area cost. Partially unrolled designs also benefit from using our scheme with energies competitive to fully serialized implementations. Power gating to reduce leakage power and reuse of computed key enable unrolled designs to be more efficient than serialized ones without compromising latency advantages. We demonstrate our approach on the SIMON-128 and AES-128 block ciphers.},
  archive      = {J_TC},
  author       = {Siva Nishok Dhanuskodi and Daniel Holcomb},
  doi          = {10.1109/TC.2017.2747546},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1414-1423},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Techniques to reduce switching and leakage energy in unrolled block ciphers},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object-level memory allocation and migration in hybrid
memory systems. <em>TC</em>, <em>69</em>(9), 1401–1413. (<a
href="https://doi.org/10.1109/TC.2020.2973134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid memory systems composed of emerging non-volatile memory (NVM) and DRAM have drawn increasing attention in recent years. To fully exploit the advantages of both NVM and DRAM, a primary goal is to properly place application data on the hybrid memories. Previous studies have focused on page migration schemes to achieve higher performance and energy efficiency. However, those schemes all rely on online page access monitoring (costly), and data migration at the page granularity may cause additional overhead due to DRAM bandwidth contention and maintenance of cache/TLB consistency. In this article, we present Object-level memory Allocation and Migration (OAM) mechanisms for hybrid memory systems. OAM exploits a profiling tool to characterize objects&#39; memory access patterns at different execution phases of applications, and applies a performance/energy model to direct the initial static memory allocation and runtime dynamic object migration between NVM and DRAM. Based on our newly-developed programming interfaces for hybrid memory systems, application source codes can be automatically transformed via static code instrumentation. We evaluate OAM on an emulated hybrid memory system, and experimental results show that OAM can significantly reduce system energy-delay-product by 61 percent on average compared to a page-interleaving data placement scheme. It can also significantly reduce data migration overhead by 83 and 69 percent compared to the state-of-the-art page migration scheme CLOCK-DWF and 2PP, respectively, while improving application performance by up to 22 and 10 percent.},
  archive      = {J_TC},
  author       = {Haikun Liu and Renshan Liu and Xiaofei Liao and Hai Jin and Bingsheng He and Yu Zhang},
  doi          = {10.1109/TC.2020.2973134},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1401-1413},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Object-level memory allocation and migration in hybrid memory systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithms for calculating correctly rounded exponential
function in double-precision arithmetic. <em>TC</em>, <em>69</em>(9),
1388–1400. (<a href="https://doi.org/10.1109/TC.2020.2972901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correct rounding provides the best approximation of the exponential function by double-precision numbers. To obtain the correctly rounded exponential of some arguments, the exponential should be calculated with high accuracy. For small arguments, even higher accuracy is required. This article presents simple and very fast algorithms for small arguments. Yet another algorithm presented here demonstrates a good maximum execution time, which may be important for critical applications. This algorithm can be combined with some other already existing algorithms to achieve the best maximum and average execution times. All proposed algorithms calculate the correctly rounded exponential function for all rounding modes and use only double-precision arithmetic for computation. In the argument reduction step, precalculated tables are used. Test implementations of these algorithms were developed in C language and are portable. Full proofs are presented either in this article itself or in its appendices.},
  archive      = {J_TC},
  author       = {Alexander Godunov},
  doi          = {10.1109/TC.2020.2972901},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1388-1400},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Algorithms for calculating correctly rounded exponential function in double-precision arithmetic},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Small constant mean-error imprecise adder/multiplier for
efficient VLSI implementation of MAC-based applications. <em>TC</em>,
<em>69</em>(9), 1376–1387. (<a
href="https://doi.org/10.1109/TC.2020.2972549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to considerable effectiveness of the imprecise computing paradigm in hardware implementation of many applications, great attention has been recently paid by many research groups to develop different novel imprecise computational blocks such as adders and multipliers. Traditionally, the imprecise blocks are developed in an application independent manner, just similar to development of a conventional precise block. This article investigates the systematic application oriented development of the imprecise computational blocks which results in more customized components. The main focus is on the development of customized imprecise adder/multiplier for efficient implementation of a general multiply-accumulate (MAC) block as the basic building block of many imprecision tolerant applications including digital signal processing and soft computing. To develop some customized blocks for the MAC, the error behaviors of the suitable imprecise adder and multiplier are first extracted by analyzing the MAC. Based on analysis results, efficient small constant mean-error imprecise adder and multiplier are developed based on a systematic mathematical-logical approach. A wide range of synthesis and simulation results are provided to demonstrate efficiency of custom developed imprecise blocks with respect to some of the best existing imprecise blocks in a general MAC and a real 2D-Convolution application.},
  archive      = {J_TC},
  author       = {Mohammad Haji Seyed Javadi and Mohammad Hossein Yalame and Hamid Reza Mahdiani},
  doi          = {10.1109/TC.2020.2972549},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1376-1387},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Small constant mean-error imprecise Adder/Multiplier for efficient VLSI implementation of MAC-based applications},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable suffix sorting on a multicore machine. <em>TC</em>,
<em>69</em>(9), 1364–1375. (<a
href="https://doi.org/10.1109/TC.2020.2972546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of methods have been proposed for suffix sorting on internal memory of RAM and external memory of hard disks. The current best results for suffix sorting on internal or external memory are achieved by several algorithms using the induced sorting (IS) method in various ways. While these algorithms are efficient, the internal ones are much different from those external in terms of the algorithm designs. A scalable IS method that can be applied for suffix sorting on both internal and external memory is highly desired. This article proposes a blockwise IS method to facilitate pipelined access on internal memory and sequential I/Os on external memory. The detailed algorithm of using this method for a 4-stage pipeline with multiple threads is described, where multiple threads are applied to parallelize not only the pipelined stages of consecutive blocks but also the tasks within each stage wherever possible. This algorithm is evaluated by our experiments on a set of realistic and artificial datasets to achieve better overall time and space performance than the existing best results from pSACAK, pDSS and pKS. Beside sorting suffixes on internal memory in linear time, the proposed method can be ported to external memory for sorting massive suffixes in linear I/O complexity.},
  archive      = {J_TC},
  author       = {Jing Yi Xie and Ge Nong and Bin Lao and Wentao Xu},
  doi          = {10.1109/TC.2020.2972546},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1364-1375},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scalable suffix sorting on a multicore machine},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BLADE: An in-cache computing architecture for edge devices.
<em>TC</em>, <em>69</em>(9), 1349–1363. (<a
href="https://doi.org/10.1109/TC.2020.2972528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Area and power-constrained edge devices are increasingly utilized to perform compute intensive workloads, necessitating increasingly area and power-efficient accelerators. In this context, in-SRAM computing performs hundreds of parallel operations on spatially local data common in many emerging workloads, while reducing power consumption due to data movement. However, in-SRAM computing faces many challenges, including integration into the existing architecture, arithmetic operation support, data corruption at high operating frequencies, inability to run at low voltages, and low area density. To meet these challenges, this article introduces BLADE, a BitLine Accelerator for Devices on the Edge. BLADE is an in-SRAM computing architecture that utilizes local wordline groups to perform computations at a frequency 2.8× higher than state-of-the-art in-SRAM computing architectures. BLADE is integrated into the cache hierarchy of low-voltage edge devices, and simulated and benchmarked at the transistor, architecture, and software abstraction levels. Experimental results demonstrate performance/energy gains over an equivalent NEON accelerated processor for a variety of edge device workloads, namely, cryptography (4× performance gain/6× energy reduction), video encoding (6×/2×), and convolutional neural networks (3×/1.5×), while maintaining the highest frequency/energy ratio (up to 2.2 Ghz@1V) of any conventional in-SRAM computing architecture, and a low area overhead of less than 8 percent.},
  archive      = {J_TC},
  author       = {William Andrew Simon and Yasir Mahmood Qureshi and Marco Rios and Alexandre Levisse and Marina Zapater and David Atienza},
  doi          = {10.1109/TC.2020.2972528},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1349-1363},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BLADE: An in-cache computing architecture for edge devices},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time scheduling and analysis of OpenMP DAG tasks
supporting nested parallelism. <em>TC</em>, <em>69</em>(9), 1335–1348.
(<a href="https://doi.org/10.1109/TC.2020.2972385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OpenMP is a promising framework to develop parallel real-time software on multi-cores. Although similar to the DAG task model, OpenMP task systems are significantly more difficult to analyze due to constraints posed by OpenMP specifications. One of the most interesting features in OpenMP is the support for nested parallelism, enjoying benefits in enhancing performance transparency of parallel libraries and promoting reuse of black-box code. Previous researches on DAG task scheduling mainly restrict to only one level of parallelism. The problem whether OpenMP tasks with multiple levels of parallelism are suitable to real-time systems remains open. In this paper, we study the real-time scheduling and analysis of OpenMP task systems supporting nested parallelism. First, we show that under existing scheduling algorithms in OpenMP implementations, nested parallelism indeed may lead to extremely bad timing behaviors where the parallel workload is sequentially executed completely. To solve this problem, we propose a new scheduling algorithm and develop two sound response time bounds by considering the trade-off between simplicity and analysis precision. Experiments demonstrate the efficiency of our methods.},
  archive      = {J_TC},
  author       = {Jinghao Sun and Nan Guan and Feng Li and Huimin Gao and Chang Shi and Wang Yi},
  doi          = {10.1109/TC.2020.2972385},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1335-1348},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Real-time scheduling and analysis of OpenMP DAG tasks supporting nested parallelism},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advance virtual channel reservation. <em>TC</em>,
<em>69</em>(9), 1320–1334. (<a
href="https://doi.org/10.1109/TC.2020.2971982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a smart communication service called Advance Virtual Channel Reservation (AVCR) to provide a highway to target packets, which can greatly reduce their contention delay in NoC. AVCR takes advantage of the fact that we can know or predict the destination of some packets ahead of their arrival at the network interface (NI). Exploiting the time interval before a packet is ready, AVCR establishes an end-to-end highway from the source NI to the destination NI. This highway is built up by reserving the virtual channel (VC) resources ahead of the target packet transmission and offering priority service to flits in the reserved VC in the wormhole router, which can avoid the target packets&#39; VC allocation and switch arbitration delay. Additionally, optimization schemes are proposed to increase resources utilization and system performance. We evaluate AVCR with GEM5 full-system simulations by using 24 benchmarks in PARSEC and OMP2012. Compared to the state-of-art mechanisms and the priority-based mechanism, experimental results show that our mechanism can significantly reduce the target packets&#39; transfer latency and thus effectively decrease the average region-of-interest (ROI) time by 18.1 percent (maximally by 29.4 percent) across all benchmarks.},
  archive      = {J_TC},
  author       = {Boqian Wang and Zhonghai Lu},
  doi          = {10.1109/TC.2020.2971982},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1320-1334},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Advance virtual channel reservation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid binary-unary hardware accelerator. <em>TC</em>,
<em>69</em>(9), 1308–1319. (<a
href="https://doi.org/10.1109/TC.2020.2971596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream-based computing such as stochastic computing has been used in recent years to create designs with significantly smaller area by harnessing unary encoding of data. However, the area saving comes at an exponential price in latency, making the area x delay cost unattractive. In this article, we present a novel method which uses a hybrid binary / unary representation to perform computations. We first divide the input range into a few sub-regions, perform unary computations on each sub-region individually, and finally pack the outputs of all sub-regions back to compact binary. Moreover, we propose a synthesis methodology and a regression model to predict an optimal or close-to-optimal design in the design space. To the best of our knowledge, we are the first to show a scalable method based on parallel bit-stream data representation that can beat conventional binary in terms of a real cost, i.e., area x delay and energy consumption in almost all functions that we tried at resolutions of 8-, 10-, and 12-bits. Our method outperforms the binary, stochastic, and fully unary methods on a number of functions, especially low-cost binary CORDIC-based functions, and on a common edge detection algorithm on FPGA and in ASIC implementation. In terms of area x delay cost, our {on FPGA, in ASIC} cost is on average only {4:72\%, 24:36\%} and {20:16\%, 60:12\%} of the parallel binary pipeline implementation at 8and 10-bit resolution, respectively. These numbers are 2-3 orders of magnitude better than the results of traditional stochastic methods. Our method is not competitive with the parallel CORDIC-based pipeline binary method for high-resolution (12-bit), highly oscillating functions such as sin (15x). However, for complex functions like gamma function, the proposed method can beat any other methods in terms of area x delay, throughput, latency, and energy per sample costs. To implement the Roberts cross edge detection algorithm, the proposed method takes 5.7 and 39.45 percent of the area x delay cost of FPGA and ASIC implementation of the binary method, respectively. In terms of energy efficiency for FPGA implementation, our method uses only 8.4, 12.7, and 27.7 percent of the energy per sample usage of serial binary implementations at 8-, 10-, and 12-bit resolutions, respectively. These numbers change to 23.9, 38.54, and 99.3 percent compared to parallel binary implementations.},
  archive      = {J_TC},
  author       = {S. Rasoul Faraji and Kia Bazargan},
  doi          = {10.1109/TC.2020.2971596},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1308-1319},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hybrid binary-unary hardware accelerator},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast exact NPN classification by co-designing canonical form
and its computation algorithm. <em>TC</em>, <em>69</em>(9), 1293–1307.
(<a href="https://doi.org/10.1109/TC.2020.2971466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NPN classification of Boolean functions is a powerful technique used in many practical applications, including logic synthesis, technology mapping, architecture exploration, circuit restructuring, and approximate logic synthesis. Computing the canonical form of a function is the most common approach to NPN classification. Exact classification of practical functions is an open problem because there are difficult functions beyond the capability of the state-of-the-art exact algorithms, which may take several months to compute a canonical form. This article proposes a new approach to exact NPN classification, in which a series of canonical forms and the algorithms to compute them are designed together. As a result, the runtime of the exact classification for difficult functions is effectively controlled by making both representation and computation cost-aware. Experimental results show that the proposed algorithm can perform exact classification of the worst-case 16-input functions in less than 3 minutes. This indicates that, for the first time, the problem of exact classification can be effectively solved for any Boolean functions with up to 16 inputs arising in practical applications.},
  archive      = {J_TC},
  author       = {Xuegong Zhou and Lingli Wang and Alan Mishchenko},
  doi          = {10.1109/TC.2020.2971466},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1293-1307},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast exact NPN classification by co-designing canonical form and its computation algorithm},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of threading libraries for high performance
computing. <em>TC</em>, <em>69</em>(9), 1279–1292. (<a
href="https://doi.org/10.1109/TC.2020.2970706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the appearance of multi-/many core machines, applications and runtime systems have evolved in order to exploit the new on-node concurrency brought by new software paradigms. POSIX threads (Pthreads) was widely-adopted for that purpose and it remains as the most used threading solution in current hardware. Lightweight thread (LWT) libraries emerged as an alternative offering lighter mechanisms to tackle the massive concurrency of current hardware. In this article, we analyze in detail the most representative threading libraries including Pthread- and LWT-based solutions. In addition, to examine the suitability of LWTs for different use cases, we develop a set of microbenchmarks consisting of OpenMP patterns commonly found in current parallel codes, and we compare the results using threading libraries and OpenMP implementations. Moreover, we study the semantics offered by threading libraries in order to expose the similarities among different LWT application programming interfaces and their advantages over Pthreads. This article exposes that LWT libraries outperform solutions based on operating system threads when tasks and nested parallelism are required.},
  archive      = {J_TC},
  author       = {Adrián Castelló and Rafael Mayo Gual and Sangmin Seo and Pavan Balaji and Enrique S. Quintana-Ortí and Antonio J. Peña},
  doi          = {10.1109/TC.2020.2970706},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1279-1292},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Analysis of threading libraries for high performance computing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic generation of analog/mixed signal virtual
platforms for smart systems. <em>TC</em>, <em>69</em>(9), 1263–1278. (<a
href="https://doi.org/10.1109/TC.2020.2970699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pervasive computing requires to build systems every day more complex and heterogeneous. Smart devices must be able to carry on sensing and actuation alongside with computation and communication. As such, many different technologies must be packed within the same object. Digital HW and SW coexist with analog components and Micro-Electro-Mechanical systems capable of sensing and controlling the physical environment. For this reason, the design of such devices must rely on the integration of many different descriptions belonging to different design domains. The high-level of heterogeneity involved in the modeling phase of the system development makes harder the validation of the system functionality, since holistic system simulation would require the integration of many different simulators. In this article, we propose a set of automatic abstraction techniques for multi-disciplines analog components. Then, we define a scheduling strategy to integrate the execution of continuous-time analog sub-components with automatically abstracted models of the digital HW parts of the system. As a final result, the proposed methodology produces a C++ virtual platform providing a holistic simulation of complex and heterogeneous devices.},
  archive      = {J_TC},
  author       = {Enrico Fraccaroli and Michele Lora and Franco Fummi},
  doi          = {10.1109/TC.2020.2970699},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1263-1278},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Automatic generation of Analog/Mixed signal virtual platforms for smart systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DS3: A system-level domain-specific system-on-chip
simulation framework. <em>TC</em>, <em>69</em>(8), 1248–1262. (<a
href="https://doi.org/10.1109/TC.2020.2986963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous systems-on-chip (SoCs) are highly favorable computing platforms due to their superior performance and energy efficiency potential compared to homogeneous architectures. They can be further tailored to a specific domain of applications by incorporating processing elements (PEs) that accelerate frequently used kernels in these applications. However, this potential is contingent upon optimizing the SoC for the target domain and utilizing its resources effectively at runtime. To this end, system-level design - including scheduling, power-thermal management algorithms and design space exploration studies - plays a crucial role. This article presents a system-level domain-specific SoC simulation (DS3) framework to address this need. DS3 enables both design space exploration and dynamic resource management for power-performance optimization of domain applications. We showcase DS3 using six real-world applications from wireless communications and radar processing domain. DS3, as well as the reference applications, is shared as open-source software to stimulate research in this area.},
  archive      = {J_TC},
  author       = {Samet E. Arda and Anish Krishnakumar and A. Alper Goksoy and Nirmal Kumbhare and Joshua Mack and Anderson L. Sartor and Ali Akoglu and Radu Marculescu and Umit Y. Ogras},
  doi          = {10.1109/TC.2020.2986963},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1248-1262},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DS3: A system-level domain-specific system-on-chip simulation framework},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative accelerators for streamlining MapReduce on
scale-up machines with incremental data aggregation. <em>TC</em>,
<em>69</em>(8), 1233–1247. (<a
href="https://doi.org/10.1109/TC.2020.3004169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The MapReduce programming paradigm has been increasingly adopted to implement data-intensive applications processing both small and large scale datasets. As most jobs in data centers have a data footprint in the order of gigabytes, emerging high-end scale-up machines are capable of running most data center processing tasks, thus significantly improving power and server density. However, this approach provides limited performance and energy efficiency because of inefficient utilization of the memory subsystem and serial execution within the MapReduce programming model. Recent work has proposed a distributed hardware acceleration architecture, called CASM, which augments each core in a scale-up machine with a lightweight compute engine. The CASM&#39;s network of accelerators operates concurrently with the cores in executing MapReduce stages and reduces significantly traffic to/from storage. In this article, we study the benefits and applicability of CASM, by offering an extensive analysis of design parameters and of its scalable performance on a wide range of applications, and exploring its applicability to incremental data aggregation tasks. Our experimental evaluation indicates that CASM reduces off-chip traffic by four times on average over a chip multiprocessor solution, while scaling well with the number of cores in the system, and it is highly effective in providing incremental results that approximate final outcomes.},
  archive      = {J_TC},
  author       = {Abraham Addisie and Valeria Bertacco},
  doi          = {10.1109/TC.2020.3004169},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1233-1247},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Collaborative accelerators for streamlining MapReduce on scale-up machines with incremental data aggregation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PaRTAA: A real-time multiprocessor for mixed-criticality
airborne systems. <em>TC</em>, <em>69</em>(8), 1221–1232. (<a
href="https://doi.org/10.1109/TC.2020.3002697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed-criticality systems, where multiple systems with varying criticality-levels share a single hardware platform, require isolation between tasks with different criticality-levels. Isolation can be achieved with software-based solutions or can be enforced by a hardware level partitioning. An asymmetric multiprocessor architecture offers hardware-based isolation at the cost of underutilized hardware resources, and the inter-core communication mechanism is often a single point of failure in such architectures. In contrast, a partitioned uniprocessor offers efficient resource utilization at the cost of limited scalability. We propose a partitioned real-time asymmetric architecture (PaRTAA) specifically designed for mixed-criticality airborne systems, featuring robust partitioning within processing elements for establishing isolation between tasks with varying criticality. The granularity in the processing element offers efficient resource utilization where inter-dependent tasks share the same processing element for sequential execution while preserving isolation, and independent tasks simultaneously execute on different processing elements as per system requirements.},
  archive      = {J_TC},
  author       = {Shibarchi Majumder and Jens Frederik Dalsgaard Nielsen and Thomas Bak},
  doi          = {10.1109/TC.2020.3002697},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1221-1232},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PaRTAA: A real-time multiprocessor for mixed-criticality airborne systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tetris: Using software/hardware co-design to enable
handheld, physics-limited 3D plane-wave ultrasound imaging. <em>TC</em>,
<em>69</em>(8), 1209–1220. (<a
href="https://doi.org/10.1109/TC.2020.2990061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High volume acquisition rates are imperative for certain medical ultrasound imaging applications, such as 3D elastography and 3D vector flow imaging. As ultrasound imaging transitions from 2D to 3D, the massive data bandwidth and billions of trigonometric operations required to reconstruct each volume leaves conventional computer architectures falling short. Despite recent algorithmic improvements, high-volume-rate ultrasound imaging remains computationally infeasible on known platforms. In this article, we expand our previous work on Tetris, a novel hardware accelerator for separable ultrasound beamforming that enables volume acquisition rates up to the physics limits of acoustic propagation delay. Through algorithmic and hardware optimizations, we enable an image reconstruction system design outclassing previously proposed accelerators in performance while lowering hardware complexity, storage, and power requirements. Tetris operates in a streaming fashion-without requiring on-chip storage of the entire receive signal-reconstructing volumes in real-time. For a representative imaging task, our proposed system generates physics-limited 13,000 volumes per second in a 2 watt power budget. The Tetris beamformer has an unprecedented power efficiency of 2.03 tera-beamforming operations per watt-an increase in efficiency of nearly 3× compared to the prior work.},
  archive      = {J_TC},
  author       = {Brendan L. West and Jian Zhou and Ronald G. Dreslinksi and Oliver D. Kripfgans and J. Brian Fowlkes and Chaitali Chakrabarti and Thomas F. Wenisch},
  doi          = {10.1109/TC.2020.2990061},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1209-1220},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tetris: Using Software/Hardware co-design to enable handheld, physics-limited 3D plane-wave ultrasound imaging},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LAWS: Locality-AWare scheme for automatic speech
recognition. <em>TC</em>, <em>69</em>(8), 1197–1208. (<a
href="https://doi.org/10.1109/TC.2020.2991002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Speech Recognition (ASR) systems are changing the way people interact with different applications on mobile devices. Fulfilling such user-interactivity requires not only a highly accurate, large-vocabulary recognition system, but also a real-time, energy-efficient solution. However, these ASR systems need high memory bandwidth and power budget, which may be impractical for most of small form-factor battery-operated devices. In this article, we propose two combined techniques implemented on top of a state-of-the-art ASR accelerator in order to significantly reduce its energy consumption and memory requirements. First, by leveraging the locality among consecutive segments of the speech signal, we develop a Locality-AWare-Scheme (LAWS) which exploits the on-chip recently-explored data while removing most of the off-chip accesses during the ASR&#39;s decoding process. As a result, we remove up to 60 percent of ASR&#39;s workload. As the second step, we introduce an approach to improve LAWS&#39;s effectiveness by selectively adapting the amount of ASR&#39;s workload, based on run-time feedback. In particular, we exploit the fact that the confidence of the ASR system varies along the recognition process. When confidence is high, the ASR system can be more restrictive and reduce the amount of work. The end design including both techniques provides a saving of more than 87 percent in the memory requests and 2.3x reduction in energy consumption, and a speedup of 2.1x with respect to a state-of-the-art baseline design.},
  archive      = {J_TC},
  author       = {Reza Yazdani and Jose-Maria Arnau and Antonio González},
  doi          = {10.1109/TC.2020.2991002},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1197-1208},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LAWS: Locality-AWare scheme for automatic speech recognition},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HEAWS: An accelerator for homomorphic encryption on the
amazon AWS FPGA. <em>TC</em>, <em>69</em>(8), 1185–1196. (<a
href="https://doi.org/10.1109/TC.2020.2988765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic Encryption makes privacy preserving computing possible in a third party owned cloud by enabling computation on the encrypted data of users. However, software implementations of homomorphic encryption are very slow on general purpose processors. With the emergence of `FPGAs as a service&#39;, hardware-acceleration of computationally heavy workloads in the cloud are getting popular. In this article we propose HEAWS, a domain-specific coprocessor architecture for accelerating homomorphic function evaluation on the encrypted data using high-performance FPGAs available in the Amazon AWS cloud. To the best of our knowledge, we are the first to report hardware acceleration of homomorphic encryption using Amazon AWS FPGAs. Utilizing the massive size of the AWS FPGAs, we design a high-performance and parallel coprocessor architecture for the FV homomorphic encryption scheme which has become popular for computing exact arithmetic on the encrypted data. We design parallel building blocks and apply pipeline processing at different levels of the implementation hierarchy, and on top of such optimizations we instantiate multiple parallel coprocessors in the FPGA to execute several homomorphic computations simultaneously. While the absolute computation time can be reduced by deploying more computational resources, efficiency of the HW/SW communication interface plays an important role in homomorphic encryption as it is computation as well as data intensive. Our implementation utilizes state of the art 512-bit XDMA feature of high bandwidth communication available in the AWS Shell to reduce the overhead of HW/SW data transfer. Moreover, we explore the design-space to identify optimal off-chip data transfer strategy for feeding the parallel coprocessors in a time-shared manner. As a result of these optimizations, our AWS-based accelerator can perform 613 homomorphic multiplications per second for a parameter set that enables homomorphic computations of depth 4. Finally, we benchmark an artificial neural network for privacy-preserving forecasting of energy consumption in a Smart Grid application and observe five times speed up.},
  archive      = {J_TC},
  author       = {Furkan Turan and Sujoy Sinha Roy and Ingrid Verbauwhede},
  doi          = {10.1109/TC.2020.2988765},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1185-1196},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HEAWS: An accelerator for homomorphic encryption on the amazon AWS FPGA},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating generative neural networks on unmodified deep
learning processors—a software approach. <em>TC</em>, <em>69</em>(8),
1172–1184. (<a href="https://doi.org/10.1109/TC.2020.3001033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative neural network is a new category of neural networks and it has been widely utilized in many applications such as content generation, unsupervised learning, segmentation, and pose estimation. It typically involves massive computing-intensive deconvolution operations that cannot be fitted to conventional neural network processors directly. However, prior works mainly investigated specialized hardware architectures through intensive hardware modifications to the existing deep learning processors to accelerate deconvolution together with the convolution. In contrast, this article proposes a novel deconvolution implementation with a software approach and enables fast and efficient deconvolution execution on the existing deep learning processors. Our proposed method reorganizes the computation of deconvolution and allows the deep learning processors to treat it as the standard convolution by splitting the original deconvolution filters into multiple small filters. Compared to prior acceleration schemes, the implemented acceleration scheme achieves 2.4× -4.3× performance speedup and reduces the energy consumption by 27.7 -54.5 percent on a set of realistic benchmarks. In addition, we have also applied the deconvolution computing approach to the off-the-shelf commodity deep learning processors. The performance of deconvolution also exhibits significant performance speedup over prior deconvolution implementations.},
  archive      = {J_TC},
  author       = {Dawen Xu and Cheng Liu and Ying Wang and Kaijie Tu and Bingsheng He and Lei Zhang},
  doi          = {10.1109/TC.2020.3001033},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1172-1184},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating generative neural networks on unmodified deep learning Processors—A software approach},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating hyperdimensional computing on FPGAs by
exploiting computational reuse. <em>TC</em>, <em>69</em>(8), 1159–1171.
(<a href="https://doi.org/10.1109/TC.2020.2992662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired hyperdimensional (HD) computing emulates cognition by computing with long-size vectors. HD computing consists of two main modules: encoder and associative search. The encoder module maps inputs into high dimensional vectors, called hypervectors. The associative search finds the closest match between the trained model (set of hypervectors) and a query hypervector by calculating a similarity metric. To perform the reasoning task for practical classification problems, HD needs to store a non-binary model and uses costly similarity metrics as cosine. In this article we propose an FPGA-based acceleration of HD exploiting Computational Reuse (HD-Core) which significantly improves the computation efficiency of both encoding and associative search modules. HD-Core enables computation reuse in both encoding and associative search modules. We observed that consecutive inputs have high similarity which can be used to reduce the complexity of the encoding step. The previously encoded hypervector is reused to eliminate the redundant operations in encoding the current input. HD-Core, additionally eliminates the majority of multiplication operations by clustering the class hypervector values, and sharing the values among all the class hypervectors. Our evaluations on several classification problems show that HD-Core can provide 4.4x energy efficiency improvement and 4.8x speedup over the optimized GPU implementation while ensuring the same quality of classification. HD-Core provides 2.4x more throughput than the stateof-the-art FPGA implementation; on average, 40 percent of this improvement comes directly from enabling computation reuse in the encoding module and the rest comes from the computation reuse in the associative search module.},
  archive      = {J_TC},
  author       = {Sahand Salamat and Mohsen Imani and Tajana Rosing},
  doi          = {10.1109/TC.2020.2992662},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1159-1171},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating hyperdimensional computing on FPGAs by exploiting computational reuse},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FPDeep: Scalable acceleration of CNN training on
deeply-pipelined FPGA clusters. <em>TC</em>, <em>69</em>(8), 1143–1158.
(<a href="https://doi.org/10.1109/TC.2020.3000118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional Neural Networks (CNNs) have revolutionized numerous applications, but the demand for ever more performance remains unabated. Scaling CNN computations to larger clusters is generally done by distributing tasks in batch mode using methods such as distributed synchronous SGD. Among the issues with this approach is that, to make the distributed cluster work with high utilization, the workload distributed to each node must be large; this implies nontrivial growth in the SGD mini-batch size. In this article we propose a framework, called FPDeep, which uses a hybrid of model and layer parallelism to configure distributed reconfigurable clusters to train CNNs. This approach has numerous benefits. First, the design does not suffer from performance loss due to batch size growth. Second, work and storage are balanced among nodes through novel workload and weight partitioning schemes. Part of the mechanism is the surprising finding that it is preferable to store excess weights in neighboring devices rather than in local off-chip memory. Third, the entire system is a fine-grained pipeline. This leads to high parallelism and utilization and also minimizes the time that features need to be cached while waiting for back-propagation. As a result, storage demand is reduced to the point where only on-chip memory is used for the convolution layers. And fourth, we find that the simplest topology, a 1D array, is preferred for interconnecting the FPGAs thus enabling widespread applicability. We evaluate FPDeep with the Alexnet, VGG-16, and VGG-19 benchmarks. Results show that FPDeep has good scalability to a large number of FPGAs, with the limiting factor being the FPGA-to-FPGA bandwidth. But with 250 Gb/s bidirectional bandwidth per FPGA, which is easily supported by current generation FPGAs, FPDeep performance shows linearity up to 100 FPGAs. Energy efficiency is evaluated with respect to GOPs/J. FPDeep provides, on average, 6.4× higher energy efficiency than comparable GPU servers.},
  archive      = {J_TC},
  author       = {Tianqi Wang and Tong Geng and Ang Li and Xi Jin and Martin Herbordt},
  doi          = {10.1109/TC.2020.3000118},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1143-1158},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FPDeep: Scalable acceleration of CNN training on deeply-pipelined FPGA clusters},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PANTHER: A programmable architecture for neural network
training harnessing energy-efficient ReRAM. <em>TC</em>, <em>69</em>(8),
1128–1142. (<a href="https://doi.org/10.1109/TC.2020.2998456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide adoption of deep neural networks has been accompanied by ever-increasing energy and performance demands due to the expensive nature of training them. Numerous special-purpose architectures have been proposed to accelerate training: both digital and hybrid digital-analog using resistive RAM (ReRAM) crossbars. ReRAM-based accelerators have demonstrated the effectiveness of ReRAM crossbars at performing matrix-vector multiplication operations that are prevalent in training. However, they still suffer from inefficiency due to the use of serial reads and writes for performing the weight gradient and update step. A few works have demonstrated the possibility of performing outer products in crossbars, which can be used to realize the weight gradient and update step without the use of serial reads and writes. However, these works have been limited to low precision operations which are not sufficient for typical training workloads. Moreover, they have been confined to a limited set of training algorithms for fully-connected layers only. To address these limitations, we propose a bit-slicing technique for enhancing the precision of ReRAM-based outer products, which is substantially different from bit-slicing for matrix-vector multiplication only. We incorporate this technique into a crossbar architecture with three variants catered to different training algorithms. To evaluate our design on different types of layers in neural networks (fully-connected, convolutional, etc.) and training algorithms, we develop PANTHER, an ISA-programmable training accelerator with compiler support. Our design can also be integrated into other accelerators in the literature to enhance their efficiency. Our evaluation shows that PANTHER achieves up to 8.02×, 54.21×, and 103× energy reductions as well as 7.16×, 4.02×, and 16× execution time reductions compared to digital accelerators, ReRAM-based accelerators, and GPUs, respectively.},
  archive      = {J_TC},
  author       = {Aayush Ankit and Izzat El Hajj and Sai Rahul Chalamalasetti and Sapan Agarwal and Matthew Marinella and Martin Foltin and John Paul Strachan and Dejan Milojicic and Wen-Mei Hwu and Kaushik Roy},
  doi          = {10.1109/TC.2020.2998456},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1128-1142},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PANTHER: A programmable architecture for neural network training harnessing energy-efficient ReRAM},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating deep neural network in-situ training with
non-volatile and volatile memory based hybrid precision synapses.
<em>TC</em>, <em>69</em>(8), 1113–1127. (<a
href="https://doi.org/10.1109/TC.2020.3000218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute-in-memory (CIM) with emerging non-volatile memories (eNVMs) is time and energy efficient for deep neural network (DNN) inference. However, challenges still remain for DNN in-situ training with eNVMs due to the asymmetric weight update behavior, high programming latency and energy consumption. To overcome these challenges, a hybrid precision synapse combining eNVMs with capacitor has been proposed. It leverages the symmetric and fast weight update in the volatile capacitor, as well as the non-volatility and large dynamic range of the eNVMs. In this article, DNN in-situ training architecture with hybrid precision synapses is proposed and system level benchmarked is conducted. First, the circuit modules required for in-situ training with hybrid precision synapses are designed and the system architecture is proposed. Then, the impact of different weight precision configurations, weight transfer interval and limited capacitor retention time on training accuracy is investigated by incorporating hardware properties into Tensorflow simulation. Finally, the system-level benchmark is conducted at 32nm technology node in the modified NeuroSim simulator for hybrid precision synapse, in comparison with the baseline designs that are solely based on eNVMs or SRAM technology. The benchmark results show that CIM accelerator based on hybrid precision synapse achieves at least 3.07x and 2.89x better energy efficiency for training compared with its eNVM counterparts and SRAM technology at 32nm node, respectively. 227x and 33.8x better energy efficiency are obtained when compared to GPU and TPU. The scaling trend of hybrid precision synapse is projected towards 7nm node and comparison with state-of-the-art 7nm SRAM technology is made.},
  archive      = {J_TC},
  author       = {Yandong Luo and Shimeng Yu},
  doi          = {10.1109/TC.2020.3000218},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1113-1127},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating deep neural network in-situ training with non-volatile and volatile memory based hybrid precision synapses},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neuromorphic system for spatial and temporal information
processing. <em>TC</em>, <em>69</em>(8), 1099–1112. (<a
href="https://doi.org/10.1109/TC.2020.3000183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic systems that learn and predict from streaming inputs hold significant promise in pervasive edge computing and its applications. In this article, a neuromorphic system that processes spatio-temporal information on the edge is proposed. Algorithmically, the system is based on hierarchical temporal memory that inherently offers online learning, resiliency, and fault tolerance. Architecturally, it is a full custom mixed-signal design with an underlying digital communication scheme and analog computational modules. Therefore, the proposed system features reconfigurability, real-time processing, low power consumption, and low-latency processing. The proposed architecture is benchmarked to predict on real-world streaming data. The network&#39;s mean absolute percentage error on the mixed-signal system is 1.129 X lower compared to its baseline algorithm model. This reduction can be attributed to device non-idealities and probabilistic formation of synaptic connections. We demonstrate that the combined effect of Hebbian learning and network sparsity also plays a major role in extending the overall network lifespan. We also illustrate that the system offers 3.46 X reduction in latency and 77.02 X reduction in power consumption when compared to a custom CMOS digital design implemented at the same technology node. By employing specific low power techniques, such as clock gating, we observe 161.37 X reduction in power consumption.},
  archive      = {J_TC},
  author       = {Abdullah M. Zyarah and Kevin Gomez and Dhireesha Kudithipudi},
  doi          = {10.1109/TC.2020.3000183},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1099-1112},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Neuromorphic system for spatial and temporal information processing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial: IEEE TC special issue on domain-specific
architectures for emerging applications. <em>TC</em>, <em>69</em>(8),
1096–1098. (<a href="https://doi.org/10.1109/TC.2020.3002674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section examine domain-specific architectures for emerging applications. Presents innovative research in domain-specific architectures across a broad range of emerging applications.},
  archive      = {J_TC},
  author       = {Lisa Wu Wills and Karthik Swaminathan},
  doi          = {10.1109/TC.2020.3002674},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1096-1098},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editorial: IEEE TC special issue on domain-specific architectures for emerging applications},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <span class="math inline"><em>π</em></span>Π-BA: Bundle
adjustment hardware accelerator based on distribution of 3D-point
observations. <em>TC</em>, <em>69</em>(7), 1083–1095. (<a
href="https://doi.org/10.1109/TC.2020.2984611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle adjustment (BA) is a fundamental optimization technique used in many crucial applications, including 3D scene reconstruction, robotic localization, camera calibration, autonomous driving, street view map generation, and even space exploration etc. Essentially, BA is a joint non-linear optimization problem, and one which can consume a significant amount of time and power, especially for large optimization problems. Previous approaches of optimizing BA performance heavily rely on parallel processing or distributed computing, which trade higher power consumption for higher performance. In this article we propose p-BA, the first hardware-software co-designed BA hardware accelerator that exploits custom hardware to simultaneously achieve higher performance and power efficiency. Specifically, based on our key observation that not all 3D points appear on all images in a BA problem, we designed a Co-Observation Optimization technique to accelerate BA operations with optimized usage of memory and computation resources. In addition, we developed a hardware-friendly differentiation method, which combines the analytic and forward automatic differentiation to calculate derivatives of projection function in the BA problem. We have implemented the proposed design on an embedded FPGA SoC, and experimental results confirm that p-BA outperforms the existing software implementations in terms of performance and power consumption.},
  archive      = {J_TC},
  author       = {Qiang Liu and Shuzhen Qin and Bo Yu and Jie Tang and Shaoshan Liu},
  doi          = {10.1109/TC.2020.2984611},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1083-1095},
  shortjournal = {IEEE Trans. Comput.},
  title        = {$\pi$π-BA: Bundle adjustment hardware accelerator based on distribution of 3D-point observations},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WooKong: A ubiquitous accelerator for recommendation
algorithms with custom instruction sets on FPGA. <em>TC</em>,
<em>69</em>(7), 1071–1082. (<a
href="https://doi.org/10.1109/TC.2020.2988209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation algorithms, such as Neighborhood-based Collaborative- Filtering (CF), have been widely applied in various emerging machine learning applications. However, under the circumstance of the explosive big data, it poses significant challenges to CF recommendation algorithms as it is becoming quite time and energy-consuming. It has to be optimized and accelerated by powerful engines to process on large data scale. To solve these problems, in this article, we propose WooKong, a ubiquitous accelerator architecture for the collaborative-filtering recommendation on FPGA. It is able to accommodate three types of CF recommendation algorithms, including User-based CF, Item-based CF, and SlopeOne recommendations algorithms, with five different similarity analysis metrics including Jaccard, Cosine, CosineIR, euclidean, and Pearson. To maintain flexibility for these different CF algorithms and metrics, we adopt custom instruction sets to manipulate the learning and prediction accelerators. We implement a hardware prototype on a real Xilinx Zynq FPGA development board. Experimental results show that the proposed learning and prediction accelerators can achieve 8.0X speedup and 1.7X speedup compared with an Intel i7 processor respectively. The accelerator has the energy benefits of up to 137.4X compared with an NVIDIA Tesla K40C GPU, with the affordable hardware cost.},
  archive      = {J_TC},
  author       = {Chao Wang and Lei Gong and Xiang Ma and Xi Li and Xuehai Zhou},
  doi          = {10.1109/TC.2020.2988209},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1071-1082},
  shortjournal = {IEEE Trans. Comput.},
  title        = {WooKong: A ubiquitous accelerator for recommendation algorithms with custom instruction sets on FPGA},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating deep learning systems via critical set
identification and model compression. <em>TC</em>, <em>69</em>(7),
1059–1070. (<a href="https://doi.org/10.1109/TC.2020.2970917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern distributed engines are increasingly deployed to accelerate large-scaled deep learning (DL) training jobs. While the parallelism of distributed workers/nodes promises the scalability, the computation and communication overheads of the underlying iterative solving algorithms, e.g., stochastic gradient decent, unfortunately become the bottleneck for distributed DL training jobs. Existing approaches address such limitations by designing more efficient synchronization algorithms and model compressing techniques, but do not adequately address issues relating to processing massive datasets. In this article, we propose ClipDL, which accelerates the deep learning systems by simultaneously decreasing the number of model parameters as well as reducing the computations on critical data only. The core component of ClipDL is the estimation of critical set based on the observation that large proportions of input data have little influence on model parameter updating in many prevalent DL algorithms. We implemented ClipDL on Spark (a popular distributed engine for big data) and BigDL (based on de-factor distributed DL training architecture, parameter server), and integrated it with representative model compression techniques. The exhaustive experiments on real DL applications and datasets show ClipDL accelerates model training process by an average of 2.32 times while only incurring accuracy losses of 1.86 percent.},
  archive      = {J_TC},
  author       = {Rui Han and Chi Harold Liu and Shilin Li and Shilin Wen and Xue Liu},
  doi          = {10.1109/TC.2020.2970917},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1059-1070},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating deep learning systems via critical set identification and model compression},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pre-defined sparsity for low-complexity convolutional neural
networks. <em>TC</em>, <em>69</em>(7), 1045–1058. (<a
href="https://doi.org/10.1109/TC.2020.2972520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high energy cost of processing deep convolutional neural networks impedes their ubiquitous deployment in energy-constrained platforms such as embedded systems and IoT devices. This article introduces convolutional layers with pre-defined sparse 2D kernels that have support sets that repeat periodically within and across filters. Due to the efficient storage of our periodic sparse kernels, the parameter savings can translate into considerable improvements in energy efficiency due to reduced DRAM accesses, thus promising significant improvements in the trade-off between energy consumption and accuracy for both training and inference. To evaluate this approach, we performed experiments with two widely accepted datasets, CIFAR-10 and Tiny ImageNet in sparse variants of the ResNet18 and VGG16 architectures. Compared to baseline models, our proposed sparse variants require up to ~82\% fewer model parameters with 5.6× fewer FLOPs with negligible loss in accuracy for ResNet18 on CIFAR-10. For VGG16 trained on Tiny ImageNet, our approach requires 5.8× fewer FLOPs and up to ~83.3\% fewer model parameters with a drop in top-5 (top-1) accuracy of only 1.2\% (~2.1\%). We also compared the performance of our proposed architectures with that of ShuffleNet and MobileNetV2. Using similar hyperparameters and FLOPs, our ResNet18 variants yield an average accuracy improvement of ~2.8\%.},
  archive      = {J_TC},
  author       = {Souvik Kundu and Mahdi Nazemi and Massoud Pedram and Keith M. Chugg and Peter A. Beerel},
  doi          = {10.1109/TC.2020.2972520},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1045-1058},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Pre-defined sparsity for low-complexity convolutional neural networks},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neural network-based on-device learning anomaly detector
for edge devices. <em>TC</em>, <em>69</em>(7), 1027–1044. (<a
href="https://doi.org/10.1109/TC.2020.2973631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised anomaly detection is an approach to identify anomalies by learning the distribution of normal data. Backpropagation neural networks (i.e., BP-NNs) based approaches have recently drawn attention because of their good generalization capability. In a typical situation, BP-NN-based models are iteratively optimized in server machines with input data gathered from the edge devices. However, (1) the iterative optimization often requires significant efforts to follow changes in the distribution of normal data (i.e., concept drift), and (2) data transfers between edge and server impose additional latency and energy consumption. To address these issues, we propose ONLAD and its IP core, named ONLAD Core. ONLAD is highly optimized to perform fast sequential learning to follow concept drift in less than one millisecond. ONLAD Core realizes on-device learning for edge devices at low power consumption, which realizes standalone execution where data transfers between edge and server are not required. Experiments show that ONLAD has favorable anomaly detection capability in an environment that simulates concept drift. Evaluations of ONLAD Core confirm that the training latency is 1.95x~6.58x faster than the other software implementations. Also, the runtime power consumption of ONLAD Core implemented on PYNQ-Z1 board, a small FPGA/CPU SoC platform, is 5.0x~25.4x lower than them.},
  archive      = {J_TC},
  author       = {Mineto Tsukada and Masaaki Kondo and Hiroki Matsutani},
  doi          = {10.1109/TC.2020.2973631},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1027-1044},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A neural network-based on-device learning anomaly detector for edge devices},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed training of support vector machine on a
multiple-FPGA system. <em>TC</em>, <em>69</em>(7), 1015–1026. (<a
href="https://doi.org/10.1109/TC.2020.2993552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Machine (SVM) is a supervised machine learning model for classification tasks. Training SVM on a large number of data samples is challenging due to the high computational cost and memory requirement. Hence, model training is supported on a high-performance server which typically runs a sequential training algorithm on centralized data. However, as we move towards massive workloads, it will be impossible to store all the data in a centralized manner and expect such sequential training algorithms to scale on traditional processors. Moreover, with the growing demands of real-time machine learning for edge analytics, it is imperative to devise an efficient training framework with relatively cheaper computations and limited memory. Therefore, we propose and implement a first-of-its-kind system of multiple FPGAs as a distributed computing framework comprising up to eight FPGA units on Amazon F1 instances with negligible communication overhead to fully parallelize, accelerate, and scale the SVM training on decentralized data. Each FPGA unit has a pipelined SVM training IP logic core operating at 125 MHz with a power dissipation of 39 Watts for accelerating its allocated computations in the overall training process. We evaluate and compare the performance of the proposed system on five real SVM benchmarks.},
  archive      = {J_TC},
  author       = {Jyotikrishna Dass and Yashwardhan Narawane and Rabi N. Mahapatra and Vivek Sarin},
  doi          = {10.1109/TC.2020.2993552},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1015-1026},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Distributed training of support vector machine on a multiple-FPGA system},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning computers with fractal von neumann
architecture. <em>TC</em>, <em>69</em>(7), 998–1014. (<a
href="https://doi.org/10.1109/TC.2020.2982159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning techniques are pervasive tools for emerging commercial applications and many dedicated machine learning computers on different scales have been deployed in embedded devices, servers, and data centers. Currently, most machine learning computer architectures still focus on optimizing performance and energy efficiency instead of programming productivity. However, with the fast development in silicon technology, programming productivity, including programming itself and software stack development, becomes the vital reason instead of performance and power efficiency that hinders the application of machine learning computers. In this article, we propose Cambricon-F, which is a series of homogeneous, sequential, multi-layer, layer-similar, and machine learning computers with same ISA. A Cambricon-F machine has a fractal von Neumann architecture to iteratively manage its components: it is with von Neumann architecture and its processing components (sub-nodes) are still Cambricon-F machines with von Neumann architecture and the same ISA. Since different Cambricon-F instances with different scales can share the same software stack on their common ISA, Cambricon-Fs can significantly improve the programming productivity. Moreover, we address four major challenges in Cambricon-F architecture design, which allow Cambricon-F to achieve a high efficiency. We implement two Cambricon-F instances at different scales, i.e., Cambricon-F100 and Cambricon-F1. Compared to GPU based machines (DGX-1 and 1080Ti), Cambricon-F instances achieve 2.82x, 5.14x better performance, 8.37x, 11.39x better efficiency on average, with 74.5, 93.8 percent smaller area costs, respectively. We further propose Cambricon-FR, which enhances the Cambricon-F machine learning computers to flexibly and efficiently support all the fractal operations with a reconfigurable fractal instruction set architecture. Compared to the Cambricon-F instances, Cambricon-FR machines achieve 1.96x, 2.49x better performance on average. Most importantly, Cambricon-FR computers are able to save the code length with a factor of 5.83, thus significantly improving the programming productivity.},
  archive      = {J_TC},
  author       = {Yongwei Zhao and Zhe Fan and Zidong Du and Tian Zhi and Ling Li and Qi Guo and Shaoli Liu and Zhiwei Xu and Tianshi Chen and Yunji Chen},
  doi          = {10.1109/TC.2020.2982159},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {998-1014},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Machine learning computers with fractal von neumann architecture},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling efficient fast convolution algorithms on GPUs via
MegaKernels. <em>TC</em>, <em>69</em>(7), 986–997. (<a
href="https://doi.org/10.1109/TC.2020.2973144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Convolutional Neural Networks (CNNs) require a massive amount of convolution operations. To address the overwhelming computation problem, Winograd and FFT fast algorithms have been used as effective approaches to reduce the number of multiplications. Inputs and filters are transformed into special domains then perform element-wise multiplication, which can be transformed into batched GEMM operation. Different stages of computation contain multiple tasks with different computation and memory behaviors, and they share intermediate data, which provides the opportunity to fuse these tasks into a monolithic kernel. But traditional kernel fusion suffers from the problem of insufficient shared memory, which limits the performance. In this article, we propose a new kernel fusion technique for fast convolution algorithms based on MegaKernel. GPU thread blocks are assigned with different computation tasks and we design a mapping algorithm to assign tasks to thread blocks. We build a scheduler which fetches and executes the tasks following the dependency relationship. Evaluation of modern CNNs shows that our techniques achieve an average of 1.25X and 1.7X speedup compared to cuDNN&#39;s two implementations on Winograd convolution algorithm.},
  archive      = {J_TC},
  author       = {Liancheng Jia and Yun Liang and Xiuhong Li and Liqiang Lu and Shengen Yan},
  doi          = {10.1109/TC.2020.2973144},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {986-997},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling efficient fast convolution algorithms on GPUs via MegaKernels},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Addressing irregularity in sparse neural networks through a
cooperative software/hardware approach. <em>TC</em>, <em>69</em>(7),
968–985. (<a href="https://doi.org/10.1109/TC.2020.2978475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have become the dominant algorithms rapidly as they achieve state-of-the-art performance in a broad range of applications such as image recognition, speech recognition, and natural language processing. However, neural networks keep moving toward deeper and larger architectures, posing a great challenge to hardware systems due to the huge amount of data and computations. Although sparsity has emerged as an effective solution for reducing the intensity of computation and memory accesses directly, irregularity caused by sparsity (including sparse synapses and neurons) prevents accelerators from completely leveraging the benefits, i.e., it also introduces costly indexing module in accelerators. In this article, we propose a cooperative software/hardware approach to address the irregularity of sparse neural networks efficiently. Initially, we observe the local convergence, namely larger weights tend to gather into small clusters during training. Based on that key observation, we propose a software-based coarse-grained pruning technique to reduce the irregularity of sparse synapses drastically. The coarse-grained pruning technique, together with local quantization, significantly reduces the size of indexes and improves the network compression ratio. We further design a multi-core hardware accelerator, Cambricon-SE, to address the remaining irregularity of sparse synapses and neurons efficiently. The novel accelerator have three key features: 1) selector modulesto filter unnecessary synapses and neurons, 2) compress/decompress modules for exploiting the sparsity in data transmission (which is rarely studied in previous work), and 3) a multi-core architecture with elevated throughput to meet the real-time processing requirement. Compared against a state-of-the-art sparse neural network accelerator, our accelerator is 1.20x and 2.72x better in terms of performance and energy efficiency, respectively. Moreover, for real-time video analysis tasks, Cambricon-SE can process 1080p video at the speed of 76.59 fps.},
  archive      = {J_TC},
  author       = {Xi Zeng and Tian Zhi and Xuda Zhou and Zidong Du and Qi Guo and Shaoli Liu and Bingrui Wang and Yuanbo Wen and Chao Wang and Xuehai Zhou and Ling Li and Tianshi Chen and Ninghui Sun and Yunji Chen},
  doi          = {10.1109/TC.2020.2978475},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {968-985},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Addressing irregularity in sparse neural networks through a cooperative Software/Hardware approach},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MViD: Sparse matrix-vector multiplication in mobile DRAM for
accelerating recurrent neural networks. <em>TC</em>, <em>69</em>(7),
955–967. (<a href="https://doi.org/10.1109/TC.2020.2984496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent Neural Networks (RNNs) spend most of their execution time performing matrix-vector multiplication (MV-mul). Because the matrices in RNNs have poor reusability and the ever-increasing size of the matrices becomes too large to fit in the on-chip storage of mobile/IoT devices, the performance and energy efficiency of MV-mul is determined by those of main-memory DRAM. Therefore, computing MV-mul within DRAM draws much attention. However, previous studies lacked consideration for the matrix sparsity, the power constraints of DRAM devices, and concurrency in accessing DRAM from processors while performing MV-mul. We propose a main-memory architecture called MViD, which performs MV-mul by placing MAC units inside DRAM banks. For higher computational efficiency, we use a sparse matrix format and exploit quantization. Because of the limited power budget for DRAM devices, we implement the MAC units only on a portion of the DRAM banks. We architect MViD to slow down or pause MV-mul for concurrently processing memory requests from processors while satisfying the limited power budget. Our results show that MViD provides 7.2× higher throughput compared to the baseline system with four DRAM ranks (performing MV-mul in a chip-multiprocessor) while running inference of Deep Speech 2 with a memory-intensive workload.},
  archive      = {J_TC},
  author       = {Byeongho Kim and Jongwook Chung and Eojin Lee and Wonkyung Jung and Sunjung Lee and Jaewan Choi and Jaehyun Park and Minbok Wi and Sukhan Lee and Jung Ho Ahn},
  doi          = {10.1109/TC.2020.2984496},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {955-967},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MViD: Sparse matrix-vector multiplication in mobile DRAM for accelerating recurrent neural networks},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CIMAT: A compute-in-memory architecture for on-chip training
based on transpose SRAM arrays. <em>TC</em>, <em>69</em>(7), 944–954.
(<a href="https://doi.org/10.1109/TC.2020.2980533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid development in deep neural networks (DNNs) is enabling many intelligent applications. However, on-chip training of DNNs is challenging due to the extensive computation and memory bandwidth requirements. To solve the bottleneck of the memory wall problem, compute-in-memory (CIM) approach exploits the analog computation along the bit line of the memory array thus significantly speeds up the vector-matrix multiplications. So far, most of the CIM-based architectures target at implementing inference engine for offline training only. In this article, we propose CIMAT, a CIM Architecture for Training. At the bitcell level, we design two versions of 7T and 8T transpose SRAM to implement bi-directional vector-to-matrix multiplication that is needed for feedforward (FF) and backprogpagation (BP). Moreover, we design the periphery circuitry, mapping strategy and the data flow for the BP process and weight update to support the on-chip training based on CIM. To further improve training performance, we explore the pipeline optimization of proposed architecture. We utilize the mature and advanced CMOS technology at 7 nm to design the CIMAT architecture with 7T/8T transpose SRAM array that supports bi-directional parallel read. We explore the 8-bit training performance of ImageNet on ResNet-18, showing that 7T-based design can achieve 3.38× higher energy efficiency (~6.02 TOPS/W), 4.34× frame rate (~4,020 fps) and only 50 percent chip size compared to the baseline architecture with conventional 6T SRAM array that supports row-by-row read only. The even better performance is obtained with 8T-based architecture, which can reach ~10.79 TOPS/W and ~48,335 fps with 74-percent chip area compared to the baseline.},
  archive      = {J_TC},
  author       = {Hongwu Jiang and Xiaochen Peng and Shanshi Huang and Shimeng Yu},
  doi          = {10.1109/TC.2020.2980533},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {944-954},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CIMAT: A compute-in-memory architecture for on-chip training based on transpose SRAM arrays},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crane: Mitigating accelerator under-utilization caused by
sparsity irregularities in CNNs. <em>TC</em>, <em>69</em>(7), 931–943.
(<a href="https://doi.org/10.1109/TC.2020.2981080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved great success in numerous AI applications. To improve inference efficiency of CNNs, researchers have proposed various pruning techniques to reduce both computation intensity and storage overhead. These pruning techniques result in multi-level sparsity irregularities in CNNs. Together with that in activation matrices, which is induced by employment of ReLU activation function, all these sparsity irregularities cause a serious problem of computation resource under-utilization in sparse CNN accelerators. To mitigate this problem, we propose a method of load-balancing based on a workload stealing technique. We demonstrate that this method can be applied to two major inference data-flows, which cover all state-of-the-art sparse CNN accelerators. Based on this method, we present an accelerator, called Crane, which addresses all kinds of sparsity irregularities in CNNs. We perform a fair comparison between Crane and state-of-the-art prior approaches. Experimental results show that Crane improves performance by 27\% ~ 88\% and reduces energy consumption by 16\% ~ 48\%, respectively, compared to the counterparts.},
  archive      = {J_TC},
  author       = {Yijin Guan and Guangyu Sun and Zhihang Yuan and Xingchen Li and Ningyi Xu and Shu Chen and Jason Cong and Yuan Xie},
  doi          = {10.1109/TC.2020.2981080},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {931-943},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Crane: Mitigating accelerator under-utilization caused by sparsity irregularities in CNNs},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editors’ introduction to the special issue on machine
learning architectures and accelerators. <em>TC</em>, <em>69</em>(7),
929–930. (<a href="https://doi.org/10.1109/TC.2020.2997574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The twelve papers in this special section focus on machine learning architectures and accelerators. Deep learning or deep neural networks (DNNs), as one of the most powerful machine learning techniques, has achieved extraordinary performance in computer vision and surveillance, speech recognition and natural language processing, healthcare and disease diagnosis, etc. Various forms of DNNs have been proposed, including Convolutional Neural Networks, Recurrent Neural Networks, Deep Reinforcement Learning, Transformer model, etc. Deep learning exhibits an offline training phase to derive the weight parameters from an excessive training dataset, as well as an online inference phase to perform classification/prediction/perception/ control tasks based on the trained model. The paper in this section aim to find a convergence of software and hardware/architecture. It aims at DNN algorithms, parallel computing, and compiler code generation techniques that are hardware/architecture friendly, as well as computer architectures that are universal and consistently highly performant on a wide range of DNN algorithms and applications. In this co-design and co-optimization framework we can mitigate the limitation of investigating in only a single direction, shedding some light on the future of embedded, ubiquitous artificial intelligence.},
  archive      = {J_TC},
  author       = {Xuehai Qian and Yanzhi Wang and Avinash Karanth},
  doi          = {10.1109/TC.2020.2997574},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {929-930},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editors’ introduction to the special issue on machine learning architectures and accelerators},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast filtering mechanism to improve efficiency of
large-scale video analytics. <em>TC</em>, <em>69</em>(6), 914–928. (<a
href="https://doi.org/10.1109/TC.2020.2970413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surveillance cameras are ubiquitous around us. Emerging full-feature object-detection models can analyze surveillance videos with high accuracy but consume much computation. Directly applying these models for practical scenarios with large-scale cameras is prohibitively expensive. This, however, is wasteful and unnecessary considering that user-defined anomalies occur rarely among these videos. Therefore, we propose FFS-VA, a multi-stage Fast Filtering Mechanism for Video Analytics, to make video analytics much cost-effective. FFS-VA filters out the frames without the user-defined events by two stream-specialized filters and a cheap full-function model, to reduce the number of frames reaching the full-feature model. FFS-VA presents a global feedback-queue approach to balance the processing speeds of different filters in intra-stream and inter-stream processes. FFS-VA designs a dynamic batch technique to achieve a trade-off between throughput and latency. FFS-VA can also efficiently scale to multiple GPUs. We evaluate FFS-VA against the state-of-the-art YOLOv3 under the same hardware and video workloads. The experimental results show that under a 12.88 percent target-object occurrence rate on two GPUs, FFS-VA can support up to 30 concurrent video streams (15× more than YOLOv3) in the online case, and obtain 10× speedup when offline analyzing a stream, with an accuracy loss of less than 2 percent.},
  archive      = {J_TC},
  author       = {Chen Zhang and Qiang Cao and Hong Jiang and Wenhui Zhang and Jingjun Li and Jie Yao},
  doi          = {10.1109/TC.2020.2970413},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {914-928},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A fast filtering mechanism to improve efficiency of large-scale video analytics},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithms for inversion mod <span
class="math inline"><em>p</em><sup><em>k</em></sup></span>. <em>TC</em>,
<em>69</em>(6), 907–913. (<a
href="https://doi.org/10.1109/TC.2020.2970411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes and analyzes all existing algorithms for computing x = a -1 (mod p k ) for a prime p, and also introduces a new algorithm based on the exact solution of linear equations using p-adic expansions. The algorithm starts with the initial value c = a -1 (mod p) and iterativelycomputes the digits of the inverse x = a -1 (mod p k ) in base p. The mod 2 version of the algorithm is more efficient than all existing algorithms for small values of k. Moreover, it stands out as being the only one that works for any p, any k, and digit-by-digit. While the new algorithm is asymptotically worse off, it requires the minimal number of arithmetic operations (just a single addition) per step, as compared to all existing algorithms.},
  archive      = {J_TC},
  author       = {Çetin Kaya Koç},
  doi          = {10.1109/TC.2020.2970411},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {907-913},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Algorithms for inversion mod $p^k$},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive thermal management framework for heterogeneous
multi-core processors. <em>TC</em>, <em>69</em>(6), 894–906. (<a
href="https://doi.org/10.1109/TC.2020.2970062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-the-shelf embedded systems have adopted heterogeneous multi-core processors which have high-performance big cores and low-power small cores. Though there are two different types of cores in heterogeneous multi-core processors, conventional DVFS (Dynamic Voltage and Frequency Scaling)-based DTM (Dynamic Thermal Management) techniques do not utilize the different types of cores to cool down hot cores. Rather, they primarily reduce the voltage and frequency of the hot cores, leading to performance degradation. In this article, we propose a novel adaptive DTM framework for heterogeneous multi-core processors, which utilizes the big and small cores to prevent performance degradation. Our proposed framework exploits two migration-based DTM techniques: 1) a technique (denoted as Migration big↔big ) that migrates applications from hot big cores (big cores whose temperature is above a pre-defined threshold) to cold big cores (big cores whose temperature is below the threshold) and 2) a technique (denoted as Migration big↔small ) that migrates all applications from the big cores to the small cores. In case of thermal emergency of the big cores, our proposed framework checks the number of cold big cores. When there exist available cold big cores, our proposed framework employs Migrationbig↔big to cool down the hot big cores while not reducing the big core frequency. On the other hand, when there does not exist any available cold big core, our proposed framework employs one between Migrationbig↔small and a DVFS-based DTM technique, which is expected to result in better performance. In our experiments on an embedded development board, our proposed framework improves the average performance by 8.9 percent, compared to ARM&#39;s DVFS-based IPA (Intelligent Power Allocation), satisfying thermal constraints. Our framework also improves the average performance by 10.4 percent, compared to a state-of-the-art predictive DVFS-based DTM technique.},
  archive      = {J_TC},
  author       = {Young Geun Kim and Minyong Kim and Joonho Kong and Sung Woo Chung},
  doi          = {10.1109/TC.2020.2970062},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {894-906},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An adaptive thermal management framework for heterogeneous multi-core processors},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep reinforcement learning based offloading game in edge
computing. <em>TC</em>, <em>69</em>(6), 883–893. (<a
href="https://doi.org/10.1109/TC.2020.2969148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a new paradigm to provide strong computing capability at the edge of pervasive radio access networks close to users. A critical research challenge of edge computing is to design an efficient offloading strategy to decide which tasks can be offloaded to edge servers with limited resources. Although many research efforts attempt to address this challenge, they need centralized control, which is not practical because users are rational individuals with interests to maximize their benefits. In this article, we study to design a decentralized algorithm for computation offloading, so that users can independently choose their offloading decisions. Game theory has been applied in the algorithm design. Different from existing work, we address the challenge that users may refuse to expose their information about network bandwidth and preference. Therefore, it requires that our solution should make the offloading decision without such knowledge. We formulate the problem as a partially observable Markov decision process (POMDP), which is solved by a policy gradient deep reinforcement learning (DRL) based approach. Extensive simulation results show that our proposal significantly outperforms existing solutions.},
  archive      = {J_TC},
  author       = {Yufeng Zhan and Song Guo and Peng Li and Jiang Zhang},
  doi          = {10.1109/TC.2020.2969148},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {883-893},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A deep reinforcement learning based offloading game in edge computing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AxMAP: Making approximate adders aware of input patterns.
<em>TC</em>, <em>69</em>(6), 868–882. (<a
href="https://doi.org/10.1109/TC.2020.2968905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making approximate computing specific to user requirements is crucial to system performance, energy-efficiency, and reliability. However, developing hardware for such optimization becomes a significant challenge due to the high cost of examining all potential choices while exploring a large design space. One determinant aspect of exploring a design space is the efficiency of evaluating error metrics, such as the Mean Error Distance (MED) and the Error Probability (EP), for each possible choice within the search space. Since computing these error-metrics is quite time-consuming, efficient calculation approaches are essential. This article proposes a novel formal approach to accurately compute the EP and MED of approximate adders for any input pattern at a linear time and space complexity. Our experimental results indicate that the proposed approach can accurately compute the error-metrics of large approximate adders at a 150 times faster speed compared to the Monte Carlo sampling methods. We then develop AxMAP, a design tool based on the proposed error-metrics computation that generates energy-efficient approximate adders for any given input pattern. When applied to image processing applications, AxMAP produces more than 150 different designs for adders that achieve superior performance and energy-efficiency compared to the existing state-of-the-art approximate adders.},
  archive      = {J_TC},
  author       = {Morteza Rezaalipour and Mohammad Rezaalipour and Masoud Dehyadegari and Mahdi Nazm Bojnordi},
  doi          = {10.1109/TC.2020.2968905},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {868-882},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AxMAP: Making approximate adders aware of input patterns},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate cost estimation of memory systems utilizing machine
learning and solutions from computer vision for design automation.
<em>TC</em>, <em>69</em>(6), 856–867. (<a
href="https://doi.org/10.1109/TC.2020.2968888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware/software co-designs are usually defined at high levels of abstractions at the beginning of the design process in order to provide a variety of options on how to realize a system. This allows for design exploration which relies on knowing the costs of different design configurations (with respect to hardware usage and firmware metrics). To this end, methods for cost estimation are frequently applied in industrial practice. However, currently used methods oversimplify the problem and ignore important features, leading to estimates which are far off from real values. In this article, we address this problem for memory systems. To this end, we borrow and re-adapt solutions based on Machine Learning (ML) which have been found suitable for problems from the domain of Computer Vision (CV). Based on that, an approach is proposed which outperforms existing methods for cost estimation. Experimental evaluations within an industrial context show that, while the accuracy of the state-of-the-art approach is frequently off by more than 20 percent for area estimation and more than 15 percent for firmware estimation, the method proposed in this article comes rather close to the actual values (just 5-7 percent off for both area and firmware). Furthermore, our approach outperforms existing methods for scalability, generalization, and decrease in manual effort.},
  archive      = {J_TC},
  author       = {Lorenzo Servadei and Edoardo Mosca and Elena Zennaro and Keerthikumara Devarajegowda and Michael Werner and Wolfgang Ecker and Robert Wille},
  doi          = {10.1109/TC.2020.2968888},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {856-867},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accurate cost estimation of memory systems utilizing machine learning and solutions from computer vision for design automation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical orchestration of disaggregated memory.
<em>TC</em>, <em>69</em>(6), 844–855. (<a
href="https://doi.org/10.1109/TC.2020.2968525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents XMemPod, a hierarchical disaggregated memory orchestration system. XMemPod virtualizes cluster wide memory to scale large memory workloads in virtualized clouds. It makes three novel contributions: (1) XMemPod offers efficient, transparent, and dynamic sharing of available memory that is disaggregated across VMs on the same host or in the cluster. (2) XMemPod provides a hierarchical memory expansion framework, which enables memory-intensive workloads on a VM to expand its memory demand over virtualized host memory first, and remote memory next, before resorting to external disk. (3) XMemPod provides a suite of optimization techniques to further improve the utilization and access latency of disaggregated memory. XMemPod is deployed on a virtualized RDMA cluster without any modifications to user applications and the OSes. Evaluated with multiple workloads on unmodified Spark, Apache Hadoop, Memcached, Redis and VoltDB, using XMemPod, throughputs of these applications improve by 11× to 612× over conventional Linux, and by 1.7× to 14× over the existing representative remote memory paging systems, and yet the total amount of network traffic consumed by XMemPod is only 24 percent of the existing approaches.},
  archive      = {J_TC},
  author       = {Wenqi Cao and Ling Liu},
  doi          = {10.1109/TC.2020.2968525},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {844-855},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hierarchical orchestration of disaggregated memory},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Request flow coordination for growing-scale solid-state
drives. <em>TC</em>, <em>69</em>(6), 832–843. (<a
href="https://doi.org/10.1109/TC.2020.2968439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance-intensive applications have led both interface and architecture changes of high-end, growing-scale solid-state drives (SSDs). However, we observe that most of the time, the actual drive performance could not be easily scaled or boosted up with the increasing of internal resources of growing-scale SSDs due to the potential congestion of I/O requests. Such observation inspires this article to look for a request flow coordination design to appropriately control and throttle the I/O request over the increasingly-complicated SSD internal organization with manageable coordination overhead. The main objective is to avoid overloading or congesting any sub-module of growing-scale SSDs by making good use of the abundant internal resources, so as to effectively improve the drive performance in terms of the request-response time. The capability of the proposed design was evaluated with realistic and intensive I/O workloads, and the results are very encouraging.},
  archive      = {J_TC},
  author       = {Ming-Chang Yang and Yuan-Hao Chang and Tei-Wei Kuo and Chun-Feng Wu},
  doi          = {10.1109/TC.2020.2968439},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {832-843},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Request flow coordination for growing-scale solid-state drives},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). XeFlow: Streamlining inter-processor pipeline execution for
the discrete CPU-GPU platform. <em>TC</em>, <em>69</em>(6), 819–831. (<a
href="https://doi.org/10.1109/TC.2020.2968302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, GPUs have achieved high throughput computing by running plenty of threads. However, owing to disjoint memory spaces of discrete CPU-GPU systems, exploiting CPU and GPU within a data processing pipeline is a non-trivial issue, which can only be resolved by the coarse-grained workflow of “copy-kernel-copy” or its variants in essence. There is an underlying bottleneck caused by frequent inter-processor invocations for fine-grained batch sizes. This article presents XeFlow that enables streamlined execution by leveraging hardware mechanisms inside new generation GPUs. XeFlow significantly reduces costly explicit copy and kernel launching within existing fashions. As an alternative, XeFlow introduces persistent operators that continuously process data through shared topics, which establish efficient inter-processor data channels via hardware page faults. Compared with the default “copy-kernel-copy” method, XeFlow shows up to 2.4χ 3.1χ performance advantages in both coarse-grained and fine-grained pipeline execution. To demonstrate its potentials, this article also evaluates two GPU-accelerated applications, including data encoding and OLAP query.},
  archive      = {J_TC},
  author       = {Zhifang Li and Beicheng Peng and Chuliang Weng},
  doi          = {10.1109/TC.2020.2968302},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {819-831},
  shortjournal = {IEEE Trans. Comput.},
  title        = {XeFlow: Streamlining inter-processor pipeline execution for the discrete CPU-GPU platform},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Page reusability-based cache partitioning for multi-core
systems. <em>TC</em>, <em>69</em>(6), 812–818. (<a
href="https://doi.org/10.1109/TC.2020.2968066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern multi-core processors provide a shared last level cache (LLC) where data from all cores are placed to improve performance. However, this opens a new challenge for cache management, owing to cache pollution. With cache pollution, data with weak temporal locality can evict other data with strong temporal locality when both are mapped into the same cache set. In this article, we propose page reusability-based cache partitioning (PRCP) for multi-core systems to maximize cache utilization by minimizing cache pollution. To achieve this, PRCP divides pages into two groups: (1) highly-reused pages and (2) lowly-reused pages. The reusability of each page is collected online via periodic page table scans. PRCP then dynamically partitions the shared cache into two corresponding areas using page coloring technique. We have implemented PRCP in Linux kernel and evaluated it using SPEC CPU2006 benchmarks. The results show that our scheme can achieve comparable performance to the optimal offline MRC-guided process-based cache partitioning scheme without a priori knowledge of workloads.},
  archive      = {J_TC},
  author       = {Jiwoong Park and Heonyoung Yeom and Yongseok Son},
  doi          = {10.1109/TC.2020.2968066},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {812-818},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Page reusability-based cache partitioning for multi-core systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prune and plant: Efficient placement and parallelism of
virtual network functions. <em>TC</em>, <em>69</em>(6), 800–811. (<a
href="https://doi.org/10.1109/TC.2020.2967661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV) is a promising solution to realize a variety of network services. By definition, virtual network functions (VNFs) are chained together to realize different services. However, chaining is not an ideal solution as service latency grows linearly with respect to the length of the chain. Motivated by the fact that many VNFs can be parallelized, we investigate parallelism of VNFs for acceleration. The dependency of the VNFs is characterized by a directed acyclic graph (DAG). We aim to deploy the VNFs in the right place and process them in parallel without violating the DAG, to minimize the overall delay. However, directly solving the delay minimization problem is NP-hard, and it may also introduce a large number of duplicated packets to burden the system. To deal with these issues, we propose the Prune and Plant (P&amp;P) scheme with polynomial computational complexity, to reduce the overall delay while limiting the number of duplicated packets. P&amp;P comprises two stages: in the Prune stage, we prune the original DAG into a series-parallel graph (SP-graph), which eliminates NP-hardness while maintaining parallelism of VNFs. In the Plant stage, we find the optimal placement for the VNFs with respect to the SP-graph. By both simulation and prototyping, we demonstrate that P&amp;P significantly outperforms benchmark schemes.},
  archive      = {J_TC},
  author       = {Wei Bao and Dong Yuan and Bing Bing Zhou and Albert Y. Zomaya},
  doi          = {10.1109/TC.2020.2967661},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {800-811},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Prune and plant: Efficient placement and parallelism of virtual network functions},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information assurance through redundant design: A novel TNU
error-resilient latch for harsh radiation environment. <em>TC</em>,
<em>69</em>(6), 789–799. (<a
href="https://doi.org/10.1109/TC.2020.2966200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In nano-scale CMOS technologies, storage cells such as latches are becoming increasingly sensitive to triple-node-upset (TNU) errors caused by harsh radiation effects. In the context of information assurance through redundant design, this article proposes a novel low-cost and TNU on-line self-recoverable latch design which is robust against harsh radiation effects. The latch mainly consists of a series of mutually interlocked 3-input Muller C-elements (CEs) that forms a circular structure. The output of any CE in the latch respectively feeds back to one input of some specified downstream CEs, making the latch completely self-recoverable from any possible TNU, i.e., the latch is completely TNU-resilient. Simulation results demonstrate the complete TNU-resiliency of the proposed latch. In addition, due to the use of fewer transistors and a high-speed path, the proposed latch reduces the delay-power-area product by approximately 91 percent compared with the state-of-the-art TNU hardened latch (TNUHL), which cannot provide a complete TNU-resiliency.},
  archive      = {J_TC},
  author       = {Aibin Yan and Yuanjie Hu and Jie Cui and Zhili Chen and Zhengfeng Huang and Tianming Ni and Patrick Girard and Xiaoqing Wen},
  doi          = {10.1109/TC.2020.2966200},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {789-799},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Information assurance through redundant design: A novel TNU error-resilient latch for harsh radiation environment},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Idler: I/o workload controlling for better responsiveness on
host-aware shingled magnetic recording drives. <em>TC</em>,
<em>69</em>(6), 777–788. (<a
href="https://doi.org/10.1109/TC.2020.2966194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Host-Aware/Drive-Managed Shingled Magnetic Recording (SMR) drives can accept non-sequential writes using a buffer called media cache. Data in the media cache will be migrated to its designated location by a cleaning process if the buffer is full (blocking cleaning) or the drive is idle (idle cleaning). However, blocking cleanings can severely extend the I/O response time. Therefore, it is crucial to fully understand the cleaning process and find ways of mitigating the caused performance degradation. In this article we further evaluate the cleaning process and propose a potential remedy scheme called Idler on Host-Aware SMR drives. Idler adaptively induces idle cleanings based on dynamic workload characteristics and media cache usages to reduce the severity of blocking cleanings. Our evaluations show that in the workloads with a small non-sequential write ratio (about 10 percent), Idler can reduce the tail response time and the workload finish time by 56-88 and 10-23 percent, respectively, compared with those without such control. With the help of an external write buffer on an SSD, the tail response time of SMR drives with Idler can be closer to that of conventional disk drives.},
  archive      = {J_TC},
  author       = {Baoquan Zhang and Ming-Hong Yang and Xuchao Xie and David H.C. Du},
  doi          = {10.1109/TC.2020.2966194},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {777-788},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Idler: I/O workload controlling for better responsiveness on host-aware shingled magnetic recording drives},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neural network based fault management scheme for reliable
image processing. <em>TC</em>, <em>69</em>(5), 764–776. (<a
href="https://doi.org/10.1109/TC.2020.2965518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional reliability approaches introduce relevant costs to achieve unconditional correctness during data processing. However, many application environments are inherently tolerant to a certain degree of inexactness or inaccuracy. In this article, we focus on the practical scenario of image processing in space, a domain where faults are a threat, while the applications are inherently tolerant to a certain degree of errors. We first introduce the concept of usability of the processed image to relax the traditional requirement of unconditional correctness, and to limit the computational overheads related to reliability. We then introduce our new flexible and lightweight fault management methodology for inaccurate application environments. A key novelty of our scheme is the utilization of neural networks to reduce the costs associated with the occurrence and the detection of faults. Experiments on two aerospace image processing case studies show overall time savings of 14.89 and 34.72 percent for the two applications, respectively, as compared with the baseline classical Duplication with Comparison scheme.},
  archive      = {J_TC},
  author       = {Matteo Biasielli and Cristiana Bolchini and Luca Cassano and Erdem Koyuncu and Antonio Miele},
  doi          = {10.1109/TC.2020.2965518},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {764-776},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A neural network based fault management scheme for reliable image processing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated performance modeling of HPC applications using
machine learning. <em>TC</em>, <em>69</em>(5), 749–763. (<a
href="https://doi.org/10.1109/TC.2020.2964767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated performance modeling and performance prediction of parallel programs are highly valuable in many use cases, such as in guiding task management and job scheduling, offering insights of application behaviors, and assisting resource requirement estimation. The performance of parallel programs is affected by numerous factors, including but not limited to hardware, applications, algorithms, and input parameters, thus an accurate performance prediction is often a challenging and daunting task. In this article, we focus on automatically predicting the execution time of parallel programs (more specifically, MPI programs) with different inputs, at different scales, and without domain knowledge. We model the correlation between the execution time and domain-independent runtime features. These features include values of variables, counters of branches, loops, and MPI communications. Through automatically instrumenting an MPI program, each execution of the program will output a feature vector and its corresponding execution time. After collecting data from executions with different inputs, a random forest machine learning approach is used to build an empirical performance model, which can predict the execution time of the program given a new input. A transfer learning method is used to reuse an existing performance model and improve the prediction accuracy on a new platform that lacks historical execution data. Our experiments and analyses of three parallel applications, Graph500, GalaxSee, and SMG2000, on three different systems confirm that our method performs well, with less than 20 percent prediction error on average.},
  archive      = {J_TC},
  author       = {Jingwei Sun and Guangzhong Sun and Shiyan Zhan and Jiepeng Zhang and Yong Chen},
  doi          = {10.1109/TC.2020.2964767},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {749-763},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Automated performance modeling of HPC applications using machine learning},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crossbar-constrained technology mapping for ReRAM based
in-memory computing. <em>TC</em>, <em>69</em>(5), 734–748. (<a
href="https://doi.org/10.1109/TC.2020.2964671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing has gained significant attention due to the potential for dramatic improvement in speed and energy. Redox-based resistive RAMs (ReRAMs), capable of non-volatile storage and logic operations simultaneously have been used for logic-in-memory computing approaches. To this effect, we propose ReRAM based VLIW Architecture for in-Memory comPuting (ReVAMP), supported by a detailed device-accurate simulation setup with peripheral circuitry. We present theoretical bounds on the minimum area required for in-memory computation of arbitrary Boolean functions specified using structural representation (And-Inverter Graph and Majority-Inverter Graph) and two-level representation (Exclusive-Sum-of-Product). To support the ReVAMP architecture, we present two technology mapping flows that fully exploit the bit-level parallelism offered by the execution of logic using ReRAM crossbar array. The area-constrained mapping (ArC) generates feasible mapping for a variety of crossbar dimensions while the delay-constrained mapping (DeC) focuses primarily on minimizing the latency of mapping. We evaluate the proposed mappings against two state-of-the-art technology in-memory computing architectures, PLiM and MAGIC along with their automation flows (SIMPLE and COMPACT). ArC and DeC outperform state-of-the-art PLiM architecture by 1.46x and 4.3x on average in latency. ArC offers significantly lower area (on average 25.27x and 6.57x), while improving the area-delay product by 1.37x and 1.12x against two mapping approaches for MAGIC respectively. In contrast, DeC achieves average area (1.45x and 3.06x) and area-delay product (1.12x and 6.36x) improvements over the mapping approaches for MAGIC architecture respectively. The proposed mapping techniques allow a variety of runtime efficiency trade-offs.},
  archive      = {J_TC},
  author       = {Debjyoti Bhattacharjee and Yaswanth Tavva and Arvind Easwaran and Anupam Chattopadhyay},
  doi          = {10.1109/TC.2020.2964671},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {734-748},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Crossbar-constrained technology mapping for ReRAM based in-memory computing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint management of CPU and NVDIMM for breaking down the
great memory wall. <em>TC</em>, <em>69</em>(5), 722–733. (<a
href="https://doi.org/10.1109/TC.2020.2964254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To provide larger memory space with lower costs, NVDIMM is a production-ready device. However, directly placing NVDIMM as the main memory would seriously degrade the system performance because of the “great memory wall” caused by the fact that in NVDIMM, the slow memory (e.g., flash memory) is several orders of magnitude slower than the fast memory (e.g., DRAM). In this article, we present a joint management framework of host/CPU and NVDIMM to break down the great memory wall by bridging the process information gap between host/CPU and NVDIMM. In this framework, a page semantic-aware strategy is proposed to precisely predict, mark, and relocate data or memory pages to the fast memory in advance by exploiting the process access patterns, so that the frequency of the slow memory accesses can be further reduced. The proposed framework with the proposed strategy was evaluated with several well-known benchmarks and the results are encouraging.},
  archive      = {J_TC},
  author       = {Chun-Feng Wu and Yuan-Hao Chang and Ming-Chang Yang and Tei-Wei Kuo},
  doi          = {10.1109/TC.2020.2964254},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {722-733},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Joint management of CPU and NVDIMM for breaking down the great memory wall},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). All-digital control-theoretic scheme to optimize energy
budget and allocation in multi-cores. <em>TC</em>, <em>69</em>(5),
706–721. (<a href="https://doi.org/10.1109/TC.2019.2963859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet-of-Things (IoT) revolution fueled new challenges and opportunities to achieve computational efficiency goals. Embedded devices are required to execute multiple applications for which a suitable distribution of the computing power must be adapted at run-time. Such complex hardware platforms have to sustain the continuous acquisition and processing of data under severe energy budget constraints, since most of them are battery powered. The state-of-the-art offers several ad-hoc contributions to selectively optimize the performance considering aspects like energy, power, thermal, or reliability. However, there is a need for a generic coordinated management strategy able to cope with all of these dimensions, while allowing the Operating System (OS) and the applications to “suggest” or constrain the actuation. This article proposes a unified control-theoretic scheme to coordinate the design of energy-budget and energy allocation solutions for multi-cores. The proposed controller can work with any actuator and it can interact, at run-time, with both the applications and the OS to optimize the actuation signals steering the computing platform. Such control scheme offers the possibility to integrate any performance related policy in the form of an energy-allocation strategy, still ensuring the theoretic exponential stability of the overall controller if the actuation of the policy, coming from the OS and the applications, “is not too fast.” To demonstrate the feasibility of our solution, we have implemented the controller into a RISC multi-core running on the Xilinx Artix 100t FPGA device, available in the the Digilent Nexys4-DDR board. Results considering two actuators and both the quadand the eight-core version of the considered computing platform, highlight the scalability of the proposed solution as well as an area overhead for the -all digital, on chip-controller limited to 0.86 percent (FFs) and 5.3 percent (LUTs) of the FPGA chip. We also considered a dynamic scenario validating the speed of the controller, where our framework has to face with modifications to the energy-allocation control policy carried out by the OS and the applications. The obtained results are collected by executing a huge mix of benchmarks and the statistical significance is accounted by executing each scenario 30 times. Such results are analyzed considering three quality metrics. First, the efficiency in exploiting the imposed budget (EFF9) that is on average 98.27 percent. Second, the overflow of the actual average power consumption with respect to the assigned budget (OνF9), which is limited to 1.43 mW on average. Last, the performance utility loss due to the control scheme that is limited to 1.87 percent on average.},
  archive      = {J_TC},
  author       = {Davide Zoni and Luca Cremona and William Fornaciari},
  doi          = {10.1109/TC.2019.2963859},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {706-721},
  shortjournal = {IEEE Trans. Comput.},
  title        = {All-digital control-theoretic scheme to optimize energy budget and allocation in multi-cores},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast encoding algorithms for reed–solomon codes with between
four and seven parity symbols. <em>TC</em>, <em>69</em>(5), 699–705. (<a
href="https://doi.org/10.1109/TC.2019.2963827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes a fast Reed-Solomon encoding algorithm with four and seven parity symbols in between. First, we show that the syndrome of Reed-Solomon codes can be computed via the Reed-Muller transform. Based on this result, the fast encoding algorithm is then derived. Analysis shows that the proposed approach asymptotically requires 3 XORs per data bit, representing an improvement over previous algorithms. The simulation demonstrates that the performance of the proposed approach improves with the increase of code length and is superior to other methods. In particular, when the parity number is 5, the proposed approach is about two times faster than other cutting-edge methods.},
  archive      = {J_TC},
  author       = {Leilei Yu and Zhichang Lin and Sian-Jheng Lin and Yunghsiang S. Han and Nenghai Yu},
  doi          = {10.1109/TC.2019.2963827},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {699-705},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast encoding algorithms for Reed–Solomon codes with between four and seven parity symbols},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental throughput allocation of heterogeneous storage
with no disruptions in dynamic setting. <em>TC</em>, <em>69</em>(5),
679–698. (<a href="https://doi.org/10.1109/TC.2019.2963385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solid-state drives (SSDs) have been added into storage systems for improving their performance, which will bring the heterogeneity into the storage medium. The throughput is one of the essential resources in heterogeneous storage systems, and how to allocate the throughput plays a crucial role in user performance. There are many types of research on the throughput allocation of heterogeneous storage systems. However, the throughput allocation of heterogeneous storage is facing new challenges in a dynamic setting, where users are not present in the system simultaneously, and enter the system dynamically. Drawing on economic gametheory, researchers have proposed many methods to tackle dynamic throughput allocation issues for heterogeneous storages, cross out enjoying Sharing Incentive (SI), Envy Freeness (EF), and Pareto Optimality (PO). However, they either relax constraints of fairness property to cause the allocation with weak fairness or interrupt some users present in the system to give up a piece of their allocations for new users entering the system, which will degrade these donors&#39; performance. Moreover, all of existing methods will cause lower resource utilization due to constraints of users&#39; dominant share equality. In this article, we propose a dynamic throughout allocation method based on gradual increase (DAGI), which can adapt to various workloads to make a fair allocation with a maximum resource utilization. Without relaxing constraints of fairness properties, when new users enter the system, DAGI can make a dynamic allocation with strong fairness by appropriately postponing the allocation of surplus throughputs, so this can provide an opportunity that DAGI can guarantee the final allocation with strong fairness when allocating remaining throughputs after all users are present in the system. Meanwhile, DAGI can gradually increase user allocation without reduction, which will not interrupt any users present in the system. Furthermore, DAGI can conduct a dynamic throughput allocation based on users&#39; local bottleneck resources, which can adapt to various workloads of users to improve resource utilization. Extensive experiments are conducted to prove the effectiveness of DAGI. The experimental results show that DAGI can achieve higher resource utilization and performance than existing methods, and can satisfy desirable game-theoretic properties with guaranteeing the strong fairness. In addition, DAGI gradually increases the allocation of each user without interrupting any user to reduce its allocation to degrade its performance.},
  archive      = {J_TC},
  author       = {ZhiSheng Huo and Limin Xiao and Minyi Guo and Xiaoling Rong},
  doi          = {10.1109/TC.2019.2963385},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {679-698},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Incremental throughput allocation of heterogeneous storage with no disruptions in dynamic setting},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CryptSQLite: SQLite with high data security. <em>TC</em>,
<em>69</em>(5), 666–678. (<a
href="https://doi.org/10.1109/TC.2019.2963303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SQLite, one of the most popular light-weighted database system, has been widely used in various systems. However, the compact design of SQLite did not make enough consideration on user data security. Specifically, anyone who has obtained the access to the database file will be able to read or tamper the data. Existing encryption-based solutions can only protect data on storage, while still exposing data when in computation. In this article, we combine the Trusted Execution Environment(TEE) technology and the authenticated encryption scheme, proposed and developed the CryptSQLite, a high security SQLite database system, which protects both the confidentiality and integrity of users&#39; data. Our security analysis proves that CryptSQLite can protect data confidentiality and integrity. Our implementation and experiments indicate that CryptSQLite incurs an average of 21 percent of extra time for SQL statement executions, compared with traditional encryption-based solutions that failed to offer rigorous security guarantees.},
  archive      = {J_TC},
  author       = {Yongzhi Wang and Yulong Shen and Cuicui Su and Jiawen Ma and Lingtong Liu and Xuewen Dong},
  doi          = {10.1109/TC.2019.2963303},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {666-678},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CryptSQLite: SQLite with high data security},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A modeling framework for reliability of erasure codes in SSD
arrays. <em>TC</em>, <em>69</em>(5), 649–665. (<a
href="https://doi.org/10.1109/TC.2019.2962691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergence of Solid-State Drives (SSDs) have evolved the data storage industry where they are rapidly replacing Hard Disk Drives (HDDs) due to their superiority in performance and power. Meanwhile, SSDs have reliability issues due to bit errors, bad blocks, and bad chips. To help reliability, Redundant-Array of Independent Disks (RAID) configurations, originally proposed to increase both performance and reliability of HDDs, are also applied to SSD arrays. However, the conventional reliability models of HDD RAID cannot be intactly applied to SSD arrays, as the nature of failures in SSDs are totally different from HDDs. Previous studies on the reliability of SSD arrays are based on the deprecated SSD failure data, and only focus on limited failure types, device failures, and page failures caused by the bit errors, while recent field studies have reported other failure types including bad blocks and bad chips, and a high correlation between failures. In this paper, we investigate the reliability of SSD arrays using field storage traces and real-system implementation of conventional and emerging erasure codes. The reliability is evaluated by statistical fault injection experiments that post-process the usage logs obtained from the real-system implementation, while the fault/failure attributes are obtained from the state-of-the-art field data by previous works. As a case study, we examine conventional RAID5 and RAID6 and emerging Partial-MDS (PMDS) codes, Sector-Disk (SD) codes, and STAIR codes in terms of both reliability and performance using an open-source software RAID controller, MD (in Linux kernel version 3.10.0-327), and arrays of Samsung 850 Pro SSDs. Our detailed analysis on the data loss breakdown shows that a) emerging erasure codes fail to replace RAID6 in terms of reliability, b) row-wise erasure codes are the most efficient choices for contemporary SSD devices, and c) previous models overestimate the SSD array reliability by up to six orders of magnitude, as they just focus on the coincidence of bad pages (bit errors) and bad chips within a data stripe that holds the minority of root cause of data loss in SSD arrays. Our experiments show that the combination of bad chips with bad blocks is recognized as the major source of data loss in RAID5 and emerging codes (contributing more than 54 and 90 percent of data loss in RAID5 and emerging codes, respectively), while RAID6 remains robust under these failure combinations. Finally, the fault injection results reveal that SSD array reliability, as well as the failure breakdown is significantly correlated with SSD type.},
  archive      = {J_TC},
  author       = {Mostafa Kishani and Saba Ahmadian and Hossein Asadi},
  doi          = {10.1109/TC.2019.2962691},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {649-665},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A modeling framework for reliability of erasure codes in SSD arrays},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Branch prediction attack on blinded scalar multiplication.
<em>TC</em>, <em>69</em>(5), 633–648. (<a
href="https://doi.org/10.1109/TC.2019.2958611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, performance counters have been used as a side channel source to monitor branch mispredictions, in order to attack cryptographic algorithms. However, the literature considers blinding techniques as effective countermeasures against such attacks. In this article, we present the first template attack on the branch predictor. We target blinded scalar multiplications with a side-channel attack that uses branch misprediction traces. Since an accurate model of the branch predictor is a crucial element of our attack, we first reverse-engineer the branch predictor. Our attack proceeds with a first online acquisition step, followed by an offline template attack with a template building phase and a template matching phase. During the template matching phase, we use a strategy we call Deduce &amp; Remove, to first infer the candidate values from templates based on a model of the branch predictor, and subsequently eliminate erroneous observations. This last step uses the properties of the target blinding technique to remove wrong guesses and thus naturally provides error correction in key retrieval. In the later part of this article, we demonstrate a template attack on Curve1174 where the double-and-add always algorithm implementation is free from conditional branching on the secret scalar. In that case, we target the data-dependent branching based on the modular reduction operations of long integer multiplications. Such implementations still exist in open source software and can be vulnerable, even if top level safeguards like blinding are used. We provide experimental results on scalar splitting, scalar randomization, and point blinding to show that the secret scalar can be correctly recovered with high confidence. Finally, we conclude with recommendations on countermeasures to thwart such attacks.},
  archive      = {J_TC},
  author       = {Sarani Bhattacharya and Clémentine Maurice and Shivam Bhasin and Debdeep Mukhopadhyay},
  doi          = {10.1109/TC.2019.2958611},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {633-648},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Branch prediction attack on blinded scalar multiplication},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive model-based scheduling in software transactional
memory. <em>TC</em>, <em>69</em>(5), 621–632. (<a
href="https://doi.org/10.1109/TC.2019.2954139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Transactional Memory (STM) stands as powerful concurrent programming paradigm, enabling atomicity, and isolation while accessing shared data. On the downside, STM may suffer from performance degradation due to excessive conflicts among concurrent transactions, which cause waste of CPU-cycles and energy because of transaction aborts. An approach to cope with this issue consists of putting in place smart scheduling strategies which temporarily suspend the execution of some transaction in order to reduce the transaction conflict rate. In this article, we present an adaptive model-based transaction scheduling technique relying on a Markov Chain-based performance model of STM systems. Our scheduling technique is adaptive in a twofold sense: (i) It controls the execution of transactions depending on throughput predictions by the model as a function of the current system state. (ii) It re-tunes on-line the Markov Chain-based model to adapt it-and the outcoming transaction scheduling decisions-to dynamic variations of the workload. We have been able to achieve the latter target thanks to the fact that our performance model is extremely lightweight. In fact, to be recomputed, it requires a reduced set of input parameters, whose values can be estimated via a few on-line samples related to the current workload dynamics. We also present a scheduler that implements our adaptive technique, which we integrated within the open source TinySTM package. Further, we report the results of an experimental study based on the STAMP benchmark suite, which has been aimed at assessing both the accuracy of our performance model in predicting the actual system throughput and the advantages of the adaptive scheduling policy over literature techniques.},
  archive      = {J_TC},
  author       = {Pierangelo Di Sanzo and Alessandro Pellegrini and Marco Sannicandro and Bruno Ciciani and Francesco Quaglia},
  doi          = {10.1109/TC.2019.2954139},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {621-632},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive model-based scheduling in software transactional memory},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mangrove: An inference-based dynamic invariant mining for
GPU architectures. <em>TC</em>, <em>69</em>(4), 606–620. (<a
href="https://doi.org/10.1109/TC.2019.2953846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likely invariants model properties that hold in operating conditions of a computing system. Dynamic mining of invariants aims at extracting logic formulas representing such properties from the system execution traces, and it is widely used for verification of intellectual property (IP) blocks. Although the extracted formulas represent likely invariants that hold in the considered traces, there is no guarantee that they are true in general for the system under verification. As a consequence, to increase the probability that the mined invariants are true in general, dynamic mining has to be performed to large sets of representative execution traces. This makes the execution-based mining process of actual IP blocks very time-consuming due to the trace lengths and to the large sets of monitored signals. This article presents Mangrove, an efficient implementation of a dynamic invariant mining algorithm for GPU architectures. Mangrove exploits inference rules, which are applied at run time to filter invariants from the execution traces and, thus, to sensibly reduce the problem complexity. Mangrove allows users to define invariant templates and, from these templates, it automatically generates kernels for parallel and efficient mining on GPU architectures. The article presents the tool, the analysis of its performance, and its comparison with the best sequential and parallel implementations at the state of the art.},
  archive      = {J_TC},
  author       = {Nicola Bombieri and Federico Busato and Alessandro Danese and Luca Piccolboni and Graziano Pravadelli},
  doi          = {10.1109/TC.2019.2953846},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {606-620},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Mangrove: An inference-based dynamic invariant mining for GPU architectures},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PRS: A pattern-directed replication scheme for heterogeneous
object-based storage. <em>TC</em>, <em>69</em>(4), 591–605. (<a
href="https://doi.org/10.1109/TC.2019.2954089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data replication is a key technique to achieve high data availability, reliability, and optimized performance in distributed storage systems. In recent years, with emerged new storage devices, heterogeneous object-based storage systems, such as a storage system with a mix of hard disk drives, solid state drives, and other non-volatile memory devices have become increasingly attractive since they combine the merits of different storage devices to deliver better promises. However, existing data replication schemes do not well consider distinct characteristics of heterogeneous storage devices yet, which could lead to suboptimal performance. This article introduces a new data replication scheme called Pattern-directed Replication Scheme (PRS) to achieve efficient data replication for heterogeneous storage systems. Different from traditional schemes, the PRS selectively replicates data objects and distributes replicas to various storage devices based on their characteristics. It aggregates objects that have I/O correlation into object groups by calculating object distance and makes replication for grouped objects according to application&#39;s data access pattern identified. In addition, the PRS uses a pseudo random algorithm to optimize replica placement by considering the storage device performance and capacity features. We have evaluated the pattern-directed replication scheme with extensive tests in Sheepdog, a typical object-based storage system. The experimental results confirm that it is a highly efficient replication scheme for heterogeneous storage systems. For instance, the read performance was improved by 105 percent to nearly 10x compared with existing replication schemes.},
  archive      = {J_TC},
  author       = {Jiang Zhou and Yong Chen and Wei Xie and Dong Dai and Shuibing He and Weiping Wang},
  doi          = {10.1109/TC.2019.2954089},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {591-605},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PRS: A pattern-directed replication scheme for heterogeneous object-based storage},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bufferless network-on-chips with bridged multiple
subnetworks for deflection reduction and energy savings. <em>TC</em>,
<em>69</em>(4), 577–590. (<a
href="https://doi.org/10.1109/TC.2019.2959307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bufferless network-on-chip (NoC) can deliver high energy efficiency, but such a NoC is subject to growing deflection when its traffic load rises. This article proposes Deflection Containment (DeC) for the bufferless NoC to address its notorious shortcomings of excessive deflection for performance improvement and energy savings. With multiple subnetworks bridged by an added link between two corresponding routers, DeC lets a contending flit in one subnetwork be forwarded to another subnetwork instead of deflected. Microarchitecture of DeC routers is rectified to shorten the critical path and lift network bandwidth. Its Cadence RTL implementations with a 15 - nm process are conducted respectively for mesh-based NoCs and torus-based NoCs. Additionally, different sized DeC-NoCs are evaluated extensively and compared with previous bufferless designs (BLESS and MinBD), uncovering that DeC with two bridged subnetworks (dubbed DeC2) for 8×8 mesh-based NoCs can lower deflection drastically by some 90 percent and energy consumption by upto 51 percent under real benchmark traffic loads, in comparison to BLESS. Under various synthetic traffic models and workloads, 16×16 torus-based DeC2-NoC sustains up to 2.33× loads when compared with its mesh-based counterpart, exhibiting the same clock rate and taking only negligible more power and area according to our full layout results.},
  archive      = {J_TC},
  author       = {Xiyue Xiang and Purushottam Sigdel and Nian-Feng Tzeng},
  doi          = {10.1109/TC.2019.2959307},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {577-590},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Bufferless network-on-chips with bridged multiple subnetworks for deflection reduction and energy savings},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance analysis for heterogeneous cloud servers using
queueing theory. <em>TC</em>, <em>69</em>(4), 563–576. (<a
href="https://doi.org/10.1109/TC.2019.2956505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the problem of selecting appropriate heterogeneous servers in cloud centers for stochastically arriving requests in order to obtain an optimal tradeoff between the expected response time and power consumption. Heterogeneous servers with uncertain setup times are far more common than homogenous ones. The heterogeneity of servers and stochastic requests pose great challenges in relation to the tradeoff between the two conflicting objectives. Using the Markov decision process, the expected response time of requests is analyzed in terms of a given number of available candidate servers. For a given system availability, a binary search method is presented to determine the number of servers selected from the candidates. An iterative improvement method is proposed to determine the best servers to select for the considered objectives. After evaluating the performance of the system parameters on the performance of algorithms using the analysis of variance, the proposed algorithm and three of its variants are compared over a large number of random and real instances. The results indicate that proposed algorithm is much more effective than the other four algorithms within acceptable CPU times.},
  archive      = {J_TC},
  author       = {Shuang Wang and Xiaoping Li and Rubén Ruiz},
  doi          = {10.1109/TC.2019.2956505},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {563-576},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Performance analysis for heterogeneous cloud servers using queueing theory},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A management scheme of multi-level retention-time queues for
improving the endurance of flash-memory storage devices. <em>TC</em>,
<em>69</em>(4), 549–562. (<a
href="https://doi.org/10.1109/TC.2019.2954398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As flash memory technology has been scaled down to 1x nm and more bits can be stored in a cell, the storage density of flash memory has been significantly improved. However, these technical trends also severely hurt the programming speed and endurance of flash memory. The internal data retention time is the duration for which a flash cell can correctly hold data. By relaxing internal data retention time, both the page programming speed and the block endurance could be improved. However, the retention time of flash memory typically requires to last for several years according to the industrial standard. Thus a refreshment scheme is required to deal with the decreasing of retention time. In this article, we propose multi-level retention-time queues with a management scheme to meet the retention-time requirement for a reliable storage system. Observing that many data are overwritten in hours or days in real workloads, multiple retention-time queues could effectively separate data with different update frequencies. There are three challenge issues for a proper design: (1) Since access pattern might change from time to time, a technical issue is how to promote/demote data so that data could be maintained in the proper retention-time queue to minimize the refreshment overhead. (2) Another technical issue is how to refresh each retention-time queue in time to guarantee data integrity. (3) Since blocks resided in different retention-time queue would suffer from different level of wearing, the third technical issue is how to estimate wearing status of flash-memory blocks in an effective and efficient manner to achieve wear leveling. In our scheme, data allocator, multi-level refresh module, garbage collector, and wear leveler are introduced to deal with these technical issues. Based on our experimental results, not only endurance and performance but also energy consumption of the flash-memory storage system could be significantly improved by our scheme.},
  archive      = {J_TC},
  author       = {David Kuang-Hui Yu and Jen-Wei Hsieh},
  doi          = {10.1109/TC.2019.2954398},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {549-562},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A management scheme of multi-level retention-time queues for improving the endurance of flash-memory storage devices},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High throughput/gate AES hardware architectures based on
datapath compression. <em>TC</em>, <em>69</em>(4), 534–548. (<a
href="https://doi.org/10.1109/TC.2019.2957355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes highly efficient Advanced Encryption Standard (AES) hardware architectures that support encryption and both encryption and decryption. New operation-reordering and register-retiming techniques presented in this article allow us to unify the inversion circuits in SubBytes and InvSubBytes without any delay overhead. In addition, a new optimization technique for minimizing linear mappings, named multiplicative-offset, further enhances the hardware efficiency. We also present a shared key scheduling datapath that can work on-the-fly in the proposed architecture. To the best of our knowledge, the proposed architecture has the shortest critical path delay and is the most efficient in terms of throughput per area among conventional AES encryption/decryption and encryption architectures with tower-field S-boxes. The proposed round-based architecture can perform AES encryption where block-wise parallelism is unavailable (e.g., cipher block chaining (CBC) mode); thus, our techniques can be globally applied to any type of architecture including pipelined ones. We evaluated the performance of the proposed and some conventional datapaths by logic synthesis with the NanGate 45-nm open-cell library. As a result, we can confirm that our proposed architectures achieve approximately 51-64 percent higher efficiency (i.e., higher bps/GE) and lower power/energy consumption than the other conventional counterparts.},
  archive      = {J_TC},
  author       = {Rei Ueno and Sumio Morioka and Noriyuki Miura and Kohei Matsuda and Makoto Nagata and Shivam Bhasin and Yves Mathieu and Tarik Graba and Jean-Luc Danger and Naofumi Homma},
  doi          = {10.1109/TC.2019.2957355},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {534-548},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High Throughput/Gate AES hardware architectures based on datapath compression},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NTTU: An area-efficient low-power NTT-uncoupled architecture
for NTT-based multiplication. <em>TC</em>, <em>69</em>(4), 520–533. (<a
href="https://doi.org/10.1109/TC.2019.2958334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large integer multiplication, or large degree polynomial multiplication, is the most time-consuming operation in fully homomorphic encryption (FHE). Low area and power consumption are difficult to maintain while achieving high performance for a large size multiplier. To address this issue, an area-efficient low-power architecture for multiplication, named NTTU, is proposed in this article. First, a combined number theoretic transform (NTT) method consisting of decimation-in-time (DIT) NTT for input in natural order and bit-reversed order is proposed to eliminate the steps of zero padding, scramble, and the first stage in NTT, thereby achieving a reduction of 7N/2 clock cycles compared with the single-type NTT method. Second, the NTT-uncoupled architecture is proposed to uncouple the multiplication components, decreasing the storage space for coefficients by 1/2 compared with state-of-the-art designs. Third, a parallel computing architecture based on a crossed memory access scheme is proposed, therein reducing the corresponding execution time by one-half compared with serial execution. Synthesized using 65 nm technology, the proposed architecture can multiply two 1024k/768k integers in 1.7 ms at 500 MHz at a cost of 13.66/7.67 million gates and 726.7/550.2 mW, and a 71.17 percent/ 30.37 percent area time product (ATP) reduction is achieved compared with the state-of-the-art ASIC designs.},
  archive      = {J_TC},
  author       = {Neng Zhang and Qiao Qin and Hang Yuan and Chenggao Zhou and Shouyi Yin and ShaoJun Wei and Leibo Liu},
  doi          = {10.1109/TC.2019.2958334},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {520-533},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NTTU: An area-efficient low-power NTT-uncoupled architecture for NTT-based multiplication},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph similarity and its applications to hardware security.
<em>TC</em>, <em>69</em>(4), 505–519. (<a
href="https://doi.org/10.1109/TC.2019.2953752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware reverse engineering is a powerful and universal tool for both security engineers and adversaries. From a defensive perspective, it allows for detection of intellectual property infringements and hardware Trojans, while it simultaneously can be used for product piracy and malicious circuit manipulations. From a designer&#39;s perspective, it is crucial to have an estimate of the costs associated with reverse engineering, yet little is known about this, especially when dealing with obfuscated hardware. The contribution at hand provides new insights into this problem, based on algorithms with sound mathematical underpinnings. Our contributions are threefold: First, we present the graph similarity problem for automating hardware reverse engineering. To this end, we improve several state-of-the-art graph similarity heuristics with optimizations tailored to the hardware context. Second, we propose a novel algorithm based on multiresolutional spectral analysis of adjacency matrices. Third, in three extensively evaluated case studies, namely (1) gate-level netlist reverse engineering, (2) hardware Trojan detection, and (3) assessment of hardware obfuscation, we demonstrate the practical nature of graph similarity algorithms.},
  archive      = {J_TC},
  author       = {Marc Fyrbiak and Sebastian Wallat and Sascha Reinhard and Nicolai Bissantz and Christof Paar},
  doi          = {10.1109/TC.2019.2953752},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {505-519},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Graph similarity and its applications to hardware security},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Arithmetic approaches for rigorous design of reliable
fixed-point LTI filters. <em>TC</em>, <em>69</em>(4), 489–504. (<a
href="https://doi.org/10.1109/TC.2019.2950658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we target the Fixed-Point (FxP) implementation of Linear Time-Invariant (LTI) filters evaluated with statespace equations. We assume that wordlengths are fixed and that our goal is to determine binary point positions that guarantee the absence of overflows while maximizing accuracy. We provide a model for the worst-case error analysis of FxP filters that gives tight bounds on the output error. Then we develop an algorithm for the determination of binary point positions that takes rounding errors and their amplification fully into account. The proposed techniques are rigorous, i.e., based on proofs, and no simulations are ever used. In practice, Floating-Point (FP) errors that occur in the implementation of FxP design routines can lead to overestimation/underestimation of resulting parameters. Thus, along with FxP analysis of digital filters, we provide FP analysis of our filter design algorithms. In particular, the core measure in our approach, Worst-Case Peak Gain, is defined as an infinite sum and has matrix powers in it. We provide fine-grained FP error analysis of its evaluation and develop multiple precision algorithms that dynamically adapt their internal precision to satisfy an a priori absolute error bound. Our techniques on multiple precision matrix algorithms, such as eigendecomposition, are of independent interest as a contribution to Computer Arithmetic. All algorithms are implemented as C libraries, integrated into an open-source filter code generator and tested on numerical examples.},
  archive      = {J_TC},
  author       = {Anastasia Volkova and Thibault Hilaire and Christoph Lauter},
  doi          = {10.1109/TC.2019.2950658},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {489-504},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Arithmetic approaches for rigorous design of reliable fixed-point LTI filters},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting asymmetric errors for LDPC decoding optimization
on 3D NAND flash memory. <em>TC</em>, <em>69</em>(4), 475–488. (<a
href="https://doi.org/10.1109/TC.2019.2959318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By stacking layers vertically, the adoption of 3D NAND has significantly increased the capacity for storage systems. The complex structure of 3D NAND introduces more errors than planer flash. To address the reliability issue, low-density parity-check (LDPC) code with a strong error correction capability is now widely applied on 3D NAND flash memory. However, LDPC has long decoding latency when the raw bit error rates (RBER) are high. This is because it needs fine-grained soft sensing between voltage states to iteratively decode the raw data. Multiple sensing voltages are applied on flash cell array to gain necessary information for decoding. In this article, a new sensing level placement scheme with reduced number of sensing levels is proposed. The basic idea for the placement scheme is motivated by three asymmetric error characteristics of flash memory: the asymmetric errors between different states, the asymmetric errors caused by voltage left-shifts and right-shifts and asymmetric errors among layers in a 3D NAND flash block. With awareness of these three types of error characteristics, reduced number of sensing levels are placed to achieve reduced read latency for LDPC decoding while maintaining the error correction capability of LDPC. Experiment analysis shows that the proposed scheme achieves significant performance improvement.},
  archive      = {J_TC},
  author       = {Qiao Li and Liang Shi and Yufei Cui and Chun Jason Xue},
  doi          = {10.1109/TC.2019.2959318},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {475-488},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploiting asymmetric errors for LDPC decoding optimization on 3D NAND flash memory},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate restoring dividers using inexact cells and
estimation from partial remainders. <em>TC</em>, <em>69</em>(4),
468–474. (<a href="https://doi.org/10.1109/TC.2019.2953751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing can be used in error-resilient applications to reduce power consumption and increase overall circuit performance. This article introduces two approximate dividers with restoring array-based architecture that achieve substantial hardware savings while maintaining high accuracy when compared to existing approximate designs. The first design replaces exact restoring divider cells with a proposed approximate cell in a column-wise fashion. The second design uses several rows of exact architecture to compute a partial remainder and then rounds and encodes the divisor and this partial remainder so that they may be used to express approximate outputs. A comprehensive accuracy and performance evaluation are performed for the proposed dividers as well as other state-of-the-art designs. When compared to an exact design, the proposed dividers have a reduced area and power consumption of 46 and 57 percent respectively while introducing minimal error. Furthermore, the trade-off between accuracy and improved performance is explored for various approximate dividers in order to determine which designs achieve the best compromise. The accuracy of the proposed dividers is then demonstrated using two image processing applications.},
  archive      = {J_TC},
  author       = {Elizabeth Adams and Suganthi Venkatachalam and Seok-Bum Ko},
  doi          = {10.1109/TC.2019.2953751},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {468-474},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Approximate restoring dividers using inexact cells and estimation from partial remainders},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). State of the journal. <em>TC</em>, <em>69</em>(4), 466–467.
(<a href="https://doi.org/10.1109/TC.2020.2976083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TC},
  author       = {Ahmed Louri},
  doi          = {10.1109/TC.2020.2976083},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {466-467},
  shortjournal = {IEEE Trans. Comput.},
  title        = {State of the journal},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy efficient on-demand dynamic branch prediction models.
<em>TC</em>, <em>69</em>(3), 453–465. (<a
href="https://doi.org/10.1109/TC.2019.2956710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The branch predictor unit (BPU) is among the main energy consuming components in out-of-order (OoO) processors. For integer applications, we find 16 percent of the processor energy is consumed by the BPU. BPU is accessed in parallel with the instruction cache before it is known if a fetch group contains control instructions. We find 85 percent of BPU lookups are done for non-branch operations, and of the remaining lookups, 42 percent are done for highly biased branches that can be predicted statically with high accuracy. We evaluate two variants of a branch prediction model that combines dynamic and static branch prediction to achieve energy improvements for power-constrained applications. These models, named on-demand branch prediction (ODBP) and path-based on-demand branch prediction (ODBP-PATH), are two novel prediction techniques that eliminate unnecessary BPU lookups using compiler generated hints to identify instructions that can be more accurately predicted statically. ODBP-PATH is an implementation of ODBP that combines static and dynamic branch prediction based on the program path of execution. For a 4-wide OoO processor, ODBP-PATH delivers 11 percent average energy-delay (ED) product improvement, and 9 percent core average energy saving on the SPEC Int 2006 benchmarks.},
  archive      = {J_TC},
  author       = {Milad Mohammadi and Song Han and Ehsan Atoofian and Amirali Baniasadi and Tor M. Aamodt and William J. Dally},
  doi          = {10.1109/TC.2019.2956710},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {453-465},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy efficient on-demand dynamic branch prediction models},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grow and prune compact, fast, and accurate LSTMs.
<em>TC</em>, <em>69</em>(3), 441–452. (<a
href="https://doi.org/10.1109/TC.2019.2954495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long short-term memory (LSTM) has been widely used for sequential data modeling. Researchers have increased LSTM depth by stacking LSTM cells to improve performance. This incurs model redundancy, increases run-time delay, and makes the LSTMs more prone to overfitting. To address these problems, we propose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM&#39;s original one-level nonlinear control gates. H-LSTM increases accuracy while employing fewer external stacked layers, thus reducing the number of parameters and run-time latency significantly. We employ grow-and-prune (GP) training to iteratively adjust the hidden layers through gradient-based growth and magnitude-based pruning of connections. This learns both the weights and the compact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for image captioning, speech recognition, and neural machine translation applications. For the NeuralTalk architecture on the MSCOCO dataset, our three models reduce the number of parameters by 38.7× [floating-point operations (FLOPs) by 45.5×], run-time latency by 4.5×, and improve the CIDEr-D score by 2.8 percent, respectively. For the DeepSpeech2 architecture on the AN4 dataset, the first model we generated reduces the number of parameters by 19.4× and run-time latency by 37.4 percent. The second model reduces the word error rate (WER) from 12.9 to 8.7 percent. For the encoder-decoder sequence-to-sequence network on the IWSLT 2014 German-English dataset, the first model we generated reduces the number of parameters by 10.8× and run-time latency by 14.2 percent. The second model increases the BLEU score from 30.02 to 30.98. Thus, GP-trained H-LSTMs can be seen to be compact, fast, and accurate.},
  archive      = {J_TC},
  author       = {Xiaoliang Dai and Hongxu Yin and Niraj K. Jha},
  doi          = {10.1109/TC.2019.2954495},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {441-452},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Grow and prune compact, fast, and accurate LSTMs},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel methods for efficient realization of logic functions
using switching lattices. <em>TC</em>, <em>69</em>(3), 427–440. (<a
href="https://doi.org/10.1109/TC.2019.2950663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-dimensional switching lattices including four-terminal switches are introduced as alternative structures to realize logic functions, aiming to outperform the designs consisting of one-dimensional two-terminal switches. Exact and approximate algorithms have been proposed for the problem of finding a switching lattice which implements a given logic function and has the minimum size, i.e., a minimum number of switches. In this article, we present an approximate algorithm, called JANUS, that explores the search space in a dichotomic search manner. It iteratively checks if the target function can be realized using a given lattice candidate, which is formalized as a satisfiability (SAT) problem. As the lattice size and the number of literals and products in the given target function increase, the size of a SAT problem grows dramatically, increasing the run-time of a SAT solver. To handle the instances that JANUS cannot cope with, we introduce a divide and conquer method called MEDEA. It partitions the target function into smaller sub-functions, finds the realizations of these sub-functions on switching lattices using JANUS, and explores alternative realizations of these subfunctions which may reduce the size of the final lattice. Moreover, we describe the realization of multiple functions in a single lattice. Experimental results show that JANUS can find better solutions than the existing approximate algorithms, even than the exact algorithm which cannot determine a minimum solution in a given time limit. On the other hand, MEDEA can find better solutions on relatively large size instances using a little computational effort when compared to the previously proposed algorithms. Moreover, on instances that the existing methods cannot handle, MEDEA can easily find a solution which is significantly better than the available solutions.},
  archive      = {J_TC},
  author       = {Levent Aksoy and Mustafa Altun},
  doi          = {10.1109/TC.2019.2950663},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {427-440},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Novel methods for efficient realization of logic functions using switching lattices},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pursuing extreme power efficiency with PPCC guided NoC DVFS.
<em>TC</em>, <em>69</em>(3), 410–426. (<a
href="https://doi.org/10.1109/TC.2019.2949807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sharp contrast to conventional performance indicative based Network-on-Chip (NoC) DVFS, where the direct relation between application performance and NoC power consumption is missing, we exploit the concept of Performance-Power Characteristic Curve (PPCC) newly proposed in the literature to approach maximum NoC power efficiency. PPCC, which defines the direct relation between application performance and NoC power consumption, consists of three distinct regions: an inertial region due to power under-provisioning, a linear region for proportional performance gain, and a saturation region due to power over-provisioning. With PPCC as a guidance, we propose Δ-DVFS, which employs a “profile-then-select” strategy to step-by-step approach maximum NoC power efficiency. Δ-DVFS is built on two observations. First, in multi-threaded applications, maximum NoC power efficiency is achieved at the boundary between the linear region and the saturation region on the PPCC. Second, PPCC stabilizes when threads repeat workloads of the same loop. This is intuitively meaningful because loop repetition stresses NoC with similar workload. Based on the observations, Δ-DVFS uses the first several loop iterations for PPCC profiling. After the profiling is done, Δ-DVFS selects and applies the optimal V/F that achieves maximum NoC power efficiency to the remaining loop iterations. To accurately and timely follow PPCC when threads proceed to different loops, Δ-DVFS utilizes an H-tree loop monitor to detect loop change among distributive threads.},
  archive      = {J_TC},
  author       = {Yuan Yao and Zhonghai Lu},
  doi          = {10.1109/TC.2019.2949807},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {410-426},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Pursuing extreme power efficiency with PPCC guided NoC DVFS},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and analysis of efficient maximum/minimum circuits
for stochastic computing. <em>TC</em>, <em>69</em>(3), 402–409. (<a
href="https://doi.org/10.1109/TC.2019.2949779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In stochastic computing (SC), a real-valued number is represented by a stochastic bit stream, encoding its value in the probability of obtaining a one. This leads to a significantly lower hardware effort for various functions and provides a higher tolerance to errors (e.g., bit flips) compared to binary radix representation. The implementation of a stochastic max/min function is important for many areas where SC has been successfully applied, such as image processing or machine learning (e.g., max pooling in neural networks). In this work, we propose a novel shift-register-based architecture for a stochastic max/min function. We show that the proposed circuit has significantly higher accuracy than state-of-the-art architectures for uncorrelated bit streams at comparable hardware costs. Moreover, we analytically proof the correctness of the proposed circuit and provide a new error analysis, based on the individual bits of the stochastic streams. Interestingly, the analysis reveals that for a certain practical bit stream length a finite optimal shift register length exists and it allows to determine the optimal length.},
  archive      = {J_TC},
  author       = {Michael Lunglmayr and Daniel Wiesinger and Werner Haselmayr},
  doi          = {10.1109/TC.2019.2949779},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {402-409},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Design and analysis of efficient Maximum/Minimum circuits for stochastic computing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-efficient pattern recognition hardware with
elementary cellular automata. <em>TC</em>, <em>69</em>(3), 392–401. (<a
href="https://doi.org/10.1109/TC.2019.2949300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of power-efficient Machine Learning Hardware is of high importance to provide Artificial Intelligence (AI) characteristics to those devices operating at the Edge. Unfortunately, state-of-the-art data-driven AI techniques such as deep learning are too costly in terms of hardware and energy requirements for Edge Computing (EC) devices. Recently, Cellular Automata (CA) have been proposed as a feasible way to implement Reservoir Computing (RC) systems in which the automaton rule is fixed and the training is performed using a linear regression model. In this work we show that Reservoir Computing based on CA may arise as a promising AI alternative for devices operating at the edge due to its intrinsic simplicity. For this purpose, a new low-power CA-based reservoir hardware is proposed and implemented in a FPGA (known as ReCA circuitry). The use of Elementary Cellular Automata (ECA) is able to further simplify the RC structure to implement a power efficient AI system suitable to be implemented in EC applications. Experiments have been conducted on the well-known MNIST handwritten digits database, obtaining competitive results in terms of processing time, circuit area, power and inference accuracy.},
  archive      = {J_TC},
  author       = {Alejandro Morán and Christiam F. Frasser and Miquel Roca and Josep L. Rosselló},
  doi          = {10.1109/TC.2019.2949300},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {392-401},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-efficient pattern recognition hardware with elementary cellular automata},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hotness- and lifetime-aware data placement and migration for
high-performance deep learning on heterogeneous memory systems.
<em>TC</em>, <em>69</em>(3), 377–391. (<a
href="https://doi.org/10.1109/TC.2019.2949408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous memory systems that comprise memory nodes with disparate architectural characteristics (e.g., DRAM and high-bandwidth memory (HBM)) have surfaced as a promising solution in a variety of computing domains ranging from embedded to high-performance computing. Since deep learning (DL) is one of the most widely-used workloads in various computing domains, it is crucial to explore efficient memory management techniques for DL applications that execute on heterogeneous memory systems. Despite extensive prior works on system software and architectural support for efficient DL, it still remains unexplored to investigate heterogeneity-aware memory management techniques for high-performance DL on heterogeneous memory systems. To bridge this gap, we analyze the characteristics of representative DL workloads on a real heterogeneous memory system. Guided by the characterization results, we propose HALO, hotness- and lifetime-aware data placement and migration for high-performance DL on heterogeneous memory systems. Through quantitative evaluation, we demonstrate the effectiveness of HALO in that it significantly outperforms various memory management policies (e.g., 28.2 percent higher performance than the HBM-Preferred policy) supported by the underlying system software and hardware, achieves the performance comparable to the ideal case with infinite HBM, incurs small performance overheads, and delivers high performance across a wide range of application working-set sizes.},
  archive      = {J_TC},
  author       = {Myeonggyun Han and Jihoon Hyun and Seongbeom Park and Woongki Baek},
  doi          = {10.1109/TC.2019.2949408},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {377-391},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hotness- and lifetime-aware data placement and migration for high-performance deep learning on heterogeneous memory systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impeccable circuits. <em>TC</em>, <em>69</em>(3), 361–376.
(<a href="https://doi.org/10.1109/TC.2019.2948617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By injecting faults, active physical attacks pose serious threats to cryptographic hardware where Concurrent Error Detection (CED) schemes are promising countermeasures. They are usually based on an Error-Detecting Code (EDC) which enables detecting certain injected faults depending on the specification of the underlying code. Here, we propose a methodology to enable correct, practical, and robust implementation of code-based CEDs. We show that straightforward hardware implementations of given code-based CEDs can suffer from severe vulnerabilities, not providing the desired protection level. In particular, propagation of faults into combinatorial logic is often ignored in security evaluation of these schemes. First, we formally define this detrimental effect and demonstrate its destructive impact. Second, we introduce an implementation strategy to limit the fault propagation effect. Third, in contrast to many other works where the fault coverage is the main focus, we present a detailed implementation strategy which can guarantee the detection of any fault covered by the underlying EDC. This holds for any time of the computation and any location in the circuit, both in data processing and control unit. In short, we provide practical guidelines how to construct efficient CED schemes with arbitrary EDCs to achieve the desired protection level. We practically evaluate the efficiency of our methodology by case studies covering different symmetric block ciphers and various linear EDCs.},
  archive      = {J_TC},
  author       = {Anita Aghaie and Amir Moradi and Shahram Rasoolzadeh and Aein Rezaei Shahmirzadi and Falk Schellenberg and Tobias Schneider},
  doi          = {10.1109/TC.2019.2948617},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {361-376},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Impeccable circuits},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ApGAN: Approximate GAN for robust low energy learning from
imprecise components. <em>TC</em>, <em>69</em>(3), 349–360. (<a
href="https://doi.org/10.1109/TC.2019.2949042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Generative Adversarial Network (GAN) is an adversarial learning approach which empowers conventional deep learning methods by alleviating the demands of massive labeled datasets. However, GAN training can be computationally-intensive limiting its feasibility in resource-limited edge devices. In this paper, we propose an approximate GAN (ApGAN) for accelerating GANs from both algorithm and hardware implementation perspectives. First, inspired by the binary pattern feature extraction method along with binarized representation entropy, the existing Deep Convolutional GAN (DCGAN) algorithm is modified by binarizing the weights for a specific portion of layers within both the generator and discriminator models. Further reduction in storage and computation resources is achieved by leveraging a novel hardware-configurable in-memory addition scheme, which can operate in the accurate and approximate modes. Finally, a memristor-based processing-in-memory accelerator for ApGAN is developed. The performance of the ApGAN accelerator on different data-sets such as Fashion-MNIST, CIFAR-10, STL-10, and celeb-A is evaluated and compared with recent GAN accelerator designs. With almost the same Inception Score (IS) to the baseline GAN, the ApGAN accelerator can increase the energy-efficiency by ~28.6× achieving 35-fold speedup compared with a baseline GPU platform. Additionally, it shows 2.5× and 5.8× higher energy-efficiency and speedup over CMOS-ASIC accelerator subject to an 11 percent reduction in IS.},
  archive      = {J_TC},
  author       = {Arman Roohi and Shadi Sheikhfaal and Shaahin Angizi and Deliang Fan and Ronald F DeMara},
  doi          = {10.1109/TC.2019.2949042},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {349-360},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ApGAN: Approximate GAN for robust low energy learning from imprecise components},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards the integration of reverse converters into the RNS
channels. <em>TC</em>, <em>69</em>(3), 342–348. (<a
href="https://doi.org/10.1109/TC.2019.2948335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conversion from a Residue Number System (RNS) to a weighted representation is a costly inter-modulo operation that introduces delay and area overhead to RNS processors, while also increasing power consumption. This paper proposes a new approach to decompose the reverse conversion into operations that can be processed by the arithmetic units already present in the RNS independent channels. This leads to a more effective reuse of the processor circuitry while enhancing parallelism. Experimental results show that, when the proposed techniques are applied to architectures based on ripple-carry adders for the traditional 3-moduli set, the delay is improved in average by 16 percent, the circuit area by 36 percent and the power consumption by 47 percent. When carry-lookahead adder topologies are considered, these improvements are in average of 45 percent for the circuit area and 58 percent for the power consumption while the delay is only slightly reduced. The proposed techniques are applied to a use case in digital filtering, showing an increase in throughput/area of up to 1.25 times, and average reductions in energy consumption of 15.6 percent. This work is a step forward to the usage of RNS in practice, since reverse conversion underpins other hard inter-modulo operations, like comparison, scaling and division.},
  archive      = {J_TC},
  author       = {Leonel Sousa and Rogério Paludo and Paulo Martins and Hector Pettenghi},
  doi          = {10.1109/TC.2019.2948335},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {342-348},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards the integration of reverse converters into the RNS channels},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lightweight key encapsulation using LDPC codes on FPGAs.
<em>TC</em>, <em>69</em>(3), 327–341. (<a
href="https://doi.org/10.1109/TC.2019.2948323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a lightweight hardware design for a recently proposed quantum-safe key encapsulation mechanism based on QC-LDPC codes called LEDAkem, which has been admitted as a round-2 candidate to the NIST post-quantum standardization project. Existing implementations focus on high speed while few of them take into account area or power efficiency, which are particularly decisive for low-cost or power constrained IoT applications. The solution we propose aims at maximizing the metric of area efficiency by rotating the QC-LDPC code representations amongst the block RAMs in digit level. Moreover, optimized parallelized computing techniques, lazy accumulation and block partition are exploited to improve key decapsulation in terms of area and timing efficiency. We show for instance that our area-optimized implementation for 128-bit security requires 6.82 x 105 cycles and 2.26 x 106 cycles to encapsulate and decapsulate a shared secret, respectively. The area-optimized design uses only 39 slices (3 percent of the available logic) and 809 slices (39 percent of the available logic) for key encapsulation and key decapsulation respectively, on a small-size low-end Xilinx Spartan-6 FPGA.},
  archive      = {J_TC},
  author       = {Jingwei Hu and Marco Baldi and Paolo Santini and Neng Zeng and San Ling and Huaxiong Wang},
  doi          = {10.1109/TC.2019.2948323},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {327-341},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight key encapsulation using LDPC codes on FPGAs},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). REMOTE: Robust external malware detection framework by using
electromagnetic signals. <em>TC</em>, <em>69</em>(3), 312–326. (<a
href="https://doi.org/10.1109/TC.2019.2945767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical systems (CPS) are controlling many critical and sensitive aspects of our physical world while being continuously exposed to potential cyber-attacks. These systems typically have limited performance, memory, and energy reserves, which limits their ability to run existing advanced malware protection, and that, in turn, makes securing them very challenging. To tackle these problems, this paper proposes, REMOTE, a new robust framework to detect malware by externally observing Electromagnetic (EM) signals emitted by an electronic computing device (e.g., a microprocessor) while running a known application, in real-time and with a low detection latency, and without any a priori knowledge of the malware. REMOTE does not require any resources or infrastructure on, or any modifications to, the monitored system itself, which makes REMOTE especially suitable for malware detection on resource-constrained devices such as embedded devices, CPSs, and Internet of Things (IoT) devices where hardware and energy resources may be limited. To demonstrate the usability of REMOTE in real-world scenarios, we port two real-world programs (an embedded medical device and an industrial PID controller), each with a meaningful attack (a code-reuse and a code-injection attack), to four different hardware platforms. We also port shellcode-based DDoS and Ransomware attacks to five different standard applications on an embedded system. To further demonstrate the applicability of REMOTE to commercial CPS, we use REMOTE to monitor a Robotic Arm. Our results on all these different hardware platforms show that, for all attacks on each of the platforms, REMOTE successfully detects each instance of an attack and has 99.9 percent true positive rates) under all these conditions. We also compare REMOTE to prior work EDDIE [1] and SYNDROME [2], and demonstrate that these prior work are unable to achieve high accuracy under these variations.},
  archive      = {J_TC},
  author       = {Nader Sehatbakhsh and Alireza Nazari and Monjur Alam and Frank Werner and Yuanda Zhu and Alenka Zajic and Milos Prvulovic},
  doi          = {10.1109/TC.2019.2945767},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {312-326},
  shortjournal = {IEEE Trans. Comput.},
  title        = {REMOTE: Robust external malware detection framework by using electromagnetic signals},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing neural network based decoders for the surface
code. <em>TC</em>, <em>69</em>(2), 300–311. (<a
href="https://doi.org/10.1109/TC.2019.2948612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching algorithms can be used for identifying errors in quantum systems, being the most famous the Blossom algorithm. Recent works have shown that small distance quantum error correction codes can be efficiently decoded by employing machine learning techniques based on neural networks (NN). Various NN-based decoders have been proposed to enhance the decoding performance and the decoding time. Their implementation differs in how the decoding is performed, at logical or physical level, as well as in several neural network related parameters. In this work, we implement and compare two NN-based decoders, a low level decoder and a high level decoder, and study how different NN parameters affect their decoding performance and execution time. Crucial parameters such as the size of the training dataset, the structure and the type of the neural network, and the learning rate used during training are discussed. After performing this comparison, we conclude that the high level decoder based on a Recurrent NN shows a better balance between decoding performance and execution time and it is much easier to train. We then test its decoding performance for different code distances, probability datasets and under the depolarizing and circuit error models.},
  archive      = {J_TC},
  author       = {Savvas Varsamopoulos and Koen Bertels and Carmen Garcia Almudever},
  doi          = {10.1109/TC.2019.2948612},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {300-311},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Comparing neural network based decoders for the surface code},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NV-journaling: Locality-aware journaling using
byte-addressable non-volatile memory. <em>TC</em>, <em>69</em>(2),
288–299. (<a href="https://doi.org/10.1109/TC.2019.2948004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern file systems rely on the journaling mechanism to maintain crash consistency. The use of non-volatile memory (NVM) significantly improves the performance of journaling file systems. However, the superior performance of NVM will increase the likelihood of the journal filling up more often, thereby increasing the frequency of checkpointing. Together with the large amount of random checkpointing I/O found in most use cases, the checkpointing process becomes a new performance bottleneck. This paper proposes NV-Journaling, a strategy that reduces the frequency of checkpointing as well as reshapes the I/O pattern of checkpointing from one of random I/O to that which is more sequential I/O. NV-Journaling introduces fine-grained commits along with a cache-friendly NVM journaling layout that exploits the idiosyncrasies of NVM technology. Under this scheme, only the modified portion of a block, rather than the entire block, is written into the NVM journal device. Doing so significantly reduces checkpoint frequency and achieves better space utilization. NV-Journaling further reshapes the I/O pattern of checkpoint using a locality-aware checkpointing process. Checkpointed blocks are classified into hot and cold blocks. NV-Journaling maintains a hot block list to absorb repeated updates, and a cold bucket list to group blocks by their proximity on disk. When a checkpoint is required, cold buckets are selected such that blocks are sequentially flushed to the hard disk. We built a prototype of NV-Journaling by modifying the JBD2 layer in the Linux kernel and evaluated it using different workloads. Our experimental results show that NV-Journaling can improve performance by up to 4.3× compared to traditional journaling.},
  archive      = {J_TC},
  author       = {Cheng Chen and Qingsong Wei and Weng-Fai Wong and Chundong Wang},
  doi          = {10.1109/TC.2019.2948004},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {288-299},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NV-journaling: Locality-aware journaling using byte-addressable non-volatile memory},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low latency floating-point division and square root unit.
<em>TC</em>, <em>69</em>(2), 274–287. (<a
href="https://doi.org/10.1109/TC.2019.2947899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digit-recurrence algorithms are widely used in actual microprocessors to compute floating-point division and square root. These iterative algorithms present a good trade-off in terms of performance, area and power. We present a floating-point division and square root unit, which implements a radix-64 floating-point division and a radix-16 floating-point square root. To have an affordable implementation, each radix-64 division iteration and radix-16 square root iteration are made of simpler radix-4 iterations: 3 radix-4 iterations in division and 2 in square root. Speculation is used between consecutive radix-4 iterations to get a reduced timing. There are three different parts in digit-recurrence implementations: initialization, digit iterations, and rounding. The digit iteration is the iterative part and it uses the same logic for several cycles. Division and square root share partially the initialization and rounding stages, whereas each one has different logicforthe digit iterations. The result is a low-latency floating-point divider and square root, requiring 11, 6, and 4 cycles for double, single and half-precision division with normalized operands and result, and 15, 8 and 5 cycles for square root. One ortwo additional cycles are needed in case of subnormal operand(s) or result.},
  archive      = {J_TC},
  author       = {Javier D. Bruguera},
  doi          = {10.1109/TC.2019.2947899},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {274-287},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low latency floating-point division and square root unit},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WAL-SSD: Address remapping-based write-ahead-logging
solid-state disks. <em>TC</em>, <em>69</em>(2), 260–273. (<a
href="https://doi.org/10.1109/TC.2019.2947897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in flash memory technology have reduced the cost-per-bit of flash storage devices such as solid-state drives (SSDs), thereby enabling the development of large-capacity SSDs for enterprise-scale storage. However, two major concerns arise in designing SSDs. First, the size of the address mapping table is increasing in proportion to the capacity of the SSD. The SSD-internal firmware, called flash translation layer (FTL), must maintain the address mapping table in the internal DRAM. Although the previously proposed demand map loading technique uses a small size of cached map table, the technique aggravates poor random performance. Second, there are many redundant writes in storage workloads, which have an adverse effect on the performance and lifetime of the SSD. For example, many transaction-supporting applications use the write-ahead-log (WAL) scheme, which writes the same data twice. To resolve these problems, we propose a novel transaction-supporting SSD, called WAL-SSD, which logs transaction data at the internally-managed WAL area and relocates the data atomically via the FTL-level remap operation at the transaction checkpointing. It can also be used to transform random write requests to sequential requests. We implemented a prototype of WAL-SSD with a real SSD device. Experiments demonstrate the performance improvement by WAL-SSD with three use cases: remap-journaling, atomic multi-block update, and random write logging.},
  archive      = {J_TC},
  author       = {Kyuhwa Han and Hyukjoong Kim and Dongkun Shin},
  doi          = {10.1109/TC.2019.2947897},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {260-273},
  shortjournal = {IEEE Trans. Comput.},
  title        = {WAL-SSD: Address remapping-based write-ahead-logging solid-state disks},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new class of single burst error correcting codes with
parallel decoding. <em>TC</em>, <em>69</em>(2), 253–259. (<a
href="https://doi.org/10.1109/TC.2019.2947425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With technology scaling, burst errors or clustered errors are becoming increasingly common in different types of memories. Multiple bit upsets due to particle strikes, write disturbance errors, and magnetic field coupling are a few of the mechanisms which cause clustered errors. In this article, a new class of single burst error correcting codes are presented which correct a single burst of any size b within a codeword. A code construction methodology is presented which enables us to construct the proposed scheme from existing codes, e.g., Hamming codes. A new single step decoding methodology for the proposed class of codes is also presented which enables faster decoding. Different code constructions using Hamming codes, and BCH codes have been presented in this paper and a comparison is made with existing schemes in terms of decoding complexity and data redundancy. The proposed scheme in all cases reduces the decoder complexity for little to no increase in data redundancy, specifically for higher burst error sizes.},
  archive      = {J_TC},
  author       = {Abhishek Das and Nur A. Touba},
  doi          = {10.1109/TC.2019.2947425},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {253-259},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A new class of single burst error correcting codes with parallel decoding},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive-length coding of image data for low-cost
approximate storage. <em>TC</em>, <em>69</em>(2), 239–252. (<a
href="https://doi.org/10.1109/TC.2019.2946795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, ever-increasing amounts of image data have been generated by users globally, and these images are routinely stored in cold storage systems in compressed formats. This article investigates the use of approximate storage that leverages the use of cheaper, lower reliability memories that can have higher error rates. Since traditional JPEG-based schemes based on variable-length coding are extremely sensitive to error, the direct use of approximate storage results in severe quality degradation. We propose an error-resilient adaptive-length coding (ALC) scheme that divides all symbols into two classes, based on their frequency of occurrence, where each class has a fixed-length codeword. This provides a balance between the reliability of fixed-length coding schemes, which have a high storage overhead, and the storage-efficiency of Huffman coding schemes, which show high levels of error on low-reliability storage platforms. Further, we use data partitioning to determine which bits are stored in approximate or reliable storage to lower the overall cost of storage. We show that ALC can be used with general non-volatile storage, and can substantially reduce the total cost compared to traditional JPEG-based storage.},
  archive      = {J_TC},
  author       = {Qianqian Fan and David J. Lilja and Sachin S. Sapatnekar},
  doi          = {10.1109/TC.2019.2946795},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {239-252},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive-length coding of image data for low-cost approximate storage},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secure and efficient control data isolation with
register-based data cloaking. <em>TC</em>, <em>69</em>(2), 226–238. (<a
href="https://doi.org/10.1109/TC.2019.2946770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attackers often exploit memory corruption vulnerabilities to overwrite control data and further gain control over victim applications. Despite progress in advanced defensive techniques, such attacks still remain a major security threat. In this article, we present Niffler, a new technique that provides lightweight and practical defense against such attacks. Niffler eliminates the threat of memory corruption over control data by cloaking all control data in registers along its execution and only spilling them into a dedicated read-only area in memory upon a shortage of registers. As an attacker cannot directly overwrite any register or read-only memory pages, no direct memory corruption on control data is feasible. Niffler is made efficient by compactly encoding return address, balancing register allocation, dynamically determining register spilling and leveraging the recent Intel Memory Protection Extensions (MPX) for control data lookup during register restoring. We implement Niffler based on LLVM and conduct a set of evaluations on SPECCPU 2006 and real-world applications. Performance evaluation shows that Niffler introduces an average of only 6.3 percent overhead on SPECCPU 2006 C programs and an average of 28.2 percent overhead on C++ programs.},
  archive      = {J_TC},
  author       = {Xiayang Wang and Fuqian Huang and Haibo Chen},
  doi          = {10.1109/TC.2019.2946770},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {226-238},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Secure and efficient control data isolation with register-based data cloaking},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing parallel i/o accesses through pattern-directed
and layout-aware replication. <em>TC</em>, <em>69</em>(2), 212–225. (<a
href="https://doi.org/10.1109/TC.2019.2946135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the performance gap between processors and storage devices keeps increasing, I/O performance becomes a critical bottleneck of modern high-performance computing systems. In this paper, we propose a pattern-directed and layout-aware data replication design, named PDLA, to improve the performance of parallel I/O systems. PDLA includes an HDD-based scheme H-PDLA and an SSD-based scheme S-PDLA. For applications with relatively low I/O concurrency, H-PDLA identifies access patterns of applications and makes a reorganized data replica for each access pattern on HDD-based servers with an optimized data layout. Moreover, to accommodate applications with high I/O concurrency, S-PDLA replicates critical access patterns that can bring performance benefits on SSD-based servers or on HDD-based and SSD-based servers. We have implemented the proposed replication scheme under MPICH2 library on top of OrangeFS file system. Experimental results show that H-PDLA can significantly improve the original parallel I/O system performance and demonstrate the advantages of S-PDLA over H-PDLA.},
  archive      = {J_TC},
  author       = {Shuibing He and Yanlong Yin and Xian-He Sun and Xuechen Zhang and Zongpeng Li},
  doi          = {10.1109/TC.2019.2946135},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {212-225},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing parallel I/O accesses through pattern-directed and layout-aware replication},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal metastability-containing sorting via parallel prefix
computation. <em>TC</em>, <em>69</em>(2), 198–211. (<a
href="https://doi.org/10.1109/TC.2019.2939818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Friedrichs et al. (TC 2018) showed that metastability can be contained when sorting inputs arising from time-to-digital converters, i.e., measurement values can be correctly sorted without resolving metastability using synchronizers first. However, this work left open whether this can be done by small circuits. We show that this is indeed possible, by providing a circuit that sorts Gray code inputs (possibly containing a metastable bit) and has asymptotically optimal depth and size. Our solution utilizes the parallel prefix computation (PPC) framework (JACM 1980). We improve this construction by bounding its fan-out by an arbitrary f ≥ 3, without affecting depth and increasing circuit size by a small constant factor only. Thus, we obtain the first PPC circuits with asymptotically optimal size, constant fan-out, and optimal depth. To show that applying the PPC framework to the sorting task is feasible, we prove that the latter can, despite potential metastability, be decomposed such that the core operation is associative. We obtain asymptotically optimal metastability-containing sorting networks. We complement these results with simulations, independently verifying the correctness as well as small size and delay of our circuits. Proofs are omitted in this version; the article with full proofs is provided online at http://arxiv.org/abs/1911.00267.},
  archive      = {J_TC},
  author       = {Johannes Bund and Christoph Lenzen and Moti Medina},
  doi          = {10.1109/TC.2019.2939818},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {198-211},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimal metastability-containing sorting via parallel prefix computation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative adaptation for energy-efficient heterogeneous
mobile SoCs. <em>TC</em>, <em>69</em>(2), 185–197. (<a
href="https://doi.org/10.1109/TC.2019.2943855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Mobile System-on-Chips (SoCs) containing CPU and GPU cores are becoming prevalent in embedded computing, and they need to execute applications concurrently. However, existing run-time management approaches do not perform adaptive mapping and thread-partitioning of applications while exploiting both CPU and GPU cores at the same time. In this paper, we propose an adaptive mapping and thread-partitioning approach for energy-efficient execution of concurrent OpenCL applications on both CPU and GPU cores while satisfying performance requirements. To start execution of concurrent applications, the approach makes mapping (number of cores and operating frequencies) and partitioning (distribution of threads between CPU and GPU) decisions to satisfy performance requirements for each application. The mapping and partitioning decisions are made by having a collaboration between the CPU and GPU cores&#39; processing capabilities such that balanced execution can be performed. During execution, adaptation is triggered when new application(s) arrive, or an executing one finishes, that frees cores. The adaptation process identifies a new mapping and thread-partitioning in a similar collaborative manner for remaining applications provided it leads to an improvement in energy efficiency. The proposed approach is experimentally validated on the Odroid-XU3 hardware platform with varying set of applications. Results show an average energy saving of 37\%, compared to existing approaches while satisfying the performance requirements.},
  archive      = {J_TC},
  author       = {Amit Kumar Singh and Karunakar Reddy Basireddy and Alok Prakash and Geoff V. Merrett and Bashir M. Al-Hashimi},
  doi          = {10.1109/TC.2019.2943855},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {185-197},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Collaborative adaptation for energy-efficient heterogeneous mobile SoCs},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Footprint-based DIMM hotplug. <em>TC</em>, <em>69</em>(2),
172–184. (<a href="https://doi.org/10.1109/TC.2019.2945562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power-efficiency has become one of the most critical concerns for HPC as we continue to scale computational capabilities. A significant fraction of system power is spent on large main memories, mainly caused by the substantial amount of DIMM standby power needed. However, while necessary for some workloads, for many workloads large memory configurations are too rich, i.e., these workloads only make use of a fraction of the available memory, causing unnecessary power usage. This observation opens new opportunities for power reduction by powering DIMMs on and off depending on the current workload. In this article, we propose footprint-based DIMM hotplug that enables a compute node to adjust the number of DIMMs that are powered on depending on the memory footprint of a running job. Our technique relies on two main subcomponents-memory footprint monitoring and DIMM management-which we both implement as part of an optimized page management system with small control overhead. Using Linux&#39;s memory hotplug capabilities, we implement our approach on a real system, and our results show that our proposed technique can save 50.6-52.1 percent of the DIMM standby energy and the CPU+DRAM energy of up to 1.50 Wh for various small-memory-footprint applications without loss of performance.},
  archive      = {J_TC},
  author       = {Shinobu Miwa and Masaya Ishihara and Hayato Yamaki and Hiroki Honda and Martin Schulz},
  doi          = {10.1109/TC.2019.2945562},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {172-184},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Footprint-based DIMM hotplug},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Per-operation reusability based allocation and migration
policy for hybrid cache. <em>TC</em>, <em>69</em>(2), 158–171. (<a
href="https://doi.org/10.1109/TC.2019.2944163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a hybrid cache consisting of SRAM and STT-RAM has attracted much attention as a future memory by complementing each other with different memory characteristics. Prior works focused on developing data allocation and migration techniques considering write-intensity to reduce write energy at STT-RAM. However, these works often neglect the impact of operation-specific reusability of a cache line. In this paper, we propose an energy-efficient per-operation reusability-based allocation and migration policy (ORAM) with a unified LRU replacement policy. First, to select an adequate memory type for allocation, we propose a cost function based on per-operation reusability - gain from an allocated cache line and loss from an evicted cache line for different memory types - which exploits the temporal locality. Besides, we present a migration policy, victim and target cache line selection scheme, to resolve memory type inconsistency between replacement policy and the allocation policy, with further energy reduction. Experiment results show an average energy reduction in the LLC and the main memory by 12.3 and 21.2 percent, and the improvement of latency and execution time by 21.2 and 8.8 percent, respectively, compared with a baseline hybrid cache management. In addition, the Energy-Delay Product (EDP) is improved by 36.9 percent over the baseline.},
  archive      = {J_TC},
  author       = {Minsik Oh and Kwangsu Kim and Duheon Choi and Hyuk-Jun Lee and Eui-Young Chung},
  doi          = {10.1109/TC.2019.2944163},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {158-171},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Per-operation reusability based allocation and migration policy for hybrid cache},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and efficient convolutional accelerator for edge
computing. <em>TC</em>, <em>69</em>(1), 138–152. (<a
href="https://doi.org/10.1109/TC.2019.2941875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are a vital approach in machine learning. However, their high complexity and energy consumption make them challenging to embed in mobile applications at the edge requiring real-time processes such as smart phones. In order to meet the real-time constraint of edge devices, recently proposed custom hardware CNN accelerators have exploited parallel processing elements (PEs) to increase throughput. However, this straightforward parallelization of PEs and high memory bandwidth require high data movement, leading to large energy consumption. As a result, only a certain number of PEs can be instantiated when designing bandwidth-limited custom accelerators targeting edge devices. While most bandwidth-limited designs claim a peak performance of a few hundred giga operations per second, their average runtime performance is substantially lower than their roofline when applied to state-of-the-art CNNs such as AlexNet, VGGNet and ResNet, as a result of low resource utilization and arithmetic intensity. In this work, we propose a zero-activation-skipping convolutional accelerator (ZASCA) that avoids noncontributory multiplications with zero-valued activations. ZASCA employs a dataflow that minimizes the gap between its average and peak performances while maximizing its arithmetic intensity for both sparse and dense representations of activations, targeting the bandwidth-limited edge computing scenario. More precisely, ZASCA achieves a performance efficiency of up to 94 percent over a set of state-of-the-art CNNs for image classification with dense representation where the performance efficiency is the ratio between the average runtime performance and the peak performance. Using its zero-skipping feature, ZASCA can further improve the performance efficiency of the state-of-the-art CNNs by up to 1.9× depending on the sparsity degree of activations. The implementation results in 65-nm TSMC CMOS technology show that, compared to the most energy-efficient accelerator, ZASCA can process convolutions from 5.5× to 17.5× faster, and is between 2.1× and 4.5× more energy efficient while occupying 2.1× less silicon area.},
  archive      = {J_TC},
  author       = {Arash Ardakani and Carlo Condo and Warren J. Gross},
  doi          = {10.1109/TC.2019.2941875},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {138-152},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast and efficient convolutional accelerator for edge computing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FACCT: FAst, compact, and constant-time discrete gaussian
sampler over integers. <em>TC</em>, <em>69</em>(1), 126–137. (<a
href="https://doi.org/10.1109/TC.2019.2940949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete Gaussian sampler is one of the fundamental tools in implementing lattice-based cryptosystems. However, a naive discrete Gaussian sampling implementation suffers from side-channel vulnerabilities, and the existing countermeasures usually introduce significant overhead in either the running speed or the memory consumption. In this paper, we propose a fast, compact, and constant-time implementation of the binary sampling algorithm, originally introduced in the BLISS signature scheme. Our implementation adapts the Rényi divergence and the transcendental function polynomial approximation techniques. The efficiency of our scheme is independent of the standard deviation, and we show evidence that our implementations are either faster or more compact than several existing constant-time samplers. In addition, we show the performance of our implementation techniques applied to and integrated with two existing signature schemes: qTesla and Falcon. On the other hand, the convolution theorems are typically adapted to sample from larger standard deviations, by combining samples with much smaller standard deviations. As an additional contribution, we show better parameters for the convolution theorems.},
  archive      = {J_TC},
  author       = {Raymond K. Zhao and Ron Steinfeld and Amin Sakzad},
  doi          = {10.1109/TC.2019.2940949},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {126-137},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FACCT: FAst, compact, and constant-time discrete gaussian sampler over integers},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scrabble: A fine-grained cache with adaptive merged block.
<em>TC</em>, <em>69</em>(1), 112–125. (<a
href="https://doi.org/10.1109/TC.2019.2939809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large fraction of the microprocessor energy is consumed by the data movement in the system. One of the reasons is the inefficiency in the conventional cache design. Cache blocks larger than a word are used in conventional caches to exploit spatial locality. However, many applications only use a small part of a cache block before its eviction. Transferring and storing unused data wastes bandwidth, energy, and limited cache space. Prior work on fine-grained caches can reduce data access and storage granularity to reduce the amount of unused data. However, small data blocks typically require greater metadata and control overhead. Sharing the common bits among tags of fine-grained blocks can reduce the metadata overhead but the constraints on which fine-grained blocks can share tag bits can cause fragmentation. This work proposes scrabble, a fine-grained cache that can merge multiple non-contiguous fine-grained blocks into a variable size merged block. The length of the shared tag is maximized to reduce the metadata overhead. The space utilization is improved by supporting merged blocks with variable size. The control overhead can be reduced by moving the merged block together from memory to the last level cache. For applications with poor spatial locality, Scrabble cache can achieve more than 40 percent of performance improvement. Even for application with good spatial locality, the speedup is still more than 7 percent. In general, for an evaluated set of benchmarks, Scrabble cache achieves an average of 2.41× effective capacity over the baseline cache with the same cache capacity which leads to a 16.7 percent performance improvement and an 11 percent on-chip energy reduction. As compared to a state-of-the-art fine-grained cache, Scrabble cache achieves a 1.25× effective capacity, a 7.9 percent speedup, and a 5.8 percent on-chip energy reduction.},
  archive      = {J_TC},
  author       = {Chao Zhang and Yuan Zeng and Xiaochen Guo},
  doi          = {10.1109/TC.2019.2939809},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {112-125},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scrabble: A fine-grained cache with adaptive merged block},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Signal strength-aware adaptive offloading with local image
preprocessing for energy efficient mobile devices. <em>TC</em>,
<em>69</em>(1), 99–111. (<a
href="https://doi.org/10.1109/TC.2019.2939239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To prolong battery life of mobile devices, image processing applications often exploit offloading techniques which run some or all of the computations on remote servers. Unfortunately, the existing offloading techniques do not consider the fact that data transmission time and energy consumption of wireless network interfaces exponentially increase when signal strength decreases. In this paper, we propose an adaptive offloading for image processing applications, which considers wireless signal strength. To improve performance and energy efficiency of offloading, we also propose to adaptively exploit local preprocessing (executing image preprocessing on local mobile devices), considering wireless signal strength; the local preprocessing usually reduces the size of transmission image in offloading. Our proposed technique estimates performance and energy consumption of the following three methods, depending on the wireless signal strength: 1) local execution (executing all the computations on the local mobile devices), 2) offloading without local preprocessing, and 3) offloading with local preprocessing. Based on the estimated performance and energy consumption, our technique employs one among the three methods, which is expected to result in the best performance or energy efficiency. In our evaluation on an off-the-shelf smartphone, when a user prefers performance to energy, our proposed technique improves performance by 27.1 percent, compared to the conventional offloading technique that does not consider the signal strength. On the other hand, when a user prefers energy to performance, our proposed technique saves system-wide (not just CPU nor wireless network interface) energy consumption by 26.3 percent, on average, compared to the conventional offloading technique.},
  archive      = {J_TC},
  author       = {Young Geun Kim and Young Seo Lee and Sung Woo Chung},
  doi          = {10.1109/TC.2019.2939239},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {99-111},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Signal strength-aware adaptive offloading with local image preprocessing for energy efficient mobile devices},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel sequence generation approach to diagnose faults in
reconfigurable scan networks. <em>TC</em>, <em>69</em>(1), 87–98. (<a
href="https://doi.org/10.1109/TC.2019.2939125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the complexity of nanoelectronic devices rapidly increasing, an efficient way to handle large number of embedded instruments became a necessity. The IEEE 1687 standard was introduced to provide flexibility in accessing and controlling such instrumentation through a reconfigurable scan chain. Nowadays, together with testing the system for defects that may affect the scan chains themselves, the diagnosis of such faults is also important. This article proposes a method for generating stimuli to precisely identify permanent high-level faults in a IEEE 1687 reconfigurable scan chain: the system is modeled as a finite state automaton where faults correspond to multiple incorrect transitions; then, a dynamic greedy algorithm is used to select a sequence of inputs able to distinguish between all possible faults. Experimental results on the widely-adopted ITC&#39;02 and ITC&#39;16 benchmark suites, as well as on synthetically generated circuits, clearly demonstrate the applicability and effectiveness of the proposed approach: generated sequences are two orders of magnitude shorter compared to previous methodologies, while the computational resources required remain acceptable even for larger benchmarks.},
  archive      = {J_TC},
  author       = {Riccardo Cantoro and Aleksa Damljanovic and Matteo Sonza Reorda and Giovanni Squillero},
  doi          = {10.1109/TC.2019.2939125},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {87-98},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A novel sequence generation approach to diagnose faults in reconfigurable scan networks},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximizing i/o throughput and minimizing performance
variation via reinforcement learning based i/o merging for SSDs.
<em>TC</em>, <em>69</em>(1), 72–86. (<a
href="https://doi.org/10.1109/TC.2019.2938956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Merging technique is widely adopted by I/O schedulers to maximize system I/O throughput. However, I/O merging could increase the latency of individual I/O, thus incurring prolonged I/O latencies and enlarged performance variations. Even with better system throughput, higher worst-case latency experienced by some requests could block the SSD storage system, which violates the QoS (Quality of Service) requirement. In order to improve QoS performance while providing higher I/O throughput, this paper proposes a reinforcement learning based I/O merging approach. Through learning the characteristic of various I/O patterns, the proposed approach makes merging decisions adaptively based on different I/O workloads. Evaluation results show that the proposed scheme is capable of reducing the standard deviation of I/O latency by 19.1 percent on average, worst-case latency by 7.3-60.9 percent at the 99.9th percentile compared with the latest I/O merging scheme, while maximizing system throughput.},
  archive      = {J_TC},
  author       = {Chao Wu and Cheng Ji and Qiao Li and Congming Gao and Riwei Pan and Chenchen Fu and Liang Shi and Chun Jason Xue},
  doi          = {10.1109/TC.2019.2938956},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {72-86},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Maximizing I/O throughput and minimizing performance variation via reinforcement learning based I/O merging for SSDs},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). KnightSim: A fast discrete event-driven simulation
methodology for computer architectural simulation. <em>TC</em>,
<em>69</em>(1), 65–71. (<a
href="https://doi.org/10.1109/TC.2019.2938507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a fast discrete event-driven simulation methodology, called KnightSim, that is intended for use in the development of future computer architectural simulations. KnightSim extends an older event-driven simulation library by (1) incorporating corrections to functional issues that were introduced by the recent additions of stack protection, pointer mangling, and source fortification in the Linux software stack, (2) incorporating optimizations to the event engine, and (3) introducing a novel parallel implementation. KnightSim implements events as independently executable x86 “KnightSim Contexts”. KnightSim Contexts comprise a mechanism for fast context execution and automatically model occupancy and contention, which readily lends itself to use in computer architectural simulations. We present the implementation methodologies of KnightSim and Parallel KnightSim with a detailed performance analysis. Our performance analysis makes direct comparisons between KnightSim, Parallel KnightSim, and the discrete event-driven simulation engines found in three different mainstream computer architectural simulators. Our results show that on average KnightSim achieves speedups of 2.8 to 11.9 over the other discrete event-driven simulation engines. Our results also show that on average Parallel KnightSim can achieve speedups over KnightSim of 1.89, 3.33, 5.84, and 9.24 for 2, 4, 8, and 16 threaded executions respectively.},
  archive      = {J_TC},
  author       = {Christopher E. Giles and Christina L. Peterson and Mark A. Heinrich},
  doi          = {10.1109/TC.2019.2938507},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {65-71},
  shortjournal = {IEEE Trans. Comput.},
  title        = {KnightSim: A fast discrete event-driven simulation methodology for computer architectural simulation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TTADF: Power efficient dataflow-based multicore co-design
flow. <em>TC</em>, <em>69</em>(1), 51–64. (<a
href="https://doi.org/10.1109/TC.2019.2937867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The era of mobile communications and the Internet of Things (IoT) has introduced numerous challenges for mobile processing platforms that are responsible for increasingly complex signal processing tasks from different application domains. In recent years, the power efficiency of computing has been improved by adding more parallelism and workload-specific computing resources to such platforms. However, programming of parallel systems can be time-consuming and challenging if only low-level programming methods are used. This work presents a dataflow-based co-design framework TTADF that reduces the design effort of both software and hardware design for mobile processing platforms. The paper presents three application examples from the fields of video coding, machine vision, and wireless communications. The application examples are mapped and profiled both on a pipelined and a shared-memory multicore platform that is generated by TTADF. The results of the TTADF co-design-based solutions are compared against previous manually created designs and a recent dataflow-based design flow, showing that TTADF provides very high energy efficiency together with a high level of automation in software and hardware design.},
  archive      = {J_TC},
  author       = {Ilkka Hautala and Jani Boutellier and Olli Silvén},
  doi          = {10.1109/TC.2019.2937867},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {51-64},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TTADF: Power efficient dataflow-based multicore co-design flow},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Utilization-tensity bound for real-time DAG tasks under
global EDF scheduling. <em>TC</em>, <em>69</em>(1), 39–50. (<a
href="https://doi.org/10.1109/TC.2019.2936477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilization bound is a well-known concept in real-time scheduling theory for sequential periodic tasks, which can be used both for quantifying the performance of scheduling algorithms and as efficient schedulability tests. However, the schedulability of parallel real time task graphs depends on not only utilization, but also another parameter tensity, the ratio between the longest path length and period. In this paper, we use utilization-tensity bounds to better characterize the schedulability of parallel real-time tasks. In particular, we derive utilization-tensity bounds for parallel DAG tasks under global EDF scheduling, which facilitate significantly more precise schedulability analysis than the state-of-the-art analysis techniques based on capacity augmentation bound and response time analysis. Moreover, we apply the above results to the federated scheduling paradigm to improve the system schedulability by choosing proper scheduling strategies for tasks with different workload and structure features.},
  archive      = {J_TC},
  author       = {Xu Jiang and Jinghao Sun and Yue Tang and Nan Guan},
  doi          = {10.1109/TC.2019.2936477},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {39-50},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Utilization-tensity bound for real-time DAG tasks under global EDF scheduling},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New flexible multiple-precision multiply-accumulate unit for
deep neural network training and inference. <em>TC</em>, <em>69</em>(1),
26–38. (<a href="https://doi.org/10.1109/TC.2019.2936192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new flexible multiple-precision multiply-accumulate (MAC) unit is proposed for deep neural network training and inference. The proposed MAC unit supports both fixed-point operations and floating-point operations. For floating-point format, the proposed unit supports one 16-bit MAC operation or sum of two 8-bit multiplications plus a 16-bit addend. To make the proposed MAC unit more versatile, the bit-width of exponent and mantissa can be flexibly exchanged. By setting the bit-width of exponent to zero, the proposed MAC unit also supports fixed-point operations. For fixed-point format, the proposed unit supports one 16-bit MAC or sum of two 8-bit multiplications plus a 16-bit addend. Moreover, the proposed unit can be further divided to support sum of four 4-bit multiplications plus a 16-bit addend. At the lowest precision, the proposed MAC unit supports accumulating of eight 1-bit logic AND operations to enable the support of binary neural networks. Compared to the standard 16-bit half-precision MAC unit, the proposed MAC unit provides more flexibility with only 21.8 percent area overhead. Compared to a standard 32-bit single-precision MAC unit, the proposed MAC unit requires much less hardware cost but still provides 8-bit exponent in the numerical format to maintain large dynamic range for deep learning computing.},
  archive      = {J_TC},
  author       = {Hao Zhang and Dongdong Chen and Seok-Bum Ko},
  doi          = {10.1109/TC.2019.2936192},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {26-38},
  shortjournal = {IEEE Trans. Comput.},
  title        = {New flexible multiple-precision multiply-accumulate unit for deep neural network training and inference},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lightweight power monitoring framework for virtualized
computing environments. <em>TC</em>, <em>69</em>(1), 14–25. (<a
href="https://doi.org/10.1109/TC.2019.2936018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive use of virtualization techniques in today&#39;s datacenters poses challenges in power monitoring since it is not possible to directly measure the power consumption of a virtual entity such as a virtual machine (VM) and a container. In this paper, we present cWatts++, a lightweight virtual power meter that enables accurate power usage measurement in virtualized computing environments such as VMs and containers of Cloud data centers. At the core of cWatts++ is its application-agnostic power model. To this end, we devise two power models (eventModel and raplModel) that are driven by CPU event counters and the Running Average Power Limit (RAPL) feature of modern Intel CPUs, respectively. While eventModel is more generic and, thus, applicable to a wide range of workloads, raplModel is particularly good for CPU-bound workloads. We have evaluated cWatts++ with its two power models in a real system using the PARSEC benchmark suite and our in-house benchmarks. Our evaluation study demonstrates that these power models have an average error of 4.55 and 1.25 percent, respectively, compared with actual power usage measurements of a real power meter, Cabac Power-Mate.},
  archive      = {J_TC},
  author       = {James Phung and Young Choon Lee and Albert Y. Zomaya},
  doi          = {10.1109/TC.2019.2936018},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {14-25},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight power monitoring framework for virtualized computing environments},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power- and cache-aware task mapping with dynamic power
budgeting for many-cores. <em>TC</em>, <em>69</em>(1), 1–13. (<a
href="https://doi.org/10.1109/TC.2019.2935446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two factors primarily affect the performance of multi-threaded tasks on many-core processors with logically-shared and physically-distributed Last-Level Cache (LLC): the LLC latencies of threads running on different cores and the per-core power budgets that aim to guarantee thermally safe operation. Two knobs affect these factors: First, the mapping of threads to cores affects both the LLC latencies and the power budgets. Second, dynamic power budgeting refines the power budgets during task execution. A mapping that spatially distributes threads across the many-core increases the power budgets, but unfortunately also increases the LLC latencies. Contrarily, mapping all threads near the center of the many-core minimizes the LLC latencies, but unfortunately also decreases the power budgets. Consequently, both metrics cannot be simultaneously optimal, which leads to a Pareto-optimization for task mapping that has formerly not been exploited. Dynamic power budgeting reallocates the power budgets according to the tasks&#39; execution phases. This results in a dynamically changing non-uniform power budget, which further increases the performance. We are the first to present a run-time algorithm PCGov combining task-agnostic task mapping and task-aware dynamic power budgeting for many-cores with shared distributed LLC. PCGov yields up to 21 percent lower response time and 13 percent lower energy consumption compared to the state-of-the-art, with a low overhead of less than 0.5 percent.},
  archive      = {J_TC},
  author       = {Martin Rapp and Mark Sagi and Anuj Pathania and Andreas Herkersdorf and Jörg Henkel},
  doi          = {10.1109/TC.2019.2935446},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Power- and cache-aware task mapping with dynamic power budgeting for many-cores},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
