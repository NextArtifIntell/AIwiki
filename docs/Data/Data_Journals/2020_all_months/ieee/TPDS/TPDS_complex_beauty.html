<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpds---210">TPDS - 210</h2>
<ul>
<li><details>
<summary>
(2020). Pattern-based dynamic compilation system for CGRAs with
online configuration transformation. <em>TPDS</em>, <em>31</em>(12),
2981–2994. (<a href="https://doi.org/10.1109/TPDS.2020.3007492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prevailing data-intensive applications, such as artificial intelligence and internet of things, demand considerable compute capability. Coarse-grained reconfigurable architectures (CGRAs) can meet this demand via providing abundant compute resources. However, compilation has become an essential problem because the increasing resources need to be orchestrated efficiently. Static compilation is insufficient due to conservative resource allocation and exponentially increasing time cost while state-of-the-art dynamic compilation still performs poorly in both generality and efficiency. This article proposes a dynamic compilation system for CGRAs through online pattern-based configuration transformation, which enables virtualization to improve resource utilization and flexibility. It utilizes statically-generated patterns to straightforwardly determine dynamic placement of registers and operations so that the transformation algorithm has a low complexity. Domain-specific features are extracted by a k-means clustering algorithm to help improve the quality of patterns. The experimental results show that statically compiled applications can be transformed onto arbitrary resources at runtime, reserving 73.5 (22.8-163.3 percent) of the original performance/resource on average, 9.1 (0-52.9 percent) better than the state-of-theart non-general methods.},
  archive      = {J_TPDS},
  author       = {Leibo Liu and Xingchen Man and Jianfeng Zhu and Shouyi Yin and Shaojun Wei},
  doi          = {10.1109/TPDS.2020.3007492},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2981-2994},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Pattern-based dynamic compilation system for CGRAs with online configuration transformation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HeteroYARN: A heterogeneous FPGA-accelerated architecture
based on YARN. <em>TPDS</em>, <em>31</em>(12), 2968–2980. (<a
href="https://doi.org/10.1109/TPDS.2019.2905201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the heterogeneous distributed platform integrating with FPGAs to accelerate computation tasks has been widely studied to deal with the deluge of data. However, most of current works suffer from poor universality and low resource utilization that run specific algorithms with the highly customized structure. Moreover, there are still many challenges, such as data curation, task scheduling, and resource management, which further limit the scalability of a CPU-FPGA distributed platform. In this paper, we present HeteroYARN, an FPGA-accelerated heterogeneous architecture based on YARN platform, which provides resource management and programming support for computing-intensive applications using FPGAs. In particular, the HeteroYARN abstracts FPGA accelerators as general resources and provides programming APIs to utilize those accelerators easily. Our HeteroYARN simplifies the request and usage of FPGA resources to enhance the efficiency of the heterogeneous framework while maintaining previous workflow unchanged. Experimental results using two representative algorithms, K-means and Naive Bayes classifier, which are accelerated by FPGAs, demonstrate the usability of the HeteroYARN framework and show performance speedup improvement by 7.5x (K-means) and 2.3x (Naive Bayes) respectively compared to conventional CPU-only applications provided by Mahout.},
  archive      = {J_TPDS},
  author       = {Ruixuan Li and Qi Yang and Yuhua Li and Xiwu Gu and Weijun Xiao and Keqin Li},
  doi          = {10.1109/TPDS.2019.2905201},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2968-2980},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HeteroYARN: A heterogeneous FPGA-accelerated architecture based on YARN},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Millimeter-scale and billion-atom reactive force field
simulation on sunway taihulight. <em>TPDS</em>, <em>31</em>(12),
2954–2967. (<a href="https://doi.org/10.1109/TPDS.2020.3008499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale molecular dynamics (MD) simulations on supercomputers play an increasingly important role in many research areas. With the capability of simulating charge equilibration (QEq), bonds and so on, Reactive force field (ReaxFF) enables the precise simulation of chemical reactions. Compared to the first principle molecular dynamics (FPMD), ReaxFF has far lower requirements on computational resources so that it can achieve higher efficiencies for large-scale simulations. In this article, we present our efforts on scaling ReaxFF on the Sunway TaihuLight Supercomputer (TaihuLight). We have carefully redesigned the force analysis and neighbor list building steps. By applying fine-grained optimizations we gain better single process performance. For the many-body interactions, we propose an isolated computation and update strategy and implement inverse trigonometric functions. For QEq, we implement a pipelined conjugate gradient (CG) approach to achieving better scalability. Furthermore, we reorganize the data layout and implement the update operation based on data locality in ReaxFF. Our experiments show that this approach can simulate chemical reactions with 1,358,954,496 atoms using 4,259,840 cores with a performance of 0.015 ns/day. To our best knowledge, this is the first realization of chemical reaction simulation with a millimeter-scale force field.},
  archive      = {J_TPDS},
  author       = {Ping Gao and Xiaohui Duan and Tingjian Zhang and Meng Zhang and Bertil Schmidt and Xun Zhang and Hongliang Sun and Wusheng Zhang and Lin Gan and Wei Xue and Haohuan Fu and Weiguo Liu and Guangwen Yang},
  doi          = {10.1109/TPDS.2020.3008499},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2954-2967},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Millimeter-scale and billion-atom reactive force field simulation on sunway taihulight},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Traffic-aware erasure-coded archival schemes for in-memory
stores. <em>TPDS</em>, <em>31</em>(12), 2938–2953. (<a
href="https://doi.org/10.1109/TPDS.2020.3009092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redundancy schemes are introduced to in-memory stores to provide fault tolerance. To achieve good trade-off between access performance and memory efficiency, it is appropriate to adopt replication and erasure coding to keep popular and unpopular data, respectively. Within such a hybrid-redundancy in-memory store, an issue of redundancy transition from replication to erasure coding (a.k.a., erasure-coded archival) should be addressed for unpopular in-memory datasets, since caching workloads exhibit long-tail distributions and most in-memory data are unpopular. If data replicas are distributed across nodes in randomly-selected racks, then subsequent data-block-replica retrieval for erasure-coded archival will create cross-rack traffic, and final parity-block relocation will cause extra cross-rack communications. In this article, we propose an encoding-oriented replica placement policy - ERP - by incorporating an interleaved declustering mechanism. We design two traffic-aware erasure-coded archival schemes -TEA-TL and TEA-SL - for ERP-powered in-memory stores by taking into account temporal locality and spatial locality, respectively. With ERP in place, both TEA-TL and TEA-SL schemes embrace the following three salient features: (i) they alleviate cross-rack traffic raised by retrieving required data-block replicas; (ii) they improve rack-level load balancing by distributing replicas via load-aware primary-rack-selection approach; and (iii) they mitigate block-relocation operations launched to sustain rack-level and node-level fault-tolerance. We conduct quantitative performance evaluations using the YCSB benchmark. The empirical results show that both TEA-TL and TEA-SL schemes not only bring forth lower cross-rack traffic than the four candidate encoding schemes, but also exhibit superb archival-throughput and rack-level-balancing performance. In particular, within a group of comparative tests using the baseline configurations, TEA-TL and TEA-SL accelerate archival throughput by at least 36.3 and 70.8 percent, respectively; both TEA-TL and TEA-SL schemes improve rack-level load-balancing by a factor of more than 1.45x relative to the four candidate encoding schemes.},
  archive      = {J_TPDS},
  author       = {Bin Xu and Jianzhong Huang and Xiao Qin and Qiang Cao},
  doi          = {10.1109/TPDS.2020.3009092},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2938-2953},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Traffic-aware erasure-coded archival schemes for in-memory stores},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards unaligned writes optimization in cloud storage with
high-performance SSDs. <em>TPDS</em>, <em>31</em>(12), 2923–2937. (<a
href="https://doi.org/10.1109/TPDS.2020.3006655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NVMe SSDs provide extremely high performance and have been widely deployed in distributed object storage systems in data centers. However, we observe that there are still severe performance degradation and write amplification under the unaligned writes scenario with high-performance SSDs. In this article, we identify that the RMW sequence which is used to handle the unaligned writes incurs severe overhead in the data path. Besides, unaligned writes incur additional metadata management overhead in the block map table. To address these problems, we propose an object-based device system named NVStore to optimize the unaligned writes in cloud storage with NVMe SSDs. NVStore provides a Flexible Cache Management to reduce the RMW operations while supporting lazy page sync and ensuring data consistency. To optimize the metadata management, NVStore proposes a KV Affinity Metadata Management which co-designs the block map and key-value store to provides a flattened and decoupled metadata management. Evaluations show that NVStore provides at most 6.11× bandwidth of BlueStore in the cluster. Besides, NVStore can reduce at most 94.7 percent of the write traffic from metadata under unaligned writes compared to BlueStore and achieves smaller data write traffic which is about 50 percent of BlueStore and 65.7 percent of FileStore.},
  archive      = {J_TPDS},
  author       = {Jiwu Shu and Fei Li and Siyang Li and Youyou Lu},
  doi          = {10.1109/TPDS.2020.3006655},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2923-2937},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards unaligned writes optimization in cloud storage with high-performance SSDs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatially bursty i/o on supercomputers: Causes, impacts and
solutions. <em>TPDS</em>, <em>31</em>(12), 2908–2922. (<a
href="https://doi.org/10.1109/TPDS.2020.3005572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the I/O characteristics of supercomputers is crucial for grasping accurate I/O workloads and uncovering potential I/O inefficiency. We collect and analyze I/O traces from two production supercomputers, and find that the I/O traffic peaks in the system not only occur in short periods of time but also originate from a minority of adjacent compute nodes, which we call spatially bursty I/O. Since modern supercomputers widely adopt I/O forwarding architecture, in which an I/O node performs I/O on behalf of a subset of compute nodes in the vicinity, spatially bursty I/O will cause significant load imbalance and underutilization on the I/O nodes. To address such problems, we quantitatively analyze the two causes of spatially bursty I/O, including uneven I/O distribution on job&#39;s processes and uneven job nodes distribution on the system. Two different solutions are proposed to mobilize more I/O nodes to participate in job&#39;s I/O activity. (1) We change the I/O node mapping, making adjacent compute nodes use different I/O nodes instead of a same one. (2) According to the job&#39;s I/O characteristics extracted from history I/O traces, we distribute the compute nodes of data-intensive jobs more sparsely to utilize more I/O nodes. Extensive evaluations of both solutions show that they can further exploit the potential of I/O forwarding layer. We have deployed the proposed I/O node mapping on a production supercomputer for 11 months. Our experience finds that it can effectively promote I/O performance, balance loads, and alleviate I/O interference.},
  archive      = {J_TPDS},
  author       = {Jie Yu and Wenxiang Yang and Fang Wang and Dezun Dong and Jinghua Feng and Yuqi Li},
  doi          = {10.1109/TPDS.2020.3005572},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2908-2922},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Spatially bursty I/O on supercomputers: Causes, impacts and solutions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPU-accelerated real-time stereo estimation with binary
neural network. <em>TPDS</em>, <em>31</em>(12), 2896–2907. (<a
href="https://doi.org/10.1109/TPDS.2020.3006238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation from stereo images is essential to many applications such as robotics and autonomous vehicles, most of which ask for the real-time response, high energy and storage efficiency. Recent work has shown deep neural networks (DNN) perform extremely well for stereo estimation. However, these state-of-the-art DNN based algorithms are challenging to be deployed into real-world applications due to the high computational complexities of DNNs. Most of them are too slow for real-time inference and require several seconds of GPU computation to process image frames. In this article, we address the problem of fast stereo estimation and propose an efficient and light-weighted stereo matching system, called StereoBit, to produce a disparity map in a real-time manner while achieving close to state-of-the-art accuracy. To achieve this goal, we propose a binary neural network to generate weighted Hamming distance for an efficient similarity join in stereo estimation. In addition, we propose a novel approximation approach to derive StereoBit network directly from the well-trained network with the cosine similarity. Our approximation strategies enable a significant speedup while maintaining almost the same accuracy compared to the network with the cosine similarity. Furthermore, we present an optimization framework for fully exploiting the computing power of StereoBit. The framework provides a significant speedup of stereo estimation routines, and at the same time, reduces the memory usage for storing parameters. The effectiveness of StereoBit is evaluated by comprehensive experiments. StereoBit can achieve 60 frames per second on an NVIDIA TITAN Xp GPU on KITTI 2012 benchmark while achieving 3-pixel non-occluded stereo error 3.56 percent.},
  archive      = {J_TPDS},
  author       = {Gang Chen and Haitao Meng and Yucheng Liang and Kai Huang},
  doi          = {10.1109/TPDS.2020.3006238},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2896-2907},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GPU-accelerated real-time stereo estimation with binary neural network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Congestion-balanced and welfare-maximized charging
strategies for electric vehicles. <em>TPDS</em>, <em>31</em>(12),
2882–2895. (<a href="https://doi.org/10.1109/TPDS.2020.3003270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase of the number of electric vehicles (EVs), it is of vital importance to develop the efficient and effective charging scheduling schemes for all the EVs. In this article, we aim to maximize the social welfare of all the EVs, charging stations (CSs) and power plant (PP), by taking into account the changing demand of each EV, the changing price, the capacity and the congestion balance between different CSs. To this end, two efficient scheduling algorithms, i.e., Centralized Charging Strategy (CCS) and Distributed Charging Strategy (DCS) are proposed. CCS has a slightly better performance than the DCS, as it takes all the information and make the decision in the central control unit. On the other hand, DCS dose not require the private information from EVs and can make decentralized decision. Extensive simulation are conducted to verify the effectiveness of the proposed algorithms, in terms of the performance, congestion balance, and computing complexity.},
  archive      = {J_TPDS},
  author       = {Qiang Tang and Kezhi Wang and Kun Yang and Yuan-sheng Luo},
  doi          = {10.1109/TPDS.2020.3003270},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2882-2895},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Congestion-balanced and welfare-maximized charging strategies for electric vehicles},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPGPU performance estimation with core and memory frequency
scaling. <em>TPDS</em>, <em>31</em>(12), 2865–2881. (<a
href="https://doi.org/10.1109/TPDS.2020.3004623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, accurate and straightforward performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is essential to determine the best frequency configuration for energy saving. In this article, we reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Compared to the cycle-level simulators, which are too slow to apply on real hardware, our model only needs simple and one-off micro-benchmarks to extract a set of hardware parameters and kernel performance counters without any source code analysis. Our experimental results show that the proposed performance model can capture the kernel performance scaling behaviors under different frequency settings and achieve decent accuracy (average errors of 3.85, 8.6, 8.82, and 8.83 percent on a set of 20 GPU kernels with four modern Nvidia GPUs).},
  archive      = {J_TPDS},
  author       = {Qiang Wang and Xiaowen Chu},
  doi          = {10.1109/TPDS.2020.3004623},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2865-2881},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GPGPU performance estimation with core and memory frequency scaling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic undervolting to improve energy efficiency on
multicore x86 CPUs. <em>TPDS</em>, <em>31</em>(12), 2851–2864. (<a
href="https://doi.org/10.1109/TPDS.2020.3004383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chip manufacturers introduce redundancy at various levels of CPU design to guarantee correct operation, even for worst-case combinations of non-idealities in process variation and system operating conditions. This redundancy is implemented partly in the form of voltage margins. However, for a wide range of real-world execution scenarios these margins are excessive and merely translate to increased power consumption, hindering the effort towards higher-energy efficiency in both HPC and general purpose computing. Our study on the x86-64 Haswell and Skylake multicore microarchitectures reveals-wide voltage margins, which vary across different microarchitectures, different chip parts of the same microarchitecture, and across different workloads. We find that it is necessary to quantify-voltage margins using multi-threaded and multi-instance workloads, as characterization with single-threaded and single-instance workloads that do not stress the CPU to its full capacity typically identifies overly optimistic margins that lead to errors when applied in realistic program execution scenarios. In addition, we introduce, deploy and evaluate a run-time governor that dynamically reduces the supply voltage of modern multicore x86-64 CPUs. Our governor employs a model that takes as input a set of performance metrics which are directly measurable via performance monitoring counters and have high predictive value for the minimum tolerable supply voltage (V min ), to predict and apply the appropriate reduction for the workload at hand. Compared with the conventional DVFS governor, our approach achieves up to 42 percent energy savings for the Skylake family and 34 percent for the Haswell family for complex, real-world applications.},
  archive      = {J_TPDS},
  author       = {Panos Koutsovasilis and Konstantinos Parasyris and Christos D. Antonopoulos and Nikolaos Bellas and Spyros Lalis},
  doi          = {10.1109/TPDS.2020.3004383},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2851-2864},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dynamic undervolting to improve energy efficiency on multicore x86 CPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resource management for power-constrained HEVC transcoding
using reinforcement learning. <em>TPDS</em>, <em>31</em>(12), 2834–2850.
(<a href="https://doi.org/10.1109/TPDS.2020.3004735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of online video streaming applications and services along with the users&#39; demand for high-quality contents require High Efficiency Video Coding (HEVC), which provides higher video quality and more compression at the cost of increased complexity. On one hand, HEVC exposes a set of dynamically tunable parameters to provide trade-offs among Quality-of-Service (QoS), performance, and power consumption of multi-core servers on the video providers&#39; data center. On the other hand, resource management of modern multi-core servers is in charge of adapting system-level parameters, such as operating frequency and multithreading, to deal with concurrent applications and their requirements. Therefore, efficient multi-user HEVC streaming necessitates joint adaptation of application-and system-level parameters. Nonetheless, dealing with such a large and dynamic design space is challenging and difficult to address through conventional resource management strategies. Thus, in this work, we develop a multi-agent Reinforcement Learning framework to jointly adjust application-and system-level parameters at runtime to satisfy the QoS of multi-user HEVC streaming in power-constrained servers. In particular, the design space, composed of all design parameters, is split into smaller independent sub-spaces. Each design sub-space is assigned to a particular agent so that it can explore it faster, yet accurately. The benefits of our approach are revealed in terms of adaptability and quality (with up to to 4× improvements in terms of QoS when compared to a static resource management scheme), and learning time (6× fasterthan an equivalent mono-agent implementation). Finally, we show that the power-capping techniques formulated outperform the hardware-based power capping with respect to quality.},
  archive      = {J_TPDS},
  author       = {Luis Costero and Arman Iranfar and Marina Zapater and Francisco D. Igual and Katzalin Olcoz and David Atienza},
  doi          = {10.1109/TPDS.2020.3004735},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2834-2850},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Resource management for power-constrained HEVC transcoding using reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scheduling periodical multi-stage jobs with fuzziness to
elastic cloud resources. <em>TPDS</em>, <em>31</em>(12), 2819–2833. (<a
href="https://doi.org/10.1109/TPDS.2020.3004134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a workflow scheduling problem with stochastic task arrival times and fuzzy task processing times and due dates. The problem is common in many real-time and workflow-based applications, where tasks with fixed stage number and linearly dependency are executed on scalable cloud resources with multiple price options. The challenges lie in proposing effective, stable, and robust algorithms under stochastic and fuzzy tasks. A triangle fuzzy number-based model is formulated. Two metrics are explored: the cost and the degree of satisfaction. An iterated heuristic framework is proposed to periodically schedule tasks, which consists of a task collection and a fuzzy task scheduling phases. Two task collection strategies are presented and two task prioritization strategies are employed. In order to achieve a high satisfaction degree, deadline constraints are defined at both job and task levels. By designing delicate experiments and applying sophisticated statistical techniques, experimental results show that the proposed algorithm is more effective and robust than the two existing methods.},
  archive      = {J_TPDS},
  author       = {Jie Zhu and Xiaoping Li and Rubén Ruiz and Wei Li and Haiping Huang and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2020.3004134},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2819-2833},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scheduling periodical multi-stage jobs with fuzziness to elastic cloud resources},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed training of deep learning models: A taxonomic
perspective. <em>TPDS</em>, <em>31</em>(12), 2802–2818. (<a
href="https://doi.org/10.1109/TPDS.2020.3003307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed deep learning systems (DDLS) train deep neural network models by utilizing the distributed resources of a cluster. Developers of DDLS are required to make many decisions to process their particular workloads in their chosen environment efficiently. The advent of GPU-based deep learning, the ever-increasing size of datasets, and deep neural network models, in combination with the bandwidth constraints that exist in cluster environments require developers of DDLS to be innovative in order to train high-quality models quickly. Comparing DDLS side-by-side is difficult due to their extensive feature lists and architectural deviations. We aim to shine some light on the fundamental principles that are at work when training deep neural networks in a cluster of independent machines by analyzing the general properties associated with training deep learning models and how such workloads can be distributed in a cluster to achieve collaborative model training. Thereby we provide an overview of the different techniques that are used by contemporary DDLS and discuss their influence and implications on the training process. To conceptualize and compare DDLS, we group different techniques into categories, thus establishing a taxonomy of distributed deep learning systems.},
  archive      = {J_TPDS},
  author       = {Matthias Langer and Zhen He and Wenny Rahayu and Yanbo Xue},
  doi          = {10.1109/TPDS.2020.3003307},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2802-2818},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed training of deep learning models: A taxonomic perspective},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable, multi-constraint, complex-objective graph
partitioning. <em>TPDS</em>, <em>31</em>(12), 2789–2801. (<a
href="https://doi.org/10.1109/TPDS.2020.3002150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce XtraPuLP, a distributed-memory graph partitioner designed to process irregular trillion-edge graphs. XtraPuLP is based on the scalable label propagation community detection technique, which has been demonstrated in various prior works as a viable means to produce high quality partitions of skewed and small-world graphs with minimal computation time. Our XtraPuLP implementation can also be generalized to compute partitions with an arbitrary number of constraints, and it can compute partitions with balanced communication load across all parts. On a collection of large sparse graphs, we show that XtraPuLP partitioning is considerably faster than state-of-the-art partitioning methods, while also demonstrating that XtraPuLP can produce partitions of real-world graphs with billion+ vertices and over a hundred billion edges in minutes. Additionally, we demonstrate XtraPuLP on a variety of applications, including large-scale graph analytics and sparse matrix-vector multiplication.},
  archive      = {J_TPDS},
  author       = {George M. Slota and Cameron Root and Karen Devine and Kamesh Madduri and Sivasankaran Rajamanickam},
  doi          = {10.1109/TPDS.2020.3002150},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2789-2801},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scalable, multi-constraint, complex-objective graph partitioning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interval job scheduling with machine launch cost.
<em>TPDS</em>, <em>31</em>(12), 2776–2788. (<a
href="https://doi.org/10.1109/TPDS.2020.3002786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an interval job scheduling problem in distributed systems. We are given a set of interval jobs, with each job specified by a size, an arrival time and a processing length. Once a job arrives, it must be placed on a machine immediately and run for a period of its processing length without interruption. The homogeneous machines to run jobs have the same capacity limits such that at anytime, the total size of the jobs running on any machine cannot exceed its capacity. Launching each machine incurs a fixed cost. After launch, a machine is charged a constant cost per time unit until it is terminated. The problem targets to minimize the total cost incurred by the machines for processing the given set of interval jobs. We focus on the algorithmic aspects of the problem in this article. For the special case where all the jobs have a unit size equal to the machine capacity, we propose an optimal offline algorithm and an optimal 2-competitive online algorithm. For the general case where jobs can have arbitrary sizes, we establish a non-trivial lower bound on the optimal solution. Based on this lower bound, we propose a 5-approximation algorithm in the offline setting. In the non-clairvoyant online setting, we design a O(μ)-competitive Modified First-Fit algorithm which is near optimal (μ is the max/min job processing length ratio). In the clairvoyant online setting, we propose an asymptotically optimal O(√log μ)-competitive algorithm based on our Modified First-Fit strategy.},
  archive      = {J_TPDS},
  author       = {Runtian Ren and Yuqing Zhu and Chuanyou Li and Xueyan Tang},
  doi          = {10.1109/TPDS.2020.3002786},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2776-2788},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Interval job scheduling with machine launch cost},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cartesian partitioning models for 2D and 3D parallel SpGEMM
algorithms. <em>TPDS</em>, <em>31</em>(12), 2763–2775. (<a
href="https://doi.org/10.1109/TPDS.2020.3000708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus is distributed-memory parallelization of sparse-general-matrix-multiplication (SpGEMM). Parallel SpGEMM algorithms are classified under one-dimensional (1D), 2D, and 3D categories denoting the number of dimensions by which the 3D sparse workcube representing the iteration space of SpGEMM is partitioned. Recently proposed successful 2D- and 3D-parallel SpGEMM algorithms benefit from upper bounds on communication overheads enforced by 2D and 3D cartesian partitioning of the workcube on 2D and 3D virtual processor grids, respectively. However, these methods are based on random cartesian partitioning and do not utilize sparsity patterns of SpGEMM instances for reducing the communication overheads. We propose hypergraph models for 2D and 3D cartesian partitioning of the workcube for further reducing the communication overheads of these 2D- and 3D- parallel SpGEMM algorithms. The proposed models utilize two- and three-phase partitioning that exploit multi-constraint hypergraph partitioning formulations. Extensive experimentation performed on 20 SpGEMM instances by using upto 900 processors demonstrate that proposed partitioning models significantly improve the scalability of 2D and 3D algorithms. For example, in 2D-parallel SpGEMM algorithm on 900 processors, the proposed partitioning model respectively achieves 85 and 42 percent decrease in total volume and total number of messages, leading to 1.63 times higher speedup compared to random partitioning, on average.},
  archive      = {J_TPDS},
  author       = {Gunduz Vehbi Demirci and Cevdet Aykanat},
  doi          = {10.1109/TPDS.2020.3000708},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2763-2775},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cartesian partitioning models for 2D and 3D parallel SpGEMM algorithms},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preemptive and low latency datacenter scheduling via
lightweight containers. <em>TPDS</em>, <em>31</em>(12), 2749–2762. (<a
href="https://doi.org/10.1109/TPDS.2019.2957754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datacenters are evolving to host heterogeneous workloads on shared clusters to reduce the operational cost and achieve higher resource utilization. However, it is challenging to schedule heterogeneous workloads with diverse resource requirements and QoS constraints. On one hand, latency-critical jobs need to be scheduled as soon as they are submitted to avoid any queuing delays. On the other hand, best-effort long jobs should be allowed to occupy the cluster when there are idle resources to improve cluster utilization. The challenge lies in how to minimize the queuing delays of short jobs while maximizing cluster utilization. In this article, we propose and develop BIG-C, a container-based resource management framework for data-intensive cluster computing. The key design is to leverage lightweight virtualization, a.k.a, containers, to make tasks preemptable in cluster scheduling. We devise two types of preemption strategies: immediate and graceful preemptions and show their effectiveness and tradeoffs with loosely-coupled MapReduce workloads as well as iterative, in-memory Spark workloads. Based on the mechanisms for task preemption, we further develop job-level and task-level preemptive policies as well as a preemptive fair share cluster scheduler. Our implementation on Yarn and evaluation with synthetic and production workloads show that low job latency and high resource utilization can be both attained when scheduling heterogeneous workloads on a contended cluster.},
  archive      = {J_TPDS},
  author       = {Wei Chen and Xiaobo Zhou and Jia Rao},
  doi          = {10.1109/TPDS.2019.2957754},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2749-2762},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Preemptive and low latency datacenter scheduling via lightweight containers},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Memory-efficient and skew-tolerant MapReduce over MPI for
supercomputing systems. <em>TPDS</em>, <em>31</em>(12), 2734–2748. (<a
href="https://doi.org/10.1109/TPDS.2019.2932066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data analytics has become an integral part of large-scale scientific computing. Among various data analytics frameworks, MapReduce has gained the most traction. Although some efforts have been made to enable efficient MapReduce for supercomputing systems, they are often limited to fairly homogeneous workloads where equal partitioning of input data across tasks results in essentially equal output or temporary data generated on each task. For workloads that are more skewed, however, current implementations can result in imbalance in memory usage and, consequently, can cause a slowdown in execution time and a loss in data scalability. To tackle this problem, we enhance a previously published memory-conscious MapReduce over MPI framework called Mimir. Our enhancements to Mimir include combiner and dynamic repartition optimizations to minimize and balance memory usage and to achieve close to optimal balance of the memory usage across processes and to reduce the execution time by up to 12 times. Experimental results show that Mimir can scale to at least 3072 processes on the Tianhe-2 supercomputer on skewed datasets.},
  archive      = {J_TPDS},
  author       = {Tao Gao and Yanfei Guo and Boyu Zhang and Pietro Cicotti and Yutong Lu and Pavan Balaji and Michela Taufer},
  doi          = {10.1109/TPDS.2019.2932066},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2734-2748},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Memory-efficient and skew-tolerant MapReduce over MPI for supercomputing systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). System error prediction for business support systems in
telecommunications networks. <em>TPDS</em>, <em>31</em>(11), 2723–2733.
(<a href="https://doi.org/10.1109/TPDS.2020.3001593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability and stability have been treated as the major requirements for the Business Support System (BSS) in telecommunications networks. It is crucial and essential for service providers to maintain good operating state of the BSS. In this article, we aim at system error prediction for a BSS, i.e., we predict occurrences of the abnormal state or behavior of the BSS. Because the occurrences of system errors are rare events in the BSS (i.e., the dataset of system status is highly imbalanced), it is highly challenging to use machine learning or deep learning algorithms to predict system error for the BSS. To address this challenge, we propose a machine learning-based framework for the system error prediction and a Frequency-based Feature Creation (FFC) algorithm to create new features to improve prediction. By adding the time-series information created by the existing features, the proposed FFC can amplify the effects of important features. Our experimental results show that the FFC significantly improves the prediction performance for the Random Forest algorithm.},
  archive      = {J_TPDS},
  author       = {En-Hau Yeh and Phone Lin and Xin-Xue Lin and Jeu-Yih Jeng and Yuguang Fang},
  doi          = {10.1109/TPDS.2020.3001593},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2723-2733},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {System error prediction for business support systems in telecommunications networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-quality shared-memory graph partitioning.
<em>TPDS</em>, <em>31</em>(11), 2710–2722. (<a
href="https://doi.org/10.1109/TPDS.2020.3001645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitioning graphs into blocks of roughly equal size such that few edges run between blocks is a frequently needed operation in processing graphs. Recently, size, variety, and structural complexity of these networks has grown dramatically. Unfortunately, previous approaches to parallel graph partitioning have problems in this context since they often show a negative trade-off between speed and quality. We present an approach to multi-level shared-memory parallel graph partitioning that produces balanced solutions, shows high speedups for a variety of large graphs and yields very good quality independently of the number of cores used. For example, in an extensive experimental study, at 79 cores, one of our closest competitors is faster but fails to meet the balance criterion in the majority of cases and another is mostly slower and incurs about 13 percent larger cut size. Important ingredients include parallel label propagation for both coarsening and refinement, parallel initial partitioning, a simple yet effective approach to parallel localized local search, and fast locality preserving hash tables.},
  archive      = {J_TPDS},
  author       = {Yaroslav Akhremtsev and Peter Sanders and Christian Schulz},
  doi          = {10.1109/TPDS.2020.3001645},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2710-2722},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High-quality shared-memory graph partitioning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Countdown slack: A run-time library to reduce energy
footprint in large-scale MPI applications. <em>TPDS</em>,
<em>31</em>(11), 2696–2709. (<a
href="https://doi.org/10.1109/TPDS.2020.3000418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power consumption of supercomputers is a major challenge for system owners, users, and society. It limits the capacity of system installations, it requires large cooling infrastructures, and it is the cause of a large carbon footprint. Reducing power during application execution without changing the application source code or increasing time-to-completion is highly desirable in real-life high-performance computing scenarios. The power management run-time frameworks proposed in the last decade are based on the assumption that the duration of communication and application phases in an MPI application can be predicted and used at run-time to trade-off communication slack with power consumption. In this article, we first show that this assumption is too general and leads to mispredictions, slowing down applications, thereby jeopardizing the claimed benefits. We then propose a new approach based on (i) the separation of communication phases and slack during MPI calls and (ii) a timeout algorithm to cope with the hardware power management latency, which jointly makes it possible to achieve performance-neutral power saving in MPI applications without requiring labor-intensive and risky application source code modifications. We validate our approach in a tier-1 production environment with widely adopted scientific applications. Our approach has a time-to-completion overhead lower than 1 percent, while it successfully exploits slack in communication phases to achieve an average energy saving of 10 percent. If we focus on a large-scale application runs, the proposed approach achieves 22 percent energy saving with an overhead of only 0.4 percent. With respect to state-of-the-art approaches, COUNTDOWN Slack is the only that always leads to an energy saving with negligible overhead (&lt;; 3 percent).},
  archive      = {J_TPDS},
  author       = {Daniele Cesarini and Andrea Bartolini and Andrea Borghesi and Carlo Cavazzoni and Mathieu Luisier and Luca Benini},
  doi          = {10.1109/TPDS.2020.3000418},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2696-2709},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Countdown slack: A run-time library to reduce energy footprint in large-scale MPI applications},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving MPI collective i/o for high volume non-contiguous
requests with intra-node aggregation. <em>TPDS</em>, <em>31</em>(11),
2682–2695. (<a href="https://doi.org/10.1109/TPDS.2020.3000458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase I/O is a well-known strategy for implementing collective MPI-IO functions. It redistributes I/O requests among the calling processes into a form that minimizes the file access costs. As modern parallel computers continue to grow into the exascale era, the communication cost of such request redistribution can quickly overwhelm collective I/O performance. This effect has been observed from parallel jobs that run on multiple compute nodes with a high count of MPI processes on each node. To reduce the communication cost, we present a new design for collective I/O by adding an extra communication layer that performs request aggregation among processes within the same compute nodes. This approach can significantly reduce inter-node communication contention when redistributing the I/O requests. We evaluate the performance and compare it with the original two-phase I/O on Cray XC40 parallel computers (Theta and Cori) with Intel KNL and Haswell processors. Using I/O patterns from two large-scale production applications and an I/O benchmark, we show our proposed method effectively reduces the communication cost and hence maintains the scalability for a large number of processes.},
  archive      = {J_TPDS},
  author       = {Qiao Kang and Sunwoo Lee and Kaiyuan Hou and Robert Ross and Ankit Agrawal and Alok Choudhary and Wei-keng Liao},
  doi          = {10.1109/TPDS.2020.3000458},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2682-2695},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving MPI collective I/O for high volume non-contiguous requests with intra-node aggregation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully homomorphic based privacy-preserving distributed
expectation maximization on cloud. <em>TPDS</em>, <em>31</em>(11),
2668–2681. (<a href="https://doi.org/10.1109/TPDS.2020.2999407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expectation maximization (EM) is a clustering-based machine learning algorithm that is widely used in many areas of science (e.g., bioinformatics and computer vision) to find maximum likelihood and maximum a posteriori estimates for models with latent variables. To deploy such an algorithm in cloud environments, security and privacy issues need be considered to avoid data breaches or abuses by external malicious parties or even by cloud service providers. However, the processing performance of the EM algorithm poses a challenge in terms of building a secure environment. This article describes an innovative and practical privacy-preserving EM algorithm for cloud systems that addresses this challenge, and estimates the EM parameters in an accurate and secure manner. Fully homomorphic encryption (FHE) is used to ensure the privacy of both the EM algorithm computations and the users&#39; sensitive data in the cloud. A distributed-based approach is also proposed to overcome the overheads of FHE computations and ensure a fast convergence of the EM algorithm. The conducted experiments demonstrate a significant improvement in the convergence time of the distributed EM algorithm, while achieving a high level of accuracy and reducing the associated computational FHE overheads.},
  archive      = {J_TPDS},
  author       = {Abdulatif Alabdulatif and Ibrahim Khalil and Albert Y. Zomaya and Zahir Tari and Xun Yi},
  doi          = {10.1109/TPDS.2020.2999407},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2668-2681},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fully homomorphic based privacy-preserving distributed expectation maximization on cloud},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative memory expansion via OS kernel support for
networked computing systems. <em>TPDS</em>, <em>31</em>(11), 2650–2667.
(<a href="https://doi.org/10.1109/TPDS.2020.2999507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of in-memory computing for bigdata analytics often causes performance bottlenecks to memory subsystem resided in operating systems (OS). This article purposes cooperative memory expansion (COMEX), an OS kernel extension. COMEX establishes a stable pool of memory collectively across nodes in a cluster and enhances OS&#39;s memory subsystem for memory aggregation from connected machines by allowing process&#39;s page table to track remote memory page frames without programmer effort or modifications to application codes. COMEX employs Remote Direct Memory Access (RDMA) for low-latency data transfer with destination kernel bypassed and does not rely on an old design of the I/O block subsystem usually adopted by all known remote paging. COMEX fits soundly in the emerging system design approach of resource disaggregation which breaks hard walls between server-centric machines into a new design paradigm of separated resource pools. The new architecture facilitates both system scaling-up and scaling-out, also eliminates imbalance resources existing in datacenters. We have implemented COMEX based on Linux kernel 3.10.87 and deployed on our 32 networked servers. Performance evaluation results under ten applications from two benchmark suites reveal the speedup of up to 170 times when application execution footprints are 10 times larger than available system memory.},
  archive      = {J_TPDS},
  author       = {Pisacha Srinuan and Xu Yuan and Nian-Feng Tzeng},
  doi          = {10.1109/TPDS.2020.2999507},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2650-2667},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cooperative memory expansion via OS kernel support for networked computing systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QWEB: High-performance event-driven web architecture with
QAT acceleration. <em>TPDS</em>, <em>31</em>(11), 2633–2649. (<a
href="https://doi.org/10.1109/TPDS.2020.2999353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware accelerators have been a promising solution to reduce the cost of cloud datacenters. This article investigates the acceleration of an important datacenter workload: the web server (or proxy) that faces high computational consumption originated from SSL/TLS processing and HTTP compression. Our study reveals that for the widely-deployed event-driven web architecture, the straight offloading of SSL/TLS or compression tasks suffers from frequent blockings in the offload I/O, leading to the underutilization of both CPU and accelerator resources. To achieve efficient acceleration, we propose QWEB, a comprehensive offload solution based on Intel QuickAssist Technology (QAT). QWEB introduces an asynchronous offload mode for SSL/TLS processing and a pipelining offload mode for HTTP compression, both allowing concurrent offload tasks from a single application process/thread. With these two novel offload modes, the blocking penalty is amortized or even eliminated, and the utilization rate of the parallel computation engines inside the QAT accelerator is greatly increased. The evaluation shows that QWEB provides up to 9x handshake performance with TLS-RSA (2048-bit) over the software baseline. Additionally, the secure data transfer throughput is enhanced by 2x for the SSL/TLS offloading only, 3.5x for the compression offloading only and 5x for the combined offloading.},
  archive      = {J_TPDS},
  author       = {Jian Li and Xiaokang Hu and David Qian and Changzheng Wei and Gordon McFadden and Brian Will and Ping Yu and Weigang Li and Haibing Guan},
  doi          = {10.1109/TPDS.2020.2999353},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2633-2649},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {QWEB: High-performance event-driven web architecture with QAT acceleration},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time-optimal leader election in population protocols.
<em>TPDS</em>, <em>31</em>(11), 2620–2632. (<a
href="https://doi.org/10.1109/TPDS.2020.2991771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present the first leader election protocol in the population protocol model that stabilizes within O(logn) parallel time in expectation with O(logn) states per agent, where n is the number of agents. Given a rough knowledge m of lg n such that m ≥ lg n and m = O(logn), the proposed protocol guarantees that exactly one leader is elected and the unique leader is kept forever thereafter. This protocol is time-optimal because it was recently proven that any leader election protocol requires Ω(logn) parallel time.},
  archive      = {J_TPDS},
  author       = {Yuichi Sudo and Fukuhito Ooshita and Taisuke Izumi and Hirotsugu Kakugawa and Toshimitsu Masuzawa},
  doi          = {10.1109/TPDS.2020.2991771},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2620-2632},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Time-optimal leader election in population protocols},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comment on “a tag encoding scheme against pollution attack
to linear network coding.” <em>TPDS</em>, <em>31</em>(11), 2618–2619.
(<a href="https://doi.org/10.1109/TPDS.2020.2999523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2014, Wu et al. proposed a tag encoding scheme, named KEPTE, to protect network coding against pollution attack. They also carefully analyzed the security of KEPTE based on the transmission of a data file through their key-pre-distributed network. In this article, we point out that their security analysis only holds for single data file transmitted in this network. If multiple files are multicasted though it, then any adversary may completely recover source node&#39;s signing key. A concrete example says that, after pre-distributing 90 keys to all the nodes in the network, it only allows to securely transmit (at most) 3 data files. More importantly, this scheme is completely insecure in standard security model for network model since the adversary is allowed to make polynomial times queries on any data files of its choice before outputting its final forgery. Finally, we also propose a twisted KEPTE scheme that is secure against any eavesdropping adversary no matter how many data files it has queried.},
  archive      = {J_TPDS},
  author       = {Jinyong Chang and Bilin Shao and Yanyan Ji and Genqing Bian},
  doi          = {10.1109/TPDS.2020.2999523},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2618-2619},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Comment on “A tag encoding scheme against pollution attack to linear network coding”},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards usable cloud storage auditing. <em>TPDS</em>,
<em>31</em>(11), 2605–2617. (<a
href="https://doi.org/10.1109/TPDS.2020.2998462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage security has gained considerable research efforts with the wide adoption of cloud computing. As a security mechanism, researchers have been investigating cloud storage auditing schemes that enable a user to verify whether the cloud keeps the user&#39;s outsourced data undamaged. However, existing schemes have usability issues in compatibility with existing real world cloud storage applications, error-tolerance, and efficiency. To mitigate this usability gap, this article proposes a new general cloud storage auditing scheme that is more usable. The proposed scheme uses the idea of integrating linear error correcting codes and linear homomorphic authentication schemes together. This integration uses only one additional block to achieve error tolerance and authentication simultaneously. To demonstrate the power of the general construction, we also propose one detailed scheme based on the proposed general construction using the Reed Solomon code and the universal hash based MAC authentication scheme, both of which are implemented over the computation-efficient Galois field $\mathrm {GF}{(2^8)}$ . We also show that the proposed scheme is secure under the standard definition. Moreover, we implemented and open-sourced the proposed scheme. Experimental results show that the proposed scheme is orders of magnitude more efficient than the state-of-the-art scheme.},
  archive      = {J_TPDS},
  author       = {Fei Chen and Fengming Meng and Tao Xiang and Hua Dai and Jianqiang Li and Jing Qin},
  doi          = {10.1109/TPDS.2020.2998462},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2605-2617},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards usable cloud storage auditing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized cost-based job scheduling in very large
heterogeneous cluster systems. <em>TPDS</em>, <em>31</em>(11),
2594–2604. (<a href="https://doi.org/10.1109/TPDS.2020.2997771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study job assignment in large, heterogeneous resource-sharing clusters of servers with finite buffers. This load balancing problem arises naturally in today&#39;s communication and big data systems, such as Amazon Web Services, Network Service Function Chains, and Stream Processing. Arriving jobs are dispatched to a server, following a load balancing policy that optimizes a performance criterion such as job completion time. Our contribution is a randomized Cost-Based Scheduling (CBS) policy in which the job assignment is driven by general cost functions of the server queue lengths. Beyond existing schemes, such as the Join the Shortest Queue (JSQ), the power of d or the SQ(d) and the capacity-weighted JSQ, the notion of CBS yields new application-specific policies such as hybrid locally uniform JSQ. As today&#39;s data center clusters have thousands of servers, exact analysis of CBS policies is tedious. In this article, we derive a scaling limit when the number of servers grows large, facilitating a comparison of various CBS policies with respect to their transient as well as steady state behavior. A byproduct of our derivations is the relationship between the queue filling proportions and the server buffer sizes, which cannot be obtained from infinite buffer models. Finally, we provide extensive numerical evaluations and discuss several applications including multi-stage systems.},
  archive      = {J_TPDS},
  author       = {Wasiur R. KhudaBukhsh and Sounak Kar and Bastian Alt and Amr Rizk and Heinz Koeppl},
  doi          = {10.1109/TPDS.2020.2997771},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2594-2604},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Generalized cost-based job scheduling in very large heterogeneous cluster systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correlation of performance optimizations and energy
consumption for stencil-based application on intel xeon scalable
processors. <em>TPDS</em>, <em>31</em>(11), 2582–2593. (<a
href="https://doi.org/10.1109/TPDS.2020.2996314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a comprehensive study of the impact of performance optimizations on the energy efficiency of a real-world CFD application called MPDATA, as well as an insightful analysis of performance-energy interaction of these optimizations with the underlying hardware that represents the first generation of Intel Xeon Scalable processors. Considering the MPDATA iterative application as a use case, we explore the fundamentals of energy and performance analysis for a memory-bound application when exposed to a set of optimization steps that increase the application performance, by improving the operational intensity of code and utilizing resources more efficiently. It is shown that for memory-bound applications, optimizing toward high performance could be a powerful strategy for improving the energy efficiency as well. In fact, for the considered performance optimizations, the energy gain is correlated with the performance gain but with varying degrees. As a result, these optimizations allow improving both performance and energy consumption radically, up to about 10.9 and 8.8 times, respectively. The impact of the Intel AVX-512 SIMD extension on the energy consumption and performance is demonstrated. Also, we discover limitations on the usability of CPU frequency scaling as a tool for balancing energy savings with admissible performance losses.},
  archive      = {J_TPDS},
  author       = {Lukasz Szustak and Roman Wyrzykowski and Tomasz Olas and Valeria Mele},
  doi          = {10.1109/TPDS.2020.2996314},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2582-2593},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Correlation of performance optimizations and energy consumption for stencil-based application on intel xeon scalable processors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MEMPHA: Model of exascale message-passing programs on
heterogeneous architectures. <em>TPDS</em>, <em>31</em>(11), 2570–2581.
(<a href="https://doi.org/10.1109/TPDS.2020.2995867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delivering optimum performance on a parallel computer is highly dependant on the efficiency of the scheduling and mapping procedure. If the composition of the parallel application is known a prior, the mapping can be accomplished statically on the compilation time. The mapping algorithm uses the model of the parallel application and maps its tasks to processors in a way to minimize the total execution time. In this article, current modeling approaches have discussed. Later, a new modeling schema named Model of Exascale Message-Passing Programs on Heterogeneous Architectures (MEMPHA) has proposed. A comparative study has been performed between MEMPHA and existing models. To exhibit the efficiency of the MEMPHA, experiments have performed on a set of data-set hypergraphs. The results obtained from the experiments show that deploying the MEMPHA helps to optimize metrics, including the congestion, total communication volume and maximum volume of data being sent or received. These improvements vary from 76 to 1 percent, depending on the metric and benchmark model. Moreover, MEMPHA supports the modeling of applications with multiple producers for a single data transmission, where the rest of the approaches fail.},
  archive      = {J_TPDS},
  author       = {Sina Zangbari Koohi and Nor Asilah Wati Abdul Hamid and Mohamed Othman and Gafurjan Ibragimov},
  doi          = {10.1109/TPDS.2020.2995867},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2570-2581},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MEMPHA: Model of exascale message-passing programs on heterogeneous architectures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Errata to “on-edge multi-task transfer learning: Model and
practice with data-driven task allocation.” <em>TPDS</em>,
<em>31</em>(11), 2569. (<a
href="https://doi.org/10.1109/TPDS.2020.2997321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents corrections to author information for the above named paper.},
  archive      = {J_TPDS},
  author       = {Qiong Chen and Zimu Zheng and Chuang Hu and Dan Wang and Fangming Liu},
  doi          = {10.1109/TPDS.2020.2997321},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2569},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Errata to “On-edge multi-task transfer learning: Model and practice with data-driven task allocation”},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Phase-aware cache partitioning to target both turnaround
time and system performance. <em>TPDS</em>, <em>31</em>(11), 2556–2568.
(<a href="https://doi.org/10.1109/TPDS.2020.2996031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Last Level Cache (LLC) plays a key role in the system performance of current multi-cores by reducing the number of long latency main memory accesses. The inter-application interference at this shared resource, however, can lead the system to undesired situations regarding performance and fairness. Recent approaches have successfully addressed fairness and turnaround time (TT) in commercial processors. Nevertheless, these approaches must face sustaining system performance, which is challenging. This work makes two main contributions. LLC behaviors regarding cache performance, data reuse and cache occupancy, that adversely impact on the final performance are identified. Second, based on these behaviors, we propose the Critical-Phase Aware Partitioning Approach (CPA), which reduces TT while sustaining (and even improving) IPC by making an effective use of the LLC space. Experimental results show that CPA outperforms CA, Dunn and KPart state-of-the-art approaches, and improves TT (over 40 percent in some workloads) over Linux default behavior while sustaining or even improving IPC by more than 3 percent in several mixes.},
  archive      = {J_TPDS},
  author       = {Lucia Pons and Julio Sahuquillo and Vicent Selfa and Salvador Petit and Julio Pons},
  doi          = {10.1109/TPDS.2020.2996031},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2556-2568},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Phase-aware cache partitioning to target both turnaround time and system performance},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient parallelism of post-quantum signature scheme
SPHINCS. <em>TPDS</em>, <em>31</em>(11), 2542–2555. (<a
href="https://doi.org/10.1109/TPDS.2020.2995562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SPHINCS was recently proposed as a stateless, quantum-resilient hash-based signature scheme. However, one possible limitation of SPHINCS is its signing speed, namely, the best known implementation merely produces a few hundred of signatures per second, which is not good enough, e.g., for a social website with a huge amount of users. Aiming at improving the singing throughput, we present highly parallel and optimized implementations of SPHINCS, which can be deployed on various multi-core platforms. As a first step, we give an elementary implementation on ×86/64 processors, which proves the effectiveness and correctness of our implementations. To obtain a significantly higherthroughput, we implement SPHINCS on Graphics Processing Units (GPUs). Furthermore, we develop a few general and hardware-specific techniques to take full advantage of the computing power of targeted platforms. We instantiate the underlying hash functions with three primitives. Our comprehensive benchmark shows that our work outperforms all the state-of-the-art implementations of SPHINCS regarding throughput with reasonable latency, and has scalability on multiple cores and multiple GPU cards. For instance, forthe key generation algorithm instantiated with ChaCha running on a GeForce GTX 1080, we obtain 5152 signatures per second which is 7.88x speedup fasterthan a recent FPGA implementation. When upgrade to TITAN Xp, 6,651 signatures are generated in one second. With four TITAN Xp GPUs, the obtained throughput satisfies vast majority scenarios.},
  archive      = {J_TPDS},
  author       = {Shuzhou Sun and Rui Zhang and Hui Ma},
  doi          = {10.1109/TPDS.2020.2995562},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2542-2555},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient parallelism of post-quantum signature scheme SPHINCS},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards fair and privacy-preserving federated deep models.
<em>TPDS</em>, <em>31</em>(11), 2524–2541. (<a
href="https://doi.org/10.1109/TPDS.2020.2996273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current standalone deep learning framework tends to result in overfitting and low utility. This problem can be addressed by either a centralized framework that deploys a central server to train a global model on the joint data from all parties, or a distributed framework that leverages a parameter server to aggregate local model updates. Server-based solutions are prone to the problem of a single-point-of-failure. In this respect, collaborative learning frameworks, such as federated learning (FL), are more robust. Existing federated learning frameworks overlook an important aspect of participation: fairness. All parties are given the same final model without regard to their contributions. To address these issues, we propose a decentralized Fair and Privacy-Preserving Deep Learning (FPPDL) framework to incorporate fairness into federated deep learning models. In particular, we design a local credibility mutual evaluation mechanism to guarantee fairness, and a three-layer onion-style encryption scheme to guarantee both accuracy and privacy. Different from existing FL paradigm, under FPPDL, each participant receives a different version of the FL model with performance commensurate with his contributions. Experiments on benchmark datasets demonstrate that FPPDL balances fairness, privacy and accuracy. It enables federated learning ecosystems to detect and isolate low-contribution parties, thereby promoting responsible participation.},
  archive      = {J_TPDS},
  author       = {Lingjuan Lyu and Jiangshan Yu and Karthik Nandakumar and Yitong Li and Xingjun Ma and Jiong Jin and Han Yu and Kee Siong Ng},
  doi          = {10.1109/TPDS.2020.2996273},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2524-2541},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards fair and privacy-preserving federated deep models},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High performance simulation of spiking neural network on
GPGPUs. <em>TPDS</em>, <em>31</em>(11), 2510–2523. (<a
href="https://doi.org/10.1109/TPDS.2020.2994123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural network (SNN) is the most commonly used computational model for neuroscience and neuromorphic computing communities. It provides more biological reality and possesses the potential to achieve high computational power and energy efficiency. Because existing SNN simulation frameworks on general-purpose graphics processing units (GPGPUs) do not fully consider the biological oriented properties of SNNs, like spike-driven, activity sparsity, etc., they suffer from insufficient parallelism exploration, irregular memory access, and load imbalance. In this article, we propose specific optimization methods to speed up the SNN simulation on GPGPU. First, we propose a fine-grained network representation as a flexible and compact intermediate representation (IR) for SNNs. Second, we propose the cross-population/-projection parallelism exploration to make full use of GPGPU resources. Third, sparsity aware load balance is proposed to deal with the activity sparsity. Finally, we further provide dedicated optimization to support multiple GPGPUs. Accordingly, BSim, a code generation framework for high-performance simulation of SNN on GPGPUs is also proposed. Tests show that, compared to a state-of-the-art GPU-based SNN simulator GeNN, BSim achieves 1.41× - 9.33× speedup for SNNs with different configurations; it outperforms other simulators much more.},
  archive      = {J_TPDS},
  author       = {Peng Qu and Youhui Zhang and Xiang Fei and Weimin Zheng},
  doi          = {10.1109/TPDS.2020.2994123},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2510-2523},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High performance simulation of spiking neural network on GPGPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient SSD cache for cloud block storage via leveraging
block reuse distances. <em>TPDS</em>, <em>31</em>(11), 2496–2509. (<a
href="https://doi.org/10.1109/TPDS.2020.2994075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solid State Drives (SSDs) are popularly used for caching in large scale cloud storage systems nowadays. Traditionally, most cache algorithms make replacement upon each miss when cache space is full. However, we observe that in a typical Cloud Block Storage (CBS) system, there is a great percentage of blocks with large reuse distances, which would result in large number of blocks being evicted out of the cache before they ever have a chance to be referenced while they are cached, significantly jeopardizing the cache efficiency. In this article, we propose LEA, Lazy Eviction cache Algorithm, for cloud block storage to efficiently remedy the cache inefficiencies caused by cache blocks with large reuse distances. LEA mainly employs two lists, Lazy Eviction List (LEL) and Block Identity List (BIL), which keep track of two types of victim blocks respectively based on their cache duration when replacements occur, to improve cache efficiency. When a cache miss happens, if the victim block has not resided in cache for longer than its reuse distance, LEA inserts the missed block identity into BIL. Otherwise, it inserts the missed block entry into LEL. We have evaluated LEA by using IO traces collected from Tencent, one of the largest network service providers in the world, and several open source traces. Experimental results show that LEA not only outperforms most of the state-of-the-art cache algorithms in hit ratio, but also greatly reduces the number of SSD writes.},
  archive      = {J_TPDS},
  author       = {Ke Zhou and Yu Zhang and Ping Huang and Hua Wang and Yongguang Ji and Bin Cheng and Ying Liu},
  doi          = {10.1109/TPDS.2020.2994075},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2496-2509},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient SSD cache for cloud block storage via leveraging block reuse distances},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Abstraction layer for standardizing APIs of task-based
engines. <em>TPDS</em>, <em>31</em>(11), 2482–2495. (<a
href="https://doi.org/10.1109/TPDS.2020.2992923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce AL4SAN, a lightweight library for abstracting the APIs of task-based runtime engines. AL4SAN unifies the expression of tasks and their data dependencies. It supports various dynamic runtime systems relying on compiler technology and user-defined APIs. It enables a single application to employ different runtimes and their respective scheduling components, while providing user-obliviousness to the underlying hardware configurations. AL4SAN exposes common front-end APIs and connects to different back-end runtimes. Experiments on performance and overhead assessments are reported on various shared- and distributed-memory systems, possibly equipped with hardware accelerators. A range of workloads, from compute-bound to memory-bound regimes, are employed as proxies for current scientific applications. The low overhead (less than 10 percent) achieved using a variety of workloads enables AL4SAN to be deployed for fast development of task-based numerical algorithms. More interestingly, AL4SAN enables runtime interoperability by switching runtimes at runtime. Blending runtime systems permits to achieve a twofold speedup on a task-based generalized symmetric eigenvalue solver, relative to state-of-the-art implementations. The ultimate goal of AL4SAN is not to create a new runtime, but to strengthen co-design of existing runtimes/applications, while facilitating user productivity and code portability. The code of AL4SAN is freely available at https://github.com/ecrc/al4san, with extensions in progress.},
  archive      = {J_TPDS},
  author       = {Rabab Alomairy and Hatem Ltaief and Mustafa Abduljabbar and David Keyes},
  doi          = {10.1109/TPDS.2020.2992923},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2482-2495},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Abstraction layer for standardizing APIs of task-based engines},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Endpoint-flexible coflow scheduling across geo-distributed
datacenters. <em>TPDS</em>, <em>31</em>(10), 2466–2481. (<a
href="https://doi.org/10.1109/TPDS.2020.2992615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, we have witnessed growing data volumes generated and stored across geographically distributed datacenters. Processing such geo-distributed datasets may suffer from significant slowdown as the underlying network flows have to go through the inter-datacenter networks with relatively low and highly heterogeneous available link bandwidth. Thus, optimizing the transmissions of inter-datacenter flows, especially coflows that capture application-level semantics, is important for improving the communication performance of such geo-distributed applications. However, prior solutions on coflow scheduling have significant limitations: they schedule coflows with already-fixed endpoints of flows, making them insufficient to optimize the coflow completion time (CCT). In this article, we focus on the problem of jointly considering endpoint placement and coflow scheduling to minimize the average CCT of coflows across geo-distributed datacenters. To solve this problem without any prior knowledge of coflow arrivals, we present a coflow-aware optimization framework called SmartCoflow. In SmartCoflow, we first apply an approximate algorithm to obtain the endpoint placement and scheduling decisions for a single coflow. Based on the single-coflow solution, we then develop an efficient online algorithm to handle the dynamically arrived coflows. Through rigorous theoretical analysis, we prove that SmartCoflow has a non-trivial competitive ratio. We also extend SmartCoflow to incorporate various design choices or requirements of applications and operators, such as enforcing an inter-datacenter bandwidth usage budget and considering coflow deadline. Through experimental results from testbed implementation and trace-driven simulations, we demonstrate that SmartCoflow can reduce the average CCT, lower bandwidth usage, and improve coflow deadline meet rate, when compared to the state-of-the-art scheduling-only method.},
  archive      = {J_TPDS},
  author       = {Wenxin Li and Xu Yuan and Keqiu Li and Heng Qi and Xiaobo Zhou and Renhai Xu},
  doi          = {10.1109/TPDS.2020.2992615},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2466-2481},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Endpoint-flexible coflow scheduling across geo-distributed datacenters},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconciling time slice conflicts of virtual machines with
dual time slice for clouds. <em>TPDS</em>, <em>31</em>(10), 2453–2465.
(<a href="https://doi.org/10.1109/TPDS.2020.2993252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of system virtualization poses a new challenge for the coarse-grained time sharing techniques for consolidation, since operating systems are running on virtual CPUs. The current system stack was designed under the assumption that operating systems can seize CPU resources at any moment. However, for the guest operating system on a virtual machine (VM), such assumption cannot be guaranteed, since virtual CPUs of VMs share a limited number of physical cores. Due to the time-sharing of physical cores, the execution of a virtual CPU is not contiguous, with a gap between the virtual and real time spaces. Such a virtual time discontinuity problem leads to significant inefficiency for lock and interrupt handling, which rely on the immediate availability of CPUs whenever the operating system requires computation. To reduce scheduling latencies of virtual CPUs, shortening time slices can be a straightforward strategy, but it may lead to the increased overhead of context switching costs across virtual machines for some workloads. It is challenging to determine a single time slice to satisfy all the VMs. In this article, we propose to have dual time slice to resolve the time slice conflict problem occurred in different types of virtual machines.},
  archive      = {J_TPDS},
  author       = {Taeklim Kim and Chang Hyun Park and Jaehyuk Huh and Jeongseob Ahn},
  doi          = {10.1109/TPDS.2020.2993252},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2453-2465},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reconciling time slice conflicts of virtual machines with dual time slice for clouds},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven derivation of an analytic model for parallel
servers with job replication. <em>TPDS</em>, <em>31</em>(10), 2435–2452.
(<a href="https://doi.org/10.1109/TPDS.2020.2992571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The job replication problem has been studied recently as a mechanism to improve performance and availability of systems with n parallel servers, each with its own queue. A dispatcher using some policy sends d (1 ≤ d ≤ n) copies of a job to d of the servers. Copies are eliminated from the system as soon as the first copy completes from any of the d servers. This article introduces a datadriven method to derive closed-form expressions for the average response time and other metrics of jobs as a function of the degree of replication d. This method consists of developing a simulator for the system in order to generate a very large number of datasets for a wide range of input parameters. A statistical and visualization analysis of the data provides the analytical models. It is important to emphasize the difference between using simulation methods to obtain the value of metrics (e.g., average response time) of a computer system given values of input parameters and using our data-driven method to obtain closed-form expressions that relate output metrics to input parameters. The latter is the focus of our approach. The analysis presented here covers results for homogeneous and heterogeneous servers with exponentially distributed service times and for homogeneous servers with hypo-exponentially and hyper-exponentially distributed service times. This article also presents a closed-form equation for the optimal replication degree for the case of homogeneous servers with hypo-exponentially distributed service times.},
  archive      = {J_TPDS},
  author       = {Noor Bajunaid and Daniel A. Menascé},
  doi          = {10.1109/TPDS.2020.2992571},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2435-2452},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Data-driven derivation of an analytic model for parallel servers with job replication},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dynamic multi–objective approach for dynamic load
balancing in heterogeneous systems. <em>TPDS</em>, <em>31</em>(10),
2421–2434. (<a href="https://doi.org/10.1109/TPDS.2020.2989869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern standards in High Performance Computing (HPC) have started to consider energy consumption and power draw as a limiting factor. New and more complex architectures have been introduced in HPC systems to afford these new restrictions, and include coprocessors such as GPGPUs for intensive computational tasks. As systems increase in heterogeneity, workload distribution becomes a more core problem to achieve the maximum efficiency in every computational component. We present a Multi-Objective Dynamic Load Balancing (DLB) approach where several objectives can be applied to tune an application. These objectives can be dynamically exchanged during the execution of an algorithm to better adapt to the resources available in a system. We have implemented the Multi-Objective DLB together with a generic heuristic engine, designed to perform multiple strategies for DLB in iterative problems. We also present Ull Multiobjective Framework (UllMF), an open-source tool that implements the Multi-Objective generic approach. UllMF separates metric gathering, objective functions to be optimized and load balancing algorithms, and improves code portability using a simple interface to reduce the costs of new implementations. We illustrate how performance and energy consumption are improved for the implemented techniques, and analyze their quality using different DLB techniques from the literature.},
  archive      = {J_TPDS},
  author       = {Alberto Cabrera and Alejandro Acosta and Francisco Almeida and Vicente Blanco},
  doi          = {10.1109/TPDS.2020.2989869},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2421-2434},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A dynamic Multi–Objective approach for dynamic load balancing in heterogeneous systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimal locality-aware task scheduling algorithm based on
bipartite graph modelling for spark applications. <em>TPDS</em>,
<em>31</em>(10), 2406–2420. (<a
href="https://doi.org/10.1109/TPDS.2020.2992073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the distributed computing framework of Spark, cross-node/rack data transfer produced by map tasks and reduce tasks are common problems resulting in performance degradation, such as prolonging of entire execution time and network congestion. To address these problems, this article utilizes the bipartite graph modelling to propose an optimal locality-aware task scheduling algorithm. By considering global optimality, the algorithm can generate the optimal scheduling solution for both the map tasks and the reduce tasks for data locality. Because of the different communication modes, this article uses a unified graph to model the map task scheduling and the reduce task scheduling respectively. Then, by calculating the communication cost matrix of tasks, we formulate an optimal task scheduling scheme to minimize overall communication cost and transform the problem as the well-known graph problem: minimum weighted bipartite matching (MWBM), which can be resolved by Kuhn-Munkres algorithm. In addition, this article proposes a locality-aware executor allocation strategy to improve the data locality further. We implement our algorithm and strategy in Spark-2.4.1 and evaluate its performance using several representative micro-benchmarks, macro-benchmarks, and HiBench benchmark suite. The experimental results verify that by reducing the network traffic and access latency, the proposed algorithm can improve the job performance substantially compared to some other task scheduling algorithms.},
  archive      = {J_TPDS},
  author       = {Zhongming Fu and Zhuo Tang and Li Yang and Chubo Liu},
  doi          = {10.1109/TPDS.2020.2992073},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2406-2420},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An optimal locality-aware task scheduling algorithm based on bipartite graph modelling for spark applications},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RMWPaxos: Fault-tolerant in-place consensus sequences.
<em>TPDS</em>, <em>31</em>(10), 2392–2405. (<a
href="https://doi.org/10.1109/TPDS.2020.2981891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building consensus sequences based on distributed, fault-tolerant consensus, as used for replicated state machines, typically requires a separate distributed state for every new consensus instance. Allocating and maintaining this state causes significant overhead. In particular, freeing the distributed, outdated states in a fault-tolerant way is not trivial and adds further complexity and cost to the system. In this article, we propose an extension to the single-decree Paxos protocol that can learn a sequence of consensus decisions `in-place&#39;, i.e., with a single set of distributed states. Our protocol does not require dynamic log structures and hence has no need for distributed log pruning, snapshotting, compaction, or dynamic resource allocation. The protocol builds a fault-tolerant atomic register that supports arbitrary read-modify-write operations. We use the concept of consistent quorums to detect whether the previous consensus still needs to be consolidated or is already finished so that the next consensus value can be safely proposed. Reading a consolidated consensus is done without state modifications and is thereby free of concurrency control and demand for serialisation. A proposer that is not interrupted reaches agreement on consecutive consensus decisions within a single message round-trip per decision by preparing the acceptors eagerly with the previous request.},
  archive      = {J_TPDS},
  author       = {Jan Skrzypczak and Florian Schintke and Thorsten Schütt},
  doi          = {10.1109/TPDS.2020.2981891},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2392-2405},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RMWPaxos: Fault-tolerant in-place consensus sequences},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integrated indexing and search service for distributed
file systems. <em>TPDS</em>, <em>31</em>(10), 2375–2391. (<a
href="https://doi.org/10.1109/TPDS.2020.2990656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data services such as search, discovery, and management in scalable distributed environments have traditionally been decoupled from the underlying file systems, and are often deployed using external databases and indexing services. However, modern data production rates, looming data movement costs, and the lack of metadata, entail revisiting the decoupled file system-data services design philosophy. In this article, we present TagIt, a scalable data management service framework aimed at scientific datasets, which can be integrated into prevalent distributed file system architectures. A key feature of TagIt is a scalable, distributed metadata indexing framework, which facilitates a flexible tagging capability to support data discovery. Furthermore, the tags can also be associated with an active operator, for pre-processing, filtering, or automatic metadata extraction, which we seamlessly offload to file servers in a load-aware fashion. We have integrated TagIt into two popular distributed file systems, i.e., GlusterFS and CephFS. Our evaluation demonstrates that TagIt can expedite data search operation by up to 10× over the extant decoupled approach.},
  archive      = {J_TPDS},
  author       = {Hyogi Sim and Awais Khan and Sudharshan S. Vazhkudai and Seung-Hwan Lim and Ali R. Butt and Youngjae Kim},
  doi          = {10.1109/TPDS.2020.2990656},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2375-2391},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An integrated indexing and search service for distributed file systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and accurate traffic measurement with hierarchical
filtering. <em>TPDS</em>, <em>31</em>(10), 2360–2374. (<a
href="https://doi.org/10.1109/TPDS.2020.2991007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches have been widely used to record traffic statistics using sub-linear space data structure. Most sketches focus on the traffic estimation of elephant flows (i.e., heavy hitters) due to their importance to many network optimization tasks, e.g., traffic engineering and load balancing. In fact, the information of aggregate mice flows (e.g., all the mice flows with the same source IP) is also crucial to many security-associated tasks, e.g., DDoS detection and network scan detection. However, the previous solutions, e.g., measuring each individual flow or using multiple sketches for independent measurement tasks, will result in worse estimation error or higher computational overhead. To conquer the above disadvantages, we propose an accurate traffic measurement framework with multiple filters, called Sketchtree, to efficiently measure both elephant flows and aggregate mice flows. These filters in Sketchtree are organized in a hierarchical manner, and help to alleviate the hash collision and improve the measurement accuracy, as the number of flows through hierarchical filters in turn will be decreased gradually. We also design some mechanisms to improve the resource utilization efficiency. To validate our proposal, we have implemented Sketchtree and conducted experimental evaluation using real campus traffic traces. The experimental results show that Sketchtree can increase the processing speed by 100 percent, and reduce the measurement error by over 30 percent compared with state-of-the-art sketches.},
  archive      = {J_TPDS},
  author       = {Haibo Wang and Hongli Xu and Liusheng Huang and Yutong Zhai},
  doi          = {10.1109/TPDS.2020.2991007},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2360-2374},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fast and accurate traffic measurement with hierarchical filtering},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A ubiquitous machine learning accelerator with automatic
parallelization on FPGA. <em>TPDS</em>, <em>31</em>(10), 2346–2359. (<a
href="https://doi.org/10.1109/TPDS.2020.2990924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been widely applied in various emerging data-intensive applications, and has to be optimized and accelerated by powerful engines to process very large scale data. Recently, the instruction set based accelerators on Field Progarmmable Gate Arrays (FPGAs) have been a promising topic for machine learning applications. The customized instructions can be further scheduled to achieve higher instruction-level parallelism. In this article, we design a ubiquitous accelerator with out-of-order automatic parallelization for large-scale data-intensive applications. The accelerator accommodates four representative applications, including clustering algorithms, deep neural networks, genome sequencing, and collaborative filtering. In order to improve the coarse-grained instruction-level parallelism, the accelerator employs an out-of-order scheduling method to enable parallel dataflow computation. We use Colored Petri Net (CPN) tools to analyze the dependences in the applications, and build a hardware prototype on the real FPGA platform. For cluster applications, the accelerator can support four different algorithms, including K-Means, SLINK, PAM, and DBSCAN. For collaborative filtering applications, it accommodates Tanimoto, euclidean, Cosine, and Pearson Correlation as Similarity metrics. For deep learning applications, we implement hardware accelerators for both training process and inference process. Finally, for genome sequencing, we design a hardware accelerator for the BWA-SW algorithm. Experimental results show that the accelerator architecture can reach up to 25X speedup against Intel processors with affordable hardware cost, insignificant power consumption, and high flexibility.},
  archive      = {J_TPDS},
  author       = {Chao Wang and Lei Gong and Xi Li and Xuehai Zhou},
  doi          = {10.1109/TPDS.2020.2990924},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2346-2359},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A ubiquitous machine learning accelerator with automatic parallelization on FPGA},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AeSpTV: An adaptive and efficient framework for sparse
tensor-vector product kernel on a high-performance computing platform.
<em>TPDS</em>, <em>31</em>(10), 2329–2345. (<a
href="https://doi.org/10.1109/TPDS.2020.2990429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional, large-scale, and sparse data, which can be neatly represented by sparse tensors, are increasingly used in various applications such as data analysis and machine learning. A high-performance sparse tensor-vector product (SpTV), one of the most fundamental operations of processing sparse tensors, is necessary for improving efficiency of related applications. In this article, we propose aeSpTV, an adaptive and efficient SpTV framework on Sunway TaihuLight supercomputer, to solve several challenges of optimizing SpTVon high-performance computing platforms. First, to map SpTV to Sunway architecture and tame expensive memory access latency and parallel writing conflict due to the intrinsic irregularity of SpTV, we introduce an adaptive SpTV parallelization. Second, to co-execute with the parallelization design while still ensuring high efficiency, we design a sparse tensor data structure named CSSoCR. Third, based on the adaptive SpTV parallelization with the novel tensor data structure, we present an autotuner that chooses the most befitting tensor partitioning method for aeSpTV using the variance analysis theory of mathematical statistics to achieve load balance. Fourth, to further leverage the computing power of Sunway, we propose customized optimizations for aeSpTV. Experimental results show that aeSpTV yields good sacalability on both thread-level and process-level parallelism of Sunway. It achieves a maximum GFLOPS of 195.69 on 128 processes. Additionally, it is proved that optimization effects of the partitioning autotuner and optimization techniques are remarkable.},
  archive      = {J_TPDS},
  author       = {Yuedan Chen and Guoqing Xiao and M. Tamer Özsu and Chubo Liu and Albert Y. Zomaya and Tao Li},
  doi          = {10.1109/TPDS.2020.2990429},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2329-2345},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AeSpTV: An adaptive and efficient framework for sparse tensor-vector product kernel on a high-performance computing platform},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-rack-aware updates in erasure-coded data centers:
Design and evaluation. <em>TPDS</em>, <em>31</em>(10), 2315–2328. (<a
href="https://doi.org/10.1109/TPDS.2020.2991021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The update performance in erasure-coded data centers is often bottlenecked by the constrained cross-rack bandwidth. We propose CAU, a cross-rack-aware update mechanism that aims to mitigate the cross-rack update traffic in erasure-coded data centers. CAU builds on three design elements: (i) selective parity updates, which select the appropriate parity update approach based on the update pattern and the data layout to reduce the cross-rack update traffic; (ii) data grouping, which relocates and groups updated data chunks in the same rack to further reduce the cross-rack update traffic; and (iii) interim replication, which stores a specified number of temporary replicas for each newly updated data chunk. We evaluate CAU via trace-driven analysis, local cluster experiments, and Amazon EC2 experiments. We show that CAU enhances state-of-the-arts by mitigating the cross-rack update traffic as well as maintaining high update performance in both local cluster and geo-distributed environments.},
  archive      = {J_TPDS},
  author       = {Zhirong Shen and Patrick P. C. Lee},
  doi          = {10.1109/TPDS.2020.2991021},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2315-2328},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cross-rack-aware updates in erasure-coded data centers: Design and evaluation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving restore performance for in-line backup system
combining deduplication and delta compression. <em>TPDS</em>,
<em>31</em>(10), 2302–2314. (<a
href="https://doi.org/10.1109/TPDS.2020.2991030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication, though being efficient in removing duplicate chunks, introduces chunk fragmentation which decreases restore performance. Rewriting algorithms are proposed to reduce the chunk fragmentation. Delta compression is often used as a complement for data deduplication to further improve storage efficiency. We observe that delta compression introduces a new type of chunk fragmentation stemming from improper delta compression for chunks of which the base chunks are fragmented. The new type of chunk fragmentation severely decreases restore performance and cannot be addressed by existing rewriting algorithms. To address this problem, we propose SDC, a scheme performing post-deduplication delta compression only for the chunks of which the bases can be directly found in the restore cache to eliminate additional disk reads for base chunks, thus avoiding the new type of chunk fragmentation. In addition, self-referenced chunks can be fragmented, which decrease restore performance, and these fragmented chunks can serve as bases to decrease the restore performance repeatedly. We propose a hybrid rewriting scheme for SDC to rewrite such fragmented chunks. Experimental results show that SDC improves the restore performance of the approach that directly performs delta compression after data deduplication by 2.9-16.9x, and achieves more than 95 percent of its compression gains.},
  archive      = {J_TPDS},
  author       = {Yucheng Zhang and Ye Yuan and Dan Feng and Chunzhi Wang and Xinyun Wu and Lingyu Yan and Deng Pan and Shuanghong Wang},
  doi          = {10.1109/TPDS.2020.2991030},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2302-2314},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving restore performance for in-line backup system combining deduplication and delta compression},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated fine-grained CPU cap control in serverless
computing platform. <em>TPDS</em>, <em>31</em>(10), 2289–2301. (<a
href="https://doi.org/10.1109/TPDS.2020.2989771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing has emerged as a new cloud computing execution model that liberates users and application developers from explicitly managing `physical&#39; resources, leaving such a resource management burden to service providers. In this article, we study the problem of resource allocation for multi-tenant serverless computing platforms explicitly taking into account workload fluctuations including sudden surges. In particular, we investigate different root causes of performance degradation in these platforms where tenants (their applications) have different workload characteristics. To this end, we develop a fine-grained CPU cap control solution as a resource manager that dynamically adjusts CPU usage limit (or CPU cap) concerning applications with same/similar performance requirements, i.e., application groups. The adjustment of CPU caps applies primarily to co-located worker processes of serverless computing platforms to minimize resource contention, which is the major source of performance degradation. The actual adjustment decisions are made based on performance metrics (e.g., throttled time and queue length) using a group-aware scheduling algorithm. The extensive experimental results performed in our local cluster confirm that the proposed resource manager can effectively eliminate the burden of explicit reservation of computing capacity, even when fluctuations and sudden surges in the incoming workload exist. We measure the robustness of the proposed resource manager by comparing it with several heuristics which extensively used in practice, including the enhanced version of round robin and the least length queue scheduling policies, under various workload intensities driven by real-world scenarios. Notably, our resource manager outperforms other heuristics by decreasing skewness and average response time up to 44 and 94 percent, respectively, while it does not over-use the CPU resources.},
  archive      = {J_TPDS},
  author       = {Young Ki Kim and M. Reza HoseinyFarahabady and Young Choon Lee and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2020.2989771},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2289-2301},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Automated fine-grained CPU cap control in serverless computing platform},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating task duplication in optimal task scheduling with
communication delays. <em>TPDS</em>, <em>31</em>(10), 2277–2288. (<a
href="https://doi.org/10.1109/TPDS.2020.2989767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task scheduling with communication delays is an NP-hard problem. Some previous attempts at finding optimal solutions to this problem have used branch-and-bound state-space search, with promising results. Duplication is an extension to the task scheduling model which allows tasks to be executed multiple times within a schedule, providing benefits to schedule length where this allows a reduction in communication costs. This article proposes the first approach to state-space search for optimal task scheduling with task duplication. Also presented are new definitions for important standard bounding metrics in the context of duplication. An extensive empirical evaluation shows that the use of duplication significantly increases the difficulty of optimal scheduling, but the proposed approach also gives certainty that a large proportion of task graphs can be scheduled more effectively when duplication is allowed, and permits to quantify the exact advantage.},
  archive      = {J_TPDS},
  author       = {Michael Orr and Oliver Sinnen},
  doi          = {10.1109/TPDS.2020.2989767},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2277-2288},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Integrating task duplication in optimal task scheduling with communication delays},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SF-sketch: A two-stage sketch for data streams.
<em>TPDS</em>, <em>31</em>(10), 2263–2276. (<a
href="https://doi.org/10.1109/TPDS.2020.2987609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches are probabilistic data structures designed for recording frequencies of items in a multi-set. They are widely used in various fields, especially for gathering Internet statistics from distributed data streams in network measurements. In a distributed streaming application with high data rates, a sketch in each monitoring node “fills up” very quickly and then its content is transferred to a remote collector responsible for answering queries. Thus, the size of the contents transferred must be kept as small as possible while meeting the desired accuracy requirement. To obtain significantly higher accuracy while keeping the same update and query speed as the best prior sketches, in this article, we propose a new sketch - the Slim-Fat (SF) sketch. The key idea behind the SF-sketch is to maintain two separate sketches: a larger sketch, the Fat-subsketch, and a smaller sketch, the Slim-subsketch. The Fat-subsketch is used for updating and periodically producing the Slim-subsketch, which is then transferred to the remote collector for answering queries quickly and accurately. We also present the error bound as well as an accurate model of the correct rate of the SF-sketch, and verify their correctness through experiments. We implemented and extensively evaluated the SF-sketch along with several prior sketches. Our results show that when the size of our Slim-subsketch and of the widely used Count-Min (CM) sketch are kept the same, our SF-sketch outperforms the CM-sketch by up to 33.1 times in terms of accuracy (when the ratio of the sizes of the Fat-subsketch and the Slim-subsketch is 16:1). We have made all source codes publicly available at Github [“Source code of SF sketches,” [Online]. Available: https://github.com/paper2017/SF-sketch].},
  archive      = {J_TPDS},
  author       = {Lingtong Liu and Yulong Shen and Yibo Yan and Tong Yang and Muhammad Shahzad and Bin Cui and Gaogang Xie},
  doi          = {10.1109/TPDS.2020.2987609},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2263-2276},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SF-sketch: A two-stage sketch for data streams},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deterministic data distribution for efficient recovery in
erasure-coded storage systems. <em>TPDS</em>, <em>31</em>(10),
2248–2262. (<a href="https://doi.org/10.1109/TPDS.2020.2987837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to individual unreliable commodity components, failures are common in large-scale distributed storage systems. Erasure codes are widely deployed in practical storage systems to provide fault tolerance with low storage overhead. However, random data distribution (RDD), commonly used in erasure-coded storage systems, induces heavy cross-rack traffic, load imbalance, and random access, which adversely affects failure recovery. In this article, with orthogonal arrays, we define a Deterministic Data Distribution (D 3 ) to uniformly distribute data/parity blocks among nodes, and propose an efficient failure recovery approach based on D 3 , which minimizes the cross-rack repair traffic against a single node failure. Thanks to the uniformity of D 3 , the proposed recovery approach balances the repair traffic not only among nodes within a rack but also among racks. We implement D 3 over Reed-Solomon codes and Locally Repairable Codes in Hadoop Distributed File System (HDFS) with a cluster of 28 machines. Compared with RDD, our experiments show that D3 significantly speeds up the failure recovery up to 2.49 times for RS codes and 1.38 times for LRCs. Moreover, D 3 supports front-end applications better than RDD in both of normal and recovery states.},
  archive      = {J_TPDS},
  author       = {Liangliang Xu and Min Lyu and Zhipeng Li and Yongkun Li and Yinlong Xu},
  doi          = {10.1109/TPDS.2020.2987837},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2248-2262},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Deterministic data distribution for efficient recovery in erasure-coded storage systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-cost datacenter load balancing with multipath transport
and top-of-rack switches. <em>TPDS</em>, <em>31</em>(10), 2232–2247. (<a
href="https://doi.org/10.1109/TPDS.2020.2989441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load balancing in datacenter networks (DCNs) is an important and challenging task for datacenter managers. A number of sophisticated technologies have been proposed to improve load balancing performance in a complicated circumstance, i.e., with various traffic characteristics. Many approaches need a high cost to implement, such as changing switch hardware. The efficiency problem has not been well addressed. MPTCP was proposed as a low-cost approach to improve data transmission in DCNs, which uses subflows to balance workloads across multiple paths. However, current MPTCP is not satisfying, especially when there are rack-local flows or many-to-one short flows. In this article, we propose DCMPTCP to improve the efficacy of MPTCP. We gradually develop three mechanisms. First, DCMPTCP identifies rack-local traffic and eliminates unnecessary subflows to reduce the overhead. Second, DCMPTCP estimates flow length and establishes subflows in a smarter way. Third, DCMPTCP strengthens explicit congestion notification to improve the congestion control performance on inter-rack many-to-one short flows. We have implemented DCMPTCP in both the Linux kernel and ns-3 simulator. Our comprehensive testbed experiments and simulations show that DCMPTCP outperforms MPTCP in both 1 Gbps testbed, and 10 Gbps large-scale simulation network.},
  archive      = {J_TPDS},
  author       = {Enhuan Dong and Xiaoming Fu and Mingwei Xu and Yuan Yang},
  doi          = {10.1109/TPDS.2020.2989441},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2232-2247},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Low-cost datacenter load balancing with multipath transport and top-of-rack switches},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lock-free parallelization for variance-reduced stochastic
gradient descent on streaming data. <em>TPDS</em>, <em>31</em>(9),
2220–2231. (<a href="https://doi.org/10.1109/TPDS.2020.2987867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic Gradient Descent (SGD) is an iterative algorithm for fitting a model to the training dataset in machine learning problems. With low computation cost, SGD is especially suited for learning from large datasets. However, the variance of SGD tends to be high because it uses only a single data point to determine the update direction at each iteration of gradient descent, rather than all available training data points. Recent research has proposed variance-reduced variants of SGD by incorporating a correction term to approximate full-data gradients. However, it is difficult to parallelize such variants with high performance and accuracy, especially on streaming data. As parallelization is a crucial requirement for large-scale applications, this article focuses on the parallel setting in a multicore machine and presents LFS-STRSAGA, a lock-free approach to parallelizing variance-reduced SGD on streaming data. LFS-STRSAGA embraces a lock-free data structure to process the arrival of streaming data in parallel, and asynchronously maintains the essential information to approximate full-data gradients with low cost. Both our theoretical and empirical results show that LFS-STRSAGA matches the accuracy of the state-of-the-art variance-reduced SGD on streaming data under sparsity assumption (common in machine learning problems), and that LFS-STRSAGA reduces the model update time by over 98 percent.},
  archive      = {J_TPDS},
  author       = {Yaqiong Peng and Zhiyu Hao and Xiaochun Yun},
  doi          = {10.1109/TPDS.2020.2987867},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2220-2231},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Lock-free parallelization for variance-reduced stochastic gradient descent on streaming data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards higher performance and robust compilation for CGRA
modulo scheduling. <em>TPDS</em>, <em>31</em>(9), 2201–2219. (<a
href="https://doi.org/10.1109/TPDS.2020.2989149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse-Grained Reconfigurable Architectures (CGRA) is a promising solution for accelerating computation intensive tasks due to its good trade-off in energy efficiency and flexibility. One of the challenging research topic is how to effectively deploy loops onto CGRAs within acceptable compilation time. Modulo scheduling (MS) has shown to be efficient on deploying loops onto CGRAs. Existing CGRA MS algorithms still suffer from the challenge of mapping loop with higher performance under acceptable compilation time, especially mapping large and irregular loops onto CGRAs with limited computational and routing resources. This is mainly due to the under utilization of the available buffer resources on CGRA, unawareness of critical mapping constraints and time consuming method of solving temporal and spatial mapping. This article focus on improving the performance and compilation robustness of the modulo scheduling mapping algorithm for CGRAs. We decomposes the CGRA MS problem into the temporal and spatial mapping problem and reorganize the processes inside these two problems. For the temporal mapping problem, we provide a comprehensive and systematic mapping flow that includes a powerful buffer allocation algorithm, and efficient interconnection &amp; computational constraints solving algorithms. For the spatial mapping problem, we develop a fast and stable spatial mapping algorithm with backtracking and reordering mechanism. Our MS mapping algorithm is able to map loops onto CGRA with higher performance and faster compilation time. Experiment results show that given the same compilation time budget, our mapping algorithm generates higher compilation success rate. Among the successfully compiled loops, our approach can improve 5.4 to 14.2 percent performance and takes x24 to x1099 less compilation time in average comparing with state-of-the-art CGRA mapping algorithms.},
  archive      = {J_TPDS},
  author       = {Zhongyuan Zhao and Weiguang Sheng and Qin Wang and Wenzhi Yin and Pengfei Ye and Jinchao Li and Zhigang Mao},
  doi          = {10.1109/TPDS.2020.2989149},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2201-2219},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards higher performance and robust compilation for CGRA modulo scheduling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boosting the performance of SSDs via fully exploiting the
plane level parallelism. <em>TPDS</em>, <em>31</em>(9), 2185–2200. (<a
href="https://doi.org/10.1109/TPDS.2020.2987894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solid state drives (SSDs) are constructed with multiple level parallel organization, including channels, chips, dies, and planes. Among these parallel levels, plane level parallelism, which is the last level parallelism of SSDs, has the most strict restrictions. Only the same type of operations that access the same address in different planes can be processed in parallel. In order to maximize the access performance, several previous works have been proposed to exploit the plane level parallelism for host accesses and internal operations of SSDs. However, our preliminary studies show that the plane level parallelism is farfrom well utilized and should be further improved. The reason is that the strict restrictions of plane level parallelism are hard to be satisfied. In this article, a from plane to die parallel optimization framework is proposed to exploit the plane level parallelism through smartly satisfying the strict restrictions all the time. In order to achieve the objective, there are at least two challenges. First, due to that host access patterns are always complex, receiving multiple same-type requests to different planes at the same time is uncommon. Second, there are many internal activities, such as garbage collection (GC), which may destroy the restrictions. In order to solve above challenges, two schemes are proposed in the SSD controller: First, a die level write construction scheme is designed to make sure there are always N pages of data written by each write operation. Second, in a further step, a die level GC scheme is proposed to activate GC in the unit of all planes in the same die. Combing the die level write and die level GC, write accesses from both host write operations and GC induced valid page movements can be processed in parallel at all time. To further improve the performance of SSDs, host write operations blocked by GCs are suggested to be processed in parallel with GC induced valid page movements, bringing lesser waiting time cost of host write operations. As a result, the GC cost and average write latency can be significantly reduced. Experiment results show that the proposed framework is able to significantly improve the write performance without read performance impact.},
  archive      = {J_TPDS},
  author       = {Congming Gao and Liang Shi and Kai Liu and Chun Jason Xue and Jun Yang and Youtao Zhang},
  doi          = {10.1109/TPDS.2020.2987894},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2185-2200},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Boosting the performance of SSDs via fully exploiting the plane level parallelism},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The workflow trace archive: Open-access data from public and
private computing infrastructures. <em>TPDS</em>, <em>31</em>(9),
2170–2184. (<a href="https://doi.org/10.1109/TPDS.2020.2984821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic, relevant, and reproducible experiments often need input traces collected from real-world environments. In this work, we focus on traces of workflows-common in datacenters, clouds, and HPC infrastructures. We show that the state-of-the-art in using workflow-traces raises important issues: (1) the use of realistic traces is infrequent and (2) the use of realistic, open-access traces even more so. Alleviating these issues, we introduce the Workflow Trace Archive (WTA), an open-access archive of workflow traces from diverse computing infrastructures and tooling to parse, validate, and analyze traces. The WTA includes &gt; 48 million workflows captured from &gt; 10 computing infrastructures, representing a broad diversity of trace domains and characteristics. To emphasize the importance of trace diversity, we characterize the WTA contents and analyze in simulation the impact of trace diversity on experiment results. Our results indicate significant differences in characteristics, properties, and workflow structures between workload sources, domains, and fields.},
  archive      = {J_TPDS},
  author       = {Laurens Versluis and Roland Mathá and Sacheendra Talluri and Tim Hegeman and Radu Prodan and Ewa Deelman and Alexandru Iosup},
  doi          = {10.1109/TPDS.2020.2984821},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2170-2184},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The workflow trace archive: Open-access data from public and private computing infrastructures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minority disk failure prediction based on transfer learning
in large data centers of heterogeneous disk systems. <em>TPDS</em>,
<em>31</em>(9), 2155–2169. (<a
href="https://doi.org/10.1109/TPDS.2020.2985346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The storage system in large scale data centers is typically built upon thousands or even millions of disks, where disk failures constantly happen. A disk failure could lead to serious data loss and thus system unavailability or even catastrophic consequences if the lost data cannot be recovered. While replication and erasure coding techniques have been widely deployed to guarantee storage availability and reliability, disk failure prediction is gaining popularity as it has the potential to prevent disk failures from occurring in the first place. Recent trends have turned toward applying machine learning approaches based on disk SMART attributes for disk failure predictions. However, traditional machine learning (ML) approaches require a large set of training data in order to deliver good predictive performance. In large-scale storage systems, new disks enter gradually to augment the storage capacity or to replace failed disks, leading storage systems to consist of small amounts of new disks from different vendors and/or different models from the same vendor as time goes on. We refer to this relatively small amount of disks as minority disks. Due to the lack of sufficient training data, traditional ML approaches fail to deliver satisfactory predictive performance in evolving storage systems which consist of heterogeneous minority disks. To address this challenge and improve the predictive performance for minority disks in large data centers, we propose a minority disk failure prediction model named TLDFP based on a transfer learning approach. Our evaluation results in two realistic datasets have demonstrated that TLDFP can deliver much more precise results and lower additional maintenance cost, compared to four popular prediction models based on traditional ML algorithms and two state-of-the-art transfer learning methods.},
  archive      = {J_TPDS},
  author       = {Ji Zhang and Ke Zhou and Ping Huang and Xubin He and Ming Xie and Bin Cheng and Yongguang Ji and Yinhu Wang},
  doi          = {10.1109/TPDS.2020.2985346},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2155-2169},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Minority disk failure prediction based on transfer learning in large data centers of heterogeneous disk systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneous edge offloading with incomplete information: A
minority game approach. <em>TPDS</em>, <em>31</em>(9), 2139–2154. (<a
href="https://doi.org/10.1109/TPDS.2020.2988161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task offloading is one of key operations in edge computing, which is essential for reducing the latency of task processing and boosting the capacity of end devices. However, the heterogeneity among tasks generated by various users makes it challenging to design efficient task offloading algorithms. In addition, the assumption of complete information for offloading decision-making does not always hold in a distributed edge computing environment. In this article, we formulate the problem of heterogeneous task offloading in a distributed environment as a minority game (MG), in which each player must make decisions independently in each turn and the players who end up on the minority side win. The multi-player MG incentivizes players to cooperate with each other in the scenarios with incomplete information, where players don&#39;t have full information about other players (e.g., the number of tasks, the required resources). To address the challenges incurred by task heterogeneity and the divergence of naive MG approaches, we propose an MG based scheme, in which tasks are divided into subtasks and instructed to form into a set of groups as possible, and the left ones are scheduled to perform decision adjustment in a probabilistic manner. We prove that our proposed algorithm can converge to a near-optimal point, and also investigate its stability and price of anarchy in terms of task processing time. Finally, we conduct a series of simulations to evaluate the effectiveness of our proposed scheme and the results indicate that our scheme can achieve around 30\% reduction of task processing time compared with other approaches. Moreover, our proposed scheme can converge to a near-optimal point, which cannot be guaranteed by naive MG approaches.},
  archive      = {J_TPDS},
  author       = {Miao Hu and Zixuan Xie and Di Wu and Yipeng Zhou and Xu Chen and Liang Xiao},
  doi          = {10.1109/TPDS.2020.2988161},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2139-2154},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Heterogeneous edge offloading with incomplete information: A minority game approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CURE: A high-performance, low-power, and reliable
network-on-chip design using reinforcement learning. <em>TPDS</em>,
<em>31</em>(9), 2125–2138. (<a
href="https://doi.org/10.1109/TPDS.2020.2986297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose CURE, a deep reinforcement learning (DRL)-based NoC design framework that simultaneously reduces network latency, improves energy-efficiency, and tolerates transient errors and permanent faults. CURE has several architectural innovations and a DRL-based hardware controller to manage design complexity and optimize trade-offs. First, in CURE, we propose reversible multi-function adaptive channels (RMCs) to reduce NoC power consumption and network latency. Second, we implement a new fault-secure adaptive error correction hardware in each router to enhance reliability for both transient errors and permanent faults. Third, we propose a router power-gating and bypass design that powers off NoC components to reduce power and extend chip lifespan. Further, for the complex dynamic interactions of these techniques, we propose using DRL to train a proactive control policy to provide improved fault-tolerance, reduced power consumption, and improved performance. Simulation using the PARSEC benchmark shows that CURE reduces end-to-end packet latency by 39 percent, improves energy efficiency by 92 percent, and lowers static and dynamic power consumption by 24 and 38 percent, respectively, over conventional solutions. Using mean-time-to-failure, we show that CURE is 7.7× more reliable than the conventional NoC design.},
  archive      = {J_TPDS},
  author       = {Ke Wang and Ahmed Louri},
  doi          = {10.1109/TPDS.2020.2986297},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2125-2138},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CURE: A high-performance, low-power, and reliable network-on-chip design using reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SaberLDA: Sparsity-aware learning of topic models on GPUs.
<em>TPDS</em>, <em>31</em>(9), 2112–2124. (<a
href="https://doi.org/10.1109/TPDS.2020.2979702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images, which are required to model datasets and a large number of topics, e.g., tens of thousands of topics for industry scale applications. Although distributed CPU systems have been used to address this problem, they are slow and resource inefficient. GPU-based systems have emerged as a promising alternative because of their high computational power and memory bandwidth. However, existing GPU-based LDA systems can only learn thousands of topics, because they use dense data structures, and have linear time complexity to the number of topics. In this article, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a warp-based sampling kernel, an efficient sparse matrix counting method, and a fine-grained load balancing strategy. SaberLDA achieves linear speedup on 4 GPUs and is 6-10 times faster than existing GPU systems in thousands of topics. It can learn 40,000 topics from a dataset of billions of tokens in two hours, which was previously only achievable using clusters of tens of CPU servers.},
  archive      = {J_TPDS},
  author       = {Kaiwei Li and Jianfei Chen and Wenguang Chen and Jun Zhu},
  doi          = {10.1109/TPDS.2020.2979702},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2112-2124},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SaberLDA: Sparsity-aware learning of topic models on GPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-efficient parallel real-time scheduling on clustered
multi-core. <em>TPDS</em>, <em>31</em>(9), 2097–2111. (<a
href="https://doi.org/10.1109/TPDS.2020.2985701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy-efficiency is a critical requirement for computation-intensive real-time applications on multi-core embedded systems. Multi-core processors enable intra-task parallelism, and in this work, we study energy-efficient real-time scheduling of constrained deadline sporadic parallel tasks, where each task is represented as a directed acyclic graph (DAG). We consider a clustered multi-core platform where processors within the same cluster run at the same speed at any given time. A new concept named speed-profile is proposed to model per-task and per-cluster energy-consumption variations during run-time to minimize the expected long-term energy consumption. To our knowledge, no existing work considers energy-aware real-time scheduling of DAG tasks with constrained deadlines, nor on a clustered multi-core platform. The proposed energy-aware real-time scheduler is implemented upon an ODROID XU-3 board to evaluate and demonstrate its feasibility and practicality. To complement our system experiments in large-scale, we have also conducted simulations that demonstrate a CPU energy saving of up to 67 percent through our proposed approach compared to existing methods.},
  archive      = {J_TPDS},
  author       = {Ashikahmed Bhuiyan and Di Liu and Aamir Khan and Abusayeed Saifullah and Nan Guan and Zhishan Guo},
  doi          = {10.1109/TPDS.2020.2985701},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2097-2111},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-efficient parallel real-time scheduling on clustered multi-core},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Customizable scale-out key-value stores. <em>TPDS</em>,
<em>31</em>(9), 2081–2096. (<a
href="https://doi.org/10.1109/TPDS.2020.2982640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise KV stores are often not well suited for HPC applications, and thus cumbersome end-to-end KV design customization is required to meet the needs of modern HPC applications. To this end, in this article we present bespoKV, an adaptive, extensible, and scale-out KV store framework. bespoKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. For the control plane, bespoKVprovides pre-built modules, called controlets, supporting common distributed functionalities (e.g., replication, consistency, and topology) and their various combinations. This decoupling allows bespoKV to take a user-provided single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Such specializations enable innovative uses of KV stores in HPC applications, especially for emerging applications that utilize KV-friendly workloads. We evaluate bespoKV in a local testbed as well as in a public cloud settings. Experiments show that bespoKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes 1.2× to 2.6× better than the state-of-the-art systems.},
  archive      = {J_TPDS},
  author       = {Ali Anwar and Yue Cheng and Hai Huang and Jingoo Han and Hyogi Sim and Dongyoon Lee and Fred Douglis and Ali R. Butt},
  doi          = {10.1109/TPDS.2020.2982640},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2081-2096},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Customizable scale-out key-value stores},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safety enhancement for real-time parallel applications in
distributed automotive embedded systems: A stable stopping approach.
<em>TPDS</em>, <em>31</em>(9), 2067–2080. (<a
href="https://doi.org/10.1109/TPDS.2020.2984719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed automotive embedded systems, safety issues run through the entire life cycle, and safety mechanisms for error handling are desirable for risk control. This article focuses on safety enhancement (i.e., safety mechanisms for error handling) for a safety-critical automotive application within its deadline. A stable stopping approach used for safety enhancement for an automotive application is proposed based on the static recovery mechanism provided in ISO 26262. The Stable Stopping-based Safety Enhancement (SSSE) approach is proposed by combining known backward recovery, proposed forward recovery, and proposed forward-and-backward recovery through primary-backup repetition. The stable stopping (i.e., SSSE) approach is a convergence algorithm, which means that when the reliability value reaches a steady state and the algorithm can stop. Experimental results reveal that the exposure level defined in ISO 26262 drops from E3 to E1 after using SSSE, and such improvement enables a safety guarantee of higher level.},
  archive      = {J_TPDS},
  author       = {Guoqi Xie and Gang Zeng and Renfa Li},
  doi          = {10.1109/TPDS.2020.2984719},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2067-2080},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Safety enhancement for real-time parallel applications in distributed automotive embedded systems: A stable stopping approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient algorithms for delay-aware NFV-enabled
multicasting in mobile edge clouds with resource sharing. <em>TPDS</em>,
<em>31</em>(9), 2050–2066. (<a
href="https://doi.org/10.1109/TPDS.2020.2983918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stringent delay requirements of many mobile applications have led to the development of mobile edge clouds, to offer low latency network services at the network edges. Most conventional network services are implemented via hardware-based network functions, including firewalls and load balancers, to guarantee service security and performance. However, implementing hardware-based network functions usually incurs both a high capital expenditure (CAPEX) and operating expenditure (OPEX). Network Function Virtualization (NFV) exhibits a potential to reduce CAPEX and OPEX significantly, by deploying software-based network functions in virtual machines (VMs) on edge-clouds. We consider a fundamental problem of NFV-enabled multicasting in a mobile edge cloud, where each multicast request has both service function chain and end-to-end delay requirements. Specifically, each multicast request requires chaining of a sequence of network functions (referred to as a service function chain) from a source to a set of destinations within specified end-to-end delay requirements. We devise an approximation algorithm with a provable approximation ratio for a single multicast request admission if its delay requirement is negligible; otherwise, we propose an efficient heuristic. Furthermore, we also consider admissions of a given set of the delay-aware NFV-enabled multicast requests, for which we devise an efficient heuristic such that the system throughput is maximized, while the implementation cost of admitted requests is minimized. We finally evaluate the performance of the proposed algorithms in a real test-bed, and experimental results show that our algorithms outperform other similar approaches reported in literature.},
  archive      = {J_TPDS},
  author       = {Haozhe Ren and Zichuan Xu and Weifa Liang and Qiufen Xia and Pan Zhou and Omer F. Rana and Alex Galis and Guowei Wu},
  doi          = {10.1109/TPDS.2020.2983918},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2050-2066},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient algorithms for delay-aware NFV-enabled multicasting in mobile edge clouds with resource sharing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An event-driven approach to serverless seismic imaging in
the cloud. <em>TPDS</em>, <em>31</em>(9), 2032–2049. (<a
href="https://doi.org/10.1109/TPDS.2020.2982626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting the cloud for high-performance computing (HPC) is a challenging task, as software for HPC applications hinges on fast network connections and is sensitive to hardware failures. Using cloud infrastructure to recreate conventional HPC clusters is therefore in many cases an infeasible solution for migrating HPC applications to the cloud. As an alternative to the generic lift and shift approach, we consider the specific application of seismic imaging and demonstrate a serverless and event-driven approach for running large-scale instances of this problem in the cloud. Instead of permanently running compute instances, our workflow is based on a serverless architecture with high throughput batch computing and event-driven computations, in which computational resources are only running as long as they are utilized. We demonstrate that this approach is very flexible and allows for resilient and nested levels of parallelization, including domain decomposition for solving the underlying partial differential equations. While the event-driven approach introduces some overhead as computational resources are repeatedly restarted, it inherently provides resilience to instance shut-downs and allows a significant reduction of cost by avoiding idle instances, thus making the cloud a viable alternative to on-premise clusters for large-scale seismic imaging.},
  archive      = {J_TPDS},
  author       = {Philipp A. Witte and Mathias Louboutin and Henryk Modzelewski and Charles Jones and James Selvage and Felix J. Herrmann},
  doi          = {10.1109/TPDS.2020.2982626},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2032-2049},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An event-driven approach to serverless seismic imaging in the cloud},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The design of fast content-defined chunking for data
deduplication based storage systems. <em>TPDS</em>, <em>31</em>(9),
2017–2031. (<a href="https://doi.org/10.1109/TPDS.2020.2984632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-Defined Chunking (CDC) has been playing a key role in data deduplication systems recently due to its high redundancy detection ability. However, existing CDC-based approaches introduce heavy CPU overhead because they declare the chunk cut-points by computing and judging the rolling hashes of the data stream byte by byte. In this article, we propose FastCDC, a Fast and efficient Content-Defined Chunking approach, for data deduplication-based storage systems. The key idea behind FastCDC is the combined use of five key techniques, namely, gear based fast rolling hash, simplifying and enhancing the Gear hash judgment, skipping sub-minimum chunk cut-points, normalizing the chunk-size distribution in a small specified region to address the problem of the decreased deduplication ratio stemming from the cut-point skipping, and last but not least, rolling two bytes each time to further speed up CDC. Our evaluation results show that, by using a combination of the five techniques, FastCDC is 3-12X faster than the state-of-the-art CDC approaches, while achieving nearly the same and even higher deduplication ratio as the classic Rabin-based CDC. In addition, our study on the deduplication throughput of FastCDC-based Destor (an open source deduplication project) indicates that FastCDC helps achieve 1.2-3.0X higher throughput than Destor based on state-of-the-art chunkers.},
  archive      = {J_TPDS},
  author       = {Wen Xia and Xiangyu Zou and Hong Jiang and Yukun Zhou and Chuanyi Liu and Dan Feng and Yu Hua and Yuchong Hu and Yucheng Zhang},
  doi          = {10.1109/TPDS.2020.2984632},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2017-2031},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The design of fast content-defined chunking for data deduplication based storage systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ESetStore: An erasure-coded storage system with fast data
recovery. <em>TPDS</em>, <em>31</em>(9), 2001–2016. (<a
href="https://doi.org/10.1109/TPDS.2020.2983411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure codes have been used extensively in large-scale storage systems to reduce the storage overhead of triplication-based storage systems. One key performance issue introduced by erasure codes is the long time needed to recover from a single failure, which occurs constantly in large-scale storage systems. We present ESetStore, a prototype erasure-coded storage system that aims to achieve fast recovery from failures. ESetStore is novel in the following aspects. We proposed a data placement algorithm named ESet for our ESetStore that can aggregate adequate I/O resources from available storage servers to recover from each single failure. We designed and implemented efficient read and write operations on our erasure-coded storage system via effective use of available I/O and computation resources. We evaluated the performance of ESetStore with extensive experiments on a cluster with 50 storage servers. The evaluation results demonstrate that our recovery performance can obtain linear performance growth by harvesting available I/O resources. With our defined parameter recovery I/O parallelism under some mild conditions, we can achieve optimal recovery performance, in which ESet enables minimal recovery time. Rather than being an alternative to improve recovery performance, our work can be an enhancement for existing solutions, such as Partial-parallel-repair (PPR), to further improve recovery performance.},
  archive      = {J_TPDS},
  author       = {Chengjian Liu and Qiang Wang and Xiaowen Chu and Yiu-Wing Leung and Hai Liu},
  doi          = {10.1109/TPDS.2020.2983411},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2001-2016},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ESetStore: An erasure-coded storage system with fast data recovery},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A black-box fork-join latency prediction model for
data-intensive applications. <em>TPDS</em>, <em>31</em>(9), 1983–2000.
(<a href="https://doi.org/10.1109/TPDS.2020.2982137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The workflows of the predominant datacenter services are underlaid by various Fork-Join structures. Due to the lack of good understanding of the performance of Fork-Join structures in general, today&#39;s datacenters often operate under low resource utilization to meet stringent service level objectives (SLOs), e.g., in terms of tail and/or mean latency, for such services. Hence, to achieve high resource utilization, while meeting stringent SLOs, it is of paramount importance to be able to accurately predict the tail and/or mean latency for a broad range of Fork-Join structures of practical interests. In this article, we propose a black-box Fork-Join model that covers a wide range of Fork-Join structures for the prediction of tail and mean latency, called ForkTail and ForkMean, respectively. We derive highly computational effective, empirical expressions for tail and mean latency as functions of means and variances of task response times. Our extensive testing results based on model-based and trace-driven simulations, as well as a real-world case study in a cloud environment demonstrate that the models can consistently predict the tail and mean latency within 20 and 15 percent prediction errors at 80 and 90 percent load levels, respectively, for heavy-tailed workloads, and at any load levels for light-tailed workloads. Moreover, our sensitivity analysis demonstrates that such errors can be well compensated for with no more than 7 percent resource overprovisioning. Consequently, the proposed prediction model can be used as a powerful tool to aid the design of tail-and-mean-latency guaranteed job scheduling and resource provisioning, especially at high load, for datacenter applications.},
  archive      = {J_TPDS},
  author       = {Minh Nguyen and Sami Alesawi and Ning Li and Hao Che and Hong Jiang},
  doi          = {10.1109/TPDS.2020.2982137},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {1983-2000},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A black-box fork-join latency prediction model for data-intensive applications},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bandwidth-aware dynamic prefetch configuration for IBM
POWER8. <em>TPDS</em>, <em>31</em>(8), 1970–1982. (<a
href="https://doi.org/10.1109/TPDS.2020.2982392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced hardware prefetch engines are being integrated in current high-performance processors. Prefetching can boost the performance of most applications, however, the induced bandwidth consumption can lead the system to a high contention for main memory bandwidth, which is a scarce resource in current multicores. In such a case, the system performance can be severely damaged. This article characterizes the applications’ behavior in an IBM POWER8 machine, which presents many prefetch settings, varying the bandwidth contention. The study reveals that the best prefetch setting for each application depends on the main memory bandwidth availability, that is, it depends on the co-running applications. Based on this study, we propose Bandwidth-Aware Prefetch Configuration (BAPC) a scalable adaptive prefetching algorithm that improves the performance of multi-program workloads. BAPC increases the performance of the applications in a 12, 15, and 16 percent of 6-, 8-, and 10-application workloads over the IBM POWER8 default configuration. In addition, BAPC reduces bandwidth consumption in 39, 42, and 45 percent, respectively.},
  archive      = {J_TPDS},
  author       = {Carlos Navarro and Josué Feliu and Salvador Petit and Maria E. Gómez and Julio Sahuquillo},
  doi          = {10.1109/TPDS.2020.2982392},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1970-1982},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Bandwidth-aware dynamic prefetch configuration for IBM POWER8},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Location-aware and budget-constrained service deployment for
composite applications in multi-cloud environment. <em>TPDS</em>,
<em>31</em>(8), 1954–1969. (<a
href="https://doi.org/10.1109/TPDS.2020.2981306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise application providers are increasingly moving their workloads to the cloud for technical and economic benefits. Multi-cloud environment makes it possible to orchestrate multiple cloud resources. With the increasing number of available cloud resources provided by multiple cloud providers at different locations with different prices, application providers face the challenge to select proper cloud resources to deploy their applications in the form of a workflow of component service units. Existing studies usually consider minimizing execution time or/and deployment cost. From the perspective of application providers, however, they also pay huge attention to application response time, including particularly network latency between deployed services and users. Meanwhile, application deployment is often subject to stringent budgetary control to ensure financial viability. This article studies a new type of composite application deployment problem that jointly considers both the performance optimization and budget control in multi-cloud at the global scale. To find solutions with minimal response time without running into the risk of over-spending, we propose a hybrid GA-based approach, featuring new design of domain-tailored service clustering, repair algorithm, solution representation, population initialization, and genetic operators. Extensive experiments using the real-world dataset demonstrate that our proposed hybrid GA approach outperforms some recently proposed approaches.},
  archive      = {J_TPDS},
  author       = {Tao Shi and Hui Ma and Gang Chen and Sven Hartmann},
  doi          = {10.1109/TPDS.2020.2981306},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1954-1969},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Location-aware and budget-constrained service deployment for composite applications in multi-cloud environment},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fireplug: Efficient and robust geo-replication of graph
databases. <em>TPDS</em>, <em>31</em>(8), 1942–1953. (<a
href="https://doi.org/10.1109/TPDS.2020.2981019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although graph-databases have been assuming an increasing relevance in applications that exhibit strong dependability requirements, including tolerance to malicious faults, few works have addressed Byzantine fault tolerance in this particular context, and previous attempts suffer from lack of flexibility and poor performance. This article describes and evaluates Fireplug, a flexible architecture to build robust geo-replicated graph databases. Fireplug can be configured to tolerate from crash to Byzantine faults, both within and across different datacenters. Furthermore, Fireplug is robust to bugs in existing graph database implementations, as it allows to combine multiple graph database instances in a cohesive manner. Thus, Fireplug can support many different deployments, according to the performance/robustness trade-offs imposed by the target application. Our evaluation shows that Fireplug is able implement Byzantine fault tolerance without penalty when compared to the built-in replication mechanism of Neo4j, which only supports crash faults. Additionally, performance optimizations introduced by Fireplug improve the overall performance by up to 900 percent in geo-replicated scenarios.},
  archive      = {J_TPDS},
  author       = {Ray Neiheiser and Luciana Rech and Manuel Bravo and Luís Rodrigues and Miguel Correia},
  doi          = {10.1109/TPDS.2020.2981019},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1942-1953},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fireplug: Efficient and robust geo-replication of graph databases},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic generation of high-performance FFT kernels on arm
and x86 CPUs. <em>TPDS</em>, <em>31</em>(8), 1925–1941. (<a
href="https://doi.org/10.1109/TPDS.2020.2977629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents AutoFFT, a template-based code generation framework that can automatically generate high-performance FFT kernels for all natural-number radices. AutoFFT is based on the Cooley-Tukey FFT algorithm, which exploits the symmetric and periodic properties of the DFT matrix, as the outer parallelization framework. Because butterflies are the core operations of the Cooley-Tukey algorithm, we explore additional symmetric and periodic properties of the DFT matrix and formulate multiple optimized calculation templates to further reduce the number of floating-point operations for butterflies of arbitrary natural numbers. To fully exploit hardware resources, we encapsulate a series of optimizations in an assembly template optimizer. Given any DFT problem, AutoFFT automatically generates C FFT kernels using these calculation templates and converts them into efficient assembly kernels using the template optimizer. Through a series of experiments on Arm, Intel, and AMD processors, we show that AutoFFT-generated kernels can outperform those in Fastest Fourier Transform in the West (FFTW), the Arm Performance Libraries (ARMPL), and the Intel Math Kernel Library (MKL).},
  archive      = {J_TPDS},
  author       = {Zhihao Li and Haipeng Jia and Yunquan Zhang and Tun Chen and Liang Yuan and Richard Vuduc},
  doi          = {10.1109/TPDS.2020.2977629},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1925-1941},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Automatic generation of high-performance FFT kernels on arm and x86 CPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). T-BASIR: Finding shutdown bugs for cloud-based applications
in cloud spot markets. <em>TPDS</em>, <em>31</em>(8), 1912–1924. (<a
href="https://doi.org/10.1109/TPDS.2020.2980265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major advantages of cloud spot instances in cloud computing is to allow stakeholders to economically deploy their applications at much lower costs than that of other types of cloud instances. In exchange, spot instances are often exposed to revocations (i.e., terminations) by cloud providers. With spot instances becoming pervasive, terminations have become a part of the normal behavior of cloud-based applications; thus, these applications may be left in an incorrect state leading to certain bugs. Unfortunately, these applications are not designed or tested to deal with this behavior in the cloud environment, and as a result, the advantages of cloud spot instances could be significantly minimized or even entirely negated. We propose a novel solution to automatically find these bugs and locate their causes in the source code. We evaluate our solution using 10 popular open-source applications. The results show that our solution not only finds more instances and different types of these bugs compared to the random approach, but it also locates the causes of these bugs to help developers improve the design of the shutdown process and is more efficient in finding instances of these bugs since it interposes at the system call layer.},
  archive      = {J_TPDS},
  author       = {Abdullah Alourani and Ajay D. Kshemkalyani and Mark Grechanik},
  doi          = {10.1109/TPDS.2020.2980265},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1912-1924},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {T-BASIR: Finding shutdown bugs for cloud-based applications in cloud spot markets},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating stochastic gradient descent based matrix
factorization on FPGA. <em>TPDS</em>, <em>31</em>(8), 1897–1911. (<a
href="https://doi.org/10.1109/TPDS.2020.2974744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix Factorization (MF) based on Stochastic Gradient Descent (SGD) is a powerful machine learning technique to derive hidden features of objects from observations. In this article, we design a highly parallel architecture based on Field-Programmable Gate Array (FPGA) to accelerate the training process of the SGD-based MF algorithm. We identify the challenges for the acceleration and propose novel algorithmic optimizations to overcome them. By transforming the SGD-based MF algorithm into a bipartite graph processing problem, we propose a 3-level hierarchical partitioning scheme that enables conflict-minimizing scheduling and processing of edges to achieve significant speedup. First, we develop a fast heuristic graph partitioning approach to partition the bipartite graph into induced subgraphs; this enables to efficiently use the on-chip memory resources of FPGA for data reuse and completely hide the data communication between FPGA and external memory. Second, we partition all the edges of each subgraph into non-overlapping matchings to extract the maximum parallelism. Third, we propose a batching algorithm to schedule the execution of the edges inside each matching to reduce the memory access conflicts to the on-chip RAMs of FPGA. Compared with non-optimized FPGA-based baseline designs, the proposed optimizations result in up to 60× data dependency reduction, 4.2× bank conflict reduction, and 15.4× speedup. We evaluate the performance of our design using a state-of-the-art FPGA device. Experimental results show that our FPGA accelerator sustains a high computing throughput of up to 217 billion floating-point operations per second (GFLOPS) for training very large real-life sparse matrices. Compared with highly-optimized GPU-based accelerators, our FPGA accelerator achieves up to 12.7× speedup. Based on our optimization methodology, we also implement a software-based design on a multi-core platform, which demonstrates 1.3× speedup compared with the state-of-the-art multi-core implementation.},
  archive      = {J_TPDS},
  author       = {Shijie Zhou and Rajgopal Kannan and Viktor K. Prasanna},
  doi          = {10.1109/TPDS.2020.2974744},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1897-1911},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating stochastic gradient descent based matrix factorization on FPGA},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing streaming parallelism on heterogeneous many-core
architectures. <em>TPDS</em>, <em>31</em>(8), 1878–1896. (<a
href="https://doi.org/10.1109/TPDS.2020.2978045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As many-core accelerators keep integrating more processing units, it becomes increasingly more difficult for a parallel application to make effective use of all available resources. An effective way of improving hardware utilization is to exploit spatial and temporal sharing of the heterogeneous processing units by multiplexing computation and communication tasks - a strategy known as heterogeneous streaming. Achieving effective heterogeneous streaming requires carefully partitioning hardware among tasks, and matching the granularity of task parallelism to the resource partition. However, finding the right resource partitioning and task granularity is extremely challenging, because there is a large number of possible solutions and the optimal solution varies across programs and datasets. This article presents an automatic approach to quickly derive a good solution for hardware resource partition and task granularity for task-based parallel applications on heterogeneous many-core architectures. Our approach employs a performance model to estimate the resulting performance of the target application under a given resource partition and task granularity configuration. The model is used as a utility to quickly search for a good configuration at runtime. Instead of hand-crafting an analytical model that requires expert insights into low-level hardware details, we employ machine learning techniques to automatically learn it. We achieve this by first learning a predictive model offline using training programs. The learned model can then be used to predict the performance of any unseen program at runtime. We apply our approach to 39 representative parallel applications and evaluate it on two representative heterogeneous many-core platforms: a CPU-XeonPhi platform and a CPU-GPU platform. Compared to the single-stream version, our approach achieves, on average, a 1.6x and 1.1x speedup on the XeonPhi and the GPU platform, respectively. These results translate to over 93 percent of the performance delivered by a theoretically perfect predictor.},
  archive      = {J_TPDS},
  author       = {Peng Zhang and Jianbin Fang and Canqun Yang and Chun Huang and Tao Tang and Zheng Wang},
  doi          = {10.1109/TPDS.2020.2978045},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1878-1896},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing streaming parallelism on heterogeneous many-core architectures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analyzing the performance trade-off in implementing
user-level threads. <em>TPDS</em>, <em>31</em>(8), 1859–1877. (<a
href="https://doi.org/10.1109/TPDS.2020.2976057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-level threads have been widely adopted as a means of achieving lightweight concurrent execution without the costs of OS-level threads. Nevertheless, the costs of managing user-level threads represent a performance barrier that dictates how fine grained the concurrency exposed by an application can be without incurring significant overheads; this in turn may translate into insufficient parallelism to exploit highly parallel systems. This article is a deep dive into the fundamental costs in implementing user-level threads. We first identify that one of the highest sources of fork-join overheads stems from deviations, events that incur context switching during the execution of a thread and disrupt a run-to-completion execution. We then conduct an in-depth investigation of a wide spectrum of methods with respect to how they handle deviations while covering both parent- and child-first scheduling policies. Our methodology involves a comprehensive instruction- and cache-level analysis of all methods on several modern CPU architectures. The primary finding of our evaluation is that dynamic promotion methods that assume the absence of deviation and dynamically provide context-switching support offer the best trade-off between performance and capability when the likelihood of deviation is low.},
  archive      = {J_TPDS},
  author       = {Shintaro Iwasaki and Abdelhalim Amer and Kenjiro Taura and Pavan Balaji},
  doi          = {10.1109/TPDS.2020.2976057},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1859-1877},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Analyzing the performance trade-off in implementing user-level threads},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of stream processing frameworks. <em>TPDS</em>,
<em>31</em>(8), 1845–1858. (<a
href="https://doi.org/10.1109/TPDS.2020.2978480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing need for real-time insights in data sparked the development of multiple stream processing frameworks. Several benchmarking studies were conducted in an effort to form guidelines for identifying the most appropriate framework for a use case. In this article, we extend this research and present the results gathered. In addition to Spark Streaming and Flink, we also include the emerging frameworks Structured Streaming and Kafka Streams. We define four workloads with custom parameter tuning. Each of these is optimized for a certain metric or for measuring performance under specific scenarios such as bursty workloads. We analyze the relationship between latency, throughput, and resource consumption and we measure the performance impact of adding different common operations to the pipeline. To ensure correct latency measurements, we use a single Kafka broker. Our results show that the latency disadvantages of using a micro-batch system are most apparent for stateless operations. With more complex pipelines, customized implementations can give event-driven frameworks a large latency advantage. Due to its micro-batch architecture, Structured Streaming can handle very high throughput at the cost of high latency. Under tight latency SLAs, Flink sustains the highest throughput. Additionally, Flink shows the least performance degradation when confronted with periodic bursts of data. When a burst of data needs to be processed right after startup, however, micro-batch systems catch up faster while event-driven systems output the first events sooner.},
  archive      = {J_TPDS},
  author       = {Giselle van Dongen and Dirk Van den Poel},
  doi          = {10.1109/TPDS.2020.2978480},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1845-1858},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Evaluation of stream processing frameworks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FULL-KV: Flexible and ultra-low-latency in-memory key-value
store system design on CPU-FPGA. <em>TPDS</em>, <em>31</em>(8),
1828–1444. (<a href="https://doi.org/10.1109/TPDS.2020.2973965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory key-value store (IMKVS) has gained great popularity in data centers. However, big data brings great challenges in performance and power consumption because of the general-purpose Von Neumann computer architecture. Remote direct memory access (RDMA) technology supporting zero-copy networking could partly alleviate the problem but is still not efficient for KVS. To overcome this problem, we present a flexible and ultra-low-latency IMKVS system named FULL-KV, based on a CPU-FPGA heterogeneous architecture. The FPGA serves as a KVS accelerator that can bypass the CPU and implement both the network stacks and the KVS processing with a highly parallel hardware architecture. The system latency of FULL-KV can achieve as low as 1.5μs/2.2μs for the PUT/GET operation, which is 3.0x/1.5x faster than current state-of-the-art hardware-based KVS systems. Besides, FULL-KV can support 4x larger values (up to 4M bytes). Given a total Ethernet bandwidth of 20Gbps, the peak throughput of the single-node FULL-KV can reach 26.0 million key-value operations per second (Mops). In the two-node test system with a commercial Ethernet switch, the peak throughput can reach 52Mops, manifesting the system scalability and practicability.},
  archive      = {J_TPDS},
  author       = {Yunhui Qiu and Jinyu Xie and Hankun Lv and Wenbo Yin and Wai-Shing Luk and Lingli Wang and Bowei Yu and Hua Chen and Xianjun Ge and Zhijian Liao and Xiaozhong Shi},
  doi          = {10.1109/TPDS.2020.2973965},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1828-1444},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FULL-KV: Flexible and ultra-low-latency in-memory key-value store system design on CPU-FPGA},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic consistency guarantee in partial quorum-based
data store. <em>TPDS</em>, <em>31</em>(8), 1815–1827. (<a
href="https://doi.org/10.1109/TPDS.2020.2973619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many NoSQL databases support quorum-based protocols, which require a subset of replicas (called a quorum) to respond to each write/read operation. These systems configure the quorum size to tune the operation latency and adopt multiple consistency levels. Some recent works illustrate that using probability models to quantify the chance of reading the last update is important because it could avoid returning stale values under eventual consistency. There are two challenging issues: (1) from inconsistent replicas, how to determine the minimum quorum size (i.e., the lowest access latency) to read the newest data at a specified probability; (2) node failure frequently happens in large-scale systems, how to guarantee the probability-based consistent reads. This article presents Probabilistic Consistency Guarantee (PCG), which is the first dynamic quorum decision and failure-aware quantification model. PCG model respectively quantifies the server-side consistency after the latest write, which reflects the object&#39;s time-varying update progress, and the possibility of reading this update when responding to the end-users. Our theoretical analysis derives several formulas to determine the quorum size of a read quorum and the consensus result selected from this quorum is the data updated by the last write at the user-specified probability. When some replicas are unavailable, our model knows how to rescale the quorum and read values from surviving replicas could reduce the stale reads caused by node failures. The experimental results in Cassandra demonstrate that the PCG model can achieve up to 77.7 percent more accurate predictions and reduce up to 48.9 percent read latency than those of the previous model.},
  archive      = {J_TPDS},
  author       = {Xin Yao and Cho-Li Wang},
  doi          = {10.1109/TPDS.2020.2973619},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1815-1827},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Probabilistic consistency guarantee in partial quorum-based data store},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local-density subspace distributed clustering for
high-dimensional data. <em>TPDS</em>, <em>31</em>(8), 1799–1814. (<a
href="https://doi.org/10.1109/TPDS.2020.2975550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed clustering is emerging along with the advent of the era of big data. However, most existing established distributed clustering methods focus on problems caused by a large amount of data rather than caused by the large dimension of data. Consequently, they suffer the “curse” of dimensionality (e.g., poor performance and heavy network overhead) when high-dimensional (HD) data are clustered. In this article, we propose a distributed algorithm, referred to as Local Density Subspace Distributed Clustering (LDSDC) algorithm, to cluster large-scale HD data, motivated by the idea that a local dense region of a HD dataset is usually distributed in a low-dimensional (LD) subspace. LDSDC follows a local-global-local processing structure, including grouping of local dense regions (atom clusters) followed by subspace Gaussian model (SGM) fitting (flexible and scalable to data dimension) at each sub-site, merging of atom clusters at every sub-site according to the merging result broadcast from the global site. Moreover, we propose a fast method to estimate the parameters of SGM for HD data, together with its convergence proof. We evaluate LDSDC on both synthetic and real datasets and compare it with four state-of-the-art methods. The experimental results demonstrate that the proposed LDSDC yields best overall performance.},
  archive      = {J_TPDS},
  author       = {Yangli-ao Geng and Qingyong Li and Mingfei Liang and Chong-Yung Chi and Juan Tan and Heng Huang},
  doi          = {10.1109/TPDS.2020.2975550},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1799-1814},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Local-density subspace distributed clustering for high-dimensional data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power guarantee for electric systems using real-time
scheduling. <em>TPDS</em>, <em>31</em>(8), 1783–1798. (<a
href="https://doi.org/10.1109/TPDS.2020.2977041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern electric systems, such as electric vehicles, mobile robots, nano satellites, and drones, require to support various power-demand operations for user applications and system maintenance. This, in turn, calls for advanced power management that jointly considers power demand by the operations and power supply from various sources, such as batteries, solar panels, and supercapacitors. In this article, we develop a power scheduling framework for a reliable energy storage system with multiple power-supply sources and multiple power-demand operations. Specifically, we develop offline power-supply guarantee analysis and online power management. The former provides an offline power-supply guarantee such that every power-demand operation completes its execution in time while the sum of power required by individual operations does not exceed the total power supplied by the entire energy storage system at any time; to this end, we develop a plain power-supply analysis as well as its improved version using real-time scheduling techniques. On the other hand, the latter efficiently utilizes the surplus power available at runtime for improving system performance; we propose two approaches, depending on whether future scheduling information of power-demanding tasks is available or not. For evaluation, we perform simulations to evaluate both the plain and improved analyses for offline power guarantee under various synthetic power-demand operations. In addition, we have built a simulation model and demonstrated that the proposed framework with the offline analysis and online management not only guarantees the required power-supply, but also enhances system performance by up to 56.49 percent.},
  archive      = {J_TPDS},
  author       = {Eugene Kim and Youngmoon Lee and Liang He and Kang G. Shin and Jinkyu Lee},
  doi          = {10.1109/TPDS.2020.2977041},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1783-1798},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Power guarantee for electric systems using real-time scheduling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid update strategy for i/o-efficient out-of-core graph
processing. <em>TPDS</em>, <em>31</em>(8), 1767–1782. (<a
href="https://doi.org/10.1109/TPDS.2020.2973143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a number of out-of-core graph processing systems have been proposed to process graphs with billions of edges on just one commodity computer, due to their high cost efficiency. To obtain a better performance, these systems adopt a full I/O model that scans all edges during the computation to avoid the inefficiency of random I/Os. Although this model ensures good I/O access locality, it leads to a large number of useless edges to be loaded when running graph algorithms that only access a small portion of edges in each iteration. An intuitive method to solve this I/O inefficiency problem is the on-demand I/O model that only accesses the active edges. However, this method only works well for the graph algorithms with very few active edges, since the I/O cost will grow rapidly as the number of active edges increases due to the increasing amount of random I/Os. In this article, we present HUS-Graph, an efficient out-of-core graph processing system to address the above I/O issues and achieve a good balance between I/O traffic and I/O access locality. HUS-Graph adopts a hybrid update strategy including two update models, Row-oriented Push (ROP) and Column-oriented Pull (COP). It supports switching between ROP and COP adaptively, for the graph algorithms that have different computation and I/O features. For traversal-based algorithms, HUS-Graph also provides an immediate propagation-based vertex update scheme to accelerate the vertex state propagation and convergence speed. Furthermore, HUS-Graph adopts a locality-optimized dual-block representation to organize graph data and an I/O-based performance prediction method to enable the system to dynamically select the optimal update model between ROP and COP. To save the disk space and further reduce I/O traffic, HUS-Graph implements a space-efficient storage format by combining several graph compression methods. Extensive experimental results show that HUS-Graph outperforms two existing out-of-core systems GraphChi and GridGraph by 1.2x-52.8x.},
  archive      = {J_TPDS},
  author       = {Xianghao Xu and Fang Wang and Hong Jiang and Yongli Cheng and Dan Feng and Yongxuan Zhang},
  doi          = {10.1109/TPDS.2020.2973143},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1767-1782},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A hybrid update strategy for I/O-efficient out-of-core graph processing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating federated learning via momentum gradient
descent. <em>TPDS</em>, <em>31</em>(8), 1754–1766. (<a
href="https://doi.org/10.1109/TPDS.2020.2975189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) provides a communication-efficient approach to solve machine learning problems concerning distributed data, without sending raw data to a central server. However, existing works on FL only utilize first-order gradient descent (GD) and do not consider the preceding iterations to gradient update which can potentially accelerate convergence. In this article, we consider momentum term which relates to the last iteration. The proposed momentum federated learning (MFL) uses momentum gradient descent (MGD) in the local update step of FL system. We establish global convergence properties of MFL and derive an upper bound on MFL convergence rate. Comparing the upper bounds on MFL and FL convergence rates, we provide conditions in which MFL accelerates the convergence. For different machine learning models, the convergence performance of MFL is evaluated based on experiments with MNIST and CIFAR-10 datasets. Simulation results confirm that MFL is globally convergent and further reveal significant convergence improvement over FL.},
  archive      = {J_TPDS},
  author       = {Wei Liu and Li Chen and Yunfei Chen and Wenyi Zhang},
  doi          = {10.1109/TPDS.2020.2975189},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1754-1766},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating federated learning via momentum gradient descent},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crocus: Enabling computing resource orchestration for inline
cluster-wide deduplication on scalable storage systems. <em>TPDS</em>,
<em>31</em>(8), 1740–1753. (<a
href="https://doi.org/10.1109/TPDS.2020.2972882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inline deduplication dramatically improves storage space utilization. However, it degrades I/O throughput due to computeintensive deduplication operations such as chunking, fingerprinting or hashing of chunk content, and redundant lookup I/Os over the network in the I/O path. In particular, the fingerprint or hash generation of content contributes largely to the degraded I/O throughput and is computationally expensive. In this article, we propose CROCUS, a framework that enables compute resource orchestration to enhance cluster-wide deduplication performance. In particular, CROCUS takes into account all compute resources such as local and remote {CPU, GPU} by managing decentralized compute pools. An opportunistic Load-Aware Fingerprint Scheduler (LAFS), distributes and offloads compute-intensive deduplication operations in a load-aware fashion to compute pools. CROCUS is highly generic and can be adopted in both inline and offline deduplication with different storage tier configurations. We implemented CROCUS in Ceph scale-out storage system. Our extensive evaluation shows that CROCUS reduces the fingerprinting overhead by 86 percent with 4KB chunk size compared to Ceph with baseline deduplication while maintaining high disk-space savings. Our proposed LAFS scheduler, when tested in different internal and external contention scenarios also showed 54 percent improvement over a fixed or static scheduling approach.},
  archive      = {J_TPDS},
  author       = {Prince Hamandawana and Awais Khan and Chang-Gyu Lee and Sungyong Park and Youngjae Kim},
  doi          = {10.1109/TPDS.2020.2972882},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1740-1753},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Crocus: Enabling computing resource orchestration for inline cluster-wide deduplication on scalable storage systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High performance GPU tensor completion with tubal-sampling
pattern. <em>TPDS</em>, <em>31</em>(7), 1724–1739. (<a
href="https://doi.org/10.1109/TPDS.2020.2975196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data completion is a problem of filling missing or unobserved elements of partially observed datasets. Data completion algorithms have received wide attention and achievements in diverse domains including data mining, signal processing, and computer vision. We observe a ubiquitous tubal-sampling pattern in big data and Internet of Things (IoT) applications, which is introduced by many reasons such as high data acquisition cost, downsampling for data compression, sensor node failures, and packet losses in low-power wireless transmissions. To meet the time and accuracy requirements of applications, data completion methods are expected to be accurate as well as fast. However, the existing methods for data completion with the tubal-sampling pattern are either accurate or fast, but not both. In this article, we propose high-performance graphics processing unit (GPU) tensor completion for data completion with the tubal-sampling pattern. First, by exploiting the convolution theorem, we split a tensor least-squares minimization problem into multiple least-squares sub-problems in the frequency domain. In this way, massive parallelisms are exposed for many-core GPU architectures while still preserving high recovery accuracy. Second, we propose computing slice-level and tube-level tasks in batches to improve GPU utilization. Third, we reduce the data transfer cost by eliminating the accesses to the CPU memory inside algorithm loop structures. The experimental results show that the proposed tensor completion is both fast and accurate. Using synthetic data of varying sizes, the proposed GPU tensor completion achieves maximum 248.18×, 7, 403.27×, and 33.27× speedups over the CPU MATLAB implementation, GPU element-sampling tensor completion in the cuTensor-tubal library, and GPU high-performance matrix completion, respectively. With a 50 percent sampling rate, the proposed GPU tensor completion achieves a recovery error of 1.40e-5, which is comparable with that of the GPU element-sampling tensor completion and three orders of magnitude better than that of the GPU high-performance matrix completion. To utilize multiple GPUs in servers, we design a multi-GPU scheme for tubal-sampling tensor completion. The multi-GPU tensor completion achieves maximum 1.89× speedup on two GPUs versus on a single GPU for medium or big tensors. We further evaluate the performance of the proposed GPU tensor completion in three real applications, namely, video transmission in wireless camera networks, RF fingerprint-based indoor localization, and seismic data completion, and it achieves maximum speedups of 448.68×, 24.63×, and 311.54×, respectively. We integrate this high-performance GPU tensor completion implementation into the cuTensor-tubal library to support various applications.},
  archive      = {J_TPDS},
  author       = {Tao Zhang and Xiao-Yang Liu and Xiaodong Wang},
  doi          = {10.1109/TPDS.2020.2975196},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1724-1739},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High performance GPU tensor completion with tubal-sampling pattern},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cost-aware partitioning for efficient large graph processing
in geo-distributed datacenters. <em>TPDS</em>, <em>31</em>(7),
1707–1723. (<a href="https://doi.org/10.1109/TPDS.2019.2955494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph processing is an emerging computation model for a wide range of applications and graph partitioning is important for optimizing the cost and performance of graph processing jobs. Recently, many graph applications store their data on geo-distributed datacenters (DCs) to provide services worldwide with low latency. This raises new challenges to existing graph partitioning methods, due to the multi-level heterogeneities in network bandwidth and communication prices in geo-distributed DCs. In this article, we propose an efficient graph partitioning method named Geo-Cut, which takes both the cost and performance objectives into consideration for large graph processing in geo-distributed DCs. Geo-Cut adopts two optimization stages. First, we propose a cost-aware streaming heuristic and utilize the one-pass streaming graph partitioning method to quickly assign edges to different DCs while minimizing inter-DC data communication cost. Second, we propose two partition refinement heuristics which identify the performance bottlenecks of geo-distributed graph processing and refine the partitioning result obtained in the first stage to reduce the inter-DC data transfer time while satisfying the budget constraint. Geo-Cut can be also applied to partition dynamic graphs thanks to its lightweight runtime overhead. We evaluate the effectiveness and efficiency of Geo-Cut using real-world graphs with both real geo-distributed DCs and simulations. Evaluation results show that Geo-Cut can reduce the inter-DC data transfer time by up to 79 percent (42 percent as the median) and reduce the monetary cost by up to 75 percent (26 percent as the median) compared to state-of-the-art graph partitioning methods with a low overhead.},
  archive      = {J_TPDS},
  author       = {Amelie Chi Zhou and Bingkun Shen and Yao Xiao and Shadi Ibrahim and Bingsheng He},
  doi          = {10.1109/TPDS.2019.2955494},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1707-1723},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cost-aware partitioning for efficient large graph processing in geo-distributed datacenters},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exact distributed load centrality computation: Algorithms,
convergence, and applications to distance vector routing. <em>TPDS</em>,
<em>31</em>(7), 1693–1706. (<a
href="https://doi.org/10.1109/TPDS.2020.2973960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many optimization techniques for networking protocols take advantage of topological information to improve performance. Often, the topological information at the core of these techniques is a centrality metric such as the Betweenness Centrality (BC) index. BC is, in fact, a centrality metric with many well-known successful applications documented in the literature, from resource allocation to routing. To compute BC, however, each node must run a centralized algorithm and needs to have the global topological knowledge; such requirements limit the feasibility of optimization procedures based on BC. To overcome restrictions of this kind, we present a novel distributed algorithm that requires only local information to compute an alternative similar metric, called Load Centrality (LC). We present the new algorithm together with a proof of its convergence and the analysis of its time complexity. The proposed algorithm is general enough to be integrated with any distance vector (DV) routing protocol. In support of this claim, we provide an implementation on top of Babel, a real-world DV protocol. We use this implementation in an emulation framework to show how LC can be exploited to reduce Babel&#39;s convergence time upon node failure, without increasing control overhead. As a key step towards the adoption of centrality-based optimization for routing, we study how the algorithm can be incrementally introduced in a network running a DV routing protocol. We show that even when only a small fraction of nodes participate in the protocol, the algorithm accurately ranks nodes according to their centrality.},
  archive      = {J_TPDS},
  author       = {Leonardo Maccari and Lorenzo Ghiro and Alessio Guerrieri and Alberto Montresor and Renato Lo Cigno},
  doi          = {10.1109/TPDS.2020.2973960},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1693-1706},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exact distributed load centrality computation: Algorithms, convergence, and applications to distance vector routing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Replica exchange MCMC hardware with automatic temperature
selection and parallel trial. <em>TPDS</em>, <em>31</em>(7), 1681–1692.
(<a href="https://doi.org/10.1109/TPDS.2020.2972359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A replica exchange Markov Chain Monte Carlo (MCMC) engine is developed with automatic temperature adjustment for solving combinatorial optimization problems by minimizing the energy of the Ising model. The automatic temperature adjustment scheme ensures that the MCMC process is optimized at every stage of the execution. This approach is performed by dynamically adjusting temperatures of all replicas, based on the properties of any given problem, in addition to the capability of automatically inserting new replicas or removing any existing replicas to achieve the best possible resource efficiency and execution time. The proposed algorithm is integrated with parallel evaluation of energy increment and update scheme. The engine is implemented on the FPGA platform with a capacity of running up to 42 replicas in pipeline, each running 1024 fully-connected Ising spins in parallel. The performance of the hardware is examined with three different classes of problems, Vertex Cover, Maximum-Cut, and Travelling Salesman using the engine in three modes, simulated annealing, with replica exchange while the adjustments are turned on or off. Up to 16x speedup is observed by turning on the replica exchange capability in addition to the advantage of eliminating the challenging process of finding an optimal annealing schedule for simulated annealing process.},
  archive      = {J_TPDS},
  author       = {Keivan Dabiri and Mehrdad Malekmohammadi and Ali Sheikholeslami and Hirotaka Tamura},
  doi          = {10.1109/TPDS.2020.2972359},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1681-1692},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Replica exchange MCMC hardware with automatic temperature selection and parallel trial},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance optimization for relative-error-bounded lossy
compression on scientific data. <em>TPDS</em>, <em>31</em>(7),
1665–1680. (<a href="https://doi.org/10.1109/TPDS.2020.2972548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific simulations in high-performance computing (HPC) environments generate vast volume of data, which may cause a severe I/O bottleneck at runtime and a huge burden on storage space for postanalysis. Unlike traditional data reduction schemes such as deduplication or lossless compression, not only can error-controlled lossy compression significantly reduce the data size but it also holds the promise to satisfy user demand on error control. Pointwise relative error bounds (i.e., compression errors depends on the data values) are widely used by many scientific applications with lossy compression since error control can adapt to the error bound in the dataset automatically. Pointwise relative-error-bounded compression is complicated and time consuming. We develop efficient precomputation-based mechanisms based on the SZ lossy compression framework. Our mechanisms can avoid costly logarithmic transformation and identify quantization factor values via a fast table lookup, greatly accelerating the relative-error-bounded compression with excellent compression ratios. In addition, we reduce traversing operations for Huffman decoding, significantly accelerating the decompression process in SZ. Experiments with eight well-known real-world scientific simulation datasets show that our solution can improve the compression and decompression rates (i.e., the speed) by about 40 and 80 p, respectively, in most of cases, making our designed lossy compression strategy the best-in-class solution in most cases.},
  archive      = {J_TPDS},
  author       = {Xiangyu Zou and Tao Lu and Wen Xia and Xuan Wang and Weizhe Zhang and Haijun Zhang and Sheng Di and Dingwen Tao and Franck Cappello},
  doi          = {10.1109/TPDS.2020.2972548},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1665-1680},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performance optimization for relative-error-bounded lossy compression on scientific data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving restore performance of packed datasets in
deduplication systems via reducing persistent fragmented chunks.
<em>TPDS</em>, <em>31</em>(7), 1651–1664. (<a
href="https://doi.org/10.1109/TPDS.2020.2972898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication, though being efficient for redundancy elimination in storage systems, introduces chunk fragmentation which severely decreases restore performance. Rewriting algorithms are proposed to reduce the chunk fragmentation. Typically, the backup software aggregates files into larger “tar” type files for storage. We observe that, in tar type datasets, a large number of Persistent Fragmented Chunks (PFCs) are repeatedly rewritten by state-of-the-art rewriting algorithms in every backup, which severely impacts restore performance. We found that the existence of PFCs is due to the traditional strategy of storing PFCs along with other chunks in the containers to preserve the stream locality, rendering them always stored in the containers with low utilization. We propose DePFC to reduce PFCs. DePFC identifies and removes PFCs from the containers preserving the stream locality, and groups them together, to increase the utilization of containers holding them for the subsequent backup, thus preventing them from being rewritten again. We further propose an FC Buffer to avoid mistaken rewrites of PFCs and grouping PFCs that cause restore cache thrashing together. Experimental results demonstrate that DePFC improves restore performance of state-of-the-art rewriting algorithms by 44.24-89.42 percent, while attaining comparable deduplication efficiency, and FC Buffer further improves restore performance.},
  archive      = {J_TPDS},
  author       = {Yucheng Zhang and Min Fu and Xinyun Wu and Fang Wang and Qiang Wang and Chunzhi Wang and Xinhua Dong and Hongmu Han},
  doi          = {10.1109/TPDS.2020.2972898},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1651-1664},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving restore performance of packed datasets in deduplication systems via reducing persistent fragmented chunks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating sparse cholesky factorization on sunway
manycore architecture. <em>TPDS</em>, <em>31</em>(7), 1636–1650. (<a
href="https://doi.org/10.1109/TPDS.2019.2953852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the performance of sparse Cholesky factorization, existing research divides the adjacent columns of the sparse matrix with the same nonzero patterns into supernodes for parallelization. However, due to the various structures of sparse matrices, the computation of the generated supernodes varies significantly, and thus hard to optimize when computed by dense matrix kernels. Therefore, how to efficiently map sparse Choleksy factorization to the emerging architectures, such as Sunway many-core processor, remains an active research direction. In this article, we propose swCholesky, which is a highly optimized implementation of sparse Cholesky factorization on Sunway processor. Specifically, we design three kernel task queues and a dense matrix library to dynamically adapt to the kernel characteristics and architecture features. In addition, we propose an auto-tuning mechanism to search for the optimal settings of the important parameters in swCholesky. Our experiments show that swCholesky achieves better performance than state-of-the-art implementations.},
  archive      = {J_TPDS},
  author       = {Mingzhen Li and Yi Liu and Hailong Yang and Zhongzhi Luan and Lin Gan and Guangwen Yang and Depei Qian},
  doi          = {10.1109/TPDS.2019.2953852},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1636-1650},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating sparse cholesky factorization on sunway manycore architecture},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compression ratio modeling and estimation across error
bounds for lossy compression. <em>TPDS</em>, <em>31</em>(7), 1621–1635.
(<a href="https://doi.org/10.1109/TPDS.2019.2938503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific simulations on high-performance computing (HPC) systems generate vast amounts of floating-point data that need to be reduced in order to lower the storage and I/O cost. Lossy compressors trade data accuracy for reduction performance and have been demonstrated to be effective in reducing data volume. However, a key hurdle to wide adoption of lossy compressors is that the trade-off between data accuracy and compression performance, particularly the compression ratio, is not well understood. Consequently, domain scientists often need to exhaust many possible error bounds before they can figure out an appropriate setup. The current practice of using lossy compressors to reduce data volume is, therefore, through trial and error, which is not efficient for large datasets which take a tremendous amount of computational resources to compress. This paper aims to analyze and estimate the compression performance of lossy compressors on HPC datasets. In particular, we predict the compression ratios of two modern lossy compressors that achieve superior performance, SZ and ZFP, on HPC scientific datasets at various error bounds, based upon the compressors&#39; intrinsic metrics collected under a given base error bound. We evaluate the estimation scheme using twenty real HPC datasets and the results confirm the effectiveness of our approach.},
  archive      = {J_TPDS},
  author       = {Jinzhen Wang and Tong Liu and Qing Liu and Xubin He and Huizhang Luo and Weiming He},
  doi          = {10.1109/TPDS.2019.2938503},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1621-1635},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Compression ratio modeling and estimation across error bounds for lossy compression},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combinatorial auctions for temperature-constrained resource
management in manycores. <em>TPDS</em>, <em>31</em>(7), 1605–1620. (<a
href="https://doi.org/10.1109/TPDS.2020.2965523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although manycore processors have plenty of cores, not all of them may run simultaneously at full speed and even some of them might need to be power-gated in order to keep the chip within safe temperature limits. Hence, a resource management technique, that allocates cores to application aiming at maximizing the system performance, will not be able to achieve its goal without taking into account the on-chip temperature and its impact on the availability of the chip&#39;s resources. However, considering a temperature constraint by the resource management will further increase its complexity, especially in manycores, and thus implementing it in a centralized scheme might lead to a computation bottleneck and a single point of failure. To avoid such scenarios, it is inevitable to distribute the computation required by the resource management technique throughout the chip. In this article, we propose a distributed resource management technique that considers temperature as an essential factor in allocating cores to applications and determining the power states of these cores and their voltage/frequency levels, while taking into account the performance models of the applications in order to maximize the overall system performance under a temperature constraint. Our proposed technique employs, for the first time, combinatorial auctions within an agent system to achieve the targeted goal in a distributed manner. The experimental evaluations show that our proposed technique achieves significant performance improvements with an average of 41\% compared to several distributed resource management techniques.},
  archive      = {J_TPDS},
  author       = {Heba Khdr and Muhammad Shafique and Santiago Pagani and Andreas Herkersdorf and Jörg Henkel},
  doi          = {10.1109/TPDS.2020.2965523},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1605-1620},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Combinatorial auctions for temperature-constrained resource management in manycores},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed graph computation meets machine learning.
<em>TPDS</em>, <em>31</em>(7), 1588–1604. (<a
href="https://doi.org/10.1109/TPDS.2020.2970047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TuX2 is a new distributed graph engine that bridges graph computation and distributed machine learning. TuX2 inherits the benefits of elegant graph computation model, efficient graph layout, and balanced parallelism to scale to billion-edge graphs, while extended and optimized for distributed machine learning to support heterogeneity in data model, Stale Synchronous Parallel in scheduling, and a new Mini-batch, Exchange, GlobalSync, and Apply ( MEGA ) model for programming. TuX2 further introduces a hybrid vertex-cut graph optimization and supports various consistency models in fault tolerance for machine learning. We have developed a set of representative distributed machine learning algorithms in TuX2 , covering both supervised and unsupervised learning. Compared to the implementations on distributed machine learning platforms, writing those algorithms in TuX2 takes only about 25 percent of the code: our graph computation model hides the detailed management of data layout, partitioning, and parallelism from developers. The extensive evaluation of TuX2 , using large datasets with up to 64 billion of edges, shows that TuX2 outperforms PowerGraph/PowerLyra, the state-of-the-art distributed graph engines, by an order of magnitude, while beating two state-of-the-art distributed machine learning systems by at least 60 percent.},
  archive      = {J_TPDS},
  author       = {Wencong Xiao and Jilong Xue and Youshan Miao and Zhen Li and Cheng Chen and Ming Wu and Wei Li and Lidong Zhou},
  doi          = {10.1109/TPDS.2020.2970047},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1588-1604},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed graph computation meets machine learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable and adaptive data replica placement for
geo-distributed cloud storages. <em>TPDS</em>, <em>31</em>(7),
1575–1587. (<a href="https://doi.org/10.1109/TPDS.2020.2968321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In geo-distributed cloud storage systems, data replication has been widely used to serve the ever more users around the world for high data reliability and availability. How to optimize the data replica placement has become one of the fundamental problems to reduce the inter-node traffic and the system overhead of accessing associated data items. In the big data era, traditional solutions may face the challenges of long running time and large overheads to handle the increasing scale of data items with time-varying user requests. Therefore, novel offline community discovery and online community adjustment schemes are proposed to solve the replica placement problem in a scalable and adaptive way. The offline scheme can find a replica placement solution based on the average read/write rates for a certain period of time. The scalability can be achieved as 1) the computation complexity is linear to the amount of data items and 2) the data-node communities can evolve in parallel for a distributed replica placement. Furthermore, the online scheme is adaptive to handle the bursty data requests, without the need to completely override the existing replica placement. Driven by real-world data traces, extensive performance evaluations demonstrate the effectiveness of our design to handle large-scale datasets.},
  archive      = {J_TPDS},
  author       = {Kaiyang Liu and Jun Peng and Jingrong Wang and Weirong Liu and Zhiwu Huang and Jianping Pan},
  doi          = {10.1109/TPDS.2020.2968321},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1575-1587},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scalable and adaptive data replica placement for geo-distributed cloud storages},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simplified workflow simulation on clouds based on
computation and communication noisiness. <em>TPDS</em>, <em>31</em>(7),
1559–1574. (<a href="https://doi.org/10.1109/TPDS.2020.2967662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many researchers rely on simulations to analyze and validate their researched methods on Cloud infrastructures. However, determining relevant simulation parameters and correctly instantiating them to match the real Cloud performance is a difficult and costly operation, as minor configuration changes can easily generate an unreliable inaccurate simulation result. Using legacy values experimentally determined by other researchers can reduce the configuration costs, but is still inaccurate as the underlying public Clouds and the number of active tenants are highly different and dynamic in time. To overcome these deficiencies, we propose a novel model that simulates the dynamic Cloud performance by introducing noise in the computation and communication tasks, determined by a small set of runtime execution data. Although the estimating method is apparently costly, a comprehensive sensitivity analysis shows that the configuration parameters determined for a certain simulation setup can be used for other simulations too, thereby reducing the tuning cost by up to 82.46 percent, while declining the simulation accuracy by only 1.98 percent on average. Extensive evaluation also shows that our novel model outperforms other state-of-the-art dynamic Cloud simulation models, leading up to 22 percent lower makespan inaccuracy.},
  archive      = {J_TPDS},
  author       = {Roland Mathá and Sasko Ristov and Thomas Fahringer and Radu Prodan},
  doi          = {10.1109/TPDS.2020.2967662},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1559-1574},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Simplified workflow simulation on clouds based on computation and communication noisiness},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reliability-aware network service provisioning in mobile
edge-cloud networks. <em>TPDS</em>, <em>31</em>(7), 1545–1558. (<a
href="https://doi.org/10.1109/TPDS.2020.2970048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Mobile Edge-Cloud (MEC) network has emerged as a promising networking paradigm to address the conflict between increasing computing-intensive applications and resource-constrained mobile Internet-of-Thing (IoT) devices with portable size and storage. In MEC environments, Virtualized Network Functions (VNFs) are deployed for provisioning network services to users to reduce the service cost on top of dedicated hardware infrastructures. However, VNFs may suffer from failures and malfunctions while network service providers have to guarantee continuously reliable services to their consumers to meet the ever-growing service demands of users, thereby securing their revenues for the service. In this article, we focus on reliable VNF service provisioning in MECs, by placing primary and backup VNF instances to cloudlets in an MEC network to meet the service reliability requirements of users. We first formulate a novel VNF service reliability problem with the aim to maximize the revenue collected by admitting as many as user requests while meeting their different reliability requirements, assuming that requests arrive into the system one by one without the knowledge of future arrivals, and the admission or rejection decision must be made immediately. We then develop two efficient online algorithms for the problem under two different backup schemes: the on-site (local) and off-site (remote) schemes, by adopting the primal-dual updating technique. Both algorithms achieve provable competitive ratios with bounded moderate resource capacity violations. We finally evaluate the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms are promising, compared with existing baseline algorithms.},
  archive      = {J_TPDS},
  author       = {Jing Li and Weifa Liang and Meitian Huang and Xiaohua Jia},
  doi          = {10.1109/TPDS.2020.2970048},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1545-1558},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reliability-aware network service provisioning in mobile edge-cloud networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partitioning tree-shaped task graphs for distributed
platforms with limited memory. <em>TPDS</em>, <em>31</em>(7), 1533–1544.
(<a href="https://doi.org/10.1109/TPDS.2020.2971200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific applications are commonly modeled as the processing of directed acyclic graphs of tasks, and for some of them, the graph takes the special form of a rooted tree. This tree expresses both the computational dependencies between tasks and their storage requirements. The problem of scheduling/traversing such a tree on a single processor to minimize its memory footprint has already been widely studied. The present article considers the parallel processing of such a tree and studies how to partition it for a homogeneous multiprocessor platform, where each processor is equipped with its own memory. We formally state the problem of partitioning the tree into subtrees, such that each subtree can be processed on a single processor (i.e., it must fit in memory), and the goal is to minimize the total resulting processing time. We prove that this problem is NP-complete, and we design polynomial-time heuristics to address it. An extensive set of simulations demonstrates the usefulness of these heuristics.},
  archive      = {J_TPDS},
  author       = {Changjiang Gou and Anne Benoit and Loris Marchal},
  doi          = {10.1109/TPDS.2020.2971200},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1533-1544},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Partitioning tree-shaped task graphs for distributed platforms with limited memory},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling analysis and cost-performance ratio optimization of
virtual machine scheduling in cloud computing. <em>TPDS</em>,
<em>31</em>(7), 1518–1532. (<a
href="https://doi.org/10.1109/TPDS.2020.2968913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential feature of cloud computing, dynamic scalability enables the cloud system to dynamically expand or shrink resources according to user needs at runtime. Effectively predicting and optimizing the cost and performance of cloud computing platforms have become one of the key research challenges in the field of cloud computing. In this article, to quantitatively predict the cost and performance of cloud computing platforms, we propose a cloud computing resource analysis model considering both hot/cold startup and hot/cold shutdown of virtual machines (VMs), and use the M/M/N/oo queuing model to analyze cloud computing platform and acquire accurate performance indicators, such as elasticity indicators, cost indicators, performance indicators, cost-performance ratios, etc. In addition, we establish a multi-objective optimization model to optimize both performance and cost of cloud computing platform. Then the optimal stopping and cost-performance optimization algorithm are applied to obtain the optimal configurations, including the number of hot startup VMs, the system service rate, the hot/cold startup rate of VMs, and the hot/cold shutdown rate. By comparing with existing optimization methods, we demonstrate the superiority of our cost-performance ratio optimization method.},
  archive      = {J_TPDS},
  author       = {Bo Wan and Jiale Dang and Zhetao Li and Hongfang Gong and Feng Zhang and Sangyoon Oh},
  doi          = {10.1109/TPDS.2020.2968913},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1518-1532},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Modeling analysis and cost-performance ratio optimization of virtual machine scheduling in cloud computing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance-aware speculative resource oversubscription for
large-scale clusters. <em>TPDS</em>, <em>31</em>(7), 1499–1517. (<a
href="https://doi.org/10.1109/TPDS.2020.2970013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a long-standing challenge to achieve a high degree of resource utilization in cluster scheduling. Resource oversubscription has become a common practice in improving resource utilization and cost reduction. However, current centralized approaches to oversubscription suffer from the issue with resource mismatch and fail to take into account other performance requirements, e.g., tail latency. In this article we present ROSE, a new resource management platform capable of conducting performance-aware resource oversubscription. ROSE allows latency-sensitive long-running applications (LRAs) to co-exist with computation-intensive batch jobs. Instead of waiting for resource allocation to be confirmed by the centralized scheduler, job managers in ROSE can independently request to launch speculative tasks within specific machines according to their suitability for oversubscription. Node agents of those machines can however, avoid any excessive resource oversubscription by means of a mechanism for admission control using multi-resource threshold control and performance-aware resource throttle. Experiments show that in case of mixed co-location of batch jobs and latency-sensitive LRAs, the CPU utilization and the disk utilization can reach 56.34 and 43.49 percent, respectively, but the 95th percentile of read latency in YCSB workloads only increases by 5.4 percent against the case of executing the LRAs alone.},
  archive      = {J_TPDS},
  author       = {Renyu Yang and Chunming Hu and Xiaoyang Sun and Peter Garraghan and Tianyu Wo and Zhenyu Wen and Hao Peng and Jie Xu and Chao Li},
  doi          = {10.1109/TPDS.2020.2970013},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1499-1517},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performance-aware speculative resource oversubscription for large-scale clusters},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). T-caching: Enhancing feasibility of in-network caching in
ICN. <em>TPDS</em>, <em>31</em>(7), 1486–1498. (<a
href="https://doi.org/10.1109/TPDS.2020.2970702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Information-Centric Networking (ICN), in-network caching is one of the core functions to implement efficient content distribution. As content is served from the network cache, redundant transmission across the same link is reduced and better quality services are provided to the user. However, caching operations (locating cached content, validating/writing content to the cache) may be a large burden on routers. In this article, we substantially reduce the caching overhead of routers by introducing T-Caching, a new in-network caching model in which routers selectively cache some of the most popular content recommended by content providers. In T-Caching, tokens are used 1) to enable routers to maximize caching benefits by simply controlling the amount of content inserted into the cache; 2) to allow content providers to recommend the most popular content as much as routers would actually cache. Our simulation and empirical studies using synthetic data, as well as real-world traces, show that T-Caching allows routers to insert much less than 1 percent of the content into the cache, compared to typical caching mechanisms. Nevertheless, the cache-hit ratio is improved by up to 2~9 times depending on the cache size. We believe that T-Caching can significantly contribute to improving feasibility and performance of in-network caching in ICN.},
  archive      = {J_TPDS},
  author       = {Sugi Lee and Ikjun Yeom and Dohyung Kim},
  doi          = {10.1109/TPDS.2020.2970702},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1486-1498},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {T-caching: Enhancing feasibility of in-network caching in ICN},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient compute-intensive job allocation in data centers
via deep reinforcement learning. <em>TPDS</em>, <em>31</em>(6),
1474–1485. (<a href="https://doi.org/10.1109/TPDS.2020.2968427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing the energy consumption of the servers in a data center via proper job allocation is desirable. Existing advanced job allocation algorithms, based on constrained optimization formulations capturing servers&#39; complex power consumption and thermal dynamics, often scale poorly with the data center size and optimization horizon. This article applies deep reinforcement learning to build an allocation algorithm for long-lasting and compute-intensive jobs that are increasingly seen among today&#39;s computation demands. Specifically, a deep Q-network is trained to allocate jobs, aiming to maximize a cumulative reward over long horizons. The training is performed offline using a computational model based on long short-term memory networks that capture the servers&#39; power and thermal dynamics. This offline training approach avoids slow online convergence, low energy efficiency, and potential server overheating during the agent&#39;s extensive state-action space exploration if it directly interacts with the physical data center in the usually adopted online learning scheme. At run time, the trained Q-network is forward-propagated with little computation to allocate jobs. Evaluation based on eight months&#39; physical state and job arrival records from a national supercomputing data center hosting 1,152 processors shows that our solution reduces computing power consumption by more than 10 percent and processor temperature by more than 4°C without sacrificing job processing throughput.},
  archive      = {J_TPDS},
  author       = {Deliang Yi and Xin Zhou and Yonggang Wen and Rui Tan},
  doi          = {10.1109/TPDS.2020.2968427},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1474-1485},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient compute-intensive job allocation in data centers via deep reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reduce operations: Send volume balancing while minimizing
latency. <em>TPDS</em>, <em>31</em>(6), 1461–1473. (<a
href="https://doi.org/10.1109/TPDS.2020.2964536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication hypergraph model was proposed in a two-phase setting for encapsulating multiple communication cost metrics (bandwidth and latency), which are proven to be important in parallelizing irregular applications. In the first phase, computational-task-to-processor assignment is performed with the objective of minimizing total volume while maintaining computational load balance. In the second phase, communication-task-to-processor assignment is performed with the objective of minimizing total number of messages while maintaining communication-volume balance. The reduce-communication hypergraph model suffers from failing to correctly encapsulate send-volume balancing. We propose a novel vertex weighting scheme that enables part weights to correctly encode send-volume loads of processors for send-volume balancing. The model also suffers from increasing the total communication volume during partitioning. To decrease this increase, we propose a method that utilizes the recursive bipartitioning framework and refines each bipartition by vertex swaps. For performance evaluation, we consider column-parallel SpMV, which is one of the most widely known applications in which the reduce-task assignment problem arises. Extensive experiments on 313 matrices show that, compared to the existing model, the proposed models achieve considerable improvements in all communication cost metrics. These improvements lead to an average decrease of 30 percent in parallel SpMV time on 512 processors for 70 matrices with high irregularity.},
  archive      = {J_TPDS},
  author       = {M. Ozan Karsavuran and Seher Acer and Cevdet Aykanat},
  doi          = {10.1109/TPDS.2020.2964536},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1461-1473},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reduce operations: Send volume balancing while minimizing latency},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Errata to “exploring fault-tolerant erasure codes for
scalable all-flash array clusters.” <em>TPDS</em>, <em>31</em>(6), 1460.
(<a href="https://doi.org/10.1109/TPDS.2020.2971074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents corrections to author affiliation information in the above mentioned article.},
  archive      = {J_TPDS},
  author       = {Sungjoon Koh and Jie Zhang and Miryeong Kwon and Jungyeon Yoon and David Donofrio and Nam Sung Kim and Myoungsoo Jung},
  doi          = {10.1109/TPDS.2020.2971074},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1460},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Errata to “Exploring fault-tolerant erasure codes for scalable all-flash array clusters”},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). P-PFC: Reducing tail latency with predictive PFC in lossless
data center networks. <em>TPDS</em>, <em>31</em>(6), 1447–1459. (<a
href="https://doi.org/10.1109/TPDS.2020.2969182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote Direct Memory Access(RDMA) technology rapidly changes the landscape of nowadays datacenter applications. Congestion control for RDMA networking is a critical challenge. As an end-to-end layer 3 congestion control mechanism, Datacenter QCN (DCQCN) alleviates the unfairness and head-of-the-line blocking problems of Priority-based Flow Control (PFC). However, a lossless network does not guarantee low latency even with DCQCN enabled. When network congestion happens, switch queues still build-up due to the response latency of end-to-end solutions. In this article, we propose Predictive PFC (P-PFC) to reduce tail latency in RDMA networks. P-PFC monitors the derivative of buffer occupation, predicts the happening of PFC trigger in the future, and proactively triggers PFC pause in advance. The benefit is that buffer usage can be maintained at a low level, hence the tail latency can be controlled. Preliminary evaluation results demonstrate that P-PFC can reduce tail latency by more than half of that in standard PFC in many scenarios, without hurting the throughput and average latency. P-PFC can also protect innocent flows compared with standard PFC according to our experiments. To our best knowledge, this is the first work of using derivative to improve PFC in lossless RDMA networks.},
  archive      = {J_TPDS},
  author       = {Chen Tian and Bo Li and Liulan Qin and Jiaqi Zheng and Jie Yang and Wei Wang and Guihai Chen and Wanchun Dou},
  doi          = {10.1109/TPDS.2020.2969182},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1447-1459},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {P-PFC: Reducing tail latency with predictive PFC in lossless data center networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An approximate communication framework for network-on-chips.
<em>TPDS</em>, <em>31</em>(6), 1434–1446. (<a
href="https://doi.org/10.1109/TPDS.2020.2968068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current multi-/many-core systems spend large amounts of time and power transmitting data across on-chip interconnects. This problem is aggravated when data-intensive applications, such as machine learning and pattern recognition, are executed in these systems. Recent studies show that some data-intensive applications can tolerate modest errors, thus opening a new design dimension, namely, trading result quality for better system performance. In this article, we explore application error tolerance and propose an approximate communication framework to reduce the power consumption and latency of network-on-chips (NoCs). The proposed framework incorporates a quality control method and a data approximation mechanism to reduce the packet size to decrease network power consumption and latency. The quality control method automatically identifies the error-resilient variables that can be approximated during transmission and calculates their error thresholds based on the quality requirements of the application by analyzing the source code. The data approximation method includes a lightweight lossy compression scheme, which significantly reduces packet size when the error-resilient variables are transmitted. This framework results in fewer flits in each data packet and reduces traffic in NoCs while guaranteeing the quality requirements of applications. Our cycle-accurate simulation using the AxBench benchmark suite shows that the proposed approximate communication framework achieves 62 percent latency reduction and 43 percent dynamic power reduction compared to previous approximate communication techniques while ensuring 95 percent result quality.},
  archive      = {J_TPDS},
  author       = {Yuechen Chen and Ahmed Louri},
  doi          = {10.1109/TPDS.2020.2968068},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1434-1446},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An approximate communication framework for network-on-chips},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A value-oriented job scheduling approach for
power-constrained and oversubscribed HPC systems. <em>TPDS</em>,
<em>31</em>(6), 1419–1433. (<a
href="https://doi.org/10.1109/TPDS.2020.2967373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate limitations in the traditional value-based algorithms for a power-constrained HPC system and evaluate their impact on HPC productivity. We expose the trade-off between allocating system-wide power budget uniformly and greedily under different system-wide power constraints in an oversubscribed system. We experimentally demonstrate that, under the tightest power constraint, the mean productivity of the greedy allocation is 38 percent higher than the uniform allocation whereas, under the intermediate power constraint, the uniform allocation has a mean productivity of 6 percent higher than the greedy allocation. We then propose a new algorithm that adapts its behavior to deliver the combined benefits of the two allocation strategies. We design a methodology with online retraining capability to create application-specific power-execution time models for a class of HPC applications. These models are used in predicting the execution time of an application on the available resources at the time of making scheduling decisions in the power-aware algorithms. We evaluate the proposed algorithm using emulation and simulation environments, and show that our adaptive strategy results in improving HPC resource utilization while delivering a mean productivity that is almost the same as the best performing algorithm across various system-wide power constraints.},
  archive      = {J_TPDS},
  author       = {Nirmal Kumbhare and Aniruddha Marathe and Ali Akoglu and Howard Jay Siegel and Ghaleb Abdulla and Salim Hariri},
  doi          = {10.1109/TPDS.2020.2967373},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1419-1433},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A value-oriented job scheduling approach for power-constrained and oversubscribed HPC systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards distributed SDN: Mobility management and flow
scheduling in software defined urban IoT. <em>TPDS</em>, <em>31</em>(6),
1400–1418. (<a href="https://doi.org/10.1109/TPDS.2018.2883438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of Internet of Things (IoT) devices with multiple radio interfaces has resulted in a number of urban-scale deployments of IoT multinetworks, where heterogeneous wireless communication solutions coexist (e.g., WiFi, Bluetooth, Cellular). Managing the multinetworks for seamless IoT access and handover, especially in mobile environments, is a key challenge. Software-defined networking (SDN) is emerging as a promising paradigm for quick and easy configuration of network devices, but its application in urban-scale multinetworks requiring heterogeneous and frequent IoT access is not well studied. In this paper we present UbiFlow, the first software-defined IoT system for combined ubiquitous flow control and mobility management in urban heterogeneous networks. UbiFlow adopts multiple controllers to divide urban-scale SDN into different geographic partitions (assigning one controller per partition) and achieve distributed control of IoT flows. A distributed hashing based overlay structure is proposed to maintain network scalability and consistency. Based on this UbiFlow overlay structure, the relevant issues pertaining to mobility management such as scalable control, fault tolerance, and load balancing have been carefully examined and studied. The UbiFlow controller differentiates flow scheduling based on per-device requirements and whole-partition capabilities. Therefore, it can present a network status view and optimized selection of access points in multinetworks to satisfy IoT flow requests, while guaranteeing network performance for each partition. Simulation and realistic testbed experiments confirm that UbiFlow can successfully achieve scalable mobility management and robust flow scheduling in IoT multinetworks; e.g., 67.21 percent throughput improvement, 72.99 percent reduced delay, and 69.59 percent jitter improvements, compared with alternative SDN systems.},
  archive      = {J_TPDS},
  author       = {Di Wu and Xiang Nie and Eskindir Asmare and Dmitri I. Arkhipov and Zhijing Qin and Renfa Li and Julie A. McCann and Keqin Li},
  doi          = {10.1109/TPDS.2018.2883438},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1400-1418},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards distributed SDN: Mobility management and flow scheduling in software defined urban IoT},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RIVA: Robust integrity verification algorithm for high-speed
file transfers. <em>TPDS</em>, <em>31</em>(6), 1387–1399. (<a
href="https://doi.org/10.1109/TPDS.2020.2966616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end integrity verification is designed to protect file transfers against silent data corruption by comparing checksum of files at source and destination end points using cryptographic hash functions such as MD5 and SHA1. However, existing implementations of end-to-end integrity verification for file transfers fall short to detect undetected disk errors that causes inconsistency between disk and cache memory. In this article, we propose Robust Integrity Verification Algorithm (RIVA) to strengthen the integrity of file transfers by forcing checksum computation tasks to read files directly from disk. RIVA achieves this by invalidating memory mappings of file pages after their transfer such that when the file is read again for checksum calculation, it will be fetched from disk and silent disk errors will be captured. We design and conduct extensive fault resilience experiments to evaluate the robustness of integrity verification algorithms against undetected disk write errors. The results indicate that while the state-of-the-art integrity verification algorithms fail to detect the injected errors for almost all file sizes, RIVA captures all of them with the help of cache invalidation. We further run statistical analysis to assess the probability of missing silent disk errors and find that RIVA reduces the likelihood by 10 to 15 orders of magnitude compared to the existing approaches. Finally, enforcing disk read in integrity verification introduces an inevitable overhead in exchange of increased robustness against silent disk errors, but RIVA keeps its overhead below 15 percent in most cases by running transfer, cache invalidation, and checksum computation processes concurrently for different portions of the same file.},
  archive      = {J_TPDS},
  author       = {Batyr Charyyev and Engin Arslan},
  doi          = {10.1109/TPDS.2020.2966616},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1387-1399},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RIVA: Robust integrity verification algorithm for high-speed file transfers},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Turbo: Dynamic and decentralized global analytics via
machine learning. <em>TPDS</em>, <em>31</em>(6), 1372–1386. (<a
href="https://doi.org/10.1109/TPDS.2020.2964667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data analytics are practiced in many fields to extract insights from massive amounts of data. With exponential growth in both the volume and variety of data, analytic queries have expanded from those executed in a single datacenter to those requiring inputs from multiple datacenters that are geographically separate or even globally distributed. Unfortunately, the software stack that supports data analytics is designed originally for a cluster environment and is not tailored to execute global analytic queries, where resources such as inter-datacenter networks may vary on the fly. Existing optimization strategies that determine the query execution plan before its execution are not able to adapt to resource variations at query runtime. In this article, we present Turbo, a lightweight and non-intrusive global analytics system that can dynamically adjust query execution plans for geo-distributed analytics in the presence of time-varying resources and network bandwidth across datacenters. Turbo uses machine learning to accurately predict the time cost of a query execution plan so that dynamic adjustments can be made to it when necessary. Turbo is non-intrusive in the sense that it does not require modifications to the existing software stack for data analytics. We have implemented a real-world prototype of Turbo, and evaluated it on a cluster of 33 instances across eight regions in the Google Cloud Platform. Our experimental results have shown that Turbo can achieve an accuracy of 95 percent for estimating time costs, and can reduce the query completion time by 41 percent.},
  archive      = {J_TPDS},
  author       = {Hao Wang and Di Niu and Baochun Li},
  doi          = {10.1109/TPDS.2020.2964667},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1372-1386},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Turbo: Dynamic and decentralized global analytics via machine learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). On-edge multi-task transfer learning: Model and practice
with data-driven task allocation. <em>TPDS</em>, <em>31</em>(6),
1357–1371. (<a href="https://doi.org/10.1109/TPDS.2019.2962435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On edge devices, data scarcity occurs as a common problem where transfer learning serves as a widely-suggested remedy. Nevertheless, transfer learning imposes heavy computation burden to the resource-constrained edge devices. Existing task allocation works usually assume all submitted tasks are equally important, leading to inefficient resource allocation at a task level when directly applied in Multi-task Transfer Learning (MTL). To address these issues, we first reveal that it is crucial to measure the impact of tasks on overall decision performance improvement and quantify task importance. We then show that task allocation with task importance for MTL (TATIM) is a variant of NP-complete Knapsack problem, where the complicated computation to solve this problem needs to be conducted repeatedly under varying contexts. To solve TATIM with high computational efficiency, we propose a Data-driven Cooperative Task Allocation (DCTA) approach. Finally, we evaluate the performance of DCTA by not only a trace-driven simulation, but also a new comprehensive real-world AIOps case study which bridges model and practice via a new architecture and main components design within AIOps system. Extensive experiments show that our DCTA reduces 3.24 times of processing time, and saves 48.4 percent energy consumption compared with the state-of-the-art when solving TATIM.},
  archive      = {J_TPDS},
  author       = {Qiong Chen and Zimu Zheng and Chuang Hu and Dan Wang and Fangming Liu},
  doi          = {10.1109/TPDS.2019.2962435},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1357-1371},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On-edge multi-task transfer learning: Model and practice with data-driven task allocation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance analysis of trial and error algorithms.
<em>TPDS</em>, <em>31</em>(6), 1343–1356. (<a
href="https://doi.org/10.1109/TPDS.2020.2964256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-free decentralized optimizations and learning are receiving increasing attention from theoretical and practical perspectives. In particular, two fully decentralized learning algorithms, namely Trial and Error Learning (TEL) and Optimal Dynamical Learning (ODL), are very appealing for a broad class of games. Indeed, ODL has the property to spend a high proportion of time in an optimum state that maximizes the sum of the utilities of all players, whereas, TEL has the property to spend a high proportion of time in an optimum state that maximizes the sum of the utilities of all players if there is a pure Nash equilibrium, otherwise, it spends a high proportion of time in a state that maximizes a trade-off between the sum of the utilities of the players and a predefined stability function. On the other hand, estimating the mean fraction of time spent in the optimum state (as well as the mean time duration to reach it) is challenging due to the high complexity and dimension of the inherent Markov chains. In this article, under some specific system model, an evaluation of the above performance metrics is provided by proposing an approximation of the considered Markov chains, which allows overcoming the problem of high dimensionality. A comparison between the two algorithms is then performed which allows a better understanding of their performance.},
  archive      = {J_TPDS},
  author       = {Jérôme Gaveau and Christophe J. Le Martret and Mohamad Assaad},
  doi          = {10.1109/TPDS.2020.2964256},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1343-1356},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performance analysis of trial and error algorithms},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ERA-LSTM: An efficient ReRAM-based architecture for long
short-term memory. <em>TPDS</em>, <em>31</em>(6), 1328–1342. (<a
href="https://doi.org/10.1109/TPDS.2019.2962806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing-in-memory (PIM) architecture based on resistive random access memory (ReRAM) crossbars is a promising solution to the memory bottleneck that long short-term memory (LSTM) faces. Based on the dataflow analysis of the LSTM computing paradigm, this article proposes to adopt the ReRAM-based analog approximate computing to conduct the LSTM-specific element-wise computation. Combined with the dot-product computation implemented with ReRAM crossbars, a new LSTM processing tile is designed to significantly reduce the demand for analog-to-digital converters (ADCs), which is the major part of power consumption of existing designs. Next, we elaborate on a mapping scheme to efficiently deploy large-scale LSTM onto multiple processing tiles. Finally, an architecture enhancement is proposed to support crossbar-friendly LSTM pruning to further improve efficiency. This overall design, named ERA-LSTM, is presented. Our evaluation shows that it can outperform two state-of-the-art FPGA-based LSTM accelerators by 103.6 and 35.9 times, respectively; compared with a state-of-the-art ReRAM-based LSTM accelerator with digital element-wise computation, it is 6.1 times more efficient. Moreover, our experiments demonstrate that the impact of hardware constraints and approximation errors on the inference accuracy can be effectively reduced by the proposed fine-tuning scheme and by optimizing the design of the approximator.},
  archive      = {J_TPDS},
  author       = {Jianhui Han and He Liu and Mingyu Wang and Zhaolin Li and Youhui Zhang},
  doi          = {10.1109/TPDS.2019.2962806},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1328-1342},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ERA-LSTM: An efficient ReRAM-based architecture for long short-term memory},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SLEEF: A portable vectorized library of c standard
mathematical functions. <em>TPDS</em>, <em>31</em>(6), 1316–1327. (<a
href="https://doi.org/10.1109/TPDS.2019.2960333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present techniques used to implement our portable vectorized library of C standard mathematical functions written entirely in C language. In order to make the library portable while maintaining good performance, intrinsic functions of vector extensions are abstracted by inline functions or preprocessor macros. We implemented the functions so that they can use sub-features of vector extensions such as fused multiply-add, mask registers, and extraction of mantissa. In order to make computation with SIMD instructions efficient, the library only uses a small number of conditional branches, and all the computation paths are vectorized. We devised a variation of the Payne-Hanek argument reduction for trigonometric functions and a floating point remainder, both of which are suitable for vector computation. We compare the performance with our library to Intel SVML.},
  archive      = {J_TPDS},
  author       = {Naoki Shibata and Francesco Petrogalli},
  doi          = {10.1109/TPDS.2019.2960333},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1316-1327},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SLEEF: A portable vectorized library of c standard mathematical functions},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Concurrent irrevocability in best-effort hardware
transactional memory. <em>TPDS</em>, <em>31</em>(6), 1301–1315. (<a
href="https://doi.org/10.1109/TPDS.2019.2963030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing best-effort requester-wins implementations of transactional memory must resort to non-speculative execution to provide forward progress in the presence of transactions that exceed hardware capacity, experience page faults or suffer high-contention leading to livelocks. Current approaches to irrevocability employ lock-based synchronization to achieve mutual exclusion when executing a transaction non-speculatively, conservatively precluding concurrency with any other transactions in order to guarantee atomicity at the cost of degrading performance. In this article, we propose a new form of concurrent irrevocability whose goal is to minimize the loss of concurrency paid when transactions resort to irrevocability to complete. By enabling optimistic concurrency control also during non-speculative execution of a transaction, our proposal allows for higher parallelism than existing schemes. We describe the extensions to the instruction set to provide concurrent irrevocable transactions as well as the architectural extensions required to realize them on a best-effort HTM system without requiring any modification to the cache coherence protocol. Our evaluation shows that our proposal achieves an average reduction of 12.5 percent in execution time across the STAMP benchmarks, with 15.8 percent on average for highly contended workloads.},
  archive      = {J_TPDS},
  author       = {Rubén Titos-Gil and Ricardo Fernández-Pascual and Alberto Ros and Manuel E. Acacio},
  doi          = {10.1109/TPDS.2019.2963030},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1301-1315},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Concurrent irrevocability in best-effort hardware transactional memory},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster parallel core maintenance algorithms in dynamic
graphs. <em>TPDS</em>, <em>31</em>(6), 1287–1300. (<a
href="https://doi.org/10.1109/TPDS.2019.2960226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the core maintenance problem for dynamic graphs which requires to update each vertex&#39;s core number with the insertion/deletion of vertices/edges. Previous algorithms can either process one edge associated with a vertex in each iteration or can only process one superior edge associated with the vertex (an edge 〈u; v〉 is a superior edge of vertex u if v&#39; core number is no less than u&#39;s core number) in each iteration. Thus for high superior-degree vertices (the vertices associated with many superior edges) insertions/deletions, previous algorithms become very inefficient. In this article, we discovered a new structure called joint edge set whose insertions/deletions make each vertex&#39;s core number change at most one. The joint edge set mainly contains all the superior edges associated with the high superior-degree vertices as long as these vertices are 3 + -hop independent. Based on this discovery, faster parallel algorithms are devised to solve the core maintenance problems. In our algorithms, we can process all edges in the joint edge set in one iteration and thus can greatly increase the parallelism and reduce the processing time. The results of extensive experiments conducted on various types of real-world, temporal, and synthetic graphs illustrate that the proposed algorithms achieve good efficiency, stability and scalability. Specifically, the new algorithms can outperform the single-edge processing algorithms by up to four orders of magnitude. Compared with the matching based algorithm and the superior edge based algorithm, our algorithms show a significant speedup up to 60× in the processing time.},
  archive      = {J_TPDS},
  author       = {Qiang-Sheng Hua and Yuliang Shi and Dongxiao Yu and Hai Jin and Jiguo Yu and Zhipen Cai and Xiuzhen Cheng and Hanhua Chen},
  doi          = {10.1109/TPDS.2019.2960226},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1287-1300},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Faster parallel core maintenance algorithms in dynamic graphs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online deadline-aware task dispatching and scheduling in
edge computing. <em>TPDS</em>, <em>31</em>(6), 1270–1286. (<a
href="https://doi.org/10.1109/TPDS.2019.2961905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study online deadline-aware task dispatching and scheduling in edge computing. We jointly considerthe management of the networking and computing resources to meet the maximum number of deadlines. We propose an online algorithm, named Dedas, which greedily schedules newly arriving tasks and considers whether to replace some existing tasks in order to make the new deadlines satisfied. We derive a non-trivial competitive ratio of Dedas theoretically, and our analysis is asymptotically tight. Besides, we implement a distributed approximation D - Dedas with a better scalability and less than 10 percent performance loss compared with the centralized algorithm Dedas. We then build DeEdge, an edge computing testbed installed with typical latency-sensitive applications such as IoT sensor monitoring and face matching. We adopt a real-world data trace from the Google cluster for large-scale emulations. Extensive testbed experiments and simulations demonstrate that the deadline miss ratio of Dedas is stable for online tasks, which is reduced by up to 60 percent compared with state-of-the-art methods. Moreover, Dedas performs well in minimizing the average task completion time.},
  archive      = {J_TPDS},
  author       = {Jiaying Meng and Haisheng Tan and Xiang-Yang Li and Zhenhua Han and Bojie Li},
  doi          = {10.1109/TPDS.2019.2961905},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1270-1286},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online deadline-aware task dispatching and scheduling in edge computing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NVGraph: Enforcing crash consistency of evolving network
analytics in NVMM systems. <em>TPDS</em>, <em>31</em>(6), 1255–1269. (<a
href="https://doi.org/10.1109/TPDS.2020.2965452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many complex networks can be modeled as evolving graphs. Existing in-memory graph data structures are designed for DRAM. Thus, they cannot effectively exploit the current and ongoing adoption of emerging non-volatile main memory (NVMM) for two reasons. (1) Ephemeral graph data structures are not crash-consistent for NVMM. (2) NVMM writes and reads and may incur higher latency than DRAM. In this article, we propose a novel persistent evolving graph data structure, named NVG RAPH , for both computing and in-memory storage of evolving graphs in NVMM. We devise NVG RAPH as a multi-version data structure, wherein a minimum of one version of its data is stored in NVMM to provide the desired durability at runtime for failure recovery, and another version is stored in both DRAM and NVMM to reduce the NVMM-induced memory latency. NVG RAPH is also a partitioned data structure. We dynamically transform the layout of NVG RAPH (e.g., changing the size of its partition in DRAM) exploiting network properties and data access patterns of workloads. For the evaluation of NVG RAPH , we implement four representative real-world graph applications: pagerank, BFS, influence maximization, and rumor source detection. The experimental results show that the performance of NVG RAPH is comparable to other in-memory data structure (e.g., CSR and LLAMA) while using 70 percent less DRAM. It scales well up to 10 billion edges and 201 snapshots and supports crash consistency. It offers up to the 21X speedup of execution time compared to the scale-up graph computation approaches (e.g., GraphChi and X-stream).},
  archive      = {J_TPDS},
  author       = {Soklong Lim and Tyler Coy and Zaixin Lu and Bin Ren and Xuechen Zhang},
  doi          = {10.1109/TPDS.2020.2965452},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1255-1269},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NVGraph: Enforcing crash consistency of evolving network analytics in NVMM systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GRP-HEFT: A budget-constrained resource provisioning scheme
for workflow scheduling in IaaS clouds. <em>TPDS</em>, <em>31</em>(6),
1239–1254. (<a href="https://doi.org/10.1109/TPDS.2019.2961098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Infrastructure as a Service (IaaS) Clouds, users are charged to utilize cloud services according to a pay-per-use model. If users intend to run their workflow applications on cloud resources within a specific budget, they have to adjust their demands for cloud resources with respect to this budget. Although several scheduling approaches have introduced solutions to optimize the makespan of workflows on a set of heterogeneous IaaS cloud resources within a certain budget, the hourly-based cost model of some well-known cloud providers (e.g., Amazon EC2 Cloud) can easily lead to a higher makespan and some schedulers may not find any feasible solution. In this article, we propose a novel resource provisioning mechanism and a workflow scheduling algorithm, named Greedy Resource Provisioning and modified HEFT (GRP-HEFT), for minimizing the makespan of a given workflow subject to a budget constraint for the hourly-based cost model of modern IaaS clouds. As a resource provisioning mechanism, we propose a greedy algorithm which lists the instance types according to their efficiency rate. For our scheduler, we modified the HEFT algorithm to consider a budget limit. GRP-HEFT is compared against state-of-the-art workflow scheduling techniques, including MOACS (MultiObjective Ant Colony System), PSO (Particle Swarm Optimization), and GA (Genetic Algorithm). The experimental results demonstrate that GRP-HEFT outperforms GA, PSO, and MOACS for several well-known scientific workflow applications for different problem sizes on average by 13.64, 19.77, and 11.69 percent, respectively. Also in terms of time complexity, GRP-HEFT outperforms GA, PSO and MOACS.},
  archive      = {J_TPDS},
  author       = {Hamid Reza Faragardi and Mohammad Reza Saleh Sedghpour and Saber Fazliahmadi and Thomas Fahringer and Nayereh Rasouli},
  doi          = {10.1109/TPDS.2019.2961098},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1239-1254},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GRP-HEFT: A budget-constrained resource provisioning scheme for workflow scheduling in IaaS clouds},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring token-oriented in-network prioritization in
datacenter networks. <em>TPDS</em>, <em>31</em>(5), 1223–1238. (<a
href="https://doi.org/10.1109/TPDS.2019.2958899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In memory computing and high-end distributed storage demand low latency, high throughput, and zero data loss simultaneously from datacenter networks. Existing reactive congestion control approaches cannot both minimize queuing latency and ensure zero data loss. A token-oriented proactive approach can achieve them together by controlling congestion even before sending data packets. However, state-of-the-art token-oriented approaches only strive to optimize network-level metrics: maximizing throughput while achieving flow-level fairness. This article answers the question of how to support objective-aware traffic scheduling in token-oriented approaches. The novelty of Token-Oriented in-network Prioritization (TOP) is that it prioritizes tokens instead of data packets. We make three contributions. Via simulations over a hypothetical TOP system, our first contribution is demonstrating the potential performance gain that can be brought by TOP. Second, we investigate the applicability of TOP. Although the overhead of enabling necessary TOP features in switches is trivial, we find that mainstream commodity datacenter switches do not support them. We hence propose a readily-deployable remedy to achieve in-network prioritization by pushing both switch and end-host hardware capacity to an extreme end. Lastly, we implement a running TOP system with Linux hosts and commodity switches, and evaluate TOP in testbeds and with large-scale simulations for various scenarios.},
  archive      = {J_TPDS},
  author       = {Kexin Liu and Bingchuan Tian and Chen Tian and Bo Li and Qingyue Wang and Jiaqi Zheng and Jiajun Sun and Yixiao Gao and Wei Wang and Guihai Chen and Wanchun Dou and Yanan Jiang and Huaping Zhou and Jingjie Jiang and Fan Zhang and Gong Zhang},
  doi          = {10.1109/TPDS.2019.2958899},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1223-1238},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploring token-oriented in-network prioritization in datacenter networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GMig: Efficient vGPU live migration with overlapped
software-based dirty page verification. <em>TPDS</em>, <em>31</em>(5),
1209–1222. (<a href="https://doi.org/10.1109/TPDS.2019.2947521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces gMig, an open-source and practical vGPU live migration solution for full virtualization. Taking the advantage of the dirty pattern of GPU workloads, gMig presents the One-Shot Pre-Copy mechanism combined with the hashing based Software Dirty Page technique to achieve efficient vGPU live migration. Particularly, we propose three core techniques for gMig: 1) Dynamic Graphics Address Remapping, which parses and manipulates GPU commands to adjust the address mapping and adapt to a different environment after migration, 2) Software Dirty Page, which utilizes a hashing based approach with sampling pre-filtering to detect page modification, overcomes the commodity GPU&#39;s hardware limitation, and speeds up the migration by only sending the dirtied pages, 3) Overlapped Migration Process, which significantly compresses the hanging overhead by overlapping the dirty page verification and transmission concurrently. Our evaluation shows that gMig achieves GPU live migration with an average downtime of 302 ms on Windows and 119 ms on Linux. With the help of Software Dirty Page, the number of GPU pages transferred during the downtime is effectively reduced by up to 80.0 percent . The design of sampling filter and overlapped processing can bring about further 30.0 and 10.0 percent improvements in page processing.},
  archive      = {J_TPDS},
  author       = {Qiumin Lu and Xiao Zheng and Jiacheng Ma and Yaozu Dong and Zhengwei Qi and Jianguo Yao and Bingsheng He and Haibing Guan},
  doi          = {10.1109/TPDS.2019.2947521},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1209-1222},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GMig: Efficient vGPU live migration with overlapped software-based dirty page verification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Massively scaling seismic processing on sunway TaihuLight
supercomputer. <em>TPDS</em>, <em>31</em>(5), 1194–1208. (<a
href="https://doi.org/10.1109/TPDS.2019.2962395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common Midpoint (CMP) and Common Reflection Surface (CRS) are widely used methods for improving the signal-to-noise ratio in the field of seismic processing. These methods are computationally intensive and require high-performance computing. This article optimizes these methods on the Sunway many-core architecture and implements large-scale seismic processing on the Sunway Taihulight supercomputer. We propose the following three optimization techniques: 1) we propose a software cache method to reduce the overhead of memory accesses, and share data among CPEs via the register communication; 2) we re-design the semblance calculation procedure to further reduce the overhead of memory accesses; 3) we propose a vectorization method to improve the performance when processing the small volume of data within short loops. The experimental results show that our implementations of CMP and CRS methods on Sunway achieve 3.50× and 3.01× speedup on average compared to the-state-of-the-art implementations on CPU. In addition, our implementation is capable to run on more than one million cores of Sunway TaihuLight with good scalability.},
  archive      = {J_TPDS},
  author       = {Yongmin Hu and Hailong Yang and Zhongzhi Luan and Lin Gan and Guangwen Yang and Depei Qian},
  doi          = {10.1109/TPDS.2019.2962395},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1194-1208},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Massively scaling seismic processing on sunway TaihuLight supercomputer},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized utility- and locality-aware replication for
heterogeneous DHT-based P2P cloud storage systems. <em>TPDS</em>,
<em>31</em>(5), 1183–1193. (<a
href="https://doi.org/10.1109/TPDS.2019.2960018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a Distributed Hash Table (DHT), Skip Graph routing overlays are exploited in several peer-to-peer (P2P) services, including P2P cloud storage. The fully decentralized replication algorithms that are applicable to the Skip Graph-based P2P cloud storage fail on improving the performance of the system with respect to both the availability of replicas as well as their response time. Additionally, they presume the system as homogeneous with respect to the nodes&#39; latency distribution, availability behavior, and bandwidth, or storage. In this article, we propose Pyramid, which is the first fully decentralized utility- and locality-aware replication approach for Skip Graph-based P2P cloud storage systems. Pyramid considers the nodes as heterogeneous with respect to their latency distribution, availability behavior, bandwidth, and storage. Pyramid is utility-aware as it maximizes the average available bandwidth of replicas per time slot (e.g., per hour). Additionally, Pyramid is locality-aware as it minimizes the average latency between nodes and their closest replica. Our simulation results show that compared to the state-of-the-art solutions that either perform good in utility-awareness, or in locality-awareness, our proposed Pyramid improves both the utility- and locality-awareness of replicas with a gain of about 1.2 and 1.1 times at the same time, respectively.},
  archive      = {J_TPDS},
  author       = {Yahya Hassanzadeh-Nazarabadi and Alptekin Küpçü and Oznur Ozkasap},
  doi          = {10.1109/TPDS.2019.2960018},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1183-1193},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Decentralized utility- and locality-aware replication for heterogeneous DHT-based P2P cloud storage systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task scheduling for energy consumption constrained parallel
applications on heterogeneous computing systems. <em>TPDS</em>,
<em>31</em>(5), 1165–1182. (<a
href="https://doi.org/10.1109/TPDS.2019.2959533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power-aware task scheduling on processors has been a research hotspot in computing systems. Given an application G containing a set N of tasks {n 1 ,...,n |N| }, and a system containing a set U of processors {u 1 ,..., u |U| }, the power-aware task scheduling generally refers to finding the appropriate processor and frequency for each task n i , so as to make sure that all the tasks can be finished efficiently and the overall energy consumption is guaranteed. In this article, we study the problem of minimizing the schedule length for energy consumption constrained parallel applications on heterogeneous computing systems, where the schedule length refers to the time interval between starting the first task and finishing the last task. For this problem, existing work adopts a policy that preassigns the minimum energy consumption for each unassigned task. Nevertheless, our analysis reveals that, such a pre-assignment policy could be unfair for the low priority tasks, and it may not achieve an optimistic schedule length. Thereby, we propose a new task scheduling algorithm that suggests a weight-based mechanism to preassign energy consumption for unassigned tasks, and we provide the rigorous proof to show its feasibility. Further, we show that this idea can be extended to solve reliability maximization problems with energy consumption constraint or with both deadline and energy consumption constraints, where the reliability refers to the probability of executing application G without failures, and the deadline constraint refers to the “allowable” maximum schedule length. We have conducted extensive experiments based on real parallel applications. The experimental results consistently demonstrate that our proposed algorithms can achieve favourable performance, compared to state-of-the-art algorithms.},
  archive      = {J_TPDS},
  author       = {Zhe Quan and Zhi-Jie Wang and Ting Ye and Song Guo},
  doi          = {10.1109/TPDS.2019.2959533},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1165-1182},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Task scheduling for energy consumption constrained parallel applications on heterogeneous computing systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reducing the impact of intensive dynamic memory allocations
in parallel multi-threaded programs. <em>TPDS</em>, <em>31</em>(5),
1152–1164. (<a href="https://doi.org/10.1109/TPDS.2019.2960514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent dynamic memory allocations (DyMAs) can significantly hinder the scalability of parallel multi-threaded programs. As the number of threads grows, DyMAs can even become the main performance bottleneck. We introduce modern tools and methods for evaluating the impact of DyMAs and present techniques for its reduction, which include scalable heap implementations, small buffer optimization, and memory pooling. Additionally, we provide a survey of state-of-the-art implementations of these techniques and study them experimentally by using a benchmark program, server simulator software, and a real-world high-performance computing application. As a result, we show that relatively small modifications in parallel program&#39;s source code or a way of its execution may substantially reduce the runtime overhead associated with the use of dynamic data structures.},
  archive      = {J_TPDS},
  author       = {Daniel Langr and Martin Kočička},
  doi          = {10.1109/TPDS.2019.2960514},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1152-1164},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reducing the impact of intensive dynamic memory allocations in parallel multi-threaded programs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). REACT: Scalable and high-performance regular expression
pattern matching accelerator for in-storage processing. <em>TPDS</em>,
<em>31</em>(5), 1137–1151. (<a
href="https://doi.org/10.1109/TPDS.2019.2953646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes REACT, a regular expression matching accelerator, which can be embedded in a modern Solid-State Drive (SSD) and a novel data access scheduling algorithm for high matching throughput. Specifically, REACT, including our data access scheduling algorithm, increases the utilization of SSD and the degree of internal memory parallelism for pattern matching processes. While the low-level flash exhibits long latency, modern SSDs in practice achieve high I/O performance by utilizing the massive internal parallelism at the system-level. However, exploiting the parallelism is limited for pattern matching since the subblocks, which constitute an input data and can be placed in multiple flash pages, should be tested in a sequence to process the input correctly. This limitation can induce low utilization of the accelerator. To address this challenge, the proposed REACT simultaneously processes multiple input streams with a parallel processing architecture to maximize matching throughput by hiding the long and irregular latency. The scheduling algorithm finds a data stream which requires a sub-block in closest time and prioritizes the access request to reduce the data stall of REACT. REACT achieves maximum 22.6 percent of matching throughput improvement on a 16channel high-performance SSD compared to the accelerator without the proposed scheduling algorithm.},
  archive      = {J_TPDS},
  author       = {Won Seob Jeong and Changmin Lee and Keunsoo Kim and Myung Kuk Yoon and Won Jeon and Myoungsoo Jung and Won Woo Ro},
  doi          = {10.1109/TPDS.2019.2953646},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1137-1151},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {REACT: Scalable and high-performance regular expression pattern matching accelerator for in-storage processing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Thread-level locking for SIMT architectures. <em>TPDS</em>,
<em>31</em>(5), 1121–1136. (<a
href="https://doi.org/10.1109/TPDS.2019.2955705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more emerging applications are moving to GPUs, thread-level synchronization has become a requirement. However, GPUs only provide warp-level and thread-block-level rather than thread-level synchronization. Moreover, it is highly possible to cause live-locks by using CPU synchronization mechanisms to implement thread-level synchronization for GPUs. In this article, we first propose a software-based thread-level synchronization mechanism called lock stealing for GPUs to avoid live-locks. We then describe how to implement our lock stealing algorithm in mutual exclusive locks and readers-writer locks with high performance. Finally, by putting it all together, we develop a thread-level locking library (TLLL) for commercial GPUs. To evaluate TLLL and show its general applicability, we use it to implement six widely used programs. We compare TLLL against the state-of-the-art ad-hoc GPU synchronization, GPU software transactional memory (STM), and CPU hardware transactional memory (HTM), respectively. The results show that, compared with the ad-hoc GPU synchronization for Delaunay mesh refinement (DMR), TLLL improves the performance by 22 percent on average on a GTX970 GPU, and shows up to 11 percent of performance improvement on a Volta V100 GPU. Moreover, it significantly reduces the required memory size. Such low memory consumption enables DMR to successfully run on the GTX970 GPU with the 10-million mesh size, and the V100 GPU with the 40-million mesh size, with which the ad-hoc synchronization can not run successfully. In addition, TLLL outperforms the GPU STM by 65 percent, and the CPU HTM (running on a Xeon E5-2620 v4 CPU with 16 hardware threads) by 43 percent on average.},
  archive      = {J_TPDS},
  author       = {Lan Gao and Yunlong Xu and Rui Wang and Zhongzhi Luan and Zhibin Yu and Depei Qian},
  doi          = {10.1109/TPDS.2019.2955705},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1121-1136},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Thread-level locking for SIMT architectures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Architectural support for NVRAM persistence in GPUs.
<em>TPDS</em>, <em>31</em>(5), 1107–1120. (<a
href="https://doi.org/10.1109/TPDS.2019.2960233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-volatile Random Access Memories (NVRAM) have emerged in recent years to bridge the performance gap between the main memory and external storage devices, such as Solid State Drives (SSD). In addition to higher storage density, NVRAM provides byte-addressability, higher bandwidth, near-DRAM latency, and easier access compared to block devices such as traditional SSDs. This enables new programming paradigms taking advantage of durability and larger memory footprint. With the range and size of GPU workloads expanding, NVRAM will present itself as a promising addition to GPU&#39;s memory hierarchy. To utilize the non-volatility of NVRAMs, programs should allow durable stores, maintaining consistency through a power loss event. This is usually done through a logging mechanism that works in tandem with a transaction execution layer which can consist of a transactional memory or a locking mechanism. Together, this results in a transaction processing system that preserves the ACID properties. GPUs are designed with high throughput in mind, leveraging high degrees of parallelism. Transactional memory proposals enable fine-grained transactions at the GPU thread-level. However, with lower write bandwidths compared to that of DRAMs, using NVRAM as-is may yield sub-optimal overall system performance when threads experience long latency. To address this problem, we propose using Helper Warps to move persistence out of the critical path of transaction execution, alleviating the impact of latencies. Our mechanism achieves a speedup of 4.4 and 1.5 under bandwidth limits of 1.6 GB/s and 12 GB/s and is projected to maintain speed advantage even when NVRAM bandwidth gets as high as hundreds of GB/s in certain cases. Due to the speedup, our proposed method also results in reduction in overall energy consumption.},
  archive      = {J_TPDS},
  author       = {Sui Chen and Lei Liu and Weihua Zhang and Lu Peng},
  doi          = {10.1109/TPDS.2019.2960233},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1107-1120},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Architectural support for NVRAM persistence in GPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient performance estimation and work-group size pruning
for OpenCL kernels on GPUs. <em>TPDS</em>, <em>31</em>(5), 1089–1106.
(<a href="https://doi.org/10.1109/TPDS.2019.2958343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphic Processing Units (GPUs) play a vital role in state-of-the-art high-performance scientific computing realm and research work towards its performance analysis is crucial but nontrivial. Extant GPU performance models are far from practical use, while fine-grained GPU simulation requires a considerably large time cost. Moreover, massive amounts of designs with various program inputs and parameter settings pose a challenge for efficient performance estimation and tuning of parallel GPU applications. To this end, this article presents a hybrid framework for the efficient performance estimation and work-group size pruning of OpenCL workloads on GPUs. The framework contains a static module used to extract the kernel execution trace from the high-level source code and a dynamical module used to mimic the kernel execution flow to estimate the runtime performance. For the design space pruning, an extra analysis is performed to filter out the redundant work-group sizes with duplicated execution traces and inferior pipelines. The proposed framework does not require any program runs to estimate the performance and find the optimal or near-optimal designs. Experiments on four Commercial Off-The-Shelf (COTS) Nvidia GPUs show that the framework can predict the runtime performance with an average error of 17.04 percent and reduce the program design space by an average of 78.47 percent.},
  archive      = {J_TPDS},
  author       = {Xiebing Wang and Xuehai Qian and Alois Knoll and Kai Huang},
  doi          = {10.1109/TPDS.2019.2958343},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1089-1106},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient performance estimation and work-group size pruning for OpenCL kernels on GPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Customer perceived value- and risk-aware multiserver
configuration for profit maximization. <em>TPDS</em>, <em>31</em>(5),
1074–1088. (<a href="https://doi.org/10.1109/TPDS.2019.2960024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the wide deployment of infrastructures and the rapid development of virtualization techniques in cloud computing, more and more enterprises begin to adopt cloud services, inspiring the emergence of various cloud service providers. The goal of cloud service providers is to pursue profit maximization. To achieve this goal, cloud service providers need to have a good understanding of the economics of cloud computing. However, the existing pricing strategies rarely consider the interaction between user requests for services and the cloud service provider and hence cannot accurately reflect the supply and demand law of the cloud service market. In addition, few previous pricing strategies take into account the risk involved in the pricing contract. In this article, we first propose a dynamic pricing strategy that is developed based on the customer perceived value (CPV) and is able to accurately capture the real situation of supply and demand in marketing. The strategy is utilized to estimate the user&#39;s demand for cloud services. We then design a profit maximization scheme that is developed based on the CPV-aware dynamic pricing strategy and considers the risk in the pricing contract. The scheme is utilized to derive the optimal multiserver configuration for maximizing the profit. Extensive simulations are carried out to verify the proposed customer perceived value and risk-aware profit maximization scheme. As compared to two state of the art benchmarking methods, the proposed scheme gains 31.6 and 30.8 percent more profit on average, respectively.},
  archive      = {J_TPDS},
  author       = {Tian Wang and Junlong Zhou and Gongxuan Zhang and Tongquan Wei and Shiyan Hu},
  doi          = {10.1109/TPDS.2019.2960024},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1074-1088},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Customer perceived value- and risk-aware multiserver configuration for profit maximization},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WEED-MC: Wavelet transform for energy efficient data
gathering and matrix completion. <em>TPDS</em>, <em>31</em>(5),
1066–1073. (<a href="https://doi.org/10.1109/TPDS.2019.2954902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed sensing based data gathering is in-efficient in small scale wireless sensor networks because of uncorrelated observations at the sink. Failure to exploit the low rank and suitable transform domain to explore the correlation structure of sensor data, results in significantly low recovery accuracy in such matrix completion algorithms for small scale networks. Targeting the spatio temporal correlation structure of the data, a novel data gathering and matrix completion scheme, which exploits the low rank property and compactness of sensor data which exists in wavelet transform domain, is proposed in this work. The compactness of the sensor data in wavelet transform domain is used for recovering the missing entries of the matrix. Experiments and simulations, for single-node and multi-node scenarios, prove the efficacy of the proposed approach over existing schemes significantly in terms of recovery accuracy even at extremely low sampling rate.},
  archive      = {J_TPDS},
  author       = {Vishal Krishna Singh and Bhoomika Nathani and Manish Kumar},
  doi          = {10.1109/TPDS.2019.2954902},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1066-1073},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {WEED-MC: Wavelet transform for energy efficient data gathering and matrix completion},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Proofs of physical reliability for cloud storage systems.
<em>TPDS</em>, <em>31</em>(5), 1048–1065. (<a
href="https://doi.org/10.1109/TPDS.2019.2958919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud service providers (CSPs) promise to reliably store repositories outsourced by clients. Unfortunately, once files have left the client&#39;s control, he has no means to verify their redundant storage. In this article, we develop Proof of Physical Reliability (PoPR) auditing mechanisms that prove that a CSP stores an outsourced repository across multiple physical storage nodes. A PoPR complements the existing proof-of-retrievability (PoR) and proof-of-data possession (PDP) methods that are concerned with file retrievability, but without any verification of the fault-tolerance to physical storage nodes failures. A PoPR goes beyond retrievability by verifying that a file is redundantly stored across multiple physical storage nodes according to a pre-agreed layout and can, therefore, survive node failures. The verification mechanism relies on a combination of storage integrity and timing tests on the simultaneous retrieval of a collection of file symbols from multiple storage nodes. Compared to the state-of-the-art, our approach accommodates CSPs with heterogeneous storage devices (hard disks, SSDs, etc.) and does not assume constant data processing nor network delays. Instead, it can operate under any delayvariance, because it relies only on (loose) delay bounds. We analytically prove the security of our construction and experimentally validate its success in heterogeneous storage settings.},
  archive      = {J_TPDS},
  author       = {Li Li and Loukas Lazos},
  doi          = {10.1109/TPDS.2019.2958919},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1048-1065},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Proofs of physical reliability for cloud storage systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general design for a scalable MPI-GPU multi-resolution 2D
numerical solver. <em>TPDS</em>, <em>31</em>(5), 1036–1047. (<a
href="https://doi.org/10.1109/TPDS.2019.2961909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a multi-GPU implementation of a Finite-Volume solver on a multi-resolution grid. The implementation completely offloads the computation to the GPUs and communications between different GPUs are implemented by means of the Message Passing Interface (MPI) API. Different domain decomposition techniques have been considered and the one based on the Hilbert Space Filling Curves (HSFC) showed optimal scalability. Several optimizations are introduced: One-to-one MPI communications among MPI ranks are completely masked by GPU computations on internal cells and a novel dynamic load balancing algorithm is introduced to minimize the waiting times at global MPI synchronization barriers. Such algorithm adapts the computational load of ranks in response to dynamical changes in the execution time of blocks and in network performances; Its capability to converge to a balanced computation has been empirically shown by numerical experiments. Tests exploit up to 64 GPUs and 83M cells and achieve an efficiency of 90 percent in weak scalability and 85 percent for strong scalability. The framework is general and the results of the article can be ported to a wide range of explicit 2D Partial Differential Equations solvers.},
  archive      = {J_TPDS},
  author       = {Massimiliano Turchetto and Alessandro Dal Palù and Renato Vacondio},
  doi          = {10.1109/TPDS.2019.2961909},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1036-1047},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A general design for a scalable MPI-GPU multi-resolution 2D numerical solver},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Making application-level crash consistency practical on
flash storage. <em>TPDS</em>, <em>31</em>(5), 1009–1020. (<a
href="https://doi.org/10.1109/TPDS.2019.2959305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the design, implementation, and evaluation of a new file system, called ACCFS, supporting application-level crash consistency as its first-class citizen functionality. With ACCFS, application data can be correctly recovered in the event of system crashes without any complex update protocol at the application level. With the help of the SHARE interface supporting atomic address remapping at the flash storage layer, ACCFS can easily and efficiently achieve crash consistency as well as single-write journaling. We prototyped ACCFS by slightly modifying the full data journal mode in ext4, implemented the SHARE interface as firmware in a commercial SSD available in the market, and carried out various experiments by running ACCFS on top of the SSD. Our preliminary experimental results are very promising. For instance, the performance of an OLTP benchmark using MySQL/InnoDB engine can be boosted by more than 2-6x by offloading the responsibility of guaranteeing the atomic write of MySQL data pages from the InnoDB engine&#39;s own journaling mechanism to ACCFS. This impressive performance gain is in part due to the single-write journaling in ACCFS and in part comes from the fact that the frequent fsync() calls caused by the complex update protocol at the application level can be avoided. ACCFS is a practical solution for the crash consistency problem in that (1) the SHARE interface can be, like the TRIM command, easily supported by commercial SSDs, (2) it can be embodied with a minor modification on the existing ext4 file system, and (3) the existing applications can be made crash consistent simply by opening files in O_ATOMIC mode while the legacy applications can be run without any change.},
  archive      = {J_TPDS},
  author       = {Dong Hyun Kang and Changwoo Min and Sang-Won Lee and Young Ik Eom},
  doi          = {10.1109/TPDS.2019.2959305},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1009-1020},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Making application-level crash consistency practical on flash storage},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large-scale automatic k-means clustering for heterogeneous
many-core supercomputer. <em>TPDS</em>, <em>31</em>(5), 997–1008. (<a
href="https://doi.org/10.1109/TPDS.2019.2955467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an automatic k-means clustering solution targeting the Sunway TaihuLight supercomputer. We first introduce a multilevel parallel partition approach that not only partitions by dataflow and centroid, but also by dimension, which unlocks the potential of the hierarchical parallelism in the heterogeneous many-core processor and the system architecture of the supercomputer. The parallel design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability. Furthermore, we propose an automatic hyper-parameter determination process for k-means clustering, by automatically generating and executing the clustering tasks with a set of candidate hyper-parameter, and then determining the optimal hyper-parameter using a proposed evaluation method. The proposed autoclustering solution can not only achieve high performance and scalability for problems with massive high-dimensional data, but also support clustering without sufficient prior knowledge for the number of targeted clusters, which can potentially increase the scope of k-means algorithm to new application areas.},
  archive      = {J_TPDS},
  author       = {Teng Yu and Wenlai Zhao and Pan Liu and Vladimir Janjic and Xiaohan Yan and Shicai Wang and Haohuan Fu and Guangwen Yang and John Thomson},
  doi          = {10.1109/TPDS.2019.2955467},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {997-1008},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Large-scale automatic K-means clustering for heterogeneous many-core supercomputer},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate NoC and memory controller architectures for
GPGPU accelerators. <em>TPDS</em>, <em>31</em>(5), 25–39. (<a
href="https://doi.org/10.1109/TPDS.2019.2958344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High interconnect bandwidth is crucial for achieving better performance in many-core GPGPU architectures that execute highly data parallel applications. The parallel warps of threads running on shader cores generate a high volume of read requests to the main memory due to the limited size of data caches at the shader cores. This leads to a scenarios with rapid arrival of an even larger volume of reply data from the DRAM, which creates a bottleneck at memory controllers (MCs) that send reply packets back to the requesting cores over the network-on-chip (NoC). Coping with such high volumes of data requires intelligent memory scheduling and innovative NoC architectures. To mitigate memory bottlenecks in GPGPUs, we first propose a novel approximate memory controller architecture (AMC) that reduces the DRAM latency by opportunistically exploiting row buffer locality and bank level parallelism in memory request scheduling, and leverages approximability of the reply data from DRAM, to reduce the number of reply packets injected into the NoC. To further realize high throughput and low energy communication in GPGPUs, we propose a low power, approximate NoC architecture (Dapper) that increases the utilization of the available network bandwidth by using single cycle overlay circuits for the reply traffic between MCs and shader cores. Experimental results show that Dapper and AMC together increase NoC throughput by up to 21 percent; and reduce NoC latency by up to 45.5 percent and energy consumed by the NoC and MC by up to 38.3 percent, with minimal impact on output accuracy, compared to state-of-the-art approximate NoC/MC architectures.},
  archive      = {J_TPDS},
  author       = {Venkata Yaswanth Raparti and Sudeep Pasricha},
  doi          = {10.1109/TPDS.2019.2958344},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {25-39},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Approximate NoC and memory controller architectures for GPGPU accelerators},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards power efficient high performance packet i/o.
<em>TPDS</em>, <em>31</em>(4), 981–996. (<a
href="https://doi.org/10.1109/TPDS.2019.2957746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, high performance packet I/O frameworks continue to flourish for their ability to process packets from high-speed links. To achieve high throughput and low latency, high performance packet I/O frameworks usually employ busy polling. As busy polling will burn all CPU cycles even if there&#39;s no packet to process, these frameworks are quite power inefficient. However, exploiting power management techniques such as DVFS and LPI in the frameworks is challenging, because neither the OS nor the frameworks can provide information (e.g., actual CPU utilization, available idle period, or the target frequency) required by these techniques. In this article, we establish a model that can formulate the packet processing flow of high performance packet I/O to help and address the above challenges. From the model, we can deduce the information needed for power management techniques, and gain the insights to balance the power and latency. After suggesting to use pause instruction to reduce CPU power within short idle period, we propose two approaches to conduct power conservation for high performance packet I/O: one with the aid of traffic information and the other without. Experiments with Intel DPDK show that both approaches can achieve significant power reduction with little latency increase.},
  archive      = {J_TPDS},
  author       = {Xuesong Li and Wenxue Cheng and Tong Zhang and Fengyuan Ren and Bailong Yang},
  doi          = {10.1109/TPDS.2019.2957746},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {981-996},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards power efficient high performance packet I/O},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). COPA: Highly cost-effective power back-up for green
datacenters. <em>TPDS</em>, <em>31</em>(4), 967–980. (<a
href="https://doi.org/10.1109/TPDS.2019.2948336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional datacenters employ costly diesel generators (DG) and uninterrupted power supplies (UPS) to back up power. However, some or even all racks of a green datacenter can still be powered by renewable energy during grid power outages. This makes the utilization of the DGs and UPSs in green datacenters significantly lower than in traditional datacenters. In this paper, we propose a highly cost-effective power back-up (COPA) approach for green datacenters by leveraging the availability characteristics of renewable energy as well as grid power outages. COPA contributes three new techniques. The first technique, called least UPS capacity planning, determines the least rated power capability and runtime of the UPSs to guarantee the normal operations of a green datacenter during grid power outages. The second technique, named cooperative UPS/renewable power supply, employs UPS and renewable energy at the same time to supply power to each rack when grid power fails. The last one, dubbed renewable-energy-aware dynamic power management, controls the power consumption dynamically based on the available capacity of renewable energy and UPS. We build an experimental cluster consisting of 10 servers, and use four representative benchmarks as well as verified data about the availability characteristics of solar and wind energy to evaluate COPA. The results show that COPA reduces 47 percent and 70 percent of the power back-up cost for a solar energy powered datacenter and a wind energy powered datacenter, respectively. Moreover, COPA guarantees the application&#39;s Service Level Agreement (SLA) for at least 20 minutes (over 79 percent outages) and 56 minutes on average while enabling the back-up power to last for at least 2 hours and for 3 hours on average, which cannot be achieved by other under-provisioning power back-up approaches.},
  archive      = {J_TPDS},
  author       = {Yan Yin and Junmin Wu and Xu Zhou and Lieven Eeckhout and Amer Qouneh and Tao Li and Zhibin Yu},
  doi          = {10.1109/TPDS.2019.2948336},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {967-980},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {COPA: Highly cost-effective power back-up for green datacenters},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online placement and scaling of geo-distributed machine
learning jobs via volume-discounting brokerage. <em>TPDS</em>,
<em>31</em>(4), 948–966. (<a
href="https://doi.org/10.1109/TPDS.2019.2955935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geo-distributed machine learning (ML) often uses large geo-dispersed data collections produced over time to train global models, without consolidating the data to a central site. In the parameter server architecture, “workers” and “parameter servers” for a geo-distributed ML job should be strategically deployed and adjusted on the fly, to allow easy access to the datasets and fast exchange of the model parameters at anytime. Despite many cloud platforms now provide volume discounts to encourage the usage of their ML resources, different geo-distributed ML jobs that run in the clouds often rent cloud resources separately and respectively, thus rarely enjoying the benefit of discounts. We study an ML broker service that aggregates geo-distributed ML jobs into cloud data centers for volume discounts via dynamic online placement and scaling of workers and parameter servers in individual jobs for long-term cost minimization. To decide the number and the placement of workers and parameter servers, we propose an efficient online algorithm which first decomposes the online problem into a series of one-shot optimization problems solvable at each individual time slot by the technique of regularization, and afterwards round the fractional decisions to the integer ones via a carefully-designed dependent rounding method. We prove a parameterized-constant competitive ratio for our online algorithm as the theoretical performance analysis, and also conduct extensive simulation studies to exhibit its close-to-offline-optimum practical performance in realistic settings.},
  archive      = {J_TPDS},
  author       = {Xiaotong Li and Ruiting Zhou and Lei Jiao and Chuan Wu and Yuhang Deng and Zongpeng Li},
  doi          = {10.1109/TPDS.2019.2955935},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {948-966},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online placement and scaling of geo-distributed machine learning jobs via volume-discounting brokerage},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient method for parallel computation of geodesic
transformation on CPU. <em>TPDS</em>, <em>31</em>(4), 935–947. (<a
href="https://doi.org/10.1109/TPDS.2019.2953057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a fast Central Processing Unit (CPU) implementation of geodesic morphological operations using stream processing. In contrast to the current state-of-the-art, that focuses on achieving insensitivity to the filter sizes with efficient data structures, the proposed approach achieves efficient computation of long chains of elementary 3 x 3 filters using multicore and Single Instruction Multiple Data (SIMD) processing. In comparison to the related methods, up to 100 times faster computation of common geodesic operators is achieved in this way, allowing for real-time processing (with over 30 FPS) of up to 1500 filters long chains, applied on 1024 x 1024 images. In addition, the proposed approach outperformed GPGPU, and proved to be more efficient than the comparable streaming method for the computation of morphological erosions and dilations with window sizes up to 183 x 183 in the case of using char and 27 x 27 when using double data types.},
  archive      = {J_TPDS},
  author       = {Danijel Žlaus and Domen Mongus},
  doi          = {10.1109/TPDS.2019.2953057},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {935-947},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient method for parallel computation of geodesic transformation on CPU},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards accurate prediction for high-dimensional and
highly-variable cloud workloads with deep learning. <em>TPDS</em>,
<em>31</em>(4), 923–934. (<a
href="https://doi.org/10.1109/TPDS.2019.2953745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource provisioning for cloud computing necessitates the adaptive and accurate prediction of cloud workloads. However, the existing methods cannot effectively predict the high-dimensional and highly-variable cloud workloads. This results in resource wasting and inability to satisfy service level agreements (SLAs). Since recurrent neural network (RNN) is naturally suitable for sequential data analysis, it has been recently used to tackle the problem of workload prediction. However, RNN often performs poorly on learning long-term memory dependencies, and thus cannot make the accurate prediction of workloads. To address these important challenges, we propose a deep Learning based Prediction Algorithm for cloud Workloads (L-PAW). First, a top-sparse auto-encoder (TSA) is designed to effectively extract the essential representations of workloads from the original high-dimensional workload data. Next, we integrate TSA and gated recurrent unit (GRU) block into RNN to achieve the adaptive and accurate prediction for highly-variable workloads. Using real-world workload traces from Google and Alibaba cloud data centers and the DUX-based cluster, extensive experiments are conducted to demonstrate the effectiveness and adaptability of the L-PAW for different types of workloads with various prediction lengths. Moreover, the performance results show that the L-PAW achieves superior prediction accuracy compared to the classic RNN-based and other workload prediction methods for high-dimensional and highly-variable real-world cloud workloads.},
  archive      = {J_TPDS},
  author       = {Zheyi Chen and Jia Hu and Geyong Min and Albert Y. Zomaya and Tarek El-Ghazawi},
  doi          = {10.1109/TPDS.2019.2953745},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {923-934},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards accurate prediction for high-dimensional and highly-variable cloud workloads with deep learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-aware application placement in mobile edge computing:
A stochastic optimization approach. <em>TPDS</em>, <em>31</em>(4),
909–922. (<a href="https://doi.org/10.1109/TPDS.2019.2950937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Quality of Service (QoS) in Mobile Edge Computing (MEC) systems is significantly dependent on the application offloading and placement decisions. Due to the movement of users in MEC networks, an optimal application placement might turn into the least efficient placement in few minutes. Thus, it is crucial to take the dynamics of the system into account when designing application placement mechanisms. On the other hand, energy consumption of servers is a significant component of the cost of services in MEC systems and must also be considered in the design of the mechanisms. In this article, we model the problem of energy-aware application placement in edge computing systems as a multi-stage stochastic program. The objective is to maximize the QoS of the system while taking into account the limited energy budget of the edge servers. To solve the problem, we design a novel parallel Sample Average Approximation (SAA) algorithm. We conduct an extensive experimental analysis to evaluate the performance of the proposed algorithm using real-world trace data.},
  archive      = {J_TPDS},
  author       = {Hossein Badri and Tayebeh Bahreini and Daniel Grosu and Kai Yang},
  doi          = {10.1109/TPDS.2019.2950937},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {909-922},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-aware application placement in mobile edge computing: A stochastic optimization approach},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HRHS: A high-performance real-time hardware scheduler.
<em>TPDS</em>, <em>31</em>(4), 897–908. (<a
href="https://doi.org/10.1109/TPDS.2019.2952136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article represents an on-line time-predictable distributed hardware scheduler solution, suitable for many-core systems. We have partitioned the Main scheduler into uniform Partial schedulers to achieve a significant gain in term of performance and scalability, while software scheduling solutions impose excessive delays (in order of thousands of clock cycles) to a system. Although we have considered the implementation of the Earliest Deadline First (EDF) algorithm for each Partial scheduler, one can use customized scheduling policies, as needed. Designers can also modify different parts of our proposed architecture to obtain more suitable hardware for their design. HRHS outperforms conventional schedulers, in terms of resource utilization (LUT, register), delay and energy consumption by 36.83, 22.93, 46.36 and 59.26 percent on average, respectively. It also overpowers clustering solutions by circumventing their intrinsic off-line characteristics. The presented designs are also implemented in ASIC with 45-nanometer technology, in which the HRHS design excels in power, area and critical path length by 49.33, 50.67, and 53.33 percent on average, respectively, over other designs implemented in this article.},
  archive      = {J_TPDS},
  author       = {Danesh Derafshi and Amin Norollah and Mohsen Khosroanjam and Hakem Beitollahi},
  doi          = {10.1109/TPDS.2019.2952136},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {897-908},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HRHS: A high-performance real-time hardware scheduler},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining size-based load balancing with round-robin for
scalable low latency. <em>TPDS</em>, <em>31</em>(4), 886–896. (<a
href="https://doi.org/10.1109/TPDS.2019.2950621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When dispatching jobs to parallel servers, or queues, the highly scalable round-robin (RR) scheme reduces the variance of interarrival times at all queues to a great extent but has no impact on the variances of service processes. Contrariwise, size-interval task assignment (SITA) routing has little impact on the variances of interarrival times but makes the service processes as deterministic as possible. In this paper, we unify both `static&#39; approaches to design a scalable load balancing framework able to control the variances of the arrival and service processes jointly. It turns out that the resulting combination significantly improves performance and is able to drive the mean job delay to zero in the large-system limit; it is known that this property is not achieved when both approaches are considered separately. Within realistic parameters, we show that the optimal number of size intervals that partition the support of the job size distribution is small with respect to the system size. This enhances the applicability of the proposed load balancing scheme at a large scale. In fact, we find that adding a little bit of information about job sizes to a dispatcher operating under RR improves performance a lot. Under the optimal scaling of size intervals and assuming highly variable job sizes, numerical simulations indicate that the proposed algorithm is competitive with the (less scalable) join-the-shortest-workload algorithm even when the system size grows large.},
  archive      = {J_TPDS},
  author       = {Jonatha Anselmi},
  doi          = {10.1109/TPDS.2019.2950621},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {886-896},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Combining size-based load balancing with round-robin for scalable low latency},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reliability aware energy optimized scheduling of
non-preemptive periodic real-time tasks on heterogeneous multiprocessor
system. <em>TPDS</em>, <em>31</em>(4), 871–885. (<a
href="https://doi.org/10.1109/TPDS.2019.2950251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher reliability and lower energy consumption are conflicting, yet among the most important design objectives for the real-time systems. Moreover, in the domain of real-time systems, non-preemptive scheduling is relatively unexplored with objectives such as reliability and energy. Thus we propose an active replication based framework to schedule a set of periodic real-time tasks in the non-preemptive heterogeneous environment such that the given reliability and timing constraints are satisfied whereas the energy consumption is minimized. First, we formulate the problem as a constraint optimization problem that provides an optimal solution; however, it does not scale well. Thus, we also propose heuristics which apply reservation of processors and reallocation of jobs, to compute suboptimal solution efficiently in terms of energy consumption as well as schedulability. Heuristics make use of the interplay of task-level reliability target, reliability of replicas, number of replicas, reliability of tasks, and energy consumption. We perform an experimental study on the test cases generated by extending UUnisort algorithm [1] and observe the effect of various simulation parameters on energy consumption and schedulability.},
  archive      = {J_TPDS},
  author       = {Niraj Kumar and Jaishree Mayank and Arijit Mondal},
  doi          = {10.1109/TPDS.2019.2950251},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {871-885},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reliability aware energy optimized scheduling of non-preemptive periodic real-time tasks on heterogeneous multiprocessor system},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel multi-stage forest-based key-value store for
holistic performance improvement. <em>TPDS</em>, <em>31</em>(4),
856–870. (<a href="https://doi.org/10.1109/TPDS.2019.2950248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Key-value (KV) stores based on multi-stage structures are widely deployed to organize massive amounts of easily searchable user data. However, current KV storage systems inevitably sacrifice at least one of the performance objectives, such as write, read, space efficiency etc., for the optimization of others. To understand the root cause of and ultimately remove such performance disparities among the representative existing KV stores, we analyze their enabling mechanisms and classify them into two fundamental models of data structures facilitating KV operations, namely, the multi-stage tree (MS-tree), and the multi-stage forest (MS-forest). We build SifrDB, a KV store on a novel split forest structure, that achieves the lowest write amplification across all workload patterns and minimizes space reservation for the compaction. To mitigate the read amplification inherent in MS-forest, we introduce a bloom filer mechanism based on Sorted String Tables (SSTs). Furthermore, we also present a highly efficient parallel search approach that fully exploits the access parallelism of modern flash-based storage devices to substantially boost the read performance. Evaluation results show that under both micro and YCSB benchmarks, SifrDB outperforms its closest competitors, i.e., the popular MS-forest implementations, making it a highly desirable choice for the modern KV stores.},
  archive      = {J_TPDS},
  author       = {Ziyi Lu and Qiang Cao and Fei Mei and Hong Jiang and Jingjun Li},
  doi          = {10.1109/TPDS.2019.2950248},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {856-870},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A novel multi-stage forest-based key-value store for holistic performance improvement},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GQoS: A QoS-oriented GPU virtualization with adaptive
capacity sharing. <em>TPDS</em>, <em>31</em>(4), 843–855. (<a
href="https://doi.org/10.1109/TPDS.2019.2948753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the virtualization technologies for cloud computing infrastructures supporting extra devices, such as GPU, require additional development and refinement. This requirement is particularly evident in the area of resource sharing and allocation under some performance constraints, like the quality of service (QoS) guarantee, in light of the closed GPU platform. This deficiency significantly limits the applicability range of the cloud platform, which aims to support the efficient and fluent execution of business and academic workloads. This paper introduces gQoS, an adaptive virtualized GPU resource capacity sharing system under the QoS target, which can share and allocate the virtualized GPU resource among workloads adaptively, guaranteeing the QoS level with stability and accuracy. We evaluate the workloads and compare our gQoS strategy with other allocation strategies. The experiments show that our strategy guarantees much better accuracy and stability in QoS control and that the total GPU resource utilization under gQoS can be rewarded with at most a 25.85 percent reduction compared with other strategies.},
  archive      = {J_TPDS},
  author       = {Qiumin Lu and Jianguo Yao and Haibing Guan and Ping Gao},
  doi          = {10.1109/TPDS.2019.2948753},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {843-855},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GQoS: A QoS-oriented GPU virtualization with adaptive capacity sharing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A holistic heterogeneity-aware data placement scheme for
hybrid parallel i/o systems. <em>TPDS</em>, <em>31</em>(4), 830–842. (<a
href="https://doi.org/10.1109/TPDS.2019.2948901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present H2DP , a holistic heterogeneity-aware data placement scheme for hybrid parallel I/O systems, which consist of HDD servers and SSD servers. Most of the existing approaches focus on server performance or application I/O pattern heterogeneity in data placement. H2DP considers three axes of heterogeneity: server performance, server space, and application I/O pattern. More specifically, H2DP determines the optimized stripe sizes on servers based on server performance, keeps only critical data on all hybrid servers and the rest data on HDD servers, and dynamically migrates data among different types of servers at run-time. This holistic heterogeneity-awareness enables H2DP to achieve high performance by alleviating server load imbalance, efficiently utilizing SSD space, and accommodating application pattern variation. We have implemented a prototype of H2DP under MPICH2 atop OrangeFS. Extensive experimental results demonstrate that H2DP significantly improve I/O system performance compared to existing data placement schemes.},
  archive      = {J_TPDS},
  author       = {Shuibing He and Zheng Li and Jiang Zhou and Yanlong Yin and Xiaohua Xu and Yong Chen and Xian-He Sun},
  doi          = {10.1109/TPDS.2019.2948901},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {830-842},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A holistic heterogeneity-aware data placement scheme for hybrid parallel I/O systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On fault-tolerant bin packing for online resource
allocation. <em>TPDS</em>, <em>31</em>(4), 817–829. (<a
href="https://doi.org/10.1109/TPDS.2019.2948327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an online fault-tolerant bin packing problem that models reliable resource allocation. In this problem, each item is replicated and has f + 1 replicas including one primary and f standbys. The packing of items is required to tolerate up to f faulty bins, i.e., to guarantee that at least one correct replica of each item is available regardless of which f bins turn to be faulty. Any feasible packing algorithm must satisfy an exclusion constraint and a space constraint. The exclusion constraint is generalized from the fault tolerance requirement and the space constraint comes from the capacity planning. The target of bin packing is to minimize the number of bins used. We first derive a lower bound on the number of bins needed by any feasible packing algorithm. We then study three heuristic algorithms named mirroring, shifting and mixing under a particular setting where all items have the same size. The mirroring algorithm has a low utilization of the bin capacity. Compared with the mirroring algorithm, the shifting algorithm requires fewer bins. However, in online packing, the process of opening bins by the shifting algorithm is not smooth. It turns out that even for packing a few items, the shifting algorithm needs to quickly open a large number of bins. The mixing algorithm adopts a dual average strategy to gradually open new bins for incoming items. We prove that the mixing algorithm is feasible and show that it balances the number of bins used and the process of opening bins. Finally, to pack items with different sizes, we extend the mirroring algorithm by adopting the First-Fit strategy and extend both the shifting and mixing algorithms by involving the harmonic strategy. The asymptotic competitive ratios of the three extended algorithms are analyzed respectively.},
  archive      = {J_TPDS},
  author       = {Chuanyou Li and Xueyan Tang},
  doi          = {10.1109/TPDS.2019.2948327},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {817-829},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On fault-tolerant bin packing for online resource allocation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantum supremacy circuit simulation on sunway TaihuLight.
<em>TPDS</em>, <em>31</em>(4), 805–816. (<a
href="https://doi.org/10.1109/TPDS.2019.2947511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid progress made by industry and academia, quantum computers with dozens of qubits or even larger size are being realized. However, the fidelity of existing quantum computers often sharply decreases as the circuit depth increases. Thus, an ideal quantum circuit simulator on classical computers, especially on high-performance computers, is needed for benchmarking and validation. We design a large-scale simulator of universal random quantum circuits, often called “quantum supremacy circuits”, and implement it on Sunway TaihuLight. The simulator can be used to accomplish the following two tasks: 1) Computing a complete output state-vector; 2) Calculating one or a few amplitudes. We target the simulation of 49-qubit circuits. For task 1), we successfully simulate such a circuit of depth 39, and for task 2) we reach the 55-depth level. To the best of our knowledge, both of the simulation results reach the largest depth for 49-qubit quantum supremacy circuits.},
  archive      = {J_TPDS},
  author       = {Riling Li and Bujiao Wu and Mingsheng Ying and Xiaoming Sun and Guangwen Yang},
  doi          = {10.1109/TPDS.2019.2947511},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {805-816},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Quantum supremacy circuit simulation on sunway TaihuLight},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resource-constrained replication strategies for hierarchical
and heterogeneous tasks. <em>TPDS</em>, <em>31</em>(4), 793–804. (<a
href="https://doi.org/10.1109/TPDS.2019.2945294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale cloud computing systems, a task is often divided into multiple subtasks which can be executed in parallel in different machines. As a result, the task completion time is constrained by the completion time of the slowest subtask. To reduce the task completion time, the strategy of replicating the straggling subtasks has been employed in cloud computing frameworks such as MapReduce and Hadoop. Analyzing mathematically the performance of such replication strategies has recently received great attention. However, most of the analytical work focuses on the case where the completion times of the subtasks are identically distributed. This assumption may not hold in practice due to the modularization and encapsulation of the computation of a task, resulting in different service requirements for different subtasks. In this paper, we consider the case where the completion times of the subtasks of a task are drawn from heterogeneous/empirical distributions. Furthermore, we consider the case where jobs consist of hierarchical tasks that are required to be executed in a specific order described by a task precedence graph. We propose a novel framework to investigate how to allocate replication resources among the subtasks such that the overall task completion time is minimized. Specifically, we devise a Lagrange multiplier-based method and a water-filling-like algorithm for integer programs. We show via analysis and simulations the optimality and efficiency of our proposed algorithms, and explore the tradeoff between cost and latency from introducing replications in a task graph.},
  archive      = {J_TPDS},
  author       = {Weng Chon Ao and Konstantinos Psounis},
  doi          = {10.1109/TPDS.2019.2945294},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {793-804},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Resource-constrained replication strategies for hierarchical and heterogeneous tasks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hotspot-aware hybrid memory management for in-memory
key-value stores. <em>TPDS</em>, <em>31</em>(4), 779–792. (<a
href="https://doi.org/10.1109/TPDS.2019.2945315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Non-Volatile Memory (NVM) technologies promise much higher memory density and energy efficiency than DRAM, at the expense of higher read/write latency and limited write endurance. Hybrid memory systems composed of DRAM and NVM have the potential to provide very large capacity of main memory for in-memory key-value (K-V) stores. However, there remains challenges to directly deploy DRAM-based K-V stores in hybrid memory systems. The performance and energy efficiency of K-V stores on hybrid memory systems have not been fully explored yet. In this paper, we propose HMCached, an in-memory K-V store built on a hybrid DRAM/NVM system. HMCached utilizes an application-level data access counting mechanism to identify frequently-accessed (hotspot) objects (i.e., K-V pairs) in NVM, and migrates them to fast DRAM to reduce the costly NVM accesses. We also propose an NVM-friendly index structure to store the frequently-updated portion of object metadata in DRAM, and thus further mitigate the NVM accesses. Moreover, we propose a benefit-aware memory reassignment policy to address the slab calcification problem in slab-based K-V store systems, and significantly improve the benefit gain from the DRAM. We implement the proposed schemes with Memcached and evaluate it with Zipfian-like workloads. Experiment results show that HMCached significantly reduces NVM accesses by 70 percent compared to the vanilla Memcached running on a DRAM/NVM hybrid memory system without any optimizations, and improves application performance by up to 50 percent. Moreover, compared to a DRAM-only system, HMCached achieves 90 percent of performance and 46 percent reduction of energy consumption for realistic (read-intensive) workloads while significantly reducing the DRAM usage by 75 percent.},
  archive      = {J_TPDS},
  author       = {Hai Jin and Zhiwei Li and Haikun Liu and Xiaofei Liao and Yu Zhang},
  doi          = {10.1109/TPDS.2019.2945315},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {779-792},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hotspot-aware hybrid memory management for in-memory key-value stores},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CCUDA: Effective co-scheduling of concurrent kernels on
GPUs. <em>TPDS</em>, <em>31</em>(4), 766–778. (<a
href="https://doi.org/10.1109/TPDS.2019.2944602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While GPUs are meantime omnipresent for many scientific and technical computations, they still continue to evolve as processors. An important recent feature is the ability to execute multiple kernels concurrently via queue streams. However, experiments show that different parameters including the behavior of kernels, the order of kernel launches and other execution configurations, e.g., the number of concurrent thread blocks, may result in different execution time for concurrent kernel execution. Since kernels may have different resource requirements, they can be classified into different classes, which are traditionally assumed as either memory-bound or compute-bound. However, a kernel may belong to the different classes on different hardware according to the hardware resources. In this paper, the definition of kernel mix intensity is introduced. Based on this, a scheduling framework called concurrent CUDA (cCUDA) is proposed to co-schedule the concurrent kernels more efficiently. It first profiles and ranks kernels with different execution behaviors and then takes the kernel resource requirements into account to partition thread blocks of different kernels and overlap them to better utilize the GPU resources. Experimental results on real hardware demonstrate performance improvement in terms of execution time of up to 1.86x, and an average speedup of 1.28x for a wide range of kernels. cCUDA is available at https://github.com/kshekofteh/cCUDA.},
  archive      = {J_TPDS},
  author       = {S.-Kazem Shekofteh and Hamid Noori and Mahmoud Naghibzadeh and Holger Fröning and Hadi Sadoghi Yazdi},
  doi          = {10.1109/TPDS.2019.2944602},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {766-778},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CCUDA: Effective co-scheduling of concurrent kernels on GPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power-aware allocation of graph jobs in geo-distributed
cloud networks. <em>TPDS</em>, <em>31</em>(4), 749–765. (<a
href="https://doi.org/10.1109/TPDS.2019.2943457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big-data, the jobs submitted to the clouds exhibit complicated structures represented by graphs, where the nodes denote the sub-tasks each of which can be accommodated at a slot in a server, while the edges indicate the communication constraints among the sub-tasks. We develop a framework for efficient allocation of graph jobs in geo-distributed cloud networks (GDCNs), explicitly considering the power consumption of the datacenters (DCs). We address the following two challenges arising in graph job allocation: i) the allocation problem belongs to NP-hard nonlinear integer programming; ii) the allocation requires solving the NP-complete sub-graph isomorphism problem, which is particularly cumbersome in large-scale GDCNs. We develop a suite of efficient solutions for GDCNs of various scales. For small-scale GDCNs, we propose an analytical approach based on convex programming. For medium-scale GDCNs, we develop a distributed allocation algorithm exploiting the processing power of DCs in parallel. Afterward, we provide a novel low-complexity (decentralized) sub-graph extraction method, based on which we introduce cloud crawlers aiming to extract allocations of good potentials for large-scale GDCNs. Given these suggested strategies, we further investigate strategy selection under both fixed and adaptive DC pricing schemes, and propose an online learning algorithm for each.},
  archive      = {J_TPDS},
  author       = {Seyyedali Hosseinalipour and Anuj Nayak and Huaiyu Dai},
  doi          = {10.1109/TPDS.2019.2943457},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {749-765},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Power-aware allocation of graph jobs in geo-distributed cloud networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An attribute-based availability model for large scale IaaS
clouds with CARMA. <em>TPDS</em>, <em>31</em>(3), 733–748. (<a
href="https://doi.org/10.1109/TPDS.2019.2943339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High availability is one of the core properties of Infrastructure as a Service (IaaS) and ensures that users have anytime access to on-demand cloud services. However, significant variations of workflow and the presence of super-tasks, mean that heterogeneous workload can severely impact the availability of IaaS clouds. Although previous work has investigated global queues, VM deployment, and failure of PMs, two aspects are yet to be fully explored: one is the impact of task size and the other is the differing features across PMs such as the variable execution rate and capacity. To address these challenges we propose an attribute-based availability model of large scale IaaS developed in the formal modeling language CARMA. The size of tasks in our model can be a fixed integer value or follow the normal, uniform or log-normal distribution. Additionally, our model also provides an easy approach to investigating how to arrange the slack and normal resources in order to achieve availability levels. The two goals of our work are providing an analysis of the availability of IaaS and showing that the use of CARMA allows us to easily model complex phenomena that were not readily captured by other existing approaches.},
  archive      = {J_TPDS},
  author       = {Hongwu Lv and Jane Hillston and Paul Piho and Huiqiang Wang},
  doi          = {10.1109/TPDS.2019.2943339},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {733-748},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An attribute-based availability model for large scale IaaS clouds with CARMA},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online scheduling of task graphs on heterogeneous platforms.
<em>TPDS</em>, <em>31</em>(3), 721–732. (<a
href="https://doi.org/10.1109/TPDS.2019.2942909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern computing platforms commonly include accelerators. We target the problem of scheduling applications modeled as task graphs on hybrid platforms made of two types of resources, such as CPUs and GPUs. We consider that task graphs are uncovered dynamically, and that the scheduler has information only on the available tasks, i.e., tasks whose predecessors have all been completed. Each task can be processed by either a CPU or a GPU, and the corresponding processing times are known. Our study extends a previous 4√m/k-competitive online algorithm by Amaris et al. [1], where mis the number of CPUs and k the number of GPUs (m≥k). We prove that no online algorithm can have a competitive ratio smaller than √m/k . We also study how adding flexibility on task processing, such as task migration or spoliation, or increasing the knowledge of the scheduler by providing it with information on the task graph, influences the lower bound. We provide a (2√m/k+1)-competitive algorithm as well as a tunable combination of a system-oriented heuristic and a competitive algorithm; this combination performs well in practice and has a competitive ratio in Θ(√m/k). We also adapt all our results to the case of multiple types of processors. Finally, simulations on different sets of task graphs illustrate how the instance properties impact the performance of the studied algorithms and show that our proposed tunable algorithm performs the best among the online algorithms in almost all cases and has even performance close to an offline algorithm.},
  archive      = {J_TPDS},
  author       = {Louis-Claude Canon and Loris Marchal and Bertrand Simon and Frédéric Vivien},
  doi          = {10.1109/TPDS.2019.2942909},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {721-732},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online scheduling of task graphs on heterogeneous platforms},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A high throughput b+tree for SIMD architectures.
<em>TPDS</em>, <em>31</em>(3), 707–720. (<a
href="https://doi.org/10.1109/TPDS.2019.2942918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {B+tree is one of the most important data structures and has been widely used in different fields. With the increase of concurrent queries and data-scale in storage, designing an efficient B+tree structure has become critical. Due to abundant computation resources, SIMD architectures provide potential opportunities to achieve high query throughput for B+tree. However, prior methods cannot achieve satisfactory performance results due to low resource utilization and poor memory performance. In this paper, we first identify the gaps between B+tree and SIMD architectures. Concurrent B+tree queries involve many global memory accesses and different divergences, which mismatch with SIMD architecture features. Based on this observation, we propose Harmonia, a novel B+tree structure to bridge the gaps. In Harmonia, a B+tree structure is divided into a key region and a child region. The key region stores the nodeswith its keys in a breadth-first order. The child region is organized as a prefix-sum array, which only stores each node&#39;s first child index in the key region. Since the prefix-sum child region is small and the children&#39;s index can be retrieved through index computations, most of it can be stored in on-chip caches, which can achieve good cache locality. To make it more efficient, Harmonia also includes two optimizations: partially-sorted aggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization. Evaluations on a 28-core INTEL CPU show that Harmonia can achieve up to 207 million queries per second, which is about 1.7X faster than that of CPU-based HB+Tree, a recent state-of-the-art solution. And on a Volta TITAN V GPU, it can achieve up to 3.6 billion queries per second, which is about 3.4X faster than that of GPU-based HB+Tree.},
  archive      = {J_TPDS},
  author       = {Weihua Zhang and Zhaofeng Yan and Yuzhe Lin and Chuanlei Zhao and Lu Peng},
  doi          = {10.1109/TPDS.2019.2942918},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {707-720},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A high throughput b+tree for SIMD architectures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring new opportunities to defeat low-rate DDoS attack
in container-based cloud environment. <em>TPDS</em>, <em>31</em>(3),
695–706. (<a href="https://doi.org/10.1109/TPDS.2019.2942591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DDoS attacks are rampant in cloud environments and continually evolve into more sophisticated and intelligent modalities, such as low-rate DDoS attacks. But meanwhile, the cloud environment is also developing in constant. Now container technology and microservice architecture are widely applied in cloud environment and compose container-based cloud environment. Comparing with traditional cloud environments, the container-based cloud environment is more lightweight in virtualization and more flexible in scaling service. Naturally, a question that arises is whether these new features of container-based cloud environment will bring new possibilities to defeat DDoS attacks. In this paper, we establish a mathematical model based on queueing theory to analyze the strengths and weaknesses of the container-based cloud environment in defeating low-rate DDoS attack. Based on this, we propose a dynamic DDoS mitigation strategy, which can dynamically regulate the number of container instances serving for different users and coordinate the resource allocation for these instances to maximize the quality of service. And extensive simulations and testbed-based experiments demonstrate our strategy can make the limited system resources be utilized sufficiently to maintain the quality of service acceptable and defeat DDoS attack effectively in the container-based cloud environment.},
  archive      = {J_TPDS},
  author       = {Zhi Li and Hai Jin and Deqing Zou and Bin Yuan},
  doi          = {10.1109/TPDS.2019.2942591},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {695-706},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploring new opportunities to defeat low-rate DDoS attack in container-based cloud environment},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HPPT-NoC: A dark-silicon inspired hierarchical TDM NoC with
efficient power-performance trading. <em>TPDS</em>, <em>31</em>(3),
675–694. (<a href="https://doi.org/10.1109/TPDS.2019.2942589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks-on-chip (NoCs) acquired substantial advancements as the typical solution for a modular, flexible and high performance communication infrastructure coping with the scalable Multi-/Manycores technology. However, the increasing chip complexity heading towards thousand cores, together with the approaching dark-silicon era, puts energy efficiency as an integral design key for future NoC-based multicores, where NoCs are significantly contributing to the total chip power. In this paper, we propose HPPT-NoC, a dark-silicon inspired energy-efficient hierarchical TDM NoC with online distributed setup-scheme. The proposed network makes use of the dim silicon parts of the chip to hierarchically connect quad-routers units. Normal routers operate at full-chip-frequency at high supply level, and hierarchical routers operate at half-chip-frequency and lower supply voltage with adequate synchronization. Routers follow a proposed TDM architecture that separates the datapath from the control-setup planes. This allows separate clocking and operating supplies between data and control and to keep the control-setup as a single-slot-cycle design independent of the datapath slot size. The proposed NoC architecture is evaluated versus a base NoC from the state-of-the-art in terms of performance and hardware results using Synopsys VCS and Synopsys Design Compiler for SAED90nm and SAED32nm technologies. The obtained results highlight the power-frequency-trading feature supported by the proposed hierarchical NoC through the configurable data-control clock relation and maintained over the different technology nodes. With the same power budget of the base NoC, the proposed architecture provides up to 74\% setup latency enhancement, 32\% increased NoC saturation load, and 21\% higher success rates, offering up to 78\% improved power delay product. On the other hand, with 38\% power savings, the proposed NoC provides up to 37\% enhanced latency and 15\% higher success rates, with 72\% enhanced power delay product. The proposed design consumes almost double the area of the base NoC, however with an average of 56\% under-clocked (dim) silicon area operating at half to quarter the maximum chip frequency. This results in reduced power density as a main concern in the dark-silicon era down to 24\% of the base NoC.},
  archive      = {J_TPDS},
  author       = {Salma Hesham and Diana Goehringer and Mohamed A. Abd El Ghany},
  doi          = {10.1109/TPDS.2019.2942589},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {675-694},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HPPT-NoC: A dark-silicon inspired hierarchical TDM NoC with efficient power-performance trading},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Random priority-based thrashing control for distributed
shared memory. <em>TPDS</em>, <em>31</em>(3), 663–674. (<a
href="https://doi.org/10.1109/TPDS.2019.2942302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared memory is widely used for inter-process communication. The shared memory abstraction allows computation to be decoupled from communication, which offers benefits, including portability and ease of programming. To enable shared memory access by processes that are on different machines, distributed shared memory (DSM) can be employed. However, DSM systems can suffer from thrashing: while different processes update certain hot data items, the largest amount of effort is spent on data synchronization, and little progress is made by each process. To avoid interference between processes during data updating while providing shared memory at page granularity, more time is reserved for a writer to hold a page in a traditional manner. In this paper, we report on complex thrashing, which can explain why extending the time of holding a page might not be sufficient to control thrashing. To increase the throughput, we propose a thrashing control mechanism that allows each process to update a set of pages during a period of time, where the pages compose a logical area. Because of the isolation of areas, updates on different areas can be performed concurrently. To allow the areas to be fairly well used, each process is assigned with a random priority for thrashing control. The thrashing control mechanism is implemented on a Linux-based DSM system. Performance results show that the execution time of the applications that are apt to cause system thrashing can be significantly reduced by our approach.},
  archive      = {J_TPDS},
  author       = {Yi-Wei Ci and Michael R. Lyu and Zhan Zhang and De-Cheng Zuo and Xiao-Zong Yang},
  doi          = {10.1109/TPDS.2019.2942302},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {663-674},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Random priority-based thrashing control for distributed shared memory},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel low cost interconnection architecture based on the
generalized hypercube. <em>TPDS</em>, <em>31</em>(3), 647–662. (<a
href="https://doi.org/10.1109/TPDS.2019.2941207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized hypercube (GH) is one key interconnection network with excellent topological properties. It contains many other interconnection topologies, such as the hypercube network, the complete graph, the mesh network, and the k-ary n-cube network. It can also be used to construct some data center networks, such as HyperX, BCube, FBFLY, and SWCube. However, the construction cost of GH is high since it contains too many links. In this paper, we propose a novel low cost interconnection architecture called the exchanged generalized hypercube (EGH). We study the properties of EGH, such as the number of edges, the degree of vertices, connectivity, diameter, and diagnosability. Then, we give a routing algorithm to find the shortest path between any two distinct vertices of EGH. Furthermore, we design an algorithm to give disjoint paths between any two distinct vertices of EGH. In addition, we propose two local diagnosis algorithms: LDTEGH and LDWBEGH in EGH under PMC model and MM model, respectively. Simulation results demonstrate that even if the proportion of faulty vertices in EGH is up to 25 percent, the probability that these two diagnosis algorithms can successfully determine the status of vertices is more than 90 percent. As far as the number of edges is concerned, the analysis shows that the construction cost of EGH is much less than that of GH. We could regard this work as the basis for proposing future new high performance topologies.},
  archive      = {J_TPDS},
  author       = {Guijuan Wang and Cheng-Kuan Lin and Jianxi Fan and Baolei Cheng and Xiaohua Jia},
  doi          = {10.1109/TPDS.2019.2941207},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {647-662},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A novel low cost interconnection architecture based on the generalized hypercube},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling encrypted boolean queries in geographically
distributed databases. <em>TPDS</em>, <em>31</em>(3), 634–646. (<a
href="https://doi.org/10.1109/TPDS.2019.2940945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The persistent growth of big data applications has being raising new challenges in managing large volumes of datasets with high scalability, confidentiality protection, and flexible types of search queries. In this paper, we propose a secure design to disassemble the private dataset with the aim to store them across geographically distributed servers while supporting secure multi-client Boolean queries. In this design, the data owner encrypts the private database with the searchable index attributes. The encrypted dataset will be disassembled and distributed evenly across multiple servers by leveraging the property of a distributed index framework. By constructing an encryption structure, generating search tokens, and enabling parallel query, we show how the proposed design performs the secure while efficient Boolean search. These queries are not only limited to those initiated by the data owner but also can be extended to support multiple authorized clients, where each client is allowed to access a necessary part of the private database. In this stage, we advocate a non-interactive authorization scheme where data owner is not required to stay online to process the query request. Moreover, the query operation can be executed in parallel, which significantly improves the search efficiency. We formally characterize the leakage profile, which allow us to follow the existing security analysis method to demonstrate that our system can guarantee data confidentiality and query privacy. To validate our protocol, we implement a system prototype and evaluate the efficiency of our construction. Through experimental results, we demonstrate the effectiveness of our protocol in terms of data outsourcing time and Boolean query time.},
  archive      = {J_TPDS},
  author       = {Xu Yuan and Xingliang Yuan and Yihe Zhang and Baochun Li and Cong Wang},
  doi          = {10.1109/TPDS.2019.2940945},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {634-646},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling encrypted boolean queries in geographically distributed databases},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous management of peak-power and reliability in
heterogeneous multicore embedded systems. <em>TPDS</em>, <em>31</em>(3),
623–633. (<a href="https://doi.org/10.1109/TPDS.2019.2940631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of reliability, power, and performance at hardware and software levels due to heterogeneity is a crucial requirement for heterogeneous multicore embedded systems. Escalating power densities have led to thermal issues for heterogeneous multicore embedded systems. This paper proposes a peak-power-aware reliability management scheme to meet power constraints through distributing power density on the whole chip such that reliability targets are satisfied. In this paper, we consider peak power consumption as a system-level power constraint to prevent system failure. To balance the power consumption, we also employ a Dynamic Frequency Scaling (DFS) method to further reduce peak power consumption and satisfy thermal constraints on the chip. We illustrate the benefits of our scheme by comparing it with state-of-the-art schemes, resulting in average in 26.5 percent less peak power consumption (up to 54.3 percent).},
  archive      = {J_TPDS},
  author       = {Mohsen Ansari and Javad Saber-Latibari and Mostafa Pasandideh and Alireza Ejlali},
  doi          = {10.1109/TPDS.2019.2940631},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {623-633},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Simultaneous management of peak-power and reliability in heterogeneous multicore embedded systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Achieving flexible global reconfiguration in NoCs using
reconfigurable rings. <em>TPDS</em>, <em>31</em>(3), 611–622. (<a
href="https://doi.org/10.1109/TPDS.2019.2940190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The communication behaviors in NoCs of chip-multiprocessors exhibit great spatial and temporal variations, which introduce significant challenges for the reconfiguration in NoCs. Existing reconfigurable NoCs are still far from ideal reconfiguration scenarios, in which globally reconfigurable interconnects can be immediately reconfigured to provide bandwidths on demand for varying traffic flows. In this paper, we propose a hybrid NoC architecture that globally reconfigures the ring-based interconnect to adapt to the varying traffic flows with a high flexibility. The ring-based interconnect has the following advantages. First, it includes horizontal rings and vertical rings, which can be dynamically combined or split to provide low-latency channels for heavy traffic flows. Second, each combined ring connects a number of nodes, thereby improving both the utilization of each ring and the probability to reuse previous reconfigurable interconnects. Finally, the reconfiguration algorithm has a linear-time complexity and can be implemented using a low-overhead hardware design, making it possible to achieve a fast reconfiguration in NoCs. The experimental results show that compared to recent reconfigurable NoCs, the proposed NoC architecture can greatly improve the saturation throughput for synthetic traffic patterns, and reduce the packet latency over 40 percent for realistic benchmarks without incurring significant area and power overhead.},
  archive      = {J_TPDS},
  author       = {Liang Wang and Leibo Liu and Jie Han and Xiaohang Wang and Shouyi Yin and Shaojun Wei},
  doi          = {10.1109/TPDS.2019.2940190},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {611-622},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Achieving flexible global reconfiguration in NoCs using reconfigurable rings},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CuTensor-tubal: Efficient primitives for tubal-rank tensor
learning operations on GPUs. <em>TPDS</em>, <em>31</em>(3), 595–610. (<a
href="https://doi.org/10.1109/TPDS.2019.2940192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensors are the cornerstone data structures in high-performance computing, big data analysis and machine learning. However, tensor computations are compute-intensive and the running time increases rapidly with the tensor size. Therefore, designing high-performance primitives on parallel architectures such as GPUs is critical for the efficiency of ever growing data processing demands. Existing GPU basic linear algebra subroutines (BLAS) libraries (e.g., NVIDIA cuBLAS) do not provide tensor primitives. Researchers have to implement and optimize their own tensor algorithms in a case-by-case manner, which is inefficient and error-prone. In this paper, we develop the cuTensor-tubal library of seven key primitives for the tubal-rank tensor model on GPUs: t-FFT, inverse t-FFT, t-product, t-SVD, t-QR, t-inverse, and t-normalization. cuTensor-tubal adopts a frequency domain computation scheme to expose the separability in the frequency domain, then maps the tube-wise and slice-wise parallelisms onto the single instruction multiple thread (SIMT) GPU architecture. To achieve good performance, we optimize the data transfer, memory accesses, and design the batched and streamed parallelization schemes for tensor operations with data-independent and data-dependent computation patterns, respectively. In the evaluations oft-product, t-SVD, t-QR, t-inverse and t-normalization, cuTensor-tubal achieves maximum 16.91x, 27.03x, 38.97x, 22.36x,15.43x speedups respectively over the CPU implementations running on dual 10-core Xeon CPUs. Two applications, namely, t-SVD-based video compression and low-tubal-rank tensor completion, are tested using our library and achieve maximum 9.80x and 269.26x speedups over multi-core CPU implementations.},
  archive      = {J_TPDS},
  author       = {Tao Zhang and Xiao-Yang Liu and Xiaodong Wang and Anwar Walid},
  doi          = {10.1109/TPDS.2019.2940192},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {595-610},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CuTensor-tubal: Efficient primitives for tubal-rank tensor learning operations on GPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FeatherCNN: Fast inference computation with TensorGEMM on
ARM architectures. <em>TPDS</em>, <em>31</em>(3), 580–594. (<a
href="https://doi.org/10.1109/TPDS.2019.2939785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning is ubiquitous in a wide field of applications ranging from research to industry. In comparison to timeconsuming iterative training of convolutional neural networks (CNNs), inference is a relatively lightweight operation making it amenable to execution on mobile devices. Nevertheless, lower latency and higher computation efficiency are crucial to allow for complex models and prolonged battery life. Addressing the aforementioned challenges, we propose FeatherCNN- a fast inference library for ARM CPUs - targeting the performance ceiling of mobile devices. FeatherCNN employs three key techniques: 1) A highly efficient TensorGEMM (generalized matrix multiplication) routine is applied to accelerate Winograd convolution on ARM CPUs, 2) General layer optimization based on custom high performance kernels improves both the computational efficiency and locality of memory access patterns for non-Winograd layers. 3) The framework design emphasizes joint layer-wise optimization using layer fusion to remove redundant calculations and memory movements. Performance evaluation reveals that FeatherCNN significantly outperforms state-ofthe-art libraries. A forward propagation pass of VGG-16 on a 64-core ARM server is 48, 14, and 12 times faster than Caffe using OpenBLAS, Caffe2 using Eigen, and NNPACK, respectively. In addition, FeatherCNN is 3.19 times faster than the recently released TensorFlow Lite library on an iPhone 7 plus. In terms of GEMM performance, FeatherCNN achieves 14.8 and 39.0 percent higher performance than Apple&#39;s Accelerate framework on an iPhone 7 plus and Eigen on a Samsung Galaxy S8, respectively. The source code of FeatherCNN library is publicly available at https://github.com/tencent/feathercnn.},
  archive      = {J_TPDS},
  author       = {Haidong Lan and Jintao Meng and Christian Hundt and Bertil Schmidt and Minwen Deng and Xiaoning Wang and Weiguo Liu and Yu Qiao and Shengzhong Feng},
  doi          = {10.1109/TPDS.2019.2939785},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {580-594},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FeatherCNN: Fast inference computation with TensorGEMM on ARM architectures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of event processing flow on asynchronous server
efficiency. <em>TPDS</em>, <em>31</em>(3), 565–579. (<a
href="https://doi.org/10.1109/TPDS.2019.2938500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous event-driven server architecture has been considered as a superior alternative to the thread-based counterpart due to reduced multithreading overhead. In this paper, we conduct empirical research on the efficiency of asynchronous Internet servers, showing that an asynchronous server may perform significantly worse than a thread-based one due to two design deficiencies. The first one is the widely adopted one-event-one-handler event processing model in current asynchronous Internet servers, which could generate frequent unnecessary context switches between event handlers, leading to significant CPU overhead of the server. The second one is a write-spin problem (i.e., repeatedly making unnecessary I/O system calls) in asynchronous servers due to some specific runtime workload and network conditions (e.g., large response size and non-trivial network latency). To address these two design deficiencies, we present a hybrid solution by exploiting the merits of different asynchronous architectures so that the server is able to adapt to dynamic runtime workload and network conditions in the cloud. Concretely, our hybrid solution applies a lightweight runtime request checking and seeks for the most efficient path to process each request from clients. Our results show that the hybrid solution can achieve from 10 to 90 percent higher throughput than all the other types of servers under the various realistic workload and network conditions in the cloud.},
  archive      = {J_TPDS},
  author       = {Shungeng Zhang and Qingyang Wang and Yasuhiko Kanemasa and Huasong Shan and Liting Hu},
  doi          = {10.1109/TPDS.2019.2938500},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {565-579},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The impact of event processing flow on asynchronous server efficiency},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fault-tolerant routing mechanism in 3D optical
network-on-chip based on node reuse. <em>TPDS</em>, <em>31</em>(3),
547–564. (<a href="https://doi.org/10.1109/TPDS.2019.2939240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three-dimensional Network-on-Chips (3D NoCs) has become a mature multi-core interconnection architecture in recent years. However, the traditional electrical lines have very limited bandwidth and high energy consumption, making the photonic interconnection promising for future 3D Optical NoCs (ONoCs). Since existing solutions cannot well guarantee the fault-tolerant ability of 3D ONoCs, in this paper, we propose a reliable optical router (OR) structure which sacrifices less redundancy to obtain more restore paths. Moreover, by using our fault-tolerant routing algorithm, the restore path can be found inside the disabled OR under the deadlock-free condition, i.e., fault-node reuse. Experimental results show that the proposed approach outperforms the previous related works by maximum 81.1 percent and 33.0 percent on average for throughput performance under different synthetic and real traffic patterns. It can improve the system average optical signal to noise ratio (OSNR) performance by maximum 26.92 percent and 12.57 percent on average, and it can improve the average energy consumption performance by 0.3 percent to 15.2 percent under different topology types/sizes, failure rates, OR structures, and payload packet sizes.},
  archive      = {J_TPDS},
  author       = {Pengxing Guo and Weigang Hou and Lei Guo and Wei Sun and Chuang Liu and Hainan Bao and Luan H. K. Duong and Weichen Liu},
  doi          = {10.1109/TPDS.2019.2939240},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {547-564},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fault-tolerant routing mechanism in 3D optical network-on-chip based on node reuse},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comment on privacy-preserving scalar product protocols as
proposed in “SPOC.” <em>TPDS</em>, <em>31</em>(3), 543–546. (<a
href="https://doi.org/10.1109/TPDS.2019.2939313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving scalar product (PPSP) protocols are an important building block for secure computation tasks in various applications. Lu et al. (TPDS&#39;13) introduced a PPSP protocol that does not rely on cryptographic assumptions and that is used in a wide range of publications to date. In this comment paper, we show that Lu et al.&#39;s protocol is insecure and should not be used. We describe specific attacks against it and, using impossibility results of Impagliazzo and Rudich (STOC&#39;89), show that it is inherently insecure and cannot be fixed without relying on at least some cryptographic assumptions.},
  archive      = {J_TPDS},
  author       = {Thomas Schneider and Amos Treiber},
  doi          = {10.1109/TPDS.2019.2939313},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {543-546},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A comment on privacy-preserving scalar product protocols as proposed in “SPOC”},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CuPC: CUDA-based parallel PC algorithm for causal structure
learning on GPU. <em>TPDS</em>, <em>31</em>(3), 530–542. (<a
href="https://doi.org/10.1109/TPDS.2019.2939126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal in many fields in the empirical sciences is to discover causal relationships among a set of variables from observational data. PC algorithm is one of the promising solutions to learn underlying causal structure by performing a number of conditional independence tests. In this paper, we propose a novel GPU-based parallel algorithm, called cuPC, to execute an order-independent version of PC. The proposed solution has two variants, cuPC-E and cuPC-S, which parallelize PC in two different ways for multivariate normal distribution. Experimental results show the scalability of the proposed algorithms with respect to the number of variables, the number of samples, and different graph densities. For instance, in one of the most challenging datasets, the runtime is reduced from more than 11 hours to about 4 seconds. On average, cuPC-E and cuPC-S achieve 500X and 1300X speedup, respectively, compared to serial implementation on CPU.},
  archive      = {J_TPDS},
  author       = {Behrooz Zarebavani and Foad Jafarinejad and Matin Hashemi and Saber Salehkaleybar},
  doi          = {10.1109/TPDS.2019.2939126},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {530-542},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CuPC: CUDA-based parallel PC algorithm for causal structure learning on GPU},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A game-theoretical approach for user allocation in edge
computing environment. <em>TPDS</em>, <em>31</em>(3), 515–529. (<a
href="https://doi.org/10.1109/TPDS.2019.2938944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Computing provides mobile and Internet-of-Things (IoT) app vendors with a new distributed computing paradigm which allows an app vendor to deploy its app at hired edge servers distributed near app users at the edge of the cloud. This way, app users can be allocated to hired edge servers nearby to minimize network latency and energy consumption. A cost-effective edge user allocation (EUA) requires maximum app users to be served with minimum overall system cost. Finding a centralized optimal solution to this EUA problem is NP-hard. Thus, we propose EUAGame, a game-theoretic approach that formulates the EUA problem as a potential game. We analyze the game and show that it admits a Nash equilibrium. Then, we design a novel decentralized algorithm for finding a Nash equilibrium in the game as a solution to the EUA problem. The performance of this algorithm is theoretically analyzed and experimentally evaluated. The results show that the EUA problem can be solved effectively and efficiently.},
  archive      = {J_TPDS},
  author       = {Qiang He and Guangming Cui and Xuyun Zhang and Feifei Chen and Shuiguang Deng and Hai Jin and Yanhui Li and Yun Yang},
  doi          = {10.1109/TPDS.2019.2938944},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {515-529},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A game-theoretical approach for user allocation in edge computing environment},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward designing cost-optimal policies to utilize IaaS
clouds with online learning. <em>TPDS</em>, <em>31</em>(3), 501–514. (<a
href="https://doi.org/10.1109/TPDS.2019.2935199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many businesses possess a small infrastructure that they can use for their computing tasks, but also often buy extra computing resources from clouds. Cloud vendors such as Amazon EC2 offer two types of purchase options: on-demand and spot instances. As tenants have limited budgets to satisfy their computing needs, it is crucial for them to determine how to purchase different options and utilize them (in addition to possible self-owned instances) in a cost-effective manner while respecting their response-time targets. In this paper, we propose a framework to design policies to allocate self-owned, on-demand and spot instances to arriving jobs. In particular, we propose a near-optimal policy to determine the number of self-owned instances and an optimal policy to determine the number of on-demand instances to buy and the number of spot instances to bid for at each time unit. Our policies rely on a small number of parameters and we use an online learning technique to infer their optimal values. Through numerical simulations, we show the effectiveness of our proposed policies, in particular that they achieve a cost reduction of up to 64.51 percent when spot and on-demand instances are considered and of up to 43.74 percent when self-owned instances are considered, compared to previously proposed or intuitive policies.},
  archive      = {J_TPDS},
  author       = {Xiaohu Wu and Patrick Loiseau and Esa Hyytiä},
  doi          = {10.1109/TPDS.2019.2935199},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {501-514},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Toward designing cost-optimal policies to utilize IaaS clouds with online learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The network-integrated storage system. <em>TPDS</em>,
<em>31</em>(2), 486–500. (<a
href="https://doi.org/10.1109/TPDS.2019.2938158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present NICE, a key-value storage system design that leverages new software-defined network capabilities to build cluster-based network-efficient storage system. NICE presents novel techniques to co-design network routing and multicast with storage replication, consistency, and load balancing to achieve higher efficiency, performance, and scalability. We implement the NICEKV prototype. NICEKV follows the NICE approach in designing four essential network-centric storage mechanisms: request routing, replication, consistency, and load balancing. Our evaluation shows that the proposed approach brings significant performance gains compared with the current systems design: up to 7× put/get performance improvement, up to 2× reduction in network load, 3× to 9× load reduction on the storage nodes, and the elimination of scalability bottlenecks present in current designs.},
  archive      = {J_TPDS},
  author       = {Ibrahim Kettaneh and Ahmed Alquraan and Hatem Takruri and Suli Yang and Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau and Samer Al-Kiswany},
  doi          = {10.1109/TPDS.2019.2938158},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {486-500},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The network-integrated storage system},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PALE: Time bounded practical agile leader election.
<em>TPDS</em>, <em>31</em>(2), 470–485. (<a
href="https://doi.org/10.1109/TPDS.2019.2933620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many tasks executed in dynamic distributed systems, such as sensor networks or enterprise environments with bring-your-own-device policy, require central coordination by a leader node. In the past it has been proven that distributed leader election in dynamic environments with constant changes and asynchronous communication is not possible. Thus, state-of-the-art leader election algorithms are not applicable in asynchronous environments with constant network changes. Some algorithms converge only after the network stabilizes (an unrealistic requirement in many dynamic environments). Other algorithms reach consensus in the presence of network changes but require a global clock or some level of communication synchrony. Determining the weakest assumptions, under which bounded leader election is possible, remains an unresolved problem. In this study we present a leader election algorithm that operates in the presence of changes and under weak (realistic) assumptions regarding message delays and regarding the clock drifts of the distributed nodes. The proposed algorithm is self-sufficient, easy to implement and can be extended to support multiple regions, self-stabilization, and mobile ad-hoc networks. We prove the algorithm&#39;s correctness and provide a complexity analysis of the time, space, and number of messages required to elect a leader.},
  archive      = {J_TPDS},
  author       = {Bronislav Sidik and Rami Puzis and Polina Zilberman and Yuval Elovici},
  doi          = {10.1109/TPDS.2019.2933620},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {470-485},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PALE: Time bounded practical agile leader election},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient and portable workgroup size tuning. <em>TPDS</em>,
<em>31</em>(2), 455–469. (<a
href="https://doi.org/10.1109/TPDS.2019.2937295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of an OpenCL program is strongly influenced by both hardware and software attributes. To achieve superior performance, developers may leverage automatic performance tuning techniques to determine the optimal parameters on the target device. Although existing approaches have shown promising tuning results in their target scenarios, other requirements such as efficiency, portability, and usability should also be considered because of the rapid growth of heterogeneous computing applications and platforms. In this paper, we re-examine the workgroup size tuning problem and propose a novel approach to meet the aforementioned requirements. We abstract the architectural details into a set of hardware parameters so that the proposed approach can be applied without the presence of target devices, which makes it more accessible to developers. The proposed approach is evaluated on 20 OpenCL kernels and six devices, including both CPUs and GPUs. Experimental results demonstrate that, with negligible overhead, our approach filters out 88.6 percent of the possible workgroup sizes on average. Among all the workgroup size candidates, the bestand worst-performing candidates can achieve average performance of 95.5 and 92.1 percent, respectively, compared with the optimal workgroup size.},
  archive      = {J_TPDS},
  author       = {Chia-Lin Yu and Shiao-Li Tsao},
  doi          = {10.1109/TPDS.2019.2937295},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {455-469},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient and portable workgroup size tuning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Achieving load-balanced, redundancy-free cluster caching
with selective partition. <em>TPDS</em>, <em>31</em>(2), 439–454. (<a
href="https://doi.org/10.1109/TPDS.2019.2931004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-intensive clusters increasingly rely on in-memory storages to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hot spots, which significantly degrade the benefits of in- memory caching. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory overhead due to cache redundancy or incur non-trivial encoding/decoding complexity. In this paper, we propose an effective approach to achieve load balancing without cache redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on the loads they contribute and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for a hot file—too few partitions are incapable of mitigating hot spots, while too many are susceptible to stragglers. We have implemented SP-Cache atop Alluxio, a popular in-memory distributed storage system, and evaluated its performance through EC2 deployment and trace-driven simulations. SP-Cache can quickly react to the changing load by dynamically re-balancing cache servers. Compared to the state-of-the-art solution, SP-Cache reduces the file access latency by up to 40 percent in both the mean and the tail, using 40 percent less memory.},
  archive      = {J_TPDS},
  author       = {Yinghao Yu and Wei Wang and Renfei Huang and Jun Zhang and Khaled Ben Letaief},
  doi          = {10.1109/TPDS.2019.2931004},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {439-454},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Achieving load-balanced, redundancy-free cluster caching with selective partition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimized block-based algorithms to label connected
components on GPUs. <em>TPDS</em>, <em>31</em>(2), 423–438. (<a
href="https://doi.org/10.1109/TPDS.2019.2934683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected Components Labeling (CCL) is a crucial step of several image processing and computer vision pipelines. Many efficient sequential strategies exist, among which one of the most effective is the use of a block-based mask to drastically cut the number of memory accesses. In the last decade, aided by the fast development of Graphics Processing Units (GPUs), a lot of data parallel CCL algorithms have been proposed along with sequential ones. Applications that entirely run in GPU can benefit from parallel implementations of CCL that allow to avoid expensive memory transfers between host and device. In this paper, two new eight-connectivity CCL algorithms are proposed, namely Block-based Union Find (BUF) and Block-based Komura Equivalence (BKE). These algorithms optimize existing GPU solutions introducing a block-based approach. Extensions for three-dimensional datasets are also discussed. In order to produce a fair comparison with previously proposed alternatives, YACCLAB, a public CCL benchmarking framework, has been extended and made suitable for evaluating also GPU algorithms. Moreover, three-dimensional datasets have been added to its collection. Experimental results on real cases and synthetically generated datasets demonstrate the superiority of the new proposals with respect to state-of-the-art, both on 2D and 3D scenarios.},
  archive      = {J_TPDS},
  author       = {Stefano Allegretti and Federico Bolelli and Costantino Grana},
  doi          = {10.1109/TPDS.2019.2934683},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {423-438},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimized block-based algorithms to label connected components on GPUs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ICELIA: A full-stack framework for STT-MRAM-based deep
learning acceleration. <em>TPDS</em>, <em>31</em>(2), 408–422. (<a
href="https://doi.org/10.1109/TPDS.2019.2937517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large variety of applications rely on deep learning to process big data, learn sophisticated features, and perform complicated tasks. Utilizing emerging non-volatile memory (NVM)&#39;s unique characteristics, including the crossbar array structure and gray-scale cell resistances, to perform neural network (NN) computation is a well-studied approach in accelerating deep learning applications. Compared to other NVM technologies, STT-MRAM has its unique advantages in performing NN computation. However, the state-of-the-art research have not utilized STT-MRAM for deep learning acceleration due to its device and architecture-level challenges. Consequently, this paper enables STT-MRAM, for the first time, as an effective and practical deep learning accelerator. In particular, it proposes a full-stack framework iCELIA spanning multiple design levels, including device-level fabrication, circuit-level enhancements, architecture-level synaptic weight quantization, and system-level accelerator design. The primary contributions of iCELIA over our prior work CELIA include a new non-uniform weight quantization scheme and much enhanced accelerator system design. The proposed framework significantly mitigates the model accuracy loss due to reduced data precision in a cohesive manner, constructing a comprehensive STT-MRAM accelerator system for fast NN computation with high energy efficiency and low cost.},
  archive      = {J_TPDS},
  author       = {Hao Yan and Hebin R. Cherian and Ethan C. Ahn and Xuehai Qian and Lide Duan},
  doi          = {10.1109/TPDS.2019.2937517},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {408-422},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ICELIA: A full-stack framework for STT-MRAM-based deep learning acceleration},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Throughput maximization of NFV-enabled multicasting in
mobile edge cloud networks. <em>TPDS</em>, <em>31</em>(2), 393–407. (<a
href="https://doi.org/10.1109/TPDS.2019.2937524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) reforms the cloud paradigm by bringing unprecedented computing capacity to the vicinity of end users at the mobile network edge. This provides end users with swift and powerful computing and storage capacities, energy efficiency, and mobility- and context-awareness support. Furthermore, Network Function Virtualization (NFV) is another promising technique that implements various network functions for many applications as pieces of software in servers or cloudlets in MEC networks. The provisioning of virtualized network services in MEC can improve user service experiences, simplify network service deployment, and ease network resource management. However, user requests arrive dynamically and different users demand different amounts of resources, while the resources in MEC are dynamically occupied or released by different services. It thus poses a significant challenge to optimize the performance of MEC through efficient computing and communication resource allocations to meet ever-growing resource demands of users. In this paper, we study NFV-enabled multicasting that is a fundamental routing problem in an MEC network, subject to resource capacities on both its cloudlets and links. Specifically, we first devise an approximation algorithm for the cost minimization problem of admitting a single NFV-enabled multicast request. We then develop an efficient algorithm for the throughput maximization problem for the admissions of a given set of NFV-enabled multicast requests. We third devise an online algorithm with a provable competitive ratio for the online throughput maximization problem when NFV-enabled multicast requests arrive one by one without the knowledge of future request arrivals. We finally evaluate the performance of the proposed algorithms through experimental simulations. Simulation results demonstrate that the proposed algorithms are promising.},
  archive      = {J_TPDS},
  author       = {Yu Ma and Weifa Liang and Jie Wu and Zichuan Xu},
  doi          = {10.1109/TPDS.2019.2937524},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {393-407},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Throughput maximization of NFV-enabled multicasting in mobile edge cloud networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A highly reliable metadata service for large-scale
distributed file systems. <em>TPDS</em>, <em>31</em>(2), 374–392. (<a
href="https://doi.org/10.1109/TPDS.2019.2937492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many massive data processing applications nowadays often need long, continuous, and uninterrupted data accesses. Distributed file systems are used as the back-end storage to provide the global namespace management and reliability guarantee. Due to increasing hardware failures and software issues with the growing system scale, metadata service reliability has become a critical issue as it has a direct impact on file and directory operations. Existing metadata management mechanisms can provide fault tolerance capability to some level but are inadequate. They often have limitations in system availability, state consistence, and performance overhead and lack an effective mechanism to offer metadata reliability. This paper introduces a novel highly reliable metadata service to address these issues in large-scale file systems. Different from traditional strategies, this proposed reliable metadata service adopts a new active-standby architecture for fault tolerance and uses a holistic approach to improve file system availability. A new shared storage pool (SSP) is designed for transparent metadata synchronization and replication between active and standby servers. Based on the SSP, a new policy called multiple actives multiple standbys (MAMS) is presented to perform metadata service recovery in case of failures. A new global state recovery strategy and a smart client fault tolerance mechanism are achieved to maintain the continuity of metadata service. We have implemented such highly reliable metadata service in a prototype file system CFS (Clover file system) and conducted extensive tests to evaluate it. Experimental results confirm that it can significantly improve file system reliability with fast failover under different failure scenarios while having negligible influence on performance. Compared with typical reliability designs in Hadoop Avatar, Hadoop HA, and Boom-FS file systems, the mean-time-to-recovery (MTTR) with the highly reliable metadata service was reduced by 80.23, 65.46 and 28.13 percent, respectively.},
  archive      = {J_TPDS},
  author       = {Jiang Zhou and Yong Chen and Weiping Wang and Shuibing He and Dan Meng},
  doi          = {10.1109/TPDS.2019.2937492},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {374-392},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A highly reliable metadata service for large-scale distributed file systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Thread isolation to improve symbiotic scheduling on SMT
multicore processors. <em>TPDS</em>, <em>31</em>(2), 359–373. (<a
href="https://doi.org/10.1109/TPDS.2019.2934955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource sharing is a critical issue in simultaneous multithreading (SMT) processors as threads running simultaneously on an SMT core compete for shared resources. Symbiotic job scheduling, which co-schedules applications with complementary resource demands, is an effective solution to maximize hardware utilization and improve overall system performance. However, symbiotic job scheduling typically distributes threads evenly among cores, i.e., all cores get assigned the same number of threads, which we find to lead to sub-optimal performance. In this paper, we show that asymmetric schedules (i.e., schedules that assign a different number of threads to each SMT core) can significantly improve performance compared to symmetric schedules. To leverage this finding, we propose thread isolation, a technique that turns symmetric schedules into asymmetric ones yielding higher overall system performance. Thread isolation identifies SMT-adverse applications and schedules them in isolation on a dedicated core to mitigate their sharp performance degradation under SMT. Our experimental results on an IBM POWER8 processor show that thread isolation improves system throughput by up to 5.5 percent compared to a state-of-the-art symmetric symbiotic job scheduler.},
  archive      = {J_TPDS},
  author       = {Josué Feliu and Julio Sahuquillo and Salvador Petit and Lieven Eeckhout},
  doi          = {10.1109/TPDS.2019.2934955},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {359-373},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Thread isolation to improve symbiotic scheduling on SMT multicore processors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coded load balancing in cache networks. <em>TPDS</em>,
<em>31</em>(2), 347–358. (<a
href="https://doi.org/10.1109/TPDS.2019.2933839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider load balancing problem in a cache network consisting of storage-enabled servers forming a distributed content delivery scenario. Previously proposed load balancing solutions cannot perfectly balance out requests among servers, which is a critical issue in practical networks. Therefore, in this paper, we investigate a coded cache content placement where coded chunks of original files are stored in servers based on the files popularity distribution. In our scheme, upon each request arrival at the delivery phase, by dispatching enough coded chunks to the request origin from the nearest servers, the requested file can be decoded. Here, we show that if n requests arrive randomly at n servers, the proposed scheme results in the maximum load of O(1) in the network. This result is shown to be valid under various assumptions for the underlying network topology. Our results should be compared to the maximum load of two baseline schemes, namely, nearest replica and power of two choices strategies, which are O(log n) and O(log log n), respectively. This finding shows that using coding, results in a considerable load balancing performance improvement, without compromising communications cost performance. This is confirmed by performing extensive simulation results, in non-asymptotic regimes as well.},
  archive      = {J_TPDS},
  author       = {Mahdi Jafari Siavoshani and Farzad Parvaresh and Ali Pourmiri and Seyed Pooya Shariatpanahi},
  doi          = {10.1109/TPDS.2019.2933839},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {347-358},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Coded load balancing in cache networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving overall performance of TLC SSD by exploiting
dissimilarity of flash pages. <em>TPDS</em>, <em>31</em>(2), 332–346.
(<a href="https://doi.org/10.1109/TPDS.2019.2934458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TLC flash has three types of pages to accommodate the three bits in each TLC physical cell exhibiting very different program latencies. This paper proposes PA-SSD to effectively improve the overall performance by exploiting the dissimilarity of TLC pages on program latency throughout the write request handling workflow. The main idea behind PA-SSD is to coordinately allocate the same type of pages for sub-requests of any given user write request, to mitigate the potential program latency imbalance among the sub-requests, and to schedule sub-requests according to their page-types. We achieve the PA-SSD design goal by answering three key research questions: (1) how to properly determine page-type for each user write request? (2) how to actually allocate a physical page for each sub-request with an assigned page-type from (1)? (3) how to effectively schedule the sub-requests in the chips queues when their page-types are judiciously allocated from (2)? To answer the first question, we propose seven page-type specifying schemes to investigate their effects under different workloads. We answer the second question by redesigning the page allocation strategy in TLC SSD to uniformly and sequentially determine physical pages for allocation following the internal programming process of TLC flash. Lastly, a page-type aware scheduling policy is presented to reorder the sub-requests within chips&#39; queues. Our experiments show that PA-SSD can accelerate both the write and read performance. Particularly, our proposed queue-depth based page-type specifying scheme improves write performance by 2.6 times and read performance by 1.5 times over the conventional TLC SSD.},
  archive      = {J_TPDS},
  author       = {Wenhui Zhang and Qiang Cao and Hong Jiang and Jie Yao},
  doi          = {10.1109/TPDS.2019.2934458},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {332-346},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving overall performance of TLC SSD by exploiting dissimilarity of flash pages},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance modeling of parallel loops on multi-socket
platforms using queueing systems. <em>TPDS</em>, <em>31</em>(2),
318–331. (<a href="https://doi.org/10.1109/TPDS.2019.2938172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the performance of parallel loops on modern shared-memory multi-socket multi-core systems in dependence of the allocated resources is an important means to achieve better system utilization. Previous prediction techniques are tied to specific architectures and do not allow for purely online performance predictions without requiring an offline analysis of the parallel program. This paper presents a practical approach based on queueing theory to model the performance of parallel programs in dependence of the number of allocated core resources. Based on the key insight that scalability of scientific parallel loops is limited by memory performance, a hierarchically constructed M/M/1/N/N queue system is used to analytically compute the response time at the different congestion points in the memory system of modern NUMA architectures. After automatically tuning the model to a specific architecture by executing a number of micro-benchmarks, the required parameter values are obtained at runtime from hardware performance counters present in modern commodity AMD and Intel processors. Evaluated with 24 OpenMP parallel loops on a 64-core AMD and a 72-core Intel multi-socket platform, the presented queueing system is able to accurately predict the speedup of parallel loops with a mean absolute percentage error of 8.3 percent on the AMD system and 6.7 percent on the Intel platform.},
  archive      = {J_TPDS},
  author       = {Younghyun Cho and Surim Oh and Bernhard Egger},
  doi          = {10.1109/TPDS.2019.2938172},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {318-331},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performance modeling of parallel loops on multi-socket platforms using queueing systems},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy and task-aware partitioning on single-ISA clustered
heterogeneous processors. <em>TPDS</em>, <em>31</em>(2), 306–317. (<a
href="https://doi.org/10.1109/TPDS.2019.2937029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous multi-core processing is increasingly adopted in embedded systems. Heterogeneous platforms can provide energy consumption reduction by employing longstanding techniques like Dynamic Voltage and Frequency Scaling (DVFS) and Dynamic Power Management (DPM). An effective energy-management strategy simultaneously exploits hardware-and software-level energy-reduction techniques. Energy-efficient partitioning is one software-level method where task allocation to heterogeneous clusters directly impacts the total system energy. In this paper, we couple the problem of energy-efficient partitioning on single-ISA heterogeneous platforms with task-aware scheduling. Tasks differ in their instruction mix, cache, memory and I/O access, execution path, and active processing and SoC circuitry. This affects their power demand. We make further use of underlying hardware frequency scaling to reduce the system energy. We propose four variants of our Task and Cluster Heterogeneity Aware Partitioning (TCHAP) targeting ARM big.LITTLE platforms, and show that our algorithms achieve up to 30 percent energy-reduction on average compared to a state-of-the-art scheme.},
  archive      = {J_TPDS},
  author       = {Ashraf Suyyagh and Zeljko Zilic},
  doi          = {10.1109/TPDS.2019.2937029},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {306-317},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy and task-aware partitioning on single-ISA clustered heterogeneous processors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wiera: Policy-driven multi-tiered geo-distributed cloud
storage system. <em>TPDS</em>, <em>31</em>(2), 294–305. (<a
href="https://doi.org/10.1109/TPDS.2019.2935727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-tiered geo-distributed cloud storage systems must tame complexity at many levels: uniform APIs for storage access, supporting flexible storage policies that meet a wide array of application metrics, determining an optimal data placement, handling uncertain network dynamics and access dynamism, and operating across many levels of heterogeneity both within and across data-centers (DCs). In this paper, we present an integrated solution called Wiera. Wiera enables the specification of data management policies both within a local DC and across DCs. Such policies enable the user to optimize for cost, performance, reliability, durability, and consistency, and to express their tradeoffs. In addition, Wiera determines an optimal data placement for the user to meet their desired tradeoffs easily in such an environment. A key aspect of Wiera is first-class support for dynamism due to network, workload, and access patterns changes. As far as we know, Wiera is the first geo-distributed cloud storage system which handles dynamism actively at run-time. Wiera allows unmodified applications to reap the benefits of flexible data/storage policies by externalizing the policy specification. We show how Wiera enables a rich specification of dynamic policies using a concise notation and describe the design and implementation of the system. We have implemented a Wiera prototype on multiple cloud environments, AWS and Azure, that illustrates potential benefits from managing dynamics and in using multiple cloud storage tiers both within and across DCs.},
  archive      = {J_TPDS},
  author       = {Kwangsung Oh and Nan Qin and Abhishek Chandra and Jon Weissman},
  doi          = {10.1109/TPDS.2019.2935727},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {294-305},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Wiera: Policy-driven multi-tiered geo-distributed cloud storage system},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing geo-distributed data analytics with coordinated
task scheduling and routing. <em>TPDS</em>, <em>31</em>(2), 279–293. (<a
href="https://doi.org/10.1109/TPDS.2019.2938164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent trends show that cloud computing is growing to span more and more globally distributed datacenters. For geo-distributed datacenters, there is an increasingly need for scheduling algorithms to place tasks across datacenters, by jointly considering WAN traffic and computation. This scheduling must deal with situations such as wide-area distributed data, data sharing, WAN bandwidth costs and datacenter capacity limits, while also minimizing makespan. However, this scheduling problem is NP-hard. We propose a new resource allocation algorithm called HPS+, an extension to Hypergraph Partition-based Scheduling. HPS+ models the combined task-data dependencies and data-datacenter dependencies as an augmented hypergraph, and adopts an improved hypergraph partition technique to minimize WAN traffic. It further uses a coordination mechanism to allocate network resources closely following the guidelines of task requirements, for minimizing the makespan. Evaluation across the real China-Astronomy-Cloud model and Google datacenter model show that HPS+ saves the amount of data transfers by upto 53 percent and reduces the makespan by 39 percent compared to existing algorithms.},
  archive      = {J_TPDS},
  author       = {Laiping Zhao and Yanan Yang and Ali Munir and Alex X. Liu and Yue Li and Wenyu Qu},
  doi          = {10.1109/TPDS.2019.2938164},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {279-293},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing geo-distributed data analytics with coordinated task scheduling and routing},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). APMigration: Improving performance of hybrid memory
performance via an adaptive page migration method. <em>TPDS</em>,
<em>31</em>(2), 266–278. (<a
href="https://doi.org/10.1109/TPDS.2019.2933521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byte-addressable, non-volatile memory (NVRAM) combines the benefits of DRAM and flash memory. However, due to its slower speed than DRAM, it is best to deploy it in combination with typical DRAM. In such Hybrid NVRAM systems, frequently accessed, hotpages can be stored in DRAM while other cold pages can reside in NVRAM, providing the benefits of both high performance (from DRAM) and lower power consumption and cost/performance (from NVRAM). While the idea seems beneficial, realizing an efficient hybrid NVRAM system requires careful page migration and accurate data temperature measurement. Existing solutions, however, often cause invalid migrations due to inaccurate data temperature accounting, because hot and cold pages are separately identified in DRAM and NVRAM regions. Moreover, since a new NVRAM frame is always allocated for each page swapped back NVRAM, a large amount of unnecessary NVRAM writes are generated during each page migration. Based on these observations, we propose APMigrate, an adaptive data migration approach for hybrid NVRAM systems. APMigrate consist of two parts, UIMigrate and LazyWriteback. UIMigrate focuses on eliminating invalid page migrations by considering data temperature in the entire DRAM-NVRAM space, while LazyWriteback focus on rewriting only dirty data back when the page is swapped back to NVRAM. Our experiments using SPEC 2006 show that APMigrate can reduce the number of migrations and improves performance by up to 90 percent compared to existing state-of-the-art approaches. For some workloads, LazyWriteback can reduce unnecessary NVRAM writes for existing page migrations by up to 75 percent.},
  archive      = {J_TPDS},
  author       = {Yujuan Tan and Baiping Wang and Zhichao Yan and Witawas Srisa-an and Xianzhang Chen and Duo Liu},
  doi          = {10.1109/TPDS.2019.2933521},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {266-278},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {APMigration: Improving performance of hybrid memory performance via an adaptive page migration method},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pache: A packet management scheme of cache in data center
networks. <em>TPDS</em>, <em>31</em>(2), 253–265. (<a
href="https://doi.org/10.1109/TPDS.2019.2931905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The communication traffic among servers is a principal bottleneck for data center networks (DCNs), in which redundant traffic has a significant impact on the performance of DCNs. In this paper, we propose Pache, a distributed and efficient cache scheme to achieve traffic redundancy elimination in DCNs. In Pache, nodes cache packets to reduce bandwidth consumption and gain performance revenue. Each cache node manages the local cache and improves cache efficiency with counting bloom filter, hash table and Trie. Each cache node broadcasts the local cache information by a bloom filter to other servers, and servers have cache information tables (cache-table) to record them, which achieves a cache sharing mechanism. Moreover, Pache employs false positive information table (fp-table) to reduce the false positive rate caused by bloom filters. A server determines an original packet or the corresponding fingerprint packet to send by querying cache-table and fp-table. Cache node placement mechanism is also designed to select which node to cache a packet. For evaluating the performance of Pache, we execute extensive simulations and conduct a case study on Amazon EC2 platform from different aspects. The results show that Pache can eliminate about 40 percent redundant traffic on average, and only increase 10 percent runtime, and it is also a scalable and feasible cache scheme in DCNs.},
  archive      = {J_TPDS},
  author       = {Tao Chen and Xiaofeng Gao and Tao Liao and Guihai Chen},
  doi          = {10.1109/TPDS.2019.2931905},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {253-265},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Pache: A packet management scheme of cache in data center networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editor’s note. <em>TPDS</em>, <em>31</em>(2), 251–252. (<a
href="https://doi.org/10.1109/TPDS.2019.2958395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TPDS},
  author       = {Manish Parashar},
  doi          = {10.1109/TPDS.2019.2958395},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {251-252},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Editor&#39;s note},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-parallel hashing techniques for GPU architectures.
<em>TPDS</em>, <em>31</em>(1), 237–250. (<a
href="https://doi.org/10.1109/TPDS.2019.2929768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hash tables are a fundamental data structure for effectively storing and accessing sparse data, with widespread usage in domains ranging from computer graphics to machine learning. This study surveys the state-of-the-art research on data-parallel hashing techniques for emerging massively-parallel, many-core GPU architectures. This survey identifies key factors affecting the performance of different techniques and suggests directions for further research.},
  archive      = {J_TPDS},
  author       = {Brenton Lessley and Hank Childs},
  doi          = {10.1109/TPDS.2019.2929768},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {237-250},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Data-parallel hashing techniques for GPU architectures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey of phase classification techniques for
characterizing variable application behavior. <em>TPDS</em>,
<em>31</em>(1), 224–236. (<a
href="https://doi.org/10.1109/TPDS.2019.2929781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptable computing is an increasingly important paradigm that specializes system resources to variable application requirements, environmental conditions, or user requirements. Adapting computing resources to variable application requirements (or application phases) is otherwise known as phase-based optimization. Phase-based optimization takes advantage of application phases, or execution intervals of an application that behave similarly, to enable effective and beneficial adaptability. In order for phase-based optimization to be effective, the phases must first be classified to determine when application phases begin and end, and ensure that system resources are accurately specialized. In this paper, we present a survey of phase classification techniques that have been proposed to exploit the advantages of adaptable computing through phase-based optimization. We focus on recent techniques and classify these techniques with respect to several factors in orderto highlight their similarities and differences. We divide the techniques by their major defining characteristics-online/offline and serial/parallel. In addition, we discuss other characteristics such as prediction and detection techniques, the characteristics used for prediction, interval type, etc. We also identify gaps in the state-of-the-art and discuss future research directions to enable and fully exploit the benefits of adaptable computing.},
  archive      = {J_TPDS},
  author       = {Keeley Criswell and Tosiron Adegbija},
  doi          = {10.1109/TPDS.2019.2929781},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {224-236},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A survey of phase classification techniques for characterizing variable application behavior},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WPaxos: Wide area network flexible consensus. <em>TPDS</em>,
<em>31</em>(1), 211–223. (<a
href="https://doi.org/10.1109/TPDS.2019.2929793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WPaxos is a multileader Paxos protocol that provides low-latency and high-throughput consensus across wide-area network (WAN) deployments. WPaxos uses multileaders, and partitions the object-space among these multileaders. Unlike statically partitioned multiple Paxos deployments, WPaxos is able to adapt to the changing access locality through object stealing. Multiple concurrent leaders coinciding in different zones steal ownership of objects from each other using phase-1 of Paxos, and then use phase-2 to commit update-requests on these objects locally until they are stolen by other leaders. To achieve fast phase-2 commits, WPaxos adopts the flexible quorums idea in a novel manner, and appoints phase-2 acceptors to be close to their respective leaders. We implemented WPaxos and evaluated it over WAN deployments across 5 AWS regions. The dynamic partitioning of the objectspace and emphasis on zone-local commits allow WPaxos to significantly outperform both partitioned Paxos deployments and leaderless Paxos approaches.},
  archive      = {J_TPDS},
  author       = {Ailidani Ailijiang and Aleksey Charapko and Murat Demirbas and Tevfik Kosar},
  doi          = {10.1109/TPDS.2019.2929793},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {211-223},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {WPaxos: Wide area network flexible consensus},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The existence of completely independent spanning trees for
some compound graphs. <em>TPDS</em>, <em>31</em>(1), 201–210. (<a
href="https://doi.org/10.1109/TPDS.2019.2931904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given two regular graphs G and H such that the vertex degree of G is equal to the number of vertices in H, the compound graph G(H) is constructed by replacing each vertex of G by a copy of Hand replacing each edge of G by an additional edge connecting random vertices in two corresponding copies of H, respectively, under the constraint that each vertex in G(H) is incident with only one additional edge, exactly. L-HSDC m is a compound graph G(H), where G is a hypercube Q m and H is a complete graph K m , which is defined by focusing on the connected relation between servers in the novel data center network HSDC m proposed in [30]. A set of k spanning trees in a graph G are called completely independent spanning trees (CISTs for short) if the paths joining every pair of vertices x and yin any two trees have neither vertex nor edge in common, except for x and y. In this paper, we give a sufficient condition for the existence of k CISTs in a kind of compound graph. Furthermore, a specific construction algorithm is provided. As corollaries of the main results, the existences of two CISTs form m ≥ 4; three CISTs form m ≥ 8 and four CISTs form m ≥ 10 in L-HSDC m (m) are gotten directly.},
  archive      = {J_TPDS},
  author       = {Xiao-Wen Qin and Rong-Xia Hao and Jou-Ming Chang},
  doi          = {10.1109/TPDS.2019.2931904},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {201-210},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The existence of completely independent spanning trees for some compound graphs},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single restart with time stamps for parallel task processing
with known and unknown processors. <em>TPDS</em>, <em>31</em>(1),
187–200. (<a href="https://doi.org/10.1109/TPDS.2019.2929173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of scheduling n tasks on m + m&#39; parallel processors, where the processing times on m processors are known while those on the remaining m&#39; processors are not known a priori. This semi-online model is an abstraction of certain heterogeneous computing systems, e.g., with them known processors representing local CPU cores and the unknown processors representing remote servers with uncertain availability of computing cycles. Our objective is to minimize the makespan of all tasks. We initially focus on the case m&#39; = 1 and propose a semi-online algorithm termed Single Restart with Time Stamps (SRTS), which has time complexity O(nlogn). We derive its competitive ratio in comparison with the optimal offline solution. If the unknown processing times are deterministic, the competitive ratio of SRTS is shown to be either always constant or asymptotically constant in practice, respectively in cases where the processing times are independent and dependent on m. A similar result is obtained when the unknown processing times are random. Furthermore, extending the ideas of SRTS, we propose a heuristic algorithm termed SRTS-Multiple (SRTS-M) for the case m&#39; &gt; 1. Finally, where tasks arrive dynamically with unknown arrival times, we extend SRTS to Dynamic SRTS (DSRTS) and find its competitive ratio. Besides the proven competitive ratios, simulation results further suggest that SRTS and SRTS-M give superior performance on average over randomly generated task processing times, substantially reducing the makespan over the best known alternatives. Interestingly, the performance gain is more significant for task processing times sampled from heavy-tailed distributions.},
  archive      = {J_TPDS},
  author       = {Jaya Prakash Champati and Ben Liang},
  doi          = {10.1109/TPDS.2019.2929173},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {187-200},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Single restart with time stamps for parallel task processing with known and unknown processors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scheduling parallel real-time tasks on the minimum number of
processors. <em>TPDS</em>, <em>31</em>(1), 171–186. (<a
href="https://doi.org/10.1109/TPDS.2019.2929048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, several parallel frameworks have emerged to utilize the increasing computational capacity of multiprocessors. Parallel tasks are distinguished from traditional sequential tasks in that the subtasks contained in a single parallel task can simultaneously execute on multiple processors. In this study, we consider the scheduling problem of minimizing the number of processors on which the parallel real-time tasks feasibly run. In particular, we focus on scheduling sporadic parallel real-time tasks, in which precedence constraints between subtasks of each parallel task are expressed using a directed acyclic graph (DAG). To address the problem, we formulate an optimization problem that aims to minimize the maximum processing capacity for executing the given tasks. We then suggest a polynomial solution consisting of three steps: (1) transform each parallel real-time task into a series of multithreaded segments, while respecting the precedence constraints of the DAG; (2) selectively extend the segment lengths; and (3) interpret the problem as a flow network to balance the flows on the terminal edges. We also provide the schedulability bound of the proposed solution: it has a capacity augmentation bound of 2. Our experimental results show that the proposed approach yields higher performance than one developed in a recent study.},
  archive      = {J_TPDS},
  author       = {Hyeonjoong Cho and Chulgoo Kim and Joohyung Sun and Arvind Easwaran and Ju-Derk Park and Byeong-Cheol Choi},
  doi          = {10.1109/TPDS.2019.2929048},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {171-186},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scheduling parallel real-time tasks on the minimum number of processors},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantum game analysis on extrinsic incentive mechanisms for
P2P services. <em>TPDS</em>, <em>31</em>(1), 159–170. (<a
href="https://doi.org/10.1109/TPDS.2019.2933416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peer-to-peer (P2P) services such as mobile P2P transmissions and resource sharing, provide efficient methods to deliver data without the deployment of any central server. Nevertheless, the free-riding phenomenon inherit in such services presses a need for incentive mechanisms to stimulate contributions of data transmissions or sharing. As a result, it is imperative to answer the following questions: whether, and if so to what extent, an incentive mechanism can invoke such contributions? To answerthese questions, we employ an n-player continuous quantum game model to analyze the general extrinsic incentive mechanisms as well as the reputation-based incentive mechanisms, a typical class of extrinsic incentive mechanisms. We focus on studying the extrinsic incentive mechanisms in this paper due to their wide scope of applications stemming from the fact that they promote cooperative behaviors by offering rewards rather than depending on the internal bounds (e.g., social ties) among peers, which may not always exist between any pair of peers. To the best of our knowledge, we are the first to analyze the extrinsic incentive mechanisms for P2P services from a quantum game perspective. Such a perspective is adopted because the extended strategy space in the quantum game broadens the range for searching optimal strategies and the introduction of entanglement makes the proposed analytical frameworks more practical due to the consideration of the peers&#39; relationships imposed by the rewards in extrinsic incentive mechanisms. Our quantum game-based analytical framework is generic because it is compatible with classic game-based schemes. The analytical results can provide a straightforward insight on evaluating the potential of the extrinsic incentive mechanisms and can serve as important references for designing new extrinsic incentive mechanisms.},
  archive      = {J_TPDS},
  author       = {Shengling Wang and Weiman Sun and Liran Ma and Weifeng Lv and Xiuzhen Cheng},
  doi          = {10.1109/TPDS.2019.2933416},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {159-170},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Quantum game analysis on extrinsic incentive mechanisms for P2P services},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimizing tardiness for data-intensive applications in
heterogeneous systems: A matching theory perspective. <em>TPDS</em>,
<em>31</em>(1), 144–158. (<a
href="https://doi.org/10.1109/TPDS.2019.2930992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing data requirements of Internet applications have driven a dramatic surge in developing new programming paradigms and complex scheduling algorithms to handle data-intensive workloads. Due to the expanding volume and the variety of such flows, their raw data are often processed on Intermediate Processing Nodes (IPNs) before being sent to servers. However, the intermediate processing constraint is rarely considered in existing flow computing models. This paper aims to minimize the tardiness of data-intensive applications in the presence of intermediate processing constraint. Motivating cases show that the tardiness is affected by both IPN locations and flow dispatching strategies. Based on the observation that dispatching flows to IPNs is essentially building a matching between flows and IPNs, a novel solution is proposed based on matching theory. In the deployment phase, a tardiness-aware deferred acceptance algorithm is developed to optimize IPN locations. In the operation phase, the Power-of-D paradigm and matching theory are combined together to dispatch flows efficiently. Evaluation results show that our solution effectively minimizes the total tardiness of data-intensive applications in heterogeneous systems.},
  archive      = {J_TPDS},
  author       = {Ke Xu and Liang Lv and Tong Li and Meng Shen and Haiyang Wang and Kun Yang},
  doi          = {10.1109/TPDS.2019.2930992},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {144-158},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Minimizing tardiness for data-intensive applications in heterogeneous systems: A matching theory perspective},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GA-par: Dependable microservice orchestration framework for
geo-distributed clouds. <em>TPDS</em>, <em>31</em>(1), 129–143. (<a
href="https://doi.org/10.1109/TPDS.2019.2929389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in composing Cloud applications have been driven by deployments of inter-networking heterogeneous microservices across multiple Cloud datacenters. System dependability has been of the upmost importance and criticality to both service vendors and customers. Security, a measurable attribute, is increasingly regarded as the representative example of dependability. Literally, with the increment of microservice types and dynamicity, applications are exposed to aggravated internal security threats and externally environmental uncertainties. Existing work mainly focuses on the QoS-aware composition of native VM-based Cloud application components, while ignoring uncertainties and security risks among interactive and interdependent container-based microservices. Still, orchestrating a set of microservices across datacenters under those constraints remains computationally intractable. This paper describes a new dependable microservice orchestration framework GA-Par to effectively select and deploy microservices whilst reducing the discrepancy between user security requirements and actual service provision. We adopt a hybrid (both whitebox and blackbox based) approach to measure the satisfaction of security requirement and the environmental impact of network QoS on system dependability. Due to the exponential grow of solution space, we develop a parallel Genetic Algorithm framework based on Spark to accelerate the operations for calculating the optimal or near-optimal solution. Large-scale real world datasets are utilized to validate models and orchestration approach. Experiments show that our solution outperforms the greedy-based security aware method with 42.34 percent improvement. GA-Par is roughly 4× faster than a Hadoop-based genetic algorithm solver and the effectiveness can be constantly guaranteed under different application scales.},
  archive      = {J_TPDS},
  author       = {Zhenyu Wen and Tao Lin and Renyu Yang and Shouling Ji and Rajiv Ranjan and Alexander Romanovsky and Changting Lin and Jie Xu},
  doi          = {10.1109/TPDS.2019.2929389},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {129-143},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GA-par: Dependable microservice orchestration framework for geo-distributed clouds},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting parallelism and vectorisation in breadth-first
search for the intel xeon phi. <em>TPDS</em>, <em>31</em>(1), 111–128.
(<a href="https://doi.org/10.1109/TPDS.2019.2927451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern applications generate massive amounts of data that is challenging to process or analyse. Graph algorithms have emerged as a solution for the analysis of such data because they can represent the entities participating in the generation of large-scale datasets in terms of vertices and their relationships in terms of edges. Graph analysis algorithms are used for finding patterns within these relationships, aiming to extract information to be further analysed. The breadth-first search (BFS) is one of the main graph search algorithms used for graph analysis and its optimisation has been widely researched using different parallel computers. However, the parallelisation of BFS has been shown to be challenging because of its inherent characteristics, including irregular memory access patterns, data dependencies and workload imbalance, that limit its scalability. This paper investigates the optimisation of the BFS on the Xeon Phi (Knights Corner), a modern parallel architecture provided with an advanced vector processor supporting the AVX-512 instruction set, using a bespoke development framework integrated with the Graph 500 benchmark. In addition, to demonstrate portability, we show results for a direct port of the algorithms to a more recent version of the Xeon Phi (Knights Landing) and to a Skylake CPU which supports most of the AVX-512 instruction set. Optimised parallel versions of two high-level algorithms for BFS were created using vectorisation, starting with the conventional top-down BFS algorithm and, building on this, a hybrid BFS algorithm. On the KNC our best implementations result in speedups of 1.37x (top-down) and 1.37x (hybrid), for a one million vertices graph, compared to the state-of-the-art. On the KNL and Skylake, the performance is higher than on KNC. In addition, we show results of our best hybrid algorithm on real-world graphs from the SNAP datasets with speedups up to 1.3x on KNC. Performance on KNL and Skylake is again higher, demonstrating the robustness and portability of our algorithm. The hybrid BFS algorithm can be further used to speed up other graph analysis algorithms and the lessons learned from vectorisation can be applied to other algorithms targeting existing and future models of the Xeon Phi and other advanced vector architectures.},
  archive      = {J_TPDS},
  author       = {Mireya Paredes and Graham Riley and Mikel Luján},
  doi          = {10.1109/TPDS.2019.2927451},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {111-128},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploiting parallelism and vectorisation in breadth-first search for the intel xeon phi},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating modern GPU interconnect: PCIe, NVLink, NV-SLI,
NVSwitch and GPUDirect. <em>TPDS</em>, <em>31</em>(1), 94–110. (<a
href="https://doi.org/10.1109/TPDS.2019.2928289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF&#39;s SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink&#39;s topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application&#39;s overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.},
  archive      = {J_TPDS},
  author       = {Ang Li and Shuaiwen Leon Song and Jieyang Chen and Jiajia Li and Xu Liu and Nathan R. Tallent and Kevin J. Barker},
  doi          = {10.1109/TPDS.2019.2928289},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {94-110},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Evaluating modern GPU interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling runtime SpMV format selection through an overhead
conscious method. <em>TPDS</em>, <em>31</em>(1), 80–93. (<a
href="https://doi.org/10.1109/TPDS.2019.2932931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix-vector multiplication (SpMV) is an important kernel and its performance is critical for many applications. Storage format selection is to select the best format to store a sparse matrix; it is essential for SpMV performance. Prior studies have focused on predicting the format that helps SpMV run fastest, but have ignored the runtime prediction and format conversion overhead. This work shows that the runtime overhead makes the predictions from previous solutions frequently sub-optimal and sometimes inferior regarding the end-to-end time. It proposes a new paradigm for SpMV storage selection, an overhead-conscious method. Through carefully designed regression models and neural network-based time series prediction models, the method captures the influence imposed on the overall program performance by the overhead and the benefits of format prediction and conversions. The method employs a novel two-stage lazy-and-light scheme to help control the possible negative effects of format predictions, and at the same time, maximize the overall format conversion benefits. Experiments show that the technique outperforms previous techniques significantly. It improves the overall performance of applications by 1.21X to 1.53X, significantly larger than the 0.83X to 1.25X upper-bound speedups overhead-oblivious methods could give.},
  archive      = {J_TPDS},
  author       = {Weijie Zhou and Yue Zhao and Xipeng Shen and Wang Chen},
  doi          = {10.1109/TPDS.2019.2932931},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {80-93},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling runtime SpMV format selection through an overhead conscious method},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EEPC: A framework for energy-efficient parallel control of
connected cars. <em>TPDS</em>, <em>31</em>(1), 64–79. (<a
href="https://doi.org/10.1109/TPDS.2019.2930500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advanced communication sensors are deployed into the modern connected vehicles (CVs), large amounts of traffic information can be collected in real-time, which gives the chance to explore the various techniques to control the routing of CVs in a ground traffic network. However, the control of CVs often suffers from energy inefficiency due to the constant changes of network capacity and traffic demand. In this paper, we propose a cost-based iterative framework, named EEPC, to explore the energy-efficient parallel control of connected vehicles. EEPC enables the control of CVs to iteratively generate a feasible solution, where the control of each vehicle is guided in an energy-efficient way routing on its own trajectory. EEPC eliminates the conflicts between CVs with a limited number of iterations and in each iteration, EEPC enables each vehicle to coordinate with other vehicles for a same road resource of the traffic network, further determining which vehicle needs the resource most. Note that at each iteration, the imposed cost is updated to guide the coordination between CVs while the energy is always used to guide the control of CVs in EEPC. In addition, we also explore the parallel control of CVs to improve the real-time performance of EEPC. We provide two parallel approaches, one is fine grain and the other is coarse grain. The fine grain performs the parallel control of single-vehicle routing while the coarse grain performs the parallel control of multi-vehicle routing. Note that fine grain adopts multi-threading techniques and coarse grain adopts MPI techniques. The simulation results show that the proposed EEPC can generate a feasible control solution. Notably, we also demonstrate that the generated solution is effective in eliminating the resource conflicts between CVs and in suggesting an energy-efficient route to each vehicle. To the best of our knowledge, this is the first work to explore energy-efficient parallel control of CVs.},
  archive      = {J_TPDS},
  author       = {Minghua Shen and Guojie Luo and Nong Xiao},
  doi          = {10.1109/TPDS.2019.2930500},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {64-79},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EEPC: A framework for energy-efficient parallel control of connected cars},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing energy-efficient MPSoC with untrustworthy 3PIP
cores. <em>TPDS</em>, <em>31</em>(1), 51–63. (<a
href="https://doi.org/10.1109/TPDS.2019.2926721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of large-scale MPSoCs and the globalization of the IC design flow give rise to two major concerns: high power density due to continuous technology scaling and security due to the untrustworthiness of the third-party intellectual property (3PIP) cores. However, little work has been undertaken to consider these two critical issues jointly during the design stage. In this paper, we propose a design methodology that minimizes the energy consumption while simultaneously protecting the MPSoC against the effects of hardware trojans. The proposed methodology consists of three main stages: 1) Task scheduling to introduce core diversity in the MPSoC in order to detect the presence of malicious modifications in the cores, or mute their effects at runtime, 2) Vendor assignment to the cores using a novel heuristic that chooses vendor-specific cores with operating speed that minimizes the total energy consumption of the MPSoC, and 3) Explore optimization opportunities for further energy savings by minimizing idle periods on the cores, which are caused by the inter-task data dependencies. Experimental results show that our solutions consume only 1/3 energy of existing solutions without increasing schedule length while satisfying the security constraints.},
  archive      = {J_TPDS},
  author       = {Yidan Sun and Guiyuan Jiang and Siew-Kei Lam and Fangxin Ning},
  doi          = {10.1109/TPDS.2019.2926721},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {51-63},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Designing energy-efficient MPSoC with untrustworthy 3PIP cores},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning research and development platform:
Characterizing and scheduling with QoS guarantees on GPU clusters.
<em>TPDS</em>, <em>31</em>(1), 34–50. (<a
href="https://doi.org/10.1109/TPDS.2019.2931558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has been widely adopted in various domains of artificial intelligence (AI), achieving dramatic developments in industry and academia. Besides giant AI companies, numerous small and medium-sized enterprises, institutes, and universities (EIUs) have focused on the research and development (R&amp;D) of DL. Considering the high cost of datacenters and high performance computing (HPC) systems, EIUs prefer adopting off-the-shelf GPU clusters as a DL R&amp;D platform for multiple users and developers to process diverse DL workloads. In such scenarios, the scheduling of multiple DL tasks on a shared GPU cluster is both significant and challenging in terms of efficiently utilizing limited resources. Existing schedulers cannot predict the resource requirements of diverse DL workloads, leading to the under-utilization of computing resources and a decline in user satisfaction. This paper proposes GENIE, a QoS-aware dynamic scheduling framework for a shared GPU cluster, which achieves users&#39; QoS guarantee and high system utilization. In accordance with an exhaustive characterization, GENIE analyzes the key factors that affect the performance of DL tasks and proposes a prediction model derived from lightweight profiling to estimate the processing rate and response latency for diverse DL workloads. Based on the prediction models, we propose a QoS-aware scheduling algorithm to identify the best placements for DL tasks and schedule them on the shared cluster. Experiments on a GPU cluster and large-scale simulations demonstrate that GENIE achieves a QoS-guarantee percentage improvement of up to 67.4 percent and a makespan reduction of up to 28.2 percent, compared to other baseline schedulers.},
  archive      = {J_TPDS},
  author       = {Zhaoyun Chen and Wei Quan and Mei Wen and Jianbin Fang and Jie Yu and Chunyuan Zhang and Lei Luo},
  doi          = {10.1109/TPDS.2019.2931558},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {34-50},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Deep learning research and development platform: Characterizing and scheduling with QoS guarantees on GPU clusters},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive alert management for balancing optimal performance
among distributed CSOCs using reinforcement learning. <em>TPDS</em>,
<em>31</em>(1), 16–33. (<a
href="https://doi.org/10.1109/TPDS.2019.2927977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large organizations typically have Cybersecurity Operations Centers (CSOCs) distributed at multiple locations that are independently managed, and they have their own cybersecurity analyst workforce. Under normal operating conditions, the CSOC locations are ideally staffed such that the alerts generated from the sensors in a work-shift are thoroughly investigated by the scheduled analysts in a timely manner. Unfortunately, when adverse events such as increase in alert arrival rates or alert investigation rates occur, alerts have to wait for a longer duration for analyst investigation, which poses a direct risk to organizations. Hence, our research objective is to mitigate the impact of the adverse events by dynamically and autonomously re-allocating alerts to other location(s) such that the performances of all the CSOC locations remain balanced. This is achieved through the development of a novel centralized adaptive decision support system whose task is to re-allocate alerts from the affected locations to other locations. This re-allocation decision is non-trivial because the following must be determined: (1) timing of a re-allocation decision, (2) number of alerts to be reallocated, and (3) selection of the locations to which the alerts must be distributed. The centralized decision-maker (henceforth referred to as agent) continuously monitors and controls the level of operational effectiveness-LOE (a quantified performance metric) of all the locations. The agent&#39;s decision-making framework is based on the principles of stochastic dynamic programming and is solved using reinforcement learning (RL). In the experiments, the RL approach is compared with both rule-based and load balancing strategies. By simulating real-world scenarios, learning the best decisions for the agent, and applying the decisions on sample realizations of the CSOC&#39;s daily operation, the results show that the RL agent outperforms both approaches by generating (near-) optimal decisions that maintain a balanced LOE among the CSOC locations. Furthermore, the scalability experiments highlight the practicality of adapting the method to a large number of CSOC locations.},
  archive      = {J_TPDS},
  author       = {Ankit Shah and Rajesh Ganesan and Sushil Jajodia and Pierangela Samarati and Hasan Cam},
  doi          = {10.1109/TPDS.2019.2927977},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {16-33},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive alert management for balancing optimal performance among distributed CSOCs using reinforcement learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A truthful and efficient incentive mechanism for demand
response in green datacenters. <em>TPDS</em>, <em>31</em>(1), 1–15. (<a
href="https://doi.org/10.1109/TPDS.2018.2882174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datacenter demand response is envisioned as a promising tool for mitigating operational stability issues faced by smart grids. It enables significant potentials in peak load reduction and facilitates the incorporation of distributed generation. Monetary refund from the smart grid can also alleviate the cloud&#39;s burden in escalating electricity cost. However, the current demand response paradigm is inefficient towards incentivizing a cloud service provider (CSP) that operates geo-distributed datacenters. To incentivize CSP participation, this work presents an auction mechanism that enables smart grids to voluntarily submit bids to the CSP to procure diverse amounts of demand response with different payments. To maximize the social welfare of the auction, the CSP that acts as the auctioneer needs to solve the winner determination problem at large-scale. By applying the proximal Jacobian alternating direction method of multipliers, we propose a distributed algorithm for each datacenter to solve a small-scale problem in a parallel fashion. Desirable properties of the proposed auction, such as social welfare maximization and truthfulness are achieved through Vickrey-Clarke-Groves (VCG) payment. Through extensive evaluations based on real datacenter workload traces and IEEE 14-bus test systems, we demonstrate that our incentive mechanism constitutes a win-win mechanism for both the geo-distributed cloud and the smart grid.},
  archive      = {J_TPDS},
  author       = {Zhi Zhou and Fangming Liu and Shutong Chen and Zongpeng Li},
  doi          = {10.1109/TPDS.2018.2882174},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A truthful and efficient incentive mechanism for demand response in green datacenters},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
