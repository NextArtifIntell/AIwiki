<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip---724">TIP - 724</h2>
<ul>
<li><details>
<summary>
(2020). IEEE signal processing society information. <em>TIP</em>,
<em>29</em>, C3. (<a
href="https://doi.org/10.1109/TIP.2020.3033252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TIP},
  doi          = {10.1109/TIP.2020.3033252},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {C3},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IEEE signal processing society information},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain adaptation by class centroid matching and local
manifold self-learning. <em>TIP</em>, <em>29</em>, 9703–9718. (<a
href="https://doi.org/10.1109/TIP.2020.3031220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation has been a fundamental technology for transferring knowledge from a source domain to a target domain. The key issue of domain adaptation is how to reduce the distribution discrepancy between two domains in a proper way such that they can be treated indifferently for learning. In this paper, we propose a novel domain adaptation approach, which can thoroughly explore the data distribution structure of target domain. Specifically, we regard the samples within the same cluster in target domain as a whole rather than individuals and assigns pseudo-labels to the target cluster by class centroid matching. Besides, to exploit the manifold structure information of target data more thoroughly, we further introduce a local manifold self-learning strategy into our proposal to adaptively capture the inherent local connectivity of target samples. An efficient iterative optimization algorithm is designed to solve the objective function of our proposal with theoretical convergence guarantee. In addition to unsupervised domain adaptation, we further extend our method to the semi-supervised scenario including both homogeneous and heterogeneous settings in a direct but elegant way. Extensive experiments on seven benchmark datasets validate the significant superiority of our proposal in both unsupervised and semi-supervised manners.},
  archive      = {J_TIP},
  author       = {Lei Tian and Yongqiang Tang and Liangchen Hu and Zhida Ren and Wensheng Zhang},
  doi          = {10.1109/TIP.2020.3031220},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9703-9718},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Domain adaptation by class centroid matching and local manifold self-learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Temporal hierarchical dictionary guided decoding for online
gesture segmentation and recognition. <em>TIP</em>, <em>29</em>,
9689–9702. (<a href="https://doi.org/10.1109/TIP.2020.3028962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online segmentation and recognition of skeleton- based gestures are challenging. Compared with offline cases, the inference of online settings can only rely on the current few frames and always completes before whole temporal movements are performed. However, incompletely performed gestures are ambiguous and their early recognition is easy to fall into local optimum. In this work, we address the problem with a temporal hierarchical dictionary to guide the hidden Markov model (HMM) decoding procedure. The intuition is that, gestures are ambiguous with high uncertainty at early performing phases, and only become discriminate after certain phases. This uncertainty naturally can be measured by entropy. Thus, we propose a measurement called “relative entropy map” (REM) to encode this temporal context to guide HMM decoding. Furthermore, we introduce a progressive learning strategy with which neural networks could learn a robust recognition of HMM states in an iterative manner. The performance of our method is intensively evaluated on three challenging databases and achieves state-of-the-art results. Our method shows the abilities of both extracting the discriminate connotations and reducing large redundancy in the HMM transition process. It is verified that our framework can achieve online recognition of continuous gesture streams even when they are halfway performed.},
  archive      = {J_TIP},
  author       = {Haoyu Chen and Xin Liu and Jingang Shi and Guoying Zhao},
  doi          = {10.1109/TIP.2020.3028962},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9689-9702},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Temporal hierarchical dictionary guided decoding for online gesture segmentation and recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A signal adaptive prediction filter for video coding using
directional total variation: Mathematical framework and parameter
selection. <em>TIP</em>, <em>29</em>, 9678–9688. (<a
href="https://doi.org/10.1109/TIP.2020.3030590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we combine video compression and modern image processing methods. Iterative filter methods for prediction signals based on classic inpainting methods are introduced and extensive parameter tests are described. In order to construct an alternative prediction filter for video coding, techniques originally employed for inpainting are applied. Thereby, the structures of the underlying prediction were incorporated into the filter construction making it signal adaptive. The resulting optimization problem is solved using the so-called Alternating Direction Method of Multipliers (ADMM). The undertaken novel parameter tests are described and it is shown that they improve the coding efficiency of the tool. The suggested filter is embedded into a software based on HEVC with additional QTBT (Quadtree plus Binary Tree) and MTT (Multi-Type-Tree) block structure. Overall, the proposed filter method obtains average bitrate savings of 1.35\% at an average encoder runtime increase of 28\% and decoder runtime increase of 38\%. UHD test sequences achieve bitrate savings of up to 3.66\% for Random Access.},
  archive      = {J_TIP},
  author       = {Jennifer Rasch and Victor Warno and Jonathan Pfaff and Caren Tischendorf and Detlev Marpe and Heiko Schwarz and Thomas Wiegand},
  doi          = {10.1109/TIP.2020.3030590},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9678-9688},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A signal adaptive prediction filter for video coding using directional total variation: Mathematical framework and parameter selection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Real-time hierarchical supervoxel segmentation via a
minimum spanning tree. <em>TIP</em>, <em>29</em>, 9665–9677. (<a
href="https://doi.org/10.1109/TIP.2020.3030502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervoxel segmentation algorithm has been applied as a preprocessing step for many vision tasks. However, existing supervoxel segmentation algorithms cannot generate hierarchical supervoxel segmentation well preserving the spatiotemporal boundaries in real time, which prevents the downstream applications from accurate and efficient processing. In this paper, we propose a real-time hierarchical supervoxel segmentation algorithm based on the minimum spanning tree (MST), which achieves state-of-the-art accuracy meanwhile at least 11× faster than existing methods. In particular, we present a dynamic graph updating operation into the iterative construction process of the MST, which can geometrically decrease the numbers of vertices and edges. In this way, the proposed method is able to generate arbitrary scales of supervoxels on the fly. We prove the efficiency of our algorithm that can produce hierarchical supervoxels in the time complexity of O(n), where n denotes the number of voxels in the input video. Quantitative and qualitative evaluations on public benchmarks demonstrate that our proposed algorithm significantly outperforms the state-of-the-art algorithms in terms of supervoxel segmentation accuracy and computational efficiency. Furthermore, we demonstrate the effectiveness of the proposed method on a downstream application of video object segmentation.},
  archive      = {J_TIP},
  author       = {Bo Wang and Yiliang Chen and Wenxi Liu and Jing Qin and Yong Du and Guoqiang Han and Shengfeng He},
  doi          = {10.1109/TIP.2020.3030502},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9665-9677},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real-time hierarchical supervoxel segmentation via a minimum spanning tree},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QNet: An adaptive quantization table generator based on
convolutional neural network. <em>TIP</em>, <em>29</em>, 9654–9664. (<a
href="https://doi.org/10.1109/TIP.2020.3030126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The JPEG is one of the most widely used lossy image-compression standards, whose compression performance depends largely on a quantization table. In this work, we utilize a Convolutional Neural Network (CNN) to generate an image-adaptive quantization table in a standard-compliant way. We first build an image set containing more than 10,000 images and generate their optimal quantization tables through a classical genetic algorithm, and then propose a method that can efficiently extract and fuse the frequency and spatial domain information of each image to train a regression network to directly generate adaptive quantization tables. In addition, we extract several representative quantization tables from the dataset and train a classification network to indicate the optimal one for each image, which further improves compression performance and computational efficiency. Tests on diverse images show that the proposed method clearly outperforms the state-of-the-art method. Compared with the standard table at the compression rate of 1.0 bpp, the regression and classification network provide average Peak Signal-to-Noise Ratio (PSNR) gains of nearly 1.2 and 1.4 dB. For the experiment under Structural Similarity Index Measurement (SSIM), the improvements are 0.4\% and 0.54\%, respectively. The proposed method also has competitive computational efficiency, as the regression and classification network only take 15 and 6.25 milliseconds, respectively, to process a 768×512 image on a single CPU core at 3.20 GHz.},
  archive      = {J_TIP},
  author       = {Xiao Yan and Yibo Fan and Kewei Chen and Xulin Yu and Xiaoyang Zeng},
  doi          = {10.1109/TIP.2020.3030126},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9654-9664},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {QNet: An adaptive quantization table generator based on convolutional neural network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local low rank approximation with a parametric disparity
model for light field compression. <em>TIP</em>, <em>29</em>, 9641–9653.
(<a href="https://doi.org/10.1109/TIP.2020.3029655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of light field dimensionality reduction for compression. We describe a local low rank approximation method using a parametric disparity model. The local support of the approximation is defined by super-rays. A super-ray can be seen as a set of super-pixels that are coherent across all light field views. A dedicated super-ray construction method is first described that constrains the super-pixels forming a given super-ray to be all of the same shape and size, dealing with occlusions. This constraint is needed so that the super-rays can be used as supports of angular dimensionality reduction based on low rank matrix approximation. The light field low rank assumption depends on how much the views are correlated, i.e., on how well they can be aligned by disparity compensation. We first introduce a parametric model describing the local variations of disparity within each super-ray. We then consider two methods for estimating the model parameters. The first method simply fits the model on an input disparity map. We then introduce a disparity estimation method using a low rank prior. This method alternatively searches for the best parameters of the disparity model and of the low rank approximation. We assess the proposed disparity parametric model, first assuming that the disparity is constant within a super-ray, and second by considering an affine disparity model. We show that using the proposed disparity parametric model and estimation algorithm gives an alignment of super-pixels across views that favours the low rank approximation compared with using disparity estimated with classical computer vision methods. The low rank matrix approximation is computed on the disparity compensated super-rays using a singular value decomposition (SVD). A coding algorithm is then described for the different components of the proposed disparity-compensated low rank approximation. Experimental results show performance gains, with a rate saving going up to 92.61\%, compared with the JPEG Pleno anchor, for real light fields captured by a Lytro Illum camera. The rate saving goes up to 37.72\% with synthetic light fields. The approach is also shown to outperform an HEVC-based light field compression scheme.},
  archive      = {J_TIP},
  author       = {Elian Dib and Mikaël Le Pendu and Xiaoran Jiang and Christine Guillemot},
  doi          = {10.1109/TIP.2020.3029655},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9641-9653},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local low rank approximation with a parametric disparity model for light field compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An ensemble of generation- and retrieval-based image
captioning with dual generator generative adversarial network.
<em>TIP</em>, <em>29</em>, 9627–9640. (<a
href="https://doi.org/10.1109/TIP.2020.3028651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning, which aims to generate a sentence to describe the key content of a query image, is an important but challenging task. Existing image captioning approaches can be categorised into two types: generation-based methods and retrieval-based methods. Retrieval-based methods describe images by retrieving pre-existing captions from a repository. Generation-based methods synthesize a new sentence that verbalizes the query image. Both ways have certain advantages but suffer from their own disadvantages. In the paper, we propose a novel EnsCaption model, which aims at enhancing an ensemble of retrieval-based and generation-based image captioning methods through a novel dual generator generative adversarial network. Specifically, EnsCaption is composed of a caption generation model that synthesizes tailored captions for the query image, a caption re-ranking model that retrieves the best-matching caption from a candidate caption pool consisting of generated captions and pre-retrieved captions, and a discriminator that learns the multi-level difference between the generated/retrieved captions and the ground-truth captions. During the adversarial training process, the caption generation model and the caption re-ranking model provide improved synthetic and retrieved candidate captions with high ranking scores from the discriminator, while the discriminator based on multi-level ranking is trained to assign low ranking scores to the generated and retrieved image captions. Our model absorbs the merits of both generation-based and retrieval-based approaches. We conduct comprehensive experiments to evaluate the performance of EnsCaption on two benchmark datasets: MSCOCO and Flickr-30K. Experimental results show that EnsCaption achieves impressive performance compared to the strong baseline methods.},
  archive      = {J_TIP},
  author       = {Min Yang and Junhao Liu and Ying Shen and Zhou Zhao and Xiaojun Chen and Qingyao Wu and Chengming Li},
  doi          = {10.1109/TIP.2020.3028651},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9627-9640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An ensemble of generation- and retrieval-based image captioning with dual generator generative adversarial network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task deep dual correlation filters for visual
tracking. <em>TIP</em>, <em>29</em>, 9614–9626. (<a
href="https://doi.org/10.1109/TIP.2020.3029897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filters combined with deep features have delivered impressive results in visual tracking task. However, existing approaches treat deep features produced by different network layers independently, limiting their representation power. To address this issue, this article proposes a multi-task deep dual correlation filters (MDDCF) based method for robust visual tracking. First, a new multi-task learning scheme is designed to take full advantage of the multi-level features of deep networks, where target representation with individual features is regarded as a single task. As such, the interdependencies between different levels of features can be better explored. Second, we reformulate the objective function of the dual correlation filters and propose a new alternating optimization method, allowing joint training of the correlation filters and network parameters. Third, we design an effective object template update scheme which can well capture the target appearance variations. Extensive experimental evaluations on seven benchmark datasets show that the proposed MDDCF tracker performs favorably against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yuhui Zheng and Xinyan Liu and Xu Cheng and Kaihua Zhang and Yi Wu and Shengyong Chen},
  doi          = {10.1109/TIP.2020.3029897},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9614-9626},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-task deep dual correlation filters for visual tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view data fusion oriented clustering via nuclear norm
minimization. <em>TIP</em>, <em>29</em>, 9600–9613. (<a
href="https://doi.org/10.1109/TIP.2020.3029883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image clustering remains challenging when handling image data from heterogeneous sources. Fusing the independent and complementary information existing in heterogeneous sources together facilitates to improve the image clustering performance. To this end, we propose a joint learning framework of multi-view image data fusion and clustering based on nuclear norm minimization. Specifically, we first formulate the problem as matrix factorization to a shared clustering indicator matrix and a representative coefficient matrix. The former is constrained with orthogonality and nonnegativity, which ensures the validation of clustering assignments. The latter is imposed with nuclear norm minimization to achieve compression of principal components for performance improvement. Then, an alternating minimization strategy is employed to efficiently decompose the multi-variable optimization problem into several small solvable sub-problems with closed-form solutions. Extensive experimental results on real-world image and video datasets demonstrate the superiority of proposed method over other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Aiping Huang and Tiesong Zhao and Chia-Wen Lin},
  doi          = {10.1109/TIP.2020.3029883},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9600-9613},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view data fusion oriented clustering via nuclear norm minimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). PoNA: Pose-guided non-local attention for human pose
transfer. <em>TIP</em>, <em>29</em>, 9584–9599. (<a
href="https://doi.org/10.1109/TIP.2020.3029455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose transfer, which aims at transferring the appearance of a given person to a target pose, is very challenging and important in many applications. Previous work ignores the guidance of pose features or only uses local attention mechanism, leading to implausible and blurry results. We propose a new human pose transfer method using a generative adversarial network (GAN) with simplified cascaded blocks. In each block, we propose a pose-guided non-local attention (PoNA) mechanism with a long-range dependency scheme to select more important regions of image features to transfer. We also design pre-posed image-guided pose feature update and post-posed pose-guided image feature update to better utilize the pose and image features. Our network is simple, stable, and easy to train. Quantitative and qualitative results on Market-1501 and DeepFashion datasets show the efficacy and efficiency of our model. Compared with state-of-the-art methods, our model generates sharper and more realistic images with rich details, while having fewer parameters and faster speed. Furthermore, our generated images can help to alleviate data insufficiency for person re-identification.},
  archive      = {J_TIP},
  author       = {Kun Li and Jinsong Zhang and Yebin Liu and Yu-Kun Lai and Qionghai Dai},
  doi          = {10.1109/TIP.2020.3029455},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9584-9599},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PoNA: Pose-guided non-local attention for human pose transfer},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Deep local feature descriptor learning with dual hard batch
construction. <em>TIP</em>, <em>29</em>, 9572–9583. (<a
href="https://doi.org/10.1109/TIP.2020.3029424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local feature descriptor learning aims to represent distinctive images or patches with the same local features, where their representation is invariant under different types of deformation. Recent studies have demonstrated that descriptor learning based on Convolutional Neural Network (CNN) is able to improve the matching performance significantly. However, they tend to ignore the importance of sample selection during the training process, leading to unstable quality of descriptors and learning efficiency. In this paper, a dual hard batch construction method is proposed to sample the hard matching and non-matching examples for training, improving the performance of the descriptor learning on different tasks. To construct the dual hard training batches, the matching examples with the minimum similarity are selected as the hard positive pairs. For each positive pair, the most similar non-matching example is then sampled from the generated hard positive pairs in the same batch as the corresponding negative. By sampling the hard positive pairs and the corresponding hard negatives, the hard batches are produced to force the CNN model to learn the descriptors with more efforts. In addition, based on the above dual hard batch construction, an ℓ 2 2 triplet loss function is built for optimizing the training model. Specifically, we analyze the superiority of the ℓ 2 2 loss function when dealing with hard examples, and also demonstrate it in the experiments. With the benefits of the proposed sampling strategy and the ℓ 2 2 triplet loss function, our method achieves better performance compared to state-of-the-art on the reference benchmarks for different matching tasks.},
  archive      = {J_TIP},
  author       = {Song Wang and Xin Guo and Yun Tie and Lin Qi and Ling Guan},
  doi          = {10.1109/TIP.2020.3029424},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9572-9583},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep local feature descriptor learning with dual hard batch construction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving person re-identification with iterative impression
aggregation. <em>TIP</em>, <em>29</em>, 9559–9571. (<a
href="https://doi.org/10.1109/TIP.2020.3029415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our impression about one person often updates after we see more aspects of him/her and this process keeps iterating given more meetings. We formulate such an intuition into the problem of person re-identification (re-ID), where the representation of a query (probe) image is iteratively updated with new information from the candidates in the gallery. Specifically, we propose a simple attentional aggregation formulation to instantiate this idea and showcase that such a pipeline achieves competitive performance on standard benchmarks including CUHK03, Market-1501 and DukeMTMC. Not only does such a simple method improve the performance of the baseline models, it also achieves comparable performance with latest advanced re-ranking methods. Another advantage of this proposal is its flexibility to incorporate different representations and similarity metrics. By utilizing stronger representations and metrics, we further demonstrate state-of-the-art person re-ID performance, which also validates the general applicability of the proposed method.},
  archive      = {J_TIP},
  author       = {Dengpan Fu and Bo Xin and Jingdong Wang and Dongdong Chen and Jianmin Bao and Gang Hua and Houqiang Li},
  doi          = {10.1109/TIP.2020.3029415},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9559-9571},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving person re-identification with iterative impression aggregation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TNLRS: Target-aware non-local low-rank modeling with
saliency filtering regularization for infrared small target detection.
<em>TIP</em>, <em>29</em>, 9546–9558. (<a
href="https://doi.org/10.1109/TIP.2020.3028457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, infrared small target detection problem has attracted substantial attention. Many works based on local low-rank model have been proven to be very successful for enhancing the discriminability during detection. However, these methods construct patches by traversing local images and ignore the correlations among different patches. Although the calculation is simplified, some texture information of the target is ignored, and targets of arbitrary forms cannot be accurately identified. In this paper, a novel target-aware method based on a non-local low-rank model and saliency filter regularization is proposed, with which the newly proposed detection framework can be tailored as a non-convex optimization problem, therein enabling joint target saliency learning in a lower dimensional discriminative manifold. More specifically, non-local patch construction is applied for the proposed target-aware low-rank model. By combining similar patches, we reconstruct them together to achieve a better generalization of non-local spatial sparsity constraints. Furthermore, to encourage target saliency learning, our proposed saliency filtering regularization term based on entropy is restricted to lie between the background and foreground. The regularization of the saliency filtering locally preserves the contexts from the target and surrounding areas and avoids the deviated approximation of the low-rank matrix. Finally, a unified optimization framework is proposed and solved with the alternative direction multiplier method (ADMM). Experimental evaluations of real infrared images demonstrate that the proposed method is more robust under different complex scenes compared with some state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Hu Zhu and Haopeng Ni and Shiming Liu and Guoxia Xu and Lizhen Deng},
  doi          = {10.1109/TIP.2020.3028457},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9546-9558},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TNLRS: Target-aware non-local low-rank modeling with saliency filtering regularization for infrared small target detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skeleton-based action recognition with multi-stream adaptive
graph convolutional networks. <em>TIP</em>, <em>29</em>, 9532–9545. (<a
href="https://doi.org/10.1109/TIP.2020.3028207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs), which generalize CNNs to more generic non-Euclidean structures, have achieved remarkable performance for skeleton-based action recognition. However, there still exist several issues in the previous GCN-based models. First, the topology of the graph is set heuristically and fixed over all the model layers and input data. This may not be suitable for the hierarchy of the GCN model and the diversity of the data in action recognition tasks. Second, the second-order information of the skeleton data, i.e., the length and orientation of the bones, is rarely investigated, which is naturally more informative and discriminative for the human action recognition. In this work, we propose a novel multi-stream attention-enhanced adaptive graph convolutional neural network (MS-AAGCN) for skeleton-based action recognition. The graph topology in our model can be either uniformly or individually learned based on the input data in an end-to-end manner. This data-driven approach increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Besides, the proposed adaptive graph convolutional layer is further enhanced by a spatial-temporal-channel attention module, which helps the model pay more attention to important joints, frames and features. Moreover, the information of both the joints and bones, together with their motion information, are simultaneously modeled in a multi-stream framework, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.},
  archive      = {J_TIP},
  author       = {Lei Shi and Yifan Zhang and Jian Cheng and Hanqing Lu},
  doi          = {10.1109/TIP.2020.3028207},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9532-9545},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Skeleton-based action recognition with multi-stream adaptive graph convolutional networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel active contour model for noisy image segmentation
based on adaptive fractional order differentiation. <em>TIP</em>,
<em>29</em>, 9520–9531. (<a
href="https://doi.org/10.1109/TIP.2020.3029443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The images used in various practices are often disturbed by noise, such as Gaussian noise, speckled noise, and salt and pepper noise. Images with noise are one of the challenges for segmentation, since the noise may cause inaccurate segmented results. To cope with the effect of noise on images during segmentation, a novel active contour model is proposed in this paper. The newly proposed model consists of fitting term, regularization term and penalty term. The fitting term is designed using a Gaussian kernel function and fractional order differentiation with an adaptively defined fractional order, which applies different orders to different pixels. The regularization term is applied to maintain the smoothness of curves. In order to ensure stable evolution of curves, a penalty term is added into the proposed model. Comparison experiments are conducted to show the effectiveness and efficiency of the proposed model.},
  archive      = {J_TIP},
  author       = {Meng-Meng Li and Bing-Zhao Li},
  doi          = {10.1109/TIP.2020.3029443},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9520-9531},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel active contour model for noisy image segmentation based on adaptive fractional order differentiation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint raindrop and haze removal from a single image.
<em>TIP</em>, <em>29</em>, 9508–9519. (<a
href="https://doi.org/10.1109/TIP.2020.3029438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent study, it was shown that, with adversarial training of an attentive generative network, it is possible to convert a raindrop degraded image into a relatively clean one. However, in real world, raindrop appearance is not only formed by individual raindrops, but also by the distant raindrops accumulation and the atmospheric veiling, namely haze. Current methods are limited in extracting accurate features from a raindrop degraded image with background scene, the blurred raindrop regions, and the haze. In this paper, we propose a new model for an image corrupted by the raindrops and the haze, and introduce an integrated multi-task algorithm to address the joint raindrop and haze removal (JRHR) problem by combining an improved estimate of the atmospheric light, a modified transmission map, a generative adversarial network (GAN) and an optimized visual attention network. The proposed algorithm can extract more accurate features for both sky and non-sky regions. Experimental evaluation has been conducted to show that the proposed algorithm significantly outperforms state-of-the-art algorithms on both synthetic and real-world images in terms of both qualitative and quantitative measures.},
  archive      = {J_TIP},
  author       = {Yina Guo and Jianguo Chen and Xiaowen Ren and Anhong Wang and Wenwu Wang},
  doi          = {10.1109/TIP.2020.3029438},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9508-9519},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint raindrop and haze removal from a single image},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boundary-aware RGBD salient object detection with
cross-modal feature sampling. <em>TIP</em>, <em>29</em>, 9496–9507. (<a
href="https://doi.org/10.1109/TIP.2020.3028170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices usually mount a depth sensor to resolve ill-posed problems, like salient object detection on cluttered background. The main barrier of exploring RGBD data is to handle the information from two different modalities. To cope with this problem, in this paper, we propose a boundary-aware cross-modal fusion network for RGBD salient object detection. In particular, to enhance the fusion of color and depth features, we present a cross-modal feature sampling module to balance the contribution of the RGB and depth features based on the statistics of their channel values. In addition, in our multi-scale dense fusion network architecture, we not only incorporate edge-sensitive losses to preserve the boundary of the detected salient region, but also refine its structure by merging the estimated saliency maps of different scales. We accomplish the multi-scale saliency map merging using two alternative methods which produce refined saliency maps via per-pixel weighted combination and an encoder-decoder network. Extensive experimental evaluations demonstrate that our proposed framework can achieve the state-of-the-art performance on several public RGBD-based datasets.},
  archive      = {J_TIP},
  author       = {Yuzhen Niu and Guanchao Long and Wenxi Liu and Wenzhong Guo and Shengfeng He},
  doi          = {10.1109/TIP.2020.3028170},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9496-9507},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boundary-aware RGBD salient object detection with cross-modal feature sampling},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual-path attention network for compressed sensing image
reconstruction. <em>TIP</em>, <em>29</em>, 9482–9495. (<a
href="https://doi.org/10.1109/TIP.2020.3023629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep neural network methods achieved much success in compressed sensing image reconstruction in recent years, they still have some issues, especially in preserving texture details. In this article, we propose a new dual-path attention network for compressed sensing image reconstruction, which is composed of a structure path, a texture path and a texture attention module. Motivated by the classical paradigm of image structure-texture decomposition, the structure path aims to reconstruct the dominant structure component of the original image, and the texture path targets at recovering the remaining texture details. To better bridge the information between two paths, the texture attention module is designed to deliver the useful structure information to the texture path and predict the texture region, thereby facilitating the recovery of texture details. Two paths are optimized with a unified loss function. In the testing phase, given the measurement vector of a new image, it can be well reconstructed by carrying out the well trained dual-path attention network and integrating the outputs of the structure path and the texture path. Experimental results on the SET5, SET11 and BSD68 testing datasets demonstrate that the proposed method achieves comparable or better results compared with some state-of-the-art deep learning based methods and conventional iterative optimization based methods in terms of reconstruction quality and robustness to noise.},
  archive      = {J_TIP},
  author       = {Yubao Sun and Jiwei Chen and Qingshan Liu and Bo Liu and Guodong Guo},
  doi          = {10.1109/TIP.2020.3023629},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9482-9495},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-path attention network for compressed sensing image reconstruction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sketch-a-segmenter: Sketch-based photo segmenter generation.
<em>TIP</em>, <em>29</em>, 9470–9481. (<a
href="https://doi.org/10.1109/TIP.2020.3028292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given pixel-level annotated data, traditional photo segmentation techniques have achieved promising results. However, these photo segmentation models can only identify objects in categories for which data annotation and training have been carried out. This limitation has inspired recent work on few-shot and zero-shot learning for image segmentation. In this article, we show the value of sketch for photo segmentation, in particular as a transferable representation to describe a concept to be segmented. We show, for the first time, that it is possible to generate a photo-segmentation model of a novel category using just a single sketch and furthermore exploit the unique fine-grained characteristics of sketch to produce more detailed segmentation. More specifically, we propose a sketch-based photo segmentation method that takes sketch as input and synthesizes the weights required for a neural network to segment the corresponding region of a given photo. Our framework can be applied at both the category-level and the instance-level, and fine-grained input sketches provide more accurate segmentation in the latter. This framework generalizes across categories via sketch and thus provides an alternative to zero-shot learning when segmenting a photo from a category without annotated training data. To investigate the instance-level relationship across sketch and photo, we create the SketchySeg dataset which contains segmentation annotations for photos corresponding to paired sketches in the Sketchy Dataset.},
  archive      = {J_TIP},
  author       = {Conghui Hu and Da Li and Yongxin Yang and Timothy M. Hospedales and Yi-Zhe Song},
  doi          = {10.1109/TIP.2020.3028292},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9470-9481},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sketch-a-segmenter: Sketch-based photo segmenter generation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rate distortion optimization: A joint framework and
algorithms for random access hierarchical video coding. <em>TIP</em>,
<em>29</em>, 9458–9469. (<a
href="https://doi.org/10.1109/TIP.2020.3028280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revisits the problem of rate distortion optimization (RDO) with focus on inter-picture dependence. A joint RDO framework which incorporates the Lagrange multiplier as one of parameters to be optimized is proposed. Simplification strategies are demonstrated for practical applications. To make the problem tractable, we consider an approach where prediction residuals of pictures in a video sequence are assumed to be emitted from a finite set of sources. Consequently the RDO problem is formulated as finding optimal coding parameters for a finite number of sources, regardless of the length of the video sequence. Specifically, in cases where a hierarchical prediction structure is used, prediction residuals of pictures at the same prediction layer are assumed to be emitted from a common source. Following this approach, we propose an iterative algorithm to alternatively optimize the selections of quantization parameters (QPs) and the corresponding Lagrange multipliers. Based on the results of the iterative algorithm, we further propose two practical algorithms to compute QPs and the Lagrange multipliers for the RA(random access) hierarchical video coding: the first practical algorithm uses a fixed formula to compute QPs and the Lagrange multipliers, and the second practical algorithm adaptively adjusts both QPs and the Lagrange multipliers. Experimental results show that these three algorithms, integrated into the HM 16.20 reference software of HEVC, can achieve considerable RD improvements over the standard HM 16.20 encoder, in the common RA test configuration.},
  archive      = {J_TIP},
  author       = {Xiangwen Wang and En-hui Yang and Da-ke He and Li Song and Xiang Yu},
  doi          = {10.1109/TIP.2020.3028280},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9458-9469},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rate distortion optimization: A joint framework and algorithms for random access hierarchical video coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SAFNet: A semi-anchor-free network with enhanced feature
pyramid for object detection. <em>TIP</em>, <em>29</em>, 9445–9457. (<a
href="https://doi.org/10.1109/TIP.2020.3028196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the field of object detection has made significant progress. The success of most state-of-the-art object detectors is derived from the use of feature pyramid and the carefully designed anchor boxes. However, the existing methods for constructing feature pyramid blindly integrate multi-scale representations on each feature hierarchy. Furthermore, these detectors also suffer from some drawbacks brought by the hand-designed anchors. To mitigate the adverse effects caused thereby, we propose a semi-anchor-free network with enhanced feature pyramid for object detection, named SAFNet. Specifically, to better construct feature pyramid, we propose a novel enhanced feature pyramid generation paradigm, which consists of two modules, i.e., adaptive feature fusion module (AFFM) and self-enhanced module (SEM). The paradigm adaptively integrates multi-scale representations in a non-linear way meanwhile suppresses the redundant semantic information for each pyramid level. Thus, a clean and enhanced feature pyramid could be obtained. In addition, an adaptive anchor generator (AAG) is designed to yield fewer but more suitable anchor boxes for each input image. Benefiting from the enhanced feature pyramid, AAG is capable of generating more accurate anchor boxes by introducing few priors. With this semi-anchor-free method, our detector has the ability to alleviate the drawbacks of hand-designed anchors meanwhile retain the merits of anchor-based methods. Extensive experiments demonstrate the effectiveness of our approach. Profited from the proposed modules, SAFNet significantly boosts the detection performance, i.e., achieving 2 points and 2.1 points higher Average Precision (AP) than RetinaNet on PASCAL VOC and MS COCO respectively.},
  archive      = {J_TIP},
  author       = {Zhenchao Jin and Bin Liu and Qi Chu and Nenghai Yu},
  doi          = {10.1109/TIP.2020.3028196},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9445-9457},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SAFNet: A semi-anchor-free network with enhanced feature pyramid for object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse BSS from poisson measurements. <em>TIP</em>,
<em>29</em>, 9429–9444. (<a
href="https://doi.org/10.1109/TIP.2020.3027986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of sparse Blind Source Separation (BSS) has been extensively studied when the noise is additive and Gaussian. This is however not the case when the measurements follow Poisson or shot noise statistics, which is customary with counting-based measurements. To that purpose, we introduce a novel sparse BSS algorithm coined pGMCA (poisson-Generalized Morphological Component Analysis) that specifically tackles the blind separation of sparse sources from measurements following Poisson statistics. The proposed algorithm builds upon Nesterov&#39;s smoothing technique to define a smooth approximation of sparse BSS, with a data fidelity term derived from the Poisson likelihood. This allows to design a block coordinate descent-based minimization procedure with a simple choice of the regularization parameter. Numerical experiments have been carried out that illustrate the robustness of the proposed method with respect to Poisson noise. The pGMCA algorithm has been further evaluated in a realistic astrophysical X-ray imaging setting.},
  archive      = {J_TIP},
  author       = {Jérôme Bobin and Imane El Hamzaoui and Adrien Picquenot and Fabio Acero},
  doi          = {10.1109/TIP.2020.3027986},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9429-9444},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sparse BSS from poisson measurements},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image interpolation using multi-scale attention-aware
inception network. <em>TIP</em>, <em>29</em>, 9413–9428. (<a
href="https://doi.org/10.1109/TIP.2020.3026632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new multi-scale deep learning (MDL) framework is proposed and exploited for conducting image interpolation in this paper. The core of the framework is a seeding network that needs to be designed for the targeted task. For image interpolation, a novel attention-aware inception network (AIN) is developed as the seeding network; it has two key stages: 1) feature extraction based on the low-resolution input image; and 2) feature-to-image mapping to enlarge image&#39;s size or resolution. Note that the designed seeding network, AIN, needs to be trained with a matched training dataset at each scale. For that, multi-scale image patches are generated using our proposed pyramid cut, which outperforms the conventional image pyramid method by completely avoiding aliasing issue. After training, the trained AINs are then combined for processing the input image in the testing stage. Extensive experimental simulation results obtained from seven image datasets (comprising 359 images in total) have clearly shown that the proposed MAIN consistently delivers highly accurate interpolated images.},
  archive      = {J_TIP},
  author       = {Jiahuan Ji and Baojiang Zhong and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2020.3026632},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9413-9428},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image interpolation using multi-scale attention-aware inception network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PrivacyNet: Semi-adversarial networks for multi-attribute
face privacy. <em>TIP</em>, <em>29</em>, 9400–9412. (<a
href="https://doi.org/10.1109/TIP.2020.3024026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has established the possibility of deducing soft-biometric attributes such as age, gender, and race from an individual&#39;s face image with high accuracy. However, this raises privacy concerns, especially when face images collected for biometric recognition purposes are used for attribute analysis without the person&#39;s consent. To address this problem, we develop a technique for imparting soft biometric privacy to face images via an image perturbation methodology. The image perturbation is undertaken using a GAN-based Semi-Adversarial Network (SAN) - referred to as PrivacyNet - that modifies an input face image such that it can be used by a face matcher for matching purposes but cannot be reliably used by an attribute classifier. Further, PrivacyNet allows a person to choose specific attributes that have to be obfuscated in the input face images (e.g., age and race), while allowing for other types of attributes to be extracted (e.g., gender). Extensive experiments using multiple face matchers, multiple age/gender/race classifiers, and multiple face datasets demonstrate the generalizability of the proposed multi-attribute privacy enhancing method across multiple face and attribute classifiers.},
  archive      = {J_TIP},
  author       = {Vahid Mirjalili and Sebastian Raschka and Arun Ross},
  doi          = {10.1109/TIP.2020.3024026},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9400-9412},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PrivacyNet: Semi-adversarial networks for multi-attribute face privacy},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-modality person re-identification via modality-aware
collaborative ensemble learning. <em>TIP</em>, <em>29</em>, 9387–9399.
(<a href="https://doi.org/10.1109/TIP.2020.2998275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible thermal person re-identification (VT-ReID) is a challenging cross-modality pedestrian retrieval problem due to the large intra-class variations and modality discrepancy across different cameras. Existing VT-ReID methods mainly focus on learning cross-modality sharable feature representations by handling the modality-discrepancy in feature level. However, the modality difference in classifier level has received much less attention, resulting in limited discriminability. In this paper, we propose a novel modality-aware collaborative ensemble (MACE) learning method with middle-level sharable two-stream network (MSTN) for VT-ReID, which handles the modality-discrepancy in both feature level and classifier level. In feature level, MSTN achieves much better performance than existing methods by capturing sharable discriminative middle-level features in convolutional layers. In classifier level, we introduce both modality-specific and modality-sharable identity classifiers for two modalities to handle the modality discrepancy. To utilize the complementary information among different classifiers, we propose an ensemble learning scheme to incorporate the modality sharable classifier and the modality specific classifiers. In addition, we introduce a collaborative learning strategy, which regularizes modality-specific identity predictions and the ensemble outputs. Extensive experiments on two cross-modality datasets demonstrate that the proposed method outperforms current state-of-the-art by a large margin, achieving rank-1/mAP accuracy 51.64\%/50.11\% on the SYSU-MM01 dataset, and 72.37\%/69.09\% on the RegDB dataset.},
  archive      = {J_TIP},
  author       = {Mang Ye and Xiangyuan Lan and Qingming Leng and Jianbing Shen},
  doi          = {10.1109/TIP.2020.2998275},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9387-9399},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-modality person re-identification via modality-aware collaborative ensemble learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tiny obstacle discovery by occlusion-aware multilayer
regression. <em>TIP</em>, <em>29</em>, 9373–9386. (<a
href="https://doi.org/10.1109/TIP.2020.3026636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edges are the fundamental visual element for discovering tiny obstacles using a monocular camera. Nevertheless, tiny obstacles often have weak and inconsistent edge cues due to various properties such as small size and similar appearance to the free space, making it hard to capture them. To this end, we propose an occlusion-based multilayer approach, which specifies the scene prior as multilayer regions and utilizes these regions in each obstacle discovery module, i.e., edge detection and proposal extraction. Firstly, an obstacle-aware occlusion edge is generated to accurately capture the obstacle contour by fusing the edge cues inside all the multilayer regions, which intensifies the object characteristics of these obstacles. Then, a multistride sliding window strategy is proposed for capturing proposals that enclose the tiny obstacles as completely as possible. Moreover, a novel obstacle-aware regression model is proposed for effectively discovering obstacles. It is formed by a primary-secondary regressor, which can learn two dissimilarities between obstacles and other categories separately, and eventually generate an obstacle-occupied probability map. The experiments are conducted on two datasets to demonstrate the effectiveness of our approach under different scenarios. And the results show that the proposed method can approximately improve accuracy by 19\% over FPHT and PHT, and achieves comparable performance to MergeNet. Furthermore, multiple experiments with different variants validate the contribution of our method. The source code is available at https://github.com/XuefengBUPT/TOD_OMR.},
  archive      = {J_TIP},
  author       = {Feng Xue and Anlong Ming and Yu Zhou},
  doi          = {10.1109/TIP.2020.3026636},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9373-9386},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Tiny obstacle discovery by occlusion-aware multilayer regression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iterative local-global collaboration learning towards
one-shot video person re-identification. <em>TIP</em>, <em>29</em>,
9360–9372. (<a href="https://doi.org/10.1109/TIP.2020.3026625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video person re-identification (video Re-ID) plays an important role in surveillance video analysis and has gained increasing attention recently. However, existing supervised methods require vast labeled identities across cameras. Although some unsupervised approaches have been exploited for video Re-ID, they are still in their infancy due to the complex nature of learning discriminative features on unlabelled data. In this article, we focus on one-shot video Re-ID and present an iterative local-global collaboration learning approach to learn robust and discriminative person representations. Specifically, it jointly considers the global video information and local frame sequence information to better capture the diverse appearance of the person for feature learning and pseudo-label estimation. Moreover, as the cross-entropy loss may induce the model to focus on identity-irrelevant factors, we introduce the variational information bottleneck as a regularization term to train the model together. It can help filter undesirable information and characterize subtle differences among persons. Since accuracy cannot always be guaranteed for pseudo-labels, we adopt a dynamic selection strategy to select part of pseudo-labeled data with higher confidence to update the training set and re-train the learning model. During training, our method iteratively executes the feature learning, pseudo-label estimation, and dynamic sample selection until all the unlabeled data have been seen. Extensive experiments on two public datasets, i.e., DukeMTMC-VideoReID and MARS, have verified the superiority of our model to several cutting-edge competitors.},
  archive      = {J_TIP},
  author       = {Meng Liu and Leigang Qu and Liqiang Nie and Maofu Liu and Lingyu Duan and Baoquan Chen},
  doi          = {10.1109/TIP.2020.3026625},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9360-9372},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Iterative local-global collaboration learning towards one-shot video person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding of curved corridor scenes based on projection
of spatial right-angles. <em>TIP</em>, <em>29</em>, 9345–9359. (<a
href="https://doi.org/10.1109/TIP.2020.3026628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Helping mobile robots understand curved corridor scenes has considerable value in computer vision. However, due to the diversity of curved corridor scenes, such as curved structures that do not satisfy Manhattan assumption, understanding them remains a challenge. Curved non-Manhattan structures can be seen as compositions of spatial right angles projected into two dimensional projections, which may help us estimate their original posture in 3D scenes. In this paper, we presented an approach for mobile robots to understand curved corridor scenes including Manhattan and curved non-Manhattan structures, from a single image. Angle projections can be assigned to different clusters via geometric inference. Then coplanar structures can be estimated. Fold structures consisting of coplanar structures can be estimated, and curved non-Manhattan structures can be approximately represented by fold structures. Based on understanding curved non-Manhattan structures, the method is practical and efficient for a navigating mobile robot in curved corridor scenes. The algorithm requires no prior training or knowledge of the camera&#39;s internal parameters. With geometric features from a monocular camera, the method is robust to calibration errors and image noise. We compared the estimated curved layout against the ground truth and measured the percentage of pixels that were incorrectly classified. The experimental results showed that the algorithm can successfully understand curved corridor scenes including both Manhattan and curved non-Manhattan structures, meeting the requirements of robot navigation in a curved corridor environment.},
  archive      = {J_TIP},
  author       = {Luping Wang and Hui Wei},
  doi          = {10.1109/TIP.2020.3026628},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9345-9359},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Understanding of curved corridor scenes based on projection of spatial right-angles},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based transforms for video coding. <em>TIP</em>,
<em>29</em>, 9330–9344. (<a
href="https://doi.org/10.1109/TIP.2020.3026627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many state-of-the-art compression systems, signal transformation is an integral part of the encoding and decoding process, where transforms provide compact representations for the signals of interest. This paper introduces a class of transforms called graph-based transforms (GBTs) for video compression, and proposes two different techniques to design GBTs. In the first technique, we formulate an optimization problem to learn graphs from data and provide solutions for optimal separable and nonseparable GBT designs, called GL-GBTs. The optimality of the proposed GL-GBTs is also theoretically analyzed based on Gaussian-Markov random field (GMRF) models for intra and inter predicted block signals. The second technique develops edge-adaptive GBTs (EA-GBTs) in order to flexibly adapt transforms to block signals with image edges (discontinuities). The advantages of EA-GBTs are both theoretically and empirically demonstrated. Our experimental results show that the proposed transforms can significantly outperform the traditional Karhunen-Loeve transform (KLT).},
  archive      = {J_TIP},
  author       = {Hilmi E. Egilmez and Yung-Hsuan Chao and Antonio Ortega},
  doi          = {10.1109/TIP.2020.3026627},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9330-9344},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph-based transforms for video coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noisy-as-clean: Learning self-supervised denoising from
corrupted image. <em>TIP</em>, <em>29</em>, 9316–9329. (<a
href="https://doi.org/10.1109/TIP.2020.3026622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised deep networks have achieved promising performance on image denoising, by learning image priors and noise statistics on plenty pairs of noisy and clean images. Unsupervised denoising networks are trained with only noisy images. However, for an unseen corrupted image, both supervised and unsupervised networks ignore either its particular image prior, the noise statistics, or both. That is, the networks learned from external images inherently suffer from a domain gap problem: the image priors and noise statistics are very different between the training and test images. This problem becomes more clear when dealing with the signal dependent realistic noise. To circumvent this problem, in this work, we propose a novel “Noisy-As-Clean” (NAC) strategy of training self-supervised denoising networks. Specifically, the corrupted test image is directly taken as the “clean” target, while the inputs are synthetic images consisted of this corrupted image and a second yet similar corruption. A simple but useful observation on our NAC is: as long as the noise is weak, it is feasible to learn a self-supervised network only with the corrupted image, approximating the optimal parameters of a supervised network learned with pairs of noisy and clean images. Experiments on synthetic and realistic noise removal demonstrate that, the DnCNN and ResNet networks trained with our self-supervised NAC strategy achieve comparable or better performance than the original ones and previous supervised/unsupervised/self-supervised networks. The code is publicly available at https://github.com/csjunxu/Noisy-As-Clean .},
  archive      = {J_TIP},
  author       = {Jun Xu and Yuan Huang and Ming-Ming Cheng and Li Liu and Fan Zhu and Zhou Xu and Ling Shao},
  doi          = {10.1109/TIP.2020.3026622},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9316-9329},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Noisy-as-clean: Learning self-supervised denoising from corrupted image},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online alternate generator against adversarial attacks.
<em>TIP</em>, <em>29</em>, 9305–9315. (<a
href="https://doi.org/10.1109/TIP.2020.3025404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of computer vision has witnessed phenomenal progress in recent years partially due to the development of deep convolutional neural networks. However, deep learning models are notoriously sensitive to adversarial examples which are synthesized by adding quasi-perceptible noises on real images. Some existing defense methods require to re-train attacked target networks and augment the train set via known adversarial attacks, which is inefficient and might be unpromising with unknown attack types. To overcome the above issues, we propose a portable defense method, online alternate generator, which does not need to access or modify the parameters of the target networks. The proposed method works by online synthesizing another image from scratch for an input image, instead of removing or destroying adversarial noises. To avoid pretrained parameters exploited by attackers, we alternately update the generator and the synthesized image at the inference stage. Experimental results demonstrate that the proposed defensive scheme and method outperforms a series of state-of-the-art defending models against gray-box adversarial attacks.},
  archive      = {J_TIP},
  author       = {Haofeng Li and Yirui Zeng and Guanbin Li and Liang Lin and Yizhou Yu},
  doi          = {10.1109/TIP.2020.3025404},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9305-9315},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Online alternate generator against adversarial attacks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image coding with data-driven transforms: Methodology,
performance and potential. <em>TIP</em>, <em>29</em>, 9292–9304. (<a
href="https://doi.org/10.1109/TIP.2020.3025203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image compression has always been an important topic in the last decades due to the explosive increase of images. The popular image compression formats are based on different transforms which convert images from the spatial domain into compact frequency domain to remove the spatial correlation. In this paper, we focus on the exploration of data-driven transform, Karhunen-Loéve transform (KLT), the kernels of which are derived from specific images via Principal Component Analysis (PCA), and design a high efficient KLT based image compression algorithm with variable transform sizes. To explore the optimal compression performance, the multiple transform sizes and categories are utilized and determined adaptively according to their rate-distortion (RD) costs. Moreover, comprehensive analyses on the transform coefficients are provided and a band-adaptive quantization scheme is proposed based on the coefficient RD performance. Extensive experiments are performed on several class-specific images as well as general images, and the proposed method achieves significant coding gain over the popular image compression standards including JPEG, JPEG 2000, and the state-of-the-art dictionary learning based methods.},
  archive      = {J_TIP},
  author       = {Xinfeng Zhang and Chao Yang and Xiaoguang Li and Shan Liu and Haitao Yang and Ioannis Katsavounidis and Shaw-Min Lei and C.-C. Jay Kuo},
  doi          = {10.1109/TIP.2020.3025203},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9292-9304},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image coding with data-driven transforms: Methodology, performance and potential},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain-translated 3D object pose estimation. <em>TIP</em>,
<em>29</em>, 9279–9291. (<a
href="https://doi.org/10.1109/TIP.2020.3025447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic 3D object models have been proven crucial in object pose estimation, as they are utilized to generate a huge number of accurately annotated data. The object pose estimation problem is usually solved for images originating from the real data domain by employing synthetic images for training data enrichment, without fully exploiting the fact that synthetic and real images may have different data distributions. In this work, we argue that 3D object pose estimation problem is easier to solve for images originating from the synthetic domain, rather than the real data domain. To this end, we propose a 3D object pose estimation framework consisting of a two-step process, where a novel pose-oriented image-to-image translation step is first employed to translate noisy real images to clean synthetic ones and then, a 3D object pose estimation method is applied on the translated synthetic images to finally predict the 3D object poses. A novel pose-oriented objective function is employed for training the image-to-image translation network, which enforces that pose-related object image characteristics are preserved in the translated images. As a result, the pose estimation network does not require real data for training purposes. Experimental evaluation has shown that the proposed framework greatly improves the 3D object pose estimation performance, when compared to state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Christos Papaioannidis and Vasileios Mygdalis and Ioannis Pitas},
  doi          = {10.1109/TIP.2020.3025447},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9279-9291},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Domain-translated 3D object pose estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On aggregation of unsupervised deep binary descriptor with
weak bits. <em>TIP</em>, <em>29</em>, 9266–9278. (<a
href="https://doi.org/10.1109/TIP.2020.3025437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the thrilling success achieved by existing binary descriptors, most of them are still in the mire of three limitations: 1) vulnerable to the geometric transformations; 2) incapable of preserving the manifold structure when learning binary codes; 3) NO guarantee to find the true match if multiple candidates happen to have the same Hamming distance to a given query. All these together make the binary descriptor less effective, given large-scale visual recognition tasks. In this paper, we propose a novel learning-based feature descriptor, namely Unsupervised Deep Binary Descriptor (UDBD), which learns transformation invariant binary descriptors via projecting the original data and their transformed sets into a joint binary space. Moreover, we involve a ℓ 2,1 -norm loss term in the binary embedding process to gain simultaneously the robustness against data noises and less probability of mistakenly flipping bits of the binary descriptor, on top of it, a graph constraint is used to preserve the original manifold structure in the binary space. Furthermore, a weak bit mechanism is adopted to find the real match from candidates sharing the same minimum Hamming distance, thus enhancing matching performance. Extensive experimental results on public datasets show the superiority of UDBD in terms of matching and retrieval accuracy over state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Gengshen Wu and Zijia Lin and Guiguang Ding and Qiang Ni and Jungong Han},
  doi          = {10.1109/TIP.2020.3025437},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9266-9278},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {On aggregation of unsupervised deep binary descriptor with weak bits},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential dual attention network for rain streak removal in
a single image. <em>TIP</em>, <em>29</em>, 9250–9265. (<a
href="https://doi.org/10.1109/TIP.2020.3025402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various weather conditions, such as rain, haze, or snow, can degrade visual quality in images/videos, which may significantly degrade the performance of related applications. In this paper, a novel framework based on sequential dual attention deep network is proposed for removing rain streaks (deraining) in a single image, called by SSDRNet (Sequential dual attention-based Single image DeRaining deep Network). Since the inherent correlation among rain steaks within an image should be stronger than that between the rain streaks and the background (non-rain) pixels, a two-stage learning strategy is implemented to better capture the distribution of rain streaks within a rainy image. The two-stage deep neural network primarily involves three blocks: residual dense blocks (RDBs), sequential dual attention blocks (SDABs), and multi-scale feature aggregation modules (MAMs), which are all delicately and specifically designed for rain removal. The two-stage strategy successfully learns very fine details of the rain steaks of the image and then clearly removes them. Extensive experimental results have shown that the proposed deep framework achieves the best performance on qualitative and quantitative metrics compared with state-of-the-art methods. The corresponding code and the trained model of the proposed SSDRNet have been available online at https://github.com/fityanul/SDAN-for-Rain-Removal.},
  archive      = {J_TIP},
  author       = {Chih-Yang Lin and Zhuang Tao and Ai-Sheng Xu and Li-Wei Kang and Fityanul Akhyar},
  doi          = {10.1109/TIP.2020.3025402},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9250-9265},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sequential dual attention network for rain streak removal in a single image},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Camera array for multi-spectral imaging. <em>TIP</em>,
<em>29</em>, 9234–9249. (<a
href="https://doi.org/10.1109/TIP.2020.3024738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many new applications arose for multi-spectral and hyper-spectral imaging. Besides modern biometric systems for identity verification, also agricultural and medical applications came up, which measure the health condition of plants and humans. Despite the growing demand, the acquisition of multi-spectral data is up to the present complicated. Often, expensive, inflexible, or low resolution acquisition setups are only obtainable for specific professional applications. To overcome these limitations, a novel camera array for multi-spectral imaging is presented in this article for generating consistent multi-spectral videos. As differing spectral images are acquired at various viewpoints, a geometrically constrained multi-camera sensor layout is introduced, which enables the formulation of novel registration and reconstruction algorithms to globally set up robust models. On average, the novel acquisition approach achieves a gain of 2.5 dB PSNR compared to recently published multi-spectral filter array imaging systems. At the same time, the proposed acquisition system ensures not only a superior spatial, but also a high spectral, and temporal resolution, while filters are flexibly exchangeable by the user depending on the application. Moreover, depth information is generated, so that 3D imaging applications, e.g., for augmented or virtual reality, become possible. The proposed camera array for multi-spectral imaging can be set up using off-the-shelf hardware, which allows for a compact design and employment in, e.g., mobile devices or drones, while being cost-effective.},
  archive      = {J_TIP},
  author       = {Nils Genser and Jürgen Seiler and André Kaup},
  doi          = {10.1109/TIP.2020.3024738},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9234-9249},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Camera array for multi-spectral imaging},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Projective double reconstructions based dictionary learning
algorithm for cross-domain recognition. <em>TIP</em>, <em>29</em>,
9220–9233. (<a href="https://doi.org/10.1109/TIP.2020.3024728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dictionary learning plays a significant role in the field of machine learning. Existing works mainly focus on learning dictionary from a single domain. In this paper, we propose a novel projective double reconstructions (PDR) based dictionary learning algorithm for cross-domain recognition. Owing the distribution discrepancy between different domains, the label information is hard utilized for improving discriminability of dictionary fully. Thus, we propose a more flexible label consistent term and associate it with each dictionary item, which makes the reconstruction coefficients have more discriminability as much as possible. Due to the intrinsic correlation between cross-domain data, the data should be reconstructed with each other. Based on this consideration, we further propose a projective double reconstructions scheme to guarantee that the learned dictionary has the abilities of data itself reconstruction and data cross-reconstruction. This also guarantees that the data from different domains can be boosted mutually for obtaining a good data alignment, making the learned dictionary have more transferability. We integrate the double reconstructions, label consistency constraint and classifier learning into a unified objective and its solution can be obtained by proposed optimization algorithm that is more efficient than the conventional ℓ1 optimization based dictionary learning methods. The experiments show that the proposed PDR not only greatly reduces the time complexity for both training and testing, but also outperforms over the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Na Han and Jigang Wu and Xiaozhao Fang and Shaohua Teng and Guoxu Zhou and Shengli Xie and Xuelong Li},
  doi          = {10.1109/TIP.2020.3024728},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9220-9233},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Projective double reconstructions based dictionary learning algorithm for cross-domain recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiplex cellular communities in multi-gigapixel colorectal
cancer histology images for tissue phenotyping. <em>TIP</em>,
<em>29</em>, 9204–9219. (<a
href="https://doi.org/10.1109/TIP.2020.3023795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computational pathology, automated tissue phenotyping in cancer histology images is a fundamental tool for profiling tumor microenvironments. Current tissue phenotyping methods use features derived from image patches which may not carry biological significance. In this work, we propose a novel multiplex cellular community-based algorithm for tissue phenotyping integrating cell-level features within a graph-based hierarchical framework. We demonstrate that such integration offers better performance compared to prior deep learning and texture-based methods as well as to cellular community based methods using uniplex networks. To this end, we construct cell-level graphs using texture, alpha diversity and multi-resolution deep features. Using these graphs, we compute cellular connectivity features which are then employed for the construction of a patch-level multiplex network. Over this network, we compute multiplex cellular communities using a novel objective function. The proposed objective function computes a low-dimensional subspace from each cellular network and subsequently seeks a common low-dimensional subspace using the Grassmann manifold. We evaluate our proposed algorithm on three publicly available datasets for tissue phenotyping, demonstrating a significant improvement over existing state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Sajid Javed and Arif Mahmood and Naoufel Werghi and Ksenija Benes and Nasir Rajpoot},
  doi          = {10.1109/TIP.2020.3023795},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9204-9219},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiplex cellular communities in multi-gigapixel colorectal cancer histology images for tissue phenotyping},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cascaded attention guidance network for single rainy image
restoration. <em>TIP</em>, <em>29</em>, 9190–9203. (<a
href="https://doi.org/10.1109/TIP.2020.3023773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring a rainy image with raindrops or rainstreaks of varying scales, directions, and densities is an extremely challenging task. Recent approaches attempt to leverage the rain distribution (e.g., location) as prior to generate satisfactory results. However, concatenation of a single distribution map with the rainy image or with intermediate feature maps is too simplistic to fully exploit the advantages of such priors. To further explore this valuable information, an advanced cascaded attention guidance network, dubbed as CAG-Net, is formulated and designed as a three-stage model. In the first stage, a multi-task learning network is constructed for producing the attention map and coarse de-raining results simultaneously. Subsequently, the coarse results and the rain distribution map are concatenated and fed to the second stage for results refinement. In this stage, the attention map generation network from the first stage is used to formulate a novel semantic consistency loss for better detail recovery. In the third stage, a novel pyramidal “where-and-how” learning mechanism is formulated. At each pyramid level, a two-branch network is designed to take the features from previous stages as inputs to generate better attention-guidance features and de-raining features, which are then combined via a gating scheme to produce the final de-raining results. Moreover, the uncertainty maps are also generated in this stage for more accurate pixel-wise loss calculation. Extensive experiments are carried out for removing raindrops or rainstreaks from both synthetic and real rainy images, and CAG-Net is demonstrated to produce significantly better results than state-of-the-art models. Codes are available at .},
  archive      = {J_TIP},
  author       = {Guoqing Wang and Changming Sun and Arcot Sowmya},
  doi          = {10.1109/TIP.2020.3023773},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9190-9203},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cascaded attention guidance network for single rainy image restoration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Registration of multi-view point sets under the perspective
of expectation-maximization. <em>TIP</em>, <em>29</em>, 9176–9189. (<a
href="https://doi.org/10.1109/TIP.2020.3024096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view registration plays a critical role in 3D model reconstruction. To solve this problem, most previous methods align point sets by either partially exploring available information or blindly utilizing unnecessary information, which may lead to undesired results or extra computation complexity. Accordingly, we propose a novel solution for the multi-view registration under the perspective of Expectation-Maximization (EM). The proposed method assumes that each data point is generated from one unique Gaussian Mixture Model (GMM), where its corresponding points in other point sets are regarded as Gaussian centroids with equal covariance and membership probabilities. As it is difficult to obtain real corresponding points in the registration problem, they are approximated by the nearest neighbor in each other aligned point sets. Based on this assumption, it is reasonable to define the likelihood function including all rigid transformations, which require to be estimated for multi-view registration. Subsequently, the EM algorithm is derived to estimate rigid transformations with one Gaussian covariance by maximizing the likelihood function. Since the GMM component number is automatically determined by the number of point sets, there is no trade-off between registration accuracy and efficiency in the proposed method. Finally, the proposed method is tested on several benchmark data sets and compared with state-of-the-art algorithms. Experimental results demonstrate its superior performance on the accuracy, efficiency, and robustness for multi-view registration.},
  archive      = {J_TIP},
  author       = {Jihua Zhu and Rui Guo and Zhongyu Li and Jing Zhang and Shanmin Pang},
  doi          = {10.1109/TIP.2020.3024096},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9176-9189},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Registration of multi-view point sets under the perspective of expectation-maximization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical feature fusion network for salient object
detection. <em>TIP</em>, <em>29</em>, 9165–9175. (<a
href="https://doi.org/10.1109/TIP.2020.3023774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Network (CNN) has shown their advantages in salient object detection. CNN can generate great saliency maps because it can obtain high-level semantic information. And the semantic information is usually achieved by stacking multiple convolutional layers and pooling layers. However, multiple pooling operations will reduce the size of the feature map and easily blur the boundary of the salient object. Therefore, such operations are not beneficial to generate great saliency results. To alleviate this issue, we propose a novel edge information-guided hierarchical feature fusion network (HFFNet). Our network fuses features hierarchically and retains accurate semantic information and clear edge information effectively. Specifically, we extract image features from different levels of VGG. Then, we fuse the features hierarchically to generate high-level semantic information and low-level edge information. In order to retain better information at different levels, we adopt a one-to-one hierarchical supervision strategy to supervise the generation of low-level information and high-level information respectively. Finally, we use low-level edge information to guide the saliency map generation, and the edge guidance fusion is able to identify saliency regions effectively. The proposed HFFNet has been extensively evaluated on five traditional benchmark datasets. The experimental results demonstrate that the proposed model is fairly effective in salient object detection compared with 10 state-of-the-art models under different evaluation indicators, and it is superior to most of the comparison models.},
  archive      = {J_TIP},
  author       = {Xuelong Li and Dawei Song and Yongsheng Dong},
  doi          = {10.1109/TIP.2020.3023774},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9165-9175},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical feature fusion network for salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Siamese local and global networks for robust face tracking.
<em>TIP</em>, <em>29</em>, 9152–9164. (<a
href="https://doi.org/10.1109/TIP.2020.3023621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved great success in several face-related tasks, such as face detection, alignment and recognition. As a fundamental problem in computer vision, face tracking plays a crucial role in various applications, such as video surveillance, human emotion detection and human-computer interaction. However, few CNN-based approaches are proposed for face (bounding box) tracking. In this article, we propose a face tracking method based on Siamese CNNs, which takes advantages of powerful representations of hierarchical CNN features learned from massive face images. The proposed method captures discriminative face information at both local and global levels. At the local level, representations for attribute patches (i.e., eyes, nose and mouth) are learned to distinguish a face from another one, which are robust to pose changes and occlusions. At the global level, representations for each whole face are learned, which take into account the spatial relationships among local patches and facial characters, such as skin color and nevus. In addition, we build a new large-scale challenging face tracking dataset to evaluate face tracking methods and to facilitate the research forward in this field. Extensive experiments on the collected dataset demonstrate the effectiveness of our method in comparison to several state-of-the-art visual tracking methods.},
  archive      = {J_TIP},
  author       = {Yuankai Qi and Shengping Zhang and Feng Jiang and Huiyu Zhou and Dacheng Tao and Xuelong Li},
  doi          = {10.1109/TIP.2020.3023621},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9152-9164},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Siamese local and global networks for robust face tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards unsupervised deep image enhancement with generative
adversarial network. <em>TIP</em>, <em>29</em>, 9140–9151. (<a
href="https://doi.org/10.1109/TIP.2020.3023615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the aesthetic quality of images is challenging and eager for the public. To address this problem, most existing algorithms are based on supervised learning methods to learn an automatic photo enhancer for paired data, which consists of low-quality photos and corresponding expert-retouched versions. However, the style and characteristics of photos retouched by experts may not meet the needs or preferences of general users. In this paper, we present an unsupervised image enhancement generative adversarial network (UEGAN), which learns the corresponding image-to-image mapping from a set of images with desired characteristics in an unsupervised manner, rather than learning on a large number of paired images. The proposed model is based on single deep GAN which embeds the modulation and attention mechanisms to capture richer global and local features. Based on the proposed model, we introduce two losses to deal with the unsupervised image enhancement: (1) fidelity loss , which is defined as a $\ell 2$ regularization in the feature domain of a pre-trained VGG network to ensure the content between the enhanced image and the input image is the same, and (2) quality loss that is formulated as a relativistic hinge adversarial loss to endow the input image the desired characteristics. Both quantitative and qualitative results show that the proposed model effectively improves the aesthetic quality of images. Our code is available at: https://github.com/eezkni/UEGAN .},
  archive      = {J_TIP},
  author       = {Zhangkai Ni and Wenhan Yang and Shiqi Wang and Lin Ma and Sam Kwong},
  doi          = {10.1109/TIP.2020.3023615},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9140-9151},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards unsupervised deep image enhancement with generative adversarial network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Consistent video style transfer via relaxation and
regularization. <em>TIP</em>, <em>29</em>, 9125–9139. (<a
href="https://doi.org/10.1109/TIP.2020.3024018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, neural style transfer has attracted more and more attention, especially for image style transfer. However, temporally consistent style transfer for videos is still a challenging problem. Existing methods, either relying on a significant amount of video data with optical flows or using single-frame regularizers, fail to handle strong motions or complex variations, therefore have limited performance on real videos. In this article, we address the problem by jointly considering the intrinsic properties of stylization and temporal consistency. We first identify the cause of the conflict between style transfer and temporal consistency, and propose to reconcile this contradiction by relaxing the objective function, so as to make the stylization loss term more robust to motions. Through relaxation, style transfer is more robust to inter-frame variation without degrading the subjective effect. Then, we provide a novel formulation and understanding of temporal consistency. Based on the formulation, we analyze the drawbacks of existing training strategies and derive a new regularization. We show by experiments that the proposed regularization can better balance the spatial and temporal performance. Based on relaxation and regularization, we design a zero-shot video style transfer framework. Moreover, for better feature migration, we introduce a new module to dynamically adjust inter-channel distributions. Quantitative and qualitative results demonstrate the superiority of our method over state-of-the-art style transfer methods. Our project is publicly available at: https://daooshee.github.io/ReReVST/ .},
  archive      = {J_TIP},
  author       = {Wenjing Wang and Shuai Yang and Jizheng Xu and Jiaying Liu},
  doi          = {10.1109/TIP.2020.3024018},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9125-9139},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Consistent video style transfer via relaxation and regularization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STFlow: Self-taught optical flow estimation using pseudo
labels. <em>TIP</em>, <em>29</em>, 9113–9124. (<a
href="https://doi.org/10.1109/TIP.2020.3024015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Deep learning of optical flow has been an active area for its empirical success. For the difficulty of obtaining accurate dense correspondence labels, unsupervised learning of optical flow has drawn more and more attention, while the accuracy is still far from satisfaction. By holding the philosophy that better estimation models can be trained with better-approximated labels, which in turn can be obtained from better estimation models, we propose a self-taught learning framework to continually improve the accuracy using self-generated pseudo labels. The estimated optical flow is first filtered by bidirectional flow consistency validation and occlusion-aware dense labels are then generated by edge-aware interpolation from selected sparse matches. Moreover, by combining reconstruction loss with regression loss on the generated pseudo labels, the performance is further improved. The experimental results demonstrate that our models achieve state-of-the-art results among unsupervised methods on the public KITTI, MPI-Sintel and Flying Chairs datasets.},
  archive      = {J_TIP},
  author       = {Zhe Ren and Wenhan Luo and Junchi Yan and Wenlong Liao and Xiaokang Yang and Alan Yuille and Hongyuan Zha},
  doi          = {10.1109/TIP.2020.3024015},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9113-9124},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {STFlow: Self-taught optical flow estimation using pseudo labels},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MSdB-NMF: MultiSpectral document image binarization
framework via non-negative matrix factorization approach. <em>TIP</em>,
<em>29</em>, 9099–9112. (<a
href="https://doi.org/10.1109/TIP.2020.3023613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method for Multispectral document image binarization (MSdB) through the Non-negative Matrix Factorization (NMF) approach. We propose a three-step MSdB-NMF framework: i) NMF-based feature extraction algorithm by introducing a new optimization problem; ii) post-processing method iii); apply any existing gray/RGB binarization scheme. In the first step, we extract N features out of B spectral bands (N &lt;; B) and their corresponding coefficient matrix. We introduce a novel objective formulation that considers the robustness (related to the noise and various types of degradations) and sparseness (related to the ratio of text pixels versus the background). We employ the multiplicative updating rules to solve the proposed minimization problem and prove the convergence of the proposed feature extraction algorithm. In the next step, we select an appropriate feature vector, equivalently the corresponding coefficient vector. We propose to select it either visually or automatically via a post-processing method, which uses the benchmark binarization methods as baseline. In the last step, we apply some existing binarization methods such as Sauvola and Howe over the selected coefficient vector. Our proposed binarization framework is applicable for any kind of MS or hyperspectral (HS) document image without considering any prior knowledge such as the side information about the spectral bands of MS/HS document image. We evaluate our proposed binarization framework over two MS document image datasets. The experimental results confirm that our proposed framework outperforms several state-of-the-art binarization schemes including the winner of the contest in MS-TEx-2015.},
  archive      = {J_TIP},
  author       = {Yaser Esmaeili Salehani and Ehsan Arabnejad and Abderrahmane Rahiche and Athmane Bakhta and Mohamed Cheriet},
  doi          = {10.1109/TIP.2020.3023613},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9099-9112},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MSdB-NMF: MultiSpectral document image binarization framework via non-negative matrix factorization approach},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based spatio-temporal feature learning for
neuromorphic vision sensing. <em>TIP</em>, <em>29</em>, 9084–9098. (<a
href="https://doi.org/10.1109/TIP.2020.3023597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic vision sensing (NVS) devices represent visual information as sequences of asynchronous discrete events (a.k.a., “spikes”) in response to changes in scene reflectance. Unlike conventional active pixel sensing (APS), NVS allows for significantly higher event sampling rates at substantially increased energy efficiency and robustness to illumination changes. However, feature representation for NVS is far behind its APS-based counterparts, resulting in lower performance in high-level computer vision tasks. To fully utilize its sparse and asynchronous nature, we propose a compact graph representation for NVS, which allows for end-to-end learning with graph convolution neural networks. We couple this with a novel end-to-end feature learning framework that accommodates both appearance-based and motion-based tasks. The core of our framework comprises a spatial feature learning module, which utilizes residual-graph convolutional neural networks (RG-CNN), for end-to-end learning of appearance-based features directly from graphs. We extend this with our proposed Graph2Grid block and temporal feature learning module for efficiently modelling temporal dependencies over multiple graphs and a long temporal extent. We show how our framework can be configured for object classification, action recognition and action similarity labeling. Importantly, our approach preserves the spatial and temporal coherence of spike events, while requiring less computation and memory. The experimental validation shows that our proposed framework outperforms all recent methods on standard datasets. Finally, to address the absence of large real-world NVS datasets for complex recognition tasks, we introduce, evaluate and make available the American Sign Language letters (ASL-DVS), as well as human action dataset (UCF101-DVS, HMDB51-DVS and ASLAN-DVS).},
  archive      = {J_TIP},
  author       = {Yin Bi and Aaron Chadha and Alhabib Abbas and Eirina Bourtsoulatze and Yiannis Andreopoulos},
  doi          = {10.1109/TIP.2020.3023597},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9084-9098},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph-based spatio-temporal feature learning for neuromorphic vision sensing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BSD-GAN: Branched generative adversarial network for
scale-disentangled representation learning and image synthesis.
<em>TIP</em>, <em>29</em>, 9073–9083. (<a
href="https://doi.org/10.1109/TIP.2020.3014608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce BSD-GAN, a novel multi-branch and scale-disentangled training method which enables unconditional Generative Adversarial Networks (GANs) to learn image representations at multiple scales, benefiting a wide range of generation and editing tasks. The key feature of BSD-GAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network of BSD-GAN, is deliberately split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively “de-freeze” the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are added. A consequence of such an explicit sub-vector designation is that we can directly manipulate and even combine latent (sub-vector) codes which model different feature scales. Extensive experiments demonstrate the effectiveness of our training method in scale-disentangled learning of image representations and synthesis of novel image contents, without any extra labels and without compromising quality of the synthesized high-resolution images. We further demonstrate several image generation and manipulation applications enabled or improved by BSD-GAN.},
  archive      = {J_TIP},
  author       = {Zili Yi and Zhiqin Chen and Hao Cai and Wendong Mao and Minglun Gong and Hao Zhang},
  doi          = {10.1109/TIP.2020.3014608},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9073-9083},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BSD-GAN: Branched generative adversarial network for scale-disentangled representation learning and image synthesis},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pose-guided person image synthesis in the non-iconic views.
<em>TIP</em>, <em>29</em>, 9060–9072. (<a
href="https://doi.org/10.1109/TIP.2020.3023853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating realistic images with the guidance of reference images and human poses is challenging. Despite the success of previous works on synthesizing person images in the iconic views, no efforts are made towards the task of pose-guided image synthesis in the non-iconic views. Particularly, we find that previous models cannot handle such a complex task, where the person images are captured in the non-iconic views by commercially-available digital cameras. To this end, we propose a new framework - Multi-branch Refinement Network (MR-Net), which utilizes several visual cues, including target person poses, foreground person body and scene images parsed. Furthermore, a novel Region of Interest (RoI) perceptual loss is proposed to optimize the MR-Net. Extensive experiments on two non-iconic datasets, Penn Action and BBC-Pose, as well as an iconic dataset - Market-1501, show the efficacy of the proposed model that can tackle the problem of pose-guided person image generation from the non-iconic views. The data, models, and codes are downloadable from https://github.com/loadder/MR-Net.},
  archive      = {J_TIP},
  author       = {Chengming Xu and Yanwei Fu and Chao Wen and Ye Pan and Yu-Gang Jiang and Xiangyang Xue},
  doi          = {10.1109/TIP.2020.3023853},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9060-9072},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pose-guided person image synthesis in the non-iconic views},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust low-rank tensor recovery via nonconvex singular value
minimization. <em>TIP</em>, <em>29</em>, 9044–9059. (<a
href="https://doi.org/10.1109/TIP.2020.3023798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor robust principal component analysis via tensor nuclear norm (TNN) minimization has been recently proposed to recover the low-rank tensor corrupted with sparse noise/outliers. TNN is demonstrated to be a convex surrogate of rank. However, it tends to over-penalize large singular values and thus usually results in biased solutions. To handle this issue, we propose a new definition of tensor logarithmic norm (TLN) as the nonconvex surrogate of rank, which can decrease the penalization on larger singular values and increase that on smaller ones simultaneously to preserve the low-rank structure of a tensor. Then, the strategy of tensor factorization is combined into the minimization of TLN to improve computational performance. To handle impulsive scenarios, we propose a nonconvex Ip-ball projection scheme with 0 &lt;; p &lt;; 1 instead of the conventional convex scheme with p = 1, which enhances the robustness against outliers. By incorporating the TLN minimization and the Ip-ball projection, we finally propose two low-rank recovery algorithms, whose resulting optimization problems are efficiently solved by the alternating direction method of multipliers (ADMM) with convergence guarantees. The proposed algorithms are applied to the synthetic data recovery and image and video restorations in real-world. Experimental results demonstrate the superior performance of the proposed methods over several state-of-the-art algorithms in terms of tensor recovery accuracy and computational efficiency.},
  archive      = {J_TIP},
  author       = {Lin Chen and Xue Jiang and Xingzhao Liu and Zhixin Zhou},
  doi          = {10.1109/TIP.2020.3023798},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9044-9059},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust low-rank tensor recovery via nonconvex singular value minimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring task structure for brain tumor segmentation from
multi-modality MR images. <em>TIP</em>, <em>29</em>, 9032–9043. (<a
href="https://doi.org/10.1109/TIP.2020.3023609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor segmentation, which aims at segmenting the whole tumor area, enhancing tumor core area, and tumor core area from each input multi-modality bio-imaging data, has received considerable attention from both academia and industry. However, the existing approaches usually treat this problem as a common semantic segmentation task without taking into account the underlying rules in clinical practice. In reality, physicians tend to discover different tumor areas by weighing different modality volume data. Also, they initially segment the most distinct tumor area, and then gradually search around to find the other two. We refer to the first property as the task-modality structure while the second property as the task-task structure, based on which we propose a novel task-structured brain tumor segmentation network (TSBTS net). Specifically, to explore the task-modality structure, we design a modality-aware feature embedding mechanism to infer the important weights of the modality data during network learning. To explore the task-task structure, we formulate the prediction of the different tumor areas as conditional dependency sub-tasks and encode such dependency in the network stream. Experiments on BraTS benchmarks show that the proposed method achieves superior performance in segmenting the desired brain tumor areas while requiring relatively lower computational costs, compared to other state-of-the-art methods and baseline models.},
  archive      = {J_TIP},
  author       = {Dingwen Zhang and Guohai Huang and Qiang Zhang and Jungong Han and Junwei Han and Yizhou Wang and Yizhou Yu},
  doi          = {10.1109/TIP.2020.3023609},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9032-9043},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring task structure for brain tumor segmentation from multi-modality MR images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning long-term structural dependencies for video salient
object detection. <em>TIP</em>, <em>29</em>, 9017–9031. (<a
href="https://doi.org/10.1109/TIP.2020.3023591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing video salient object detection (VSOD) methods focus on exploring either short-term or long-term temporal information. However, temporal information is exploited in a global frame-level or regular grid structure, neglecting inter-frame structural dependencies. In this article, we propose to learn long-term structural dependencies with a structure-evolving graph convolutional network (GCN). Particularly, we construct a graph for the entire video using a fast supervoxel segmentation method, in which each node is connected according to spatio-temporal structural similarity. We infer the inter-frame structural dependencies of salient object using convolutional operations on the graph. To prune redundant connections in the graph and better adapt to the moving salient object, we present an adaptive graph pooling to evolve the structure of the graph by dynamically merging similar nodes, learning better hierarchical representations of the graph. Experiments on six public datasets show that our method outperforms all other state-of-the-art methods. Furthermore, We also demonstrate that our proposed adaptive graph pooling can effectively improve the supervoxel algorithm in the term of segmentation accuracy.},
  archive      = {J_TIP},
  author       = {Bo Wang and Wenxi Liu and Guoqiang Han and Shengfeng He},
  doi          = {10.1109/TIP.2020.3023591},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9017-9031},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning long-term structural dependencies for video salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust face super-resolution via position relation model
based on global face context. <em>TIP</em>, <em>29</em>, 9002–9016. (<a
href="https://doi.org/10.1109/TIP.2020.3023580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because Face Super-Resolution (FSR) tends to infer High-Resolution (HR) face image by breaking the given Low-Resolution (LR) image into individual patches and inferring the HR correspondence one patch by one separately, Super-Resolution (SR) of face images with serious degradation, especially with occlusion, is still a challenging problem of the computer vision field. To address this problem, we propose a patch-level face model for FSR, which we called the position relation model. This model consists of the mapping relationships in every face position to the rest of the face positions based on similarity. In other words, we build a constraint for each patch position via the relationship in this model from the global range of face. Once an individual input LR image patch is seriously deteriorated, the substitute patch in whole face range can be sought according to the relationship of the model at this position as the provider of the LR information. In this way, the lost facial structures can be compensated by knowledge located in remote pixels or structure information which leads to better high-resolution face images. The LR images with degradations, not only the serious low-quality degradation, e.g. noise, blur, but also the occlusions, can be effectively hallucinated into HR ones. Quantitative and qualitative evaluations on the public datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Liang Chen and Jinshan Pan and Junjun Jiang and Jiawei Zhang and Yi Wu},
  doi          = {10.1109/TIP.2020.3023580},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9002-9016},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust face super-resolution via position relation model based on global face context},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deterministic model fitting by local-neighbor preservation
and global-residual optimization. <em>TIP</em>, <em>29</em>, 8988–9001.
(<a href="https://doi.org/10.1109/TIP.2020.3023576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric model fitting has been widely used in many computer vision tasks. However, it remains as a challenging task when handing multiple-structural data contaminated by noises and outliers. Most previous work on model fitting cannot guarantee the consistency of their solutions due to their randomness, precluding them from many real-world applications. In this research, we propose a fast two-view approximately deterministic model fitting scheme (called LGF), to provide consistent solutions for multiple-structural data. The proposed LGF scheme starts from defining preference function by preserving local neighborhood relationship, and then adopts the min-hash technique to roughly sample subsets. By this way, it is able to cover all model instances in data in the parameter space with a high probability. After that, LGF refines the previous sampled subsets by global-residual optimization. Furthermore, we propose a simple yet effective model selection framework to estimate the number and the parameters of model instances in data. Extensive experiments on real images show that the proposed LGF scheme is able to observe superior or very competitive performance on both accuracy and speed over several state-of-the-art model fitting methods.},
  archive      = {J_TIP},
  author       = {Guobao Xiao and Jiayi Ma and Shiping Wang and Changwen Chen},
  doi          = {10.1109/TIP.2020.3023576},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8988-9001},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deterministic model fitting by local-neighbor preservation and global-residual optimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep video deblurring using sharpness features from
exemplars. <em>TIP</em>, <em>29</em>, 8976–8987. (<a
href="https://doi.org/10.1109/TIP.2020.3023534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video deblurring is a challenging problem as the blur in videos is usually caused by camera shake, object motion, depth variation, etc. Existing methods usually impose handcrafted image priors or use end-to-end trainable networks to solve this problem. However, using image priors usually leads to highly non-convex problems while directly using end-to-end trainable networks in a regression generates over-smoothes details in the restored images. In this article, we explore the sharpness features from exemplars to help the blur removal and details restoration. We first estimate optical flow to explore the temporal information which can help to make full use of neighboring information. Then, we develop an encoder and decoder network and explore the sharpness features from exemplars to guide the network for better image restoration. We train the proposed algorithm in an end-to-end manner and show that using sharpness features from exemplars can help blur removal and details restoration. Both quantitative and qualitative evaluations demonstrate that our method performs favorably against state-of-the-art approaches on the benchmark video deblurring datasets and real-world images.},
  archive      = {J_TIP},
  author       = {Xinguang Xiang and Hao Wei and Jinshan Pan},
  doi          = {10.1109/TIP.2020.3023534},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8976-8987},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep video deblurring using sharpness features from exemplars},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Group sparsity residual constraint with non-local priors for
image restoration. <em>TIP</em>, <em>29</em>, 8960–8975. (<a
href="https://doi.org/10.1109/TIP.2020.3021291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group sparse representation (GSR) has made great strides in image restoration producing superior performance, realized through employing a powerful mechanism to integrate the local sparsity and nonlocal self-similarity of images. However, due to some form of degradation (e.g., noise, down-sampling or pixels missing), traditional GSR models may fail to faithfully estimate sparsity of each group in an image, thus resulting in a distorted reconstruction of the original image. This motivates us to design a simple yet effective model that aims to address the above mentioned problem. Specifically, we propose group sparsity residual constraint with nonlocal priors (GSRC-NLP) for image restoration. Through introducing the group sparsity residual constraint, the problem of image restoration is further defined and simplified through attempts at reducing the group sparsity residual. Towards this end, we first obtain a good estimation of the group sparse coefficient of each original image group by exploiting the image nonlocal self-similarity (NSS) prior along with self-supervised learning scheme, and then the group sparse coefficient of the corresponding degraded image group is enforced to approximate the estimation. To make the proposed scheme tractable and robust, two algorithms, i.e., iterative shrinkage/thresholding (IST) and alternating direction method of multipliers (ADMM), are employed to solve the proposed optimization problems for different image restoration tasks. Experimental results on image denoising, image inpainting and image compressive sensing (CS) recovery, demonstrate that the proposed GSRC-NLP based image restoration algorithm is comparable to state-of-the-art denoising methods and outperforms several testing image inpainting and image CS recovery methods in terms of both objective and perceptual quality metrics.},
  archive      = {J_TIP},
  author       = {Zhiyuan Zha and Xin Yuan and Bihan Wen and Jiantao Zhou and Ce Zhu},
  doi          = {10.1109/TIP.2020.3021291},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8960-8975},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Group sparsity residual constraint with non-local priors for image restoration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A signal-processing framework for occlusion of 3D scene to
improve the rendering quality of views. <em>TIP</em>, <em>29</em>,
8944–8959. (<a href="https://doi.org/10.1109/TIP.2020.3020650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusions will reduce the performance of systems in many computer vision applications with discontinuous surfaces of 3D scenes. We explore a signal-processing framework of occlusions based on the light ray visibility to improve the rendering quality of views. An occlusion field (OCF) theory is derived by calculating the relationship between the occluded light rays and the nonoccluded light rays to quantify the occlusion degree (OCD). The OCF framework can describe the various in-scene information captured by the changes in the camera configuration (i.e., position and direction) through a quantitative description of the occlusion information. From a spectral analysis of the OCF, we mathematically derive analytical functions to determine the changing relationship between the scene and the camera configuration. A reconstruction filter can be designed to achieve interference cancellation and compensate for the missing information caused by the occlusions. Our measurements of different occlusions using this OCF framework included both synthetic and actual scenes. The experimental results show that the proposed OCF framework can improves the rendering quality of views and outperforms other known occlusion quantization schemes in a complex scene.},
  archive      = {J_TIP},
  author       = {Changjian Zhu and Hong Zhang and Qiuming Liu and Zhixian Zhuang and Li Yu},
  doi          = {10.1109/TIP.2020.3020650},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8944-8959},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A signal-processing framework for occlusion of 3D scene to improve the rendering quality of views},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Isosceles constraints for person re-identification.
<em>TIP</em>, <em>29</em>, 8930–8943. (<a
href="https://doi.org/10.1109/TIP.2020.3020648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the existing works of person re-identification (ReID), batch hard triplet loss has achieved great success. However, it only cares about the hardest samples within the batch. For any probe, there are massive mismatched samples (crucial samples) outside the batch which are closer than the matched samples. To reduce the disruptive influence of crucial samples, we propose a novel isosceles contraint for triplet. Theoretically, we show that if a matched pair has equal distance to any one of mismatched sample, the matched pair should be infinitely close. Motivated by this, the isosceles constraint is designed for the two mismatched pairs of each triplet, to restrict some matched pairs with equal distance to different mismatched samples. Meanwhile, to ensure that the distance of mismatched pairs are larger than the matched pairs, margin constraints are necessary. Minimizing the isosceles and margin constraints with respect to the feature extraction network makes the matched pairs closer and the mismatched pairs farther away than the matched ones. By this way, crucial samples are effectively reduced and the performance on ReID is improved greatly. Likewise, our isosceles contraint can be applied to quadruplet as well. Comprehensive experimental evaluations on Market-1501, DukeMTMC-reID and CUHK03 datasets demonstrate the advantages of our isosceles constraint over the related state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Furong Xu and Bingpeng Ma and Hong Chang and Shiguang Shan},
  doi          = {10.1109/TIP.2020.3020648},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8930-8943},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Isosceles constraints for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unified generative adversarial networks for controllable
image-to-image translation. <em>TIP</em>, <em>29</em>, 8916–8929. (<a
href="https://doi.org/10.1109/TIP.2020.3021789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a unified Generative Adversarial Network (GAN) for controllable image-to-image translation, i.e., transferring an image from a source to a target domain guided by controllable structures. In addition to conditioning on a reference image, we show how the model can generate images conditioned on controllable structures, e.g., class labels, object keypoints, human skeletons, and scene semantic maps. The proposed model consists of a single generator and a discriminator taking a conditional image and the target controllable structure as input. In this way, the conditional image can provide appearance information and the controllable structure can provide the structure information for generating the target result. Moreover, our model learns the image-to-image mapping through three novel losses, i.e., color loss, controllable structure guided cycle-consistency loss, and controllable structure guided self-content preserving loss. Also, we present the Fréchet ResNet Distance (FRD) to evaluate the quality of the generated images. Experiments on two challenging image translation tasks, i.e., hand gesture-to-gesture translation and cross-view image translation, show that our model generates convincing results, and significantly outperforms other state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a unified solution, thus it can be applied to solving other controllable structure guided image translation tasks such as landmark guided facial expression translation and keypoint guided person image generation. To the best of our knowledge, we are the first to make one GAN framework work on all such controllable structure guided image translation tasks. Code is available at https://github.com/Ha0Tang/GestureGAN.},
  archive      = {J_TIP},
  author       = {Hao Tang and Hong Liu and Nicu Sebe},
  doi          = {10.1109/TIP.2020.3021789},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8916-8929},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unified generative adversarial networks for controllable image-to-image translation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient and accurate 3D finger knuckle matching using
surface key points. <em>TIP</em>, <em>29</em>, 8903–8915. (<a
href="https://doi.org/10.1109/TIP.2020.3021294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contactless 3D finger knuckle is a new biometric identifier which can offer an accurate, efficient and convenient alternative for the personal identification. The current 3D finger knuckle recognition methods are limited by computationally complex or inefficient matching algorithms, which attempt to compute the matching scores from all possible translational and rotational parameters for matching a pair of templates. The strength of such approach lies in its simplicity and reliability for accurately matching intra-class samples, but expensive computational time is required. Furthermore, attempting on excessive numbers of translational and rotational parameters can also degrade the overall recognition accuracy because the imposter matches can be increased. In fact, this conventional matching approach is commonly adopted in many biometric studies, but its drawbacks have not received adequate attention. This article addresses such 3D finger knuckle recognition problem by developing a more efficient matching approach using surface key points extracted from 3D finger knuckle surfaces. Our comparative experimental results with the state-of-the art method on a publicly available 3D finger knuckle database indicates that our approach can offer over 23 times faster with performance improvement on the accuracy. Although the focus of our work is on 3D finger knuckle recognition, we also present the performance of our method on other publicly available databases with similar 3D biometric patterns including 3D palmprint and 3D fingerprint, to validate the effectiveness of the proposed approach.},
  archive      = {J_TIP},
  author       = {Kevin H. M. Cheng and Ajay Kumar},
  doi          = {10.1109/TIP.2020.3021294},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8903-8915},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient and accurate 3D finger knuckle matching using surface key points},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Progressive cross-modal semantic network for zero-shot
sketch-based image retrieval. <em>TIP</em>, <em>29</em>, 8892–8902. (<a
href="https://doi.org/10.1109/TIP.2020.3020383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal retrieval task that involves searching natural images through the use of free-hand sketches under the zero-shot scenario. Most previous methods project the sketch and image features into a low-dimensional common space for efficient retrieval, and meantime align the projected features to their semantic features (e.g., category-level word vectors) in order to transfer knowledge from seen to unseen classes. However, the projection and alignment are always coupled; as a result, there is a lack of explicit alignment that consequently leads to unsatisfactory zero-shot retrieval performance. To address this issue, we propose a novel progressive cross-modal semantic network. More specifically, it first explicitly aligns the sketch and image features to semantic features, then projects the aligned features to a common space for subsequent retrieval. We further employ cross-reconstruction loss to encourage the aligned features to capture complete knowledge about the two modalities, along with multi-modal Euclidean loss that guarantees similarity between the retrieval features from a sketch-image pair. Extensive experiments conducted on two popular large-scale datasets demonstrate that our proposed approach outperforms state-of-the-art competitors to a remarkable extent: by more than 3\% on the Sketchy dataset and about 6\% on the TU-Berlin dataset in terms of retrieval accuracy.},
  archive      = {J_TIP},
  author       = {Cheng Deng and Xinxun Xu and Hao Wang and Muli Yang and Dacheng Tao},
  doi          = {10.1109/TIP.2020.3020383},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8892-8902},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive cross-modal semantic network for zero-shot sketch-based image retrieval},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forecasting future action sequences with attention: A new
approach to weakly supervised action forecasting. <em>TIP</em>,
<em>29</em>, 8880–8891. (<a
href="https://doi.org/10.1109/TIP.2020.3021497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future human action forecasting from partial observations of activities is an important problem in many practical applications such as assistive robotics, video surveillance and security. We present a method to forecast actions for the unseen future of the video using a neural machine translation technique that uses encoder-decoder architecture. The input to this model is the observed RGB video, and the objective is to forecast the correct future symbolic action sequence. Unlike prior methods that make action predictions for some unseen percentage of video one for each frame, we predict the complete action sequence that is required to accomplish the activity. We coin this task action sequence forecasting. To cater for two types of uncertainty in the future predictions, we propose a novel loss function. We show a combination of optimal transport and future uncertainty losses help to improve results. We evaluate our model in three challenging video datasets (Charades, MPII cooking and Breakfast). We extend our action sequence forecasting model to perform weakly supervised action forecasting on two challenging datasets, the Breakfast and the 50Salads. Specifically, we propose a model to predict actions of future unseen frames without using frame level annotations during training. Using Fisher vector features, our supervised model outperforms the state-of-the-art action forecasting model by 0.83\% and 7.09\% on the Breakfast and the 50Salads datasets respectively. Our weakly supervised model is only 0.6\% behind the most recent state-of-the-art supervised model and obtains comparable results to other published fully supervised methods, and sometimes even outperforms them on the Breakfast dataset. Most interestingly, our weakly supervised model outperforms prior models by 1.04\% leveraging on proposed weakly supervised architecture, and effective use of attention mechanism and loss functions.},
  archive      = {J_TIP},
  author       = {Yan Bin Ng and Basura Fernando},
  doi          = {10.1109/TIP.2020.3021497},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8880-8891},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Forecasting future action sequences with attention: A new approach to weakly supervised action forecasting},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complexity of shapes embedded in zn with a bias towards
squares. <em>TIP</em>, <em>29</em>, 8870–8879. (<a
href="https://doi.org/10.1109/TIP.2020.3021316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape complexity is a hard-to-quantify quality, mainly due to its relative nature. Biased by Euclidean thinking, circles are commonly considered as the simplest. However, their constructions as digital images are only approximations to the ideal form. Consequently, complexity orders computed in reference to circle are unstable. Unlike circles which lose their circleness in digital images, squares retain their qualities. Hence, we consider squares (hypercubes in Z n ) to be the simplest shapes relative to which complexity orders are constructed. Using the connection between L ∞ norm and squares we effectively encode squareness-adapted simplification through which we obtain multi-scale complexity measure, where scale determines the level of interest to the boundary. The emergent scale above which the effect of a boundary feature (appendage) disappears is related to the ratio of the contacting width of the appendage to that of the main body. We discuss what zero complexity implies in terms of information repetition and constructibility and what kind of shapes in addition to squares have zero complexity.},
  archive      = {J_TIP},
  author       = {Mazlum Ferhat Arslan and Sibel Tari},
  doi          = {10.1109/TIP.2020.3021316},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8870-8879},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Complexity of shapes embedded in zn with a bias towards squares},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point2SpatialCapsule: Aggregating features and spatial
relationships of local regions on point clouds using spatial-aware
capsules. <em>TIP</em>, <em>29</em>, 8855–8869. (<a
href="https://doi.org/10.1109/TIP.2020.3019925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning discriminative shape representation directly on point clouds is still challenging in 3D shape analysis and understanding. Recent studies usually involve three steps: first splitting a point cloud into some local regions, then extracting the corresponding feature of each local region, and finally aggregating all individual local region features into a global feature as shape representation using simple max-pooling. However, such pooling-based feature aggregation methods do not adequately take the spatial relationships (e.g. the relative locations to other regions) between local regions into account, which greatly limits the ability to learn discriminative shape representation. To address this issue, we propose a novel deep learning network, named Point2SpatialCapsule, for aggregating features and spatial relationships of local regions on point clouds, which aims to learn more discriminative shape representation. Compared with the traditional max-pooling based feature aggregation networks, Point2SpatialCapsule can explicitly learn not only geometric features of local regions but also the spatial relationships among them. Point2SpatialCapsule consists of two main modules. To resolve the disorder problem of local regions, the first module, named geometric feature aggregation, is designed to aggregate the local region features into the learnable cluster centers, which explicitly encodes the spatial locations from the original 3D space. The second module, named spatial relationship aggregation, is proposed for further aggregating the clustered features and the spatial relationships among them in the feature space using the spatial-aware capsules developed in this article. Compared to the previous capsule network based methods, the feature routing on the spatial-aware capsules can learn more discriminative spatial relationships among local regions for point clouds, which establishes a direct mapping between log priors and the spatial locations through feature clusters. Experimental results demonstrate that Point2SpatialCapsule outperforms the state-of-the-art methods in the 3D shape classification, retrieval and segmentation tasks under the well-known ModelNet and ShapeNet datasets.},
  archive      = {J_TIP},
  author       = {Xin Wen and Zhizhong Han and Xinhai Liu and Yu-Shen Liu},
  doi          = {10.1109/TIP.2020.3019925},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8855-8869},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Point2SpatialCapsule: Aggregating features and spatial relationships of local regions on point clouds using spatial-aware capsules},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning a single model with a wide range of quality factors
for JPEG image artifacts removal. <em>TIP</em>, <em>29</em>, 8842–8854.
(<a href="https://doi.org/10.1109/TIP.2020.3020389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lossy compression brings artifacts into the compressed image and degrades the visual quality. In recent years, many compression artifacts removal methods based on convolutional neural network (CNN) have been developed with great success. However, these methods usually train a model based on one specific value or a small range of quality factors. Obviously, if the test images quality factor does not match to the assumed value range, then degraded performance will be resulted. With this motivation and further consideration of practical usage, a highly robust compression artifacts removal network is proposed in this article. Our proposed network is a single model approach that can be trained for handling a wide range of quality factors while consistently delivering superior or comparable image artifacts removal performance. To demonstrate, we focus on the JPEG compression with quality factors, ranging from 1 to 60. Note that a turnkey success of our proposed network lies in the novel utilization of the quantization tables as part of the training data. Furthermore, it has two branches in parallel-i.e., the restoration branch and the global branch. The former effectively removes the local artifacts, such as ringing artifacts removal. On the other hand, the latter extracts the global features of the entire image that provides highly instrumental image quality improvement, especially effective on dealing with the global artifacts, such as blocking, color shifting. Extensive experimental results performed on color and grayscale images have clearly demonstrated the effectiveness and efficacy of our proposed single-model approach on the removal of compression artifacts from the decoded image.},
  archive      = {J_TIP},
  author       = {Jianwei Li and Yongtao Wang and Haihua Xie and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2020.3020389},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8842-8854},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning a single model with a wide range of quality factors for JPEG image artifacts removal},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PNEN: Pyramid non-local enhanced networks. <em>TIP</em>,
<em>29</em>, 8831–8841. (<a
href="https://doi.org/10.1109/TIP.2020.3019644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing neural networks proposed for low-level image processing tasks are usually implemented by stacking convolution layers with limited kernel size. Every convolution layer merely involves in context information from a small local neighborhood. More contextual features can be explored as more convolution layers are adopted. However it is difficult and costly to take full advantage of long-range dependencies. We propose a novel non-local module, Pyramid Non-local Block, to build up connection between every pixel and all remain pixels. The proposed module is capable of efficiently exploiting pairwise dependencies between different scales of low-level structures. The target is fulfilled through first learning a query feature map with full resolution and a pyramid of reference feature maps with downscaled resolutions. Then correlations with multi-scale reference features are exploited for enhancing pixel-level feature representation. The calculation procedure is economical considering memory consumption and computational cost. Based on the proposed module, we devise a Pyramid Non-local Enhanced Networks for edge-preserving image smoothing which achieves state-of-the-art performance in imitating three classical image smoothing algorithms. Additionally, the pyramid non-local block can be directly incorporated into convolution neural networks for other image restoration tasks. We integrate it into two existing methods for image denoising and single image super-resolution, achieving consistently improved performance.},
  archive      = {J_TIP},
  author       = {Feida Zhu and Chaowei Fang and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2020.3019644},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8831-8841},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PNEN: Pyramid non-local enhanced networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive graph representation learning for video person
re-identification. <em>TIP</em>, <em>29</em>, 8821–8830. (<a
href="https://doi.org/10.1109/TIP.2020.3001693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the remarkable progress of applying deep learning models in video person re-identification (Re-ID). A key factor for video person Re-ID is to effectively construct discriminative and robust video feature representations for many complicated situations. Part-based approaches employ spatial and temporal attention to extract representative local features. While correlations between parts are ignored in the previous methods, to leverage the relations of different parts, we propose an innovative adaptive graph representation learning scheme for video person Re-ID, which enables the contextual interactions between relevant regional features. Specifically, we exploit the pose alignment connection and the feature affinity connection to construct an adaptive structure-aware adjacency graph, which models the intrinsic relations between graph nodes. We perform feature propagation on the adjacency graph to refine regional features iteratively, and the neighbor nodes&#39; information is taken into account for part feature representation. To learn compact and discriminative representations, we further propose a novel temporal resolution-aware regularization, which enforces the consistency among different temporal resolutions for the same identities. We conduct extensive evaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID, experimental results achieve the competitive performance which demonstrates the effectiveness of our proposed method. Code is available at https://github.com/weleen/AGRL.pytorch.},
  archive      = {J_TIP},
  author       = {Yiming Wu and Omar El Farouk Bourahla and Xi Li and Fei Wu and Qi Tian and Xue Zhou},
  doi          = {10.1109/TIP.2020.3001693},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8821-8830},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive graph representation learning for video person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatiotemporal tree filtering for enhancing image change
detection. <em>TIP</em>, <em>29</em>, 8805–8820. (<a
href="https://doi.org/10.1109/TIP.2020.3017339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection has received extensive attention because of its realistic significance and broad application fields. However, none of the existing change detection algorithms can handle all scenarios and tasks so far. Different from the most of contributions from the research community in recent years, this paper does not work on designing new change detection algorithms. We, instead, solve the problem from another perspective by enhancing the raw detection results after change detection. As a result, the proposed method is applicable to various kinds of change detection methods, and regardless of how the results are detected. In this paper, we propose Fast Spatiotemporal Tree Filter (FSTF), a purely unsupervised detection method, to enhance coarse binary detection masks obtained by different kinds of change detection methods. In detail, the proposed FSTF has adopted a volumetric structure to effectively synthesize spatiotemporal information of the same target from the current time and history frames to enhance detection. The computational complexity analyzed in the view of graph theory also show that the fast realization of FSTF is a linear time algorithm, which is capable of handling efficient on-line detection tasks. Finally, comprehensive experiments based on qualitative and quantitative analysis verify that FSTF-based change detection enhancement is superior to several other state-of-the-art methods including fully connected Conditional Random Field (CRF), joint bilateral filter, and guided filter. It is illustrated that FSTF is versatile enough to also improve saliency detection as well as semantic image segmentation.},
  archive      = {J_TIP},
  author       = {Dawei Li and Siyuan Yan and Mingbo Zhao and Tommy W. S. Chow},
  doi          = {10.1109/TIP.2020.3017339},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8805-8820},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatiotemporal tree filtering for enhancing image change detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous surface reflectance and fluorescence spectra
estimation. <em>TIP</em>, <em>29</em>, 8791–8804. (<a
href="https://doi.org/10.1109/TIP.2020.2973810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is widespread interest in estimating the fluorescence properties of natural materials in an image. However, the separation between reflected and fluoresced components is difficult, because it is impossible to distinguish reflected and fluoresced photons without controlling the illuminant spectrum. We show how to jointly estimate the reflectance and fluorescence from a single set of images acquired under multiple illuminants. We present a framework based on a linear approximation to the physical equations describing image formation in terms of surface spectral reflectance and fluorescence due to multiple fluorophores. We relax the non-convex, inverse estimation problem in order to jointly estimate the reflectance and fluorescence properties in a single optimization step. We provide a software implementation of the solver for our method and prior methods. We evaluate the accuracy and reliability of the method using both simulations and experimental data. To evaluate the methods experimentally we built a custom imaging system using a monochrome camera, a filter wheel with bandpass transmissive filters and a small number of light emitting diodes. We compared the methods based upon our framework with the ground truth as well as with prior methods.},
  archive      = {J_TIP},
  author       = {Henryk Blasinski and Joyce Farrell and Brian Wandell},
  doi          = {10.1109/TIP.2020.2973810},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8791-8804},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Simultaneous surface reflectance and fluorescence spectra estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Texture classification using pair-wise difference
pooling-based bilinear convolutional neural networks. <em>TIP</em>,
<em>29</em>, 8776–8790. (<a
href="https://doi.org/10.1109/TIP.2020.3019185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture is normally represented by aggregating local features based on the assumption of spatial homogeneity. Effective texture features are always the research focus even though both hand-crafted and deep learning approaches have been extensively investigated. Motivated by the success of Bilinear Convolutional Neural Networks (BCNNs) in fine-grained image recognition, we propose to incorporate the BCNN with the Pair-wise Difference Pooling (i.e. BCNN-PDP) for texture classification. The BCNN-PDP is built on top of a set of feature maps extracted at a convolutional layer of the pre-trained CNN. Compared with the outer product used by the original BCNN feature set, the pair-wise difference not only captures the pair-wise relationship between two sets of features but also encodes the difference between each pair of features. Considering the importance of the gradient data to the representation of image structures, we further generalise the BCNN-PDP feature set to two sets of feature maps computed from the original image and its gradient magnitude map respectively, i.e. the Fused BCNN-PDP (F-BCNN-PDP) feature set. In addition, the BCNN-PDP can be applied to two different CNNs and is referred to as the Asymmetric BCNN-PDP (A-BCNN-PDP). The three PDP-based BCNN feature sets can also be extracted at multiple scales. Since the dimensionality of the BCNN feature vectors is very high, we propose a new yet simple Block-wise PCA (BPCA) method in order to derive more compact feature vectors. The proposed methods are tested on seven different datasets along with 21 baseline feature sets. The results show that the proposed feature sets are superior, or at least comparable, to their counterparts across different datasets.},
  archive      = {J_TIP},
  author       = {Xinghui Dong and Huiyu Zhou and Junyu Dong},
  doi          = {10.1109/TIP.2020.3019185},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8776-8790},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Texture classification using pair-wise difference pooling-based bilinear convolutional neural networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). EraseNet: End-to-end text removal in the wild.
<em>TIP</em>, <em>29</em>, 8760–8775. (<a
href="https://doi.org/10.1109/TIP.2020.3018859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text removal has attracted increasing research interests owing to its valuable applications in privacy protection, camera-based virtual reality translation, and image editing. However, existing approaches, which fall short on real applications, are mainly because they were evaluated on synthetic or unrepresentative datasets. To fill this gap and facilitate this research direction, this article proposes a real-world dataset called SCUT-EnsText that consists of 3,562 diverse images selected from public scene text reading benchmarks, and each image is scrupulously annotated to provide visually plausible erasure targets. With SCUT-EnsText, we design a novel GAN-based model termed EraseNet that can automatically remove text located on the natural images. The model is a two-stage network that consists of a coarse-erasure sub-network and a refinement sub-network. The refinement sub-network targets improvement in the feature representation and refinement of the coarse outputs to enhance the removal performance. Additionally, EraseNet contains a segmentation head for text perception and a local-global SN-Patch-GAN with spectral normalization (SN) on both the generator and discriminator for maintaining the training stability and the congruity of the erased regions. A sufficient number of experiments are conducted on both the previous public dataset and the brand-new SCUT-EnsText. Our EraseNet significantly outperforms the existing state-of-the-art methods in terms of all metrics, with remarkably superior higher-quality results. The dataset and code will be made available at https://github.com/HCIILAB/SCUT-EnsText .},
  archive      = {J_TIP},
  author       = {Chongyu Liu and Yuliang Liu and Lianwen Jin and Shuaitao Zhang and Canjie Luo and Yongpan Wang},
  doi          = {10.1109/TIP.2020.3018859},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8760-8775},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {EraseNet: End-to-end text removal in the wild},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning layer-skippable inference network. <em>TIP</em>,
<em>29</em>, 8747–8759. (<a
href="https://doi.org/10.1109/TIP.2020.3018269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of learning good representations for machine learning tasks can be very computationally expensive. Typically, the model learned on training set is leveraged to infer the labels of testing data. Interestingly, this learning and inference paradigm, however, is quite different from the typical inference scheme of human biological visual systems. Essentially, neuroscience studies have shown that the right hemisphere of the human brain predominantly makes a fast processing of low-frequency spatial signals, while the left hemisphere more focuses on analyzing high-frequency information in a slower way. And the low-pass analysis helps facilitate the high-pass analysis via feedback. Inspired by this biological vision mechanism, this article explores the possibility of learning a layer-skippable inference network. Specifically, we propose a layer-skippable network that dynamically carries out coarse-to-fine object categorization. Such a network has two branches to jointly deal with both coarse and fine-grained classification tasks. The layer-skipping mechanism is proposed to learn a gating network by generating dynamic inference graphs, and reducing the computational cost by detouring the inference path from some layers. This adaptive path inference strategy endows the deep networks with dynamic structures, making the networks enjoy greater flexibility and larger capacity. To efficiently train the gating network, a novel ranking-based loss function is adopted. Furthermore, the learned representations are enhanced by the proposed top-down feedback mechanism and feature-wise affine transformation, individually. The former one employs features of a coarse branch to help the fine-grained object recognition task, while the latter one encodes the selected path to enhance the final feature representations. Extensive experiments are conducted on several widely used coarse-to-fine object categorization benchmarks, and promising results are achieved by our proposed model. Quite surprisingly, our layer-skipping mechanism improves the network robustness to adversarial attacks.},
  archive      = {J_TIP},
  author       = {Yu-Gang Jiang and Changmao Cheng and Hangyu Lin and Yanwei Fu},
  doi          = {10.1109/TIP.2020.3018269},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8747-8759},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning layer-skippable inference network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multilevel optimization for registration of deformable point
clouds. <em>TIP</em>, <em>29</em>, 8735–8746. (<a
href="https://doi.org/10.1109/TIP.2020.3019649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling deformation is one of the biggest challenges associated with point cloud registration. When deformation happens due to the motion of an animated object which actively changes its location and general shape, registration of two instances of the same object turns out to be a challenging task. The focus of this work is to address the problem by leveraging the complementary attributes of local and global geometric structures of the point clouds. We define an energy function which consists of local and global terms, as well as a semi-local term to model the intermediate level geometry of the point cloud. The local energy estimates the transformation parameters at the lowest level by assuming a reduced deformation model. The parameters are estimated in a closed form solution, which are then used to assign the initial probability of a stochastic model working at the intermediate level. The global energy term estimates the overall transformation parameters by minimizing a nonlinear least square function via Gauss-Newton optimization framework. The total energy is optimized in a block coordinate descent fashion, updating one term at a time while keeping others constant. Experiments on three publicly available datasets show that the method performs significantly better than several state-of-the-art algorithms in registering pairwise point cloud data.},
  archive      = {J_TIP},
  author       = {Ayan Chaudhury},
  doi          = {10.1109/TIP.2020.3019649},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8735-8746},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multilevel optimization for registration of deformable point clouds},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconstructing 3D shapes from multiple sketches using direct
shape optimization. <em>TIP</em>, <em>29</em>, 8721–8734. (<a
href="https://doi.org/10.1109/TIP.2020.3018865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D shape reconstruction from multiple hand-drawn sketches is an intriguing way to 3D shape modeling. Currently, state-of-the-art methods employ neural networks to learn a mapping from multiple sketches from arbitrary view angles to a 3D voxel grid. Because of the cubic complexity of 3D voxel grids, however, neural networks are hard to train and limited to low resolution reconstructions, which leads to a lack of geometric detail and low accuracy. To resolve this issue, we propose to reconstruct 3D shapes from multiple sketches using direct shape optimization (DSO), which does not involve deep learning models for direct voxel-based 3D shape generation. Specifically, we first leverage a conditional generative adversarial network (CGAN) to translate each sketch into an attenuance image that captures the predicted geometry from a given viewpoint. Then, DSO minimizes a project-and-compare loss to reconstruct the 3D shape such that it matches the predicted attenuance images from the view angles of all input sketches. Based on this, we further propose a progressive update approach to handle inconsistencies among a few hand-drawn sketches for the same 3D shape. Our experimental results show that our method significantly outperforms the state-of-the-art methods under widely used benchmarks and produces intuitive results in an interactive application.},
  archive      = {J_TIP},
  author       = {Zhizhong Han and Baorui Ma and Yu-Shen Liu and Matthias Zwicker},
  doi          = {10.1109/TIP.2020.3018865},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8721-8734},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reconstructing 3D shapes from multiple sketches using direct shape optimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RPD-GAN: Learning to draw realistic paintings with
generative adversarial network. <em>TIP</em>, <em>29</em>, 8706–8720.
(<a href="https://doi.org/10.1109/TIP.2020.3018856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Painting style transfer is an attractive and challenging computer vision problem that aims to transfer painting styles onto natural images. Existing advanced methods tackle this problem from the perspective of Neural Style Transfer (NST) or unsupervised cross-domain image translation. For both two types of methods, attention has been focused on reproducing artistic painting styles of representative artists (e.g., Vincent Van Gogh). In this article, instead of transferring styles of artistic paintings, we focus on automatic generation of realistic paintings, for example, making the machine draw a gouache before a still life, paint a sketch of a landscape, or draw a pen-and-ink portrait of a person, etc. Besides capturing the precise target styles, synthesis of realistic paintings is more demanding in preserving original content features and image structures, for which existing advanced methods are not sufficient to generate satisfactory results. Aimed at this problem, we propose RPD-GAN (Realistic Painting Drawing Generative Adversarial Network), an unsupervised cross-domain image translation framework for realistic painting style transfer. At the heart of our model is the decomposition of the image stylization mapping into four stages: feature encoding, feature de-stylization, feature re-stylization, and feature decoding, where the functionalities of these stages are implemented by additionally embedding a content-consistency constraint and a style-alignment constraint at feature space to the classic CycleGAN architecture. By enforcing these constraints, both the content-preserving and style-capturing capabilities of the model are enhanced, leading to higher-quality stylization results. Extensive experiments demonstrate the effectiveness and superiority of our RPD-GAN in drawing realistic paintings.},
  archive      = {J_TIP},
  author       = {Xiang Gao and Yingjie Tian and Zhiquan Qi},
  doi          = {10.1109/TIP.2020.3018856},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8706-8720},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RPD-GAN: Learning to draw realistic paintings with generative adversarial network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised learning of detailed 3D face reconstruction.
<em>TIP</em>, <em>29</em>, 8696–8705. (<a
href="https://doi.org/10.1109/TIP.2020.3017347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an end-to-end learning framework for detailed 3D face reconstruction from a single image. Our approach uses a 3DMM-based coarse model and a displacement map in UV-space to represent a 3D face. Unlike previous work addressing the problem, our learning framework does not require supervision of surrogate ground-truth 3D models computed with traditional approaches. Instead, we utilize the input image itself as supervision during learning. In the first stage, we combine a photometric loss and a facial perceptual loss between the input face and the rendered face, to regress a 3DMM-based coarse model. In the second stage, both the input image and the regressed texture of the coarse model are unwrapped into UV-space, and then sent through an image-to-image translation network to predict a displacement map in UV-space. The displacement map and the coarse model are used to render a final detailed face, which again can be compared with the original input image to serve as a photometric loss for the second stage. The advantage of learning displacement map in UV-space is that face alignment can be explicitly done during the unwrapping, thus facial details are easier to learn from large amount of data. Extensive experiments demonstrate the superiority of our method over previous work.},
  archive      = {J_TIP},
  author       = {Yajing Chen and Fanzi Wu and Zeyu Wang and Yibing Song and Yonggen Ling and Linchao Bao},
  doi          = {10.1109/TIP.2020.3017347},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8696-8705},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised learning of detailed 3D face reconstruction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video coding for machines: A paradigm of collaborative
compression and intelligent analytics. <em>TIP</em>, <em>29</em>,
8680–8695. (<a href="https://doi.org/10.1109/TIP.2020.3016485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video coding, which targets to compress and reconstruct the whole frame, and feature compression, which only preserves and transmits the most critical information, stand at two ends of the scale. That is, one is with compactness and efficiency to serve for machine vision, and the other is with full fidelity, bowing to human perception. The recent endeavors in imminent trends of video compression, e.g. deep learning based coding tools and end-to-end image/video coding, and MPEG-7 compact feature descriptor standards, i.e. Compact Descriptors for Visual Search and Compact Descriptors for Video Analysis, promote the sustainable and fast development in their own directions, respectively. In this article, thanks to booming AI technology, e.g. prediction and generation models, we carry out exploration in the new area, Video Coding for Machines (VCM), arising from the emerging MPEG standardization efforts. 1 Towards collaborative compression and intelligent analytics, VCM attempts to bridge the gap between feature coding for machine vision and video coding for human vision. Aligning with the rising Analyze then Compress instance Digital Retina, the definition, formulation, and paradigm of VCM are given first. Meanwhile, we systematically review state-of-the-art techniques in video compression and feature compression from the unique perspective of MPEG standardization, which provides the academic and industrial evidence to realize the collaborative compression of video and feature streams in a broad range of AI applications. Finally, we come up with potential VCM solutions, and the preliminary results have demonstrated the performance and efficiency gains. Further direction is discussed as well.},
  archive      = {J_TIP},
  author       = {Lingyu Duan and Jiaying Liu and Wenhan Yang and Tiejun Huang and Wen Gao},
  doi          = {10.1109/TIP.2020.3016485},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8680-8695},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video coding for machines: A paradigm of collaborative compression and intelligent analytics},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An α-matte boundary defocus model-based cascaded network for
multi-focus image fusion. <em>TIP</em>, <em>29</em>, 8668–8679. (<a
href="https://doi.org/10.1109/TIP.2020.3018261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing an all-in-focus image with a single camera is difficult since the depth of field of the camera is usually limited. An alternative method to obtain the all-in-focus image is to fuse several images that are focused at different depths. However, existing multi-focus image fusion methods cannot obtain clear results for areas near the focused/defocused boundary (FDB). In this article, a novel α-matte boundary defocus model is proposed to generate realistic training data with the defocus spread effect precisely modeled, especially for areas near the FDB. Based on this α-matte defocus model and the generated data, a cascaded boundary-aware convolutional network termed MMF-Net is proposed and trained, aiming to achieve clearer fusion results around the FDB. Specifically, the MMF-Net consists of two cascaded subnets for initial fusion and boundary fusion. These two subnets are designed to first obtain a guidance map of FDB and then refine the fusion near the FDB. Experiments demonstrate that with the help of the new α-matte boundary defocus model, the proposed MMF-Net outperforms the state-of-the-art methods both qualitatively and quantitatively.},
  archive      = {J_TIP},
  author       = {Haoyu Ma and Qingmin Liao and Juncheng Zhang and Shaojun Liu and Jing-Hao Xue},
  doi          = {10.1109/TIP.2020.3018261},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8668-8679},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An α-matte boundary defocus model-based cascaded network for multi-focus image fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic feature integration for simultaneous detection of
salient object, edge, and skeleton. <em>TIP</em>, <em>29</em>,
8652–8667. (<a href="https://doi.org/10.1109/TIP.2020.3017352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object segmentation, edge detection, and skeleton extraction are three contrasting low-level pixel-wise vision problems, where existing works mostly focused on designing tailored methods for each individual task. However, it is inconvenient and inefficient to store a pre-trained model for each task and perform multiple different tasks in sequence. There are methods that solve specific related tasks jointly but require datasets with different types of annotations supported at the same time. In this article, we first show some similarities shared by these tasks and then demonstrate how they can be leveraged for developing a unified framework that can be trained end-to-end. In particular, we introduce a selective integration module that allows each task to dynamically choose features at different levels from the shared backbone based on its own characteristics. Furthermore, we design a task-adaptive attention module, aiming at intelligently allocating information for different tasks according to the image content priors. To evaluate the performance of our proposed network on these tasks, we conduct exhaustive experiments on multiple representative datasets. We will show that though these tasks are naturally quite different, our network can work well on all of them and even perform better than current single-purpose state-of-the-art methods. In addition, we also conduct adequate ablation analyses that provide a full understanding of the design principles of the proposed framework.},
  archive      = {J_TIP},
  author       = {Jiang-Jiang Liu and Qibin Hou and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2020.3017352},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8652-8667},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic feature integration for simultaneous detection of salient object, edge, and skeleton},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Screen content video quality assessment: Subjective and
objective study. <em>TIP</em>, <em>29</em>, 8636–8651. (<a
href="https://doi.org/10.1109/TIP.2020.3018256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we make the first attempt to study the subjective and objective quality assessment for the screen content videos (SCVs). For that, we construct the first large-scale video quality assessment (VQA) database specifically for the SCVs, called the screen content video database (SCVD). This SCVD provides 16 reference SCVs, 800 distorted SCVs, and their corresponding subjective scores, and it is made publicly available for research usage. The distorted SCVs are generated from each reference SCV with 10 distortion types and 5 degradation levels for each distortion type. Each distorted SCV is rated by at least 32 subjects in the subjective test. Furthermore, we propose the first full-reference VQA model for the SCVs, called the spatiotemporal Gabor feature tensor-based model (SGFTM), to objectively evaluate the perceptual quality of the distorted SCVs. This is motivated by the observation that 3D-Gabor filter can well stimulate the visual functions of the human visual system (HVS) on perceiving videos, being more sensitive to the edge and motion information that are often-encountered in the SCVs. Specifically, the proposed SGFTM exploits 3D-Gabor filter to individually extract the spatiotemporal Gabor feature tensors from the reference and distorted SCVs, followed by measuring their similarities and later combining them together through the developed spatiotemporal feature tensor pooling strategy to obtain the final SGFTM score. Experimental results on SCVD have shown that the proposed SGFTM yields a high consistency on the subjective perception of SCV quality and consistently outperforms multiple classical and state-of-the-art image/video quality assessment models.},
  archive      = {J_TIP},
  author       = {Shan Cheng and Huanqiang Zeng and Jing Chen and Junhui Hou and Jianqing Zhu and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2020.3018256},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8636-8651},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Screen content video quality assessment: Subjective and objective study},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep spatial transformation for pose-guided person image
generation and animation. <em>TIP</em>, <em>29</em>, 8622–8635. (<a
href="https://doi.org/10.1109/TIP.2020.3018224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose-guided person image generation and animation aim to transform a source person image to target poses. These tasks require spatial manipulation of source data. However, Convolutional Neural Networks are limited by the lack of ability to spatially transform the inputs. In this article, we propose a differentiable global-flow local-attention framework to reassemble the inputs at the feature level. This framework first estimates global flow fields between sources and targets. Then, corresponding local source feature patches are sampled with content-aware local attention coefficients. We show that our framework can spatially transform the inputs in an efficient manner. Meanwhile, we further model the temporal consistency for the person image animation task to generate coherent videos. The experiment results of both image generation and animation tasks demonstrate the superiority of our model. Besides, additional results of novel view synthesis and face image animation show that our model is applicable to other tasks requiring spatial transformation. The source code of our project is available at https://github.com/RenYurui/Global-Flow-Local-Attention .},
  archive      = {J_TIP},
  author       = {Yurui Ren and Ge Li and Shan Liu and Thomas H. Li},
  doi          = {10.1109/TIP.2020.3018224},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8622-8635},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep spatial transformation for pose-guided person image generation and animation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object discovery from a single unlabeled image by mining
frequent itemsets with multi-scale features. <em>TIP</em>, <em>29</em>,
8606–8621. (<a href="https://doi.org/10.1109/TIP.2020.3015543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of our work is to discover dominant objects in a very general setting where only a single unlabeled image is given. This is far more challenge than typical co-localization or weakly-supervised localization tasks. To tackle this problem, we propose a simple but effective pattern mining-based method, called Object Location Mining (OLM), which exploits the advantages of data mining and feature representation of pre-trained convolutional neural networks (CNNs). Specifically, we first convert the feature maps from a pre-trained CNN model into a set of transactions, and then discovers frequent patterns from transaction database through pattern mining techniques. We observe that those discovered patterns, i.e., co-occurrence highlighted regions, typically hold appearance and spatial consistency. Motivated by this observation, we can easily discover and localize possible objects by merging relevant meaningful patterns. Extensive experiments on a variety of benchmarks demonstrate that OLM achieves competitive localization performance compared with the state-of-the-art methods. We also evaluate our approach compared with unsupervised saliency detection methods and achieves competitive results on seven benchmark datasets. Moreover, we conduct experiments on fine-grained classification to show that our proposed method can locate the entire object and parts accurately, which can benefit to improving the classification results significantly.},
  archive      = {J_TIP},
  author       = {Runsheng Zhang and Yaping Huang and Mengyang Pu and Jian Zhang and Qingji Guan and Qi Zou and Haibin Ling},
  doi          = {10.1109/TIP.2020.3015543},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8606-8621},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Object discovery from a single unlabeled image by mining frequent itemsets with multi-scale features},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Revealing the invisible with model and data shrinking for
composite-database micro-expression recognition. <em>TIP</em>,
<em>29</em>, 8590–8605. (<a
href="https://doi.org/10.1109/TIP.2020.3018222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite-database micro-expression recognition is attracting increasing attention as it is more practical for real-world applications. Though the composite database provides more sample diversity for learning good representation models, the important subtle dynamics are prone to disappearing in the domain shift such that the models greatly degrade their performance, especially for deep models. In this article, we analyze the influence of learning complexity, including input complexity and model complexity, and discover that the lower-resolution input data and shallower-architecture model are helpful to ease the degradation of deep models in composite-database task. Based on this, we propose a recurrent convolutional network (RCN) to explore the shallower-architecture and lower-resolution input data, shrinking model and input complexities simultaneously. Furthermore, we develop three parameter-free modules (i.e., wide expansion, shortcut connection and attention unit) to integrate with RCN without increasing any learnable parameters. These three modules can enhance the representation ability in various perspectives while preserving not-very-deep architecture for lower-resolution data. Besides, three modules can further be combined by an automatic strategy (a neural architecture search strategy) and the searched architecture becomes more robust. Extensive experiments on the MEGC2019 dataset (composited of existing SMIC, CASME II and SAMM datasets) have verified the influence of learning complexity and shown that RCNs with three modules and the searched combination outperform the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Zhaoqiang Xia and Wei Peng and Huai-Qian Khor and Xiaoyi Feng and Guoying Zhao},
  doi          = {10.1109/TIP.2020.3018222},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8590-8605},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revealing the invisible with model and data shrinking for composite-database micro-expression recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Restoration of fast moving objects. <em>TIP</em>,
<em>29</em>, 8577–8589. (<a
href="https://doi.org/10.1109/TIP.2020.3016490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If an object is photographed at motion in front of a static background, the object will be blurred while the background sharp and partially occluded by the object. The goal is to recover the object appearance from such blurred image. We adopt the image formation model for fast moving objects and consider objects undergoing 2D translation and rotation. For this scenario we formulate the estimation of the object shape, appearance, and motion from a single image and known background as a constrained optimization problem with appropriate regularization terms. Both similarities and differences with blind deconvolution are discussed with the latter caused mainly by the coupling of the object appearance and shape in the acquisition model. Necessary conditions for solution uniqueness are derived and a numerical solution based on the alternating direction method of multipliers is presented. The proposed method is evaluated on a new dataset.},
  archive      = {J_TIP},
  author       = {Jan Kotera and Jiří Matas and Filip Šroubek},
  doi          = {10.1109/TIP.2020.3016490},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8577-8589},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Restoration of fast moving objects},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image restoration via simultaneous nonlocal self-similarity
priors. <em>TIP</em>, <em>29</em>, 8561–8576. (<a
href="https://doi.org/10.1109/TIP.2020.3015545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through exploiting the image nonlocal self-similarity (NSS) prior by clustering similar patches to construct patch groups, recent studies have revealed that structural sparse representation (SSR) models can achieve promising performance in various image restoration tasks. However, most existing SSR methods only exploit the NSS prior from the input degraded (internal) image, and few methods utilize the NSS prior from external clean image corpus; how to jointly exploit the NSS priors of internal image and external clean image corpus is still an open problem. In this article, we propose a novel approach for image restoration by simultaneously considering internal and external nonlocal self-similarity (SNSS) priors that offer mutually complementary information. Specifically, we first group nonlocal similar patches from images of a training corpus. Then a group-based Gaussian mixture model (GMM) learning algorithm is applied to learn an external NSS prior. We exploit the SSR model by integrating the NSS priors of both internal and external image data. An alternating minimization with an adaptive parameter adjusting strategy is developed to solve the proposed SNSS-based image restoration problems, which makes the entire algorithm more stable and practical. Experimental results on three image restoration applications, namely image denoising, deblocking and deblurring, demonstrate that the proposed SNSS produces superior results compared to many popular or state-of-the-art methods in both objective and perceptual quality measurements.},
  archive      = {J_TIP},
  author       = {Zhiyuan Zha and Xin Yuan and Jiantao Zhou and Ce Zhu and Bihan Wen},
  doi          = {10.1109/TIP.2020.3015545},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8561-8576},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image restoration via simultaneous nonlocal self-similarity priors},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised agent learning for unsupervised cross-domain
person re-identification. <em>TIP</em>, <em>29</em>, 8549–8560. (<a
href="https://doi.org/10.1109/TIP.2020.3016869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (Re-ID) has better scalability and practicability than supervised Re-ID in the actual deployment. However, it is difficult to learn a discriminative Re-ID model without annotations. To address the above issue, we propose an end-to-end Self-supervised Agent Learning (SAL) algorithm by exploiting a set of agents as a bridge to reduce domain gaps for unsupervised cross-domain person Re-ID. The proposed SAL model enjoys several merits. First, to the best of our knowledge, this is the first work to exploit self-supervised learning for unsupervised person Re-ID. Second, our model has designed three effective learning mechanisms including supervised label learning in source domain, similarity consistency learning in target domain, and self-supervised learning in cross domain, which can learn domain-invariant yet discriminative representations through the principled lens of agent learning by reducing domain discrepancy adaptively. Extensive experimental results on three standard benchmarks demonstrate that the proposed SAL performs favorably against state-of-the-art unsupervised person Re-ID methods.},
  archive      = {J_TIP},
  author       = {Kongzhu Jiang and Tianzhu Zhang and Yongdong Zhang and Feng Wu and Yong Rui},
  doi          = {10.1109/TIP.2020.3016869},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8549-8560},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised agent learning for unsupervised cross-domain person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Revisiting anchor mechanisms for temporal action
localization. <em>TIP</em>, <em>29</em>, 8535–8548. (<a
href="https://doi.org/10.1109/TIP.2020.3016486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the current action localization methods follow an anchor-based pipeline: depicting action instances by pre-defined anchors, learning to select the anchors closest to the ground truth, and predicting the confidence of anchors with refinements. Pre-defined anchors set prior about the location and duration for action instances, which facilitates the localization for common action instances but limits the flexibility for tackling action instances with drastic varieties, especially for extremely short or extremely long ones. To address this problem, this paper proposes a novel anchor-free action localization module that assists action localization by temporal points. Specifically, this module represents an action instance as a point with its distances to the starting boundary and ending boundary, alleviating the pre-defined anchor restrictions in terms of action localization and duration. The proposed anchor-free module is capable of predicting the action instances whose duration is either extremely short or extremely long. By combining the proposed anchor-free module with a conventional anchor-based module, we propose a novel action localization framework, called A2Net. The cooperation between anchor-free and anchor-based modules achieves superior performance to the state-of-the-art on THUMOS14 (45.5\% vs. 42.8\%). Furthermore, comprehensive experiments demonstrate the complementarity between the anchor-free and the anchor-based module, making A2Net simple but effective.},
  archive      = {J_TIP},
  author       = {Le Yang and Houwen Peng and Dingwen Zhang and Jianlong Fu and Junwei Han},
  doi          = {10.1109/TIP.2020.3016486},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8535-8548},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting anchor mechanisms for temporal action localization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Universal face photo-sketch style transfer via multiview
domain translation. <em>TIP</em>, <em>29</em>, 8519–8534. (<a
href="https://doi.org/10.1109/TIP.2020.3016502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face photo-sketch style transfer aims to convert a representation of a face from the photo (or sketch) domain to the sketch (respectively, photo) domain while preserving the character of the subject. It has wide-ranging applications in law enforcement, forensic investigation and digital entertainment. However, conventional face photo-sketch synthesis methods usually require training images from both the source domain and the target domain, and are limited in that they cannot be applied to universal conditions where collecting training images in the source domain that match the style of the test image is unpractical. This problem entails two major challenges: 1) designing an effective and robust domain translation model for the universal situation in which images of the source domain needed for training are unavailable, and 2) preserving the facial character while performing a transfer to the style of an entire image collection in the target domain. To this end, we present a novel universal face photo-sketch style transfer method that does not need any image from the source domain for training. The regression relationship between an input test image and the entire training image collection in the target domain is inferred via a deep domain translation framework, in which a domain-wise adaption term and a local consistency adaption term are developed. To improve the robustness of the style transfer process, we propose a multiview domain translation method that flexibly leverages a convolutional neural network representation with hand-crafted features in an optimal way. Qualitative and quantitative comparisons are provided for universal unconstrained conditions of unavailable training images from the source domain, demonstrating the effectiveness and superiority of our method for universal face photo-sketch style transfer.},
  archive      = {J_TIP},
  author       = {Chunlei Peng and Nannan Wang and Jie Li and Xinbo Gao},
  doi          = {10.1109/TIP.2020.3016502},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8519-8534},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Universal face photo-sketch style transfer via multiview domain translation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian adversarial spectral clustering with unknown
cluster number. <em>TIP</em>, <em>29</em>, 8506–8518. (<a
href="https://doi.org/10.1109/TIP.2020.3016491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is a popular tool in many unsupervised computer vision and machine learning tasks. Recently, due to the encouraging performance of deep neural networks, many conventional spectral clustering methods have been extended to the deep framework. Although these deep spectral clustering methods are quite powerful and effective, learning the cluster number from data is still a challenge. In this article, we aim to tackle this problem by integrating the spectral clustering, generative adversarial network and low rank model within a unified Bayesian framework. First, we adapt the low rank method to the cluster number estimation problem. Then, an adversarial-learning-based deep clustering method is proposed and incorporated. When introducing the spectral clustering method into our model clustering procedure, a hidden space structure preservation term is proposed. Via a Bayesian framework, the structure preservation term is embedded into the generative process, which can then be used to deduce a spectral clustering in the optimization procedure. Finally, we derive a variational-inference-based method and embed it into the network optimization and learning procedure. Experiments on different datasets prove that our model has the cluster number estimation capability and show that our method can outperform many similar graph clustering methods.},
  archive      = {J_TIP},
  author       = {Xulun Ye and Jieyu Zhao and Yu Chen and Li-Jun Guo},
  doi          = {10.1109/TIP.2020.3016491},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8506-8518},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bayesian adversarial spectral clustering with unknown cluster number},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual saliency via embedding hierarchical knowledge in a
deep neural network. <em>TIP</em>, <em>29</em>, 8490–8505. (<a
href="https://doi.org/10.1109/TIP.2020.3016464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been extensively applied in image processing, including visual saliency map prediction of images. A major difficulty in using a DNN for visual saliency prediction is the lack of labeled ground truth of visual saliency. A powerful DNN usually contains a large number of trainable parameters. This condition can easily lead to model over-fitting. In this study, we develop a novel method that overcomes such difficulty by embedding hierarchical knowledge of existing visual saliency models in a DNN. We achieve the objective of exploiting the knowledge contained in the existing visual saliency models by using saliency maps generated by local, global, and semantic models to tune and fix about 92.5\% of the parameters in our network in a hierarchical manner. As a result, the number of trainable parameters that need to be tuned by the ground truth is considerably reduced. This reduction enables us to fully utilize the power of a large DNN and overcome the issue of over-fitting at the same time. Furthermore, we introduce a simple but very effective center prior in designing the learning cost function of the DNN by attaching high importance to the errors around the image center. We also present extensive experimental results on four commonly used public databases to demonstrate the superiority of the proposed method over classical and state-of-the-art methods on various evaluation metrics.},
  archive      = {J_TIP},
  author       = {Fei Zhou and Rongguo Yao and Guangsen Liao and Bozhi Liu and Guoping Qiu},
  doi          = {10.1109/TIP.2020.3016464},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8490-8505},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visual saliency via embedding hierarchical knowledge in a deep neural network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate long-term multiple people tracking using video and
body-worn IMUs. <em>TIP</em>, <em>29</em>, 8476–8489. (<a
href="https://doi.org/10.1109/TIP.2020.3013801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern approaches for video-based multiple people tracking rely on human appearance to exploit similarities between person detections. Consequently, tracking accuracy degrades if this kind of information is not discriminative or if people change apparel. In contrast, we present a method to fuse video information with additional motion signals from body-worn inertial measurement units (IMUs). In particular, we propose a neural network to relate person detections with IMU orientations, and formulate a graph labeling problem to obtain a tracking solution that is globally consistent with the video and inertial recordings. The fusion of visual and inertial cues provides several advantages. The association of detection boxes in the video and IMU devices is based on motion, which is independent of a person&#39;s outward appearance. Furthermore, inertial sensors provide motion information irrespective of visual occlusions. Hence, once detections in the video are associated with an IMU device, intermediate positions can be reconstructed from corresponding inertial sensor data, which would be unstable using video only. Since no dataset exists for this new setting, we release a dataset of challenging tracking sequences, containing video and IMU recordings together with ground-truth annotations. We evaluate our approach on our new dataset, achieving an average IDF1 score of 91.2\%. The proposed method is applicable to any situation that allows one to equip people with inertial sensors.},
  archive      = {J_TIP},
  author       = {Roberto Henschel and Timo Von Marcard and Bodo Rosenhahn},
  doi          = {10.1109/TIP.2020.3013801},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8476-8489},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Accurate long-term multiple people tracking using video and body-worn IMUs},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient fire detection method based on multiscale
feature extraction, implicit deep supervision and channel attention
mechanism. <em>TIP</em>, <em>29</em>, 8467–8475. (<a
href="https://doi.org/10.1109/TIP.2020.3016431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in vision-based fire detection is driven by convolutional neural networks. However, the existing methods fail to achieve a good tradeoff among accuracy, model size, and speed. In this paper, we propose an accurate fire detection method that achieves a better balance in the abovementioned aspects. Specifically, a multiscale feature extraction mechanism is employed to capture richer spatial details, which can enhance the discriminative ability of fire-like objects. Then, the implicit deep supervision mechanism is utilized to enhance the interaction among information flows through dense skip connections. Finally, a channel attention mechanism is employed to selectively emphasize the contribution between different feature maps. Experimental results demonstrate that our method achieves 95.3\% accuracy, which outperforms the suboptimal method by 2.5\%. Moreover, the speed and model size of our method are 3.76\% faster on the GPU and 63.64\% smaller than the suboptimal method, respectively.},
  archive      = {J_TIP},
  author       = {Songbin Li and Qiandong Yan and Peng Liu},
  doi          = {10.1109/TIP.2020.3016431},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8467-8475},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An efficient fire detection method based on multiscale feature extraction, implicit deep supervision and channel attention mechanism},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-shot image dehazing. <em>TIP</em>, <em>29</em>,
8457–8466. (<a href="https://doi.org/10.1109/TIP.2020.3016134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study two less-touched challenging problems in single image dehazing neural networks, namely, how to remove haze from a given image in an unsupervised and zero-shot manner. To the ends, we propose a novel method based on the idea of layer disentanglement by viewing a hazy image as the entanglement of several “simpler” layers, i.e., a hazy-free image layer, transmission map layer, and atmospheric light layer. The major advantages of the proposed ZID are two-fold. First, it is an unsupervised method that does not use any clean images including hazy-clean pairs as the ground-truth. Second, ZID is a “zero-shot” method, which just uses the observed single hazy image to perform learning and inference. In other words, it does not follow the conventional paradigm of training deep model on a large scale dataset. These two advantages enable our method to avoid the labor-intensive data collection and the domain shift issue of using the synthetic hazy images to address the real-world images. Extensive comparisons show the promising performance of our method compared with 15 approaches in the qualitative and quantitive evaluations. The source code could be found at http://www.pengxi.me.},
  archive      = {J_TIP},
  author       = {Boyun Li and Yuanbiao Gou and Jerry Zitao Liu and Hongyuan Zhu and Joey Tianyi Zhou and Xi Peng},
  doi          = {10.1109/TIP.2020.3016134},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8457-8466},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-shot image dehazing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal deep unfolding for guided image super-resolution.
<em>TIP</em>, <em>29</em>, 8443–8456. (<a
href="https://doi.org/10.1109/TIP.2020.3014729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction of a high resolution image given a low resolution observation is an ill-posed inverse problem in imaging. Deep learning methods rely on training data to learn an end-to-end mapping from a low-resolution input to a high-resolution output. Unlike existing deep multimodal models that do not incorporate domain knowledge about the problem, we propose a multimodal deep learning design that incorporates sparse priors and allows the effective integration of information from another image modality into the network architecture. Our solution relies on a novel deep unfolding operator, performing steps similar to an iterative algorithm for convolutional sparse coding with side information; therefore, the proposed neural network is interpretable by design. The deep unfolding architecture is used as a core component of a multimodal framework for guided image super-resolution. An alternative multimodal design is investigated by employing residual learning to improve the training efficiency. The presented multimodal approach is applied to super-resolution of near-infrared and multi-spectral images as well as depth upsampling using RGB images as side information. Experimental results show that our model outperforms state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Iman Marivani and Evaggelia Tsiligianni and Bruno Cornelis and Nikos Deligiannis},
  doi          = {10.1109/TIP.2020.3014729},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8443-8456},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multimodal deep unfolding for guided image super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised learning of optical flow with CNN-based
non-local filtering. <em>TIP</em>, <em>29</em>, 8429–8442. (<a
href="https://doi.org/10.1109/TIP.2020.3013168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating optical flow from successive video frames is one of the fundamental problems in computer vision and image processing. In the era of deep learning, many methods have been proposed to use convolutional neural networks (CNNs) for optical flow estimation in an unsupervised manner. However, the performance of unsupervised optical flow approaches is still unsatisfactory and often lagging far behind their supervised counterparts, primarily due to over-smoothing across motion boundaries and occlusion. To address these issues, in this paper, we propose a novel method with a new post-processing term and an effective loss function to estimate optical flow in an unsupervised, end-to-end learning manner. Specifically, we first exploit a CNN-based non-local term to refine the estimated optical flow by removing noise and decreasing blur around motion boundaries. This is implemented via automatically learning weights of dependencies over a large spatial neighborhood. Because of its learning ability, the method is effective for various complicated image sequences. Secondly, to reduce the influence of occlusion, a symmetrical energy formulation is introduced to detect the occlusion map from refined bi-directional optical flows. Then the occlusion map is integrated to the loss function. Extensive experiments are conducted on challenging datasets, i.e. FlyingChairs, MPI-Sintel and KITTI to evaluate the performance of the proposed method. The state-of-the-art results demonstrate the effectiveness of our proposed method.},
  archive      = {J_TIP},
  author       = {Long Tian and Zhigang Tu and Dejun Zhang and Jun Liu and Baoxin Li and Junsong Yuan},
  doi          = {10.1109/TIP.2020.3013168},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8429-8442},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised learning of optical flow with CNN-based non-local filtering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical u-shape attention network for salient object
detection. <em>TIP</em>, <em>29</em>, 8417–8428. (<a
href="https://doi.org/10.1109/TIP.2020.3011554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection aims at locating the most conspicuous objects in natural images, which usually acts as a very important pre-processing procedure in many computer vision tasks. In this paper, we propose a simple yet effective Hierarchical U-shape Attention Network (HUAN) to learn a robust mapping function for salient object detection. Firstly, a novel attention mechanism is formulated to improve the well-known U-shape network, in which the memory consumption can be extensively reduced and the mask quality can be significantly improved by the resulting U-shape Attention Network (UAN). Secondly, a novel hierarchical structure is constructed to well bridge the low-level and high-level feature representations between different UANs, in which both the intra-network and inter-network connections are considered to explore the salient patterns from a local to global view. Thirdly, a novel Mask Fusion Network (MFN) is designed to fuse the intermediate prediction results, so as to generate a salient mask which is in higher-quality than any of those inputs. Our HUAN can be trained together with any backbone network in an end-to-end manner, and high-quality masks can be finally learned to represent the salient objects. Extensive experimental results on several benchmark datasets show that our method significantly outperforms most of the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Sanping Zhou and Jinjun Wang and Jimuyang Zhang and Le Wang and Dong Huang and Shaoyi Du and Nanning Zheng},
  doi          = {10.1109/TIP.2020.3011554},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8417-8428},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical U-shape attention network for salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RGBD salient object detection via disentangled cross-modal
fusion. <em>TIP</em>, <em>29</em>, 8407–8416. (<a
href="https://doi.org/10.1109/TIP.2020.3014734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth is beneficial for salient object detection (SOD) for its additional saliency cues. Existing RGBD SOD methods focus on tailoring complicated cross-modal fusion topologies, which although achieve encouraging performance, are with a high risk of over-fitting and ambiguous in studying cross-modal complementarity. Different from these conventional approaches combining cross-modal features entirely without differentiating, we concentrate our attention on decoupling the diverse cross-modal complements to simplify the fusion process and enhance the fusion sufficiency. We argue that if cross-modal heterogeneous representations can be disentangled explicitly, the cross-modal fusion process can hold less uncertainty, while enjoying better adaptability. To this end, we design a disentangled cross-modal fusion network to expose structural and content representations from both modalities by cross-modal reconstruction. For different scenes, the disentangled representations allow the fusion module to easily identify and incorporate desired complements for informative multi-modal fusion. Extensive experiments show the effectiveness of our designs and a large outperformance over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Hao Chen and Yongjian Deng and Youfu Li and Tzu-Yi Hung and Guosheng Lin},
  doi          = {10.1109/TIP.2020.3014734},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8407-8416},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RGBD salient object detection via disentangled cross-modal fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised deep cross-modality spectral hashing.
<em>TIP</em>, <em>29</em>, 8391–8406. (<a
href="https://doi.org/10.1109/TIP.2020.3014727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel framework, namely Deep Cross-modality Spectral Hashing (DCSH), to tackle the unsupervised learning problem of binary hash codes for efficient cross-modal retrieval. The framework is a two-step hashing approach which decouples the optimization into (1) binary optimization and (2) hashing function learning. In the first step, we propose a novel spectral embedding-based algorithm to simultaneously learn single-modality and binary cross-modality representations. While the former is capable of well preserving the local structure of each modality, the latter reveals the hidden patterns from all modalities. In the second step, to learn mapping functions from informative data inputs (images and word embeddings) to binary codes obtained from the first step, we leverage the powerful CNN for images and propose a CNN-based deep architecture to learn text modality. Quantitative evaluations on three standard benchmark datasets demonstrate that the proposed DCSH method consistently outperforms other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Tuan Hoang and Thanh-Toan Do and Tam V. Nguyen and Ngai-Man Cheung},
  doi          = {10.1109/TIP.2020.3014727},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8391-8406},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised deep cross-modality spectral hashing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hamming embedding sensitivity guided fusion network for 3D
shape representation. <em>TIP</em>, <em>29</em>, 8381–8390. (<a
href="https://doi.org/10.1109/TIP.2020.3013138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional multi-modal data are used to represent 3D objects in the real world in different ways. Features separately extracted from multimodality data are often poorly correlated. Recent solutions leveraging the attention mechanism to learn a joint-network for the fusion of multimodality features have weak generalization capability. In this paper, we propose a hamming embedding sensitivity network to address the problem of effectively fusing multimodality features. The proposed network called HamNet is the first end-to-end framework with the capacity to theoretically integrate data from all modalities with a unified architecture for 3D shape representation, which can be used for 3D shape retrieval and recognition. HamNet uses the feature concealment module to achieve effective deep feature fusion. The basic idea of the concealment module is to re-weight the features from each modality at an early stage with the hamming embedding of these modalities. The hamming embedding also provides an effective solution for fast retrieval tasks on a large scale dataset. We have evaluated the proposed method on the large-scale ModelNet40 dataset for the tasks of 3D shape classification, single modality and cross-modality retrieval. Comprehensive experiments and comparisons with state-of-the-art methods demonstrate that the proposed approach can achieve superior performance.},
  archive      = {J_TIP},
  author       = {Biao Gong and Chenggang Yan and Junjie Bai and Changqing Zou and Yue Gao},
  doi          = {10.1109/TIP.2020.3013138},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8381-8390},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hamming embedding sensitivity guided fusion network for 3D shape representation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). S-LWSR: Super lightweight super-resolution network.
<em>TIP</em>, <em>29</em>, 8368–8380. (<a
href="https://doi.org/10.1109/TIP.2020.3014953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep models have achieved great success in the field of single-image super-resolution (SISR) by incorporating a large number of parameters to obtain satisfactory performance. However, this achievement typically gives rise to high computational complexity, which greatly restricts deep SISR applications in deployment on mobile devices with limited computation and storage resources. To address this problem, in this article, we propose a flexibly adjustable super-lightweight SISR pipeline: s-LWSR. First, to efficiently abstract features from low-resolution images, we design a highly efficient U-shaped backbone, along with an information pool, which is constructed to mix multilevel information from the first half of our pipeline. Second, a compression mechanism based on depthwise-separable convolution is employed to further reduce the number of parameters with a negligible degradation in performance. Third, by revealing the specific role of activation in many deep models, we remove several activation layers in our super-resolution (SR) model to retain useful information, leading to a further improvement in the final performance. Extensive experiments demonstrate that our s-LWSR, with limited parameters and operations, can achieve a similar performance to that of other cumbersome but state-of-the-art (SOTA) deep SR methods.},
  archive      = {J_TIP},
  author       = {Biao Li and Bo Wang and Jiabin Liu and Zhiquan Qi and Yong Shi},
  doi          = {10.1109/TIP.2020.3014953},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8368-8380},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {S-LWSR: Super lightweight super-resolution network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual tracking with multiview trajectory prediction.
<em>TIP</em>, <em>29</em>, 8355–8367. (<a
href="https://doi.org/10.1109/TIP.2020.3014952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progresses in visual tracking have greatly improved the tracking performance. However, challenges such as occlusion and view change remain obstacles in real world deployment. A natural solution to these challenges is to use multiple cameras with multiview inputs, though existing systems are mostly limited to specific targets (e.g. human), static cameras, and/or require camera calibration. To break through these limitations, we propose a generic multiview tracking (GMT) framework that allows camera movement, while requiring neither specific object model nor camera calibration. A key innovation in our framework is a cross-camera trajectory prediction network (TPN), which implicitly and dynamically encodes camera geometric relations, and hence addresses missing target issues such as occlusion. Moreover, during tracking, we assemble information across different cameras to dynamically update a novel collaborative correlation filter (CCF), which is shared among cameras to achieve robustness against view change. The two components are integrated into a correlation filter tracking framework, where features are trained offline using existing single view tracking datasets. For evaluation, we first contribute a new generic multiview tracking dataset (GMTD) with careful annotations, and then run experiments on the GMTD and CAMPUS datasets. The proposed GMT algorithm shows clear advantages in terms of robustness over state-of-the-art ones.},
  archive      = {J_TIP},
  author       = {Minye Wu and Haibin Ling and Ning Bi and Shenghua Gao and Qiang Hu and Hao Sheng and Jingyi Yu},
  doi          = {10.1109/TIP.2020.3014952},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8355-8367},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visual tracking with multiview trajectory prediction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative filtering of correlated noise: Exact
transform-domain variance for improved shrinkage and patch matching.
<em>TIP</em>, <em>29</em>, 8339–8354. (<a
href="https://doi.org/10.1109/TIP.2020.3014721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filters perform denoising through transform-domain shrinkage of a group of similar patches extracted from an image. Existing collaborative filters of stationary correlated noise have all used simple approximations of the transform noise power spectrum adopted from methods which do not employ patch grouping and instead operate on a single patch. We note the inaccuracies of these approximations and introduce a method for the exact computation of the noise power spectrum. Unlike earlier methods, the calculated noise variances are exact even when noise in one patch is correlated with noise in any of the other patches. We discuss the adoption of the exact noise power spectrum within shrinkage, in similarity testing (patch matching), and in aggregation. We also introduce effective approximations of the spectrum for faster computation. Extensive experiments support the proposed method over earlier crude approximations used by image denoising filters such as Block-Matching and 3D-filtering (BM3D), demonstrating dramatic improvement in many challenging conditions.},
  archive      = {J_TIP},
  author       = {Ymir Mäkinen and Lucio Azzari and Alessandro Foi},
  doi          = {10.1109/TIP.2020.3014721},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8339-8354},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Collaborative filtering of correlated noise: Exact transform-domain variance for improved shrinkage and patch matching},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MATNet: Motion-attentive transition network for zero-shot
video object segmentation. <em>TIP</em>, <em>29</em>, 8326–8338. (<a
href="https://doi.org/10.1109/TIP.2020.3013162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel end-to-end learning neural network, i.e., MATNet, for zero-shot video object segmentation (ZVOS). Motivated by the human visual attention behavior, MATNet leverages motion cues as a bottom-up signal to guide the perception of object appearance. To achieve this, an asymmetric attention block, named Motion-Attentive Transition (MAT), is proposed within a two-stream encoder network to firstly identify moving regions and then attend appearance learning to capture the full extent of objects. Putting MATs in different convolutional layers, our encoder becomes deeply interleaved, allowing for close hierarchical interactions between object apperance and motion. Such a biologically-inspired design is proven to be superb to conventional two-stream structures, which treat motion and appearance independently in separate streams and often suffer severe overfitting to object appearance. Moreover, we introduce a bridge network to modulate multi-scale spatiotemporal features into more compact, discriminative and scale-sensitive representations, which are subsequently fed into a boundary-aware decoder network to produce accurate segmentation with crisp boundaries. We perform extensive quantitative and qualitative experiments on four challenging public benchmarks, i.e., DAVIS 16 , DAVIS 17 , FBMS and YouTube-Objects. Results show that our method achieves compelling performance against current state-of-the-art ZVOS methods. To further demonstrate the generalization ability of our spatiotemporal learning framework, we extend MATNet to another relevant task: dynamic visual attention prediction (DVAP). The experiments on two popular datasets (i.e., Hollywood-2 and UCF-Sports) further verify the superiority of our model (our code is available at https://github.com/tfzhou/MATNet).},
  archive      = {J_TIP},
  author       = {Tianfei Zhou and Jianwu Li and Shunzhou Wang and Ran Tao and Jianbing Shen},
  doi          = {10.1109/TIP.2020.3013162},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8326-8338},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MATNet: Motion-attentive transition network for zero-shot video object segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Facial expression recognition in videos using dynamic
kernels. <em>TIP</em>, <em>29</em>, 8316–8325. (<a
href="https://doi.org/10.1109/TIP.2020.3011846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of facial expressions across various actors, contexts, and recording conditions in real-world videos involves identifying local facial movements. Hence, it is important to discover the formation of expressions from local representations captured from different parts of the face. So in this paper, we propose a dynamic kernel-based representation for facial expressions that assimilates facial movements captured using local spatio-temporal representations in a large universal Gaussian mixture model (uGMM). These dynamic kernels are used to preserve local similarities while handling global context changes for the same expression by utilizing the statistics of uGMM. We demonstrate the efficacy of dynamic kernel representation using three different dynamic kernels, namely, explicit mapping based, probability-based, and matching-based, on three standard facial expression datasets, namely, MMI, AFEW, and BP4D. Our evaluations show that probability-based kernels are the most discriminative among the dynamic kernels. However, in terms of computational complexity, intermediate matching kernels are more efficient as compared to the other two representations.},
  archive      = {J_TIP},
  author       = {Nazil Perveen and Debaditya Roy and Krishna Mohan Chalavadi},
  doi          = {10.1109/TIP.2020.3011846},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8316-8325},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Facial expression recognition in videos using dynamic kernels},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optical flow based co-located reference frame for video
compression. <em>TIP</em>, <em>29</em>, 8303–8315. (<a
href="https://doi.org/10.1109/TIP.2020.3014723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel bi-directional motion compensation framework that extracts existing motion information associated with the reference frames and interpolates an additional reference frame candidate that is co-located with the current frame. The approach generates a dense motion field by performing optical flow estimation, so as to capture complex motion between the reference frames without recourse to additional side information. The estimated optical flow is then complemented by transmission of offset motion vectors to correct for possible deviation from the linearity assumption in the interpolation. Various optimization schemes specifically tailored to the video coding framework are presented to further improve the performance. To accommodate applications where decoder complexity is a cardinal concern, a block-constrained speed-up algorithm is also proposed. Experimental results show that the main approach and optimization methods yield significant coding gains across a diverse set of video sequences. Further experiments focus on the trade-off between performance and complexity, and demonstrate that the proposed speed-up algorithm offers complexity reduction by a large factor while maintaining most of the performance gains.},
  archive      = {J_TIP},
  author       = {Bohan Li and Jingning Han and Yaowu Xu and Kenneth Rose},
  doi          = {10.1109/TIP.2020.3014723},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8303-8315},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optical flow based co-located reference frame for video compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating visually aligned sound from videos. <em>TIP</em>,
<em>29</em>, 8292–8302. (<a
href="https://doi.org/10.1109/TIP.2020.3009820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the task of generating sound from natural videos, and the sound should be both temporally and content-wise aligned with visual signals. This task is extremely challenging because some sounds generated outside a camera can not be inferred from video content. The model may be forced to learn an incorrect mapping between visual content and these irrelevant sounds. To address this challenge, we propose a framework named RegNet. In this framework, we first extract appearance and motion features from video frames to better distinguish the object that emits sound from complex background information. We then introduce an innovative audio forwarding regularizer that directly considers the real sound as input and outputs bottlenecked sound features. Using both visual and bottlenecked sound features for sound prediction during training provides stronger supervision for the sound prediction. The audio forwarding regularizer can control the irrelevant sound component and thus prevent the model from learning an incorrect mapping between video frames and sound emitted by the object that is out of the screen. During testing, the audio forwarding regularizer is removed to ensure that RegNet can produce purely aligned sound only from visual features. Extensive evaluations based on Amazon Mechanical Turk demonstrate that our method significantly improves both temporal and content-wise alignment. Remarkably, our generated sound can fool the human with a 68.12\% success rate. Code and pre-trained models are publicly available at https://github.com/PeihaoChen/regnet.},
  archive      = {J_TIP},
  author       = {Peihao Chen and Yang Zhang and Mingkui Tan and Hongdong Xiao and Deng Huang and Chuang Gan},
  doi          = {10.1109/TIP.2020.3009820},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8292-8302},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generating visually aligned sound from videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boosting feature matching accuracy with pairwise affine
estimation. <em>TIP</em>, <em>29</em>, 8278–8291. (<a
href="https://doi.org/10.1109/TIP.2020.3013384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local image feature matching lies in the heart of many computer vision applications. Achieving high matching accuracy is challenging when significant geometric difference exists between the source and target images. The traditional matching pipeline addresses the geometric difference by introducing the concept of support region. Around each feature point, the support region defines a neighboring area characterized by estimated attributes like scale, orientation, affine shape, etc. To correctly assign support region is not an easy job, especially when each feature is processed individually. In this article, we propose to estimate the relative affine transformation for every pair of to-be-compared features. This “tailored” measurement of geometric difference is more precise and helps improve the matching accuracy. Our pipeline can be incorporated into most existing 2D local image feature detectors and descriptors. We comprehensively evaluate its performance with various experiments on a diversified selection of benchmark datasets. The results show that the majority of tested detectors/descriptors gain additional matching accuracy with proposed pipeline.},
  archive      = {J_TIP},
  author       = {Ji Dai and Shiwei Jin and Junkang Zhang and Truong Q. Nguyen},
  doi          = {10.1109/TIP.2020.3013384},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8278-8291},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boosting feature matching accuracy with pairwise affine estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain adaptation by joint distribution invariant
projections. <em>TIP</em>, <em>29</em>, 8264–8277. (<a
href="https://doi.org/10.1109/TIP.2020.3013167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation addresses the learning problem where the training data are sampled from a source joint distribution (source domain), while the test data are sampled from a different target joint distribution (target domain). Because of this joint distribution mismatch, a discriminative classifier naively trained on the source domain often generalizes poorly to the target domain. In this article, we therefore present a Joint Distribution Invariant Projections (JDIP) approach to solve this problem. The proposed approach exploits linear projections to directly match the source and target joint distributions under the L 2 -distance. Since the traditional kernel density estimators for distribution estimation tend to be less reliable as the dimensionality increases, we propose a least square method to estimate the L 2 -distance without the need to estimate the two joint distributions, leading to a quadratic problem with analytic solution. Furthermore, we introduce a kernel version of JDIP to account for inherent nonlinearity in the data. We show that the proposed learning problems can be naturally cast as optimization problems defined on the product of Riemannian manifolds. To be comprehensive, we also establish an error bound, theoretically explaining how our method works and contributes to reducing the target domain generalization error. Extensive empirical evidence demonstrates the benefits of our approach over state-of-the-art domain adaptation methods on several visual data sets.},
  archive      = {J_TIP},
  author       = {Sentao Chen and Mehrtash Harandi and Xiaona Jin and Xiaowei Yang},
  doi          = {10.1109/TIP.2020.3013167},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8264-8277},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Domain adaptation by joint distribution invariant projections},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ORDNet: Capturing omni-range dependencies for scene parsing.
<em>TIP</em>, <em>29</em>, 8251–8263. (<a
href="https://doi.org/10.1109/TIP.2020.3013142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to capture dependencies between spatial positions is essential to many visual tasks, especially the dense labeling problems like scene parsing. Existing methods can effectively capture long-range dependencies with self-attention mechanism while short ones by local convolution. However, there is still much gap between long-range and short-range dependencies, which largely reduces the models&#39; flexibility in application to diverse spatial scales and relationships in complicated natural scene images. To fill such a gap, we develop a Middle-Range (MR) branch to capture middle-range dependencies by restricting self-attention into local patches. Also, we observe that the spatial regions which have large correlations with others can be emphasized to exploit long-range dependencies more accurately, and thus propose a Reweighed Long-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an Omni-Range Dependencies Network (ORDNet) which can effectively capture short-, middle- and long-range dependencies. Our ORDNet is able to extract more comprehensive context information and well adapt to complex spatial variance in scene images. Extensive experiments show that our proposed ORDNet outperforms previous state-of-the-art methods on three scene parsing benchmarks including PASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of capturing omni-range dependencies in deep models for scene parsing task.},
  archive      = {J_TIP},
  author       = {Shaofei Huang and Si Liu and Tianrui Hui and Jizhong Han and Bo Li and Jiashi Feng and Shuicheng Yan},
  doi          = {10.1109/TIP.2020.3013142},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8251-8263},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ORDNet: Capturing omni-range dependencies for scene parsing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gabor feature-based LogDemons with inertial constraint for
nonrigid image registration. <em>TIP</em>, <em>29</em>, 8238–8250. (<a
href="https://doi.org/10.1109/TIP.2020.3013169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonrigid image registration plays an important role in the field of computer vision and medical application. The methods based on Demons algorithm for image registration usually use intensity difference as similarity criteria. However, intensity based methods can not preserve image texture details well and are limited by local minima. In order to solve these problems, we propose a Gabor feature based LogDemons registration method in this article, called GFDemons. We extract Gabor features of the registered images to construct feature similarity metric since Gabor filters are suitable to extract image texture information. Furthermore, because of the weak gradients in some image regions, the update fields are too small to transform the moving image to the fixed image correctly. In order to compensate this deficiency, we propose an inertial constraint strategy based on GFDemons, named IGFDemons, using the previous update fields to provide guided information for the current update field. The inertial constraint strategy can further improve the performance of the proposed method in terms of accuracy and convergence. We conduct experiments on three different types of images and the results demonstrate that the proposed methods achieve better performance than some popular methods.},
  archive      = {J_TIP},
  author       = {Ying Wen and Cheng Xu and Yue Lu and Qingli Li and Haibin Cai and Lianghua He},
  doi          = {10.1109/TIP.2020.3013169},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8238-8250},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gabor feature-based LogDemons with inertial constraint for nonrigid image registration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep graph-convolutional image denoising. <em>TIP</em>,
<em>29</em>, 8226–8237. (<a
href="https://doi.org/10.1109/TIP.2020.3013166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-local self-similarity is well-known to be an effective prior for the image denoising problem. However, little work has been done to incorporate it in convolutional neural networks, which surpass non-local model-based methods despite only exploiting local information. In this paper, we propose a novel end-to-end trainable neural network architecture employing layers based on graph convolution operations, thereby creating neurons with non-local receptive fields. The graph convolution operation generalizes the classic convolution to arbitrary graphs. In this work, the graph is dynamically computed from similarities among the hidden features of the network, so that the powerful representation learning capabilities of the network are exploited to uncover self-similar patterns. We introduce a lightweight Edge-Conditioned Convolution which addresses vanishing gradient and over-parameterization issues of this particular graph convolution. Extensive experiments show state-of-the-art performance with improved qualitative and quantitative results on both synthetic Gaussian noise and real noise.},
  archive      = {J_TIP},
  author       = {Diego Valsesia and Giulia Fracastoro and Enrico Magli},
  doi          = {10.1109/TIP.2020.3013166},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8226-8237},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep graph-convolutional image denoising},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A transform coding strategy for dynamic point clouds.
<em>TIP</em>, <em>29</em>, 8213–8225. (<a
href="https://doi.org/10.1109/TIP.2020.3011811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of real-time 3D sensing devices and algorithms (e.g., multiview capturing systems, Time-of-Flight depth cameras, LIDAR sensors), as well as the widespreading of enhanced user applications processing 3D data, have motivated the investigation of innovative and effective coding strategies for 3D point clouds. Several compression algorithms, as well as some standardization efforts, has been proposed in order to achieve high compression ratios and flexibility at a reasonable computational cost. This paper presents a transform-based coding strategy for dynamic point clouds that combines a non-linear transform for geometric data with a linear transform for color data; both operations are region-adaptive in order to fit the characteristics of the input 3D data. Temporal redundancy is exploited both in the adaptation of the designed transform and in predicting the attributes at the current instant from the previous ones. Experimental results showed that the proposed solution obtained a significant bit rate reduction in lossless geometry coding and an improved rate-distortion performance in the lossy coding of color components with respect to state-of-the-art strategies.},
  archive      = {J_TIP},
  author       = {Simone Milani and Enrico Polo and Simone Limuti},
  doi          = {10.1109/TIP.2020.3011811},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8213-8225},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A transform coding strategy for dynamic point clouds},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Background noise filtering and distribution dividing for
crowd counting. <em>TIP</em>, <em>29</em>, 8199–8212. (<a
href="https://doi.org/10.1109/TIP.2020.3009030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is a challenging problem due to the diverse crowd distribution and background interference. In this paper, we propose a new approach for head size estimation to reduce the impact of different crowd scale and background noise. Different from just using local information of distance between human heads, the global information of the people distribution in the whole image is also under consideration. We obey the order of far- to near-region (small to large) to spread head size, and ensure that the propagation is uninterrupted by inserting dummy head points. The estimated head size is further exploited, such as dividing the crowd into parts of different densities and generating a high-fidelity head mask. On the other hand, we design three different head mask usage mechanisms and the corresponding head masks to analyze where and which mask could lead to better background filtering. Based on the learned masks, two competitive models are proposed which can perform robust crowd estimation against background noise and diverse crowd scale. We evaluate the proposed method on three public crowd counting datasets of ShanghaiTech, UCFQNRF and UCFCC_50. Experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art crowd counting approaches.},
  archive      = {J_TIP},
  author       = {Hong Mo and Wenqi Ren and Yuan Xiong and Xiaoqi Pan and Zhong Zhou and Xiaochun Cao and Wei Wu},
  doi          = {10.1109/TIP.2020.3009030},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8199-8212},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Background noise filtering and distribution dividing for crowd counting},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). MCMT-GAN: Multi-task coherent modality transferable GAN for
3D brain image synthesis. <em>TIP</em>, <em>29</em>, 8187–8198. (<a
href="https://doi.org/10.1109/TIP.2020.3011557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to synthesize multi-modality data is highly desirable for many computer-aided medical applications, e.g. clinical diagnosis and neuroscience research, since rich imaging cohorts offer diverse and complementary information unraveling human tissues. However, collecting acquisitions can be limited by adversary factors such as patient discomfort, expensive cost and scanner unavailability. In this paper, we propose a multi-task coherent modality transferable GAN (MCMT-GAN) to address this issue for brain MRI synthesis in an unsupervised manner. Through combining the bidirectional adversarial loss, cycle-consistency loss, domain adapted loss and manifold regularization in a volumetric space, MCMT-GAN is robust for multi-modality brain image synthesis with visually high fidelity. In addition, we complement discriminators collaboratively working with segmentors which ensure the usefulness of our results to segmentation task. Experiments evaluated on various cross-modality synthesis show that our method produces visually impressive results with substitutability for clinical post-processing and also exceeds the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yawen Huang and Feng Zheng and Runmin Cong and Weilin Huang and Matthew R. Scott and Ling Shao},
  doi          = {10.1109/TIP.2020.3011557},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8187-8198},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MCMT-GAN: Multi-task coherent modality transferable GAN for 3D brain image synthesis},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Context-aware graph label propagation network for saliency
detection. <em>TIP</em>, <em>29</em>, 8177–8186. (<a
href="https://doi.org/10.1109/TIP.2020.3002083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a large number of existing methods for saliency detection have mainly focused on designing complex network architectures to aggregate powerful features from backbone networks. However, contextual information is not well utilized, which often causes false background regions and blurred object boundaries. Motivated by these issues, we propose an easy-to-implement module that utilizes the edge-preserving ability of superpixels and the graph neural network to interact the context of superpixel nodes. In more detail, we first extract the features from the backbone network and obtain the superpixel information of images. This step is followed by superpixel pooling in which we transfer the irregular superpixel information to a structured feature representation. To propagate the information among the foreground and background regions, we use a graph neural network and self-attention layer to better evaluate the degree of saliency degree. Additionally, an affinity loss is proposed to regularize the affinity matrix to constrain the propagation path. Moreover, we extend our module to a multiscale structure with different numbers of superpixels. Experiments on five challenging datasets show that our approach can improve the performance of three baseline methods in terms of some popular evaluation metrics.},
  archive      = {J_TIP},
  author       = {Wei Ji and Xi Li and Lina Wei and Fei Wu and Yueting Zhuang},
  doi          = {10.1109/TIP.2020.3002083},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8177-8186},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Context-aware graph label propagation network for saliency detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantics-preserving graph propagation for zero-shot object
detection. <em>TIP</em>, <em>29</em>, 8163–8176. (<a
href="https://doi.org/10.1109/TIP.2020.3011807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing object detection models are restricted to detecting objects from previously seen categories, an approach that tends to become infeasible for rare or novel concepts. Accordingly, in this paper, we explore object detection in the context of zero-shot learning, i.e., Zero-Shot Object Detection (ZSD), to concurrently recognize and localize objects from novel concepts. Existing ZSD algorithms are typically based on a strict mapping-transfer strategy that suffers from a significant visual-semantic gap. To bridge the gap, we propose a novel Semantics-Preserving Graph Propagation model for ZSD based on Graph Convolutional Networks (GCN). More specifically, we develop a graph construction module to flexibly build category graphs by leveraging diverse correlations between category nodes; this is followed by two semantics-preserving graph propagation modules that enhance both category and region representations. Benefiting from the multi-step graph propagation process, both the semantic description and structural knowledge exhibited in prior category graphs can be effectively leveraged to boost the generalization capability of the learned projection function. Experiments on existing seen/unseen splits of three popular object detection datasets demonstrate that the proposed approach performs favorably against state-of-the-art ZSD methods.},
  archive      = {J_TIP},
  author       = {Caixia Yan and Qinghua Zheng and Xiaojun Chang and Minnan Luo and Chung-Hsing Yeh and Alexander G. Hauptman},
  doi          = {10.1109/TIP.2020.3011807},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8163-8176},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantics-preserving graph propagation for zero-shot object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep-like hashing-in-hash for visual retrieval: An
embarrassingly simple method. <em>TIP</em>, <em>29</em>, 8149–8162. (<a
href="https://doi.org/10.1109/TIP.2020.3011796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing hashing methods have yielded significant performance in image and multimedia retrieval, which can be categorized into two groups: shallow hashing and deep hashing. However, there still exist some intrinsic limitations among them. The former generally adopts a one-step strategy to learn the hashing codes for discovering the discriminative binary feature, but the latent discriminative information in the learned hashing codes is not well exploited. The latter, as deep neural network based hashing models, can learn highly discriminative and compact features, but relies on large-scale data and computation resources for numerous network parameters tuning with back-propagation optimization. Straightforward training of deep hashing models from scratch on small-scale data is almost impossible. Therefore, in order to develop efficient but effective learning to hash algorithm that depends only on small-scale data, we propose a novel non-neural network based deep-like learning framework, i.e. multi-level cascaded hashing (MCH) approach with hierarchical learning strategy, for image retrieval. The contributions are threefold. First, a hashing-in-hash architecture is designed in MCH, which inherits the excellent traits of traditional neural networks based deep learning, such that discriminative binary features that are beneficial to image retrieval can be effectively captured. Second, in each level the binary features of all preceding levels and the visual appearance feature are simultaneously cascaded as inputs of all subsequent levels to retrain, which fully exploits the implicated discriminative information. Third, a basic learning to hash (BLH) model with label constraint is proposed for hierarchical learning. Without loss of generality, the existing hashing models can be easily integrated into our MCH framework. We show experimentally on small- and large-scale visual retrieval tasks that our method outperforms several state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Lei Zhang and Ji Liu and Fuxiang Huang and Yang Yang and David Zhang},
  doi          = {10.1109/TIP.2020.3011796},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8149-8162},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep-like hashing-in-hash for visual retrieval: An embarrassingly simple method},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Speeding up VP9 intra encoder with hierarchical deep
learning-based partition prediction. <em>TIP</em>, <em>29</em>,
8134–8148. (<a href="https://doi.org/10.1109/TIP.2020.3011270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In VP9 video codec, the sizes of blocks are decided during encoding by recursively partitioning 64 × 64 superblocks using rate-distortion optimization (RDO). This process is computationally intensive because of the combinatorial search space of possible partitions of a superblock. Here, we propose a deep learning based alternative framework to predict the intra-mode superblock partitions in the form of a four-level partition tree, using a hierarchical fully convolutional network (H-FCN). We created a large database of VP9 superblocks and the corresponding partitions to train an H-FCN model, which was subsequently integrated with the VP9 encoder to reduce the intra-mode encoding time. The experimental results establish that our approach speeds up intra-mode encoding by 69.7\% on average, at the expense of a 1.71\% increase in the Bjøntegaard-Delta bitrate (BD-rate). While VP9 provides several built-in speed levels which are designed to provide faster encoding at the expense of decreased rate-distortion performance, we find that our model is able to outperform the fastest recommended speed level of the reference VP9 encoder for the good quality intra encoding configuration, in terms of both speedup and BD-rate.},
  archive      = {J_TIP},
  author       = {Somdyuti Paul and Andrey Norkin and Alan C. Bovik},
  doi          = {10.1109/TIP.2020.3011270},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8134-8148},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Speeding up VP9 intra encoder with hierarchical deep learning-based partition prediction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reliable multi-kernel subtask graph correlation tracker.
<em>TIP</em>, <em>29</em>, 8120–8133. (<a
href="https://doi.org/10.1109/TIP.2020.3009883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many astonishing correlation filter trackers pay limited concentration on the tracking reliability and locating accuracy. To solve the issues, we propose a reliable and accurate cross correlation particle filter tracker via graph regularized multi-kernel multi-subtask learning. Specifically, multiple non-linear kernels are assigned to multi-channel features with reliable feature selection. Each kernel space corresponds to one type of reliable and discriminative features. Then, we define the trace of each target subregion with one feature as a single view, and their multi-view cooperations and interdependencies are exploited to jointly learn multi-kernel subtask cross correlation particle filters, and make them complement and boost each other. The learned filters consist of two complementary parts: weighted combination of base kernels and reliable integration of base filters. The former is associated to feature reliability with importance map, and the weighted information reflects different tracking contribution to accurate location. The second part is to find the reliable target subtasks via the response map, to exclude the distractive subtasks or backgrounds. Besides, the proposed tracker constructs the Laplacian graph regularization via cross similarity of different subtasks, which not only exploits the intrinsic structure among subtasks, and preserves their spatial layout structure, but also maintains the temporal-spatial consistency of subtasks. Comprehensive experiments on five datasets demonstrate its remarkable and competitive performance against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Baojie Fan and Yang Cong and Jiandong Tian and Yandong Tang},
  doi          = {10.1109/TIP.2020.3009883},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8120-8133},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reliable multi-kernel subtask graph correlation tracker},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining spatial-temporal similarity for visual tracking.
<em>TIP</em>, <em>29</em>, 8107–8119. (<a
href="https://doi.org/10.1109/TIP.2020.2981813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filter (CF) is a critical technique to improve accuracy and speed in the field of visual object tracking. Despite being studied extensively, most existing CF methods suffer from failing to make the most of the inherent spatial-temporal prior of videos. To address this limitation, as consecutive frames are eminently resemble in most videos, we investigate a novel scheme to predict targets&#39; future state by exploiting previous observations. Specifically, in this paper, we propose a prediction based CF tracking framework by learning the spatial-temporal similarity of consecutive frames for sample managing, template regularization, and training response pre-weighting. We model the learning problem theoretically as a novel objective and provide effective optimization algorithms to solve the learning task. In addition, we implement two CF trackers with different features. Extensive experiments are conducted on three popular benchmarks to validate our scheme. The encouraging results demonstrate that the proposed scheme can significantly boost the accuracy of CF tracking, and the two trackers achieve competitive performances against state-of-the-art trackers. We finally present a comprehensive analysis on the efficacy of our proposed method and the efficiency of our trackers to facilitate real-world visual tracking applications.},
  archive      = {J_TIP},
  author       = {Yu Zhang and Xingyu Gao and Zhenyu Chen and Huicai Zhong and Hongtao Xie and Chenggang Yan},
  doi          = {10.1109/TIP.2020.2981813},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8107-8119},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Mining spatial-temporal similarity for visual tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised feature selection via data reconstruction and
side information. <em>TIP</em>, <em>29</em>, 8097–8106. (<a
href="https://doi.org/10.1109/TIP.2020.3011253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data reconstruction, which aims at preserving statistical properties of the data during the reconstruction has become a new criterion for feature selection. Although feature selection could benefit from the perspective of data reconstruction, it is unable to exploit other crucial information, namely, graph structure and pairwise constraints. To address previously mentioned deficiency, we propose a novel feature selection approach in this paper, known as unsupervised feature selection via data reconstruction and side information. More specifically, the proposed method takes advantage of the prior knowledge regarding pairwise constraints (side information), the minimization of data reconstruction error, and the graph embedding simultaneously, such that pivotal features are selected with preserving data manifold structure. To obtain the robust solution, a robust loss function is applied to the feature selection problem, which interpolates between ℓ 1 -norm and ℓ 2 -norm. Eventually, extensive experiments are conducted to demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Rui Zhang and Xuelong Li},
  doi          = {10.1109/TIP.2020.3011253},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8097-8106},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised feature selection via data reconstruction and side information},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Latent complete row space recovery for multi-view subspace
clustering. <em>TIP</em>, <em>29</em>, 8083–8096. (<a
href="https://doi.org/10.1109/TIP.2020.3010631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering has been applied to applications such as image processing and video surveillance, and has attracted increasing attention. Most existing methods learn view-specific self-representation matrices, and construct a combined affinity matrix from multiple views. The affinity construction process is time-consuming, and the combined affinity matrix is not guaranteed to reflect the whole true subspace structure. To overcome these issues, the Latent Complete Row Space Recovery (LCRSR) method is proposed. Concretely, LCRSR is based on the assumption that the multi-view observations are generated from an underlying latent representation, which is further assumed to collect the authentic samples drawn exactly from multiple subspaces. LCRSR is able to recover the row space of the latent representation, which not only carries complete information from multiple views but also determines the subspace membership under certain conditions. LCRSR does not involve the graph construction procedure and is solved with an efficient and convergent algorithm, thereby being more scalable to large-scale datasets. The effectiveness and efficiency of LCRSR are validated by clustering various kinds of multi-view data and illustrated in the background subtraction task.},
  archive      = {J_TIP},
  author       = {Hong Tao and Chenping Hou and Yuhua Qian and Jubo Zhu and Dongyun Yi},
  doi          = {10.1109/TIP.2020.3010631},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8083-8096},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Latent complete row space recovery for multi-view subspace clustering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fractal dimension of color fractal images with correlated
color components. <em>TIP</em>, <em>29</em>, 8069–8082. (<a
href="https://doi.org/10.1109/TIP.2020.3011283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We mathematically prove that color fractal images with two and three correlated color components generated with the midpoint displacement approach obey the property of self-similarity, thus enabling the estimation of their color fractal dimension. We generate various sets of color fractal images with two and three correlated color components, controlled both by the Hurst parameter and the variance-covariance matrix, with and without a global normalization, and use them for the calibration of the embraced color fractal dimension estimator. We improve the existing fractal dimension estimator based on probabilistic box-counting by reducing the variance of the regression line estimators through the iterative elimination of most error-ed measurement points. We independently estimate the variance-covariance matrix and Hurst parameter for the sets of generated color fractal images with correlated color components. We show the experimental results and discuss both the improvements and the limitations of the proposed approach.},
  archive      = {J_TIP},
  author       = {Mihai Ivanovici},
  doi          = {10.1109/TIP.2020.3011283},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8069-8082},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fractal dimension of color fractal images with correlated color components},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised learning of image segmentation based on
differentiable feature clustering. <em>TIP</em>, <em>29</em>, 8055–8068.
(<a href="https://doi.org/10.1109/TIP.2020.3011269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of convolutional neural networks (CNNs) for unsupervised image segmentation was investigated in this study. Similar to supervised image segmentation, the proposed CNN assigns labels to pixels that denote the cluster to which the pixel belongs. In unsupervised image segmentation, however, no training images or ground truth labels of pixels are specified beforehand. Therefore, once a target image is input, the pixel labels and feature representations are jointly optimized, and their parameters are updated by the gradient descent. In the proposed approach, label prediction and network parameter learning are alternately iterated to meet the following criteria: (a) pixels of similar features should be assigned the same label, (b) spatially continuous pixels should be assigned the same label, and (c) the number of unique labels should be large. Although these criteria are incompatible, the proposed approach minimizes the combination of similarity loss and spatial continuity loss to find a plausible solution of label assignment that balances the aforementioned criteria well. The contributions of this study are four-fold. First, we propose a novel end-to-end network of unsupervised image segmentation that consists of normalization and an argmax function for differentiable clustering. Second, we introduce a spatial continuity loss function that mitigates the limitations of fixed segment boundaries possessed by previous work. Third, we present an extension of the proposed method for segmentation with scribbles as user input, which showed better accuracy than existing methods while maintaining efficiency. Finally, we introduce another extension of the proposed method: unseen image segmentation by using networks pre-trained with a few reference images without re-training the networks. The effectiveness of the proposed approach was examined on several benchmark datasets of image segmentation.},
  archive      = {J_TIP},
  author       = {Wonjik Kim and Asako Kanezaki and Masayuki Tanaka},
  doi          = {10.1109/TIP.2020.3011269},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8055-8068},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised learning of image segmentation based on differentiable feature clustering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Controllable image processing via adaptive FilterBank
pyramid. <em>TIP</em>, <em>29</em>, 8043–8054. (<a
href="https://doi.org/10.1109/TIP.2020.3009844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image processing operators often provide some control parameters to tweak the final results. Recently, different convolutional neural networks have been used to approximate or improve these operators. However, in those methods, one single model can only handle one operator of a specific parameter value and does not support parameter tuning. In this paper, we propose a new plugin module, “Adaptive Filterbank Pyramid”, which can be inserted into a backbone network to support multiple operators and continuous parameter tuning. Our module explicitly represents one operator with one filterbank pyramid. To generate the results of a specific operator, the corresponding filterbank pyramid is convolved with the intermediate feature pyramid produced by the backbone network. The weights of the filterbank pyramid are directly regressed by another sub-network, which is jointly trained with the backbone network and adapted to the input parameter, thus enabling continuous parameter tuning. We applied the proposed module for a large variety of image processing tasks, including image smoothing, image denoising, image deblocking, image enhancement and neural style transfer. Experiments show that our method is generalized to different types of image processing tasks and different backbone network structures. Compared to the single-operator-single-parameter baseline, our method can produce comparable results but is significantly more efficient in both training and testing.},
  archive      = {J_TIP},
  author       = {Dongdong Chen and Qingnan Fan and Jing Liao and Angelica Aviles-Rivero and Lu Yuan and Nenghai Yu and Gang Hua},
  doi          = {10.1109/TIP.2020.3009844},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8043-8054},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Controllable image processing via adaptive FilterBank pyramid},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A truncated matrix decomposition for hyperspectral image
super-resolution. <em>TIP</em>, <em>29</em>, 8028–8042. (<a
href="https://doi.org/10.1109/TIP.2020.3009830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image super-resolution addresses the problem of fusing a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI) to produce a high-resolution hyperspectral image (HR-HSI). In this paper, we propose a novel fusion approach for hyperspectral image super-resolution by exploiting the specific properties of matrix decomposition, which consists of four main steps. First, an endmember extraction algorithm is used to extract an initial spectral matrix from LR-HSI. Then, with the initial spectral matrix, we estimate the spatial matrix, i.e., the spatial-contextual information, from the degraded observations of HR-HSI. Third, the spatial matrix is further utilized to estimate the spectral matrix from LR-HSI by solving a least squares (LS)-based problem. Finally, the target HR-HSI is constructed by combing the estimated spectral and spatial matrixes. In particular, two models are proposed to estimate the spatial matrix. One is a simple case that involves a LS-based problem, and the other is an elaborate case that consists of two fidelity terms and a spatial regularizer, where the spatial regularizer aiming to restrain the range of solutions is achieved by exploiting the superpixel-level low-rank characteristics of HR-HSI. Experiment results conducted on both synthetic and real data sets demonstrate the effectiveness of the proposed approach as compared to other hyperspectral image super-resolution methods.},
  archive      = {J_TIP},
  author       = {Jianjun Liu and Zebin Wu and Liang Xiao and Jun Sun and Hong Yan},
  doi          = {10.1109/TIP.2020.3009830},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8028-8042},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A truncated matrix decomposition for hyperspectral image super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning deeply aggregated alternating minimization for
general inverse problems. <em>TIP</em>, <em>29</em>, 8012–8027. (<a
href="https://doi.org/10.1109/TIP.2020.3010082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regularization-based image restoration is one of the most powerful tools in image processing and computer vision thanks to its flexibility for handling various inverse problems. However, designing an optimal regularization function still remains unsolved since natural images and related scene types have a complex structure. In this paper, we present a general and principled framework, called deeply aggregated alternating minimization (DeepAM). We design a convolutional neural network (CNN) to implicitly parameterize the regularizer of the alternating minimization (AM) algorithm. Contrary to the conventional AM algorithm based on a point-wise proximal mapping, the DeepAM projects intermediate estimate into a set of natural images via deep aggregation. Since the CNN is fully integrated into the AM procedure, all parameters can be jointly optimized through end-to-end training. These properties enable the DeepAM to converge with a small number of iterations, while maintaining an algorithmic simplicity. We show that the DeepAM outperforms state-of-the-art methods, including nonlocal-based methods, Plug-and-Play regularization, and recent data-driven approaches. The effectiveness of our framework is demonstrated in a variety of image restoration tasks: Guassian denoising, deraining, deblurring, super-resolution, color-guided depth upsampling, and RGB/NIR restoration.},
  archive      = {J_TIP},
  author       = {Hyungjoo Jung and Youngjung Kim and Dongbo Min and Hyunsung Jang and Namkoo Ha and Kwanghoon Sohn},
  doi          = {10.1109/TIP.2020.3010082},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8012-8027},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning deeply aggregated alternating minimization for general inverse problems},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating target image-label pairs for unsupervised domain
adaptation. <em>TIP</em>, <em>29</em>, 7997–8011. (<a
href="https://doi.org/10.1109/TIP.2020.3009853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning demonstrates its impressive success across various machine learning problems. However, its performance often suffers in the case where the training and test data sets follow different distributions, due to the domain shift. Most current domain adaptation methods minimize the discrepancy between the source and target domains by enforcing the alignment of their marginal distributions without considering the class-level matching. Consequently, data from different classes may become close together after mapping. To address this issue, we propose an unsupervised domain adaptation method by generating image-label pairs in the target domain, in which the model is augmented with the generated target pairs and achieve class-level transfer. Specifically, we integrate generative adversarial networks (GAN) into the model predictor, where the generator fed with labels aims to produce corresponding target domain images with a well-designed semantic loss. Meanwhile, compared to previous methods which focus on discrepancy reduction across domains, i.e., image to image translation, our model focuses on semantic preservation during image generation. Our model is straightforward yet effective for unsupervised domain adaptation problems. Without any labels in the target domain in all the experiments, we demonstrate the validity of our approach by presenting the plausible generated target image-label pairs. In addition, our proposed method achieves the best or comparable performance on multiple unsupervised domain adaptation datasets which include image classification and semantic segmentation.},
  archive      = {J_TIP},
  author       = {Rui Li and Wenming Cao and Si Wu and Hau-San Wong},
  doi          = {10.1109/TIP.2020.3009853},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7997-8011},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generating target image-label pairs for unsupervised domain adaptation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lightening network for low-light image enhancement.
<em>TIP</em>, <em>29</em>, 7984–7996. (<a
href="https://doi.org/10.1109/TIP.2020.3008396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement is a challenging task that has attracted considerable attention. Pictures taken in low-light conditions often have bad visual quality. To address the problem, we regard the low-light enhancement as a residual learning problem that is to estimate the residual between low- and normal-light images. In this paper, we propose a novel Deep Lightening Network (DLN) that benefits from the recent development of Convolutional Neural Networks (CNNs). The proposed DLN consists of several Lightening Back-Projection (LBP) blocks. The LBPs perform lightening and darkening processes iteratively to learn the residual for normal-light estimations. To effectively utilize the local and global features, we also propose a Feature Aggregation (FA) block that adaptively fuses the results of different LBPs. We evaluate the proposed method on different datasets. Numerical results show that our proposed DLN approach outperforms other methods under both objective and subjective metrics.},
  archive      = {J_TIP},
  author       = {Li-Wen Wang and Zhi-Song Liu and Wan-Chi Siu and Daniel P. K. Lun},
  doi          = {10.1109/TIP.2020.3008396},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7984-7996},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Lightening network for low-light image enhancement},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic sampling networks for efficient action recognition
in videos. <em>TIP</em>, <em>29</em>, 7970–7983. (<a
href="https://doi.org/10.1109/TIP.2020.3007826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing action recognition methods are mainly based on clip-level classifiers such as two-stream CNNs or 3D CNNs, which are trained from the randomly selected clips and applied to densely sampled clips during testing. However, this standard setting might be suboptimal for training classifiers and also requires huge computational overhead when deployed in practice. To address these issues, we propose a new framework for action recognition in videos, called Dynamic Sampling Networks (DSN), by designing a dynamic sampling module to improve the discriminative power of learned clip-level classifiers and as well increase the inference efficiency during testing. Specifically, DSN is composed of a sampling module and a classification module, whose objective is to learn a sampling policy to on-the-fly select which clips to keep and train a clip-level classifier to perform action recognition based on these selected clips, respectively. In particular, given an input video, we train an observation network in an associative reinforcement learning setting to maximize the rewards of the selected clips with a correct prediction. We perform extensive experiments to study different aspects of the DSN framework on four action recognition datasets: UCF101, HMDB51, THUMOS14, and ActivityNet v1.3. The experimental results demonstrate that DSN is able to greatly improve the inference efficiency by only using less than half of the clips, which can still obtain a slightly better or comparable recognition accuracy to the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Yin-Dong Zheng and Zhaoyang Liu and Tong Lu and Limin Wang},
  doi          = {10.1109/TIP.2020.3007826},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7970-7983},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic sampling networks for efficient action recognition in videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-term action dependence-based hierarchical deep
association for multi-athlete tracking in sports videos. <em>TIP</em>,
<em>29</em>, 7957–7969. (<a
href="https://doi.org/10.1109/TIP.2020.3009034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking multiple athletes in sports videos is a very challenging Multi-Object Tracking (MOT) task, as athletes generally share high similarity in appearance with large deformations. In this paper, unlike the existing hand-crafted solutions, we propose a novel and effective approach to this issue, which hierarchically associates detections of the same identity through discriminative and robust deep features. First, in detection association, we make use of athlete appearances and poses instead of traditional position cues to generate short tracklets for better initialization. Second, in tracklet association, a new deep architecture, namely Siamese Tracklet Affinity Networks (STAN), is presented, which is able to bi-directionally simulate the unseen dynamics of actions, comprehensively models the long-term action dependences, and sequentially estimates their affinity. Such hierarchical association is finally solved as a minimum-cost network flow problem. We extensively evaluate the proposed approach on the APIDIS, NCAA Basketball and VolleyTrack (newly collected) databases, and the experimental results show its advantages.},
  archive      = {J_TIP},
  author       = {Longteng Kong and Di Huang and Yunhong Wang},
  doi          = {10.1109/TIP.2020.3009034},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7957-7969},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Long-term action dependence-based hierarchical deep association for multi-athlete tracking in sports videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Light field image quality assessment via the light field
coherence. <em>TIP</em>, <em>29</em>, 7945–7956. (<a
href="https://doi.org/10.1109/TIP.2020.3008856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel full-reference image quality assessment (IQA) method for evaluating the quality of the distorted light field (LF) image against its reference LF image is proposed, called the log-Gabor feature-based light field coherence (LGF-LFC). Based on the fact that to compare two LF images, it essentially boils down to measure how coherent of these two LF images, we attempt to measure the degree of their LF coherence (LFC). To pursue this goal, the salient features from the reference and distorted LF images under comparison need to be extracted. By considering that the Gabor feature has the ability to well characterize the human visual system (HVS) perception, and the special characteristics of the LF images, the multi -scale and single -scale Gabor feature extraction schemes are developed to extract the multi-scale log-Gabor features from the sub-aperture images (SAIs) and the single-scale log-Gabor feature from the epi-polar images (EPIs), respectively. Note that the former can reflect the image details (via the SAIs), while the latter indicates the viewing consistency (via the EPI’s depth information). The similarity measurements are subsequently conducted on the comparison of their SAIs and that of their EPIs separately, followed by combining them together for arriving at the final score. Extensive simulation results have clearly demonstrated that the proposed LGF-LFC is more consistent with the perception of the HVS on the quality evaluation of the LF images than multiple classical and state-of-the-art IQA methods.},
  archive      = {J_TIP},
  author       = {Yu Tian and Huanqiang Zeng and Junhui Hou and Jing Chen and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2020.3008856},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7945-7956},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Light field image quality assessment via the light field coherence},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boosting compressed sensing using local measurements and
sliding window reconstruction. <em>TIP</em>, <em>29</em>, 7931–7944. (<a
href="https://doi.org/10.1109/TIP.2020.3007822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the framework of compressed sensing, image data is measured using less measurements than the total number of pixels. Each measurement consists of a (random) linear combination of all pixels. Since image data is approximately sparse in an appropriate transform domain, a reasonable reconstruction is possible for many measurement matrices, especially for i.i.d. Gaussian measurement matrices. In a seemingly different field, non-regular sampling techniques such as three-quarter sampling have shown promising results to enhance the resolution of an imaging sensor by effectively sub-sampling a higher resolution image. Here, the measurements can be described as linear combinations of only three pixels, which can also be seen as a (spectral) compressed sensing measurement. Since each measurement is spatially localized, the reconstruction can be performed in overlapping sliding windows. In this work, we show that compressed sensing reconstruction algorithms can greatly benefit from such an overlapping sliding window reconstruction. Compared to conventional block-wise compressed sensing with i.i.d. Gaussian measurement matrices, the reconstruction quality in terms of the PSNR increases up to +5dB using small, local i.d.d. Gaussian measurement blocks. Additionally, we propose a local joint sparse deconvolution and extrapolation (L-JSDE) to reconstruct images from arbitrary local measurements. For several applications with local measurements we show that L-JSDE increases the PSNR by +2.2dB relative to conventional block-wise i.i.d. Gaussian measurements reconstructed with the state-of-the-art reconstruction algorithm D-AMP using the same overall sampling density.},
  archive      = {J_TIP},
  author       = {Simon Grosche and Andy Regensky and Jürgen Seiler and André Kaup},
  doi          = {10.1109/TIP.2020.3007822},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7931-7944},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boosting compressed sensing using local measurements and sliding window reconstruction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved multi-view convolutional neural network for 3D
object retrieval. <em>TIP</em>, <em>29</em>, 7917–7930. (<a
href="https://doi.org/10.1109/TIP.2020.3008970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robust and discriminative representations is essential for 3D object retrieval. In this paper, we present an improved Multi-view Convolutional Neural Network (MVCNN) for view-based 3D object representation learning. Our technical contributions are divided into two aspects. First, we propose to employ Group-view Similarity Learning (GSL) over the multi-view representations before the aggregation operation ( i.e. , max-pooling in MVCNN). We assume that the similarity information among the view groups of different 3D objects can provide an important cue but has been neglected more or less by previous methods. To enhance it, we add a branch to the original MVCNN architecture and learn to maintain such group-view similarity relationships. Second, we utilize an end-to-end metric learning loss function to improve the representation learning process. In particular, we propose an improved Triplet-Center Loss (TCL) named Adaptive Margin based Triplet-Center Loss (AMTCL). The original TCL assumes a fixed and common margin to control the relative distance relationship between a sample to its corresponding class center and to the nearest negative center. Though TCL has demonstrated its great capacity on the 3D object retrieval task, however, when considering the distinguishability between samples of one class and samples of another class, we assume that it would be more appropriate that the margin takes different values based on the distinguishability of samples of different classes. Therefore we propose to adaptively and dynamically adjust the margin hyperparameter based on the normalized confusion matrix which is obtained on the training set during the training process. Extensive experiments on several public 3D shape benchmarks show that our method, GSL + AMTCL, can learn more suitable representations for 3D object retrieval, obtaining superior performance against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Xinwei He and Song Bai and Jiajia Chu and Xiang Bai},
  doi          = {10.1109/TIP.2020.3008970},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7917-7930},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An improved multi-view convolutional neural network for 3D object retrieval},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HAM: Hidden anchor mechanism for scene text detection.
<em>TIP</em>, <em>29</em>, 7904–7916. (<a
href="https://doi.org/10.1109/TIP.2020.3008863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct regression and anchor are the two mainly effective and prevailing mechanisms in the paradigm of scene text detection. However, the use of direct regression-based methods may be challenging during optimization without the help of anchors as references. Unfortunately, the anchor-based methods always suffer from the careful design of the anchors, degrading the robustness to complex scenes. To address the above-mentioned problems, we propose a novel hidden anchor mechanism (HAM) especially for scene text detection. The predictions of anchors are innovatively regarded as hidden layers, and the weighted sum of the predictions is integrated into a direct regression-based network. Hence, the architecture of our HAM still has the characteristic of simplicity as with direct regression-based methods. Moreover, it is easier to optimize anchors as references with this type of method than with direct regression-based methods. In this way, our network can take advantage of both direct regression and anchor mechanisms. In addition, we decouple three kinds of one-dimensional anchors from three-dimensional anchors, greatly reducing the number of anchors in text bounding box matching without performance degradation. We also propose a post-processing technique for long text detection, named iterative regression box (IRB), which takes a few additional computational costs and can be easily generalized to other methods. Experiments on several public datasets demonstrate that the proposed method achieves state-of-the-art performance. Code is available at https://github.com/hjbplayer/HAM .},
  archive      = {J_TIP},
  author       = {Jie-Bo Hou and Xiaobin Zhu and Chang Liu and Kekai Sheng and Long-Huang Wu and Hongfa Wang and Xu-Cheng Yin},
  doi          = {10.1109/TIP.2020.3008863},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7904-7916},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HAM: Hidden anchor mechanism for scene text detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced 3DTV regularization and its applications on HSI
denoising and compressed sensing. <em>TIP</em>, <em>29</em>, 7889–7903.
(<a href="https://doi.org/10.1109/TIP.2020.3007840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The total variation (TV) is a powerful regularization term encoding the local smoothness prior structure underlying images. By combining the TV regularization term with low rank prior, the 3D total variation (3DTV) regularizer has achieved advanced performance in general hyperspectral image (HSI) processing tasks. Intrinsically, 3DTV assumes i.i.d. sparsity structures on all bands of the gradient maps calculated along the spectrum and space of an HSI. This, however, largely deviates from the real-world cases, where the gradient maps generally have different while correlated gradient map structures across all bands. To alleviate this issue, we propose an enhanced 3DTV (E-3DTV) regularization term beyond the conventional. Instead of imposing sparsity on gradient maps themselves, the new term calculates sparsity on the subspace bases on gradient maps along all bands of an HSI, which naturally encodes the correlation and difference among all these bands, and thus more faithfully reflects the insightful configurations of an HSI. The E-3DTV term can easily replace the conventional 3DTV term and be embedded into an HSI processing model to ameliorate its performance. We made such attempts on two typical related tasks: HSI denoising and compressed sensing. The superiority of our proposed method is substantiated by extensive experiments on synthetic and real HSI data, visually and quantitatively on both tasks, as compared with current state-of-the-arts. The code of our algorithm is released at https://github.com/andrew-pengjj/Enhanced-3DTV.git .},
  archive      = {J_TIP},
  author       = {Jiangjun Peng and Qi Xie and Qian Zhao and Yao Wang and Leung Yee and Deyu Meng},
  doi          = {10.1109/TIP.2020.3007840},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7889-7903},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhanced 3DTV regularization and its applications on HSI denoising and compressed sensing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-cue semi-supervised color constancy with limited
training samples. <em>TIP</em>, <em>29</em>, 7875–7888. (<a
href="https://doi.org/10.1109/TIP.2020.3007823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color constancy is one of the fundamental tasks in computer vision. Many supervised methods, including recently proposed Convolutional Neural Networks (CNN)-based methods, have been proved to work well on this problem, but they often require a sufficient number of labeled data. However, it is expensive and time-consuming to collect a large number of labeled training images with accurately measured illumination. In order to reduce the dependence on labeled images and leverage unlabeled ones without measured illumination, we propose a novel semi-supervised framework with limited training samples for illumination estimation. Our key insight is that the images with similar features from different cues will share similar lighting conditions. Consequently, three graphs based on three visual cues, low-level RGB color distribution, mid-level initial illuminant estimates and high-level scene content, are constructed to represent the relationship among different images. Then a multi-cue semi-supervised color constancy method (MSCC) is proposed after integrating these three graphs into a unified model. Extensive experiments on benchmark datasets demonstrate that our proposed MSCC method outperforms nearly all the existing supervised methods with limited labeled samples. Even with no unlabeled samples, MSCC still obtains better performance and stableness than most supervised methods.},
  archive      = {J_TIP},
  author       = {Xinwei Huang and Bing Li and Shuai Li and Wenjuan Li and Weihua Xiong and Xuanwu Yin and Weiming Hu and Hong Qin},
  doi          = {10.1109/TIP.2020.3007823},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7875-7888},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-cue semi-supervised color constancy with limited training samples},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Truncated low-rank and total p variation constrained color
image completion and its moreau approximation algorithm. <em>TIP</em>,
<em>29</em>, 7861–7874. (<a
href="https://doi.org/10.1109/TIP.2020.3008367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, low-rank (LR) and total variation (TV) constrained tensor completion algorithms have been broadly studied for image restoration. These algorithms, however, ignore the difference of the intrinsic properties along spatial structure, spectral correlation, and unfolded mode. In this paper, we go further by providing a detailed comparison of the LR and TV properties in matrix and tensor cases, and figure out the LRTV constraints for pixel matrices are more evident and accordant than for others. This inspires us to develop a simple yet effective multichannel LRTV model that is capable of genuinely discovering the intrinsic properties with reduced computational cost. Moreover, due to the suboptimality of nuclear norm and $l_{1}$ norm in approximating the essential low rank and low gradient properties, we employ two enhanced constraints, i.e., truncated nuclear norm (TNN) and total $p$ variation ${\text{T}}_{p}\text{V}$ , for a better performance. This results in a challenging problem since that both TNN and ${\text{T}}_{p}\text{V}$ are nonsmooth and nonconvex. Observing that the Moreau approximation of ${\text{T}}_{p}\text{V}$ constraint is a continuous difference-of-convex function, we then develop a first-order method by repeatedly computing two simple proximal operators. Under mild assumption, we further prove that the sequence generated by our method clusters at a stationary point. Extensive experimental results on color image completion show the efficacy and efficiency of our method over state-of-the-art competitors.},
  archive      = {J_TIP},
  author       = {Jianwei Zheng and Ping Yang and Xi Yang and Shengyong Chen},
  doi          = {10.1109/TIP.2020.3008367},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7861-7874},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Truncated low-rank and total p variation constrained color image completion and its moreau approximation algorithm},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). A comprehensive benchmark for single image compression
artifact reduction. <em>TIP</em>, <em>29</em>, 7845–7860. (<a
href="https://doi.org/10.1109/TIP.2020.3007828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive study and evaluation of existing single image compression artifact removal algorithms using a new 4K resolution benchmark. This benchmark is called the Large-Scale Ideal Ultra high-definition 4K (LIU4K), and it includes including diversified foreground objects and background scenes with rich structures. Compression artifact removal, as a common post-processing technique, aims at alleviating undesirable artifacts, such as blockiness, ringing, and banding caused by quantization and approximation in the compression process. In this work, a systematic listing of the reviewed methods is presented based on their basic models (handcrafted models and deep networks). The main contributions and novelties of these methods are highlighted, and the main development directions are summarized, including architectures, multi-domain sources, signal structures, and new targeted units. Furthermore, based on a unified deep learning configuration ( i.e. same training data, loss function, optimization algorithm, etc. ), we evaluate recent deep learning-based methods based on diversified evaluation measures. The experimental results show state-of-the-art performance comparisons of existing methods based on both full-reference, non-reference, and task-driven metrics. Our survey gives a comprehensive reference source for future research on single image compression artifact removal and inspires new directions in related fields.},
  archive      = {J_TIP},
  author       = {Jiaying Liu and Dong Liu and Wenhan Yang and Sifeng Xia and Xiaoshuai Zhang and Yuanying Dai},
  doi          = {10.1109/TIP.2020.3007828},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7845-7860},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A comprehensive benchmark for single image compression artifact reduction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative unsupervised domain adaptation for medical
image diagnosis. <em>TIP</em>, <em>29</em>, 7834–7844. (<a
href="https://doi.org/10.1109/TIP.2020.3006377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based medical image diagnosis has shown great potential in clinical medicine. However, it often suffers two major difficulties in real-world applications: 1) only limited labels are available for model training, due to expensive annotation costs over medical images; 2) labeled images may contain considerable label noise ( e.g., mislabeling labels) due to diagnostic difficulties of diseases. To address these, we seek to exploit rich labeled data from relevant domains to help the learning in the target task via Unsupervised Domain Adaptation (UDA). Unlike most UDA methods that rely on clean labeled data or assume samples are equally transferable, we innovatively propose a Collaborative Unsupervised Domain Adaptation algorithm, which conducts transferability-aware adaptation and conquers label noise in a collaborative way. We theoretically analyze the generalization performance of the proposed method, and also empirically evaluate it on both medical and general images. Promising experimental results demonstrate the superiority and generalization of the proposed method.},
  archive      = {J_TIP},
  author       = {Yifan Zhang and Ying Wei and Qingyao Wu and Peilin Zhao and Shuaicheng Niu and Junzhou Huang and Mingkui Tan},
  doi          = {10.1109/TIP.2020.3006377},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7834-7844},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Collaborative unsupervised domain adaptation for medical image diagnosis},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end single image fog removal using enhanced cycle
consistent adversarial networks. <em>TIP</em>, <em>29</em>, 7819–7833.
(<a href="https://doi.org/10.1109/TIP.2020.3007844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image defogging is a classical and challenging problem in computer vision. Existing methods towards this problem mainly include handcrafted priors based methods that rely on the use of the atmospheric degradation model and learning-based approaches that require paired fog-fogfree training example images. In practice, however, prior-based methods are prone to failure due to their own limitations and paired training data are extremely difficult to acquire. Moreover, there are few studies on the unpaired trainable defogging network in this field. Thus, inspired by the principle of CycleGAN network, we have developed an end-to-end learning system that uses unpaired fog and fogfree training images, adversarial discriminators and cycle consistency losses to automatically construct a fog removal system. Similar to CycleGAN, our system has two transformation paths; one maps fog images to a fogfree image domain and the other maps fogfree images to a fog image domain. Instead of one stage mapping, our system uses a two stage mapping strategy in each transformation path to enhance the effectiveness of fog removal. Furthermore, we make explicit use of prior knowledge in the networks by embedding the atmospheric degradation principle and a sky prior for mapping fogfree images to the fog images domain. In addition, we also contribute the first real world nature fog-fogfree image dataset for defogging research. Our multiple real fog images dataset (MRFID) contains images of 200 natural outdoor scenes. For each scene, there is one clear image and corresponding four foggy images of different fog densities manually selected from a sequence of images taken by a fixed camera over the course of one year. Qualitative and quantitative comparison against several state-of-the-art methods on both synthetic and real world images demonstrate that our approach is effective and performs favorably for recovering a clear image from a foggy image.},
  archive      = {J_TIP},
  author       = {Wei Liu and Xianxu Hou and Jiang Duan and Guoping Qiu},
  doi          = {10.1109/TIP.2020.3007844},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7819-7833},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {End-to-end single image fog removal using enhanced cycle consistent adversarial networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An object context integrated network for joint learning of
depth and optical flow. <em>TIP</em>, <em>29</em>, 7807–7818. (<a
href="https://doi.org/10.1109/TIP.2020.3007843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised depth prediction and optical flow estimation have achieved promising performance due to the advanced deep network architectures. Since the ground truths are difficult to be collected, many recent works try to learn the depth and flow in an unsupervised manner. However, existing methods only use features from convolutional layers or a simple aggregation of multi-level features to predict the depth and flow maps, which is insufficient to exploit context information. In this paper, we attempt to exploit object contextual information and investigate the effect of the object context for joint learning of depth and optical flow. Specifically, we present a novel combination of object context and the framework of joint learning depth and optical flow. Our proposed network can exploit and integrate the object context for both tasks by aggregating the context according to pair-wise similarities. Furthermore, we adopt the existing spatial pyramid network (SPN) to estimate the depth and flow in a coarse-to-fine strategy effectively. Given temporally adjacent stereo pairs, our network can be trained end-to-end in an unsupervised manner and can predict the depth and flow maps simultaneously. We conduct experiments on two publicly available datasets, KITTI2012 and KITTI2015. Our proposed approach yields comparable performance on both depth and flow tasks, compared to the recent deep learning-based approaches. Experimental results demonstrate that exploiting object contextual information is useful and beneficial for depth and optical flow estimation.},
  archive      = {J_TIP},
  author       = {Mingliang Zhai and Xuezhi Xiang and Ning Lv and Xiangdong Kong and Abdulmotaleb El Saddik},
  doi          = {10.1109/TIP.2020.3007843},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7807-7818},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An object context integrated network for joint learning of depth and optical flow},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mutual context network for jointly estimating egocentric
gaze and action. <em>TIP</em>, <em>29</em>, 7795–7806. (<a
href="https://doi.org/10.1109/TIP.2020.3007841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address two coupled tasks of gaze prediction and action recognition in egocentric videos by exploring their mutual context: the information from gaze prediction facilitates action recognition and vice versa. Our assumption is that during the procedure of performing a manipulation task, on the one hand, what a person is doing determines where the person is looking at. On the other hand, the gaze location reveals gaze regions which contain important and information about the undergoing action and also the non-gaze regions that include complimentary clues for differentiating some fine-grained actions. We propose a novel mutual context network (MCN) that jointly learns action-dependent gaze prediction and gaze-guided action recognition in an end-to-end manner. Experiments on multiple egocentric video datasets demonstrate that our MCN achieves state-of-the-art performance of both gaze prediction and action recognition. The experiments also show that action-dependent gaze patterns could be learned with our method.},
  archive      = {J_TIP},
  author       = {Yifei Huang and Minjie Cai and Zhenqiang Li and Feng Lu and Yoichi Sato},
  doi          = {10.1109/TIP.2020.3007841},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7795-7806},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Mutual context network for jointly estimating egocentric gaze and action},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A data-driven model-based regression applied to panchromatic
sharpening. <em>TIP</em>, <em>29</em>, 7779–7794. (<a
href="https://doi.org/10.1109/TIP.2020.3007824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion is growing interest in recent years, thanks to the huge amount of data acquired everyday by sensors on board of satellite platforms. The enhancement of the spatial resolution of a multispectral (MS) image through the use of a panchromatic (PAN) image, usually called pansharpening, is getting more and more relevant. In this work, we focus on the problem of the estimation of the injection coefficients that rule the enhancement of the spatial resolution of the MS image by properly adding the PAN details. In particular, a statistical analysis of the residuals coming from the linear multivariate regression between details extracted from the PAN image and the MS image is performed. A novel hybrid model is introduced for accurately describing the statistical distribution of these residuals, together with a procedure for efficiently estimating both the parameters of the residual distribution and the injection coefficients. The improvements achieved by the proposed approach are assessed using two very high resolution datasets acquired by the WorldView-3 and Worldview-4 satellites. The benefits of the proposed approach are particularly clear when vegetated areas are involved in the fusion process.},
  archive      = {J_TIP},
  author       = {Paolo Addesso and Gemine Vivone and Rocco Restaino and Jocelyn Chanussot},
  doi          = {10.1109/TIP.2020.3007824},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7779-7794},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A data-driven model-based regression applied to panchromatic sharpening},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). DID: Disentangling-imprinting-distilling for continuous
low-shot detection. <em>TIP</em>, <em>29</em>, 7765–7778. (<a
href="https://doi.org/10.1109/TIP.2020.3006397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical applications often face a challenging continuous low-shot detection scenario, where a target detection task only has a few annotated training images, and a number of such new tasks come in sequence. To address this challenge, we propose a generic detection scheme via Disentangling-Imprinting-Distilling (DID). DID can leverage delicate transfer insights into the main development flow of deep learning, i.e., architecture design (Disentangling), model initialization (Imprinting), and training methodology (Distilling). This allows DID to be a simple but effective solution for continuous low-shot detection. In addition, DID can integrate the supervision from different detection tasks into a progressive learning procedure. As a result, one can efficiently adapt the previous detector for a new low-shot task, while maintaining the learned detection knowledge in the history. Finally, we evaluate our DID on a number of challenging settings in continuous/incremental low-shot detection. All the results demonstrate that our DID outperforms the recent state-of-the-art approaches. The code and models are available at https://github.com/chenxy99/DID.},
  archive      = {J_TIP},
  author       = {Xianyu Chen and Yali Wang and Jianzhuang Liu and Yu Qiao},
  doi          = {10.1109/TIP.2020.3006397},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7765-7778},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DID: Disentangling-imprinting-distilling for continuous low-shot detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SABER: A systems approach to blur estimation and reduction
in x-ray imaging. <em>TIP</em>, <em>29</em>, 7751–7764. (<a
href="https://doi.org/10.1109/TIP.2020.3006339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blur in X-ray radiographs not only reduces the sharpness of image edges but also reduces the overall contrast. The effective blur in a radiograph is the combined effect of blur from multiple sources such as the detector panel, X-ray source spot, and system motion. In this paper, we use a systems approach to model the point spread function (PSF) of the effective radiographic blur as the convolution of multiple PSFs, where each PSF models one of the various sources of blur. In particular, we model the combined contribution of X-ray source and detector blurs while assuming negligible contribution from other forms of blur. Then, we present a numerical optimization algorithm for estimating the source and detector PSFs from multiple radiographs acquired at different X-ray source to object (SOD) and object to detector distances (ODD). Finally, we computationally reduce blur in radiographs using deblurring algorithms that use the estimated PSFs from the previous step. Our approach to estimate and reduce blur is called SABER, which is an acronym for systems approach to blur estimation and reduction.},
  archive      = {J_TIP},
  author       = {K. Aditya Mohan and Robert M. Panas and Jefferson A. Cuadra},
  doi          = {10.1109/TIP.2020.3006339},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7751-7764},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SABER: A systems approach to blur estimation and reduction in X-ray imaging},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image restoration using joint patch-group-based sparse
representation. <em>TIP</em>, <em>29</em>, 7735–7750. (<a
href="https://doi.org/10.1109/TIP.2020.3005515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation has achieved great success in various image processing and computer vision tasks. For image processing, typical patch-based sparse representation (PSR) models usually tend to generate undesirable visual artifacts, while group-based sparse representation (GSR) models lean to produce over-smooth effects. In this paper, we propose a new sparse representation model, termed joint patch-group based sparse representation (JPG-SR). Compared with existing sparse representation models, the proposed JPG-SR provides an effective mechanism to integrate the local sparsity and nonlocal self-similarity of images. We then apply the proposed JPG-SR to image restoration tasks, including image inpainting and image deblocking. An iterative algorithm based on the alternating direction method of multipliers (ADMM) framework is developed to solve the proposed JPG-SR based image restoration problems. Experimental results demonstrate that the proposed JPG-SR is effective and outperforms many state-of-the-art methods in both objective and perceptual quality.},
  archive      = {J_TIP},
  author       = {Zhiyuan Zha and Xin Yuan and Bihan Wen and Jiachao Zhang and Jiantao Zhou and Ce Zhu},
  doi          = {10.1109/TIP.2020.3005515},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7735-7750},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image restoration using joint patch-group-based sparse representation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bag of color features for color constancy. <em>TIP</em>,
<em>29</em>, 7722–7734. (<a
href="https://doi.org/10.1109/TIP.2020.3004921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel color constancy approach, called Bag of Color Features (BoCF), building upon Bag-of-Features pooling. The proposed method substantially reduces the number of parameters needed for illumination estimation. At the same time, the proposed method is consistent with the color constancy assumption stating that global spatial information is not relevant for illumination estimation and local information (edges, etc.) is sufficient. Furthermore, BoCF is consistent with color constancy statistical approaches and can be interpreted as a learning-based extension of many statistical approaches. To further improve the illumination estimation accuracy, we propose a novel attention mechanism for the BoCF model with two variants based on self-attention. BoCF approach and its variants achieve competitive, compared to the state of the art, results while requiring much fewer parameters on three benchmark datasets: ColorChecker RECommended, INTEL-TUT version 2, and NUS8.},
  archive      = {J_TIP},
  author       = {Firas Laakom and Nikolaos Passalis and Jenni Raitoharju and Jarno Nikkanen and Anastasios Tefas and Alexandros Iosifidis and Moncef Gabbouj},
  doi          = {10.1109/TIP.2020.3004921},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7722-7734},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bag of color features for color constancy},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Double graph regularized double dictionary learning for
image classification. <em>TIP</em>, <em>29</em>, 7707–7721. (<a
href="https://doi.org/10.1109/TIP.2020.3004246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel double graph regularized double dictionary learning (DGRDDL) method for image classification. The proposed method jointly constructs a number of class-specific sub-dictionaries to capture the most discriminative features (class-specific information) of each class, and a class-shared dictionary to model the common patterns (class-shared information) shared by the images from different classes. A novel double graph regularization is proposed to correctly represent and differentiate these two types of information. Specifically, an intra-class similarity graph constraint is imposed on the representation coefficients over the class-specific dictionaries, and an inter-class similarity graph constraint is applied on the representation coefficients over the class-shared dictionary. In this way, the representations learned by the proposed DGRDDL method can correctly model the local similarity relationships of the class-specific and the class-shared information in images, respectively. Moreover, due to the differences between the intra-class and inter-class similarity graphs, the two types of information can be appropriately separated and captured by the learned dictionaries. We evaluate the performance of the proposed method on six public datasets and compared against those of seven benchmark methods. The experimental results demonstrate the effectiveness and superiority of the proposed method in image classification over the benchmark dictionary learning methods.},
  archive      = {J_TIP},
  author       = {Yi Rong and Shengwu Xiong and Yongsheng Gao},
  doi          = {10.1109/TIP.2020.3004246},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7707-7721},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Double graph regularized double dictionary learning for image classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time quality assessment of pediatric MRI via
semi-supervised deep nonlocal residual neural networks. <em>TIP</em>,
<em>29</em>, 7697–7706. (<a
href="https://doi.org/10.1109/TIP.2020.2992079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce an image quality assessment (IQA) method for pediatric T1- and T2-weighted MR images. IQA is first performed slice-wise using a nonlocal residual neural network (NR-Net) and then volume-wise by agglomerating the slice QA results using random forest. Our method requires only a small amount of quality-annotated images for training and is designed to be robust to annotation noise that might occur due to rater errors and the inevitable mix of good and bad slices in an image volume. Using a small set of quality-assessed images, we pre-train NR-Net to annotate each image slice with an initial quality rating (i.e., pass, questionable, fail), which we then refine by semi-supervised learning and iterative self-training. Experimental results demonstrate that our method, trained using only samples of modest size, exhibit great generalizability, capable of real-time (milliseconds per volume) large-scale IQA with near-perfect accuracy.},
  archive      = {J_TIP},
  author       = {Siyuan Liu and Kim-Han Thung and Weili Lin and Pew-Thian Yap and Dinggang Shen},
  doi          = {10.1109/TIP.2020.2992079},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7697-7706},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real-time quality assessment of pediatric MRI via semi-supervised deep nonlocal residual neural networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal target detection by sparse coding: Application to
paint loss detection in paintings. <em>TIP</em>, <em>29</em>, 7681–7696.
(<a href="https://doi.org/10.1109/TIP.2020.3005520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation based methods have demonstrated their superior performance in target detection tasks compared to more traditional approaches such as matched subspace detectors and adaptive subspace detectors. However, the existing sparsity-based target detection methods were mostly formulated for and validated on a single imaging modality (sometimes with multiple spectral bands). In many application domains, including art investigation, multimodal data, acquired by different sensors are readily available, and yet, efficient processing techniques for such data are still scarce. In this paper, we propose a sparsity-based multimodal target detection method that processes jointly the information from multiple imaging modalities in a kernel feature space, and making use of the spatial context. We develop our target detector such to be robust to errors in labelled data, which is especially important in applications like digital painting analysis, where pixel-wise manual annotations are unreliable. We apply the proposed method to a challenging application of paint loss detection in master paintings and we demonstrate its effectiveness on a case study with multimodal acquisitions of the Ghent Altarpiece.},
  archive      = {J_TIP},
  author       = {Shaoguang Huang and Bruno Cornelis and Bart Devolder and Maximiliaan Martens and Aleksandra Pizurica},
  doi          = {10.1109/TIP.2020.3005520},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7681-7696},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multimodal target detection by sparse coding: Application to paint loss detection in paintings},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rain o’er me: Synthesizing real rain to derain with data
distillation. <em>TIP</em>, <em>29</em>, 7668–7680. (<a
href="https://doi.org/10.1109/TIP.2020.3005517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a weakly-supervised technique for learning to remove rain from images without using synthetic rain software. The method is based on a two-stage data distillation approach, which requires only some unpaired rainy and clean images to generate supervision. First, a rainy image is paired with a coarsely derained version using on a simple filtering technique (“rain-to-clean”). Then a clean image is randomly matched with the rainy soft-labeled pair. Through a shared deep neural network, the rain that is removed from the first image is then added to the clean image to generate a second pair (“clean-to-rain”). The neural network simultaneously learns to map both images such that high resolution structure in the clean images can inform the deraining of the rainy images. Demonstrations show that this approach can address those visual characteristics of rain not easily synthesized by software in the usual way.},
  archive      = {J_TIP},
  author       = {Huangxing Lin and Yanlong Li and Xueyang Fu and Xinghao Ding and Yue Huang and John Paisley},
  doi          = {10.1109/TIP.2020.3005517},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7668-7680},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rain o’er me: Synthesizing real rain to derain with data distillation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biased mixtures of experts: Enabling computer vision
inference under data transfer limitations. <em>TIP</em>, <em>29</em>,
7656–7667. (<a href="https://doi.org/10.1109/TIP.2020.3005508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel mixture-of-experts class to optimize computer vision models in accordance with data transfer limitations at test time. Our approach postulates that the minimum acceptable amount of data allowing for highly-accurate results can vary for different input space partitions. Therefore, we consider mixtures where experts require different amounts of data, and train a sparse gating function to divide the input space for each expert. By appropriate hyperparameter selection, our approach is able to bias mixtures of experts towards selecting specific experts over others. In this way, we show that the data transfer optimization between visual sensing and processing can be solved as a convex optimization problem. To demonstrate the relation between data availability and performance, we evaluate biased mixtures on a range of mainstream computer vision problems, namely: (i) single shot detection, (ii) image super resolution, and (iii) realtime video action classification. For all cases, and when experts constitute modified baselines to meet different limits on allowed data utility, biased mixtures significantly outperform previous work optimized to meet the same constraints on available data.},
  archive      = {J_TIP},
  author       = {Alhabib Abbas and Yiannis Andreopoulos},
  doi          = {10.1109/TIP.2020.3005508},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7656-7667},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Biased mixtures of experts: Enabling computer vision inference under data transfer limitations},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Learning symmetry consistent deep CNNs for face completion.
<em>TIP</em>, <em>29</em>, 7641–7655. (<a
href="https://doi.org/10.1109/TIP.2020.3005241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional networks (CNNs) have achieved great success in face completion to generate plausible facial structures. These methods, however, are limited in maintaining global consistency among face components and recovering fine facial details. On the other hand, reflectional symmetry is a prominent property of face images and benefits face analysis and consistency modeling, yet remaining uninvestigated in deep face completion. In this work, we leverage two kinds of symmetry-enforcing modules to form a symmetry-consistent CNN model (i.e., SymmFCNet) for effective face completion. For missing pixels on only one of the half-faces, an illumination-reweighted warping subnet is developed to guide the warping and illumination reweighting of the other half-face. As for missing pixels on both of half-faces, we present a generative reconstruction subnet together with a perceptual symmetry loss to enforce symmetry consistency of recovered structures. The SymmFCNet is constructed by stacking generative reconstruction subnet upon illumination-reweighted warping subnet, and can be learned in an end-to-end manner. Experiments show that SymmFCNet can generate globally consistent results on images with synthetic and real occlusions, and performs favorably against state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Xiaoming Li and Guosheng Hu and Jieru Zhu and Wangmeng Zuo and Meng Wang and Lei Zhang},
  doi          = {10.1109/TIP.2020.3005241},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7641-7655},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning symmetry consistent deep CNNs for face completion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigating task-driven latent feasibility for nonconvex
image modeling. <em>TIP</em>, <em>29</em>, 7629–7640. (<a
href="https://doi.org/10.1109/TIP.2020.3004733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Properly modeling latent image distributions plays an important role in a variety of image-related vision problems. Most exiting approaches aim to formulate this problem as optimization models (e.g., Maximum A Posterior, MAP) with handcrafted priors. In recent years, different CNN modules are also considered as deep priors to regularize the image modeling process. However, these explicit regularization techniques require deep understandings on the problem and elaborately mathematical skills. In this work, we provide a new perspective, named Task-driven Latent Feasibility (TLF), to incorporate specific task information to narrow down the solution space for the optimization-based image modeling problem. Thanks to the flexibility of TLF, both designed and trained constraints can be embedded into the optimization process. By introducing control mechanisms based on the monotonicity and boundedness conditions, we can also strictly prove the convergence of our proposed inference process. We demonstrate that different types of image modeling problems, such as image deblurring and rain streaks removals, can all be appropriately addressed within our TLF framework. Extensive experiments also verify the theoretical results and show the advantages of our method against existing state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Risheng Liu and Pan Mu and Jian Chen and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TIP.2020.3004733},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7629-7640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Investigating task-driven latent feasibility for nonconvex image modeling},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatio-temporal memory attention for image captioning.
<em>TIP</em>, <em>29</em>, 7615–7628. (<a
href="https://doi.org/10.1109/TIP.2020.3004729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual attention has been successfully applied in image captioning to selectively incorporate the most relevant areas to the language generation procedure. However, the attention in current image captioning methods is only guided by the hidden state of language model, e.g. LSTM (Long-Short Term Memory), indirectly and implicitly, and thus the attended areas are weakly relevant at different time steps. Besides the spatial relationship of attention areas, the temporal relationship in attention is crucial for image captioning according to the attention transmission mechanism of human vision. In this paper, we propose a new spatio-temporal memory attention (STMA) model to learn the spatio-temporal relationship in attention for image captioning. The STMA introduces the memory mechanism to the attention model through a tailored LSTM, where the new cell is used to memorize and propagate the attention information, and the output gate is used to generate attention weights. The attention in STMA transmits with memory adaptively and dependently, which builds strong temporal connections of attentions and learns the spatio-temporal relationship of attended areas simultaneously. Besides, the proposed STMA is flexible to combine with attention-based image captioning frameworks. Experiments on MS COCO dataset demonstrate the superiority of the proposed STMA model in exploring the spatio-temporal relationship in attention and improving the current attention-based image captioning.},
  archive      = {J_TIP},
  author       = {Junzhong Ji and Cheng Xu and Xiaodan Zhang and Boyue Wang and Xinhang Song},
  doi          = {10.1109/TIP.2020.3004729},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7615-7628},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatio-temporal memory attention for image captioning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Just noticeable distortion-based perceptual rate control in
HEVC. <em>TIP</em>, <em>29</em>, 7603–7614. (<a
href="https://doi.org/10.1109/TIP.2020.3004714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a just noticeable distortion (JND)-based perceptual rate control method for high efficiency video coding (HEVC). First, the JND factor of a coding unit has been mathematically shown to be an approximation of the average pixel-level JND weight, which means that it can also be used as a weight for bitrate allocation. Second, rate-distortion (R-D) modelling is conducted based on the JND factor. Finally, the proposed R-D model is integrated into an existing rate control framework to improve the coding efficiency, and the proposed algorithm is implemented in the newest video coding standard. As the experimental results reveal, compared with HEVC reference software, our algorithm achieves significantly improved coding performance, subjective coding quality and bitrate accuracy.},
  archive      = {J_TIP},
  author       = {Mingliang Zhou and Xuekai Wei and Sam Kwong and Weijia Jia and Bin Fang},
  doi          = {10.1109/TIP.2020.3004714},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7603-7614},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Just noticeable distortion-based perceptual rate control in HEVC},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous reconstruction and moving object detection from
compressive sampled surveillance videos. <em>TIP</em>, <em>29</em>,
7590–7602. (<a href="https://doi.org/10.1109/TIP.2020.3004696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatially distributed digital cameras in Wireless Multimedia Sensor Networks (WMSN) are provided with miniature batteries resulting in power constraints. These cameras acquire compressive measurements of video at a rate significantly below the Nyquist rate and transmit them wirelessly in the network. Thus, the encoding side (transmitter) is made less complex at the expense of increased complexity at the more resourceful decoder (receiver). Well grounded on this relevant practical scenario, a unified system is proposed that integrates detection of moving objects into the Compressive Sensing (CS) recovery framework and thereby realizing simultaneous data recovery and object detection in a single optimization problem which guarantees fast response. In this work, a new tensor RPCA approach is proposed to accomplish this requirement. For background separation, low rank approximation is done on the highly correlated background components. A Laplace function based surrogate for tensor tubal rank is formulated to provide adaptive thresholding for the singular value tubes of the background tensor. Moreover, the spatio-temporal continuity of the foreground is explored using 3D-Piecewise Smoothness Constraints combinations based Anisotropic Total Variation (3D-PSCATV) regularization. Additionally, $l_{1}$ regularization has been adopted to describe the sparsity of moving objects. The proposed model is solved using Alternative Direction Method of Multipliers (ADMM) scheme. The quantitative and qualitative results validate the superior performance of the proposed method against the compared approaches.},
  archive      = {J_TIP},
  author       = {Anju Jose Tom and Sudhish N. George},
  doi          = {10.1109/TIP.2020.3004696},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7590-7602},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Simultaneous reconstruction and moving object detection from compressive sampled surveillance videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-grained spatial alignment model for person
re-identification with focal triplet loss. <em>TIP</em>, <em>29</em>,
7578–7589. (<a href="https://doi.org/10.1109/TIP.2020.3004267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances of person re-identification have well advocated the usage of human body cues to boost performance. However, most existing methods still retain on exploiting a relatively coarse-grained local information. Such information may include redundant backgrounds that are sensitive to the apparently similar persons when facing challenging scenarios like complex poses, inaccurate detection, occlusion and misalignment. In this paper we propose a novel Fine-Grained Spatial Alignment Model (FGSAM) to mine fine-grained local information to handle the aforementioned challenge effectively. In particular, we first design a pose resolve net with channel parse blocks (CPB) to extract pose information in pixel-level. This network allows the proposed model to be robust to complex pose variations while suppressing the redundant backgrounds caused by inaccurate detection and occlusion. Given the extracted pose information, a locally reinforced alignment mode is further proposed to address the misalignment problem between different local parts by considering different local parts along with attribute information in a fine-grained way. Finally, a focal triplet loss is designed to effectively train the entire model, which imposes a constraint on the intra-class and an adaptively weight adjustment mechanism to handle the hard sample problem. Extensive evaluations and analysis on Market1501, DukeMTMC-reid and PETA datasets demonstrate the effectiveness of FGSAM in coping with the problems of misalignment, occlusion and complex poses.},
  archive      = {J_TIP},
  author       = {Qinqin Zhou and Bineng Zhong and Xiangyuan Lan and Gan Sun and Yulun Zhang and Baochang Zhang and Rongrong Ji},
  doi          = {10.1109/TIP.2020.3004267},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7578-7589},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fine-grained spatial alignment model for person re-identification with focal triplet loss},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). FusionNet: An unsupervised convolutional variational
network for hyperspectral and multispectral image fusion. <em>TIP</em>,
<em>29</em>, 7565–7577. (<a
href="https://doi.org/10.1109/TIP.2020.3004261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to hardware limitations of the imaging sensors, it is challenging to acquire images of high resolution in both spatial and spectral domains. Fusing a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI) to obtain an HR-HSI in an unsupervised manner has drawn considerable attention. Though effective, most existing fusion methods are limited due to the use of linear parametric modeling for the spectral mixture process, and even the deep learning-based methods only focus on deterministic fully-connected networks without exploiting the spatial correlation and local spectral structures of the images. In this paper, we propose a novel variational probabilistic autoencoder framework implemented by convolutional neural networks, in order to fuse the spatial and spectral information contained in the LR-HSI and HR-MSI, called FusionNet. The FusionNet consists of a spectral generative network, a spatial-dependent prior network, and a spatial-spectral variational inference network, which are jointly optimized in an unsupervised manner, leading to an end-to-end fusion system. Further, for fast adaptation to different observation scenes, we give a meta-learning explanation to the fusion problem, and combine the FusionNet with meta-learning in a synergistic manner. Effectiveness and efficiency of the proposed method are evaluated based on several publicly available datasets, demonstrating that the proposed FusionNet outperforms the state-of-the-art fusion methods.},
  archive      = {J_TIP},
  author       = {Zhengjue Wang and Bo Chen and Ruiying Lu and Hao Zhang and Hongwei Liu and Pramod K. Varshney},
  doi          = {10.1109/TIP.2020.3004261},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7565-7577},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FusionNet: An unsupervised convolutional variational network for hyperspectral and multispectral image fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MetaSearch: Incremental product search via deep
meta-learning. <em>TIP</em>, <em>29</em>, 7549–7564. (<a
href="https://doi.org/10.1109/TIP.2020.3004249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of image processing and computer vision technology, content-based product search is applied in a wide variety of common tasks, such as online shopping, automatic checkout systems, and intelligent logistics. Given a product image as a query, existing product search systems mainly perform the retrieval process using predefined databases with fixed product categories. However, real-world applications often require inserting new categories or updating existing products in the product database. When using existing product search methods, the image feature extraction models must be retrained and database indexes must be rebuilt to accommodate the updated data, and these operations incur high costs for data annotation and training time. To this end, we propose a few-shot incremental product search framework with meta-learning, which requires very few annotated images and has a reasonable training time. In particular, our framework contains a multipooling-based product feature extractor that learns a discriminative representation for each product, and we also design a meta-learning-based feature adapter to guarantee the robustness of the few-shot features. Furthermore, when expanding new categories in batches during a product search, we reconstruct the few-shot features by using an incremental weight combiner to accommodate the incremental search task. Through extensive experiments, we demonstrate that the proposed framework achieves excellent performance for new products while still guaranteeing the high search accuracy of the base categories after gradually expanding new product categories without forgetting.},
  archive      = {J_TIP},
  author       = {Qi Wang and Xinchen Liu and Wu Liu and An-An Liu and Wenyin Liu and Tao Mei},
  doi          = {10.1109/TIP.2020.3004249},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7549-7564},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MetaSearch: Incremental product search via deep meta-learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised texture filtering with shallow to deep
understanding. <em>TIP</em>, <em>29</em>, 7537–7548. (<a
href="https://doi.org/10.1109/TIP.2020.3004043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposed a semi-supervised method for automatic texture filtering. Our method leveraged a limited amount of labeled data and a large amount of unlabeled data to train Generative Adversarial Networks (GANs). Separate loss functions were designed for both labeled and unlabeled datasets. Our main contribution is the introduction of knowledge extracted from shallow and deep layers in neural networks. Loss defined within shallow layers preserves the edge, while loss defined within the deep layers identifies the semantic content and conversely removes the small-scale texture variations. This contribution directly addresses the major challenge for texture filtering, distinguishing the structural content from non-structural textures at the pixel level. The extracted information, in our study, improved the content and color consistency before and after the process of filtering, for unlabeled samples in particular. The proposed method offers twofold benefits: first, significant reductions in the amounts of time and effort expended in reconstructing the labeled dataset, especially given the delicate operations required at the pixel level; second, a reduction in over-fitting, in supervised learning with a small amount of labeled data, by utilizing a large amount of unlabeled data. The results confirm that our method can perform comparably with non-learning-based methods, alleviating the demand for the determination of optimal parameter values.},
  archive      = {J_TIP},
  author       = {Xing Gao and Xu Wu and Panpan Xu and Shihui Guo and Minghong Liao and Wencheng Wang},
  doi          = {10.1109/TIP.2020.3004043},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7537-7548},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised texture filtering with shallow to deep understanding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contrast enhancement using novel white balancing parameter
optimization for perceptually invisible images. <em>TIP</em>,
<em>29</em>, 7525–7536. (<a
href="https://doi.org/10.1109/TIP.2020.3004036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel white balancing algorithm is proposed in this paper to automatically enhance the global contrast degraded imperceptible images. The technique is applied on four publicly available image dataset, CSIQ, KADID, TID and SIPI. Colour images consist of three channels viz. Red, Blue and Green. A contrast degraded colour image visually appears similar to an image with one or more distorted channel. 12 images are obtained by enhancing one channel of the contrast degraded image at the cost of other channel using White Balancing algorithm. Four images with best quantitative performance metrics, visual similarity index (VSI), gradient magnitude similarity index (GMSD), patch-based contrast quality index (PCQI) and peak signal-to-noise ratio (PSNR) determines the pair of weak and prominent channels. An optimization algorithm then enhances these channels and the image with the best quantitative performance metrics is chosen as the enhanced image. Quantitative and qualitative results demonstrate that the proposed method produces an enhanced image with superior perceptual quality, and gives the best average results for all the parameters across every dataset as compared to the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Mohit Kumar and Ashish Kumar Bhandari},
  doi          = {10.1109/TIP.2020.3004036},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7525-7536},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Contrast enhancement using novel white balancing parameter optimization for perceptually invisible images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep joint deinterlacing and denoising for single shot
dual-ISO HDR reconstruction. <em>TIP</em>, <em>29</em>, 7511–7524. (<a
href="https://doi.org/10.1109/TIP.2020.3004014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HDR images have traditionally been obtained by merging multiple exposures each captured with a different exposure time. However, this approach entails longer capture times and necessitates deghosting if the captured scene contains moving objects. With the advent of modern camera sensors that can perform per-pixel exposure modulation, it is now possible to capture all of the required exposures within a single shot. The new challenge then becomes how to best combine different pixels with different exposure values into a single full-resolution and low-noise HDR image. We propose a joint multi-exposure frame deinterlacing and denoising algorithm powered by deep convolutional neural networks (DCNN). In our algorithm, we first train two DCNNs, with one tuned for reconstructing low exposures and the other for high exposures. Each DCNN takes the same mosaicked dual-ISO input image and outputs either the low exposure or high exposure depending on the type of the network. The resulting exposures can be demosaicked and converted to the desired target color space prior to HDR assembly. Our evaluations indicate that the quality of our results significantly surpasses the state-of-the-art in single-image HDR reconstruction algorithms.},
  archive      = {J_TIP},
  author       = {Uğur çoğalan and Ahmet Oğuz Akyüz},
  doi          = {10.1109/TIP.2020.3004014},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7511-7524},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep joint deinterlacing and denoising for single shot dual-ISO HDR reconstruction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task decomposition and synchronization for semantic
biomedical image segmentation. <em>TIP</em>, <em>29</em>, 7497–7510. (<a
href="https://doi.org/10.1109/TIP.2020.3003735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is essentially important to biomedical image analysis. Many recent works mainly focus on integrating the Fully Convolutional Network (FCN) architecture with sophisticated convolution implementation and deep supervision. Such complex networks need large training datasets, a requirement which is challenging for medical image analysis. In this paper, we propose to decompose the single segmentation task into three subsequent sub-tasks, including (1) pixel-wise image semantic segmentation, (2) prediction of the instance class labels of the objects within the image, and (3) classification of the scene the image belonging to. While these three sub-tasks are trained to optimize their individual loss functions at different perceptual levels, we propose to allow their interaction within the task-task context ensemble. Moreover, we propose a novel sync-regularization to penalize the deviation between the outputs of the pixel-wise semantic segmentation and the instance class prediction tasks. These effective regularizations help FCN utilize context information comprehensively and attain accurate segmentation, even though the number of images for training may be limited in many biomedical applications. We have successfully applied our framework to three diverse 2D/3D medical image datasets, including Robotic Scene Segmentation Challenge 18 (ROBOT18), Brain Tumor Segmentation Challenge 18 (BRATS18), and Retinal Fundus Glaucoma Challenge (REFUGE18). We have achieved outperformed or comparable performance in all the three challenges. Our code, typical data and trained models are available at https://github.com/xuhuaren/TDSNet .},
  archive      = {J_TIP},
  author       = {Xuhua Ren and Sahar Ahmad and Lichi Zhang and Lei Xiang and Dong Nie and Fan Yang and Qian Wang and Dinggang Shen},
  doi          = {10.1109/TIP.2020.3003735},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7497-7510},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Task decomposition and synchronization for semantic biomedical image segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerate CTU partition to real time for HEVC encoding with
complexity control. <em>TIP</em>, <em>29</em>, 7482–7496. (<a
href="https://doi.org/10.1109/TIP.2020.3003730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, extensive approaches have been proposed for reducing the encoding complexity of high efficiency video coding, by predicting the coding tree unit partition using deep neural networks. However, these approaches cannot work in real time due to the complexity of the network architectures. In this paper, we propose a network pruning approach to accelerate a state-of-the-art deep neural network model, for real-time coding tree unit partition. Specifically, we first investigate the computational complexity throughout the network, and find that most calculations can be simplified by pruning the weight parameters. Considering that the number of weight parameters drastically differs by network layer and partition level, we design an adaptive pruning scheme by applying a well-suitable retention ratio of weight parameters to each layer at a level. The retention ratio indicates the ratio of weight parameters after and before pruning. By varying the retention ratios, we can obtain several accelerated network models with different levels of complexity. We further propose a complexity control algorithm by applying different accelerated models to different coding tree units, to ensure that the actual encoding complexity is close to a given target. To guarantee the rate-distortion performance, we model the complexity control algorithm as a convex optimization problem, and we can obtain a closed-form solution. Experimental results show that our approach can accelerate the original deep neural network model by 17-20 times, with little expense on the Bjøntegaard delta bit-rate. For complexity control, we achieve high control accuracy with a control error of less than 2\% for most video sequences.},
  archive      = {J_TIP},
  author       = {Tianyi Li and Mai Xu and Xin Deng and Liquan Shen},
  doi          = {10.1109/TIP.2020.3003730},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7482-7496},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Accelerate CTU partition to real time for HEVC encoding with complexity control},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improve person re-identification with part awareness
learning. <em>TIP</em>, <em>29</em>, 7468–7481. (<a
href="https://doi.org/10.1109/TIP.2020.3003442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) aims to predict whether two images from different cameras belong to the same person. Due to low image quality and variance in view point and body pose, it remains a difficult task. To solve the task, a model is supposed to appropriately capture features that describe body regions for identification. With the simple intuition that explicitly incorporating ReID model with part awareness could be beneficial for learning a more discriminative feature space, we propose part segmentation as an assistant body perception task during the training of a ReID model. Specifically, we add a lightweight segmentation head to the backbone of ReID model during training, which is supervised with part labels. Note that our segmentation head is only introduced during training and that it does not change network input or the way of extracting ReID feature. Experiments show that part segmentation considerably improves the performance of ReID. Through quantitative and qualitative analyses, we further reveal that body part perception helps ReID model to capture a set of more diverse features from the body, with decreased similarity between part features and increased focus on different body regions. We experiment with various representative ReID models and achieve consistent improvement on several large-scale datasets including Market1501, CUHK03, DukeMTMC-reID and MSMT17. E.g. on MSMT17, our method increases Rank-1 Accuracy of GlobalPool-ResNet-50, PCB and MGN by 2.3\%, 2.9\% and 3.9\%, respectively. Incorporated with MGN, our model achieves state-of-the-art performance, with Rank-1 Accuracy 95.8\%, 78.8\%, 90.0\% and 84.0\% on four datasets, respectively.},
  archive      = {J_TIP},
  author       = {Houjing Huang and Wenjie Yang and Jinbin Lin and Guan Huang and Jiamiao Xu and Guoli Wang and Xiaotang Chen and Kaiqi Huang},
  doi          = {10.1109/TIP.2020.3003442},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7468-7481},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improve person re-identification with part awareness learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scripted video generation with a bottom-up generative
adversarial network. <em>TIP</em>, <em>29</em>, 7454–7467. (<a
href="https://doi.org/10.1109/TIP.2020.3003227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating videos given a text description (such as a script) is non-trivial due to the intrinsic complexity of image frames and the structure of videos. Although Generative Adversarial Networks (GANs) have been successfully applied to generate images conditioned on a natural language description, it is still very challenging to generate realistic videos in which the frames are required to follow both spatial and temporal coherence. In this paper, we propose a novel Bottom-up GAN (BoGAN) method for generating videos given a text description. To ensure the coherence of the generated frames and also make the whole video match the language descriptions semantically, we design a bottom-up optimisation mechanism to train BoGAN. Specifically, we devise a region-level loss via attention mechanism to preserve the local semantic alignment and draw details in different sub-regions of video conditioned on words which are most relevant to them. Moreover, to guarantee the matching between text and frame, we introduce a frame-level discriminator, which can also maintain the fidelity of each frame and the coherence across frames. Last, to ensure the global semantic alignment between whole video and given text, we apply a video-level discriminator. We evaluate the effectiveness of the proposed BoGAN on two synthetic datasets (i.e., SBMG and TBMG) and two real-world datasets (i.e., MSVD and KTH).},
  archive      = {J_TIP},
  author       = {Qi Chen and Qi Wu and Jian Chen and Qingyao Wu and Anton van den Hengel and Mingkui Tan},
  doi          = {10.1109/TIP.2020.3003227},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7454-7467},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scripted video generation with a bottom-up generative adversarial network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No reference quality assessment for 3D synthesized views by
local structure variation and global naturalness change. <em>TIP</em>,
<em>29</em>, 7443–7453. (<a
href="https://doi.org/10.1109/TIP.2020.3003218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth image based rendering (DIBR) has been widely used to generate different virtual viewpoints of the same scene from the new perspective. However, DIBR tends to introduce annoying artifacts including blurring, discontinuity, blocking, and stretching, etc.. Thus, to improve DIBR performance, it is important to accurately measure the visual quality of synthesized views. In this paper, we propose a novel and effective no reference (NR) quality assessment method for 3D synthesized views by local variation and global change (LVGC). More specifically, we firstly compute the Gaussian derivatives for the input image to extract structure and chromatic features. Then, we use the local binary pattern (LBP) operator to encode the structure and chromatic feature maps, which are used to calculate quality-aware features to measure the local structural and chromatic distortion. Besides, we extract luminance features by global change to evaluate the naturalness of 3D synthesized views. With these extracted features, we utilize random forest regression (RFR) to train the quality prediction model from visual features to human ratings. Experimental results on three public benchmark databases demonstrate the effectiveness of our method on estimating visual quality of 3D synthesized views.},
  archive      = {J_TIP},
  author       = {Jiebin Yan and Yuming Fang and Rengang Du and Yan Zeng and Yifan Zuo},
  doi          = {10.1109/TIP.2020.3003218},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7443-7453},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No reference quality assessment for 3D synthesized views by local structure variation and global naturalness change},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PMBANet: Progressive multi-branch aggregation network for
scene depth super-resolution. <em>TIP</em>, <em>29</em>, 7427–7442. (<a
href="https://doi.org/10.1109/TIP.2020.3002664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth map super-resolution is an ill-posed inverse problem with many challenges. First, depth boundaries are generally hard to reconstruct particularly at large magnification factors. Second, depth regions on fine structures and tiny objects in the scene are destroyed seriously by downsampling degradation. To tackle these difficulties, we propose a progressive multi-branch aggregation network (PMBANet), which consists of stacked MBA blocks to fully address the above problems and progressively recover the degraded depth map. Specifically, each MBA block has multiple parallel branches: 1) The reconstruction branch is proposed based on the designed attention-based error feed-forward/-back modules, which iteratively exploits and compensates the downsampling errors to refine the depth map by imposing the attention mechanism on the module to gradually highlight the informative features at depth boundaries. 2) We formulate a separate guidance branch as prior knowledge to help to recover the depth details, in which the multi-scale branch is to learn a multi-scale representation that pays close attention at objects of different scales, while the color branch regularizes the depth map by using auxiliary color information. Then, a fusion block is introduced to adaptively fuse and select the discriminative features from all the branches. The design methodology of our whole network is well-founded, and extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in comparison with the state-of-the-art methods. Our code and models are available at https://github.com/Sunbaoli/PMBANet_DSR/ .},
  archive      = {J_TIP},
  author       = {Xinchen Ye and Baoli Sun and Zhihui Wang and Jingyu Yang and Rui Xu and Haojie Li and Baopu Li},
  doi          = {10.1109/TIP.2020.3002664},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7427-7442},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PMBANet: Progressive multi-branch aggregation network for scene depth super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end blind image quality prediction with cascaded deep
neural network. <em>TIP</em>, <em>29</em>, 7414–7426. (<a
href="https://doi.org/10.1109/TIP.2020.3002478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep convolutional neural network (CNN) has achieved great success in image recognition. Many image quality assessment (IQA) methods directly use recognition-oriented CNN for quality prediction. However, the properties of IQA task is different from image recognition task. Image recognition should be sensitive to visual content and robust to distortion, while IQA should be sensitive to both distortion and visual content. In this paper, an IQA-oriented CNN method is developed for blind IQA (BIQA), which can efficiently represent the quality degradation. CNN is large-data driven, while the sizes of existing IQA databases are too small for CNN optimization. Thus, a large IQA dataset is firstly established, which includes more than one million distorted images (each image is assigned with a quality score as its substitute of Mean Opinion Score (MOS), abbreviated as pseudo-MOS). Next, inspired by the hierarchical perception mechanism (from local structure to global semantics) in human visual system, a novel IQA-orientated CNN method is designed, in which the hierarchical degradation is considered. Finally, by jointly optimizing the multilevel feature extraction, hierarchical degradation concatenation (HDC) and quality prediction in an end-to-end framework, the Cascaded CNN with HDC (named as CaHDC) is introduced. Experiments on the benchmark IQA databases demonstrate the superiority of CaHDC compared with existing BIQA methods. Meanwhile, the CaHDC (with about 0.73M parameters) is lightweight comparing to other CNN-based BIQA models, which can be easily realized in the microprocessing system. The dataset and source code of the proposed method are available at https://web.xidian.edu.cn/wjj/paper.html.},
  archive      = {J_TIP},
  author       = {Jinjian Wu and Jupo Ma and Fuhu Liang and Weisheng Dong and Guangming Shi and Weisi Lin},
  doi          = {10.1109/TIP.2020.3002478},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7414-7426},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {End-to-end blind image quality prediction with cascaded deep neural network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressed image restoration via artifacts-free PCA basis
learning and adaptive sparse modeling. <em>TIP</em>, <em>29</em>,
7399–7413. (<a href="https://doi.org/10.1109/TIP.2020.3002452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visually unpleasant compression artifacts frequently appear in block-based transform coding, especially at low bit rates. This paper presents a new artifact reduction scheme based on Bayesian sparse modeling and artifacts-free PCA basis learning. To avoid the effect of blocking artifacts, we propose to learn artifacts-free PCA basis from clean images. We concatenate the clean patches and their compressed counterparts to learn paired distribution prior via the Gaussian Mixture Model (GMM). By this way, the GMM characterizes the mapping between the clean image and its compressed version. To restore a compressed patch, the best matched GMM component is assigned using the patch in the compressed image subspace. The artifacts-free PCA basis is obtained according to the mapping learned by the paired GMM. In practice, the statistical distributions of different sparse coefficients in different patches may dramatically vary with image contents. Instead of using a global zero-mean distribution for all coefficients, we propose to adaptively model the prior of each band in a Bayesian framework. The expectation and variance of each band are adaptively learned from the similar patches within the image. Thus, different transform bands are regularized unequally according to the learned priors. Experimental results show that the proposed scheme outperforms most of the compared schemes in terms of both objective quality and perceptual quality.},
  archive      = {J_TIP},
  author       = {Qiang Song and Ruiqin Xiong and Xiaopeng Fan and Dong Liu and Feng Wu and Tiejun Huang and Wen Gao},
  doi          = {10.1109/TIP.2020.3002452},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7399-7413},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Compressed image restoration via artifacts-free PCA basis learning and adaptive sparse modeling},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FoveaBox: Beyound anchor-based object detection.
<em>TIP</em>, <em>29</em>, 7389–7398. (<a
href="https://doi.org/10.1109/TIP.2020.3002345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. In FoveaBox, an instance is assigned to adjacent feature levels to make the model more accurate.We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO and Pascal VOC object detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. The code has been made publicly available at https://github.com/taokong/FoveaBox .},
  archive      = {J_TIP},
  author       = {Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},
  doi          = {10.1109/TIP.2020.3002345},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7389-7398},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FoveaBox: Beyound anchor-based object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Watershed-based superpixels with global and local boundary
marching. <em>TIP</em>, <em>29</em>, 7375–7388. (<a
href="https://doi.org/10.1109/TIP.2020.3002078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixels are widely used in computer vision applications, as they conserve the running costs of subsequent processing while preserving the original performance. In most of the existing algorithms, the boundary adherence and the compactness of superpixels are necessarily inter-inhibitive because the color/gradient information is balanced against the position constraints, and the set criteria define all pixels indiscriminately. In this paper, we present a two-phase superpixel segmentation method based on the watershed transformation. After designing a new approach for calculating the flooding priority, we propose a new strategy with two distinct criteria for global and local refinement of the boundary pixels. These criteria reduce the compromise between the boundary adherence and compactness. Unlike the indiscriminate standards, our method applies different treatments to pixels in different environments, preserving the color homogeneity in content-rich areas while improving the regularity of the superpixels in content-plain regions. The superior accuracy and computing time of our proposed method are verified in comparison experiments with several state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Ye Yuan and Zhiliang Zhu and Hai Yu and Wei Zhang},
  doi          = {10.1109/TIP.2020.3002078},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7375-7388},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Watershed-based superpixels with global and local boundary marching},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of affine motion-compensated prediction in video
coding. <em>TIP</em>, <em>29</em>, 7359–7374. (<a
href="https://doi.org/10.1109/TIP.2020.3001734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion-compensated prediction is used in video coding standards like High Efficiency Video Coding (HEVC) as one key element of data compression. Commonly, a purely translational motion model is employed. In order to also cover non-translational motion types like rotation or scaling (zoom), e. g. contained in aerial video sequences such as captured from unmanned aerial vehicles (UAV), an affine motion model can be applied. In this work, a model for affine motion-compensated prediction in video coding is derived. Using the rate-distortion theory and the displacement estimation error caused by inaccurate affine motion parameter estimation, the minimum required bit rate for encoding the prediction error is determined. In this model, the affine transformation parameters are assumed to be affected by statistically independent estimation errors, which all follow a zero-mean Gaussian distributed probability density function (pdf). The joint pdf of the estimation errors is derived and transformed into the pdfof the location-dependent displacement estimation error in the image. The latter is related to the minimum required bit rate for encoding the prediction error. Similar to the derivations of the fully affine motion model, a four-parameter simplified affine model is investigated. Both models are of particular interest since they are considered for the upcoming video coding standard Versatile Video Coding (VVC) succeeding HEVC. Both models provide valuable information about the minimum bit rate for encoding the prediction error as a function of affine estimation accuracies.},
  archive      = {J_TIP},
  author       = {Holger Meuel and Jörn Ostermann},
  doi          = {10.1109/TIP.2020.3001734},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7359-7374},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Analysis of affine motion-compensated prediction in video coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Jointly learning commonality and specificity dictionaries
for person re-identification. <em>TIP</em>, <em>29</em>, 7345–7358. (<a
href="https://doi.org/10.1109/TIP.2020.3001424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite advances in person re-identification (re-ID), it is still far from meeting the needs of real-world applications due to tremendous visual ambiguity in person appearance across cameras. To overcome this problem, we propose a person re-ID method by decomposing a pedestrian&#39;s appearance feature into different components. This method assumes that each pedestrian image is composed of person-shared components that reflect the similarities of different pedestrians and person-specific components that reflect unique identity information. Based on this assumption, we propose to reduce the ambiguity in visual features by removing person-shared components from pedestrian visual features. To this end, we develop a framework for learning a pair of commonality and specificity dictionaries, while introducing a distance constraint to force the particularities of the same person over the specificity dictionary to have the same coding coefficients and the coding coefficients of different pedestrians to have weak correlation. Furthermore, considering the similarity of the commonality dictionary and the sparsity of the specificity dictionary, low-rank and sparse regularization terms are introduced into the dictionary learning framework to improve their representation ability and discriminative ability. Extensive experimental results show that the proposed algorithm outperforms or is competitive with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Huafeng Li and Jiajia Xu and Zhengtao Yu and Jiebo Luo},
  doi          = {10.1109/TIP.2020.3001424},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7345-7358},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Jointly learning commonality and specificity dictionaries for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling generalized rate-distortion functions.
<em>TIP</em>, <em>29</em>, 7331–7344. (<a
href="https://doi.org/10.1109/TIP.2020.3001405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many multimedia applications require precise understanding of the rate-distortion characteristics measured by the function relating visual quality to media attributes, for which we term it the generalized rate-distortion (GRD) function. In this study, we explore the GRD behavior of compressed digital videos in a two-dimensional space of bitrate and resolution. Our analysis on a large-scale video dataset reveals that empirical parametric models are systematically biased while exhaustive search methods require excessive computation time to depict the GRD surfaces. By exploiting the properties that all GRD functions share, we develop an Robust Axial-Monotonic Clough-Tocher (RAMCT) interpolation method to model the GRD function. This model allows us to accurately reconstruct the complete GRD function of a source video content from a moderate number of measurements. To further reduce the computational cost, we present a novel sampling scheme based on a probabilistic model and an information measure. The proposed sampling method constructs a sequence of quality queries by minimizing the overall informativeness in the remaining samples. Experimental results show that the proposed algorithm significantly outperforms state-of-the-art approaches in accuracy and efficiency. Finally, we demonstrate the usage of the proposed model in three applications: rate-distortion curve prediction, per-title encoding profile generation, and video encoder comparison.},
  archive      = {J_TIP},
  author       = {Zhengfang Duanmu and Wentao Liu and Zhuoran Li and Zhou Wang},
  doi          = {10.1109/TIP.2020.3001405},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7331-7344},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Modeling generalized rate-distortion functions},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective super-resolution methods for paired electron
microscopic images. <em>TIP</em>, <em>29</em>, 7317–7330. (<a
href="https://doi.org/10.1109/TIP.2020.3000964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with investigating super-resolution algorithms and solutions for handling electron microscopic images. We note two main aspects differentiating the problem discussed here from those considered in the literature. The first difference is that in the electron imaging setting. We have a pair of physical high-resolution and low-resolution images, rather than a physical image with its downsampled counterpart. The high-resolution image covers about 25\% of the view field of the low-resolution image, and the objective is to enhance the area of the low-resolution image where there is no high-resolution counterpart. The second difference is that the physics behind electron imaging is different from that of optical (visible light) photos. The implication is that super-resolution models trained by optical photos are not effective when applied to electron images. Focusing on the unique properties, we devise a global and local registration method to match the high- and low-resolution image patches and explore training strategies for applying deep learning super-resolution methods to the paired electron images. We also present a simple, non-local-mean approach as an alternative. This alternative performs as a close runner-up to the deep learning approaches, but it takes less time to train and entertains a simpler model structure.},
  archive      = {J_TIP},
  author       = {Yanjun Qian and Jiaxi Xu and Lawrence F. Drummy and Yu Ding},
  doi          = {10.1109/TIP.2020.3000964},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7317-7330},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Effective super-resolution methods for paired electron microscopic images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep pyramidal pooling with attention for person
re-identification. <em>TIP</em>, <em>29</em>, 7306–7316. (<a
href="https://doi.org/10.1109/TIP.2020.3000904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning discriminative, view-invariant and multi-scale representations of object appearance with different semantic levels is of paramount importance for person Re-Identification (ReID). Recently, the community has focused on learning deep Re-ID models to capture a single holistic representation. To improve the achieved results, additional visual attributes and object part-driven models have been considered, inevitably introducing additional human annotation labor or computational efforts. In this paper, we argue that pyramid-inspired methods capturing multi-scale information may overcome such requirements. Precisely, multi-scale pooled regions representing visual information of an object are integrated within a novel deep architecture factorizing them into discriminative features at multiple semantic levels. These are exploited through an attention mechanism later considered in an identification-similarity multi-task loss, trained by means of a curriculum learning strategy. Extensive results on three person ReID benchmarks demonstrate that better performance than existing methods are achieved. Code is available at https://github.com/iN1k1.},
  archive      = {J_TIP},
  author       = {Niki Martinel and Gian Luca Foresti and Christian Micheloni},
  doi          = {10.1109/TIP.2020.3000904},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7306-7316},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep pyramidal pooling with attention for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scanning imaging restoration of moving or dynamically
deforming objects. <em>TIP</em>, <em>29</em>, 7290–7305. (<a
href="https://doi.org/10.1109/TIP.2020.3000663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The raster scanning imaging mode is widely used in scanning electron microscopes (SEMs), transmission electron microscopes (TEM), and atomic force microscopes (AFM), and can achieve subatomic resolution. However, only a point on the shallow surface of an object can be imaged at one time using the raster scanning imaging mode, whereas the entire surface of the object can be imaged in the image plane once and instantaneously using the optical imaging mode, which is a parallel imaging mode. Therefore, the image distortion and blur for the scanning imaging mode are different from the optical imaging. In this paper, we propose a theory to describe the mechanism of the scanning imaging process and restore the degraded image (distorted and blurred image) obtained using an SEM. The theory consists of a scanning equation, motion or deformation equations, and an assumption called the intensity-invariant hypothesis. Numerical simulations of the scanning imaging process and restoration of the degraded images are performed using the scanning imaging formulas, spatial non-uniform point spread function, and inverse restoration algorithms, including algebraic, interpolation, and their hybrid methods to verify the feasibility of our theory. In situ experiments on uniform linear motion, uniaxial tensile, and fatigue were also conducted to demonstrate the validity and efficiency of the proposed scanning imaging theory and restoration methods. We anticipate that this imaging and restoration theory will enable the scanning imaging mode to be used in in situ dynamic imaging and for mechanical property measurement of materials.},
  archive      = {J_TIP},
  author       = {Hongfu Xie and Jiecun Liang and Zhen Wang and Minghui Liao and Xide Li},
  doi          = {10.1109/TIP.2020.3000663},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7290-7305},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scanning imaging restoration of moving or dynamically deforming objects},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image deraining using time-lapse data. <em>TIP</em>,
<em>29</em>, 7274–7289. (<a
href="https://doi.org/10.1109/TIP.2020.3000612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging on recent advances in deep convolutional neural networks (CNNs), single image deraining has been studied as a learning task, achieving an outstanding performance over traditional hand-designed approaches. Current CNNs based deraining approaches adopt the supervised learning framework that uses a massive training data generated with synthetic rain streaks, having a limited generalization ability on real rainy images. To address this problem, we propose a novel learning framework for single image deraining that leverages time-lapse sequences instead of the synthetic image pairs. The deraining networks are trained using the time-lapse sequences in which both camera and scenes are static except for time-varying rain streaks. Specifically, we formulate a background consistency loss such that the deraining networks consistently generate the same derained images from the time-lapse sequences. We additionally introduce two loss functions, the structure similarity loss that encourages the derained image to be similar with an input rainy image and the directional gradient loss using the assumption that the estimated rain streaks are likely to be sparse and have dominant directions. To consider various rain conditions, we leverage a dynamic fusion module that effectively fuses multi-scale features. We also build a novel large-scale time-lapse dataset providing real world rainy images containing various rain conditions. Experiments demonstrate that the proposed method outperforms state-of-the-art techniques on synthetic and real rainy images both qualitatively and quantitatively. On the high-level vision tasks under severe rainy conditions, it has been shown that the proposed method can be utilized as a pre-preprocessing step for subsequent tasks.},
  archive      = {J_TIP},
  author       = {Jaehoon Cho and Seungryong Kim and Dongbo Min and Kwanghoon Sohn},
  doi          = {10.1109/TIP.2020.3000612},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7274-7289},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Single image deraining using time-lapse data},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MVSNet++: Learning depth-based attention pyramid features
for multi-view stereo. <em>TIP</em>, <em>29</em>, 7261–7273. (<a
href="https://doi.org/10.1109/TIP.2020.3000611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Multi-View Stereo (MVS) is to reconstruct 3D point-cloud model from multiple views. On the basis of the considerable progress of deep learning, an increasing amount of research has moved from traditional MVS methods to learning-based ones. However, two issues remain unsolved in the existing state-of-the-art methods: (1) only high-level information is considered for depth estimation. This may reduce the localization accuracy of 3D points as the learned model lacks spatial information; and (2) most of the methods require additional post-processing or network refinement to generate a smooth 3D model. This significantly increases the number of model parameters or the computational complexity. To this end, we propose MVSNet++, an end-to-end trainable network for dense depth estimation. Such an estimated depth map can further be applied to 3D model reconstruction. Different from previous methods, in the proposed method, we first adopt feature pyramid structures for both feature extraction and cost volume regularization. This can lead to accurate 3D point localization by fusing multi-level information. To generate smooth depth map, we then carefully integrate instance normalization into MVSNet++ without increasing model parameters and computational burden. Furthermore, we additionally design three loss functions and integrate Curriculum Learning framework into the training process, which can lead to an accurate reconstruction of 3D model. MVSNet++ is evaluated on DTU and Tanks &amp; Temples benchmarks with comprehensive ablation studies. Experimental results demonstrate that our proposed method performs favorably against previous state-of-the-art methods, showing the accuracy and effectiveness of the proposed MVSNet++.},
  archive      = {J_TIP},
  author       = {Po-Heng Chen and Hsiao-Chien Yang and Kuan-Wen Chen and Yong-Sheng Chen},
  doi          = {10.1109/TIP.2020.3000611},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7261-7273},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MVSNet++: Learning depth-based attention pyramid features for multi-view stereo},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unified intra mode coding based on short and long range
correlations. <em>TIP</em>, <em>29</em>, 7245–7260. (<a
href="https://doi.org/10.1109/TIP.2020.3000351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a consensus regarding the intra prediction technique in video coding that the spatial redundancy can be efficiently removed by the locally accessible reference samples with certain projections and interpolations. In this paper, we revisit the short and long range correlations of the image content in the context of video coding, and it is interesting to find that the natural scene videos exhibit substantially different characteristics from screen content videos. This motivates us to redesign the intra mode coding method based on both short and long range correlations, as the existing approaches based on local content correlations cannot always effectively capture the most probable mode. One key feature of the proposed method is that it achieves unified content adaptive coding and is applicable across different video content. Experimental results on the versatile video coding (VVC) platform VTM-3.0 show the effectiveness of the proposed approach, leading to 3.73\% bit rate savings for screen content videos and 0.10\% bit rate savings for natural scene videos under all intra configuration.},
  archive      = {J_TIP},
  author       = {Junru Li and Meng Wang and Li Zhang and Kai Zhang and Hongbin Liu and Shiqi Wang and Siwei Ma and Wen Gao},
  doi          = {10.1109/TIP.2020.3000351},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7245-7260},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unified intra mode coding based on short and long range correlations},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Framelet representation of tensor nuclear norm for
third-order tensor completion. <em>TIP</em>, <em>29</em>, 7233–7244. (<a
href="https://doi.org/10.1109/TIP.2020.3000349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aim of this paper is to develop a framelet representation of the tensor nuclear norm for third-order tensor recovery. In the literature, the tensor nuclear norm can be computed by using tensor singular value decomposition based on the discrete Fourier transform matrix, and tensor completion can be performed by the minimization of the tensor nuclear norm which is the relaxation of the sum of matrix ranks from all Fourier transformed matrix frontal slices. These Fourier transformed matrix frontal slices are obtained by applying the discrete Fourier transform on the tubes of the original tensor. In this paper, we propose to employ the framelet representation of each tube so that a framelet transformed tensor can be constructed. Because of framelet basis redundancy, the representation of each tube is sparsely represented. When the matrix slices of the original tensor are highly correlated, we expect the corresponding sum of matrix ranks from all framelet transformed matrix frontal slices would be small, and the resulting tensor completion can be performed much better. The proposed minimization model is convex and global minimizers can be obtained. Numerical results on several types of multi-dimensional data (videos, multispectral images, and magnetic resonance imaging data) have tested and shown that the proposed method outperformed the other testing methods.},
  archive      = {J_TIP},
  author       = {Tai-Xiang Jiang and Michael K. Ng and Xi-Le Zhao and Ting-Zhu Huang},
  doi          = {10.1109/TIP.2020.3000349},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7233-7244},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Framelet representation of tensor nuclear norm for third-order tensor completion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A local flatness based variational approach to retinex.
<em>TIP</em>, <em>29</em>, 7217–7232. (<a
href="https://doi.org/10.1109/TIP.2020.2999858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A topic of continued interest in Retinex over the years has been finding ways to implement it with computational models of improved accuracy and efficiency. We have devised a new approach to digitally implementing the Retinex using a local deviation based variational model. The new model leads to improvements in the computed image quality with respect to illumination correction and image enhancement. Several contributions are made: 1) a new prior constraint, which we call local flatness, is proposed, and a new measure of Local Deviation (LD) is developed to quantify the degree of local illumination flatness; 2) a variational problem is defined and the solution is found by a logical sequence of steps; 3) discrete implementation of the variational solution is shown to effectively estimate and remove uneven illumination, yielding an accurate recovered image. Unlike other physical prior based variational Retinex models, which use the L2 norm of the illumination gradient to enforce smoothness of illumination, our LD prior selectively imposes local flatness on illumination by calculating the deviation between the estimated illumination surface to a reference plane. In the experiments, pseudo ground truth images are created by superimposing uneven illumination on real scenes, providing an effective way to objectively assess algorithm performance. The experimental results show that our method can reconstruct more accurate recovered images than other state-of-the-art methods, while maintaining good contrast.},
  archive      = {J_TIP},
  author       = {Meng Tang and Fengying Xie and Rui Zhang and Zhiguo Jiang and Alan C. Bovik},
  doi          = {10.1109/TIP.2020.2999858},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7217-7232},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A local flatness based variational approach to retinex},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MEF-GAN: Multi-exposure image fusion via generative
adversarial networks. <em>TIP</em>, <em>29</em>, 7203–7216. (<a
href="https://doi.org/10.1109/TIP.2020.2999855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an end-to-end architecture for multi-exposure image fusion based on generative adversarial networks, termed as MEF-GAN. In our architecture, a generator network and a discriminator network are trained simultaneously to form an adversarial relationship. The generator is trained to generate a real-like fused image based on the given source images which is expected to fool the discriminator. Correspondingly, the discriminator is trained to distinguish the generated fused images from the ground truth. The adversarial relationship makes the fused image not limited to the restriction of the content loss. Therefore, the fused images are closer to the ground truth in terms of probability distribution, which can compensate for the insufficiency of single content loss. Moreover, aiming at the problem that the luminance of multi-exposure images varies greatly with spatial location, the self-attention mechanism is employed in our architecture to allow for attention-driven and long-range dependency. Thus, local distortion, confusing results, or inappropriate representation can be corrected in the fused image. Qualitative and quantitative experiments are performed on publicly available datasets, where the results demonstrate that MEF-GAN outperforms the state-of-the-art, in terms of both visual effect and objective evaluation metrics. Our code is publicly available at https://github.com/jiayi-ma/MEF-GAN.},
  archive      = {J_TIP},
  author       = {Han Xu and Jiayi Ma and Xiao-Ping Zhang},
  doi          = {10.1109/TIP.2020.2999855},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7203-7216},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MEF-GAN: Multi-exposure image fusion via generative adversarial networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OFF-eNET: An optimally fused fully end-to-end network for
automatic dense volumetric 3D intracranial blood vessels segmentation.
<em>TIP</em>, <em>29</em>, 7192–7202. (<a
href="https://doi.org/10.1109/TIP.2020.2999854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intracranial blood vessels segmentation from computed tomography angiography (CTA) volumes is a promising biomarker for diagnosis and therapeutic treatment in cerebrovascular diseases. These segmentation outputs are a fundamental requirement in the development of automated decision support systems for preoperative assessment or intraoperative guidance in neuropathology. The state-of-the-art in medical image segmentation methods are reliant on deep learning architectures based on convolutional neural networks. However, despite their popularity, there is a research gap in the current deep learning architectures optimized to address the technical challenges in blood vessel segmentation. These challenges include: (i) the extraction of concrete brain vessels close to the skull; and (ii) the precise marking of the vessel locations. We propose an Optimally Fused Fully end-to-end Network (OFF-eNET) for automatic segmentation of the volumetric 3D intracranial vascular structures. OFF-eNET comprises of three modules. In the first module, we exploit the up-skip connections to enhance information flow, and dilated convolution for detailed preservation of spatial feature map that are designed for thin blood vessels. In the second module, we employ residual mapping along with inception module for speedy network convergence and richer visual representation. For the third module, we make use of the transferred knowledge in the form of cascaded training strategy to gradually optimize the three segmentation stages (basic, complete, and enhanced) to segment thin vessels located close to the skull. All these modules are designed to be computationally efficient. Our OFF-eNET, evaluated using 70 CTA image volumes, resulted in 90.75\% performance in the segmentation of intracranial blood vessels and outperformed the state-of-the-art counterparts.},
  archive      = {J_TIP},
  author       = {Anam Nazir and Muhammad Nadeem Cheema and Bin Sheng and Huating Li and Ping Li and Po Yang and Younhyun Jung and Jing Qin and Jinman Kim and David Dagan Feng},
  doi          = {10.1109/TIP.2020.2999854},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7192-7202},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {OFF-eNET: An optimally fused fully end-to-end network for automatic dense volumetric 3D intracranial blood vessels segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence-based large-scale dense multi-view stereo.
<em>TIP</em>, <em>29</em>, 7176–7191. (<a
href="https://doi.org/10.1109/TIP.2020.2999853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Albeit remarkable progress has been made to improve the accuracy and completeness of multi-view stereo (MVS), existing methods still suffer from either sparse reconstructions of low-textured surfaces or heavy computational burden. In this paper, we propose a Confidence-based Large-scale Dense Multi-view Stereo (CLD-MVS) method for high resolution imagery. Firstly, we formulate MVS as a multi-view depth estimation problem, and employ a normal-aware efficient PatchMatch stereo to estimate the initial depth and normal map for each reference view. A self-supervised deep learning method is then developed to predict the spatial confidence for multi-view depth maps, which is combined with cross-view consistency to generate the ground control points. Subsequently, a confidence-driven and boundary-aware interpolation scheme using static and dynamic guidance is adopted to synthesize dense depth and normal maps. Finally, a refinement procedure which leverages synthesized depth and normal as prior is conducted to estimate cross-view consistent surface. Experiments show that the proposed CLD-MVS method achieves high geometric completeness while preserving fine-scale details. In particular, it has ranked No. 1 on the ETH3D high-resolution MVS benchmark in terms of F 1 -score.},
  archive      = {J_TIP},
  author       = {Zhaoxin Li and Wangmeng Zuo and Zhaoqi Wang and Lei Zhang},
  doi          = {10.1109/TIP.2020.2999853},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7176-7191},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Confidence-based large-scale dense multi-view stereo},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constrained design of deep iris networks. <em>TIP</em>,
<em>29</em>, 7166–7175. (<a
href="https://doi.org/10.1109/TIP.2020.2999211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the promise of recent deep neural networks to provide more accurate and efficient iris recognition compared to traditional techniques, there are vital properties of the classic IrisCode which are almost unable to be achieved with current deep iris networks: the compactness of model and the small number of computing operations (FLOPs). This paper casts the iris network design process as a constrained optimization problem which takes model size and computation into account as learning criteria. On one hand, this allows us to fully automate the network design process to search for the optimal iris network architecture with the highest recognition accuracy confined to the computation and model compactness constraints. On the other hand, it allows us to investigate the optimality of the classic IrisCode and recent deep iris networks. It also enables us to learn an optimal iris network and demonstrate state-of-the-art performance with less computation and memory requirements.},
  archive      = {J_TIP},
  author       = {Kien Nguyen and Clinton Fookes and Sridha Sridharan},
  doi          = {10.1109/TIP.2020.2999211},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7166-7175},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Constrained design of deep iris networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning spatial and spatio-temporal pixel aggregations for
image and video denoising. <em>TIP</em>, <em>29</em>, 7153–7165. (<a
href="https://doi.org/10.1109/TIP.2020.2999209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing denoising methods typically restore clear results by aggregating pixels from the noisy input. Instead of relying on hand-crafted aggregation schemes, we propose to explicitly learn this process with deep neural networks. We present a spatial pixel aggregation network and learn the pixel sampling and averaging strategies for image denoising. The proposed model naturally adapts to image structures and can effectively improve the denoised results. Furthermore, we develop a spatio-temporal pixel aggregation network for video denoising to efficiently sample pixels across the spatio-temporal space. Our method is able to solve the misalignment issues caused by large motion in dynamic scenes. In addition, we introduce a new regularization term for effectively training the proposed video denoising model. We present extensive analysis of the proposed method and demonstrate that our model performs favorably against the state-of-the-art image and video denoising approaches on both synthetic and real-world data.},
  archive      = {J_TIP},
  author       = {Xiangyu Xu and Muchen Li and Wenxiu Sun and Ming-Hsuan Yang},
  doi          = {10.1109/TIP.2020.2999209},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7153-7165},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning spatial and spatio-temporal pixel aggregations for image and video denoising},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convexity shape prior for level set-based image segmentation
method. <em>TIP</em>, <em>29</em>, 7141–7152. (<a
href="https://doi.org/10.1109/TIP.2020.2998981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an image segmentation model that incorporates convexity shape priori using level set representations. In the past decade, several discrete and continuous methods have been developed to solve this problem. Our method comes from the observation that the signed distance function of a convex region must be a convex function. Based on this observation, we transfer the complicated geometrical convexity shape priori into some simple constraints on the signed distance function. We propose a simple algorithm to keep these constraints exactly. The proposed method could be easily applied to level set based segmentation models, such as the well-known Chan-Vese mode and the active contour models. By setting some good initial curves, the proposed method can easily segment convex objects from images with complicated background. We demonstrate the performance of the proposed methods on both synthetic images and real images, as well as the comparison to some state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Shi Yan and Xue-Cheng Tai and Jun Liu and Hai-Yang Huang},
  doi          = {10.1109/TIP.2020.2998981},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7141-7152},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Convexity shape prior for level set-based image segmentation method},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast learning of spatially regularized and content aware
correlation filter for visual tracking. <em>TIP</em>, <em>29</em>,
7128–7140. (<a href="https://doi.org/10.1109/TIP.2020.2998978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a good balance between accuracy and speed, correlation filter (CF) has become a popular and dominant visual object tracking scheme. It implicitly extends the training samples by circular shifts of a given target patch, which serve as negative samples for fast online learning of the filters. Since all these shifted patches are not real negative samples of the target, CF tracking scheme suffers from the annoying boundary effects that can greatly harm the tracking performance, especially under challenging situations, like occlusion and fast temporal variation. Spatial regularization is known as a potent way to alleviate such boundary effects, but with the cost of highly increased time complexity, caused by complex optimization imported by spatial regularization. In this paper, we propose a new fast learning approach to content-aware spatial regularization, namely weighted sample based CF tracking (WSCF). In WSCF, specifically, we present a simple yet effective energy function that implicitly weighs different training samples by spatial deviations. With the energy function, the learning of correlation filters is composed of two subproblems with closed-form solution and can be efficiently solved in an alternate way. We further develop a content-aware updating strategy to dynamically refine the weight distribution to well adapt to the temporal variations of the target and background. Finally, the proposed WSCF is used to enhance two state-of-the-art CF trackers to significantly boost their tracking accuracy, with little sacrifice on the tracking speed. Extensive experiments on five benchmarks validate the effectiveness of the proposed approach.},
  archive      = {J_TIP},
  author       = {Ruize Han and Wei Feng and Song Wang},
  doi          = {10.1109/TIP.2020.2998978},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7128-7140},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast learning of spatially regularized and content aware correlation filter for visual tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-guided multi-path knowledge aggregation for aerial
saliency prediction. <em>TIP</em>, <em>29</em>, 7117–7127. (<a
href="https://doi.org/10.1109/TIP.2020.2998977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging vision platform, a drone can look from many abnormal viewpoints which brings many new challenges into the classic vision task of video saliency prediction. To investigate these challenges, this paper proposes a large-scale video dataset for aerial saliency prediction, which consists of ground-truth salient object regions of 1,000 aerial videos, annotated by 24 subjects. To the best of our knowledge, it is the first large-scale video dataset that focuses on visual saliency prediction on drones. Based on this dataset, we propose a Model-guided Multi-path Network (MM-Net) that serves as a baseline model for aerial video saliency prediction. Inspired by the annotation process in eye-tracking experiments, MM-Net adopts multiple information paths, each of which is initialized under the guidance of a classic saliency model. After that, the visual saliency knowledge encoded in the most representative paths is selected and aggregated to improve the capability of MM-Net in predicting spatial saliency in aerial scenarios. Finally, these spatial predictions are adaptively combined with the temporal saliency predictions via a spatiotemporal optimization algorithm. Experimental results show that MM-Net outperforms ten state-of-the-art models in predicting aerial video saliency.},
  archive      = {J_TIP},
  author       = {Kui Fu and Jia Li and Yu Zhang and Hongze Shen and Yonghong Tian},
  doi          = {10.1109/TIP.2020.2998977},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7117-7127},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Model-guided multi-path knowledge aggregation for aerial saliency prediction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to align via wasserstein for person
re-identification. <em>TIP</em>, <em>29</em>, 7104–7116. (<a
href="https://doi.org/10.1109/TIP.2020.2998931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing successful person re-identification (Re-ID) models often employ the part-level representation to extract the fine-grained information, but commonly use the loss that is particularly designed for global features, ignoring the relationship between semantic parts. In this paper, we present a novel triplet loss that emphasizes the salient parts and also takes the consideration of alignment. This loss is based on the crossing-bing matching metric that also known as Wasserstein Distance. It measures how much effort it would take to move the embeddings of local features to align two distributions, such that it is able to find an optimal transport matrix to re-weight the distance of different local parts. The distributions in support of local parts is produced via a new attention mechanism, which is calculated by the inner product between high-level global feature and local features, representing the importance of different semantic parts w.r.t. identification. We show that the obtained optimal transport matrix can not only distinguish the relevant and misleading parts, and hence assign different weights to them, but also rectify the original distance according to the learned distributions, resulting in an elegant solution for the mis-alignment issue. Besides, the proposed method is easily implemented in most Re-ID learning system with end-to-end training style, and can obviously improve their performance. Extensive experiments and comparisons with recent Re-ID methods manifest the competitive performance of our method.},
  archive      = {J_TIP},
  author       = {Zhizhong Zhang and Yuan Xie and Ding Li and Wensheng Zhang and Qi Tian},
  doi          = {10.1109/TIP.2020.2998931},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7104-7116},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to align via wasserstein for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-term video prediction via criticization and
retrospection. <em>TIP</em>, <em>29</em>, 7090–7103. (<a
href="https://doi.org/10.1109/TIP.2020.2998297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video prediction refers to predicting and generating future video frames given a set of consecutive frames. Conventional video prediction methods usually criticize the discrepancy between the ground-truth and predictions frame by frame. As the prediction error accumulates recursively, these methods would easily become out of control and are often confined to the short-term horizon. In this paper, we introduce a retrospection process to rectify the prediction errors beyond criticizing the future prediction. The introduced retrospection process is designed to look back what have been learned from the past and rectify the prediction deficiencies. To this end, we build a retrospection network to reconstruct the past frames given the currently predicted frames. A retrospection loss is introduced to push the retrospection frames being consistent with the observed frames, so that the prediction error is alleviated. On the other hand, an auxiliary route is built by reversing the flow of time and executing a similar retrospection. These two routes interact with each other to boost the performance of retrospection network and enhance the understanding of dynamics across frames, especially for the long-term horizon. An adversarial loss is employed to generate more realistic results in both prediction and retrospection process. In addition, the proposed method can be used to extend many state-of-the-art video prediction methods. Extensive experiments on the natural video dataset demonstrate the advantage of introducing the retrospection process for long-term video prediction.},
  archive      = {J_TIP},
  author       = {Xinyuan Chen and Chang Xu and Xiaokang Yang and Dacheng Tao},
  doi          = {10.1109/TIP.2020.2998297},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7090-7103},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Long-term video prediction via criticization and retrospection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new polarization image demosaicking algorithm by
exploiting inter-channel correlations with guided filtering.
<em>TIP</em>, <em>29</em>, 7076–7089. (<a
href="https://doi.org/10.1109/TIP.2020.2998281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a fast and effective polarization image demosaicking algorithm, which explores inter-channel dependency of Stokes parameters for the minimization of residual aliasing artifacts after cubic spline interpolation. A guided filtering approach is used for denoising. An optimization based on the confidence level of the aforementioned guided filtering, the correlations between the demosaicked image and input, as well as the total intensity, angle and degree of linear polarization, is constructed and solved with Newton&#39;s method. Experimental results demonstrate that the proposed algorithm can surpass the existing methods in terms of both objective root mean squared error and structural similarity index by at least 36.0\% and 3.4\%, respectively, and by close visual inspection of the clarity of objects in the angle and degree of linear polarization images. The proposed algorithm consists of only convolutions and element-wise operations, making it fast and parallelizable for efficient GPU acceleration. An image of size 512 × 612 × 4 can be processed within 10 s on i7-6700k CPU, and gains further 5 times speedup with M4000M GPU.},
  archive      = {J_TIP},
  author       = {Shumin Liu and Jiajia Chen and Yuan Xun and Xiaojin Zhao and Chip-Hong Chang},
  doi          = {10.1109/TIP.2020.2998281},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7076-7089},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A new polarization image demosaicking algorithm by exploiting inter-channel correlations with guided filtering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). The TVp regularized mumford-shah model for image labeling
and segmentation. <em>TIP</em>, <em>29</em>, 7061–7075. (<a
href="https://doi.org/10.1109/TIP.2020.2997524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Mumford-Shah model is an important tool for image labeling and segmentation, which pursues a piecewise smooth approximation of the original image and the boundaries with the shortest length. In contrast to previous efforts, which use the total variation regularization to measure the total length of the boundaries, we build up a novel piecewise smooth Mumford-Shah model by utilizing a non-convex ℓ p regularity term for p ∈ (0,1), which can well preserve sharp edges and eliminate geometric staircasing effects. We present optimization algorithms with convergence verification, where all subproblems can be solved by either the closed-form solution or fast Fourier transform (FFT). The method is compared to piecewise constant labeling algorithm and several state-of-the-art piecewise smooth Mumford-Shah models based on image decomposition approximations. Both labeling and segmentation results on synthetic and real images confirm the robustness and efficiency of the proposed method.},
  archive      = {J_TIP},
  author       = {Yutong Li and Chunlin Wu and Yuping Duan},
  doi          = {10.1109/TIP.2020.2997524},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7061-7075},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {The TVp regularized mumford-shah model for image labeling and segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Remove cosine window from correlation filter-based visual
trackers: When and how. <em>TIP</em>, <em>29</em>, 7045–7060. (<a
href="https://doi.org/10.1109/TIP.2020.2997521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filters (CFs) have been continuously advancing the state-of-the-art tracking performance and have been extensively studied in the recent few years. Nonetheless, the existing CF trackers adopt a cosine window to spatially reweight base image to alleviate boundary discontinuity. However, cosine window emphasizes more on the central regions of base image and has the risk of contaminating negative training samples during model learning. On the other hand, spatial regularization deployed in many recent CF trackers plays a similar role as cosine window by enforcing spatial penalty on CF coefficients. Therefore, we in this paper investigate the feasibility to remove cosine window from CF trackers with spatial regularization. When simply removing cosine window, CF with spatial regularization still suffers from small degree of boundary discontinuity. To tackle this issue, binary and Gaussian shaped mask functions are further introduced for eliminating boundary discontinuity while reweighting the estimation error of each training sample, and can be incorporated with multiple CF trackers with spatial regularization. In comparison to the baseline methods with cosine window, our methods are effective in handling boundary discontinuity and sample contamination, thereby benefiting tracking performance. Extensive experiments on four benchmarks show that our methods perform favorably against the state-of-the-art trackers using either handcrafted or deep CNN features.},
  archive      = {J_TIP},
  author       = {Feng Li and Xiaohe Wu and Wangmeng Zuo and David Zhang and Lei Zhang},
  doi          = {10.1109/TIP.2020.2997521},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7045-7060},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Remove cosine window from correlation filter-based visual trackers: When and how},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new hybrid level set approach. <em>TIP</em>, <em>29</em>,
7032–7044. (<a href="https://doi.org/10.1109/TIP.2020.2997331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid active contour models with the combination of region and edge information have attracted great interests in image segmentation. To the best of our knowledge, however, the theoretical foundation of these hybrid models with level set evolution is insufficient and limited. More specifically, the weighting factors of their energy terms are difficult to select and are often empirically determined without definite theoretical basis. This problem is particularly prominent in the case of multi-object segmentation when more level set functions must be computed simultaneously. To cope with these challenges, this paper proposes a new level set approach for constructing hybrid active contour models with reliable energy weights, where the weights of region and edge terms can be constrained by the optimization condition deduced from the proposed method. It can be regarded as a general approach since many existing region-based models can be easily used to construct new hybrid models using their equivalent two-phase formulations. Some representative as well as state-of-the-art models are taken as examples to demonstrate the generality of our method. The respective comparative studies validate that under the guidance of the optimization condition, segmentation accuracy, robustness, and computational efficiency can be improved compared with the original models which are used to construct the new hybrid ones.},
  archive      = {J_TIP},
  author       = {Weihang Zhang and Xue Wang and Junfeng Chen and Wei You},
  doi          = {10.1109/TIP.2020.2997331},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7032-7044},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A new hybrid level set approach},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised multi-view constrained convolutional network
for accurate depth estimation. <em>TIP</em>, <em>29</em>, 7019–7031. (<a
href="https://doi.org/10.1109/TIP.2020.2997247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate depth estimation from images is a fundamental problem in computer vision. In this paper, we propose an unsupervised learning based method to predict high-quality depth map from multiple images. A novel multi-view constrained DenseDepthNet is designed for this task. Our DenseDepthNet can effectively leverage both the low-level and high-level features of input images and generate appealing results, especially with sharp details. We employ the public datasets KITTI and Cityscapes for training in an end-to-end unsupervised fashion. A novel depth consistency loss based on multi-view geometry constraint is also applied to the corresponding points across pairwise images, which helps to improve the quality of predicted depth maps significantly. We conduct comprehensive evaluations on our DenseDepthNet and our depth consistency loss function. Experiments validate that our method outperforms the state-of-the-art unsupervised methods and produce comparable results with supervised methods.},
  archive      = {J_TIP},
  author       = {Yuyang Zhang and Shibiao Xu and Baoyuan Wu and Jian Shi and Weiliang Meng and Xiaopeng Zhang},
  doi          = {10.1109/TIP.2020.2997247},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7019-7031},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised multi-view constrained convolutional network for accurate depth estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bi-modal progressive mask attention for fine-grained
recognition. <em>TIP</em>, <em>29</em>, 7006–7018. (<a
href="https://doi.org/10.1109/TIP.2020.2996736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional fine-grained image recognition is required to distinguish different subordinate categories (e.g., birds species) based on the visual cues beneath raw images. Due to both small inter-class variations and large intra-class variations, it is desirable to capture the subtle differences between these sub-categories, which is crucial but challenging for fine-grained recognition. Recently, language modality aggregation has been proved as a successful technique to improve visual recognition in the experience. In this paper, we introduce an end-to-end trainable Progressive Mask Attention (PMA) model for fine-grained recognition by leveraging both visual and language modalities. Our Bi-Modal PMA model can not only stage-by-stage capture the most discriminative part in the visual modality by our mask-based fashion, but also explore the out-of-visual-domain knowledge from the language modality in an interactional alignment paradigm. Specifically, at each stage, a self-attention module is proposed to attend to the key patch from images or text descriptions. Besides, a query-relational module is designed to seize the key words/phrases of texts and further bridge the connection between two modalities. Later, the learned representations of bi-modality from multiple stages are aggregated as the final features for recognition. Our Bi-Modal PMA model only needs raw images and raw text descriptions, without requiring bounding boxes/part annotations in images or key word annotations in texts. By conducting comprehensive experiments on fine-grained benchmark datasets, we demonstrate that the proposed method achieves superior performance over the competing baselines, on either vision and language bi-modality or single visual modality.},
  archive      = {J_TIP},
  author       = {Kaitao Song and Xiu-Shen Wei and Xiangbo Shu and Ren-Jie Song and Jianfeng Lu},
  doi          = {10.1109/TIP.2020.2996736},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7006-7018},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bi-modal progressive mask attention for fine-grained recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Progressive feature matching: Incremental graph construction
and optimization. <em>TIP</em>, <em>29</em>, 6992–7005. (<a
href="https://doi.org/10.1109/TIP.2020.2996092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel feature matching algorithm that systematically utilizes the geometric properties of image features such as position, scale, and orientation, in addition to the conventional descriptor vectors. In challenging scenes, in which repetitive structures and large view changes are present, it is difficult to find correct correspondences using conventional approaches that only use descriptors, as the descriptor distances of correct matches may not be the least among the candidates. The feature matching problem is formulated as a Markov random field (MRF) that uses descriptor distances and relative geometric similarities together. Assuming that the layout of the nearby features does not considerably change, we propose the bidirectional transfer measure to gauge the geometric consistency between the pairs of feature correspondences. The unmatched features are explicitly modeled in the MRF to minimize their negative impact. Instead of solving the MRF on the entire features at once, we start with a small set of confident feature matches, and then progressively expand the MRF with the remaining candidate matches. The proposed progressive approach yields better feature matching performance and faster processing time. Experimental results show that the proposed algorithm provides better feature correspondences in many challenging scenes, i.e., more matches with higher inlier ratio and lower computational cost than those of the state-of-the-art algorithms. The source code of our implementation is open to the public.},
  archive      = {J_TIP},
  author       = {Sehyung Lee and Jongwoo Lim and Il Hong Suh},
  doi          = {10.1109/TIP.2020.2996092},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6992-7005},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive feature matching: Incremental graph construction and optimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-modal recurrent attention networks for facial
expression recognition. <em>TIP</em>, <em>29</em>, 6977–6991. (<a
href="https://doi.org/10.1109/TIP.2020.2996086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep neural networks based methods have achieved state-of-the-art performance on various facial expression recognition tasks. Despite such progress, previous researches for facial expression recognition have mainly focused on analyzing color recording videos only. However, the complex emotions expressed by people with different skin colors under different lighting conditions through dynamic facial expressions can be fully understandable by integrating information from multi-modal videos. We present a novel method to estimate dimensional emotion states, where color, depth, and thermal recording videos are used as a multi-modal input. Our networks, called multi-modal recurrent attention networks (MRAN), learn spatiotemporal attention volumes to robustly recognize the facial expression based on attention-boosted feature volumes. We leverage the depth and thermal sequences as guidance priors for color sequence to selectively focus on emotional discriminative regions. We also introduce a novel benchmark for multi-modal facial expression recognition, termed as multi-modal arousal-valence facial expression recognition (MAVFER), which consists of color, depth, and thermal recording videos with corresponding continuous arousal-valence scores. The experimental results show that our method can achieve the state-of-the-art results in dimensional facial expression recognition on color recording datasets including RECOLA, SEWA and AFEW, and a multi-modal recording dataset including MAVFER.},
  archive      = {J_TIP},
  author       = {Jiyoung Lee and Sunok Kim and Seungryong Kim and Kwanghoon Sohn},
  doi          = {10.1109/TIP.2020.2996086},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6977-6991},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-modal recurrent attention networks for facial expression recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning recurrent 3D attention for video-based person
re-identification. <em>TIP</em>, <em>29</em>, 6963–6976. (<a
href="https://doi.org/10.1109/TIP.2020.2995272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose to learn recurrent 3D attention (A3D) for video-based person re-identification. Attention model plays a key role in both spatial and temporal domains for video representation. Most existing methods apply spatial attention model to extract feature from a single image and aggregate image features with attentive temporal pooling or RNN. However, the inherent consistencies and correlations between spatial and temporal clues are not leveraged. Our A3D method aims to utilize the joint constraints of temporal and spatial attentions to enhance the robustness of attention model. Towards this goal, we treat the pedestrian video as a unified 3D bin where the temporal domain is denoted as an additional dimension. Then we develop an attention agent to iteratively select the locations of the salient spatial-temporal parts in the 3D bin. In addition, we formulate our sequential 3D attention learning as a Markov Decision Process and train the representation network and attention detector with the policy gradient method in an end-to-end manner. We evaluate the proposed method on three challenging datasets including iLIDS-VID, PRID-2011 and the large-scale MARS dataset, and consistently improve the performance in comparison with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Guangyi Chen and Jiwen Lu and Ming Yang and Jie Zhou},
  doi          = {10.1109/TIP.2020.2995272},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6963-6976},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning recurrent 3D attention for video-based person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dehazing evaluation: Real-world benchmark datasets,
criteria, and baselines. <em>TIP</em>, <em>29</em>, 6947–6962. (<a
href="https://doi.org/10.1109/TIP.2020.2995264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On benchmark images, modern dehazing methods are able to achieve very comparable results whose differences are too subtle for people to qualitatively judge. Thus, it is imperative to adopt quantitative evaluation on a vast number of hazy images. However, existing quantitative evaluation schemes are not convincing due to a lack of appropriate datasets and poor correlations between metrics and human perceptions. In this work, we attempt to address these issues, and we make two contributions. First, we establish two benchmark datasets, i.e., the BEnchmark Dataset for Dehazing Evaluation (BeDDE) and the EXtension of the BeDDE (exBeDDE), which had been lacking for a long period of time. The BeDDE is used to evaluate dehazing methods via full reference image quality assessment (FR-IQA) metrics. It provides hazy images, clear references, haze level labels, and manually labeled masks that indicate the regions of interest (ROIs) in image pairs. The exBeDDE is used to assess the performance of dehazing evaluation metrics. It provides extra dehazed images and subjective scores from people. To the best of our knowledge, the BeDDE is the first dehazing dataset whose image pairs were collected in natural outdoor scenes without any simulation. Second, we provide a new insight that dehazing involves two separate aspects, i.e., visibility restoration and realness restoration, which should be evaluated independently; thus, to characterize them, we establish two criteria, i.e., the visibility index (VI) and the realness index (RI), respectively. The effectiveness of the criteria is verified through extensive experiments. Furthermore, 14 representative dehazing methods are evaluated as baselines using our criteria on BeDDE. Our datasets and relevant code are available at https://github.com/xiaofeng94/BeDDE-for-defogging.},
  archive      = {J_TIP},
  author       = {Shiyu Zhao and Lin Zhang and Shuaiyi Huang and Ying Shen and Shengjie Zhao},
  doi          = {10.1109/TIP.2020.2995264},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6947-6962},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dehazing evaluation: Real-world benchmark datasets, criteria, and baselines},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Few-shot text style transfer via deep feature similarity.
<em>TIP</em>, <em>29</em>, 6932–6946. (<a
href="https://doi.org/10.1109/TIP.2020.2995062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating text to have a consistent style with only a few observed highly-stylized text samples is a difficult task for image processing. The text style involving the typography, i.e., font, stroke, color, decoration, effects, etc., should be considered for transfer. In this paper, we propose a novel approach to stylize target text by decoding weighted deep features from only a few referenced samples. The deep features, including content and style features of each referenced text, are extracted from a Convolutional Neural Network (CNN) that is optimized for character recognition. Then, we calculate the similarity scores of the target text and the referenced samples by measuring the distance along the corresponding channels from the content features of the CNN when considering only the content, and assign them as the weights for aggregating the deep features. To enforce the stylized text to be realistic, a discriminative network with adversarial loss is employed. We demonstrate the effectiveness of our network by conducting experiments on three different datasets which have various styles, fonts, languages, etc. Additionally, the coefficients for character style transfer, including the character content, the effect of similarity matrix, the number of referenced characters, the similarity between characters, and performance evaluation by a new protocol are analyzed for better understanding our proposed framework.},
  archive      = {J_TIP},
  author       = {Anna Zhu and Xiongbo Lu and Xiang Bai and Seiichi Uchida and Brian Kenji Iwana and Shengwu Xiong},
  doi          = {10.1109/TIP.2020.2995062},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6932-6946},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Few-shot text style transfer via deep feature similarity},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and accurate tensor completion with total variation
regularized tensor trains. <em>TIP</em>, <em>29</em>, 6918–6931. (<a
href="https://doi.org/10.1109/TIP.2020.2995061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new tensor completion method based on tensor trains. The to-be-completed tensor is modeled as a low-rank tensor train, where we use the known tensor entries and their coordinates to update the tensor train. A novel tensor train initialization procedure is proposed specifically for image and video completion, which is demonstrated to ensure fast convergence of the completion algorithm. The tensor train framework is also shown to easily accommodate Total Variation and Tikhonov regularization due to their low-rank tensor train representations. Image and video inpainting experiments verify the superiority of the proposed scheme in terms of both speed and scalability, where a speedup of up to 155× is observed compared to state-of-the-art tensor completion methods at a similar accuracy. Moreover, we demonstrate the proposed scheme is especially advantageous over existing algorithms when only tiny portions (say, 1\%) of the to-be-completed images/videos are known.},
  archive      = {J_TIP},
  author       = {Ching-Yun Ko and Kim Batselier and Lucas Daniel and Wenjian Yu and Ngai Wong},
  doi          = {10.1109/TIP.2020.2995061},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6918-6931},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast and accurate tensor completion with total variation regularized tensor trains},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An ILP model for multi-label MRFs with connectivity
constraints. <em>TIP</em>, <em>29</em>, 6909–6917. (<a
href="https://doi.org/10.1109/TIP.2020.2995056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integer Linear Programming (ILP) formulations of multi-label Markov random fields (MRFs) models with global connectivity priors were investigated previously in computer vision. In these works, only Linear Programming (LP) relaxations [1] or simplified versions [2] of the problem were solved. This paper investigates the ILP of MRF with exact connectivity priors via a branch-and-cut method, which provably finds globally optimal solutions. It enforces connectivity priors iteratively by a cutting plane method, and provides feasible solutions with a guarantee on sub-optimality even if we terminate it earlier. The proposed ILP can be applied as a post-processing method on top of any existing multi-label segmentation approach. As it provides globally optimal solution, it can be used off-line to serve as quality check for any fast on-line algorithm. Furthermore, the scribble based model presented in this paper could be potentially used to generate ground-truth proposals for any deep learning based segmentation. We demonstrate the power and usefulness of our model by extensive experiments on the BSDS500 and PASCAL VOC dataset. The experiments show that our proposed model achieves great performance, yielding provably global optimum in most instances and that provably good optimization solutions also provide good segmentation accuracy, even with the limited computing time of few seconds.},
  archive      = {J_TIP},
  author       = {Ruobing Shen and Bo Tang and Andrea Lodi and Andrea Tramontani and Ismail Ben Ayed},
  doi          = {10.1109/TIP.2020.2995056},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6909-6917},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An ILP model for multi-label MRFs with connectivity constraints},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient low-resolution face recognition via bridge
distillation. <em>TIP</em>, <em>29</em>, 6898–6908. (<a
href="https://doi.org/10.1109/TIP.2020.2995049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition in the wild is now advancing towards light-weight models, fast inference speed and resolution-adapted capability. In this paper, we propose a bridge distillation approach to turn a complex face model pretrained on private high-resolution faces into a light-weight one for low-resolution face recognition. In our approach, such a cross-dataset resolution-adapted knowledge transfer problem is solved via two-step distillation. In the first step, we conduct cross-dataset distillation to transfer the prior knowledge from private high-resolution faces to public high-resolution faces and generate compact and discriminative features. In the second step, the resolution-adapted distillation is conducted to further transfer the prior knowledge to synthetic low-resolution faces via multi-task learning. By learning low-resolution face representations and mimicking the adapted high-resolution knowledge, a light-weight student model can be constructed with high efficiency and promising accuracy in recognizing low-resolution faces. Experimental results show that the student model performs impressively in recognizing low-resolution faces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed reaches up to 14,705, 934 and 763 faces per second on GPU, CPU and mobile phone, respectively.},
  archive      = {J_TIP},
  author       = {Shiming Ge and Shengwei Zhao and Chenyu Li and Yu Zhang and Jia Li},
  doi          = {10.1109/TIP.2020.2995049},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6898-6908},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient low-resolution face recognition via bridge distillation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dark and bright channel prior embedded network for dynamic
scene deblurring. <em>TIP</em>, <em>29</em>, 6885–6897. (<a
href="https://doi.org/10.1109/TIP.2020.2995048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the significant progress on convolutional neural networks (CNNs) in dynamic scene deblurring. While most of the CNN models are generally learned by the reconstruction loss defined on training data, incorporating suitable image priors as well as regularization terms into the network architecture could boost the deblurring performance. In this work, we propose a Dark and Bright Channel Priors embedded Network (DBCPeNet) to plug the channel priors into a neural network for effective dynamic scene deblurring. A novel trainable dark and bright channel priors embedded layer (DBCPeL) is developed to aggregate both channel priors and blurry image representations, and a sparse regularization is introduced to regularize the DBCPeNet model learning. Furthermore, we present an effective multi-scale network architecture, namely image full scale exploitation (IFSE), which works in both coarse-to-fine and fine-to-coarse manners for better exploiting information flow across scales. Experimental results on the GoPro and Köhler datasets show that our proposed DBCPeNet performs favorably against state-of-the-art deep image deblurring methods in terms of both quantitative metrics and visual quality.},
  archive      = {J_TIP},
  author       = {Jianrui Cai and Wangmeng Zuo and Lei Zhang},
  doi          = {10.1109/TIP.2020.2995048},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6885-6897},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dark and bright channel prior embedded network for dynamic scene deblurring},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse domain gaussianization for multi-variate statistical
modeling of retinal OCT images. <em>TIP</em>, <em>29</em>, 6873–6884.
(<a href="https://doi.org/10.1109/TIP.2020.2994454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a multivariate statistical model that is suitable for describing Optical Coherence Tomography (OCT) images is introduced. The proposed model is comprised of a multivariate Gaussianization function in sparse domain. Such an approach has two advantages, i.e. 1) finding a function that can effectively transform the input - which is often not Gaussian - into normally distributed samples enables the reliable application of methods that assume Gaussianity, 2) although multivariate Gaussianization in spatial domain is a complicated task and rarely results in closed-form analytical model, by transferring data to sparse domain, our approach facilitates multivariate statistical modeling of OCT images. To this end, a proper multivariate probability density function (pdf) which considers all three properties of OCT images in sparse domains (i.e. compression, clustering, and persistence properties) is designed and the proposed sparse domain Gaussianization framework is established. Using this multivariate model, we show that the OCT images often follow a 2-component multivariate Laplace mixture model in the sparse domain. To evaluate the performance of the proposed model, it is employed for OCT image denoising in a Bayesian framework. Visual and numerical comparison with previous prominent methods reveals that our method improves the overall contrast of the image, preserves edges, suppresses background noise to a desirable amount, but is less capable of maintaining tissue texture. As a result, this method is suitable for applications where edge preservation is crucial, and a clean noiseless image is desired.},
  archive      = {J_TIP},
  author       = {Zahra Amini and Hossein Rabbani and Ivan Selesnick},
  doi          = {10.1109/TIP.2020.2994454},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6873-6884},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sparse domain gaussianization for multi-variate statistical modeling of retinal OCT images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-atlas brain parcellation using squeeze-and-excitation
fully convolutional networks. <em>TIP</em>, <em>29</em>, 6864–6872. (<a
href="https://doi.org/10.1109/TIP.2020.2994445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-atlas parcellation (MAP) is carried out on a brain image by propagating and fusing labelled regions from brain atlases. Typical nonlinear registration-based label propagation is time-consuming and sensitive to inter-subject differences. Recently, deep learning parcellation (DLP) has been proposed to avoid nonlinear registration for better efficiency and robustness than MAP. However, most existing DLP methods neglect using brain atlases, which contain high-level information (e.g., manually labelled brain regions), to provide auxiliary features for improving the parcellation accuracy. In this paper, we propose a novel multi-atlas DLP method for brain parcellation. Our method is based on fully convolutional networks (FCN) and squeeze-and-excitation (SE) modules. It can automatically and adaptively select features from the most relevant brain atlases to guide parcellation. Moreover, our method is trained via a generative adversarial network (GAN), where a convolutional neural network (CNN) with multi-scale l 1 loss is used as the discriminator. Benefiting from brain atlases, our method outperforms MAP and state-of-the-art DLP methods on two public image datasets (LPBA40 and NIREP-NA0).},
  archive      = {J_TIP},
  author       = {Zhenyu Tang and Xianli Liu and Yang Li and Pew-Thian Yap and Dinggang Shen},
  doi          = {10.1109/TIP.2020.2994445},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6864-6872},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-atlas brain parcellation using squeeze-and-excitation fully convolutional networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image deraining using bilateral recurrent network.
<em>TIP</em>, <em>29</em>, 6852–6863. (<a
href="https://doi.org/10.1109/TIP.2020.2994443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image deraining has received considerable progress based on deep convolutional neural network (CNN). In existing deep deraining methods, CNNs are deployed to extract rain streaks while failing in learning direct mapping from rainy image to clean background image, and their architectures become more and more complicated. In this work, we first propose a single recurrent network (SRN) by recursively unfolding a shallow residual network, where a recurrent layer is adopted to propagate deep features across multiple stages. This simple SRN is effective not only in learning residual mapping for extracting rain streaks, but also in learning direct mapping for predicting clean background image. Furthermore, two SRNs are coupled to simultaneously exploit rain streak layer and clean background image layer. Instead of naive combination, we propose bilateral LSTMs, which not only can respectively propagate deep features of rain streak layer and background image layer across stages, but also bring the interplay between these two SRNs, finally forming bilateral recurrent network (BRN). The experimental results demonstrate that our BRN notably outperforms state-of-the-art deep deraining networks on synthetic datasets quantitatively and qualitatively. The proposed methods also perform more favorably in terms of generalization performance on real-world rainy dataset. All the source code and pre-trained models are available at https://github.com/csdwren/RecDerain.},
  archive      = {J_TIP},
  author       = {Dongwei Ren and Wei Shang and Pengfei Zhu and Qinghua Hu and Deyu Meng and Wangmeng Zuo},
  doi          = {10.1109/TIP.2020.2994443},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6852-6863},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Single image deraining using bilateral recurrent network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating neural networks into the blind deblurring
framework to compete with the end-to-end learning-based methods.
<em>TIP</em>, <em>29</em>, 6841–6851. (<a
href="https://doi.org/10.1109/TIP.2020.2994413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the end-to-end learning-based methods have been proven effective for the blind image deblurring. Without human-made assumptions or numerical algorithms, they are able to restore images with fewer artifacts and better perceptual quality. However, in practice, these methods suffer from limited performance under complex motion scenario and produces unnatural results sometimes. In this paper, in order to overcome their limitations, we propose to integrate deep convolution neural networks into a conventional deblurring framework. Specifically, we propose Stacked Estimation Residual Net (SEN) to estimate the motion flow map and Recurrent Prior Generative and Adversarial Net (RP-GAN) to learn the implicit image prior for the optimization. Comparing with the state-of-the-art end-to-end learning-based methods, the proposed method restores image content more naturally and shows better generalization ability.},
  archive      = {J_TIP},
  author       = {Junde Wu and Xiaoguang Di},
  doi          = {10.1109/TIP.2020.2994413},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6841-6851},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Integrating neural networks into the blind deblurring framework to compete with the end-to-end learning-based methods},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MV-GNN: Multi-view graph neural network for compression
artifacts reduction. <em>TIP</em>, <em>29</em>, 6829–6840. (<a
href="https://doi.org/10.1109/TIP.2020.2994412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inevitable compression artifacts in multi-view video (MVV) can clearly degrade the quality of experience in many interaction-oriented 3D visual applications. Under the framework of asymmetric coding, low-quality images can be enhanced with high-quality images from the neighboring viewpoints considering the similarity among different views. However, compression artifacts and warping error cause different cross-view quality gaps for various sequences, and thus the contribution of cross-view priors can hardly be located and considered in previous works. In this paper, we propose a multi-view graph neural network (MV-GNN) to reduce compression artifacts in multi-view compressed images. We dedicate to design a fusion mechanism which can exploit contributions from neighboring viewpoints and meanwhile suppress the misleading information. In our method, a GNN-based fusion mechanism is designed to fuse the cross-view information under the aggregation and update mechanism of GNN. Experiments show that 1.672 dB and 0.0242 average gains on PSNR and SSIM metrics can be obtained, respectively. For the subjective evaluations, blocking effect in the compressed images are clearly suppressed and the damaged object boundary are better recovered. The experimental results demonstrate that our MV-GNN outperforms the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Xin He and Qiong Liu and You Yang},
  doi          = {10.1109/TIP.2020.2994412},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6829-6840},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MV-GNN: Multi-view graph neural network for compression artifacts reduction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral image compressive sensing reconstruction using
subspace-based nonlocal tensor ring decomposition. <em>TIP</em>,
<em>29</em>, 6813–6828. (<a
href="https://doi.org/10.1109/TIP.2020.2994411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image compressive sensing reconstruction (HSI-CSR) can largely reduce the high expense and low efficiency of transmitting HSI to ground stations by storing a few compressive measurements, but how to precisely reconstruct the HSI from a few compressive measurements is a challenging issue. It has been proven that considering the global spectral correlation, spatial structure, and nonlocal self-similarity priors of HSI can achieve satisfactory reconstruction performances. However, most of the existing methods cannot simultaneously capture the mentioned priors and directly design the regularization term to the HSI. In this article, we propose a novel subspace-based nonlocal tensor ring decomposition method (SNLTR) for HSI-CSR. Instead of designing the regularization of the low-rank approximation to the HSI, we assume that the HSI lies in a low-dimensional subspace. Moreover, to explore the nonlocal self-similarity and preserve the spatial structure of HSI, we introduce a nonlocal tensor ring decomposition strategy to constrain the related coefficient image, which can decrease the computational cost compared to the methods that directly employ the nonlocal regularization to HSI. Finally, a well-known alternating minimization method is designed to efficiently solve the proposed SNLTR. Extensive experimental results demonstrate that our SNLTR method can significantly outperform existing approaches for HSI-CSR.},
  archive      = {J_TIP},
  author       = {Yong Chen and Ting-Zhu Huang and Wei He and Naoto Yokoya and Xi-Le Zhao},
  doi          = {10.1109/TIP.2020.2994411},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6813-6828},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral image compressive sensing reconstruction using subspace-based nonlocal tensor ring decomposition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crowd counting via cross-stage refinement networks.
<em>TIP</em>, <em>29</em>, 6800–6812. (<a
href="https://doi.org/10.1109/TIP.2020.2994410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is challenging due to unconstrained imaging factors, e.g., background clutters, non-uniform distribution of people, large scale and perspective variations. Dealing with these problems using deep neural networks requires rich prior knowledge and multi-scale contextual representations. In this paper, we propose a Cross-stage Refinement Network (CRNet) that can refine predicted density maps progressively based on hierarchical multi-level density priors. In particular, CRNet is composed of several fully convolutional networks. They are stacked together recursively with the previous output as the next input, and each of them serves to utilize previous density output to gradually correct prediction errors of crowd areas and refine the predicted density maps at different stages. Cross-stage multi-level density priors are further exploited in our recurrent framework by the cross-stage skip layers based on ConvLSTM. To cope with different challenges of unconstrained crowd scenes, we explore different crowd-specific data augmentation methods to mimic real-world scenarios and enrich crowd feature representations from different aspects. Extensive experiments show the proposed method achieves superior performances against state-of-the-art methods on four widely-used challenging benchmarks in terms of counting accuracy and density map quality. Code and models are available at this https://github.com/lytgftyf/Crowd-Counting-via-Cross-stage-Refinement-Networks.},
  archive      = {J_TIP},
  author       = {Yongtuo Liu and Qiang Wen and Haoxin Chen and Wenxi Liu and Jing Qin and Guoqiang Han and Shengfeng He},
  doi          = {10.1109/TIP.2020.2994410},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6800-6812},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Crowd counting via cross-stage refinement networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Group feedback capsule network. <em>TIP</em>, <em>29</em>,
6789–6799. (<a href="https://doi.org/10.1109/TIP.2020.2993931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In capsule networks (CapsNets), the capsule is made up of collections of neurons. Their adjacent capsule layers are connected using routing-by-agreement mechanisms in an unsupervised way. The routing-by-agreement mechanisms have two main drawbacks: a) too many parameters and high computation complexity; b) the cluster distribution assumptions of these routing mechanisms may not hold in some complex real-world data. In this paper, we propose a novel Group Feedback Capsule Network (GF-CapsNet) which adopts a supervised routing strategy called group-routing. Compared with the previous routing strategies which globally transform each capsule, Group-routing equally splits capsules into groups where capsules locally share the same transformation weights, reducing routing parameters. To address the second drawback, we devise a distance network to directly predict capsules in a supervised way without making distribution assumptions. Our proposed group-routing captures local information of low-level capsules by group-wise transformation and supervisedly predicts high-level ones in a feedback way to address two drawbacks respectively. We conduct experiments on CIFAR-10/100 and SVHN datasets and the results show that our method can perform better against state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Xinpeng Ding and Nannan Wang and Xinbo Gao and Jie Li and Xiaoyu Wang and Tongliang Liu},
  doi          = {10.1109/TIP.2020.2993931},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6789-6799},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Group feedback capsule network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PMHLD: Patch map-based hybrid learning DehazeNet for single
image haze removal. <em>TIP</em>, <em>29</em>, 6773–6788. (<a
href="https://doi.org/10.1109/TIP.2020.2993407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured in a hazy environment usually suffer from bad visibility and missing information. Over many years, learning-based and handcrafted prior-based dehazing algorithms have been rigorously developed. However, both algorithms exhibit some weaknesses in terms of haze removal performance. Therefore, in this work, we have proposed the patch-map-based hybrid learning DehazeNet, which integrates these two strategies by using a hybrid learning technique involving the patch map and a bi-attentive generative adversarial network. In this method, the reasons limiting the performance of the dark channel prior (DCP) have been analyzed. A new feature called the patch map has been defined for selecting the patch size adaptively. Using this map, the limitations of the DCP (e.g., color distortion and failure to recover images involving white scenes) can be addressed efficiently. In addition, to further enhance the performance of the method for haze removal, a patch-map-based DCP has been embedded into the network, and this module has been trained with the atmospheric light generator, patch map selection module, and refined module simultaneously. A combination of traditional and learning-based methods can efficiently improve the haze removal performance of the network. Experimental results show that the proposed method can achieve better reconstruction results compared to other state-of-the-art haze removal algorithms.},
  archive      = {J_TIP},
  author       = {Wei-Ting Chen and Hao-Yu Fang and Jian-Jiun Ding and Sy-Yen Kuo},
  doi          = {10.1109/TIP.2020.2993407},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6773-6788},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PMHLD: Patch map-based hybrid learning DehazeNet for single image haze removal},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Removing arbitrary-scale rain streaks via fractal band
learning with self-supervision. <em>TIP</em>, <em>29</em>, 6759–6772.
(<a href="https://doi.org/10.1109/TIP.2020.2993406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven rain streak removal methods, most of which rely on synthesized paired data, usually come across the generalization problem when being applied in real scenarios. In this paper, we propose a novel deep-learning based rain streak removal method injected with self-supervision to obtain the capacity of removing more varied-scale rain streaks in practical applications. To this end, in this work, efforts are made from two perspectives. First, considering that rain streak removal is highly correlated with texture characteristics, we create a fractal band learning (FBL) network based on frequency band recovery. It integrates commonly seen band feature operations as neural forms and effectively improves the capacity to capture discriminative features for deraining. Second, to further improve the generalization ability of FBL to remove rain streaks of varied scales, we incorporate scale-robust self-supervision to regularize the network training. The constraint forces the extracted features of an input rain image at different scales to be equivalent after rescaling operations. Therefore, our method can offer similar responses based on solely image content without the interference of scale change and is capable to remove varied-scale rain streaks. Extensive experiments in quantitative and qualitative evaluations demonstrate the superiority of our method for rain streak removal, especially for the real cases where very large rain streaks exist, and prove the effectiveness of each component.},
  archive      = {J_TIP},
  author       = {Wenhan Yang and Shiqi Wang and Jiaying Liu},
  doi          = {10.1109/TIP.2020.2993406},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6759-6772},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Removing arbitrary-scale rain streaks via fractal band learning with self-supervision},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised feature augmentation for large image object
detection. <em>TIP</em>, <em>29</em>, 6745–6758. (<a
href="https://doi.org/10.1109/TIP.2020.2993403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input scale plays an important role in modern detection frameworks, and an optimal training scale for images exists empirically. However, the optimal one usually cannot be reached in facing extremely large images under the memory constraint. In this study, we explore the scale effect inside the object detection pipeline and find that feature upsampling with the introduction of high-resolution information benefits the detection. Compared with direct input upscaling, feature upsampling trades a small performance loss for a large amount of memory savings. From these observations, we propose a self-supervised feature augmentation network, which takes downsampled images as inputs and aims to generate comparable features with the ones when feeding upscaled images to networks. We present a guided feature upsampling module, which takes downsampled images as inputs, to learn upscaled feature representations with the supervision of real large features acquired from upscaled images. In a self-supervised learning manner, we can introduce detailed information of images to the network. For an efficient feature upsampling, we design a residualized sub-pixel convolution block based on a sub-pixel convolution layer, which involves considerable information in upsampling process. Experiments on Mapillary Vistas Dataset (MVD), Cityscapes, and COCO are conducted to demonstrate the effectiveness of our method. On the MVD and Cityscapes detection benchmarks, in which the images are extremely large, our method surpasses current approaches. On COCO, the proposed method obtains comparable results to existing methods but with higher efficiency.},
  archive      = {J_TIP},
  author       = {Xingjia Pan and Fan Tang and Weiming Dong and Yang Gu and Zhichao Song and Yiping Meng and Pengfei Xu and Oliver Deussen and Changsheng Xu},
  doi          = {10.1109/TIP.2020.2993403},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6745-6758},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised feature augmentation for large image object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An unordered image stitching method based on binary tree and
estimated overlapping area. <em>TIP</em>, <em>29</em>, 6734–6744. (<a
href="https://doi.org/10.1109/TIP.2020.2993134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the complex computation and time-consuming problem during unordered image stitching, we present a method based on the binary tree and the estimated overlapping areas to stitch images without order in this paper. For image registration, the overlapping areas between input images are estimated, so that the extraction and matching of feature points are only performed in these areas. For image stitching, we build a model of the binary tree to stitch each two matched images without sorting. Compared to traditional methods, our method significantly reduces the computational time of matching irrelevant image pairs and improves the efficiency of image registration and stitching. Moreover, the stitching model of the binary tree proposed in this paper further reduces the distortion of the panorama. Experimental results show that the number of extracted feature points in the estimated overlapping area is approximately 0.3~0.6 times of that in the entire image by using the same method, which greatly reduces the computational time of feature extraction and matching. Compared to the exhaustive image matching method, our approach only takes about 1/3 of the time to find all matching images.},
  archive      = {J_TIP},
  author       = {Zhong Qu and Jun Li and Kang-Hua Bao and Zhi-Chao Si},
  doi          = {10.1109/TIP.2020.2993134},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6734-6744},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An unordered image stitching method based on binary tree and estimated overlapping area},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-exponential transverse relaxation times estimation
from magnetic resonance images under rician noise and spatial
regularization. <em>TIP</em>, <em>29</em>, 6721–6733. (<a
href="https://doi.org/10.1109/TIP.2020.2993114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relaxation signal inside each voxel of magnetic resonance images (MRI) is commonly fitted by a multi-exponential decay curve. The estimation of a discrete multi-component relaxation model parameters from magnitude MRI data is a challenging nonlinear inverse problem since it should be conducted on the entire image voxels under non-Gaussian noise statistics. This paper proposes an efficient algorithm allowing the joint estimation of relaxation time values and their amplitudes using different criteria taking into account a Rician noise model, combined with a spatial regularization accounting for low spatial variability of relaxation time constants and amplitudes between neighboring voxels. The Rician noise hypothesis is accounted for either by an adapted nonlinear least squares algorithm applied to a corrected least squares criterion or by a majorization-minimization approach applied to the maximum likelihood criterion. In order to solve the resulting large-scale non-negativity constrained optimization problem with a reduced numerical complexity and computing time, an optimization algorithm based on a majorization approach ensuring separability of variables between voxels is proposed. The minimization is carried out iteratively using an adapted Levenberg-Marquardt algorithm that ensures convergence by imposing a sufficient decrease of the objective function and the non-negativity of the parameters. The importance of the regularization alongside the Rician noise incorporation is shown both visually and numerically on a simulated phantom and on magnitude MRI images acquired on fruit samples.},
  archive      = {J_TIP},
  author       = {Christian El Hajj and Saïd Moussaoui and Guylaine Collewet and Maja Musse},
  doi          = {10.1109/TIP.2020.2993114},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6721-6733},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-exponential transverse relaxation times estimation from magnetic resonance images under rician noise and spatial regularization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpret neural networks by extracting critical
subnetworks. <em>TIP</em>, <em>29</em>, 6707–6720. (<a
href="https://doi.org/10.1109/TIP.2020.2993098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks have achieved excellent performance in many fields of artificial intelligence. The requirements for the interpretability and robustness of neural networks are also increasing. In this paper, we propose to understand the functional mechanism of neural networks by extracting critical subnetworks. Specifically, we denote the critical subnetworks as a group of important channels across layers such that if they were suppressed to zeros, the final test performance would deteriorate severely. This novel perspective can not only reveal the layerwise semantic behavior within the model but also present more accurate visual explanations appearing in the data through attribution methods. Moreover, we propose two adversarial example detection methods based on the properties of sample-specific and class-specific subnetworks, which provides the possibility for increasing the model robustness.},
  archive      = {J_TIP},
  author       = {Yulong Wang and Hang Su and Bo Zhang and Xiaolin Hu},
  doi          = {10.1109/TIP.2020.2993098},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6707-6720},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interpret neural networks by extracting critical subnetworks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-term tracking with deep tracklet association.
<em>TIP</em>, <em>29</em>, 6694–6706. (<a
href="https://doi.org/10.1109/TIP.2020.2993073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, most multiple object tracking (MOT) algorithms adopt the idea of tracking-by-detection. Relevant research shows that the performance of the detector obviously affects the tracker, while the improvement of detector is gradually slowing down in recent years. Therefore, trackers using tracklet (short trajectory) are proposed to generate more complete trajectories. Although there are various tracklet generation algorithms, the fragmentation problem still often occurs in crowded scenes. In this paper, we introduce an iterative clustering method that generates more tracklets while maintaining high confidence. Our method shows robust performance on avoiding internal identity switch. Then we propose a deep association method for tracklet association. In terms of motion and appearance, we construct motion evaluation network (MEN) and appearance evaluation network (AEN) to learn long-term features of tracklets for association. In order to explore more robust features of tracklets, a tracklet-based training mechanism is also introduced. Tracklet groups are used as the input of the networks instead of discrete detections. Experimental results show that our training method enhances the performance of the networks. In addition, our tracking framework generates more complete trajectories while maintaining the unique identity of each target as the same time. On the latest MOT 2017 benchmark, we achieve state-of-the-art results.},
  archive      = {J_TIP},
  author       = {Yang Zhang and Hao Sheng and Yubin Wu and Shuai Wang and Weifeng Lyu and Wei Ke and Zhang Xiong},
  doi          = {10.1109/TIP.2020.2993073},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6694-6706},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Long-term tracking with deep tracklet association},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured dictionary learning for image denoising under
mixed gaussian and impulse noise. <em>TIP</em>, <em>29</em>, 6680–6693.
(<a href="https://doi.org/10.1109/TIP.2020.2992895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although image denoising as a basic task of image restoration has been widely studied in the past decades, there are not many studies on mixed noise denoising. In this paper, we propose two structured dictionary learning models to recover images corrupted by mixed Gaussian and impulse noise. These two models can be merged as l p -norm fidelity plus l q -norm regularization. The fidelity term is used to fit image patches and the regularization term is employed for sparse coding. Particularly, we utilize proximal (and proximal linearized) alternating minimization methods as the main solvers to deal with these two models. We remove the Gaussian noise under the assumption that the uncorrupted image can be approximated with a linear representation under an appropriate orthogonal basis. We use different ways to remove impulse noise for these two models. The experimental results are reported to compare the existing methods and demonstrate the performance of the proposed denoising model is better than the other existing methods in terms of some quality assessment metrics.},
  archive      = {J_TIP},
  author       = {Hong Zhu and Michael K. Ng},
  doi          = {10.1109/TIP.2020.2992895},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6680-6693},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structured dictionary learning for image denoising under mixed gaussian and impulse noise},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three dimensional root CT segmentation using
multi-resolution encoder-decoder networks. <em>TIP</em>, <em>29</em>,
6667–6679. (<a href="https://doi.org/10.1109/TIP.2020.2992893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the complex problem of reliably segmenting root structure from soil in X-ray Computed Tomography (CT) images. We utilise a deep learning approach, and propose a state-of-the-art multi-resolution architecture based on encoder-decoders. While previous work in encoder-decoders implies the use of multiple resolutions simply by downsampling and upsampling images, we make this process explicit, with branches of the network tasked separately with obtaining local high-resolution segmentation, and wider low-resolution contextual information. The complete network is a memory efficient implementation that is still able to resolve small root detail in large volumetric images. We compare against a number of different encoder-decoder based architectures from the literature, as well as a popular existing image analysis tool designed for root CT segmentation. We show qualitatively and quantitatively that a multi-resolution approach offers substantial accuracy improvements over a both a small receptive field size in a deep network, or a larger receptive field in a shallower network. We then further improve performance using an incremental learning approach, in which failures in the original network are used to generate harder negative training examples. Our proposed method requires no user interaction, is fully automatic, and identifies large and fine root material throughout the whole volume.},
  archive      = {J_TIP},
  author       = {Mohammadreza Soltaninejad and Craig J. Sturrock and Marcus Griffiths and Tony P. Pridmore and Michael P. Pound},
  doi          = {10.1109/TIP.2020.2992893},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6667-6679},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Three dimensional root CT segmentation using multi-resolution encoder-decoder networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Textual-visual reference-aware attention network for visual
dialog. <em>TIP</em>, <em>29</em>, 6655–6666. (<a
href="https://doi.org/10.1109/TIP.2020.2992888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual dialog is a challenging task in multimedia understanding, which requires the dialog agent to answer a series of questions that are based on an input image. The critical issue to produce an exact answer is how to model the mutual semantic interaction among feature representations of the image, question-answer history, and current question. In this study, we propose a textual-visual Reference-Aware Attention Network (RAA-Net), which aims to effectively fuse Q (question), H (history), Vl (local vision), and Vg (global vision) to infer the exact answer. In the multimodal feature flows, RAA-Net first learns the textual context through multi-head attention between Q and H and then guides the textual reference semantics to the image to capture visual reference semantics by self-and cross-reference-aware attention in and between Vl and Vg. In the proposed RAA-Net, we exploit the two-stage (intraand inter-) visual reasoning mechanism on Vl and Vg. Extensive experiments on the VisDial v0.9 and v1.0 datasets show that RAA-Net achieves state-of-the-art performance. Visualization results on both visual and textual attention maps further validate the remarkable interpretability achieved by our solution.},
  archive      = {J_TIP},
  author       = {Dan Guo and Hui Wang and Shuhui Wang and Meng Wang},
  doi          = {10.1109/TIP.2020.2992888},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6655-6666},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Textual-visual reference-aware attention network for visual dialog},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SAR image speckle filtering with context covariance matrix
formulation and similarity test. <em>TIP</em>, <em>29</em>, 6641–6654.
(<a href="https://doi.org/10.1109/TIP.2020.2992883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speckle filtering of synthetic aperture radar (SAR) image is a necessary pre-processing for many subsequent applications. The challenge lies in how to adaptively select a sufficient number of similar pixels for an unbiased estimator generation. A novel SAR speckle filter is proposed and the core idea contains two aspects. Firstly, a context covariance matrix representation is developed within a local neighborhood to characterize the contexture information. Then, the Wishart statistic test is extended to examine the similarity of context covariance matrices. The extended similarity test indicator derived from context covariance matrices is verified to be sensitive for similar pixel localization. Thereafter, a sample averaging estimator is adopted based on the similar samples determined by the context covariance matrices similarity test (the proposed method is named as the CCM+SimiTest). Furthermore, a fast similarity test computation scheme is established which can handle large images smoothly even with a normal laptop. Intensive experimental studies with Radarsat-2, MiniSAR and ALOS-2 datasets are carried out. Comparisons with several state-of-the-art methods from both subjective and objective viewpoints demonstrate the superiority of the proposed method.},
  archive      = {J_TIP},
  author       = {Si-Wei Chen},
  doi          = {10.1109/TIP.2020.2992883},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6641-6654},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SAR image speckle filtering with context covariance matrix formulation and similarity test},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Light field synthesis by training deep network in the
refocused image domain. <em>TIP</em>, <em>29</em>, 6630–6640. (<a
href="https://doi.org/10.1109/TIP.2020.2992354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field imaging, which captures spatial-angular information of light incident on image sensors, enables many interesting applications such as image refocusing and augmented reality. However, due to the limited sensor resolution, a trade-off exists between the spatial and angular resolutions. To increase the angular resolution, view synthesis techniques have been adopted to generate new views from existing views. However, traditional learning-based view synthesis mainly considers the image quality of each view of the light field and neglects the quality of the refocused images. In this paper, we propose a new loss function called refocused image error (RIE) to address the issue. The main idea is that the image quality of the synthesized light field should be optimized in the refocused image domain because it is where the light field is viewed. We analyze the behavior of RIE in the spectral domain and test the performance of our approach against previous approaches on both real (INRIA) and software-rendered (HCI) light field datasets using objective assessment metrics such as MSE, MAE, PSNR, SSIM, and GMSD. Experimental results show that the light field generated by our method results in better refocused images than previous methods.},
  archive      = {J_TIP},
  author       = {Chang-Le Liu and Kuang-Tsu Shih and Jiun-Woei Huang and Homer H. Chen},
  doi          = {10.1109/TIP.2020.2992354},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6630-6640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Light field synthesis by training deep network in the refocused image domain},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust estimation of absolute camera pose via intersection
constraint and flow consensus. <em>TIP</em>, <em>29</em>, 6615–6629. (<a
href="https://doi.org/10.1109/TIP.2020.2992336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the absolute camera pose requires 3D-to-2D correspondences of points and/or lines. However, in practice, these correspondences are inevitably corrupted by outliers, which affects the pose estimation. Existing outlier removal strategies for robust pose estimation have some limitations. They are only applicable to points, rely on prior pose information, or fail to handle high outlier ratios. By contrast, we propose a general and accurate outlier removal strategy. It can be integrated with various existing pose estimation methods originally vulnerable to outliers, and is applicable to points, lines, and the combination of both. Moreover, it does not rely on any prior pose information. Our strategy has a nested structure composed of the outer and inner modules. First, our outer module leverages our intersection constraint, i.e., the projection rays or planes defined by inliers intersect at the camera center. Our outer module alternately computes the inlier probabilities of correspondences and estimates the camera pose. It can run reliably and efficiently under high outlier ratios. Second, our inner module exploits our flow consensus. The 2D displacement vectors or 3D directed arcs generated by inliers exhibit a common directional regularity, i.e., follow a dominant trend of flow. Our inner module refines the inlier probabilities obtained at each iteration of our outer module. This refinement improves the accuracy and facilitates the convergence of our outer module. Experiments on both synthetic data and real-world images have shown that our method outperforms state-of-the-art approaches in terms of accuracy and robustness.},
  archive      = {J_TIP},
  author       = {Haoang Li and Ji Zhao and Jean-Charles Bazin and Yun-Hui Liu},
  doi          = {10.1109/TIP.2020.2992336},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6615-6629},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust estimation of absolute camera pose via intersection constraint and flow consensus},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Polarimetric SAR image semantic segmentation with 3D
discrete wavelet transform and markov random field. <em>TIP</em>,
<em>29</em>, 6601–6614. (<a
href="https://doi.org/10.1109/TIP.2020.2992177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarimetric synthetic aperture radar (PolSAR) image segmentation is currently of great importance in image processing for remote sensing applications. However, it is a challenging task due to two main reasons. Firstly, the label information is difficult to acquire due to high annotation costs. Secondly, the speckle effect embedded in the PolSAR imaging process remarkably degrades the segmentation performance. To address these two issues, we present a contextual PolSAR image semantic segmentation method in this paper. With a newly defined channel-wise consistent feature set as input, the three-dimensional discrete wavelet transform (3D-DWT) technique is employed to extract discriminative multi-scale features that are robust to speckle noise. Then Markov random field (MRF) is further applied to enforce label smoothness spatially during segmentation. By simultaneously utilizing 3D-DWT features and MRF priors for the first time, contextual information is fully integrated during the segmentation to ensure accurate and smooth segmentation. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on three real benchmark PolSAR image data sets. Experimental results indicate that the proposed method achieves promising segmentation accuracy and preferable spatial consistency using a minimal number of labeled pixels.},
  archive      = {J_TIP},
  author       = {Haixia Bi and Lin Xu and Xiangyong Cao and Yong Xue and Zongben Xu},
  doi          = {10.1109/TIP.2020.2992177},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6601-6614},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Polarimetric SAR image semantic segmentation with 3D discrete wavelet transform and markov random field},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FADE: Feature aggregation for depth estimation with
multi-view stereo. <em>TIP</em>, <em>29</em>, 6590–6600. (<a
href="https://doi.org/10.1109/TIP.2020.2991883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both structural and contextual information is essential and widely used in image analysis. However, current multi-view stereo (MVS) approaches usually use a single common pre-trained model as pixel descriptor to extract features, which mix structural and contextual information together and thus increase the difficulty of matching correspondence. In this paper, we propose FADE (feature aggregation for depth estimation), which treats spatial and context information separately and focuses on aggregating features for efficient learning of the MVS problem. Spatial information includes image details such as edges and corners, whereas context information comprises object features such as shapes and traits. To aggregate these multi-level features, we use an attention mechanism to select important features for matching. We then build a plane sweep volume by using a homography backward warping method to generate match candidates. Furthermore, we propose a novel cost volume regularization network aims to minimize the noise in the matching candidates. Finally, we take advantage of 3D stacked hourglass and regression to produces high-quality depth maps. With these well-aggregated features, FADE can efficiently perform dense depth reconstruction, achieving state-of-the-art performance in terms of accuracy and requiring the least amount of model parameters.},
  archive      = {J_TIP},
  author       = {Hsiao-Chien Yang and Po-Heng Chen and Kuan-Wen Chen and Chen-Yi Lee and Yong-Sheng Chen},
  doi          = {10.1109/TIP.2020.2991883},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6590-6600},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FADE: Feature aggregation for depth estimation with multi-view stereo},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A unified deep model for joint facial expression
recognition, face synthesis, and face alignment. <em>TIP</em>,
<em>29</em>, 6574–6589. (<a
href="https://doi.org/10.1109/TIP.2020.2991549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition, face synthesis, and face alignment are three coherently related tasks and can be solved in a joint framework. To achieve this goal, in this paper, we propose a novel end-to-end deep learning model by exploiting the expression code, geometry code and generated data jointly for simultaneous pose-invariant facial expression recognition, face image synthesis, and face alignment. The proposed deep model enjoys several merits. First, to the best of our knowledge, this is the first work to address these three tasks jointly in a unified deep model to complement and enhance each other. Second, the proposed model can effectively disentangle the global and local identity representation from different expression and geometry codes. As a result, it can automatically generate facial images with different expressions under arbitrary geometry codes. Third, these three tasks can further boost their performance for each other via our model. Extensive experimental results on three standard benchmarks demonstrate that the proposed deep model performs favorably against state-of-the-art methods on the three tasks.},
  archive      = {J_TIP},
  author       = {Feifei Zhang and Tianzhu Zhang and Qirong Mao and Changsheng Xu},
  doi          = {10.1109/TIP.2020.2991549},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6574-6589},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A unified deep model for joint facial expression recognition, face synthesis, and face alignment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel deep learning pipeline for retinal vessel detection
in fluorescein angiography. <em>TIP</em>, <em>29</em>, 6561–6573. (<a
href="https://doi.org/10.1109/TIP.2020.2991530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While recent advances in deep learning have significantly advanced the state of the art for vessel detection in color fundus (CF) images, the success for detecting vessels in fluorescein angiography (FA) has been stymied due to the lack of labeled ground truth datasets. We propose a novel pipeline to detect retinal vessels in FA images using deep neural networks (DNNs) that reduces the effort required for generating labeled ground truth data by combining two key components: cross-modality transfer and human-in-the-loop learning. The cross-modality transfer exploits concurrently captured CF and fundus FA images. Binary vessels maps are first detected from CF images with a pre-trained neural network and then are geometrically registered with and transferred to FA images via robust parametric chamfer alignment to a preliminary FA vessel detection obtained with an unsupervised technique. Using the transferred vessels as initial ground truth labels for deep learning, the human-in-the-loop approach progressively improves the quality of the ground truth labeling by iterating between deep-learning and labeling. The approach significantly reduces manual labeling effort while increasing engagement. We highlight several important considerations for the proposed methodology and validate the performance on three datasets. Experimental results demonstrate that the proposed pipeline significantly reduces the annotation effort and the resulting deep learning methods outperform prior existing FA vessel detection methods by a significant margin. A new public dataset, RECOVERY-FA19, is introduced that includes high-resolution ultra-widefield images and accurately labeled ground truth binary vessel maps.},
  archive      = {J_TIP},
  author       = {Li Ding and Mohammad H. Bawany and Ajay E. Kuriyan and Rajeev S. Ramchandran and Charles C. Wykoff and Gaurav Sharma},
  doi          = {10.1109/TIP.2020.2991530},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6561-6573},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel deep learning pipeline for retinal vessel detection in fluorescein angiography},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep ranking for image zero-shot multi-label classification.
<em>TIP</em>, <em>29</em>, 6549–6560. (<a
href="https://doi.org/10.1109/TIP.2020.2991527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past decade, both multi-label learning and zero-shot learning have attracted huge research attention, and significant progress has been made. Multi-label learning algorithms aim to predict multiple labels given one instance, while most existing zero-shot learning approaches target at predicting a single testing label for each unseen class via transferring knowledge from auxiliary seen classes to target unseen classes. However, relatively less effort has been made on predicting multiple labels in the zero-shot setting, which is nevertheless a quite challenging task. In this work, we investigate and formalize a flexible framework consisting of two components, i.e., visual-semantic embedding and zero-shot multi-label prediction. First, we present a deep regression model to project the visual features into the semantic space, which explicitly exploits the correlations in the intermediate semantic layer of word vectors and makes label prediction possible. Then, we formulate the label prediction problem as a pairwise one and employ Ranking SVM to seek the unique multi-label correlations in the embedding space. Furthermore, we provide a transductive multi-label zero-shot prediction approach that exploits the testing data manifold structure. We demonstrate the effectiveness of the proposed approach on three popular multi-label datasets with state-of-the-art performance obtained on both conventional and generalized ZSL settings.},
  archive      = {J_TIP},
  author       = {Zhong Ji and Biying Cui and Huihui Li and Yu-Gang Jiang and Tao Xiang and Timothy Hospedales and Yanwei Fu},
  doi          = {10.1109/TIP.2020.2991527},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6549-6560},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep ranking for image zero-shot multi-label classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic neighborhood-aware deep facial expression
recognition. <em>TIP</em>, <em>29</em>, 6535–6548. (<a
href="https://doi.org/10.1109/TIP.2020.2991510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from many other attributes, facial expression can change in a continuous way, and therefore, a slight semantic change of input should also lead to the output fluctuation limited in a small scale. This consistency is important. However, current Facial Expression Recognition (FER) datasets may have the extreme imbalance problem, as well as the lack of data and the excessive amounts of noise, hindering this consistency and leading to a performance decreasing when testing. In this paper, we not only consider the prediction accuracy on sample points, but also take the neighborhood smoothness of them into consideration, focusing on the stability of the output with respect to slight semantic perturbations of the input. A novel method is proposed to formulate semantic perturbation and select unreliable samples during training, reducing the bad effect of them. Experiments show the effectiveness of the proposed method and state-of-the-art results are reported, getting closer to an upper limit than the state-of-the-art methods by a factor of 30\% in AffectNet, the largest in-the-wild FER database by now.},
  archive      = {J_TIP},
  author       = {Yongjian Fu and Xintian Wu and Xi Li and Zhijie Pan and Daxin Luo},
  doi          = {10.1109/TIP.2020.2991510},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6535-6548},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic neighborhood-aware deep facial expression recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Task-oriented network for image dehazing. <em>TIP</em>,
<em>29</em>, 6523–6534. (<a
href="https://doi.org/10.1109/TIP.2020.2991509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze interferes the transmission of scene radiation and significantly degrades color and details of outdoor images. Existing deep neural networks-based image dehazing algorithms usually use some common networks. The network design does not model the image formation of haze process well, which accordingly leads to dehazed images containing artifacts and haze residuals in some special scenes. In this paper, we propose a task-oriented network for image dehazing, where the network design is motivated by the image formation of haze process. The task-oriented network involves a hybrid network containing an encoder and decoder network and a spatially variant recurrent neural network which is derived from the hazy process. In addition, we develop a multi-stage dehazing algorithm to further improve the accuracy by filtering haze residuals in a step-by-step fashion. To constrain the proposed network, we develop a dual composition loss, content-based pixel-wise loss and total variation constraint. We train the proposed network in an end-to-end manner and analyze its effect on image dehazing. Experimental results demonstrate that the proposed algorithm achieves favorable performance against state-of-the-art dehazing methods.},
  archive      = {J_TIP},
  author       = {Runde Li and Jinshan Pan and Min He and Zechao Li and Jinhui Tang},
  doi          = {10.1109/TIP.2020.2991509},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6523-6534},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Task-oriented network for image dehazing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation of MR brain images through hidden markov random
field and hybrid metaheuristic algorithm. <em>TIP</em>, <em>29</em>,
6507–6522. (<a href="https://doi.org/10.1109/TIP.2020.2990346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is one of the most critical tasks in Magnetic Resonance (MR) images analysis. Since the performance of most current image segmentation methods is suffered by noise and intensity non-uniformity artifact (INU), a precise and artifact resistant method is desired. In this work, we propose a new segmentation method combining a new Hidden Markov Random Field (HMRF) model and a novel hybrid metaheuristic method based on Cuckoo search (CS) and Particle swarm optimization algorithms (PSO). The new model uses adaptive parameters to allow balancing between the segmented components of the model. In addition, to improve the quality of searching solutions in the Maximum a posteriori (MAP) estimation of the HMRF model, the hybrid metaheuristic algorithm is introduced. This algorithm takes into account both the advantages of CS and PSO algorithms in searching ability by cooperating them with the same population in a parallel way and with a solution selection mechanism. Since CS and PSO are performing exploration and exploitation in the search space, respectively, hybridizing them in an intelligent way can provide better solutions in terms of quality. Furthermore, initialization of the population is carefully taken into account to improve the performance of the proposed method. The whole algorithm is evaluated on benchmark images including both the simulated and real MR brain images. Experimental results show that the proposed method can achieve satisfactory performance for images with noise and intensity inhomogeneity, and provides better results than its considered competitors.},
  archive      = {J_TIP},
  author       = {Thuy Xuan Pham and Patrick Siarry and Hamouche Oulhadj},
  doi          = {10.1109/TIP.2020.2990346},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6507-6522},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Segmentation of MR brain images through hidden markov random field and hybrid metaheuristic algorithm},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). No-reference image quality assessment: An attention driven
approach. <em>TIP</em>, <em>29</em>, 6496–6506. (<a
href="https://doi.org/10.1109/TIP.2020.2990342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle no-reference image quality assessment (NR-IQA), which aims to predict the perceptual quality of a distorted image without referencing its pristine-quality counterpart. Inspired by the free-energy principle, we assume that, while perceiving a distorted image, the human visual system (HVS) tends to predict the pristine image then estimates the perceptual quality based on the distorted-restored pair. Furthermore, the perceptual quality depends heavily on the way how human beings attend to distorted images, namely, the cooperation of foveal vision and the eye movement mechanism. Inspired by these properties of the HVS, given the distorted-restored pair, we implement an attention-driven NR-IQA method with reinforcement learning (RL). The model learns a policy to attend to several regions parallelly. The observations of the fixation regions are aggregated in a weighted average way, which is inspired by the robust averaging strategy. For policy learning, the rewards are derived from two tasks-classifying the distortion type and estimating the perceptual score. The goal of policy learning is to maximize the expectation of the accumulated rewards. Extensive experiments on LIVE, TID2008, TID2013 and CSIQ demonstrate the superiority of our methods.},
  archive      = {J_TIP},
  author       = {Diqi Chen and Yizhou Wang and Wen Gao},
  doi          = {10.1109/TIP.2020.2990342},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6496-6506},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No-reference image quality assessment: An attention driven approach},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OSLNet: Deep small-sample classification with an orthogonal
softmax layer. <em>TIP</em>, <em>29</em>, 6482–6495. (<a
href="https://doi.org/10.1109/TIP.2020.2990277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep neural network of multiple nonlinear layers forms a large function space, which can easily lead to overfitting when it encounters small-sample data. To mitigate overfitting in small-sample classification, learning more discriminative features from small-sample data is becoming a new trend. To this end, this paper aims to find a subspace of neural networks that can facilitate a large decision margin. Specifically, we propose the Orthogonal Softmax Layer (OSL), which makes the weight vectors in the classification layer remain orthogonal during both the training and test processes. The Rademacher complexity of a network using the OSL is only 1K, where K is the number of classes, of that of a network using the fully connected classification layer, leading to a tighter generalization error bound. Experimental results demonstrate that the proposed OSL has better performance than the methods used for comparison on four small-sample benchmark datasets, as well as its applicability to large-sample datasets. Codes are available at: https://github.com/dongliangchang/OSLNet.},
  archive      = {J_TIP},
  author       = {Xiaoxu Li and Dongliang Chang and Zhanyu Ma and Zheng-Hua Tan and Jing-Hao Xue and Jie Cao and Jingyi Yu and Jun Guo},
  doi          = {10.1109/TIP.2020.2990277},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6482-6495},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {OSLNet: Deep small-sample classification with an orthogonal softmax layer},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shearlet enhanced snapshot compressive imaging.
<em>TIP</em>, <em>29</em>, 6466–6481. (<a
href="https://doi.org/10.1109/TIP.2020.2989550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snapshot compressive imaging (SCI) is a promising approach to capture high-dimensional data with low dimensional sensors. With modest modifications to off-the-shelf cameras, SCI cameras encode multiple frames into a single measurement frame. These correlated frames can then be retrieved by reconstruction algorithms. Existing reconstruction algorithms suffer from low speed or low fidelity. In this paper, we propose a novel reconstruction algorithm, namely, Shearlet enhanced Snapshot Compressive Imaging (SeSCI), which exploits the sparsity of the image representation in both frequency domain and shearlet domain. Towards this end, we first derive our SeSCI algorithm under the alternating direction method of multipliers (ADMM) framework. We then propose an efficient solution of SeSCI algorithm. Moreover, we prove that the improved SeSCI algorithm converges to a fixed point. Experimental results on both synthetic data and real data captured by SCI cameras demonstrate the significant advantages of SeSCI, which outperforms the conventional algorithms by more than 2dB in PSNR. At the same time, the SeSCI achieves a speed-up more than 100× over the state-of-the-art algorithm.},
  archive      = {J_TIP},
  author       = {Peihao Yang and Linghe Kong and Xiao-Yang Liu and Xin Yuan and Guihai Chen},
  doi          = {10.1109/TIP.2020.2989550},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6466-6481},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Shearlet enhanced snapshot compressive imaging},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning from synthetic images via active pseudo-labeling.
<em>TIP</em>, <em>29</em>, 6452–6465. (<a
href="https://doi.org/10.1109/TIP.2020.2989100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic visual data refers to the data automatically rendered by the mature computer graphic algorithms. With the rapid development of these techniques, we can now collect photo-realistic synthetic images with accurate pixel-level annotations without much effort. However, due to the domain gaps between synthetic data and real data, in terms of not only visual appearance but also label distribution, directly applying models trained on synthetic images to real ones can hardly yield satisfactory performance. Since the collection of accurate labels for real images is very laborious and time-consuming, developing algorithms which can learn from synthetic images is of great significance. In this paper, we propose a novel framework, namely Active Pseudo-Labeling (APL), to reduce the domain gaps between synthetic images and real images. In APL framework, we first predict pseudo-labels for the unlabeled real images in the target domain by actively adapting the style of the real images to source domain. Specifically, the style of real images is adjusted via a novel task guided generative model, and then pseudo-labels are predicted for these actively adapted images. Lastly, we fine-tune the source-trained model in the pseudo-labeled target domain, which helps to fit the distribution of the real data. Experiments on both semantic segmentation and object detection tasks with several challenging benchmark data sets demonstrate the priority of our proposed method compared to the existing state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Liangchen Song and Yonghao Xu and Lefei Zhang and Bo Du and Qian Zhang and Xinggang Wang},
  doi          = {10.1109/TIP.2020.2989100},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6452-6465},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning from synthetic images via active pseudo-labeling},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PiCANet: Pixel-wise contextual attention learning for
accurate saliency detection. <em>TIP</em>, <em>29</em>, 6438–6451. (<a
href="https://doi.org/10.1109/TIP.2020.2988568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing saliency models typically incorporate contexts holistically. However, for each pixel, usually only part of its context region contributes to saliency prediction, while other parts are likely either noise or distractions. In this paper, we propose a novel pixel-wise contextual attention network (PiCANet) to selectively attend to informative context locations at each pixel. The proposed PiCANet generates an attention map over the contextual region of each pixel and construct attentive contextual features via selectively incorporating the features of useful context locations. We present three formulations of the PiCANet via embedding the pixel-wise contextual attention mechanism into the pooling and convolution operations with attending to global or local contexts. All the three models are fully differentiable and can be integrated with convolutional neural networks with joint training. In this work, we introduce the proposed PiCANets into a U-Net model for salient object detection. The generated global and local attention maps can learn to incorporate global contrast and regional smoothness, which help localize and highlight salient objects more accurately and uniformly. Experimental results show that the proposed PiCANets perform effectively for saliency detection against the state-of-the-art methods. Furthermore, we demonstrate the effectiveness and generalization ability of the PiCANets on semantic segmentation and object detection with improved performance.},
  archive      = {J_TIP},
  author       = {Nian Liu and Junwei Han and Ming-Hsuan Yang},
  doi          = {10.1109/TIP.2020.2988568},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6438-6451},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PiCANet: Pixel-wise contextual attention learning for accurate saliency detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interweaved prediction for video coding. <em>TIP</em>,
<em>29</em>, 6422–6437. (<a
href="https://doi.org/10.1109/TIP.2020.2987432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the emerging next generation video coding standard Versatile Video Coding (VVC) developed by the Joint Video Exploration Team (JVET), sub-block-based inter-prediction plays a key role in promising coding tools such as Affine Motion Compensation (AMC) and sub-block-based Temporal Motion Vector Prediction (sbTMVP). With sub-block-based inter-prediction, a coding block is divided into sub-blocks, and the motion information of each sub-block is derived individually. Although sub-block-based inter-prediction can provide a higher quality prediction benefiting from a finer motion granularity, it still suffers two problems: uneven prediction quality and boundary discontinuity. In this paper, we present a method of interweaved prediction to further improve sub-block-based inter-prediction. With interweaved prediction, a coding block with AMC or sbTMVP mode is divided into sub-blocks with two different dividing patterns, so that a corner position of a sub-block in one dividing pattern coincides with the central position of a sub-block in the other dividing pattern. Then two auxiliary predictions are generated by AMC or sbTMVP with the two dividing patterns, independently. The final prediction is calculated as a weighted-sum of the two auxiliary predictions. Theoretical analysis and statistical data prove that interweaved prediction can significantly mitigate the two problems in sub-block-based inter-prediction. Simulation results show that the proposed methods can achieve 0.64\% BD-rate saving on average with the random access configurations. On sequences with rich affine motions, the average BD-rate saving can be up to 2.54\%.},
  archive      = {J_TIP},
  author       = {Kai Zhang and Li Zhang and Hongbin Liu and Jizheng Xu and Zhipin Deng and Yue Wang},
  doi          = {10.1109/TIP.2020.2987432},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6422-6437},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interweaved prediction for video coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Fast optical flow extraction from compressed video.
<em>TIP</em>, <em>29</em>, 6409–6421. (<a
href="https://doi.org/10.1109/TIP.2020.2985866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the fast optical flow extractor, a filtering method that recovers artifact-free optical flow fields from HEVC-compressed video. To extract accurate optical flow fields, we form a regularized optimization problem that considers the smoothness of the solution and the pixelwise confidence weights of an artifact- ridden HEVC motion field. Solving such an optimization problem is slow, so we first convert the problem into a confidence-weighted filtering task. By leveraging the already-available HEVC motion parameters, we achieve a 100-fold speed-up in the running times compared to similar methods, while producing subpixel-accurate flow estimates. The fast optical flow extractor is useful when video frames are already available in coded formats. Our method is not specific to a coder, and works with motion fields from video coders such as H.264/AVC and HEVC.},
  archive      = {J_TIP},
  author       = {Sean I. Young and Bernd Girod and David Taubman},
  doi          = {10.1109/TIP.2020.2985866},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6409-6421},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast optical flow extraction from compressed video},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NPSA: Nonorthogonal principal skewness analysis.
<em>TIP</em>, <em>29</em>, 6396–6408. (<a
href="https://doi.org/10.1109/TIP.2020.2984849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal skewness analysis (PSA) has been introduced for feature extraction in hyperspectral imagery. As a third-order generalization of principal component analysis (PCA), its solution of searching for the local maximum skewness direction is transformed into the problem of calculating the eigenpairs (the eigenvalues and the corresponding eigenvectors) of a coskewness tensor. By combining a fixed-point method with an orthogonal constraint, the new eigenpairs are prevented from converging to the same previously determined maxima. However, in general, the eigenvectors of the supersymmetric tensor are not inherently orthogonal, which implies that the results obtained by the search strategy used in PSA may unavoidably deviate from the actual eigenpairs. In this paper, we propose a new nonorthogonal search strategy to solve this problem and the new algorithm is named nonorthogonal principal skewness analysis (NPSA). The contribution of NPSA lies in the finding that the search space of the eigenvector to be determined can be enlarged by using the orthogonal complement of the Kronecker product of the previous eigenvector with itself, instead of its orthogonal complement space. We also give a detailed theoretical proof on why we can obtain the more accurate eigenpairs through the new search strategy by comparison with PSA. In addition, after some algebraic derivations, the complexity of the presented algorithm is also greatly reduced. Experiments with both simulated data and real multi/hyperspectral imagery demonstrate its validity in feature extraction.},
  archive      = {J_TIP},
  author       = {Xiurui Geng and Lei Wang},
  doi          = {10.1109/TIP.2020.2984849},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6396-6408},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NPSA: Nonorthogonal principal skewness analysis},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AF-net: A convolutional neural network approach to phase
detection autofocus. <em>TIP</em>, <em>29</em>, 6386–6395. (<a
href="https://doi.org/10.1109/TIP.2019.2947349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important for an autofocus system to accurately and quickly find the in-focus lens position so that sharp images can be captured without human intervention. Phase detectors have been embedded in image sensors to improve the performance of autofocus; however, the phase shift estimation between the left and right phase images is sensitive to noise. In this paper, we propose a robust model based on convolutional neural network to address this issue. Our model includes four convolutional layers to extract feature maps from the phase images and a fully-connected network to determine the lens movement. The final lens position error of our model is five times smaller than that of a state-of-the-art statistical PDAF method. Furthermore, our model works consistently well for all initial lens positions. All these results verify the robustness of our model.},
  archive      = {J_TIP},
  author       = {Chi-Jui Ho and Chin-Cheng Chan and Homer H. Chen},
  doi          = {10.1109/TIP.2019.2947349},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6386-6395},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AF-net: A convolutional neural network approach to phase detection autofocus},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Utilising low complexity CNNs to lift non-local redundancies
in video coding. <em>TIP</em>, <em>29</em>, 6372–6385. (<a
href="https://doi.org/10.1109/TIP.2020.2991525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital media is ubiquitous and produced in ever-growing quantities. This necessitates a constant evolution of compression techniques, especially for video, in order to maintain efficient storage and transmission. In this work, we aim at exploiting non-local redundancies in video data that remain difficult to erase for conventional video codecs. We design convolutional neural networks with a particular emphasis on low memory and computational footprint. The parameters of those networks are trained on the fly, at encoding time, to predict the residual signal from the decoded video signal. After the training process has converged, the parameters are compressed and signalled as part of the code of the underlying video codec. The method can be applied to any existing video codec to increase coding gains while its low computational footprint allows for an application under resource-constrained conditions. Building on top of High Efficiency Video Coding, we achieve coding gains similar to those of pretrained denoising CNNs while only requiring about 1\% of their computational complexity. Through extensive experiments, we provide insights into the effectiveness of our network design decisions. In addition, we demonstrate that our algorithm delivers stable performance under conditions met in practical video compression: our algorithm performs without significant performance loss on very long random access segments (up to 256 frames) and with moderate performance drops can even be applied to single frames in high-resolution low delay settings.},
  archive      = {J_TIP},
  author       = {Jan P. Klopp and Liang-Gee Chen and Shao-Yi Chien},
  doi          = {10.1109/TIP.2020.2991525},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6372-6385},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Utilising low complexity CNNs to lift non-local redundancies in video coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The fourier-argand representation: An optimal basis of
steerable patterns. <em>TIP</em>, <em>29</em>, 6357–6371. (<a
href="https://doi.org/10.1109/TIP.2020.2990483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the convolution between a 2D signal and a corresponding filter with variable orientations is a basic problem that arises in various tasks ranging from low level image processing (e.g. ridge/edge detection) to high level computer vision (e.g. pattern recognition). Through decades of research, there still lacks an efficient method for solving this problem. In this paper, we investigate this problem from the perspective of approximation by considering the following problem: what is the optimal basis for approximating all rotated versions of a given bivariate function? Surprisingly, solely minimising the L 2 -approximation-error leads to a rotation-covariant linear expansion, which we name Fourier-Argand representation. This representation presents two major advantages: 1) rotation-covariance of the basis, which implies a “strong steerability” - rotating by an angle α corresponds to multiplying each basis function by a complex scalar e -ikα ; 2) optimality of the Fourier-Argand basis, which ensures a few number of basis functions suffice to accurately approximate complicated patterns and highly direction-selective filters. We show the relation between the Fourier-Argand representation and the Radon transform, leading to an efficient implementation of the decomposition for digital filters. We also show how to retrieve accurate orientation of local structures/patterns using a fast frequency estimation algorithm.},
  archive      = {J_TIP},
  author       = {Tianle Zhao and Thierry Blu},
  doi          = {10.1109/TIP.2020.2990483},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6357-6371},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {The fourier-argand representation: An optimal basis of steerable patterns},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coupled real-synthetic domain adaptation for real-world deep
depth enhancement. <em>TIP</em>, <em>29</em>, 6343–6356. (<a
href="https://doi.org/10.1109/TIP.2020.2988574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in depth sensing technologies have allowed simultaneous acquisition of both color and depth data under different environments. However, most depth sensors have lower resolution than that of the associated color channels and such a mismatch can affect applications that require accurate depth recovery. Existing depth enhancement methods use simplistic noise models and cannot generalize well under real-world conditions. In this paper, a coupled real-synthetic domain adaptation method is proposed, which enables domain transfer between high-quality depth simulators and real depth camera information for super-resolution depth recovery. The method first enables the realistic degradation from synthetic images, and then enhances degraded depth data to high quality with a color-guided sub-network. The key advantage of the work is that it generalizes well to real-world datasets without further training or fine-tuning. Detailed quantitative and qualitative results are presented, and it is demonstrated that the proposed method achieves improved performance compared to previous methods fine-tuned on the specific datasets.},
  archive      = {J_TIP},
  author       = {Xiao Gu and Yao Guo and Fani Deligianni and Guang-Zhong Yang},
  doi          = {10.1109/TIP.2020.2988574},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6343-6356},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Coupled real-synthetic domain adaptation for real-world deep depth enhancement},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Viewport-adaptive scalable multi-user virtual reality
mobile-edge streaming. <em>TIP</em>, <em>29</em>, 6330–6342. (<a
href="https://doi.org/10.1109/TIP.2020.2986547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) holds tremendous potential to advance our society, expected to make impact on quality of life, energy conservation, and the economy. To bring us closer to this vision, the present paper investigates a novel communications system that integrates for the first time scalable multi-layer 360° video tiling, viewport-adaptive rate-distortion optimal resource allocation, and VR-centric edge computing and caching, to enable next generation high-quality untethered VR streaming. Our system comprises a collection of 5G small cells that can pool their communication, computing, and storage resources to collectively deliver scalable 360° video content to mobile VR clients at much higher quality. The major contributions of the paper are the rigorous design of multi-layer 360° tiling and related models of statistical user navigation, analysis and optimization of edge-based multi-user VR streaming that integrates viewport adaptation and server cooperation, and base station 360° video packet scheduling. We also explore the possibility of network coded data operation and its implications for the analysis, optimization, and system performance we pursue in this setting. The advances introduced by our framework over the state-of-the-art comprise considerable gains in delivered immersion fidelity, featuring much higher 360° viewport peak signal to noise ratio (PSNR) and VR video frame rates and spatial resolutions.},
  archive      = {J_TIP},
  author       = {Jacob Chakareski},
  doi          = {10.1109/TIP.2020.2986547},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6330-6342},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Viewport-adaptive scalable multi-user virtual reality mobile-edge streaming},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image restoration by combined order regularization with
optimal spatial adaptation. <em>TIP</em>, <em>29</em>, 6315–6329. (<a
href="https://doi.org/10.1109/TIP.2020.2988146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Total Variation (TV) and related extensions have been popular in image restoration due to their robust performance and wide applicability. While the original formulation is still relevant after two decades of extensive research, its extensions that combine derivatives of first and second orders are now being explored for better performance, with examples being Combined Order TV (COTV) and Total Generalized Variation (TGV). As an improvement over such multi-order convex formulations, we propose a novel non-convex regularization functional which adaptively combines Hessian-Schatten (HS) norm and first order TV (TV1) functionals with spatially varying weight. This adaptive weight itself is controlled by another regularization term; the total cost becomes the sum of this adaptively weighted HS-TV1 term, the regularization term for the adaptive weight, and the data-fitting term. The reconstruction is obtained by jointly minimizing w.r.t. the required image and the adaptive weight. We construct a block coordinate descent method for this minimization with proof of convergence, which alternates between minimization w.r.t. the required image and the adaptive weights. We derive exact computational formula for minimization w.r.t. the adaptive weight, and construct an ADMM algorithm for minimization w.r.t. to the required image. We compare the proposed method with existing regularization methods, and a recently proposed Deep GAN method using image recovery examples including MRI reconstruction and microscopy deconvolution.},
  archive      = {J_TIP},
  author       = {Sanjay Viswanath and Manu Ghulyani and Simon De Beco and Maxime Dahan and Muthuvel Arigovindan},
  doi          = {10.1109/TIP.2020.2988146},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6315-6329},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image restoration by combined order regularization with optimal spatial adaptation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FormNet: Formatted learning for image restoration.
<em>TIP</em>, <em>29</em>, 6302–6314. (<a
href="https://doi.org/10.1109/TIP.2020.2990603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a deep CNN to tackle the image restoration problem by learning formatted information. Previous deep learning based methods directly learn the mapping from corrupted images to clean images, and may suffer from the gradient exploding/vanishing problems of deep neural networks. We propose to address the image restoration problem by learning the structured details and recovering the latent clean image together, from the shared information between the corrupted image and the latent image. In addition, instead of learning the pure difference (corruption), we propose to add a residual formatting layer and an adversarial block to format the information to structured one, which allows the network to converge faster and boosts the performance. Furthermore, we propose a cross-level loss net to ensure both pixel-level accuracy and semantic-level visual quality. Evaluations on public datasets show that the proposed method performs favorably against existing approaches quantitatively and qualitatively.},
  archive      = {J_TIP},
  author       = {Jianbo Jiao and Wei-Chih Tu and Ding Liu and Shengfeng He and Rynson W. H. Lau and Thomas S. Huang},
  doi          = {10.1109/TIP.2020.2990603},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6302-6314},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FormNet: Formatted learning for image restoration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditional variational image deraining. <em>TIP</em>,
<em>29</em>, 6288–6301. (<a
href="https://doi.org/10.1109/TIP.2020.2990606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deraining is an important yet challenging image processing task. Though deterministic image deraining methods are developed with encouraging performance, they are infeasible to learn flexible representations for probabilistic inference and diverse predictions. Besides, rain intensity varies both in spatial locations and across color channels, making this task more difficult. In this paper, we propose a Conditional Variational Image Deraining (CVID) network for better deraining performance, leveraging the exclusive generative ability of Conditional Variational Auto-Encoder (CVAE) on providing diverse predictions for the rainy image. To perform spatially adaptive deraining, we propose a spatial density estimation (SDE) module to estimate a rain density map for each image. Since rain density varies across different color channels, we also propose a channel-wise (CW) deraining scheme. Experiments on synthesized and real-world datasets show that the proposed CVID network achieves much better performance than previous deterministic methods on image deraining. Extensive ablation studies validate the effectiveness of the proposed SDE module and CW scheme in our CVID network. The code is available at https://github.com/Yingjun-Du/VID .},
  archive      = {J_TIP},
  author       = {Yingjun Du and Jun Xu and Xiantong Zhen and Ming-Ming Cheng and Ling Shao},
  doi          = {10.1109/TIP.2020.2990606},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6288-6301},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Conditional variational image deraining},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LFNet: Light field fusion network for salient object
detection. <em>TIP</em>, <em>29</em>, 6276–6287. (<a
href="https://doi.org/10.1109/TIP.2020.2990341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel light field fusion network-LFNet, a CNNs-based light field saliency model using 4D light field data containing abundant spatial and contextual information. The proposed method can reliably locate and identify salient objects even in a complex scene. Our LFNet contains a light field refinement module (LFRM) and a light field integration module (LFIM) which can fully refine and integrate focusness, depths and objectness cues from light field image. The LFRM learns the light field residual between light field and RGB images for refining features with useful light field cues, and then the LFIM weights each refined light field feature and learns spatial correlation between them to predict saliency maps. Our method can take full advantage of light field information and achieve excellent performance especially in complex scenes, e.g., similar foreground and background, multiple or transparent objects and low-contrast environment. Experiments show our method outperforms the state-of-the-art 2D, 3D and 4D methods across three light field datasets.},
  archive      = {J_TIP},
  author       = {Miao Zhang and Wei Ji and Yongri Piao and Jingjing Li and Yu Zhang and Shuang Xu and Huchuan Lu},
  doi          = {10.1109/TIP.2020.2990341},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6276-6287},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LFNet: Light field fusion network for salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Day and night-time dehazing by local airlight estimation.
<em>TIP</em>, <em>29</em>, 6264–6275. (<a
href="https://doi.org/10.1109/TIP.2020.2988203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an effective fusion-based technique to enhance both day-time and night-time hazy scenes. When inverting the Koschmieder light transmission model, and by contrast with the common implementation of the popular dark-channel [1], we estimate the airlight on image patches and not on the entire image. Local airlight estimation is adopted because, under night-time conditions, the lighting generally arises from multiple localized artificial sources, and is thus intrinsically non-uniform. Selecting the sizes of the patches is, however, non-trivial. Small patches are desirable to achieve fine spatial adaptation to the atmospheric light, but large patches help improve the airlight estimation accuracy by increasing the possibility of capturing pixels with airlight appearance (due to severe haze). For this reason, multiple patch sizes are considered to generate several images, that are then merged together. The discrete Laplacian of the original image is provided as an additional input to the fusion process to reduce the glowing effect and to emphasize the finest image details. Similarly, for day-time scenes we apply the same principle but use a larger patch size. For each input, a set of weight maps are derived so as to assign higher weights to regions of high contrast, high saliency and small saturation. Finally the derived inputs and the normalized weight maps are blended in a multi-scale fashion using a Laplacian pyramid decomposition. Extensive experimental results demonstrate the effectiveness of our approach as compared with recent techniques, both in terms of computational efficiency and the quality of the outputs.},
  archive      = {J_TIP},
  author       = {Cosmin Ancuti and Codruta O. Ancuti and Christophe De Vleeschouwer and Alan C. Bovik},
  doi          = {10.1109/TIP.2020.2988203},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6264-6275},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Day and night-time dehazing by local airlight estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deblurring face images using uncertainty guided multi-stream
semantic networks. <em>TIP</em>, <em>29</em>, 6251–6263. (<a
href="https://doi.org/10.1109/TIP.2020.2990354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel multi-stream architecture and training methodology that exploits semantic labels for facial image deblurring. The proposed Uncertainty Guided Multi-Stream Semantic Network (UMSN) processes regions belonging to each semantic class independently and learns to combine their outputs into the final deblurred result. Pixel-wise semantic labels are obtained using a segmentation network. A predicted confidence measure is used during training to guide the network towards the challenging regions of the human face such as the eyes and nose. The entire network is trained in an end-to-end fashion. Comprehensive experiments on three different face datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art face deblurring methods. Code is available at: https://github.com/rajeevyasarla/UMSN-Face-Deblurring.},
  archive      = {J_TIP},
  author       = {Rajeev Yasarla and Federico Perazzi and Vishal M. Patel},
  doi          = {10.1109/TIP.2020.2990354},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6251-6263},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deblurring face images using uncertainty guided multi-stream semantic networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rate control for video-based point cloud compression.
<em>TIP</em>, <em>29</em>, 6237–6250. (<a
href="https://doi.org/10.1109/TIP.2020.2989576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate control is a necessary tool for video-based point cloud compression (V-PCC). However, there is no solution specified on this topic yet. In this paper, we propose the first rate control algorithm for V-PCC. Generally, a rate control algorithm is divided into two processes: bit allocation and bitrate control. In V-PCC, the total bits are composed of three parts: the header information including the auxiliary information and occupancy map, the geometry video, and the attribute video. The bit allocation aims to assign the total bits to these three parts. Since the auxiliary information and occupancy map are encoded losslessly, the bit cost of the header information is fixed. Therefore, we only need to assign bits between the geometry and attribute videos. Our first key contribution is the proposed video-level bit allocation algorithm between the geometry and attribute videos to optimize the overall reconstructed point cloud quality. Then we assign geometry and attribute video bits to each group of pictures (GOP), each frame, and each basic unit (BU). Our second key contribution is that we assign zero bits to the BUs with only unoccupied pixels. The unoccupied pixels are useless for the reconstructed quality of the point cloud and therefore should be assigned zero bits. In the bitrate control process, the encoding parameters are determined, and the model parameters are updated for each frame and BU to achieve the target bits. Our third key contribution is that we propose a BU-level model updating scheme to handle the case where various patches may be placed in different positions in neighboring frames. We use the auxiliary information to find the corresponding BU in the previous frame and apply its model parameters to the current BU. The proposed algorithms are implemented in the V-PCC and High Efficiency Video Coding (HEVC) reference software. The experimental results show that the proposed rate control algorithm can achieve significant bitrate savings compared with the state-of-the-art method.},
  archive      = {J_TIP},
  author       = {Li Li and Zhu Li and Shan Liu and Houqiang Li},
  doi          = {10.1109/TIP.2020.2989576},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6237-6250},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rate control for video-based point cloud compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Personalized image enhancement using neural spline color
transforms. <em>TIP</em>, <em>29</em>, 6223–6236. (<a
href="https://doi.org/10.1109/TIP.2020.2989584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we present SpliNet, a novel CNN-based method that estimates a global color transform for the enhancement of raw images. The method is designed to improve the perceived quality of the images by reproducing the ability of an expert in the field of photo editing. The transformation applied to the input image is found by a convolutional neural network specifically trained for this purpose. More precisely, the network takes as input a raw image and produces as output one set of control points for each of the three color channels. Then, the control points are interpolated with natural cubic splines and the resulting functions are globally applied to the values of the input pixels to produce the output image. Experimental results compare favorably against recent methods in the state of the art on the MIT-Adobe FiveK dataset. Furthermore, we also propose an extension of the SpliNet in which a single neural network is used to model the style of multiple reference retouchers by embedding them into a user space. The style of new users can be reproduced without retraining the network, after a quick modeling stage in which they are positioned in the user space on the basis of their preferences on a very small set of retouched images.},
  archive      = {J_TIP},
  author       = {Simone Bianco and Claudio Cusano and Flavio Piccoli and Raimondo Schettini},
  doi          = {10.1109/TIP.2020.2989584},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6223-6236},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Personalized image enhancement using neural spline color transforms},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video captioning with object-aware spatio-temporal
correlation and aggregation. <em>TIP</em>, <em>29</em>, 6209–6222. (<a
href="https://doi.org/10.1109/TIP.2020.2988435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning is a significant challenging task in computer vision and natural language processing, aiming to automatically describe video content by natural language sentences. Comprehensive understanding of video is the key for accurate video captioning, which needs to not only capture the global content and salient objects in video, but also understand the spatio-temporal relations of objects, including their temporal trajectories and spatial relationships. Thus, it is important for video captioning to capture the objects&#39; relationships both within and across frames. Therefore, in this paper, we propose an object-aware spatio-temporal graph (OSTG) approach for video captioning. It constructs spatio-temporal graphs to depict objects with their relations, where the temporal graphs represent objects&#39; inter-frame dynamics, and the spatial graphs represent objects&#39; intra-frame interactive relationships. The main novelties and advantages are: (1) Bidirectional temporal alignment: Bidirectional temporal graph is constructed along and reversely along the temporal order to perform bidirectional temporal alignment for objects across different frames, which provides complementary clues to capture the inter-frame temporal trajectories for each salient object. (2) Graph based spatial relation learning: Spatial relation graph is constructed among objects in each frame by considering their relative spatial locations and semantic correlations, which is exploited to learn relation features that encode intra-frame relationships for salient objects. (3) Object-aware feature aggregation: Trainable VLAD (vector of locally aggregated descriptors) models are deployed to perform object-aware feature aggregation on objects&#39; local features, which learn discriminative aggregated representations for better video captioning. A hierarchical attention mechanism is also developed to distinguish contributions of different object instances. Experiments on two widely-used datasets, MSR-VTT and MSVD, demonstrate our proposed approach achieves state-of-the-art performances in terms of BLEU@4, METEOR and CIDEr metrics.},
  archive      = {J_TIP},
  author       = {Junchao Zhang and Yuxin Peng},
  doi          = {10.1109/TIP.2020.2988435},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6209-6222},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video captioning with object-aware spatio-temporal correlation and aggregation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual-branch network with a subtle motion detector for
microaction recognition in videos. <em>TIP</em>, <em>29</em>, 6194–6208.
(<a href="https://doi.org/10.1109/TIP.2020.2989864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By involving only subtle motions of body parts, video-based microaction recognition is a very important but challenging problem. Most existing action recognition methods are developed for general actions, and the current state-of-the-art methods usually largely rely on high-layer features learned from convolutional neural networks (CNNs). High-layer CNN features usually contain more semantic information but less detailed information. However, detailed information can be important for microactions due to the motion subtleness of such actions. In this paper, we propose to more effectively learn midlayer CNN features for enhancing microaction recognition. More specifically, we develop a new dual-branch network for microaction recognition: one branch uses the high-layer CNN features for classification, and the second branch further explores the midlayer CNN features for classification. In the second branch, we introduce a novel subtle motion detector consisting of three modules: 1) a discriminative spatial-temporal feature learning module, which further learns the subtle motion features corresponding to the discriminative spatial-temporal regions, 2) a parallel multiplier attention module, which further refines the features learned in channels and spatial-temporal domains, and 3) an activation fusion module, which fuses the max and average activations from midlayer CNN features for classification. In the experiments, we build a new microaction video dataset, where the micromotions of interest are mixed with other larger general motions such as walking. Comprehensive experimental results verify that the proposed method yields new state-of-the-art performance in two microaction video datasets, while its performance on two general-action video datasets is also very promising.},
  archive      = {J_TIP},
  author       = {Yang Mi and Xingyuan Zhang and Zhongguo Li and Song Wang},
  doi          = {10.1109/TIP.2020.2989864},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6194-6208},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-branch network with a subtle motion detector for microaction recognition in videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Characterizing generalized rate-distortion performance of
video coding: An eigen analysis approach. <em>TIP</em>, <em>29</em>,
6180–6193. (<a href="https://doi.org/10.1109/TIP.2020.2988437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate-distortion (RD) theory is at the heart of lossy data compression. Here we aim to model the generalized RD (GRD) trade-off between the visual quality of a compressed video and its encoding profiles (e.g., bitrate and spatial resolution). We first define the theoretical functional space W of the GRD function by analyzing its mathematical properties. We show that W is a convex set in a Hilbert space, inspiring a computational model of the GRD function, and a method of estimating model parameters from sparse measurements. To demonstrate the feasibility of our idea, we collect a large-scale database of real-world GRD functions, which turn out to live in a low-dimensional subspace of W. Combining the GRD reconstruction framework and the learned low-dimensional space, we create a low-parameter eigen GRD method to accurately estimate the GRD function of a source video content from only a few queries. Experimental results on the database show that the learned GRD method significantly outperforms state-of-the-art empirical RD estimation methods both in accuracy and efficiency. Last, we demonstrate the promise of the proposed model in video codec comparison.},
  archive      = {J_TIP},
  author       = {Zhengfang Duanmu and Wentao Liu and Zhuoran Li and Kede Ma and Zhou Wang},
  doi          = {10.1109/TIP.2020.2988437},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6180-6193},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Characterizing generalized rate-distortion performance of video coding: An eigen analysis approach},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Back-projection based fidelity term for ill-posed linear
inverse problems. <em>TIP</em>, <em>29</em>, 6164–6179. (<a
href="https://doi.org/10.1109/TIP.2020.2988779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ill-posed linear inverse problems appear in many image processing applications, such as deblurring, super-resolution and compressed sensing. Many restoration strategies involve minimizing a cost function, which is composed of fidelity and prior terms, balanced by a regularization parameter. While a vast amount of research has been focused on different prior models, the fidelity term is almost always chosen to be the least squares (LS) objective, that encourages fitting the linearly transformed optimization variable to the observations. In this paper, we examine a different fidelity term, which has been implicitly used by the recently proposed iterative denoising and backward projections (IDBP) framework. This term encourages agreement between the projection of the optimization variable onto the row space of the linear operator and the pseudo-inverse of the linear operator (“back-projection”) applied on the observations. We analytically examine the difference between the two fidelity terms for Tikhonov regularization and identify cases (such as a badly conditioned linear operator) where the new term has an advantage over the standard LS one. Moreover, we demonstrate empirically that the behavior of the two induced cost functions for sophisticated convex and non-convex priors, such as total-variation, BM3D, and deep generative models, correlates with the obtained theoretical analysis.},
  archive      = {J_TIP},
  author       = {Tom Tirer and Raja Giryes},
  doi          = {10.1109/TIP.2020.2988779},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6164-6179},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Back-projection based fidelity term for ill-posed linear inverse problems},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint angular refinement and reconstruction for
single-particle cryo-EM. <em>TIP</em>, <em>29</em>, 6151–6163. (<a
href="https://doi.org/10.1109/TIP.2020.2984313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-particle cryo-electron microscopy (cryo-EM) reconstructs the three-dimensional (3D) structure of bio-molecules from a large set of 2D projection images with random and unknown orientations. A crucial step in the single-particle cryo-EM pipeline is 3D refinement, which resolves a high-resolution 3D structure from an initial approximate volume by refining the estimation of the orientation of each projection. In this work, we propose a new approach that refines the projection angles on the continuum. We formulate the optimization problem over the density map and the orientations jointly. The density map is updated using the efficient alternating-direction method of multipliers, while the orientations are updated through a semi-coordinate-wise gradient descent for which we provide an explicit derivation of the gradient. Our method eliminates the requirement for a fine discretization of the orientation space and does away with the classical but computationally expensive template-matching step. Numerical results demonstrate the feasibility and performance of our approach compared to several baselines.},
  archive      = {J_TIP},
  author       = {Mona Zehni and Laurène Donati and Emmanuel Soubies and Zhizhen Zhao and Michael Unser},
  doi          = {10.1109/TIP.2020.2984313},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6151-6163},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint angular refinement and reconstruction for single-particle cryo-EM},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast depth and mode decision in intra prediction for quality
SHVC. <em>TIP</em>, <em>29</em>, 6136–6150. (<a
href="https://doi.org/10.1109/TIP.2020.2988167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scalable High Efficiency Video Coding (SHVC) is the extension of High Efficiency Video Coding (HEVC). In intra prediction for quality SHVC, a Coding Unit (CU) is recursively divided into a quadtree-based structure from the largest 64×64 CU to the smallest 8×8 CU, in which 35 intra prediction modes and Inter-Layer Reference (ILR) mode are checked to determine the best possible mode. This leads to very high coding efficiency but also results in an extremely high coding complexity. To improve coding speed while maintaining coding efficiency, in this paper, we propose a new efficient algorithm for fast intra prediction for enhancement layer in SHVC. First, temporal and spatial correlations, as well as their correlation degrees, are combined in a Naive Bayes classifier to predict depth probabilities and skip depths with low likelihood. Second, for a given depth candidate, we combine ILR mode probability with Partial Zero Blocks (PZBs) based on the Sum of Squared Differences (SSD) to determine whether the ILR mode is the best one. In that case, we can skip intra prediction, which requires very high complexity. Third, initial Intra Modes (IMs) are obtained through Sobel operator, and are combined with the relationship between IMs and their corresponding Hadamard Cost (HC) values to predict candidate IMs in Rough Mode Decision (RMD). Then, an analytical criterion of early termination is developed based on the HC values of two neighboring IMs in the Rate-Distortion Optimization (RDO) process. Finally, we combine depth probabilities and the distribution of residual coefficients at the current depth to early terminate depth selection. The proposed scheme can significantly decrease the complexity of depth determination while reducing the complexity of mode decision for a depth candidate. Our experimental results demonstrate that the proposed scheme can achieve a speed up gain of more than 80\% in average, while maintaining coding efficiency.},
  archive      = {J_TIP},
  author       = {Dayong Wang and Yu Sun and Ce Zhu and Weisheng Li and Frederic Dufaux and Jiangtao Luo},
  doi          = {10.1109/TIP.2020.2988167},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6136-6150},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast depth and mode decision in intra prediction for quality SHVC},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time correlation tracking via joint model compression
and transfer. <em>TIP</em>, <em>29</em>, 6123–6135. (<a
href="https://doi.org/10.1109/TIP.2020.2989544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filters (CF) have received considerable attention in visual tracking because of their computational efficiency. Leveraging deep features via off-the-shelf CNN models (e.g., VGG), CF trackers achieve state-of-the-art performance while consuming a large number of computing resources. This limits deep CF trackers to be deployed to many mobile platforms on which only a single-core CPU is available. In this paper, we propose to jointly compress and transfer off-the-shelf CNN models within a knowledge distillation framework. We formulate a CNN model pretrained from the image classification task as a teacher network, and distill this teacher network into a lightweight student network as the feature extractor to speed up CF trackers. In the distillation process, we propose a fidelity loss to enable the student network to maintain the representation capability of the teacher network. Meanwhile, we design a tracking loss to adapt the objective of the student network from object recognition to visual tracking. The distillation process is performed offline on multiple layers and adaptively updates the student network using a background-aware online learning scheme. The online adaptation stage exploits the background contents to improve the feature discrimination of the student network. Extensive experiments on six standard datasets demonstrate that the lightweight student network accelerates the speed of state-of-the-art deep CF trackers to real-time on a single-core CPU while maintaining almost the same tracking accuracy.},
  archive      = {J_TIP},
  author       = {Ning Wang and Wengang Zhou and Yibing Song and Chao Ma and Houqiang Li},
  doi          = {10.1109/TIP.2020.2989544},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6123-6135},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real-time correlation tracking via joint model compression and transfer},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Aligning discriminative and representative features: An
unsupervised domain adaptation method for building damage assessment.
<em>TIP</em>, <em>29</em>, 6110–6122. (<a
href="https://doi.org/10.1109/TIP.2020.2988175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building assessment is highly prioritized during rescue operations and damage relief after hurricane disasters. Although machine learning has made remarkable improvement in building damage classification, it remains challenging because classifiers must be trained using a massive amount of labeled data. Furthermore, data labeling is labor intensive, costly, and unavailable after a disaster. To address this issue, we propose an unsupervised domain adaptation method with aligned discriminative and representative features (ADRF), which leverage a substantial amount of labeled data of relevant disaster scenes for new classification tasks. The remote sensing imageries of different disasters are collected using different sensors, viewpoints, times, even at various places. Compared with the public datasets used in the domain adaptation community, the remote sensing imageries are more complicated which exhibit characteristics of lower discrimination between categories and higher diversity within categories. As a result, pursuing domain invariance is a huge challenge. To achieve this goal, we build a framework with ADRF to improve the discriminative and representative capability of the extracted features to facilitate the classification task. The ADRF framework consists of three pipelines: a classifier for the labeled data of the source domain and one autoencoder each for the source and target domains. The latent variables of autoencoders are forced to observe unit Gaussian distributions by minimizing the maximum mean discrepancy (MMD), whereas the marginal distributions of both domains are aligned via the MMD. As a case study, two challenging transfer tasks using the hurricane Sandy, Maria, and Irma datasets are investigated. Experimental results demonstrate that ADRF achieves overall accuracy of 71.6\% and 84.1\% in the transfer tasks from dataset Sandy to dataset Maria and dataset Irma, respectively.},
  archive      = {J_TIP},
  author       = {Yundong Li and Wei Hu and Hongguang Li and Han Dong and Baochang Zhang and Qing Tian},
  doi          = {10.1109/TIP.2020.2988175},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6110-6122},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Aligning discriminative and representative features: An unsupervised domain adaptation method for building damage assessment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Receptive multi-granularity representation for person
re-identification. <em>TIP</em>, <em>29</em>, 6096–6109. (<a
href="https://doi.org/10.1109/TIP.2020.2986878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key for person re-identification is achieving consistent local details for discriminative representation across variable environments. Current stripe-based feature learning approaches have delivered impressive accuracy, but do not make a proper trade-off between diversity, locality, and robustness, which easily suffers from part semantic inconsistency for the conflict between rigid partition and misalignment. This paper proposes a receptive multi-granularity learning approach to facilitate stripe-based feature learning. This approach performs local partition on the intermediate representations to operate receptive region ranges, rather than current approaches on input images or output features, thus can enhance the representation of locality while remaining proper local association. Toward this end, the local partitions are adaptively pooled by using significance-balanced activations for uniform stripes. Random shifting augmentation is further introduced for a higher variance of person appearing regions within bounding boxes to ease misalignment. By two-branch network architecture, different scales of discriminative identity representation can be learned. In this way, our model can provide a more comprehensive and efficient feature representation without larger model storage costs. Extensive experiments on intra-dataset and cross-dataset evaluations demonstrate the effectiveness of the proposed approach. Especially, our approach achieves a state-of-the-art accuracy of 96.2\%@Rank-1 or 90.0\%@mAP on the challenging Market-1501 benchmark.},
  archive      = {J_TIP},
  author       = {Guanshuo Wang and Yufeng Yuan and Jiwei Li and Shiming Ge and Xi Zhou},
  doi          = {10.1109/TIP.2020.2986878},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6096-6109},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Receptive multi-granularity representation for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Gaussian lifting for fast bilateral and nonlocal means
filtering. <em>TIP</em>, <em>29</em>, 6082–6095. (<a
href="https://doi.org/10.1109/TIP.2020.2984357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many fast implementations of the bilateral and the nonlocal filters were proposed based on lattice and vector quantization, e.g. clustering, in higher dimensions. However, these approaches can still be inefficient owing to the complexities in the resampling process or in filtering the high-dimensional resampled signal. In contrast, simply scalar resampling the high-dimensional signal after decorrelation presents the opportunity to filter signals using multi-rate signal processing techniques. This work proposes the Gaussian lifting framework for efficient and accurate bilateral and nonlocal means filtering, appealing to the similarities between separable wavelet transforms and Gaussian pyramids. Accurately implementing the filter is important not only for image processing applications, but also for a number of recently proposed bilateral-regularized inverse problems, where the accuracy of the solutions depends ultimately on accurate filter implementations. We show that our Gaussian lifting approach filters images more accurately and efficiently across many filter scales. Adaptive lifting schemes for bilateral and nonlocal means filtering are also explored.},
  archive      = {J_TIP},
  author       = {Sean I. Young and Bernd Girod and David Taubman},
  doi          = {10.1109/TIP.2020.2984357},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6082-6095},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gaussian lifting for fast bilateral and nonlocal means filtering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Steerable ePCA: Rotationally invariant exponential family
PCA. <em>TIP</em>, <em>29</em>, 6069–6081. (<a
href="https://doi.org/10.1109/TIP.2020.2988139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In photon-limited imaging, the pixel intensities are affected by photon count noise. Many applications require an accurate estimation of the covariance of the underlying 2-D clean images. For example, in X-ray free electron laser (XFEL) single molecule imaging, the covariance matrix of 2-D diffraction images is used to reconstruct the 3-D molecular structure. Accurate estimation of the covariance from low-photon-count images must take into account that pixel intensities are Poisson distributed, hence the classical sample covariance estimator is highly biased. Moreover, in single molecule imaging, including in-plane rotated copies of all images could further improve the accuracy of covariance estimation. In this paper we introduce an efficient and accurate algorithm for covariance matrix estimation of count noise 2-D images, including their uniform planar rotations and possibly reflections. Our procedure, steerable ePCA, combines in a novel way two recently introduced innovations. The first is a methodology for principal component analysis (PCA) for Poisson distributions, and more generally, exponential family distributions, called ePCA. The second is steerable PCA, a fast and accurate procedure for including all planar rotations when performing PCA. The resulting principal components are invariant to the rotation and reflection of the input images. We demonstrate the efficiency and accuracy of steerable ePCA in numerical experiments involving simulated XFEL datasets and rotated face images from Yale Face Database B.},
  archive      = {J_TIP},
  author       = {Zhizhen Zhao and Lydia T. Liu and Amit Singer},
  doi          = {10.1109/TIP.2020.2988139},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6069-6081},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Steerable ePCA: Rotationally invariant exponential family PCA},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Study of subjective and objective quality assessment of
audio-visual signals. <em>TIP</em>, <em>29</em>, 6054–6068. (<a
href="https://doi.org/10.1109/TIP.2020.2988148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topics of visual and audio quality assessment (QA) have been widely researched for decades, yet nearly all of this prior work has focused only on single-mode visual or audio signals. However, visual signals rarely are presented without accompanying audio, including heavy-bandwidth video streaming applications. Moreover, the distortions that may separately (or conjointly) afflict the visual and audio signals collectively shape user-perceived quality of experience (QoE). This motivated us to conduct a subjective study of audio and video (A/V) quality, which we then used to compare and develop A/V quality measurement models and algorithms. The new LIVE-SJTU Audio and Video Quality Assessment (A/V-QA) Database includes 336 A/V sequences that were generated from 14 original source contents by applying 24 different A/V distortion combinations on them. We then conducted a subjective A/V quality perception study on the database towards attaining a better understanding of how humans perceive the overall combined quality of A/V signals. We also designed four different families of objective A/V quality prediction models, using a multimodal fusion strategy. The different types of A/V quality models differ in both the unimodal audio and video quality prediction models comprising the direct signal measurements and in the way that the two perceptual signal modes are combined. The objective models are built using both existing state-of-the-art audio and video quality prediction models and some new prediction models, as well as quality-predictive features delivered by a deep neural network. The methods of fusing audio and video quality predictions that are considered include simple product combinations as well as learned mappings. Using the new subjective A/V database as a tool, we validated and tested all of the objective A/V quality prediction models. We will make the database publicly available to facilitate further research.},
  archive      = {J_TIP},
  author       = {Xiongkuo Min and Guangtao Zhai and Jiantao Zhou and Mylène C. Q. Farias and Alan Conrad Bovik},
  doi          = {10.1109/TIP.2020.2988148},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6054-6068},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Study of subjective and objective quality assessment of audio-visual signals},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimized quantization constraints set for image
restoration and its GPU implementation. <em>TIP</em>, <em>29</em>,
6043–6053. (<a href="https://doi.org/10.1109/TIP.2020.2988131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel optimized quantization constraint set, acting as an add-on to existing DCT-based image restoration algorithms. The constraint set is created based on generalized Gaussian distribution which is more accurate than the commonly used uniform, Gaussian or Laplacian distributions when modeling DCT coefficients. More importantly, the proposed constraint set is optimized for individual input images and thus it is able to enhance image quality significantly in terms of signal-to-noise ratio. Experimental results indicate that the signal-to-noise ratio is improved by at least 6.78\% on top of the existing state-of-the-art methods, with a corresponding expense of only 0.38\% in processing time. The proposed algorithm has also been implemented in GPU, and the processing speed increases further by 20 times over that of CPU implementation. This makes the algorithm well suited for fast image retrieval in security and quality monitoring system.},
  archive      = {J_TIP},
  author       = {Shumin Liu and Jiajia Chen and Ye Ai and Susanto Rahardja},
  doi          = {10.1109/TIP.2020.2988131},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6043-6053},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An optimized quantization constraints set for image restoration and its GPU implementation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-lambertian photometric stereo network based on inverse
reflectance model with collocated light. <em>TIP</em>, <em>29</em>,
6032–6042. (<a href="https://doi.org/10.1109/TIP.2020.2987176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current non-Lambertian photometric stereo methods generally require a large number of images to ensure accurate surface normal estimation. To achieve accurate surface normal recovery under a sparse set of lights, this paper proposes a non-Lambertian photometric stereo network based on a derived inverse reflectance model with collocated light. The model is deduced using monotonicity of isotropic reflectance and the univariate property of collocated light to decouple the surface normal from the reflectance function. Thus, the surface normal can be estimated by three steps, i.e., model fitting, shadow rejection, and normal estimation. We leverage a supervised deep learning technique to enhance the shadow rejection ability and the flexibility of the inverse reflectance model. Shadows are handled through max-pooling. Information from a neighborhood image patch is utilized to improve the flexibility to various reflectances. Experiments using both synthetic and real images demonstrate that the proposed method achieves state-of-the-art accuracy in surface normal estimation.},
  archive      = {J_TIP},
  author       = {Xi Wang and Zhenxiong Jian and Mingjun Ren},
  doi          = {10.1109/TIP.2020.2987176},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6032-6042},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Non-lambertian photometric stereo network based on inverse reflectance model with collocated light},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence-guided self refinement for action prediction in
untrimmed videos. <em>TIP</em>, <em>29</em>, 6017–6031. (<a
href="https://doi.org/10.1109/TIP.2020.2987425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing methods formulate the action prediction task as recognizing early parts of actions in trimmed videos. In this paper, we focus on predicting actions from ongoing untrimmed videos where actions might not happen at the very beginning of videos. It is extremely challenging to predict actions in such untrimmed videos due to ambiguous or even no information of actions in the early parts of videos. To address this problem, we propose a prediction confidence that assesses the decision quality of a prediction model. Guided by the confidence, the model continuously refines the prediction results by itself with the increasing observed video frames. Specifically, we build a Self Prediction Refining Network (SPR-Net) which incrementally learns the confidence for action prediction. SPR-Net consists of three modules: a temporal hybrid network, an incremental confidence learner, and a self-refining Gumbel softmax sampler. The temporal hybrid network generates the action category distributions by integrating static scene and dynamic motion information. The incremental confidence learner calculates the confidence in an incremental manner, judging the extent to which the temporal hybrid network should believe its prediction result. The self-refining Gumbel softmax sampler models the mutual relationship between the prediction confidence and the category distribution, which enables them to be jointly learned in an end-to-end fashion. We also present a sparse self-attention mechanism to encode local spatio-temporal features into the frame-level motion representation to further improve the prediction performance. Extensive experiments on five datasets (i.e., UT-Interaction, BIT-Interaction, UCF101, THUMOS14, and ActivityNet) validate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Jingyi Hou and Xinxiao Wu and Ruiqi Wang and Jiebo Luo and Yunde Jia},
  doi          = {10.1109/TIP.2020.2987425},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6017-6031},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Confidence-guided self refinement for action prediction in untrimmed videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-dimensional autofocus technique based on spatial
frequency domain fragmentation. <em>TIP</em>, <em>29</em>, 6006–6016.
(<a href="https://doi.org/10.1109/TIP.2020.2988143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing two-dimensional (2D) autofocus algorithms, exploiting the known structure of 2D phase error, lack the error estimation accuracy in the case of a low signal-to-noise ratio (SNR) due to the non-exhaustive use of the echo signal. In this paper, we propose a novel 2D autofocus algorithm, thoroughly utilizing all the available data and therefore achieving superior estimation performance. Via analytical study, we show that the partial derivative of the 2D error with respect to the azimuth frequency is approximable as a function of single argument, after appropriate change of variable. The established property enables a scheme for the azimuth phase error (APE) measurement, where the polar formatted data are fragmented along the range frequency and then aligned along the azimuth frequency, in order to equalize phase gradients in different fragments. On the one hand, such a scheme avoids the necessity to divide the full-aperture signal into subapertures, while on the other hand, it involves the whole signal support. Improved accuracy of the resulting estimate is achieved through the joint inter-fragment estimation of the APE gradient. The proposed algorithm, based on the mentioned scheme, was validated via computer simulations. The conducted experiments confirmed its preference against the existing techniques. The preference is particularly distinct for low SNR imagery.},
  archive      = {J_TIP},
  author       = {Pavel A. Makarov},
  doi          = {10.1109/TIP.2020.2988143},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6006-6016},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Two-dimensional autofocus technique based on spatial frequency domain fragmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-ISO long-exposure image denoising based on quantitative
blob characterization. <em>TIP</em>, <em>29</em>, 5993–6005. (<a
href="https://doi.org/10.1109/TIP.2020.2986687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blob detection and image denoising are fundamental, sometimes related tasks in computer vision. In this paper, we present a computational method to quantitatively measure blob characteristics using normalized unilateral second-order Gaussian kernels. This method suppresses non-blob structures while yielding a quantitative measurement of the position, prominence and scale of blobs, which can facilitate the tasks of blob reconstruction and blob reduction. Subsequently, we propose a denoising scheme to address high-ISO long-exposure noise, which sometimes spatially shows a blob appearance, employing a blob reduction procedure as a cheap preprocessing for conventional denoising methods. We apply the proposed denoising methods to real-world noisy images as well as standard images that are corrupted by real noise. The experimental results demonstrate the superiority of the proposed methods over state-of-the-art denoising methods.},
  archive      = {J_TIP},
  author       = {Gang Wang and Carlos Lopez-Molina and Bernard De Baets},
  doi          = {10.1109/TIP.2020.2986687},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5993-6005},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-ISO long-exposure image denoising based on quantitative blob characterization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PBR-net: Imitating physically based rendering using deep
neural network. <em>TIP</em>, <em>29</em>, 5980–5992. (<a
href="https://doi.org/10.1109/TIP.2020.2987169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically based rendering has been widely used to generate photo-realistic images, which greatly impacts industry by providing appealing rendering, such as for entertainment and augmented reality, and academia by serving large scale high-fidelity synthetic training data for data hungry methods like deep learning. However, physically based rendering heavily relies on ray-tracing, which can be computational expensive in complicated environment and hard to parallelize. In this paper, we propose an end-to-end deep learning based approach to generate physically based rendering efficiently. Our system consists of two stacked neural networks, which effectively simulates the physical behavior of the rendering process and produces photo-realistic images. The first network, namely shading network, is designed to predict the optimal shading image from surface normal, depth and illumination; the second network, namely composition network, learns to combine the predicted shading image with the reflectance to generate the final result. Our approach is inspired by intrinsic image decomposition, and thus it is more physically reasonable to have shading as intermediate supervision. Extensive experiments show that our approach is robust to noise thanks to a modified perceptual loss and even outperforms the physically based rendering systems in complex scenes given a reasonable time budget.},
  archive      = {J_TIP},
  author       = {Peng Dai and Zhuwen Li and Yinda Zhang and Shuaicheng Liu and Bing Zeng},
  doi          = {10.1109/TIP.2020.2987169},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5980-5992},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PBR-net: Imitating physically based rendering using deep neural network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quality prediction on deep generative images. <em>TIP</em>,
<em>29</em>, 5964–5979. (<a
href="https://doi.org/10.1109/TIP.2020.2987180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks have been utilized in a wide variety of applications including image generation. In particular, generative adversarial networks (GANs) are able to produce highly realistic pictures as part of tasks such as image compression. As with standard compression, it is desirable to be able to automatically assess the perceptual quality of generative images to monitor and control the encode process. However, existing image quality algorithms are ineffective on GAN generated content, especially on textured regions and at high compressions. Here we propose a new “naturalness”-based image quality predictor for generative images. Our new GAN picture quality predictor is built using a multi-stage parallel boosting system based on structural similarity features and measurements of statistical similarity. To enable model development and testing, we also constructed a subjective GAN image quality database containing (distorted) GAN images and collected human opinions of them. Our experimental results indicate that our proposed GAN IQA model delivers superior quality predictions on the generative image datasets, as well as on traditional image quality datasets.},
  archive      = {J_TIP},
  author       = {Hyunsuk Ko and Dae Yeol Lee and Seunghyun Cho and Alan C. Bovik},
  doi          = {10.1109/TIP.2020.2987180},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5964-5979},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Quality prediction on deep generative images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online tensor sparsifying transform based on temporal
superpixels from compressive spectral video measurements. <em>TIP</em>,
<em>29</em>, 5953–5963. (<a
href="https://doi.org/10.1109/TIP.2020.2985871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral videos contain highly redundant information across spatial, spectral and temporal axes which can be exploited through a temporal-data-learned sparsifying basis. However, in compressive spectral video acquisition, tackling dictionary learning is time-consuming since it increases the computational complexity and presents drawbacks for real-time processing, where offline learning is required. This paper introduces a tensor-decomposition learning (TenDL) framework for simultaneous online sparsifying and recovering the spatial-spectral-temporal information of a spectral video performed on several temporal superpixels (TSP-TenDL) for time processing reduction. The framework is composed of two main stages: preprocessing and joint estimation. The preprocessing stage includes a strategy for a grayscale approximation of the video to provide a suitable initialization of the sparsifying basis to be learned. To fully exploit the high signal correlation, a set of temporal superpixels is estimated from the grayscale approximation, reducing the reconstruction time of the large-scale data. Then, the outcome of the first stage is used to estimate the basis and the signal coefficients, where an optimization problem is solved to learn and reconstruct the basis and the signal, respectively, following a block-descent coordinate strategy. The proposed approach is compared from simulations with an offline-learned based method, traditional matrix-based recovery algorithms and the tensor-based recovery, the two latter using a fixed basis, where TSP-TenDL exhibits higher image quality results and lower computation time. Specifically, our methodology gains up to 7dB in terms of PSNR and a speedup of up to 6.6× compared with state-of-the-art counterparts.},
  archive      = {J_TIP},
  author       = {Kareth M. León-López and Henry Arguello Fuentes},
  doi          = {10.1109/TIP.2020.2985871},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5953-5963},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Online tensor sparsifying transform based on temporal superpixels from compressive spectral video measurements},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The performance of quality metrics in assessing
error-concealed video quality. <em>TIP</em>, <em>29</em>, 5937–5952. (<a
href="https://doi.org/10.1109/TIP.2020.2984356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In highly-interactive video streaming applications such as video conferencing, tele-presence, or tele-operation, retransmission is typically not used, due to the tight deadline of the application. In such cases, the lost or erroneous data must be concealed. While various error concealment techniques exist, there is no defined rule to compare their perceived quality. In this paper, the performance of 16 existing image and video quality metrics (PSNR, SSIM, VQM, etc.) evaluating error-concealed video quality is studied. The encoded video is subjected to packet loss and the loss is concealed using various error concealment techniques. We show that the subjective quality of the video cannot be necessarily predicted from the visual quality of the error-concealed frame alone. We then apply the metrics to the error-concealed images/videos and evaluate their success in predicting the scores reported by human subjects. The error-concealed videos are judged by image quality metrics applied on the lossy frame, or by video quality metrics applied on the video clip containing that lossy frame; this way, the impact of error propagation is also considered by the objective metrics. The measurement and comparison of the results show that, mostly though not always, measuring the objective quality of the video is a better way to judge the error concealment performance. Moreover, our experiments show that when the objective quality metrics are used for the assessment of the performance of an error concealment technique, they do not behave as they would for general quality assessment. In fact, some newly developed metrics show the correct decision only about 60\% of the time, leading to an unacceptable error rate of as much as 40\%. Our analysis shows which specific quality metrics are relatively more suitable for error-concealed videos.},
  archive      = {J_TIP},
  author       = {Mohammad Kazemi and Mohammad Ghanbari and Shervin Shirmohammadi},
  doi          = {10.1109/TIP.2020.2984356},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5937-5952},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {The performance of quality metrics in assessing error-concealed video quality},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ASTS: A unified framework for arbitrary shape text spotting.
<em>TIP</em>, <em>29</em>, 5924–5936. (<a
href="https://doi.org/10.1109/TIP.2020.2984082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary shape text spotting remains a challenging computer vision task. In this paper, we propose an end-to-end trainable unified framework for arbitrary shape text spotting to overcome the limitations inherent in the existing methods. Specifically, we propose to perceive and understand text based on different levels of semantics, i.e., holistic-, pixel- and sequence-level semantics, and then unify the recognized semantics for robust text spotting. To implement the framework, we customize the detection and mask branches of Mask R-CNN to explore both holistic- and pixel-level semantics for text recognition. According to the recognition results, the text spotting task can then be formulated in the two-dimensional feature space. Then, by feeding the two-dimensional feature maps into an additional text recognition branch, our framework further delivers one-dimensional sequence-level semantics for text recognition based on an attention-based sequence-to-sequence network. Finally, the results from all the three levels of semantics are merged as the final result. Therefore, our framework is capable of simultaneously recognizing texts from both the one- and two-dimensional perspectives, achieving highly comprehensive text recognition. In addition, because some existing datasets lack character-level annotations, the extensive descriptions of texts from our framework further allow us to use only word-level annotations as weak supervision for training a robust text spotting model. Experiments on ICDAR 2013, ICDAR 2015, and Total-Text show that our framework achieves state-of-the-art performance for both detection and recognition.},
  archive      = {J_TIP},
  author       = {Juhua Liu and Zhe Chen and Bo Du and Dacheng Tao},
  doi          = {10.1109/TIP.2020.2984082},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5924-5936},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ASTS: A unified framework for arbitrary shape text spotting},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep recognition of vanishing-point-constrained building
planes in urban street views. <em>TIP</em>, <em>29</em>, 5912–5923. (<a
href="https://doi.org/10.1109/TIP.2020.2986894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach to recognizing vanishing-point-constrained building planes from a single image of street view. We first design a novel convolutional neural network (CNN) architecture that generates geometric segmentation of per-pixel orientations from a single street-view image. The network combines two-stream features of general visual cues and surface normals in gated convolution layers, and employs a deeply supervised loss that encapsulates multi-scale convolutional features. Our experiments on a new benchmark with fine-grained plane segmentations of real-world street views show that our network outperforms state-of-the-arts methods of both semantic and geometric segmentation. The pixel-wise segmentation exhibits coarse boundaries and discontinuities. We then propose to rectify the pixel-wise segmentation into perspectively-projected quads based on spatial proximity between the segmentation masks and exterior line segments detected through an image processing. We demonstrate how the results can be utilized to perspectively overlay images and icons on building planes in input photos, and provide visual cues for various applications.},
  archive      = {J_TIP},
  author       = {Zhiliang Zeng and Mengyang Wu and Wei Zeng and Chi-Wing Fu},
  doi          = {10.1109/TIP.2020.2986894},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5912-5923},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep recognition of vanishing-point-constrained building planes in urban street views},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Efficient and effective context-based convolutional entropy
modeling for image compression. <em>TIP</em>, <em>29</em>, 5900–5911.
(<a href="https://doi.org/10.1109/TIP.2020.2985225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise estimation of the probabilistic structure of natural images plays an essential role in image compression. Despite the recent remarkable success of end-to-end optimized image compression, the latent codes are usually assumed to be fully statistically factorized in order to simplify entropy modeling. However, this assumption generally does not hold true and may hinder compression performance. Here we present context-based convolutional networks (CCNs) for efficient and effective entropy modeling. In particular, a 3D zigzag scanning order and a 3D code dividing technique are introduced to define proper coding contexts for parallel entropy decoding, both of which boil down to place translation-invariant binary masks on convolution filters of CCNs. We demonstrate the promise of CCNs for entropy modeling in both lossless and lossy image compression. For the former, we directly apply a CCN to the binarized representation of an image to compute the Bernoulli distribution of each code for entropy estimation. For the latter, the categorical distribution of each code is represented by a discretized mixture of Gaussian distributions, whose parameters are estimated by three CCNs. We then jointly optimize the CCN-based entropy model along with analysis and synthesis transforms for rate-distortion performance. Experiments on the Kodak and Tecnick datasets show that our methods powered by the proposed CCNs generally achieve comparable compression performance to the state-of-the-art while being much faster.},
  archive      = {J_TIP},
  author       = {Mu Li and Kede Ma and Jane You and David Zhang and Wangmeng Zuo},
  doi          = {10.1109/TIP.2020.2985225},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5900-5911},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient and effective context-based convolutional entropy modeling for image compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Query-biased self-attentive network for query-focused video
summarization. <em>TIP</em>, <em>29</em>, 5889–5899. (<a
href="https://doi.org/10.1109/TIP.2020.2985868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the task of query-focused video summarization, which takes user queries and long videos as inputs and generates query-focused video summaries. Compared to video summarization, which mainly concentrates on finding the most diverse and representative visual contents as a summary, the task of query-focused video summarization considers the user&#39;s intent and the semantic meaning of generated summary. In this paper, we propose a method, named query-biased self-attentive network (QSAN) to tackle this challenge. Our key idea is to utilize the semantic information from video descriptions to generate a generic summary and then to combine the information from the query to generate a query-focused summary. Specifically, we first propose a hierarchical self-attentive network to model the relative relationship at three levels, which are different frames from a segment, different segments of the same video, textual information of video description and its related visual contents. We train the model on video caption dataset and employ a reinforced caption generator to generate a video description, which can help us locate important frames or shots. Then we build a query-aware scoring module to compute the query-relevant score for each shot and generate the query-focused summary. Extensive experiments on the benchmark dataset demonstrate the competitive performance of our approach compared to some methods.},
  archive      = {J_TIP},
  author       = {Shuwen Xiao and Zhou Zhao and Zijian Zhang and Ziyu Guan and Deng Cai},
  doi          = {10.1109/TIP.2020.2985868},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5889-5899},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Query-biased self-attentive network for query-focused video summarization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Scene recognition with prototype-agnostic scene layout.
<em>TIP</em>, <em>29</em>, 5877–5888. (<a
href="https://doi.org/10.1109/TIP.2020.2986599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting the spatial structure in scene images is a key research direction for scene recognition. Due to the large intra-class structural diversity, building and modeling flexible structural layout to adapt various image characteristics is a challenge. Existing structural modeling methods in scene recognition either focus on predefined grids or rely on learned prototypes, which all have limited representative ability. In this paper, we propose Prototype-agnostic Scene Layout (PaSL) construction method to build the spatial structure for each image without conforming to any prototype. Our PaSL can flexibly capture the diverse spatial characteristic of scene images and have considerable generalization capability. Given a PaSL, we build Layout Graph Network (LGN) where regions in PaSL are defined as nodes and two kinds of independent relations between regions are encoded as edges. The LGN aims to incorporate two topological structures (formed in spatial and semantic similarity dimensions) into image representations through graph convolution. Extensive experiments show that our approach achieves state-of-the-art results on widely recognized MIT67 and SUN397 datasets without multi-model or multi-scale fusion. Moreover, we also conduct the experiments on one of the largest scale datasets, Places365. The results demonstrate the proposed method can be well generalized and obtains competitive performance.},
  archive      = {J_TIP},
  author       = {Gongwei Chen and Xinhang Song and Haitao Zeng and Shuqiang Jiang},
  doi          = {10.1109/TIP.2020.2986599},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5877-5888},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scene recognition with prototype-agnostic scene layout},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LR3M: Robust low-light enhancement via low-rank regularized
retinex model. <em>TIP</em>, <em>29</em>, 5862–5876. (<a
href="https://doi.org/10.1109/TIP.2020.2984098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise causes unpleasant visual effects in low-light image/video enhancement. In this paper, we aim to make the enhancement model and method aware of noise in the whole process. To deal with heavy noise which is not handled in previous methods, we introduce a robust low-light enhancement approach, aiming at well enhancing low-light images/videos and suppressing intensive noise jointly. Our method is based on the proposed Low-Rank Regularized Retinex Model (LR3M), which is the first to inject low-rank prior into a Retinex decomposition process to suppress noise in the reflectance map. Our method estimates a piece-wise smoothed illumination and a noise-suppressed reflectance sequentially, avoiding remaining noise in the illumination and reflectance maps which are usually presented in alternative decomposition methods. After getting the estimated illumination and reflectance, we adjust the illumination layer and generate our enhancement result. Furthermore, we apply our LR3M to video low-light enhancement. We consider inter-frame coherence of illumination maps and find similar patches through reflectance maps of successive frames to form the low-rank prior to make use of temporal correspondence. Our method performs well for a wide variety of images and videos, and achieves better quality both in enhancing and denoising, compared with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Xutong Ren and Wenhan Yang and Wen-Huang Cheng and Jiaying Liu},
  doi          = {10.1109/TIP.2020.2984098},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5862-5876},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LR3M: Robust low-light enhancement via low-rank regularized retinex model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatially-variant CNN-based point spread function estimation
for blind deconvolution and depth estimation in optical microscopy.
<em>TIP</em>, <em>29</em>, 5848–5861. (<a
href="https://doi.org/10.1109/TIP.2020.2986880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical microscopy is an essential tool in biology and medicine. Imaging thin, yet non-flat objects in a single shot (without relying on more sophisticated sectioning setups) remains challenging as the shallow depth of field that comes with high-resolution microscopes leads to unsharp image regions and makes depth localization and quantitative image interpretation difficult. Here, we present a method that improves the resolution of light microscopy images of such objects by locally estimating image distortion while jointly estimating object distance to the focal plane. Specifically, we estimate the parameters of a spatially-variant Point Spread Function (PSF) model using a Convolutional Neural Network (CNN), which does not require instrumentor object-specific calibration. Our method recovers PSF parameters from the image itself with up to a squared Pearson correlation coefficient of 0.99 in ideal conditions, while remaining robust to object rotation, illumination variations, or photon noise. When the recovered PSFs are used with a spatially-variant and regularized Richardson-Lucy (RL) deconvolution algorithm, we observed up to 2.1 dB better Signal-to-Noise Ratio (SNR) compared to other Blind Deconvolution (BD) techniques. Following microscope-specific calibration, we further demonstrate that the recovered PSF model parameters permit estimating surface depth with a precision of 2 μm and over an extended range when using engineered PSFs. Our method opens up multiple possibilities for enhancing images of non-flat objects with minimal need for a priori knowledge about the optical setup.},
  archive      = {J_TIP},
  author       = {Adrian Shajkofci and Michael Liebling},
  doi          = {10.1109/TIP.2020.2986880},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5848-5861},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatially-variant CNN-based point spread function estimation for blind deconvolution and depth estimation in optical microscopy},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An exact and fast CBCT reconstruction via pseudo-polar
fourier transform-based discrete grangeat’s formula. <em>TIP</em>,
<em>29</em>, 5832–5847. (<a
href="https://doi.org/10.1109/TIP.2020.2985874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent application of Fourier Based Iterative Reconstruction Method (FIRM) has made it possible to achieve high-quality 2D images from a fan beam Computed Tomography (CT) scan with a limited number of projections in a fast manner. The proposed methodology in this article is designed to provide 3D Radon space in linogram fashion to facilitate the use of FIRM with cone beam projections (CBP) for the reconstruction of 3D images in a sparse view angles Cone Beam CT (CBCT). For this reason, in the first phase, the 3D Radon space is generated using CBP data after discretization and optimization of the famous Grangeat&#39;s formula. The method used in this process involves fast Pseudo Polar Fourier transform (PPFT) based on 2D and 3D Discrete Radon Transformation (DRT) algorithms with no wraparound effects. In the second phase, we describe reconstruction of the objects with available Radon values, using direct inverse of 3D PPFT. The method presented in this section eliminates noises caused by interpolation from polar to Cartesian space and exhibits no thorn, V-shaped and wrinkle artifacts. This method reduces the complexity to O(n 3 log n) for images of size n × n × n. The Cone to Radon conversion (Cone2Radon) Toolbox in the first phase and MATLAB/ Python toolbox in the second phase were tested on three digital phantoms and experiments demonstrate fast and accurate cone beam image reconstruction due to proposed modifications in all three stages of Grangeat&#39;s method.},
  archive      = {J_TIP},
  author       = {Niloufar Teyfouri and Hossein Rabbani and Raheleh Kafieh and Iraj Jabbari},
  doi          = {10.1109/TIP.2020.2985874},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5832-5847},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An exact and fast CBCT reconstruction via pseudo-polar fourier transform-based discrete grangeat’s formula},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A joint label space for generalized zero-shot
classification. <em>TIP</em>, <em>29</em>, 5817–5831. (<a
href="https://doi.org/10.1109/TIP.2020.2986892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental problem of Zero-Shot Learning (ZSL) is that the one-hot label space is discrete, which leads to a complete loss of the relationships between seen and unseen classes. Conventional approaches rely on using semantic auxiliary information, e.g. attributes, to re-encode each class so as to preserve the inter-class associations. However, existing learning algorithms only focus on unifying visual and semantic spaces without jointly considering the label space. More importantly, because the final classification is conducted in the label space through a compatibility function, the gap between attribute and label spaces leads to significant performance degradation. Therefore, this paper proposes a novel pathway that uses the label space to jointly reconcile visual and semantic spaces directly, which is named Attributing Label Space (ALS). In the training phase, one-hot labels of seen classes are directly used as prototypes in a common space, where both images and attributes are mapped. Since mappings can be optimized independently, the computational complexity is extremely low. In addition, the correlation between semantic attributes has less influence on visual embedding training because features are mapped into labels instead of attributes. In the testing phase, the discrete condition of label space is removed, and priori one-hot labels are used to denote seen classes and further compose labels of unseen classes. Therefore, the label space is very discriminative for the Generalized ZSL (GZSL), which is more reasonable and challenging for real-world applications. Extensive experiments on five benchmarks manifest improved performance over all of compared state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Jin Li and Xuguang Lan and Yang Long and Yang Liu and Xingyu Chen and Ling Shao and Nanning Zheng},
  doi          = {10.1109/TIP.2020.2986892},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5817-5831},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A joint label space for generalized zero-shot classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Fast multi-scale structural patch decomposition for
multi-exposure image fusion. <em>TIP</em>, <em>29</em>, 5805–5816. (<a
href="https://doi.org/10.1109/TIP.2020.2987133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposure bracketing is crucial to high dynamic range imaging, but it is prone to halos for static scenes and ghosting artifacts for dynamic scenes. The recently proposed structural patch decomposition for multi-exposure fusion (SPD-MEF) has achieved reliable performance in deghosting, but suffers from visible halo artifacts and is computationally expensive. In addition, its relationship to other MEF methods is unclear. We show that without explicitly performing structural patch decomposition, we arrive at an unnormalized version of SPD-MEF, which enjoys an order of 30× speed-up, and is closely related to pixel-level MEF methods as well as the standard two-layer decomposition method for MEF. Moreover, we develop a fast multi-scale SPD-MEF method, which can effectively reduce halo artifacts. Experimental results demonstrate the effectiveness of the proposed MEF method in terms of speed and quality.},
  archive      = {J_TIP},
  author       = {Hui Li and Kede Ma and Hongwei Yong and Lei Zhang},
  doi          = {10.1109/TIP.2020.2987133},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5805-5816},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast multi-scale structural patch decomposition for multi-exposure image fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-quality proposals for weakly supervised object
detection. <em>TIP</em>, <em>29</em>, 5794–5804. (<a
href="https://doi.org/10.1109/TIP.2020.2987161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant efforts made so far for Weakly Supervised Object Detection (WSOD), proposal generation and proposal selection are still two major challenges. In this paper, we focus on addressing the two challenges by generating and selecting high-quality proposals. To be specific, for proposal generation, we combine selective search and a Gradient-weighted Class Activation Mapping (Grad-CAM) based technique to generate more proposals having higher Intersection-Over-Union (IOU) with ground truth boxes than those obtained by greedy search approaches, which can better envelop the entire objects. As regards proposal selection, for each object class, we choose as many confident positive proposals as possible and meanwhile only select class-specific hard negatives to focus training on more discriminative negative proposals by up-weighting their losses, which can make training more effective. The proposed proposal generation and proposal selection approaches are generic and thus can be broadly applied to many WSOD methods. In this work, we unify them into the framework of Online Instance Classifier Refinement (OICR). Experimental results on the PASCAL VOC 2007 and 2012 datasets and MS COCO dataset demonstrate that our method significantly improves the baseline method OICR by large margins (13.4\% mAP and 11.6\% CorLoc gains on the VOC 2007 dataset, 15.0\% mAP and 8.9\% CorLoc gains on the VOC 2012 dataset, and 6.4\% mAP and 5.0\% CorLoc gains on the COCO dataset) and achieves the state-of-the-art results compared with existing methods.},
  archive      = {J_TIP},
  author       = {Gong Cheng and Junyu Yang and Decheng Gao and Lei Guo and Junwei Han},
  doi          = {10.1109/TIP.2020.2987161},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5794-5804},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-quality proposals for weakly supervised object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STA-CNN: Convolutional spatial-temporal attention learning
for action recognition. <em>TIP</em>, <em>29</em>, 5783–5793. (<a
href="https://doi.org/10.1109/TIP.2020.2984904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks have achieved excellent successes for object recognition in still images. However, the improvement of Convolutional Neural Networks over the traditional methods for recognizing actions in videos is not so significant, because the raw videos usually have much more redundant or irrelevant information than still images. In this paper, we propose a Spatial-Temporal Attentive Convolutional Neural Network (STA-CNN) which selects the discriminative temporal segments and focuses on the informative spatial regions automatically. The STA-CNN model incorporates a Temporal Attention Mechanism and a Spatial Attention Mechanism into a unified convolutional network to recognize actions in videos. The novel Temporal Attention Mechanism automatically mines the discriminative temporal segments from long and noisy videos. The Spatial Attention Mechanism firstly exploits the instantaneous motion information in optical flow features to locate the motion salient regions and it is then trained by an auxiliary classification loss with a Global Average Pooling layer to focus on the discriminative non-motion regions in the video frame. The STA-CNN model achieves the state-of-the-art performance on two of the most challenging datasets, UCF-101 (95.8\%) and HMDB-51 (71.5\%).},
  archive      = {J_TIP},
  author       = {Hao Yang and Chunfeng Yuan and Li Zhang and Yunda Sun and Weiming Hu and Stephen J. Maybank},
  doi          = {10.1109/TIP.2020.2984904},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5783-5793},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {STA-CNN: Convolutional spatial-temporal attention learning for action recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perceptual temporal incoherence-guided stereo video
retargeting. <em>TIP</em>, <em>29</em>, 5767–5782. (<a
href="https://doi.org/10.1109/TIP.2020.2984899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo video retargeting aims at minimizing shape and depth distortions with temporal coherence in resizing a stereo video content to a desired size. Existing methods extend stereo image retargeting schemes to stereo video retargeting by adding additional temporal constraints that demand temporal coherence in all corresponding regions. However, such a straightforward extension incurs conflicts among multiple requirements (i.e., shape and depth preservation and their temporal coherence), thus failing to meet one or more of these requirements satisfactorily. To mitigate conflicts among depth, shape, and temporal constraints and avoid degrading temporal coherence perceptually, we relax temporal constraints for non-paired regions at frame boundaries, derive new temporal constraints to improve human viewing experience of a 3D scene, and propose an efficient grid-based implementation for stereo video retargeting. Experimental results demonstrate that our method achieves superior visual quality over existing methods.},
  archive      = {J_TIP},
  author       = {Bing Li and Chia-Wen Lin and Shan Liu and Tiejun Huang and Wen Gao and C.-C. Jay Kuo},
  doi          = {10.1109/TIP.2020.2984899},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5767-5782},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceptual temporal incoherence-guided stereo video retargeting},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Learning to reconstruct and understand indoor scenes from
sparse views. <em>TIP</em>, <em>29</em>, 5753–5766. (<a
href="https://doi.org/10.1109/TIP.2020.2986712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new method for simultaneous 3D reconstruction and semantic segmentation for indoor scenes. Unlike existing methods that require recording a video using a color camera and/or a depth camera, our method only needs a small number of ( e.g. , 3~5) color images from uncalibrated sparse views, which significantly simplifies data acquisition and broadens applicable scenarios. To achieve promising 3D reconstruction from sparse views with limited overlap, our method first recovers the depth map and semantic information for each view, and then fuses the depth maps into a 3D scene. To this end, we design an iterative deep architecture, named IterNet , to estimate the depth map and semantic segmentation alternately. To obtain accurate alignment between views with limited overlap, we further propose a joint global and local registration method to reconstruct a 3D scene with semantic information. We also make available a new indoor synthetic dataset, containing photorealistic high-resolution RGB images, accurate depth maps and pixel-level semantic labels for thousands of complex layouts. Experimental results on public datasets and our dataset demonstrate that our method achieves more accurate depth estimation, smaller semantic segmentation errors, and better 3D reconstruction results over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Jingyu Yang and Ji Xu and Kun Li and Yu-Kun Lai and Huanjing Yue and Jianzhi Lu and Hao Wu and Yebin Liu},
  doi          = {10.1109/TIP.2020.2986712},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5753-5766},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to reconstruct and understand indoor scenes from sparse views},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advancing image understanding in poor visibility
environments: A collective benchmark study. <em>TIP</em>, <em>29</em>,
5737–5752. (<a href="https://doi.org/10.1109/TIP.2020.2981922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing enhancement methods are empirically expected to help the high-level end computer vision task: however, that is observed to not always be the case in practice. We focus on object or face detection in poor visibility enhancements caused by bad weathers (haze, rain) and low light conditions. To provide a more thorough examination and fair comparison, we introduce three benchmark sets collected in real-world hazy, rainy, and low-light conditions, respectively, with annotated objects/faces. We launched the UG 2+ challenge Track 2 competition in IEEE CVPR 2019, aiming to evoke a comprehensive discussion and exploration about whether and how low-level vision techniques can benefit the high-level automatic visual recognition in various scenarios. To our best knowledge, this is the first and currently largest effort of its kind. Baseline results by cascading existing enhancement and detection models are reported, indicating the highly challenging nature of our new data as well as the large room for further technical innovations. Thanks to a large participation from the research community, we are able to analyze representative team solutions, striving to better identify the strengths and limitations of existing mindsets as well as the future directions.},
  archive      = {J_TIP},
  author       = {Wenhan Yang and Ye Yuan and Wenqi Ren and Jiaying Liu and Walter J. Scheirer and Zhangyang Wang and Taiheng Zhang and Qiaoyong Zhong and Di Xie and Shiliang Pu and Yuqiang Zheng and Yanyun Qu and Yuhong Xie and Liang Chen and Zhonghao Li and Chen Hong and Hao Jiang and Siyuan Yang and Yan Liu and Xiaochao Qu and Pengfei Wan and Shuai Zheng and Minhui Zhong and Taiyi Su and Lingzhi He and Yandong Guo and Yao Zhao and Zhenfeng Zhu and Jinxiu Liang and Jingwen Wang and Tianyi Chen and Yuhui Quan and Yong Xu and Bo Liu and Xin Liu and Qi Sun and Tingyu Lin and Xiaochuan Li and Feng Lu and Lin Gu and Shengdi Zhou and Cong Cao and Shifeng Zhang and Cheng Chi and Chubing Zhuang and Zhen Lei and Stan Z. Li and Shizheng Wang and Ruizhe Liu and Dong Yi and Zheming Zuo and Jianning Chi and Huan Wang and Kai Wang and Yixiu Liu and Xingyu Gao and Zhenyu Chen and Chang Guo and Yongzhou Li and Huicai Zhong and Jing Huang and Heng Guo and Jianfei Yang and Wenjuan Liao and Jiangang Yang and Liguo Zhou and Mingyue Feng and Likun Qin},
  doi          = {10.1109/TIP.2020.2981922},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5737-5752},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advancing image understanding in poor visibility environments: A collective benchmark study},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to explore saliency for stereoscopic videos via
component-based interaction. <em>TIP</em>, <em>29</em>, 5722–5736. (<a
href="https://doi.org/10.1109/TIP.2020.2985531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we devise a saliency prediction model for stereoscopic videos that learns to explore saliency inspired by the component-based interactions including spatial, temporal, as well as depth cues. The model first takes advantage of specific structure of 3D residual network (3D-ResNet) to model the saliency driven by spatio-temporal coherence from consecutive frames. Subsequently, the saliency inferred by implicit-depth is automatically derived based on the displacement correlation between left and right views by leveraging a deep convolutional network (ConvNet). Finally, a component-wise refinement network is devised to produce final saliency maps over time by aggregating saliency distributions obtained from multiple components. In order to further facilitate research towards stereoscopic video saliency, we create a new dataset including 175 stereoscopic video sequences with diverse content, as well as their dense eye fixation annotations. Extensive experiments support that our proposed model can achieve superior performance compared to the state-of-the-art methods on all publicly available eye fixation datasets.},
  archive      = {J_TIP},
  author       = {Qiudan Zhang and Xu Wang and Shiqi Wang and Zhenhao Sun and Sam Kwong and Jianmin Jiang},
  doi          = {10.1109/TIP.2020.2985531},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5722-5736},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to explore saliency for stereoscopic videos via component-based interaction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color constancy by reweighting image feature maps.
<em>TIP</em>, <em>29</em>, 5711–5721. (<a
href="https://doi.org/10.1109/TIP.2020.2985296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel illuminant color estimation framework is proposed for computational color constancy, which incorporates the high representational capacity of deep-learning-based models and the great interpretability of assumption-based models. The well-designed building block, feature map reweight unit (ReWU), helps to achieve comparative accuracy on benchmark datasets with respect to prior state-of-the-art deep learning based models while requiring more compact model size and cheaper computational cost. In addition to local color estimation, a confidence estimation branch is also included such that the model is able to simultaneously produce point estimate and its uncertainty estimate, which provides useful clues for local estimates aggregation and multiple illumination estimation. The source code and the dataset have been made available (https://github.com/QiuJueqin/Reweight-CC).},
  archive      = {J_TIP},
  author       = {Jueqin Qiu and Haisong Xu and Zhengnan Ye},
  doi          = {10.1109/TIP.2020.2985296},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5711-5721},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color constancy by reweighting image feature maps},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Densely distilled flow-based knowledge transfer in
teacher-student framework for image classification. <em>TIP</em>,
<em>29</em>, 5698–5710. (<a
href="https://doi.org/10.1109/TIP.2020.2984362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new teacher–student framework (TSF)-based knowledge transfer method, in which knowledge in the form of dense flow across layers is distilled from a pre-trained “teacher” deep neural network (DNN) and transferred to another “student” DNN. In the case of distilled knowledge, multiple overlapped flow-based items of information from the pre-trained teacher DNN are densely extracted across layers. Transference of the densely extracted teacher information is then achieved in the TSF using repetitive sequential training from bottom to top between the teacher and student DNN models. In other words, to efficiently transmit extracted useful teacher information to the student DNN, we perform bottom-up step-by-step transfer of densely distilled knowledge. The performance of the proposed method in terms of image classification accuracy and fast optimization is compared with those of existing TSF-based knowledge transfer methods for application to reliable image datasets, including CIFAR-10, CIFAR-100, MNIST, and SVHN. When the dense flow-based sequential knowledge transfer scheme is employed in the TSF, the trained student ResNet more accurately reflects the rich information of the pre-trained teacher ResNet and exhibits superior accuracy to the existing TSF-based knowledge transfer methods for all benchmark datasets considered in this study.},
  archive      = {J_TIP},
  author       = {Ji-Hoon Bae and Doyeob Yeo and Junho Yim and Nae-Soo Kim and Cheol-Sig Pyo and Junmo Kim},
  doi          = {10.1109/TIP.2020.2984362},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5698-5710},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Densely distilled flow-based knowledge transfer in teacher-student framework for image classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Phase recovery guarantees from designed coded diffraction
patterns in optical imaging. <em>TIP</em>, <em>29</em>, 5687–5697. (<a
href="https://doi.org/10.1109/TIP.2020.2985208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase retrieval is an inverse problem that consists in estimating a scene from diffraction intensities. This problem appears in optical imaging, which has three main diffraction zones where the measurements can be acquired, i.e., near, middle and far. Recent works have theoretically solved this inverse problem for the far zone, creating redundancy in the measurement process by including a coded aperture, which allows to modulate the scene and acquire coded diffraction patterns (CDP). However, in the state-of-the-art, the PR problem has not been theoretically studied for CDP at the near and middle zones. Moreover, the structure of the coded aperture is selected at random, leading to suboptimal estimations. Indeed, some of the coding elements employed in the literature are unfeasible because they increase the power of the scene in the modulation process. This paper provides theoretical guarantees for the recovery of a scene from CDP acquired at the three diffraction zones using admissible modulations. Based on the theoretical results, it is established that the image reconstruction quality directly depends on the coded aperture structure; therefore, a design strategy is proposed. In fact, when the scene can be sparsely represented in some basis, its support can be better estimated for a suitable choice of the coding elements in the modulation process. Experimental results show that the scene is successfully recovered by using designed coded apertures with up to 40\% less measurements compared to non-designed ensembles.},
  archive      = {J_TIP},
  author       = {Andrés Guerrero and Samuel Pinilla and Henry Arguello},
  doi          = {10.1109/TIP.2020.2985208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5687-5697},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Phase recovery guarantees from designed coded diffraction patterns in optical imaging},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Extended motion diffusion-based change detection for
airport ground surveillance. <em>TIP</em>, <em>29</em>, 5677–5686. (<a
href="https://doi.org/10.1109/TIP.2020.2984854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection in airport ground is important for airport security. Due to the particularity of ground environment, e.g. haze and camouflage, airport ground change detection is generally incomplete. If an incomplete detection is used as reference for the detection in subsequent frames, it may result in noticeable detection defects across the frames. In this paper, extended motion diffusion (EMD) is proposed to address the problems. The core idea of the EMD is to design a novel model insensitive to incomplete detection. Firstly the one-to-many correspondence in traditional motion diffusion is extended in the prediction step of EMD to build up correspondence from incomplete detection to intact objects. Prior information, e.g. aircraft motion prior and ground structure prior, is employed in the development of the correspondence. Then based on the correspondence a number of new samples are synthesized and filtered in the identification step of the EMD to compensate possible detection defects. Finally, the reserved samples are collected to train a foreground model, which is used in conjunction with another background model for classification. The proposed method is verified based on the Airport Ground Video Surveillance (AGVS) benchmark. Experimental results show effectiveness of the proposed algorithm in dealing with haze and camouflage.},
  archive      = {J_TIP},
  author       = {Xiang Zhang and Honggang Wu and Min Wu and Celimuge Wu},
  doi          = {10.1109/TIP.2020.2984854},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5677-5686},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Extended motion diffusion-based change detection for airport ground surveillance},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Super-resolution of optical coherence tomography images by
scale mixture models. <em>TIP</em>, <em>29</em>, 5662–5676. (<a
href="https://doi.org/10.1109/TIP.2020.2984896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new statistical model is proposed for the single image super-resolution of retinal Optical Coherence Tomography (OCT) images. OCT imaging relies on interfero-metry, which explains why OCT images suffer from a high level of noise. Moreover, data subsampling is carried out during the acquisition of OCT A-scans and B-scans. So, it is necessary to utilize effective super-resolution algorithms to reconstruct high-resolution clean OCT images. In this paper, a nonlocal sparse model-based Bayesian framework is proposed for OCT restoration. For this reason, by characterizing nonlocal patches with similar structures, known as a group, the sparse coefficients of each group of OCT images are modeled by the scale mixture models. In this base, the coefficient vector is decomposed into the point-wise product of a random vector and a positive scaling variable. Estimation of the sparse coefficients depends on the proposed distribution for the random vector and scaling variable where the Laplacian random vector and Generalized Extreme-Value (GEV) scale parameter (Laplacian+GEV model) show the best goodness of fit for each group of OCT images. Finally, a new OCT super-resolution method based on this new scale mixture model is introduced, where the maximum a posterior estimation of both sparse coefficients and scaling variables are calculated efficiently by applying an alternating minimization method. Our experimental results prove that the proposed OCT super-resolution method based on the Laplacian+GEV model outperforms other competing methods in terms of both subjective and objective visual qualities.},
  archive      = {J_TIP},
  author       = {Parisa Ghaderi Daneshmand and Hossein Rabbani and Alireza Mehridehnavi},
  doi          = {10.1109/TIP.2020.2984896},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5662-5676},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Super-resolution of optical coherence tomography images by scale mixture models},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image clustering via deep embedded dimensionality reduction
and probability-based triplet loss. <em>TIP</em>, <em>29</em>,
5652–5661. (<a href="https://doi.org/10.1109/TIP.2020.2984360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image clustering is more challenging than image classification. Without supervised information, current deep learning methods are difficult to be directly applied to image clustering problems. Image clustering needs to deal with three main problems: 1) the curse of dimensionality caused by high-dimensional image data; 2) extracting the effective image features; 3) combining feature extraction, dimensionality reduction and clustering. In this paper, we propose a new clustering framework called Deep Embedded Dimensionality Reduction Clustering (DERC) via Probability-Based Triplet Loss, which effectively solves the above issues. To the best of our knowledge, the DERC is the first framework that effectively combines image embedding, dimensionality reduction, and clustering into the image clustering process. We also propose to incorporate a novel probability-based triplet loss measure to retrain the DERC network as a unified framework. By integrating the reconstruction loss and the probability-based triplet loss, we can improve the image clustering accuracy. Extensive experiments show that our proposed methods outperform state-of-the-art methods on many commonly used datasets.},
  archive      = {J_TIP},
  author       = {Yuanjie Yan and Hongyan Hao and Baile Xu and Jian Zhao and Furao Shen},
  doi          = {10.1109/TIP.2020.2984360},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5652-5661},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image clustering via deep embedded dimensionality reduction and probability-based triplet loss},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CGAN-TM: A novel domain-to-domain transferring method for
person re-identification. <em>TIP</em>, <em>29</em>, 5641–5651. (<a
href="https://doi.org/10.1109/TIP.2020.2985545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) is a technique aiming to recognize person cross different cameras. Although some supervised methods have achieved favorable performance, they are far from practical application owing to the lack of labeled data. Thus, unsupervised person re-ID methods are in urgent need. Generally, the commonly used approach in existing unsupervised methods is to first utilize the source image dataset for generating a model in supervised manner, and then transfer the source image domain to the target image domain. However, images may lose their identity information after translation, and the distributions between different domains are far away. To solve these problems, we propose an image domain-to-domain translation method by keeping pedestrian&#39;s identity information and pulling closer the domains&#39; distributions for unsupervised person re-ID tasks. Our work exploits the CycleGAN to transfer the existing labeled image domain to the unlabeled image domain. Specially, a Self-labeled Triplet Net is proposed to maintain the pedestrian identity information, and maximum mean discrepancy is introduced to pull the domain distribution closer. Extensive experiments have been conducted and the results demonstrate that the proposed method performs superiorly than the state-of-the-art unsupervised methods on DukeMTMC-reID and Market-1501.},
  archive      = {J_TIP},
  author       = {Yingzhi Tang and Xi Yang and Nannan Wang and Bin Song and Xinbo Gao},
  doi          = {10.1109/TIP.2020.2985545},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5641-5651},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CGAN-TM: A novel domain-to-domain transferring method for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). All-pass parametric image registration. <em>TIP</em>,
<em>29</em>, 5625–5640. (<a
href="https://doi.org/10.1109/TIP.2020.2984897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image registration is a required step in many practical applications that involve the acquisition of multiple related images. In this paper, we propose a methodology to deal with both the geometric and intensity transformations in the image registration problem. The main idea is to modify an accurate and fast elastic registration algorithm (Local All-Pass—LAP) so that it returns a parametric displacement field, and to estimate the intensity changes by fitting another parametric expression. Although we demonstrate the methodology using a low-order parametric model, our approach is highly flexible and easily allows substantially richer parametrisations, while requiring only limited extra computation cost. In addition, we propose two novel quantitative criteria to evaluate the accuracy of the alignment of two images (“salience correlation”) and the number of degrees of freedom (“parsimony”) of a displacement field, respectively. Experimental results on both synthetic and real images demonstrate the high accuracy and computational efficiency of our methodology. Furthermore, we demonstrate that the resulting displacement fields are more parsimonious than the ones obtained in other state-of-the-art image registration approaches.},
  archive      = {J_TIP},
  author       = {Xinxin Zhang and Christopher Gilliam and Thierry Blu},
  doi          = {10.1109/TIP.2020.2984897},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5625-5640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {All-pass parametric image registration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-reference video quality assessment using natural
spatiotemporal scene statistics. <em>TIP</em>, <em>29</em>, 5612–5624.
(<a href="https://doi.org/10.1109/TIP.2020.2984879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust spatiotemporal representations of natural videos have several applications including quality assessment, action recognition, object tracking etc. In this paper, we propose a video representation that is based on a parameterized statistical model for the spatiotemporal statistics of mean subtracted and contrast normalized (MSCN) coefficients of natural videos. Specifically, we propose an asymmetric generalized Gaussian distribution (AGGD) to model the statistics of MSCN coefficients of natural videos and their spatiotemporal Gabor bandpass filtered outputs. We then demonstrate that the AGGD model parameters serve as good representative features for distortion discrimination. Based on this observation, we propose a supervised learning approach using support vector regression (SVR) to address the no-reference video quality assessment (NRVQA) problem. The performance of the proposed algorithm is evaluated on publicly available video quality assessment (VQA) datasets with both traditional and in-capture/authentic distortions. We show that the proposed algorithm delivers competitive performance on traditional (synthetic) distortions and acceptable performance on authentic distortions. The code for our algorithm will be released at https://www.iith.ac.in/lfovia/downloads.html.},
  archive      = {J_TIP},
  author       = {Sathya Veera Reddy Dendi and Sumohana S. Channappayya},
  doi          = {10.1109/TIP.2020.2984879},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5612-5624},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No-reference video quality assessment using natural spatiotemporal scene statistics},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new polyphase down-sampling-based multiple description
image coding. <em>TIP</em>, <em>29</em>, 5596–5611. (<a
href="https://doi.org/10.1109/TIP.2020.2984876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple description coding (MDC) is an efficient source coding technique for error-prone transmission over multiple channels. In this paper, we focus on the design of a new polyphase down-sampling based MDC (NPDS-MDC) for image signals. The encoding of our proposed NPDS-MDC consists of three steps. First, we perform down-sampling on each ${N} \times {N}$ image block according to the quincunx down-sampling pattern. Second, we propose a new transform and apply it to the down-sampled pixels to produce the side descriptions. Third, we develop an error compensation algorithm to reduce the compression distortion occurring on the down-sampled pixels. In our scheme, the side decoding is performed posterior to image interpolation with reference to the down-sampled compressed pixels. Moreover, the central decoding is achieved by interlacing the side descriptions. We also propose a compression-constrained central deblocking algorithm to further improve the efficiency of the central decoding. The experimental results indicate that our proposed MDC scheme offers clearly superior performance, especially at high bit rates, as compared to the state-of-the-art methods for various types of images.},
  archive      = {J_TIP},
  author       = {Shuyuan Zhu and Zhiying He and Xiandong Meng and Jiantao Zhou and Yuanfang Guo and Bing Zeng},
  doi          = {10.1109/TIP.2020.2984876},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5596-5611},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A new polyphase down-sampling-based multiple description image coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anisotropic convolution for image classification.
<em>TIP</em>, <em>29</em>, 5584–5595. (<a
href="https://doi.org/10.1109/TIP.2020.2985875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks are built upon simple but useful convolution modules. The traditional convolution has a limitation on feature extraction and object localization due to its fixed scale and geometric structure. Besides, the loss of spatial information also restricts the networks’ performance and depth. To overcome these limitations, this paper proposes a novel anisotropic convolution by adding a scale factor and a shape factor into the traditional convolution. The anisotropic convolution augments the receptive fields flexibly and dynamically depending on the valid sizes of objects. In addition, the anisotropic convolution is a generalized convolution. The traditional convolution, dilated convolution and deformable convolution can be viewed as its special cases. Furthermore, in order to improve the training efficiency and avoid falling into a local optimum, this paper introduces a simplified implementation of the anisotropic convolution. The anisotropic convolution can be applied to arbitrary convolutional networks and the enhanced networks are called ACNs (anisotropic convolutional networks). Experimental results show that ACNs achieve better performance than many state-of-the-art methods and the baseline networks in tasks of image classification and object localization, especially in classification task of tiny images.},
  archive      = {J_TIP},
  author       = {Wenjuan Li and Bing Li and Chunfeng Yuan and Yangxi Li and Haohao Wu and Weiming Hu and Fangshi Wang},
  doi          = {10.1109/TIP.2020.2985875},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5584-5595},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Anisotropic convolution for image classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying and detecting collective motion in crowd scenes.
<em>TIP</em>, <em>29</em>, 5571–5583. (<a
href="https://doi.org/10.1109/TIP.2020.2985284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People in crowd scenes always exhibit consistent behaviors and form collective motions. The analysis of collective motion has motivated a surge of interest in computer vision. Nevertheless, the effort is hampered by the complex nature of collective motions. Considering the fact that collective motions are formed by individuals, this paper proposes a new framework for both quantifying and detecting collective motion by investigating the spatio-temporal behavior of individuals. The main contributions of this work are threefold: 1) an intention-aware model is built to fully capture the intrinsic dynamics of individuals; 2) a structure-based collectiveness measurement is developed to accurately quantify the collective properties of crowds; 3) a multi-stage clustering strategy is formulated to detect both the local and global behavior consistency in crowd scenes. Experiments on real world data sets show that our method is able to handle crowds with various structures and time-varying dynamics. Especially, the proposed method shows nearly 10\% improvement over the competitors in terms of NMI, Purity and RI. Its applicability is illustrated in the context of anomaly detection and semantic scene segmentation.},
  archive      = {J_TIP},
  author       = {Xuelong Li and Mulin Chen and Qi Wang},
  doi          = {10.1109/TIP.2020.2985284},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5571-5583},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Quantifying and detecting collective motion in crowd scenes},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motion segmentation of RGB-d sequences: Combining semantic
and motion information using statistical inference. <em>TIP</em>,
<em>29</em>, 5557–5570. (<a
href="https://doi.org/10.1109/TIP.2020.2984893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an innovative method for motion segmentation in RGB-D dynamic videos with multiple moving objects. The focus is on finding static, small or slow moving objects (often overlooked by other methods) that their inclusion can improve the motion segmentation results. In our approach, semantic object based segmentation and motion cues are combined to estimate the number of moving objects, their motion parameters and perform segmentation. Selective object-based sampling and correspondence matching are used to estimate object specific motion parameters. The main issue with such an approach is the over segmentation of moving parts due to the fact that different objects can have the same motion (e.g. background objects). To resolve this issue, we propose to identify objects with similar motions by characterizing each motion by a distribution of a simple metric and using a statistical inference theory to assess their similarities. To demonstrate the significance of the proposed statistical inference, we present an ablation study, with and without static objects inclusion, on SLAM accuracy using the TUM-RGBD dataset. To test the effectiveness of the proposed method for finding small or slow moving objects, we applied the method to RGB-D MultiBody and SBM-RGBD motion segmentation datasets. The results showed that we can improve the accuracy of motion segmentation for small objects while remaining competitive on overall measures.},
  archive      = {J_TIP},
  author       = {Sundaram Muthu and Ruwan Tennakoon and Tharindu Rathnayake and Reza Hoseinnezhad and David Suter and Alireza Bab-Hadiashar},
  doi          = {10.1109/TIP.2020.2984893},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5557-5570},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Motion segmentation of RGB-D sequences: Combining semantic and motion information using statistical inference},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving description-based person re-identification by
multi-granularity image-text alignments. <em>TIP</em>, <em>29</em>,
5542–5556. (<a href="https://doi.org/10.1109/TIP.2020.2984883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Description-based person re-identification (Re-id) is an important task in video surveillance that requires discriminative cross-modal representations to distinguish different people. It is difficult to directly measure the similarity between images and descriptions due to the modality heterogeneity (the cross-modal problem). And all samples belonging to a single category (the fine-grained problem) makes this task even harder than the conventional image-description matching task. In this paper, we propose a Multi-granularity Image-text Alignments (MIA) model to alleviate the cross-modal fine-grained problem for better similarity evaluation in description-based person Re-id. Specifically, three different granularities, i.e. , global-global, global-local and local-local alignments are carried out hierarchically. Firstly, the global-global alignment in the Global Contrast (GC) module is for matching the global contexts of images and descriptions. Secondly, the global-local alignment employs the potential relations between local components and global contexts to highlight the distinguishable components while eliminating the uninvolved ones adaptively in the Relation-guided Global-local Alignment (RGA) module. Thirdly, as for the local-local alignment, we match visual human parts with noun phrases in the Bi-directional Fine-grained Matching (BFM) module. The whole network combining multiple granularities can be end-to-end trained without complex pre-processing. To address the difficulties in training the combination of multiple granularities, an effective step training strategy is proposed to train these granularities step-by-step. Extensive experiments and analysis have shown that our method obtains the state-of-the-art performance on the CUHK-PEDES dataset and outperforms the previous methods by a significant margin.},
  archive      = {J_TIP},
  author       = {Kai Niu and Yan Huang and Wanli Ouyang and Liang Wang},
  doi          = {10.1109/TIP.2020.2984883},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5542-5556},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving description-based person re-identification by multi-granularity image-text alignments},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-range binocular vision target geolocation using
handheld electronic devices in outdoor environment. <em>TIP</em>,
<em>29</em>, 5531–5541. (<a
href="https://doi.org/10.1109/TIP.2020.2984898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binocular vision is a passive method of simulating the human visual principle to perceive the distance to a target. Traditional binocular vision applied to target localization is usually suitable for short-range area and indoor environment. This paper presents a novel vision-based geolocation method for long-range targets in outdoor environment, using handheld electronic devices such as smart phones and tablets. This method solves the problems in long-range localization and determining geographic coordinates of the targets in outdoor environment. It is noted that these sensors necessary for binocular vision geolocation such as the camera, GPS, and inertial measurement unit (IMU), are intergrated in these handheld electronic devices. This method, employing binocular localization model and coordinate transformations, is provided for these handheld electronic devices to obtain the GPS coordinates of the targets. Finally, two types of handheld electronic devices are used to conduct the experiments for targets in long range up to 500m. The experimental results show that this method yields the target geolocation accuracy along horizontal direction with nearly 20m, achieving comparable or even better performance than monocular vision methods.},
  archive      = {J_TIP},
  author       = {Fang Deng and Lele Zhang and Feng Gao and Huangbin Qiu and Xin Gao and Jie Chen},
  doi          = {10.1109/TIP.2020.2984898},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5531-5541},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Long-range binocular vision target geolocation using handheld electronic devices in outdoor environment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic spatial predicted background. <em>TIP</em>,
<em>29</em>, 5517–5530. (<a
href="https://doi.org/10.1109/TIP.2020.2983598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for online background modeling for static video cameras - Dynamic Spatial Predicted Background (DSPB). Our unique method employs a small subset of image pixels to predict the whole scene by exploiting pixel correlations (distant and close). DSPB acts as a hybrid model combining successful elements taken from two major approaches: local-adaptive that propose to fit a distribution pixelwise, and global-linear that reconstruct the background by finding a low-rank version of the scene. To our knowledge, this is the first attempt to combine these approaches in a unified system. DSPB models the scene as a superposition of illumination effects and predicts each pixel&#39;s value by a linear estimator comprised of only 5 pixels of the scene and can initialize the background starting from the 5th frame. By doing so, we keep the computational load low, allowing our method to be used in many real-time applications using simple hardware. The suggested prediction model of scene appearance is novel, and the scheme is very accurate and efficient computationally. We show the method merits on an application for video FG-BG separation, and how some of the main existing approaches may be challenged and how their drawbacks are less dominant in our model. Experimental results validate our findings, by computation speed and mean F-measure values on several public datasets. We also examine how results may improve by analyzing each video individually according to its content. DSPB can be successfully incorporated in other image processing tasks like change detection, video compression and video inpainting.},
  archive      = {J_TIP},
  author       = {Yaniv Tocker and Rami R. Hagege and Joseph M. Francos},
  doi          = {10.1109/TIP.2020.2983598},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5517-5530},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic spatial predicted background},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational osmosis for non-linear image fusion.
<em>TIP</em>, <em>29</em>, 5507–5516. (<a
href="https://doi.org/10.1109/TIP.2020.2983537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new variational model for non-linear image fusion. Our approach is based on the use of an osmosis energy term related to the one studied in Vogel et al. and Weickert et al. The minimization of the proposed non-convex energy realizes visually plausible image data fusion, invariant to multiplicative brightness changes. On the practical side, it requires minimal supervision and parameter tuning and can encode prior information on the structure of the images to be fused. For the numerical solution of the proposed model, we develop a primal-dual algorithm and we apply the resulting minimization scheme to solve multi-modal face fusion, color transfer and cultural heritage conservation problems. Visual and quantitative comparisons to state-of-the-art approaches prove the out-performance and the flexibility of our method.},
  archive      = {J_TIP},
  author       = {Simone Parisotto and Luca Calatroni and Aurélie Bugeau and Nicolas Papadakis and Carola-Bibiane Schönlieb},
  doi          = {10.1109/TIP.2020.2983537},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5507-5516},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational osmosis for non-linear image fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Temporal reasoning graph for activity recognition.
<em>TIP</em>, <em>29</em>, 5491–5506. (<a
href="https://doi.org/10.1109/TIP.2020.2985219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite great success has been achieved in activity analysis, it still has many challenges. Most existing works in activity recognition pay more attention to designing efficient architecture or video sampling strategy. However, due to the property of fine-grained action and long term structure in video, activity recognition is expected to reason temporal relation between video sequences. In this paper, we propose an efficient temporal reasoning graph (TRG) to simultaneously capture the appearance features and temporal relation between video sequences at multiple time scales. Specifically, we construct learnable temporal relation graphs to explore temporal relation on the multi-scale range. Additionally, to facilitate multi-scale temporal relation extraction, we design a multi-head temporal adjacent matrix to represent multi-kinds of temporal relations. Eventually, a multi-head temporal relation aggregator is proposed to extract the semantic meaning of those features convolving through the graphs. Extensive experiments are performed on widely-used large-scale datasets, such as Something-Something, Charades and Jester, and the results show that our model can achieve state-of-the-art performance. Further analysis shows that temporal relation reasoning with our TRG can extract discriminative features for activity recognition.},
  archive      = {J_TIP},
  author       = {Jingran Zhang and Fumin Shen and Xing Xu and Heng Tao Shen},
  doi          = {10.1109/TIP.2020.2985219},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5491-5506},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Temporal reasoning graph for activity recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised person re-identification via cross-camera
similarity exploration. <em>TIP</em>, <em>29</em>, 5481–5490. (<a
href="https://doi.org/10.1109/TIP.2020.2982826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most person re-identification (re-ID) approaches are based on supervised learning, which requires manually annotated data. However, it is not only resource-intensive to acquire identity annotation but also impractical for large-scale data. To relieve this problem, we propose a cross-camera unsupervised approach that makes use of unsupervised style-transferred images to jointly optimize a convolutional neural network (CNN) and the relationship among the individual samples for person re-ID. Our algorithm considers two fundamental facts in the re-ID task, i.e., variance across diverse cameras and similarity within the same identity. In this paper, we propose an iterative framework which overcomes the camera variance and achieves across-camera similarity exploration. Specifically, we apply an unsupervised style transfer model to generate style-transferred training images with different camera styles. Then we iteratively exploit the similarity within the same identity from both the original and the style-transferred data. We start with considering each training image as a different class to initialize the Convolutional Neural Network (CNN) model. Then we measure the similarity and gradually group similar samples into one class, which increases similarity within each identity. We also introduce a diversity regularization term in the clustering to balance the cluster distribution. The experimental results demonstrate that our algorithm is not only superior to state-of-the-art unsupervised re-ID approaches, but also performs favorably compared with other competing unsupervised domain adaptation methods (UDA) and semi-supervised learning methods.},
  archive      = {J_TIP},
  author       = {Yutian Lin and Yu Wu and Chenggang Yan and Mingliang Xu and Yi Yang},
  doi          = {10.1109/TIP.2020.2982826},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5481-5490},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised person re-identification via cross-camera similarity exploration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rate-distortion driven decomposition of multiview imagery to
diffuse and specular components. <em>TIP</em>, <em>29</em>, 5469–5480.
(<a href="https://doi.org/10.1109/TIP.2020.2983849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose an overcomplete representation of multiview imagery for the purpose of compression. We present a rate-distortion (R-D) driven approach to decompose multiview datasets into two additive parts which can be interpreted as diffuse and specular content. We choose distinct and different sparsifying transforms for the diffuse and specular components and employ an R-D inspired measure as our optimization cost function to drive the decomposition based solely on compressibility. We first describe a framework which performs data separation in a registered domain to avoid the complexity of warping between views. Then a more comprehensive approach is proposed to separate specular data progressively from coordinates of multiple reference views. Experimental results show a coding gain of up to 0.6 dB for synthetic datasets and up to 0.9 dB for real datasets.},
  archive      = {J_TIP},
  author       = {Maryam Haghighat and Reji Mathew and David Taubman},
  doi          = {10.1109/TIP.2020.2983849},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5469-5480},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rate-distortion driven decomposition of multiview imagery to diffuse and specular components},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Web-shaped model for head pose estimation: An approach for
best exemplar selection. <em>TIP</em>, <em>29</em>, 5457–5468. (<a
href="https://doi.org/10.1109/TIP.2020.2984373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation is a sensitive topic in video surveillance/smart ambient scenarios since head rotations can hide/distort discriminative features of the face. Face recognition would often tackle the problem of video frames where subjects appear in poses making it quite impossible. In this respect, the selection of the frames with the best face orientation can allow triggering recognition only on these, therefore decreasing the possibility of errors. This paper proposes a novel approach to head pose estimation for smart cities and video surveillance scenarios, aiming at this goal. The method relies on a cascade of two models: the first one predicts the positions of 68 well-known face landmarks; the second one applies a web-shaped model over the detected landmarks, to associate each of them to a specific face sector. The method can work on detected faces at a reasonable distance and with a resolution that is supported by several present devices. Results of experiments executed over some classical pose estimation benchmarks, namely Point &#39;04, Biwi, and AFLW datasets show good performance in terms of both pose estimation and computing time. Further results refer to noisy images that are typical of the addressed settings. Finally, examples demonstrate the selection of the best frames from videos captured in video surveillance conditions.},
  archive      = {J_TIP},
  author       = {Paola Barra and Silvio Barra and Carmen Bisogni and Maria De Marsico and Michele Nappi},
  doi          = {10.1109/TIP.2020.2984373},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5457-5468},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Web-shaped model for head pose estimation: An approach for best exemplar selection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel symmetry driven siamese network for THz concealed
object verification. <em>TIP</em>, <em>29</em>, 5447–5456. (<a
href="https://doi.org/10.1109/TIP.2020.2983554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security inspection aims to improve the high detection rate as well as reduce the false alarm rate. However, it still suffers from two challenges affecting its robustness. 1) Existing security inspection methods are mostly designed for natural images, which cannot reflect the uniqueness and imaging principle of THz images. 2) Existing methods is sensitive to noise interference and pose variations. This work revisits these challenges and presents a novel symmetry driven Siamese network (SDSN) for THz concealed object verification. Our idea is to employ a specially designed network architecture for THz concealed object verification. First, to reflect the uniqueness and the special property of THz images, Siamese network with Contrastive loss is used for feature extraction along with symmetrical prior information consideration, which can learn symmetrical metrics from the same person. Second, to alleviate the impact of noise interference and pose variations, the adaptive identity normalization (A-IDN) is proposed to normalize the symmetrical metrics each person. Finally, to enhance the generalization of network, an adaptive selective threshold based on Gaussian mixture model (AST-GMM) is designed, which serves as a classifier for the final classification results. Extensive experiments show that SDSN significantly improves the accuracy. Specially, SDSN outperforms the state-of-the-art methods without symmetrical prior information on THz security dataset.},
  archive      = {J_TIP},
  author       = {Xi Yang and Haoyuan Guo and Nannan Wang and Bin Song and Xinbo Gao},
  doi          = {10.1109/TIP.2020.2983554},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5447-5456},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel symmetry driven siamese network for THz concealed object verification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learned fast HEVC intra coding. <em>TIP</em>, <em>29</em>,
5431–5446. (<a href="https://doi.org/10.1109/TIP.2020.2982832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In High Efficiency Video Coding (HEVC), excellent rate-distortion (RD) performance is achieved in part by having a flexible quadtree coding unit (CU) partition and a large number of intra-prediction modes. Such an excellent RD performance is achieved at the expense of much higher computational complexity. In this paper, we propose a learned fast HEVC intra coding (LFHI) framework taking into account the comprehensive factors of fast intra coding to reach an improved configurable tradeoff between coding performance and computational complexity. First, we design a low-complex shallow asymmetric-kernel CNN (AK-CNN) to efficiently extract the local directional texture features of each block for both fast CU partition and fast intra-mode decision. Second, we introduce the concept of the minimum number of RDO candidates (MNRC) into fast mode decision, which utilizes AK-CNN to predict the minimum number of best candidates for RDO calculation to further reduce the computation of intra-mode selection. Third, an evolution optimized threshold decision (EOTD) scheme is designed to achieve configurable complexity-efficiency tradeoffs. Finally, we propose an interpolation-based prediction scheme that allows for our framework to be generalized to all quantization parameters (QPs) without the need for training the network on each QP. The experimental results demonstrate that the LFHI framework has a high degree of parallelism and achieves a much better complexity-efficiency tradeoff, achieving up to 75.2\% intra-mode encoding complexity reduction with negligible rate-distortion performance degradation, superior to the existing fast intra-coding schemes.},
  archive      = {J_TIP},
  author       = {Zhibo Chen and Jun Shi and Weiping Li},
  doi          = {10.1109/TIP.2020.2982832},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5431-5446},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learned fast HEVC intra coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STC-GAN: Spatio-temporally coupled generative adversarial
networks for predictive scene parsing. <em>TIP</em>, <em>29</em>,
5420–5430. (<a href="https://doi.org/10.1109/TIP.2020.2983567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive scene parsing is a task of assigning pixel-level semantic labels to a future frame of a video. It has many applications in vision-based artificial intelligent systems, e.g., autonomous driving and robot navigation. Although previous work has shown its promising performance in semantic segmentation of images and videos, it is still quite challenging to anticipate future scene parsing with limited annotated training data. In this paper, we propose a novel model called STC-GAN, Spatio-Temporally Coupled Generative Adversarial Networks for predictive scene parsing, which employ both convolutional neural networks and convolutional long short-term memory (LSTM) in the encoder-decoder architecture. By virtue of STC-GAN, both spatial layout and semantic context can be captured by the spatial encoder effectively, while motion dynamics are extracted by the temporal encoder accurately. Furthermore, a coupled architecture is presented for establishing joint adversarial training where the weights are shared and features are transformed in an adaptive fashion between the future frame generation model and predictive scene parsing model. Consequently, the proposed STC-GAN is able to learn valuable features from unlabeled video data. We evaluate our proposed STC-GAN on two public datasets, i.e., Cityscapes and CamVid. Experimental results demonstrate that our method outperforms the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Mengshi Qi and Yunhong Wang and Annan Li and Jiebo Luo},
  doi          = {10.1109/TIP.2020.2983567},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5420-5430},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {STC-GAN: Spatio-temporally coupled generative adversarial networks for predictive scene parsing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse graph regularized mesh color edit propagation.
<em>TIP</em>, <em>29</em>, 5408–5419. (<a
href="https://doi.org/10.1109/TIP.2020.2980962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh color edit propagation aims to propagate the color from a few color strokes to the whole mesh, which is useful for mesh colorization, color enhancement and color editing, etc. Compared with image edit propagation, luminance information is not available for 3D mesh data, so the color edit propagation is more difficult on 3D meshes than images, with far less research carried out. This paper proposes a novel solution based on sparse graph regularization. Firstly, a few color strokes are interactively drawn by the user, and then the color will be propagated to the whole mesh by minimizing a sparse graph regularized nonlinear energy function. The proposed method effectively measures geometric similarity over shapes by using a set of complementary multiscale feature descriptors, and effectively controls color bleeding via a sparse ℓ 1 optimization rather than quadratic minimization used in existing work. The proposed framework can be applied for the task of interactive mesh colorization, mesh color enhancement and mesh color editing. Extensive qualitative and quantitative experiments show that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Bo Li and Yu-Kun Lai and Paul L. Rosin},
  doi          = {10.1109/TIP.2020.2980962},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5408-5419},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sparse graph regularized mesh color edit propagation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-granularity canonical appearance pooling for remote
sensing scene classification. <em>TIP</em>, <em>29</em>, 5396–5407. (<a
href="https://doi.org/10.1109/TIP.2020.2983560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognising remote sensing scene images remains challenging due to large visual-semantic discrepancies. These mainly arise due to the lack of detailed annotations that can be employed to align pixel-level representations with high-level semantic labels. As the tagging process is labour-intensive and subjective, we hereby propose a novel Multi-Granularity Canonical Appearance Pooling (MG-CAP) to automatically capture the latent ontological structure of remote sensing datasets. We design a granular framework that allows progressively cropping the input image to learn multi-grained features. For each specific granularity, we discover the canonical appearance from a set of pre-defined transformations and learn the corresponding CNN features through a maxout-based Siamese style architecture. Then, we replace the standard CNN features with Gaussian covariance matrices and adopt the proper matrix normalisations for improving the discriminative power of features. Besides, we provide a stable solution for training the eigenvalue-decomposition function (EIG) in a GPU and demonstrate the corresponding back-propagation using matrix calculus. Extensive experiments have shown that our framework can achieve promising results in public remote sensing scene datasets.},
  archive      = {J_TIP},
  author       = {Shidong Wang and Yu Guan and Ling Shao},
  doi          = {10.1109/TIP.2020.2983560},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5396-5407},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-granularity canonical appearance pooling for remote sensing scene classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized convolutional sparse coding with unknown noise.
<em>TIP</em>, <em>29</em>, 5386–5395. (<a
href="https://doi.org/10.1109/TIP.2020.2980980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional sparse coding (CSC) can learn representative shift-invariant patterns from data. However, existing CSC methods assume the Gaussian noise, which can be restrictive in some challenging applications. In this paper, we propose a generalized CSC model capable of handling complicated unknown noise. The noise is modeled by the Gaussian mixture model, which can approximate any continuous probability density function. The Expectation-Maximization algorithm is used to solve the resultant learning problem. For efficient optimization, the crux is to speed up the convolution in the frequency domain while keeping the other computations involving the weight matrix in the spatial domain. We design an efficient solver for the weighted CSC problem in the M-step. The dictionary and codes are updated simultaneously by an efficient nonconvex accelerated proximal gradient algorithm. The resultant procedure, called generalized convolutional sparse coding (GCSC), obtains the same space complexity and a smaller running time than existing CSC methods (which are limited to the Gaussian noise). Extensive experiments on synthetic and real-world noisy data sets validate that GCSC can model the noise effectively and obtain high-quality filters and representations.},
  archive      = {J_TIP},
  author       = {Yaqing Wang and James T. Kwok and Lionel M. Ni},
  doi          = {10.1109/TIP.2020.2980980},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5386-5395},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized convolutional sparse coding with unknown noise},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based non-convex low-rank regularization for image
compression artifact reduction. <em>TIP</em>, <em>29</em>, 5374–5385.
(<a href="https://doi.org/10.1109/TIP.2020.2975931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Block transform coded images usually suffer from annoying artifacts at low bit-rates, because of the independent quantization of DCT coefficients. Image prior models play an important role in compressed image reconstruction. Natural image patches in a small neighborhood of the high-dimensional image space usually exhibit an underlying sub-manifold structure. To model the distribution of signal, we extract sub-manifold structure as prior knowledge. We utilize graph Laplacian regularization to characterize the sub-manifold structure at patch level. And similar patches are exploited as samples to estimate distribution of a particular patch. Instead of using Euclidean distance as similarity metric, we propose to use graph-domain distance to measure the patch similarity. Then we perform low-rank regularization on the similar-patch group, and incorporate a non-convex l p enalty to surrogate matrix rank. Finally, an alternatively minimizing strategy is employed to solve the non-convex problem. Experimental results show that our proposed method is capable of achieving more accurate reconstruction than the state-of-the-art methods in both objective and perceptual qualities.},
  archive      = {J_TIP},
  author       = {Jing Mu and Ruiqin Xiong and Xiaopeng Fan and Dong Liu and Feng Wu and Wen Gao},
  doi          = {10.1109/TIP.2020.2975931},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5374-5385},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph-based non-convex low-rank regularization for image compression artifact reduction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Practically lossless affine image transformation.
<em>TIP</em>, <em>29</em>, 5367–5373. (<a
href="https://doi.org/10.1109/TIP.2020.2982260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contribution we introduce an almost lossless affine 2D image transformation method. To this end we extend the theory of the well-known Chirp-z transform to allow for fully affine transformation of general n-dimensional images. In addition we give a practical spatial and spectral zero-padding approach dramatically reducing losses of our transform, where usual transforms introduce blurring artifacts due to sub-optimal interpolation. The proposed method improves the mean squared error by approx. a factor of 1800 compared to the commonly used linear interpolation, and by a factor of 250 to the best competitor. We derive the transform from basic principles with special attention to implementation details and supplement this paper with python code for 2D images. In demonstration experiments we show the superior image quality compared to usual approaches, when using our method. However runtimes are considerably larger than when using toolbox algorithms.},
  archive      = {J_TIP},
  author       = {Daniel Pflugfelder and Hanno Scharr},
  doi          = {10.1109/TIP.2020.2982260},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5367-5373},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Practically lossless affine image transformation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient in-loop filtering based on enhanced deep
convolutional neural networks for HEVC. <em>TIP</em>, <em>29</em>,
5352–5366. (<a href="https://doi.org/10.1109/TIP.2020.2982534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The raw video data can be compressed much by the latest video coding standard, high efficiency video coding (HEVC). However, the block-based hybrid coding used in HEVC will incur lots of artifacts in compressed videos, the video quality will be severely influenced. To settle this problem, the in-loop filtering is used in HEVC to eliminate artifacts. Inspired by the success of deep learning, we propose an efficient in-loop filtering algorithm based on the enhanced deep convolutional neural networks (EDCNN) for significantly improving the performance of in-loop filtering in HEVC. Firstly, the problems of traditional convolutional neural networks models, including the normalization method, network learning ability, and loss function, are analyzed. Then, based on the statistical analyses, the EDCNN is proposed for efficiently eliminating the artifacts, which adopts three solutions, including a weighted normalization method, a feature information fusion block, and a precise loss function. Finally, the PSNR enhancement, PSNR smoothness, RD performance, subjective test, and computational complexity/GPU memory consumption are employed as the evaluation criteria, and experimental results show that when compared with the filter in HM16.9, the proposed in-loop filtering algorithm achieves an average of 6.45\% BDBR reduction and 0.238 dB BDPSNR gains.},
  archive      = {J_TIP},
  author       = {Zhaoqing Pan and Xiaokai Yi and Yun Zhang and Byeungwoo Jeon and Sam Kwong},
  doi          = {10.1109/TIP.2020.2982534},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5352-5366},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient in-loop filtering based on enhanced deep convolutional neural networks for HEVC},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep saliency hashing for fine-grained retrieval.
<em>TIP</em>, <em>29</em>, 5336–5351. (<a
href="https://doi.org/10.1109/TIP.2020.2971105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, hashing methods have been proved to be effective and efficient for large-scale Web media search. However, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have a subtle difference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained hashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which automatically mines salient regions and learns semantic-preserving hashing codes simultaneously. DSaH is a two-step end-to-end model consisting of an attention network and a hashing network. Our loss function contains three basic components, including the semantic loss, the saliency loss, and the quantization loss. As the core of DSaH, the saliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval datasets for performance evaluation. Experimental results on fine-grained datasets, including Oxford Flowers, Stanford Dogs, and CUB Birds demonstrate that our DSaH performs the best for the fine-grained retrieval task and beats the strongest competitor (DTQ) by approximately 10\% on both Stanford Dogs and CUB Birds. DSaH is also comparable to several state-of-the-art hashing methods on CIFAR-10 and NUS-WIDE.},
  archive      = {J_TIP},
  author       = {Sheng Jin and Hongxun Yao and Xiaoshuai Sun and Shangchen Zhou and Lei Zhang and Xiansheng Hua},
  doi          = {10.1109/TIP.2020.2971105},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5336-5351},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep saliency hashing for fine-grained retrieval},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fusion of magnetic resonance and ultrasound images for
endometriosis detection. <em>TIP</em>, <em>29</em>, 5324–5335. (<a
href="https://doi.org/10.1109/TIP.2020.2975977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new fusion method for magnetic resonance (MR) and ultrasound (US) images, which aims at combining the advantages of each modality, i.e., good contrast and signal to noise ratio for the MR image and good spatial resolution for the US image. The proposed algorithm is on an inverse problem, performing a super-resolution of the MR image and a denoising of the US image. A polynomial function is introduced to model the relationships between the gray levels of the MR and US images. The resulting inverse problem is solved using a proximal alternating linearized minimization algorithm. The accuracy and the interest of the fusion algorithm are shown quantitatively and qualitatively via evaluations on synthetic and experimental phantom data.},
  archive      = {J_TIP},
  author       = {Oumaima El Mansouri and Fabien Vidal and Adrian Basarab and Pierre Payoux and Denis Kouamé and Jean-Yves Tourneret},
  doi          = {10.1109/TIP.2020.2975977},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5324-5335},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fusion of magnetic resonance and ultrasound images for endometriosis detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image recovery via transform learning and low-rank modeling:
The power of complementary regularizers. <em>TIP</em>, <em>29</em>,
5310–5323. (<a href="https://doi.org/10.1109/TIP.2020.2980753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works on adaptive sparse and on low-rank signal modeling have demonstrated their usefulness in various image/video processing applications. Patch-based methods exploit local patch sparsity, whereas other works apply low-rankness of grouped patches to exploit image non-local structures. However, using either approach alone usually limits performance in image reconstruction or recovery applications. In this work, we propose a simultaneous sparsity and low-rank model, dubbed STROLLR, to better represent natural images. In order to fully utilize both the local and non-local image properties, we develop an image restoration framework using a transform learning scheme with joint low-rank regularization. The approach owes some of its computational efficiency and good performance to the use of transform learning for adaptive sparse representation rather than the popular synthesis dictionary learning algorithms, which involve approximation of NP-hard sparse coding and expensive learning steps. We demonstrate the proposed framework in various applications to image denoising, inpainting, and compressed sensing based magnetic resonance imaging. Results show promising performance compared to state-of-the-art competing methods.},
  archive      = {J_TIP},
  author       = {Bihan Wen and Yanjun Li and Yoram Bresler},
  doi          = {10.1109/TIP.2020.2980753},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5310-5323},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image recovery via transform learning and low-rank modeling: The power of complementary regularizers},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Outdoor RGBD instance segmentation with residual regretting
learning. <em>TIP</em>, <em>29</em>, 5301–5309. (<a
href="https://doi.org/10.1109/TIP.2020.2975711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor semantic segmentation with RGBD input has received decent progress recently, but studies on instance-level objects in outdoor scenarios meet challenges due to the ambiguity in the acquired outdoor depth map. To tackle this problem, we proposed a residual regretting mechanism, incorporated into current flexible, general and solid instance segmentation framework Mask R-CNN in an end-to-end manner. Specifically, regretting cascade is designed to gradually refine and fully unearth useful information in depth maps, acting in a filtering and backup way. Additionally, embedded by a novel residual connection structure, the regretting module combines RGB and depth branches with pixel-level mask robustly. Extensive experiments on the challenging Cityscapes and KITTI dataset manifest the effectiveness of our residual regretting scheme for handling outdoor depth map. Our approach achieves state-of-the-art performance on RGBD instance segmentation, with 13.4\% relative improvement over Mask R-CNN on Cityscapes by depth cue.},
  archive      = {J_TIP},
  author       = {Zhengtian Xu and Shu Liu and Jianping Shi and Cewu Lu},
  doi          = {10.1109/TIP.2020.2975711},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5301-5309},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Outdoor RGBD instance segmentation with residual regretting learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Similarity-preserving linkage hashing for online image
retrieval. <em>TIP</em>, <em>29</em>, 5289–5300. (<a
href="https://doi.org/10.1109/TIP.2020.2981879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online image hashing aims to update hash functions on-the-fly along with newly arriving data streams, which has found broad applications in computer vision and beyond. To this end, most existing methods update hash functions simply using discrete labels or pairwise similarity to explore intra-class relationships, which, however, often deteriorates search performance when facing a domain gap or semantic shift. One reason is that they ignore the particular semantic relationships among different classes, which should be taken into account in updating hash functions. Besides, the common characteristics between the label vectors (can be regarded as a sort of binary codes) and to-be-learned binary hash codes have left unexploited. In this paper, we present a novel online hashing method, termed Similarity Preserving Linkage Hashing (SPLH), which not only utilizes pairwise similarity to learn the intra-class relationships, but also fully exploits a latent linkage space to capture the inter-class relationships and the common characteristics between label vectors and to-be-learned hash codes. Specifically, SPLH first maps the independent discrete label vectors and binary hash codes into a linkage space, through which the relative semantic distance between data points can be assessed precisely. As a result, the pairwise similarities within the newly arriving data stream are exploited to learn the latent semantic space to benefit binary code learning. To learn the model parameters effectively, we further propose an alternating optimization algorithm. Extensive experiments conducted on three widely-used datasets demonstrate the superior performance of SPLH over several state-of-the-art online hashing methods.},
  archive      = {J_TIP},
  author       = {Mingbao Lin and Rongrong Ji and Shen Chen and Xiaoshuai Sun and Chia-Wen Lin},
  doi          = {10.1109/TIP.2020.2981879},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5289-5300},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Similarity-preserving linkage hashing for online image retrieval},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Dynamic scene deblurring by depth guided model.
<em>TIP</em>, <em>29</em>, 5273–5288. (<a
href="https://doi.org/10.1109/TIP.2020.2980173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic scene blur is usually caused by object motion, depth variation as well as camera shake. Most existing methods usually solve this problem using image segmentation or fully end-to-end trainable deep convolutional neural networks by considering different object motions or camera shakes. However, these algorithms are less effective when there exist depth variations. In this work, we propose a deep neural convolutional network that exploits the depth map for dynamic scene deblurring. Given a blurred image, we first extract the depth map and adopt a depth refinement network to restore the edges and structure in the depth map. To effectively exploit the depth map, we adopt the spatial feature transform layer to extract depth features and fuse with the image features through scaling and shifting. Our image deblurring network thus learns to restore a clear image under the guidance of the depth map. With substantial experiments and analysis, we show that the depth information is crucial to the performance of the proposed model. Finally, extensive quantitative and qualitative evaluations demonstrate that the proposed model performs favorably against the state-of-the-art dynamic scene deblurring approaches as well as conventional depth-based deblurring algorithms.},
  archive      = {J_TIP},
  author       = {Lerenhan Li and Jinshan Pan and Wei-Sheng Lai and Changxin Gao and Nong Sang and Ming-Hsuan Yang},
  doi          = {10.1109/TIP.2020.2980173},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5273-5288},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic scene deblurring by depth guided model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaze estimation by exploring two-eye asymmetry.
<em>TIP</em>, <em>29</em>, 5259–5272. (<a
href="https://doi.org/10.1109/TIP.2020.2982828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye gaze estimation is increasingly demanded by recent intelligent systems to facilitate a range of interactive applications. Unfortunately, learning the highly complicated regression from a single eye image to the gaze direction is not trivial. Thus, the problem is yet to be solved efficiently. Inspired by the two-eye asymmetry as two eyes of the same person may appear uneven, we propose the face-based asymmetric regression-evaluation network (FARE-Net) to optimize the gaze estimation results by considering the difference between left and right eyes. The proposed method includes one face-based asymmetric regression network (FAR-Net) and one evaluation network (E-Net). The FAR-Net predicts 3D gaze directions for both eyes and is trained with the asymmetric mechanism, which asymmetrically weights and sums the loss generated by two-eye gaze directions. With the asymmetric mechanism, the FAR-Net utilizes the eyes that can achieve high performance to optimize network. The E-Net learns the reliabilities of two eyes to balance the learning of the asymmetric mechanism and symmetric mechanism. Our FARE-Net achieves leading performances on MPIIGaze, EyeDiap and RT-Gene datasets. Additionally, we investigate the effectiveness of FARE-Net by analyzing the distribution of errors and ablation study.},
  archive      = {J_TIP},
  author       = {Yihua Cheng and Xucong Zhang and Feng Lu and Yoichi Sato},
  doi          = {10.1109/TIP.2020.2982828},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5259-5272},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gaze estimation by exploring two-eye asymmetry},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attribute-guided attention for referring expression
generation and comprehension. <em>TIP</em>, <em>29</em>, 5244–5258. (<a
href="https://doi.org/10.1109/TIP.2020.2979010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring expression is a special kind of verbal expression. The goal of referring expression is to refer to a particular object in some scenarios. Referring expression generation and comprehension are two inverse tasks within the field. Considering the critical role that visual attributes play in distinguishing the referred object from other objects, we propose an attribute-guided attention model to address the two tasks. In our proposed framework, attributes collected from referring expressions are used as explicit supervision signals on the generation and comprehension modules. The online predicted attributes of the visual object can benefit both tasks in two aspects: First, attributes can be directly embedded into the generation and comprehension modules, distinguishing the referred object as additional visual representations. Second, since attributes have their correspondence in both visual and textual space, an attribute-guided attention module is proposed as a bridging part to link the counterparts in visual representation and textual expression. Attention weights learned on both visual feature and word embeddings validate our motivation. We experiment on three standard datasets of RefCOCO, RefCOCO+ and RefCOCOg commonly used in this field. Both quantitative and qualitative results demonstrate the effectiveness of our proposed framework. The experimental results show significant improvements over baseline methods, and are favorably comparable to the state-of-the-art results. Further ablation study and analysis clearly demonstrate the contribution of each module, which could provide useful inspirations to the community.},
  archive      = {J_TIP},
  author       = {Jingyu Liu and Wei Wang and Liang Wang and Ming-Hsuan Yang},
  doi          = {10.1109/TIP.2020.2979010},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5244-5258},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attribute-guided attention for referring expression generation and comprehension},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A weighted fidelity and regularization-based method for
mixed or unknown noise removal from images on graphs. <em>TIP</em>,
<em>29</em>, 5229–5243. (<a
href="https://doi.org/10.1109/TIP.2020.2969076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising technologies in a Euclidean domain have achieved good results and are becoming mature. However, in recent years, many real-world applications encountered in computer vision and geometric modeling involve image data defined in irregular domains modeled by huge graphs, which results in the problem on how to solve image denoising problems defined on graphs. In this paper, we propose a novel model for removing mixed or unknown noise in images on graphs. The objective is to minimize the sum of a weighted fidelity term and a sparse regularization term that additionally utilizes wavelet frame transform on graphs to retain feature details of images defined on graphs. Specifically, the weighted fidelity term with $\ell _{1}$ -norm and $\ell _{2}$ -norm is designed based on a analysis of the distribution of mixed noise. The augmented Lagrangian and accelerated proximal gradient methods are employed to achieve the optimal solution to the problem. Finally, some supporting numerical results and comparative analyses with other denoising algorithms are provided. It is noted that we investigate image denoising with unknown noise or a wide range of mixed noise, especially the mixture of Poisson, Gaussian, and impulse noise. Experimental results reported for synthetic and real images on graphs demonstrate that the proposed method is effective and efficient, and exhibits better performance for the removal of mixed or unknown noise in images on graphs than other denoising algorithms in the literature. The method can effectively remove mixed or unknown noise and retain feature details of images on graphs. It delivers a new avenue for denoising images in irregular domains.},
  archive      = {J_TIP},
  author       = {Cong Wang and Ziyue Yan and Witold Pedrycz and MengChu Zhou and Zhiwu Li},
  doi          = {10.1109/TIP.2020.2969076},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5229-5243},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A weighted fidelity and regularization-based method for mixed or unknown noise removal from images on graphs},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-weather image alignment via latent generative model
with intensity consistency. <em>TIP</em>, <em>29</em>, 5216–5228. (<a
href="https://doi.org/10.1109/TIP.2020.2980210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image alignment/registration/correspondence is a critical prerequisite for many vision-based tasks, and it has been widely studied in computer vision. However, aligning images from different domains, such as cross-weather/season road scenes, remains a challenging problem. Inspired by the success of classic intensity-constancy-based image alignment methods and the modern generative adversarial network (GAN) technology, we propose a cross-weather road scene alignment method called latent generative model with intensity constancy. From a novel perspective, the alignment problem is formulated as a constrained 2D flow optimization problem with latent encoding, which can be decoded into an intensity-constancy image on the latent image manifold. The manifold is parameterized by a pre-trained GAN, which is able to capture statistic characteristics from large datasets. Moreover, we employ the learned manifold to constrain the warped latent image identical to the target image, thereby producing a realistic warping effect. Experimental results on several cross-weather/season road scene datasets demonstrate that our approach can significantly outperform the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Huabing Zhou and Jiayi Ma and Chiu C. Tan and Yanduo Zhang and Haibin Ling},
  doi          = {10.1109/TIP.2020.2980210},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5216-5228},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-weather image alignment via latent generative model with intensity consistency},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive and robust edge detection method based on edge
proportion statistics. <em>TIP</em>, <em>29</em>, 5206–5215. (<a
href="https://doi.org/10.1109/TIP.2020.2980170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection is one of the most fundamental operations in the field of image analysis and computer vision as a critical preprocessing step for high-level tasks. It is difficult to give a generic threshold that works well on all images as the image contents are totally different. This paper presents an adaptive, robust and effective edge detector for real-time applications. According to the 2D entropy, the images can be clarified into three groups, each attached with a reference percentage value based on the edge proportion statistics. Compared with the attached points along the gradient direction, anchor points were extracted with high probability to be edge pixels. Taking the segment direction into account, these points were then jointed into different edge segments, each of which was a clean, contiguous, 1-pixel wide chain of pixels. Experimental results indicate that the proposed edge detector outperforms the traditional edge following methods in terms of detection accuracy. Besides, the detection results can be used as the input information for post-processing applications in real-time.},
  archive      = {J_TIP},
  author       = {Yang Liu and Zongwu Xie and Hong Liu},
  doi          = {10.1109/TIP.2020.2980170},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5206-5215},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An adaptive and robust edge detection method based on edge proportion statistics},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-target multi-camera tracking by tracklet-to-target
assignment. <em>TIP</em>, <em>29</em>, 5191–5205. (<a
href="https://doi.org/10.1109/TIP.2020.2980070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the Multi-Target Multi-Camera Tracking task (MTMCT), which aims at tracking multiple targets within a multi-camera network. As the trajectory of each target is inherently split into multiple sub-trajectories (namely local tracklets) in a multi-camera network, a major challenge of MTMCT is how to accurately match the local tracklets generated within each camera across different cameras and generate a complete global trajectory for each target, i.e., the cross-camera tracklet matching problem. We solve the cross-camera tracklet matching problem by TRACklet-to-Target Assignment (TRACTA), and propose the Restricted Non-negative Matrix Factorization (RNMF) algorithm to compute the optimal assignment solution that meets a set of constraints, which should be in force in practice. TRACTA can correct the tracking errors caused by occlusions and missed detections in local tracklets, and produce a complete global trajectory for each target across all the cameras. Moreover, we also develop an analytical way of estimating the total number of targets in the camera network, which plays an important role to compute the tracklet-to-target assignment. Experimental evaluations and ablation studies on four MTMCT benchmark datasets show the superiority of the proposed TRACTA method.},
  archive      = {J_TIP},
  author       = {Yuhang He and Xing Wei and Xiaopeng Hong and Weiwei Shi and Yihong Gong},
  doi          = {10.1109/TIP.2020.2980070},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5191-5205},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-target multi-camera tracking by tracklet-to-target assignment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Small object augmentation of urban scenes for real-time
semantic segmentation. <em>TIP</em>, <em>29</em>, 5175–5190. (<a
href="https://doi.org/10.1109/TIP.2020.2976856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a key step in scene understanding for autonomous driving. Although deep learning has significantly improved the segmentation accuracy, current high-quality models such as PSPNet and DeepLabV3 are inefficient given their complex architectures and reliance on multi-scale inputs. Thus, it is difficult to apply them to real-time or practical applications. On the other hand, existing real-time methods cannot yet produce satisfactory results on small objects such as traffic lights, which are imperative to safe autonomous driving. In this paper, we improve the performance of real-time semantic segmentation from two perspectives, methodology and data. Specifically, we propose a real-time segmentation model coined Narrow Deep Network (NDNet) and build a synthetic dataset by inserting additional small objects into the training images. The proposed method achieves 65.7\% mean intersection over union (mIoU) on the Cityscapes test set with only 8.4G floating-point operations (FLOPs) on $1024\times 2048$ inputs. Furthermore, by re-training the existing PSPNet and DeepLabV3 models on our synthetic dataset, we obtained an average 2\% mIoU improvement on small objects.},
  archive      = {J_TIP},
  author       = {Zhengeng Yang and Hongshan Yu and Mingtao Feng and Wei Sun and Xuefei Lin and Mingui Sun and Zhi-Hong Mao and Ajmal Mian},
  doi          = {10.1109/TIP.2020.2976856},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5175-5190},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Small object augmentation of urban scenes for real-time semantic segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pose-based view synthesis for vehicles: A perspective aware
method. <em>TIP</em>, <em>29</em>, 5163–5174. (<a
href="https://doi.org/10.1109/TIP.2020.2980130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the problem of novel view synthesis for vehicles. Some previous works solve the problem of novel view synthesis in a controlled 3D environment by exploiting additional 3D details ( i.e. , camera viewpoints and underlying 3D models). However, in real scenarios, the 3D details are difficult to obtain. In this case, we find that introducing vehicle pose to represent the views of vehicles is an alternative paradigm to solve the lack of 3D details. In novel view synthesis, preserving local details is one of the most challenging problems. To address this problem, we propose a perspective-aware generative model (PAGM). We are motivated by the prior that vehicles are made of quadrilateral planes. Preserving these rigid planes during image generation ensures that image details are kept. To this end, a classic image transformation method is leveraged, i.e. , perspective transformation. In our GAN-based system, the perspective transformation is applied to the encoder feature maps, and the resulting maps are regarded as new conditions for the decoder. This strategy preserves the quadrilateral planes all the way through the network, thus shuttling the texture details from the input image to the generated image. In the experiments, we show that PAGM can generate high-quality vehicle images with fine details. Quantitatively, our method is superior to several competing approaches employing either GAN or the perspective transformation. Code is available at: https://github.com/ilvkai/view-synthesis-for-vehicles},
  archive      = {J_TIP},
  author       = {Kai Lv and Hao Sheng and Zhang Xiong and Wei Li and Liang Zheng},
  doi          = {10.1109/TIP.2020.2980130},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5163-5174},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pose-based view synthesis for vehicles: A perspective aware method},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boosting structure consistency for multispectral and
multimodal image registration. <em>TIP</em>, <em>29</em>, 5147–5162. (<a
href="https://doi.org/10.1109/TIP.2020.2980972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral imaging plays a vital role in the area of computer vision and computational photography. As spectral band images can be misaligned due to imaging device movement or alternation, image registration is necessary to avoid spectral information distortion. The current registration measures specialized for multispectral data are typically robust yet complex, requiring excessive computation. The common measures such as sum of squared differences (SSD) and sum of absolute differences (SAD) are computationally efficient whereas they perform poorly on multispectral data. To cope with this challenge, we propose a structure consistency boosting (SCB) transform that aims at boosting the structural similarity of multispectral images. With SCB, the common measures can be employed for multispectral image registration. The SCB transform exploits the fact that inherent edge structures maintain relative saliency locally despite the nonlinear variation between band images. A statistical prior of the natural image, which is based on the gradient-intensity correlation, is explored to build a parametric form of SCB. Experimental results validate that the SCB transform outperforms current similarity enhancement algorithms, and performs better than the state-of-the-art multispectral registration measures. Thanks to the generality of the statistical prior, the SCB transform is also applicable to various multimodal data such as flash/no-flash images and medical images.},
  archive      = {J_TIP},
  author       = {Si-Yuan Cao and Hui-Liang Shen and Shu-Jie Chen and Chunguang Li},
  doi          = {10.1109/TIP.2020.2980972},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5147-5162},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boosting structure consistency for multispectral and multimodal image registration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Orthogonal directional transforms using discrete directional
laplacian eigen solutions for beyond HEVC intra coding. <em>TIP</em>,
<em>29</em>, 5136–5146. (<a
href="https://doi.org/10.1109/TIP.2020.2977863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce new transforms for efficient compression of image blocks with directional preferences. Each transform, which is an orthogonal basis for a specific direction, is constructed from an eigen-decomposition of a discrete directional Laplacian system matrix. The method is a natural extension of the DCT, expressing the Laplacian in Cartesian coordinates rotated to some predetermined angles. Symmetry properties of the transforms over square domains lead to efficient computation and compact storage of the directional transforms. A version of the directional transforms was implemented within the beyond HEVC software and demonstrated significant improvement for intra block coding.},
  archive      = {J_TIP},
  author       = {Itsik Dvir and Dror Irony and David Drezner and Ady Ecker and Amiram Allouche and Natan Peterfreund},
  doi          = {10.1109/TIP.2020.2977863},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5136-5146},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Orthogonal directional transforms using discrete directional laplacian eigen solutions for beyond HEVC intra coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NLH: A blind pixel-level non-local method for real-world
image denoising. <em>TIP</em>, <em>29</em>, 5121–5135. (<a
href="https://doi.org/10.1109/TIP.2020.2980116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-local self similarity (NSS) is a powerful prior of natural images for image denoising. Most of existing denoising methods employ similar patches, which is a patch-level NSS prior. In this paper, we take one step forward by introducing a pixel-level NSS prior, i.e., searching similar pixels across a non-local region. This is motivated by the fact that finding closely similar pixels is more feasible than similar patches in natural images, which can be used to enhance image denoising performance. With the introduced pixel-level NSS prior, we propose an accurate noise level estimation method, and then develop a blind image denoising method based on the lifting Haar transform and Wiener filtering techniques. Experiments on benchmark datasets demonstrate that, the proposed method achieves much better performance than previous non-deep methods, and is still competitive with existing state-of-the-art deep learning based methods on real-world image denoising. The code is publicly available at https://github.com/njusthyk1972/NLH .},
  archive      = {J_TIP},
  author       = {Yingkun Hou and Jun Xu and Mingxia Liu and Guanghai Liu and Li Liu and Fan Zhu and Ling Shao},
  doi          = {10.1109/TIP.2020.2980116},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5121-5135},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NLH: A blind pixel-level non-local method for real-world image denoising},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian high resolution range profile reconstruction of
high-speed moving target from under-sampled data. <em>TIP</em>,
<em>29</em>, 5110–5120. (<a
href="https://doi.org/10.1109/TIP.2020.2980149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtained by wide band radar system, high resolution range profile (HRRP) is the projection of scatterers of target to the radar line-of-sight (LOS). HRRP reconstruction is unavoidable for inverse synthetic aperture radar (ISAR) imaging, and of particular usage for target recognition, especially in cases that the ISAR image of target is not able to be achieved. For the high-speed moving target, however, its HRRP is stretched by the high order phase error. To obtain well-focused HRRP, the phase error induced by target velocity should be compensated, utilizing either measured or estimated target velocity. Noting in case of under-sampled data, the traditional velocity estimation and HRRP reconstruction algorithms become invalid, a novel HRRP reconstruction of high-speed target for under-sampled data is proposed. The Laplacian scale mixture (LSM) is used as the sparse prior of HRRP, and the variational Bayesian inference is utilized to derive its posterior, so as to reconstruct it with high resolution from the under-sampled data. Additionally, during the reconstruction of HRRP, the target velocity is estimated via joint constraint of entropy minimization and sparseness of HRRP to compensate the high order phase error brought by the target velocity to concentrate HRRP. Experimental results based on both simulated and measured data validate the effectiveness of the proposed Bayesian HRRP reconstruction algorithm.},
  archive      = {J_TIP},
  author       = {Shuanghui Zhang and Yongxiang Liu and Xiang Li and Guoan Bi},
  doi          = {10.1109/TIP.2020.2980149},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5110-5120},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bayesian high resolution range profile reconstruction of high-speed moving target from under-sampled data},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A benchmark for sparse coding: When group sparsity meets
rank minimization. <em>TIP</em>, <em>29</em>, 5094–5109. (<a
href="https://doi.org/10.1109/TIP.2020.2972109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse coding has achieved a great success in various image processing tasks. However, a benchmark to measure the sparsity of image patch/group is missing since sparse coding is essentially an NP-hard problem. This work attempts to fill the gap from the perspective of rank minimization. We firstly design an adaptive dictionary to bridge the gap between group-based sparse coding (GSC) and rank minimization. Then, we show that under the designed dictionary, GSC and the rank minimization problems are equivalent, and therefore the sparse coefficients of each patch group can be measured by estimating the singular values of each patch group. We thus earn a benchmark to measure the sparsity of each patch group because the singular values of the original image patch groups can be easily computed by the singular value decomposition (SVD). This benchmark can be used to evaluate performance of any kind of norm minimization methods in sparse coding through analyzing their corresponding rank minimization counterparts. Towards this end, we exploit four well-known rank minimization methods to study the sparsity of each patch group and the weighted Schatten l p -norm minimization (WSNM) is found to be the closest one to the real singular values of each patch group. Inspired by the aforementioned equivalence regime of rank minimization and GSC, WSNM can be translated into a non-convex weighted l p -norm minimization problem in GSC. By using the earned benchmark in sparse coding, the weighted l p -norm minimization is expected to obtain better performance than the three other norm minimization methods, i.e., l p -norm, l p -norm and weighted l p -norm. To verify the feasibility of the proposed benchmark, we compare the weighted l p -norm minimization against the three aforementioned norm minimization methods in sparse coding. Experimental results on image restoration applications, namely image inpainting and image compressive sensing recovery, demonstrate that the proposed scheme is feasible and outperforms many state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Zhiyuan Zha and Xin Yuan and Bihan Wen and Jiantao Zhou and Jiachao Zhang and Ce Zhu},
  doi          = {10.1109/TIP.2020.2972109},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5094-5109},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A benchmark for sparse coding: When group sparsity meets rank minimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Context-integrated and feature-refined network for
lightweight object parsing. <em>TIP</em>, <em>29</em>, 5079–5093. (<a
href="https://doi.org/10.1109/TIP.2020.2978583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation for lightweight object parsing is a very challenging task, because both accuracy and efficiency (e.g., execution speed, memory footprint or computational complexity) should all be taken into account. However, most previous works pay too much attention to one-sided perspective, either accuracy or speed, and ignore others, which poses a great limitation to actual demands of intelligent devices. To tackle this dilemma, we propose a novel lightweight architecture named Context-Integrated and Feature-Refined Network (CIFReNet). The core components of CIFReNet are the Long-skip Refinement Module (LRM) and the Multi-scale Context Integration Module (MCIM). The LRM is designed to ease the propagation of spatial information between low-level and high-level stages. Furthermore, channel attention mechanism is introduced into the process of long-skip learning to boost the quality of low-level feature refinement. Meanwhile, the MCIM consists of three cascaded Dense Semantic Pyramid (DSP) blocks with image-level features, which is presented to encode multiple context information and enlarge the field of view. Specifically, the proposed DSP block exploits a dense feature sampling strategy to enhance the information representations without significantly increasing the computation cost. Comprehensive experiments are conducted on three benchmark datasets for object parsing including Cityscapes, CamVid, and Helen. As indicated, the proposed method reaches a better trade-off between accuracy and efficiency compared with the other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Bin Jiang and Wenxuan Tu and Chao Yang and Junsong Yuan},
  doi          = {10.1109/TIP.2020.2978583},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5079-5093},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Context-integrated and feature-refined network for lightweight object parsing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Occlusion-aware region-based 3D pose tracking of objects
with temporally consistent polar-based local partitioning. <em>TIP</em>,
<em>29</em>, 5065–5078. (<a
href="https://doi.org/10.1109/TIP.2020.2973512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region-based methods have become the state-of-art solution for monocular 6-DOF object pose tracking in recent years. However, two main challenges still remain: the robustness to heterogeneous configurations (both foreground and background), and the robustness to partial occlusions. In this paper, we propose a novel region-based monocular 3D object pose tracking method to tackle these problems. Firstly, we design a new strategy to define local regions, which is simple yet efficient in constructing discriminative local color histograms. Contrary to previous methods which define multiple circular regions around the object contour, we propose to define multiple overlapped, fan-shaped regions according to polar coordinates. This local region partitioning strategy produces much less number of local regions that need to be maintained and updated, while still being temporally consistent. Secondly, we propose to detect occluded pixels using edge distance and color cues. The proposed occlusion detection strategy is seamlessly integrated into the region-based pose optimization pipeline via a pixel-wise weight function, which significantly alleviates the interferences caused by partial occlusions. We demonstrate the effectiveness of the proposed two new strategies with a careful ablation study. Furthermore, we compare the performance of our method with the most recent state-of-art region-based methods in a recently released large dataset, in which the proposed method achieves competitive results with a higher average tracking success rate. Evaluations on two real-world datasets also show that our method is capable of handling realistic tracking scenarios.},
  archive      = {J_TIP},
  author       = {Leisheng Zhong and Xiaolin Zhao and Yu Zhang and Shunli Zhang and Li Zhang},
  doi          = {10.1109/TIP.2020.2973512},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5065-5078},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Occlusion-aware region-based 3D pose tracking of objects with temporally consistent polar-based local partitioning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust facial landmark detection via heatmap-offset
regression. <em>TIP</em>, <em>29</em>, 5050–5064. (<a
href="https://doi.org/10.1109/TIP.2020.2976765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial landmark detection aims at localizing multiple keypoints for a given facial image, which usually suffers from variations caused by arbitrary pose, diverse facial expression and partial occlusion. In this paper, we develop a two-stage regression network for facial landmark detection on unconstrained conditions. Our model consists of a Structural Hourglass Network (SHN) for detecting the initial locations of all facial landmarks based on heatmap generation, and a Global Constraint Network (GCN) for further refining the detected locations based on offset estimation. Specifically, SHN introduces an improved Inception-ResNet unit as basic building block, which can effectively improve the receptive field and learn contextual feature representations. In the meanwhile, a novel loss function with adaptive weight is proposed to make the whole model focus on the hard landmarks precisely. GCN attempts to explore the spatial contextual relationship between facial landmarks and refine the initial locations of facial landmarks by optimizing the global constraint. Moreover, we develop a pre-processing network to generate features with different scales, which will be transmitted to SHN and GCN for effective feature representations. Different from existing models, the proposed method realizes the heatmap-offset framework, which combines the outputs of heatmaps generated by SHN and coordinates estimated by GCN, to obtain an accurate prediction. The extensive experimental results on several challenging datasets, including 300W, COFW, AFLW, and 300-VW confirm that our method achieve competitive performance compared with the state-of-the-art algorithms.},
  archive      = {J_TIP},
  author       = {Junfeng Zhang and Haifeng Hu and Shenming Feng},
  doi          = {10.1109/TIP.2020.2976765},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5050-5064},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust facial landmark detection via heatmap-offset regression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Image denoising via sequential ensemble learning.
<em>TIP</em>, <em>29</em>, 5038–5049. (<a
href="https://doi.org/10.1109/TIP.2020.2978645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is about removing measurement noise from input image for better signal-to-noise ratio. In recent years, there has been great progress on the development of data-driven approaches for image denoising, which introduce various techniques and paradigms from machine learning in the design of image denoisers. This paper aims at investigating the application of ensemble learning in image denoising, which combines a set of simple base denoisers to form a more effective image denoiser. Based on different types of image priors, two types of base denoisers in the form of transform-shrinkage are proposed for constructing the ensemble. Then, with an effective re-sampling scheme, several ensemble-learning-based image denoisers are constructed using different sequential combinations of multiple proposed base denoisers. The experiments showed that sequential ensemble learning can effectively boost the performance of image denoising.},
  archive      = {J_TIP},
  author       = {Xuhui Yang and Yong Xu and Yuhui Quan and Hui Ji},
  doi          = {10.1109/TIP.2020.2978645},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5038-5049},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image denoising via sequential ensemble learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STAR: A structure and texture aware retinex model.
<em>TIP</em>, <em>29</em>, 5022–5037. (<a
href="https://doi.org/10.1109/TIP.2020.2974060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinex theory is developed mainly to decompose an image into the illumination and reflectance components by analyzing local image derivatives. In this theory, larger derivatives are attributed to the changes in reflectance, while smaller derivatives are emerged in the smooth illumination. In this paper, we utilize exponentiated local derivatives (with an exponent $\gamma $ ) of an observed image to generate its structure map and texture map. The structure map is produced by been amplified with $\gamma &amp;gt;1$ , while the texture map is generated by been shrank with $\gamma &amp;lt; 1$ . To this end, we design exponential filters for the local derivatives, and present their capability on extracting accurate structure and texture maps, influenced by the choices of exponents $\gamma $ . The extracted structure and texture maps are employed to regularize the illumination and reflectance components in Retinex decomposition. A novel Structure and Texture Aware Retinex (STAR) model is further proposed for illumination and reflectance decomposition of a single image. We solve the STAR model by an alternating optimization algorithm. Each sub-problem is transformed into a vectorized least squares regression, with closed-form solutions. Comprehensive experiments on commonly tested datasets demonstrate that, the proposed STAR model produce better quantitative and qualitative performance than previous competing methods, on illumination and reflectance decomposition, low-light image enhancement, and color correction. The code is publicly available at https://github.com/csjunxu/STAR .},
  archive      = {J_TIP},
  author       = {Jun Xu and Yingkun Hou and Dongwei Ren and Li Liu and Fan Zhu and Mengyang Yu and Haoqian Wang and Ling Shao},
  doi          = {10.1109/TIP.2020.2974060},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5022-5037},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {STAR: A structure and texture aware retinex model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). RAPNet: Residual atrous pyramid network for
importance-aware street scene parsing. <em>TIP</em>, <em>29</em>,
5010–5021. (<a href="https://doi.org/10.1109/TIP.2020.2978339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Street Scene Parsing (SSP) is a fundamental and important step for autonomous driving and traffic scene understanding. Recently, Fully Convolutional Network (FCN) based methods have delivered expressive performances with the help of large-scale dense-labeling datasets. However, in urban traffic environments, not all the labels contribute equally for making the control decision. Certain labels such as pedestrian, car, bicyclist, road lane or sidewalk would be more important in comparison with labels for vegetation, sky or building. Based on this fact, in this paper we propose a novel deep learning framework, named Residual Atrous Pyramid Network (RAPNet), for importance-aware SSP. More specifically, to incorporate the importance of various object classes, we propose an Importance-Aware Feature Selection (IAFS) mechanism which automatically selects the important features for label predictions. The IAFS can operate in each convolutional block, and the semantic features with different importance are captured in different channels so that they are automatically assigned with corresponding weights. To enhance the labeling coherence, we also propose a Residual Atrous Spatial Pyramid (RASP) module to sequentially aggregate global-to-local context information in a residual refinement manner. Extensive experiments on two public benchmarks have shown that our approach achieves new state-of-the-art performances, and can consistently obtain more accurate results on the semantic classes with high importance levels.},
  archive      = {J_TIP},
  author       = {Pingping Zhang and Wei Liu and Yinjie Lei and Hongyu Wang and Huchuan Lu},
  doi          = {10.1109/TIP.2020.2978339},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5010-5021},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RAPNet: Residual atrous pyramid network for importance-aware street scene parsing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective matrix normalization for fine-grained visual
recognition. <em>TIP</em>, <em>29</em>, 4996–5009. (<a
href="https://doi.org/10.1109/TIP.2020.2977457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bilinear pooling achieves great success in fine-grained visual recognition (FGVC). Recent methods have shown that the matrix power normalization can stabilize the second-order information in bilinear features, but some problems, e.g., redundant information and over-fitting, remain to be resolved. In this paper, we propose an efficient Multi-Objective Matrix Normalization (MOMN) method that can simultaneously normalize a bilinear representation in terms of square-root, low-rank, and sparsity. These three regularizers can not only stabilize the second-order information, but also compact the bilinear features and promote model generalization. In MOMN, a core challenge is how to jointly optimize three non-smooth regularizers of different convex properties. To this end, MOMN first formulates them into an augmented Lagrange formula with approximated regularizer constraints. Then, auxiliary variables are introduced to relax different constraints, which allow each regularizer to be solved alternately. Finally, several updating strategies based on gradient descent are designed to obtain consistent convergence and efficient implementation. Consequently, MOMN is implemented with only matrix multiplication, which is well-compatible with GPU acceleration, and the normalized bilinear features are stabilized and discriminative. Experiments on five public benchmarks for FGVC demonstrate that the proposed MOMN is superior to existing normalization-based methods in terms of both accuracy and efficiency. The code is available: https://github.com/mboboGO/MOMN.},
  archive      = {J_TIP},
  author       = {Shaobo Min and Hantao Yao and Hongtao Xie and Zheng-Jun Zha and Yongdong Zhang},
  doi          = {10.1109/TIP.2020.2977457},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4996-5009},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-objective matrix normalization for fine-grained visual recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DDcGAN: A dual-discriminator conditional generative
adversarial network for multi-resolution image fusion. <em>TIP</em>,
<em>29</em>, 4980–4995. (<a
href="https://doi.org/10.1109/TIP.2020.2977573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we proposed a new end-to-end model, termed as dual-discriminator conditional generative adversarial network (DDcGAN), for fusing infrared and visible images of different resolutions. Our method establishes an adversarial game between a generator and two discriminators. The generator aims to generate a real-like fused image based on a specifically designed content loss to fool the two discriminators, while the two discriminators aim to distinguish the structure differences between the fused image and two source images, respectively, in addition to the content loss. Consequently, the fused image is forced to simultaneously keep the thermal radiation in the infrared image and the texture details in the visible image. Moreover, to fuse source images of different resolutions, e.g., a low-resolution infrared image and a high-resolution visible image, our DDcGAN constrains the downsampled fused image to have similar property with the infrared image. This can avoid causing thermal radiation information blurring or visible texture detail loss, which typically happens in traditional methods. In addition, we also apply our DDcGAN to fusing multi-modality medical images of different resolutions, e.g., a low-resolution positron emission tomography image and a high-resolution magnetic resonance image. The qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our DDcGAN over the state-of-the-art, in terms of both visual effect and quantitative metrics. Our code is publicly available at https://github.com/jiayi-ma/DDcGAN.},
  archive      = {J_TIP},
  author       = {Jiayi Ma and Han Xu and Junjun Jiang and Xiaoguang Mei and Xiao-Ping Zhang},
  doi          = {10.1109/TIP.2020.2977573},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4980-4995},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DDcGAN: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A blind multiscale spatial regularization framework for
kernel-based spectral unmixing. <em>TIP</em>, <em>29</em>, 4965–4979.
(<a href="https://doi.org/10.1109/TIP.2020.2978342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introducing spatial prior information in hyperspectral imaging (HSI) analysis has led to an overall improvement of the performance of many HSI methods applied for denoising, classification, and unmixing. Extending such methodologies to nonlinear settings is not always straightforward, specially for unmixing problems where the consideration of spatial relationships between neighboring pixels might comprise intricate interactions between their fractional abundances and nonlinear contributions. In this paper, we consider a multiscale regularization strategy for nonlinear spectral unmixing with kernels. The proposed methodology splits the unmixing problem into two sub-problems at two different spatial scales: a coarse scale containing low-dimensional structures, and the original fine scale. The coarse spatial domain is defined using superpixels that result from a multiscale transformation. Spectral unmixing is then formulated as the solution of quadratically constrained optimization problems, which are solved efficiently by exploring their strong duality and a reformulation of their dual cost functions in the form of root-finding problems. Furthermore, we employ a theory-based statistical framework to devise a consistent strategy to estimate all required parameters, including both the regularization parameters of the algorithm and the number of superpixels of the transformation, resulting in a truly blind (from the parameters setting perspective) unmixing method. Experimental results attest the superior performance of the proposed method when comparing with other, state-of-the-art, related strategies.},
  archive      = {J_TIP},
  author       = {Ricardo Augusto Borsoi and Tales Imbiriba and José Carlos Moreira Bermudez and Cédric Richard},
  doi          = {10.1109/TIP.2020.2978342},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4965-4979},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A blind multiscale spatial regularization framework for kernel-based spectral unmixing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color image demosaicing using progressive collaborative
representation. <em>TIP</em>, <em>29</em>, 4952–4964. (<a
href="https://doi.org/10.1109/TIP.2020.2975978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a progressive collaborative representation (PCR) framework is proposed that is able to incorporate any existing color image demosaicing method for further boosting its demosaicing performance. Our PCR consists of two phases: (i) offline training and (ii) online refinement. In phase (i), multiple training-and-refining stages will be performed. In each stage, a new dictionary will be established through the learning of a large number of feature-patch pairs, extracted from the demosaicked images of the current stage and their corresponding original full-color images. After training, a projection matrix will be generated and exploited to refine the current demosaicked image. The updated image with improved image quality will be used as the input for the next training-and-refining stage and performed the same processing likewise. At the end of phase (i), all the projection matrices generated as above-mentioned will be exploited in phase (ii) to conduct online demosaicked image refinement of the test image. Extensive simulations conducted on two commonly-used test datasets (i.e., IMAX and Kodak) for evaluating the demosaicing algorithms have clearly demonstrated that our proposed PCR framework is able to constantly boost the performance of any image demosaicing method we experimented, in terms of objective and subjective performance evaluations.},
  archive      = {J_TIP},
  author       = {Zhangkai Ni and Kai-Kuang Ma and Huanqiang Zeng and Baojiang Zhong},
  doi          = {10.1109/TIP.2020.2975978},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4952-4964},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color image demosaicing using progressive collaborative representation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SIF: Self-inspirited feature learning for person
re-identification. <em>TIP</em>, <em>29</em>, 4942–4951. (<a
href="https://doi.org/10.1109/TIP.2020.2975712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The re-identification (ReID) task has received increasing studies in recent years and its performance has gained significant improvement. The progress mainly comes from searching for new network structures to learn person representations. However, limited efforts have been made to explore the potential performance of existing ReID networks directly by better training scheme, which leaves a large space for ReID research. In this paper, we propose a Self-Inspirited Feature Learning (SIF) method to enhance the performance of given ReID networks from the viewpoint of optimization. We design a simple adversarial learning scheme to encourage a network to learn more discriminative person representation. In our method, an auxiliary branch is added into the network only in the training stage, while the structure of the original network stays unchanged during the testing stage. In summary, SIF has three aspects of advantages: 1) it is designed under general setting; 2) it is compatible with many existing feature learning networks on the ReID task; 3) it is easy to implement and has steady performance. We evaluate the performance of SIF on three public ReID datasets: Market1501, DuckMTMC-reID, and CUHK03(both labeled and detected). The results demonstrate significant improvement in performance brought by SIF. We also apply SIF to obtain state-of-the-art results on all the three datasets. Specifically, mAP / Rank-1 accuracy are: 87.6\%/95.2\% (without re-rank) on Market1501, 79.4\%/89.8\% on DuckMTMC-reID, 77.0\%/79.5\% on CUHK03 (labeled) and 73.9\%/76.6\% on CUHK03 (detected), respectively.},
  archive      = {J_TIP},
  author       = {Long Wei and Zhenyong Wei and Zhongming Jin and Zhengxu Yu and Jianqiang Huang and Deng Cai and Xiaofei He and Xian-Sheng Hua},
  doi          = {10.1109/TIP.2020.2975712},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4942-4951},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SIF: Self-inspirited feature learning for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding and predicting the memorability of outdoor
natural scenes. <em>TIP</em>, <em>29</em>, 4927–4941. (<a
href="https://doi.org/10.1109/TIP.2020.2975957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memorability measures how easily an image is to be memorized after glancing, which may contribute to designing magazine covers, tourism publicity materials, and so forth. Recent works have shed light on the visual features that make generic images, object images or face photographs memorable. However, these methods are not able to effectively predict the memorability of outdoor natural scene images. To overcome this shortcoming of previous works, in this paper, we provide an attempt to answer: “what exactly makes outdoor natural scenes memorable”. To this end, we first establish a large-scale outdoor natural scene image memorability (LNSIM) database, containing 2,632 outdoor natural scene images with their ground truth memorability scores and the multi-label scene category annotations. Then, similar to previous works, we mine our database to investigate how low-, middle- and high-level handcrafted features affect the memorability of outdoor natural scenes. In particular, we find that the high-level feature of scene category is rather correlated with outdoor natural scene memorability, and the deep features learnt by deep neural network (DNN) are also effective in predicting the memorability scores. Moreover, combining the deep features with the category feature can further boost the performance of memorability prediction. Therefore, we propose an end-to-end DNN based outdoor natural scene memorability (DeepNSM) predictor, which takes advantage of the learned category-related features. Then, the experimental results validate the effectiveness of our DeepNSM model, exceeding the state-of-the-art methods. Finally, we try to understand the reason of the good performance for our DeepNSM model, and also study the cases that our DeepNSM model succeeds or fails to accurately predict the memorability of outdoor natural scenes.},
  archive      = {J_TIP},
  author       = {Jiaxin Lu and Mai Xu and Ren Yang and Zulin Wang},
  doi          = {10.1109/TIP.2020.2975957},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4927-4941},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Understanding and predicting the memorability of outdoor natural scenes},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multiple-instance densely-connected ConvNet for aerial
scene classification. <em>TIP</em>, <em>29</em>, 4911–4926. (<a
href="https://doi.org/10.1109/TIP.2020.2975718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast with nature scenes, aerial scenes are often composed of many objects crowdedly distributed on the surface in bird’s view, the description of which usually demands more discriminative features as well as local semantics. However, when applied to scene classification, most of the existing convolution neural networks (ConvNets) tend to depict global semantics of images, and the loss of low- and mid-level features can hardly be avoided, especially when the model goes deeper. To tackle these challenges, in this paper, we propose a multiple-instance densely-connected ConvNet (MIDC-Net) for aerial scene classification. It regards aerial scene classification as a multiple-instance learning problem so that local semantics can be further investigated. Our classification model consists of an instance-level classifier, a multiple instance pooling and followed by a bag-level classification layer. In the instance-level classifier, we propose a simplified dense connection structure to effectively preserve features from different levels. The extracted convolution features are further converted into instance feature vectors. Then, we propose a trainable attention-based multiple instance pooling. It highlights the local semantics relevant to the scene label and outputs the bag-level probability directly. Finally, with our bag-level classification layer, this multiple instance learning framework is under the direct supervision of bag labels. Experiments on three widely-utilized aerial scene benchmarks demonstrate that our proposed method outperforms many state-of-the-art methods by a large margin with much fewer parameters.},
  archive      = {J_TIP},
  author       = {Qi Bi and Kun Qin and Zhili Li and Han Zhang and Kai Xu and Gui-Song Xia},
  doi          = {10.1109/TIP.2020.2975718},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4911-4926},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multiple-instance densely-connected ConvNet for aerial scene classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatially constrained probabilistic model for robust image
segmentation. <em>TIP</em>, <em>29</em>, 4898–4910. (<a
href="https://doi.org/10.1109/TIP.2020.2975717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, the hidden Markov random field (HMRF) represents the class label distribution of an image in probabilistic model based segmentation. The class label distributions provided by existing HMRF models consider either the number of neighboring pixels with similar class labels or the spatial distance of neighboring pixels with dissimilar class labels. Also, this spatial information is only considered for estimation of class labels of the image pixels, while its contribution in parameter estimation is completely ignored. This, in turn, deteriorates the parameter estimation, resulting in sub-optimal segmentation performance. Moreover, the existing models assign equal weightage to the spatial information for class label estimation of all pixels throughout the image, which, create significant misclassification for the pixels in boundary region of image classes. In this regard, the paper develops a new clique potential function and a new class label distribution, incorporating the information of image class parameters. Unlike existing HMRF model based segmentation techniques, the proposed framework introduces a new scaling parameter that adaptively measures the contribution of spatial information for class label estimation of image pixels. The importance of the proposed framework is depicted by modifying the HMRF based segmentation methods. The advantage of proposed class label distribution is also demonstrated irrespective of the underlying intensity distributions. The comparative performance of the proposed and existing class label distributions in HMRF model is demonstrated both qualitatively and quantitatively for brain MR image segmentation, HEp-2 cell delineation, natural image and object segmentation.},
  archive      = {J_TIP},
  author       = {Abhirup Banerjee and Pradipta Maji},
  doi          = {10.1109/TIP.2020.2975717},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4898-4910},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A spatially constrained probabilistic model for robust image segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind universal bayesian image denoising with gaussian noise
level learning. <em>TIP</em>, <em>29</em>, 4885–4897. (<a
href="https://doi.org/10.1109/TIP.2020.2976814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind and universal image denoising consists of using a unique model that denoises images with any level of noise. It is especially practical as noise levels do not need to be known when the model is developed or at test time. We propose a theoretically-grounded blind and universal deep learning image denoiser for additive Gaussian noise removal. Our network is based on an optimal denoising solution, which we call fusion denoising. It is derived theoretically with a Gaussian image prior assumption. Synthetic experiments show our network&#39;s generalization strength to unseen additive noise levels. We also adapt the fusion denoising network architecture for image denoising on real images. Our approach improves real-world grayscale additive image denoising PSNR results for training noise levels and further on noise levels not seen during training. It also improves state-of-the-art color image denoising performance on every single noise level, by an average of $0.1dB$ , whether trained on or not.},
  archive      = {J_TIP},
  author       = {Majed El Helou and Sabine Süsstrunk},
  doi          = {10.1109/TIP.2020.2976814},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4885-4897},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind universal bayesian image denoising with gaussian noise level learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ICNet: Information conversion network for RGB-d based
salient object detection. <em>TIP</em>, <em>29</em>, 4873–4884. (<a
href="https://doi.org/10.1109/TIP.2020.2976689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D based salient object detection (SOD) methods leverage the depth map as a valuable complementary information for better SOD performance. Previous methods mainly resort to exploit the correlation between RGB image and depth map in three fusion domains: input images, extracted features, and output results. However, these fusion strategies cannot fully capture the complex correlation between the RGB image and depth map. Besides, these methods do not fully explore the cross-modal complementarity and the cross-level continuity of information, and treat information from different sources without discrimination. In this paper, to address these problems, we propose a novel Information Conversion Network (ICNet) for RGB-D based SOD by employing the siamese structure with encoder-decoder architecture. To fuse high-level RGB and depth features in an interactive and adaptive way, we propose a novel Information Conversion Module (ICM), which contains concatenation operations and correlation layers. Furthermore, we design a Cross-modal Depth-weighted Combination (CDC) block to discriminate the cross-modal features from different sources and to enhance RGB features with depth features at each level. Extensive experiments on five commonly tested datasets demonstrate the superiority of our ICNet over 15 state-of-the-art RGB-D based SOD methods, and validate the effectiveness of the proposed ICM and CDC block.},
  archive      = {J_TIP},
  author       = {Gongyang Li and Zhi Liu and Haibin Ling},
  doi          = {10.1109/TIP.2020.2976689},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4873-4884},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ICNet: Information conversion network for RGB-D based salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PhaseNet 2.0: Phase unwrapping of noisy data based on deep
learning approach. <em>TIP</em>, <em>29</em>, 4862–4872. (<a
href="https://doi.org/10.1109/TIP.2020.2977213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase unwrapping is an ill-posed classical problem in many practical applications of significance such as 3D profiling through fringe projection, synthetic aperture radar and magnetic resonance imaging. Conventional phase unwrapping techniques estimate the phase either by integrating through the confined path (referred to as path-following methods) or by minimizing the energy function between the wrapped phase and the approximated true phase (referred to as minimum-norm approaches). However, these conventional methods have some critical challenges like error accumulation and high computational time and often fail under low SNR conditions. To address these problems, this paper proposes a novel deep learning framework for unwrapping the phase and is referred to as “PhaseNet 2.0”. The phase unwrapping problem is formulated as a dense classification problem and a fully convolutional DenseNet based neural network is trained to predict the wrap-count at each pixel from the wrapped phase maps. To train this network, we simulate arbitrary shapes and propose new loss function that integrates the residues by minimizing the difference of gradients and also uses L 1 loss to overcome class imbalance problem. The proposed method, unlike our previous approach PhaseNet, does not require post-processing, highly robust to noise, accurately unwraps the phase even at the severe noise level of -5 dB, and can unwrap the phase maps even at relatively high dynamic ranges. Simulation results from the proposed framework are compared with different classes of existing phase unwrapping methods for varying SNR values and discontinuity, and these evaluations demonstrate the advantages of the proposed framework. We also demonstrate the generality of the proposed method on 3D reconstruction of synthetic CAD models that have diverse structures and finer geometric variations. Finally, the proposed method is applied to real-data for 3D profiling of objects using fringe projection technique and digital holographic interferometry. The proposed framework achieves significant improvements over existing methods while being highly efficient with interactive frame-rates on modern GPUs.},
  archive      = {J_TIP},
  author       = {G. E. Spoorthi and Rama Krishna Sai Subrahmanyam Gorthi and Subrahmanyam Gorthi},
  doi          = {10.1109/TIP.2020.2977213},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4862-4872},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PhaseNet 2.0: Phase unwrapping of noisy data based on deep learning approach},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Temporal incoherence-free video retargeting using foreground
aware extrapolation. <em>TIP</em>, <em>29</em>, 4848–4861. (<a
href="https://doi.org/10.1109/TIP.2020.2977171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.},
  archive      = {J_TIP},
  author       = {Sung In Cho and Suk-Ju Kang},
  doi          = {10.1109/TIP.2020.2977171},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4848-4861},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Temporal incoherence-free video retargeting using foreground aware extrapolation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lower bound on transmission using non-linear bounding
function in single image dehazing. <em>TIP</em>, <em>29</em>, 4832–4847.
(<a href="https://doi.org/10.1109/TIP.2020.2975909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visibility of an image captured in poor weather (such as haze, fog, mist, smog) degrades due to scattering of light by atmospheric particles. Single image dehazing (SID) methods are used to restore visibility from a single hazy image. The SID is a challenging problem due to its ill-posed nature. Typically, the atmospheric scattering model (ATSM) is used to solve SID problem. The transmission and atmospheric light are two prime parameters of ATSM. The accuracy and effectiveness of SID depends on accurate value of transmission and atmospheric light. The proposed method translates transmission estimation problem into estimation of the difference between minimum color channel of hazy and haze-free image. The translated problem presents a lower bound on transmission and is used to minimize reconstruction error in dehazing. The lower bound depends upon the bounding function (BF) and a quality control parameter. A non-linear model is then proposed to estimate BF for accurate estimation of transmission. The proposed quality control parameter can be utilized to tune the effect of dehazing. The accuracy obtained by the proposed method for transmission is compared with state of the art dehazing methods. Visual comparison of dehazed images and objective evaluation further validates the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Suresh Chandra Raikwar and Shashikala Tapaswi},
  doi          = {10.1109/TIP.2020.2975909},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4832-4847},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Lower bound on transmission using non-linear bounding function in single image dehazing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DRPL: Deep regression pair learning for multi-focus image
fusion. <em>TIP</em>, <em>29</em>, 4816–4831. (<a
href="https://doi.org/10.1109/TIP.2020.2976190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel deep network is proposed for multi-focus image fusion, named Deep Regression Pair Learning (DRPL). In contrast to existing deep fusion methods which divide the input image into small patches and apply a classifier to judge whether the patch is in focus or not, DRPL directly converts the whole image into a binary mask without any patch operation, subsequently tackling the difficulty of the blur level estimation around the focused/defocused boundary. Simultaneously, a pair learning strategy, which takes a pair of complementary source images as inputs and generates two corresponding binary masks, is introduced into the model, greatly imposing the complementary constraint on each pair and making a large contribution to the performance improvement. Furthermore, as the edge or gradient does exist in the focus part while there is no similar property for the defocus part, we also embed a gradient loss to ensure the generated image to be all-in-focus. Then the structural similarity index (SSIM) is utilized to make a trade-off between the reference and fused images. Experimental results conducted on the synthetic and real-world datasets substantiate the effectiveness and superiority of DRPL compared with other state-of-the-art approaches. The source code can be found in https://github.com/sasky1/DPRL.},
  archive      = {J_TIP},
  author       = {Jinxing Li and Xiaobao Guo and Guangming Lu and Bob Zhang and Yong Xu and Feng Wu and David Zhang},
  doi          = {10.1109/TIP.2020.2976190},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4816-4831},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DRPL: Deep regression pair learning for multi-focus image fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Principal component adversarial example. <em>TIP</em>,
<em>29</em>, 4804–4815. (<a
href="https://doi.org/10.1109/TIP.2020.2975918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite having achieved excellent performance on various tasks, deep neural networks have been shown to be susceptible to adversarial examples, i.e., visual inputs crafted with structural imperceptible noise. To explain this phenomenon, previous works implicate the weak capability of the classification models and the difficulty of the classification tasks. These explanations appear to account for some of the empirical observations but lack deep insight into the intrinsic nature of adversarial examples, such as the generation method and transferability. Furthermore, previous works generate adversarial examples completely rely on a specific classifier (model). Consequently, the attack ability of adversarial examples is strongly dependent on the specific classifier. More importantly, adversarial examples cannot be generated without a trained classifier. In this paper, we raise a question: what is the real cause of the generation of adversarial examples? To answer this question, we propose a new concept, called the adversarial region, which explains the existence of adversarial examples as perturbations perpendicular to the tangent plane of the data manifold. This view yields a clear explanation of the transfer property across different models of adversarial examples. Moreover, with the notion of the adversarial region, we propose a novel target-free method to generate adversarial examples via principal component analysis. We verify our adversarial region hypothesis on a synthetic dataset and demonstrate through extensive experiments on real datasets that the adversarial examples generated by our method have competitive or even strong transferability compared with model-dependent adversarial example generating methods. Moreover, our experiment shows that the proposed method is more robust to defensive methods than previous methods.},
  archive      = {J_TIP},
  author       = {Yonggang Zhang and Xinmei Tian and Ya Li and Xinchao Wang and Dacheng Tao},
  doi          = {10.1109/TIP.2020.2975918},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4804-4815},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Principal component adversarial example},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep multi-modal explanation model for zero-shot learning.
<em>TIP</em>, <em>29</em>, 4788–4803. (<a
href="https://doi.org/10.1109/TIP.2020.2975980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) has attracted significant attention due to its capabilities of classifying new images from unseen classes. To perform the classification task for ZSL, learning visual and semantic embeddings has been the main research approach in existing literature. At the same time, generating complementary explanations to justify the classification decision has remained largely unexplored. In this paper, we propose to address a new and challenging task, namely explainable zero-shot learning (XZSL), which aims to generate visual and textual explanations to support the classification decision. To accomplish this task, we build a novel Deep Multi-modal Explanation (DME) model that incorporates a joint visual-attribute embedding module and a multi-channel explanation module in an end-to-end fashion. In contrast to existing ZSL approaches, our visual-attribute embedding is associated not only with the decision, but also with new visual and textual explanations. For visual explanations, we first capture several attribute activation maps (AAM) and then merge them into a class activation map (CAM) that visually infers which region of an image is relevant to the class. Textual explanations are generated from the multi-channel explanation module, jointly integrating three long short-term memory models (LSTMs) each of which is conditioned on a different feature representation. Additionally, we suggest that the DME model can retain explanatory consistency for similar instances and explanatory diversity for diverse instances. We conduct qualitative and quantitative experiments to assess the model for ZSL classification and explanation. Specifically, the ablation studies verify the effectiveness of the components in our model. Our results on three well-known datasets are competitive with prior approaches. More importantly, the joint training of our embedding and explanation modules demonstrates mutual performance improvements between ZSL classification and explanation. We shed more light on DME to analyze and diagnose its advantages and limitations.},
  archive      = {J_TIP},
  author       = {Yu Liu and Tinne Tuytelaars},
  doi          = {10.1109/TIP.2020.2975980},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4788-4803},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A deep multi-modal explanation model for zero-shot learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised robust mixture models in RKHS for
abnormality detection in medical images. <em>TIP</em>, <em>29</em>,
4772–4787. (<a href="https://doi.org/10.1109/TIP.2020.2975958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormality detection in medical images is a one-class classification problem for which existing methods typically involve variants of kernel principal component analysis or one-class support vector machines. However, existing methods rely on highly-curated training sets with full supervision, often using heuristics for model fitting or ignore the variances of the data within principal subspaces. In contrast, we propose novel methods that can work with imperfectly curated datasets using robust statistical learning, by extending the multivariate generalized-Gaussian distribution to a reproducing kernel Hilbert space (RKHS) and employing it within a mixture model. We propose a novel semi-supervised extension of our learning scheme, showing that a small amount of expert feedback through high-quality labeled data of the outlier class can boost performance. We propose expectation maximization for our semi-supervised robust mixture-model learning in RKHS, using solely the Gram matrix and without the explicit lifting map. Our methods incorporate optimal component means, principal directions, and variances for abnormality detection. Results on four large public datasets on retinopathy and cancer, compared against a variety of contemporary methods, show that our method gives benefits over the state of the art in one-class classification for abnormality detection.},
  archive      = {J_TIP},
  author       = {Nitin Kumar and Suyash P. Awate},
  doi          = {10.1109/TIP.2020.2975958},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4772-4787},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised robust mixture models in RKHS for abnormality detection in medical images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the harmony of the composite image by
spatial-separated attention module. <em>TIP</em>, <em>29</em>,
4759–4771. (<a href="https://doi.org/10.1109/TIP.2020.2975979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image composition is one of the most important applications in image processing. However, the inharmonious appearance between the spliced region and background degrade the quality of the image. Thus, we address the problem of Image Harmonization: Given a spliced image and the mask of the spliced region, we try to harmonize the “style” of the pasted region with the background (non-spliced region). Previous approaches have been focusing on learning directly by the neural network. In this work, we start from an empirical observation: the differences can only be found in the spliced region between the spliced image and the harmonized result while they share the same semantic information and the appearance in the non-spliced region. Thus, in order to learn the feature map in the masked region and the others individually, we propose a novel attention module named Spatial-Separated Attention Module (S 2 AM). Furthermore, we design a novel image harmonization framework by inserting the S 2 AM in the coarser low-level features of the Unet structure by two different ways. Besides image harmonization, we make a big step for harmonizing the composite image without the specific mask under previous observation. The experiments show that the proposed S 2 AM performs better than other state-of-the-art attention modules in our task. Moreover, we demonstrate the advantages of our model against other state-of-the-art image harmonization methods via criteria from multiple points of view.},
  archive      = {J_TIP},
  author       = {Xiaodong Cun and Chi-Man Pun},
  doi          = {10.1109/TIP.2020.2975979},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4759-4771},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving the harmony of the composite image by spatial-separated attention module},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A joint relationship aware neural network for single-image
3D human pose estimation. <em>TIP</em>, <em>29</em>, 4747–4758. (<a
href="https://doi.org/10.1109/TIP.2020.2972104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the task of 3D human pose estimation from a single RGB image, which is challenging without depth information. Recently many deep learning methods are proposed and achieve great improvements due to their strong representation learning. However, most existing methods ignore the relationship between joint features. In this paper, a joint relationship aware neural network is proposed to take both global and local joint relationship into consideration. First, a whole feature block representing all human body joints is extracted by a convolutional neural network. A Dual Attention Module (DAM) is applied on the whole feature block to generate attention weights. By exploiting the attention module, the global relationship between the whole joints is encoded. Second, the weighted whole feature block is divided into some individual joint features. To capture salient joint feature, the individual joint features are refined by individual DAMs. Finally, a joint angle prediction constraint is proposed to consider local joint relationship. Quantitative and qualitative experiments on 3D human pose estimation benchmarks demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Xiangtao Zheng and Xiumei Chen and Xiaoqiang Lu},
  doi          = {10.1109/TIP.2020.2972104},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4747-4758},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A joint relationship aware neural network for single-image 3D human pose estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). MDLatLRR: A novel decomposition method for infrared and
visible image fusion. <em>TIP</em>, <em>29</em>, 4733–4746. (<a
href="https://doi.org/10.1109/TIP.2020.2975984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image decomposition is crucial for many image processing tasks, as it allows to extract salient features from source images. A good image decomposition method could lead to a better performance, especially in image fusion tasks. We propose a multi-level image decomposition method based on latent low-rank representation(LatLRR), which is called MDLatLRR. This decomposition method is applicable to many image processing fields. In this paper, we focus on the image fusion task. We build a novel image fusion framework based on MDLatLRR which is used to decompose source images into detail parts(salient features) and base parts. A nuclear-norm based fusion strategy is used to fuse the detail parts and the base parts are fused by an averaging strategy. Compared with other state-of-the-art fusion methods, the proposed algorithm exhibits better fusion performance in both subjective and objective evaluation.},
  archive      = {J_TIP},
  author       = {Hui Li and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1109/TIP.2020.2975984},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4733-4746},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MDLatLRR: A novel decomposition method for infrared and visible image fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fusion of heterogeneous adversarial networks for single
image dehazing. <em>TIP</em>, <em>29</em>, 4721–4732. (<a
href="https://doi.org/10.1109/TIP.2020.2975986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel image dehazing method. Typical deep learning models for dehazing are trained on paired synthetic indoor dataset. Therefore, these models may be effective for indoor image dehazing but less so for outdoor images. We propose a heterogeneous Generative Adversarial Networks (GAN) based method composed of a cycle-consistent Generative Adversarial Networks (CycleGAN) for producing haze-clear images and a conditional Generative Adversarial Networks (cGAN) for preserving textural details. We introduce a novel loss function in the training of the fused network to minimize GAN generated artifacts, to recover fine details, and to preserve color components. These networks are fused via a convolutional neural network (CNN) to generate dehazed image. Extensive experiments demonstrate that the proposed method significantly outperforms the state-of-the-art methods on both synthetic and real-world hazy images.},
  archive      = {J_TIP},
  author       = {Jaihyun Park and David K. Han and Hanseok Ko},
  doi          = {10.1109/TIP.2020.2975986},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4721-4732},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fusion of heterogeneous adversarial networks for single image dehazing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral and multispectral image fusion using optimized
twin dictionaries. <em>TIP</em>, <em>29</em>, 4709–4720. (<a
href="https://doi.org/10.1109/TIP.2020.2968773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral or spatial dictionary has been widely used in fusing low-spatial-resolution hyperspectral (LH) images and high-spatial-resolution multispectral (HM) images. However, only using spectral dictionary is insufficient for preserving spatial information, and vice versa. To address this problem, a new LH and HM image fusion method termed OTD using optimized twin dictionaries is proposed in this paper. The fusion problem of OTD is formulated analytically in the framework of sparse representation, as an optimization of twin spectral-spatial dictionaries and their corresponding sparse coefficients. More specifically, the spectral dictionary representing the generalized spectrums and its spectral sparse coefficients are optimized by utilizing the observed LH and HM images in the spectral domain; and the spatial dictionary representing the spatial information and its spatial sparse coefficients are optimized by modeling the rest of high-frequency information in the spatial domain. In addition, without non-negative constraints, the alternating direction methods of multipliers (ADMM) are employed to implement the above optimization process. Comparison results with the related state-of-the-art fusion methods on various datasets demonstrate that our proposed OTD method achieves a better fusion performance in both spatial and spectral domains.},
  archive      = {J_TIP},
  author       = {Xiaolin Han and Jing Yu and Jing-Hao Xue and Weidong Sun},
  doi          = {10.1109/TIP.2020.2968773},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4709-4720},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral and multispectral image fusion using optimized twin dictionaries},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Residual learning for salient object detection.
<em>TIP</em>, <em>29</em>, 4696–4708. (<a
href="https://doi.org/10.1109/TIP.2020.2975919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep learning based salient object detection methods improve the performance by introducing multi-scale strategies into fully convolutional neural networks (FCNs). The final result is obtained by integrating all the predictions at each scale. However, the existing multi-scale based methods suffer from several problems: 1) it is difficult to directly learn discriminative features and filters to regress high-resolution saliency masks for each scale; 2) rescaling the multi-scale features could pull in many redundant and inaccurate values, and this weakens the representational ability of the network. In this paper, we propose a residual learning strategy and introduce to gradually refine the coarse prediction scale-by-scale. Concretely, instead of directly predicting the finest-resolution result at each scale, we learn to predict residuals to remedy the errors between coarse saliency map and scale-matching ground truth masks. We employ a Dilated Convolutional Pyramid Pooling (DCPP) module to generate the coarse prediction and guide the the residual learning process through several novel Attentional Residual Modules (ARMs). We name our network as Residual Refinement Network (R 2 Net). We demonstrate the effectiveness of the proposed method against other state-of-the-art algorithms on five released benchmark datasets. Our R 2 Net is a fully convolutional network which does not need any post-processing and achieves a real-time speed of 33 FPS when it is run on one GPU.},
  archive      = {J_TIP},
  author       = {Mengyang Feng and Huchuan Lu and Yizhou Yu},
  doi          = {10.1109/TIP.2020.2975919},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4696-4708},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Residual learning for salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The devil is in the channels: Mutual-channel loss for
fine-grained image classification. <em>TIP</em>, <em>29</em>, 4683–4695.
(<a href="https://doi.org/10.1109/TIP.2020.2973812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to solving fine-grained image categorization is finding discriminate and local regions that correspond to subtle visual traits. Great strides have been made, with complex networks designed specifically to learn part-level discriminate feature representations. In this paper, we show that it is possible to cultivate subtle details without the need for overly complicated network designs or training mechanisms - a single loss is all it takes. The main trick lies with how we delve into individual feature channels early on, as opposed to the convention of starting from a consolidated feature map. The proposed loss function, termed as mutual-channel loss (MC-Loss), consists of two channel-specific components: a discriminality component and a diversity component. The discriminality component forces all feature channels belonging to the same class to be discriminative, through a novel channel-wise attention mechanism. The diversity component additionally constraints channels so that they become mutually exclusive across the spatial dimension. The end result is therefore a set of feature channels, each of which reflects different locally discriminative regions for a specific class. The MC-Loss can be trained end-to-end, without the need for any bounding-box/part annotations, and yields highly discriminative regions during inference. Experimental results show our MC-Loss when implemented on top of common base networks can achieve state-of-the-art performance on all four fine-grained categorization datasets (CUB-Birds, FGVC-Aircraft, Flowers-102, and Stanford Cars). Ablative studies further demonstrate the superiority of the MC-Loss when compared with other recently proposed general-purpose losses for visual classification, on two different base networks. Codes are available at: https://github.com/dongliangchang/Mutual-Channel-Loss.},
  archive      = {J_TIP},
  author       = {Dongliang Chang and Yifeng Ding and Jiyang Xie and Ayan Kumar Bhunia and Xiaoxu Li and Zhanyu Ma and Ming Wu and Jun Guo and Yi-Zhe Song},
  doi          = {10.1109/TIP.2020.2973812},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4683-4695},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {The devil is in the channels: Mutual-channel loss for fine-grained image classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Person search by separated modeling and a mask-guided
two-stream CNN model. <em>TIP</em>, <em>29</em>, 4669–4682. (<a
href="https://doi.org/10.1109/TIP.2020.2973513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we tackle the problem of person search, which is a challenging task consisted of pedestrian detection and person re-identification (re-ID). Instead of sharing representations in a single joint model, we find that separating detector and re-ID feature extraction yields better performance. In order to extract more representative features for each identity, we segment out the foreground person from the original image patch. We propose a simple yet effective re-ID method, which models foreground person and original image patches individually, and obtains enriched representations from two separate CNN streams. We also propose a Confidence Weighted Stream Attention method which further re-adjusts the relative importance of the two streams by incorporating the detection confidence. Furthermore, we simplify the whole pipeline by incorporating semantic segmentation into the re-ID network, which is trained by bounding boxes as weakly-annotated masks and identification labels simultaneously. From the experiments on two standard person search benchmarks i.e. CUHK-SYSU and PRW, we achieve mAP of 83.3\% and 32.8\% respectively, surpassing the state of the art by a large margin. The extensive ablation study and model inspection further justifies our motivation.},
  archive      = {J_TIP},
  author       = {Di Chen and Shanshan Zhang and Wanli Ouyang and Jian Yang and Ying Tai},
  doi          = {10.1109/TIP.2020.2973513},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4669-4682},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Person search by separated modeling and a mask-guided two-stream CNN model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Soft-edge assisted network for single image
super-resolution. <em>TIP</em>, <em>29</em>, 4656–4668. (<a
href="https://doi.org/10.1109/TIP.2020.2973769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of single image super-resolution (SISR) is a highly ill-posed inverse problem since reconstructing the high-frequency details from a low-resolution image is challenging. Most previous CNN-based super-resolution (SR) methods tend to directly learn the mapping from the low-resolution image to the high-resolution image through some complex convolutional neural networks. However, the method of blindly increasing the depth of the network is not the best choice because the performance improvement of such methods is marginal but the computational cost is huge. A more efficient method is to integrate the image prior knowledge into the model to assist the image reconstruction. Indeed, the soft-edge has been widely applied in many computer vision tasks as the role of an important image feature. In this paper, we propose a Soft-edge assisted Network (SeaNet) to reconstruct the high-quality SR image with the help of image soft-edge. The proposed SeaNet consists of three sub-nets: a rough image reconstruction network (RIRN), a soft-edge reconstruction network (Edge-Net), and an image refinement network (IRN). The complete reconstruction process consists of two stages. In Stage-I, the rough SR feature maps and the SR soft-edge are reconstructed by the RIRN and Edge-Net, respectively. In Stage-II, the outputs of the previous stages are fused and then fed to the IRN for high-quality SR image reconstruction. Extensive experiments show that our SeaNet converges rapidly and achieves excellent performance under the assistance of image soft-edge. The code is available at https://gitlab.com/junchenglee/seanet-pytorch.},
  archive      = {J_TIP},
  author       = {Faming Fang and Juncheng Li and Tieyong Zeng},
  doi          = {10.1109/TIP.2020.2973769},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4656-4668},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Soft-edge assisted network for single image super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep collaborative multi-view hashing for large-scale image
search. <em>TIP</em>, <em>29</em>, 4643–4655. (<a
href="https://doi.org/10.1109/TIP.2020.2974065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing could significantly accelerate large-scale image search by transforming the high-dimensional features into binary Hamming space, where efficient similarity search can be achieved with very fast Hamming distance computation and extremely low storage cost. As an important branch of hashing methods, multi-view hashing takes advantages of multiple features from different views for binary hash learning. However, existing multi-view hashing methods are either based on shallow models which fail to fully capture the intrinsic correlations of heterogeneous views, or unsupervised deep models which suffer from insufficient semantics and cannot effectively exploit the complementarity of view features. In this paper, we propose a novel Deep Collaborative Multi-view Hashing (DCMVH) method to deeply fuse multi-view features and learn multi-view hash codes collaboratively under a deep architecture. DCMVH is a new deep multi-view hash learning framework. It mainly consists of 1) multiple view-specific networks to extract hidden representations of different views, and 2) a fusion network to learn multi-view fused hash code. DCMVH associates different layers with instance-wise and pair-wise semantic labels respectively. In this way, the discriminative capability of representation layers can be progressively enhanced and meanwhile the complementarity of different view features can be exploited effectively. Finally, we develop a fast discrete hash optimization method based on augmented Lagrangian multiplier to efficiently solve the binary hash codes. Experiments on public multi-view image search datasets demonstrate our approach achieves substantial performance improvement over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Lei Zhu and Xu Lu and Zhiyong Cheng and Jingjing Li and Huaxiang Zhang},
  doi          = {10.1109/TIP.2020.2974065},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4643-4655},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep collaborative multi-view hashing for large-scale image search},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Text co-detection in multi-view scene. <em>TIP</em>,
<em>29</em>, 4627–4642. (<a
href="https://doi.org/10.1109/TIP.2020.2973511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view scene analysis has been widely explored in computer vision, including numerous practical applications. The texts in multi-view scenes are often detected by following the existing text detection method in a single image, which however ignores the multi-view corresponding constraint. The multi-view correspondences may contain structure, location information and assist difficulties induced by factors like occlusion and perspective distortion, which are deficient in the single image scene. In this paper, we address the corresponding text detection task and propose a novel text co-detection method to identify the co-occurring texts among multi-view scene images with compositions of detection and correspondence under large environmental variations. In our text co-detection method, the visual and geometrical correspondences are designed to explore texts holding high pairwise representation similarity and guide the exploitation of texts with geometrical correspondences, simultaneously. To guarantee the pairwise consistency among multiple images, we additionally incorporate the cycle consistency constraint, which guarantees alignments of text correspondences in the image set. Finally, text correspondence is represented by a permutation matrix and solved via positive semidefinite and low-rank constraints. Moreover, we also collect a new text co-detection dataset consisting of multi-view image groups obtained from the same scene with different photographing conditions. The experiments show that our text co-detection obtains satisfactory performance and outperforms the related state-of-the-art text detection methods.},
  archive      = {J_TIP},
  author       = {Chuan Wang and Huazhu Fu and Liang Yang and Xiaochun Cao},
  doi          = {10.1109/TIP.2020.2973511},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4627-4642},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Text co-detection in multi-view scene},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image deblurring utilizing inertial sensors and a
short-long-short exposure strategy. <em>TIP</em>, <em>29</em>,
4614–4626. (<a href="https://doi.org/10.1109/TIP.2020.2973499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image blur caused by camera movement is common in long-exposure photography. A recent approach to address image blur is to record camera motion via inertial sensors in imaging equipment such as smartphones and single-lens reflex (SLR) cameras. However, because of device performance limitations, directly estimating a blur kernel from sensor data is infeasible. Previous works that have attempted to correct blurry image content via sensor data have also been susceptible to theoretical defects. Here, we propose a novel method of deblurring images that uses inertial sensors and a short-long-short (SLS) exposure strategy. Assisted short-exposure images captured before and after the formal long-exposure image are employed to correct the sensor data. A half-blind deconvolution algorithm is proposed to refine the estimated kernel. An extra smoothing filter is integrated into the framework to address the coarse initial kernel. Hence, we propose a fast solution for optimization that uses the iteratively reweighted least squares (IRLS) method in the frequency domain. We evaluate these methods via several blind deconvolutions. Quantitative indicators and the visual performance of the image deblurring results show that our method performs better than previous methods in terms of image quality restoration and computational time cost. This method will increase the feasibility of applying deblurring to imaging devices.},
  archive      = {J_TIP},
  author       = {Chenwei Yang and Huajun Feng and Zhihai Xu and Yueting Chen and Qi Li},
  doi          = {10.1109/TIP.2020.2973499},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4614-4626},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image deblurring utilizing inertial sensors and a short-long-short exposure strategy},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressive radar imaging of stationary indoor targets with
low-rank plus jointly sparse and total variation regularizations.
<em>TIP</em>, <em>29</em>, 4598–4613. (<a
href="https://doi.org/10.1109/TIP.2020.2973819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of wall clutter mitigation and image reconstruction for through-wall radar imaging (TWRI) of stationary targets by seeking a model that incorporates low-rank (LR), joint sparsity (JS), and total variation (TV) regularizers. The motivation of the proposed model is that LR regularizer captures the low-dimensional structure of wall clutter; JS guarantees a small fraction of target occupancy and the similarity of sparsity profile among channel images; TV regularizer promotes the spatial continuity of target regions and mitigates background noise. The task of wall clutter mitigation and target image reconstruction is formulated as an optimization problem comprising LR, JS, and TV regularization terms. To handle this problem efficiently, an iterative algorithm based on the forward-backward proximal gradient splitting technique is introduced, which captures wall clutter and yields target images simultaneously. Extensive experiments are conducted on real radar data under compressive sensing scenarios. The results show that the proposed model enhances target localization and clutter mitigation even when radar measurements are significantly reduced.},
  archive      = {J_TIP},
  author       = {Van Ha Tang and Abdesselam Bouzerdoum and Son Lam Phung},
  doi          = {10.1109/TIP.2020.2973819},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4598-4613},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Compressive radar imaging of stationary indoor targets with low-rank plus jointly sparse and total variation regularizations},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D skeletal gesture recognition via hidden states
exploration. <em>TIP</em>, <em>29</em>, 4583–4597. (<a
href="https://doi.org/10.1109/TIP.2020.2974061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal dynamics is an open issue for modeling human body gestures. A solution is resorting to the generative models, such as the hidden Markov model (HMM). Nevertheless, most of the work assumes fixed anchors for each hidden state, which make it hard to describe the explicit temporal structure of gestures. Based on the observation that a gesture is a time series with distinctly defined phases, we propose a new formulation to build temporal compositions of gestures by the low-rank matrix decomposition. The only assumption is that the gesture&#39;s “hold” phases with static poses are linearly correlated among each other. As such, a gesture sequence could be segmented into temporal states with semantically meaningful and discriminative concepts. Furthermore, different to traditional HMMs which tend to use specific distance metric for clustering and ignore the temporal contextual information when estimating the emission probability, we utilize the long short-term memory to learn probability distributions over states of HMM. The proposed method is validated on multiple challenging datasets. Experiments demonstrate that our approach can effectively work on a wide range of gestures, and achieve state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Xin Liu and Henglin Shi and Xiaopeng Hong and Haoyu Chen and Dacheng Tao and Guoying Zhao},
  doi          = {10.1109/TIP.2020.2974061},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4583-4597},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D skeletal gesture recognition via hidden states exploration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral variability aware blind hyperspectral image
unmixing based on convex geometry. <em>TIP</em>, <em>29</em>, 4568–4582.
(<a href="https://doi.org/10.1109/TIP.2020.2974062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image unmixing has proven to be a useful technique to interpret hyperspectral data, and is a prolific research topic in the community. Most of the approaches used to perform linear unmixing are based on convex geometry concepts, because of the strong geometrical structure of the linear mixing model. However, many algorithms based on convex geometry are still used in spite of the underlying model not considering the intra-class variability of the materials. A natural question is to wonder to what extent these concepts and tools (Intrinsic Dimensionality estimation, endmember extraction algorithms, pixel purity) are still relevant when spectral variability comes into play. In this paper, we first analyze their robustness in a case where the linear mixing model holds in each pixel, but the endmembers vary in each pixel according to a prescribed variability model. In the light of this analysis, we propose an integrated unmixing chain which tries to adress the shortcomings of the classical tools used in the linear case, based on our previously proposed extended linear mixing model. We show the interest of the proposed approach on simulated and real datasets.},
  archive      = {J_TIP},
  author       = {Lucas Drumetz and Jocelyn Chanussot and Christian Jutten and Wing-Kin Ma and Akira Iwasaki},
  doi          = {10.1109/TIP.2020.2974062},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4568-4582},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spectral variability aware blind hyperspectral image unmixing based on convex geometry},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Deep multiphase level set for scene parsing. <em>TIP</em>,
<em>29</em>, 4556–4567. (<a
href="https://doi.org/10.1109/TIP.2019.2957915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Fully Convolutional Network (FCN) seems to be the go-to architecture for image segmentation, including semantic scene parsing. However, it is difficult for a generic FCN to predict semantic labels around the object boundaries, thus FCN-based methods usually produce parsing results with inaccurate boundaries. Meanwhile, many works have demonstrate that level set based active contours are superior to the boundary estimation in sub-pixel accuracy. However, they are quite sensitive to initial settings. To address these limitations, in this paper we propose a novel Deep Multiphase Level Set (DMLS) method for semantic scene parsing, which efficiently incorporates multiphase level sets into deep neural networks. The proposed method consists of three modules, i.e., recurrent FCNs, adaptive multiphase level set, and deeply supervised learning. More specifically, recurrent FCNs learn multi-level representations of input images with different contexts. Adaptive multiphase level set drives the discriminative contour for each semantic class, which makes use of the advantages of both global and local information. In each time-step of the recurrent FCNs, deeply supervised learning is incorporated for model training. Extensive experiments on three public benchmarks have shown that our proposed method achieves new state-of-the-art performances. The source codes will be released at https://github.com/Pchank/DMLS-for-SSP.},
  archive      = {J_TIP},
  author       = {Pingping Zhang and Wei Liu and Yinjie Lei and Hongyu Wang and Huchuan Lu},
  doi          = {10.1109/TIP.2019.2957915},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4556-4567},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep multiphase level set for scene parsing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence measure guided single image de-raining.
<em>TIP</em>, <em>29</em>, 4544–4555. (<a
href="https://doi.org/10.1109/TIP.2020.2973802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image de-raining is an extremely challenging problem since the rainy images contain rain streaks which often vary in size, direction and density. This varying characteristic of rain streaks affect different parts of the image differently. Previous approaches have attempted to address this problem by leveraging some prior information to remove rain streaks from a single image. One of the major limitations of these approaches is that they do not consider the location information of rain drops in the image. We extend our previous work UMRL network, and propose Image Quality-based single image Deraining using Confidence measure (QuDeC), network addresses this issue by learning the quality or distortion level of each patch in the rainy image, and further processes this information to learn the rain content at different scales. In addition, we introduce a technique which guides the network to learn the network weights based on the confidence measure about the estimate of both quality at each location and residual rain streak information (residual map). Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Rajeev Yasarla and Vishal M. Patel},
  doi          = {10.1109/TIP.2020.2973802},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4544-4555},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Confidence measure guided single image de-raining},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Second-order spectral transform block for 3D shape
classification and retrieval. <em>TIP</em>, <em>29</em>, 4530–4543. (<a
href="https://doi.org/10.1109/TIP.2020.2967579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel network block, dubbed as second-order spectral transform block, for 3D shape retrieval and classification. This network block generalizes the second-order pooling to 3D surface by designing a learnable non-linear transform on the spectrum of the pooled descriptor. The proposed block consists of following two components. First, the second-order average (SO-Avr) and max-pooling (SO-Max) operations are designed on 3D surface to aggregate local descriptors, which are shown to be more discriminative than the popular average-pooling or max-pooling. Second, a learnable spectral transform parameterized by mixture of power function is proposed to perform non-linear feature mapping in the space of pooled descriptors, i.e., manifold of symmetric positive definite matrix for SO-Avr, and space of symmetric matrix for SO-Max. The proposed block can be plugged into existing network architectures to aggregate local shape descriptors for boosting their performance. We apply it to a shallow network for non-rigid 3D shape analysis and to existing networks for rigid shape analysis, where it improves the first-tier retrieval accuracy by 7.2\% on SHREC&#39;14 Real dataset and achieves state-of-the-art classification accuracy on ModelNet40. As an extension, we apply our block to 2D image classification, showing its superiority compared with traditional second-order pooling methods. We also provide theoretical and experimental analysis on stability of the proposed second-order spectral transform block.},
  archive      = {J_TIP},
  author       = {Ruixuan Yu and Jian Sun and Huibin Li},
  doi          = {10.1109/TIP.2020.2967579},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4530-4543},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Second-order spectral transform block for 3D shape classification and retrieval},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). One-pass multi-task networks with cross-task guided
attention for brain tumor segmentation. <em>TIP</em>, <em>29</em>,
4516–4529. (<a href="https://doi.org/10.1109/TIP.2020.2973510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance has emerged as one of the major challenges for medical image segmentation. The model cascade (MC) strategy, a popular scheme, significantly alleviates the class imbalance issue via running a set of individual deep models for coarse-to-fine segmentation. Despite its outstanding performance, however, this method leads to undesired system complexity and also ignores the correlation among the models. To handle these flaws in the MC approach, we propose in this paper a light-weight deep model, i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better than MC does, while requiring only one-pass computation for brain tumor segmentation. First, OM-Net integrates the separate segmentation tasks into one deep model, which consists of shared parameters to learn joint features, as well as task-specific parameters to learn discriminative features. Second, to more effectively optimize OM-Net, we take advantage of the correlation among tasks to design both an online training data transfer strategy and a curriculum learning-based training strategy. Third, we further propose sharing prediction results between tasks, which enables us to design a cross-task guided attention (CGA) module. By following the guidance of the prediction results provided by the previous task, CGA can adaptively recalibrate channel-wise feature responses based on the category-specific statistics. Finally, a simple yet effective post-processing method is introduced to refine the segmentation results of the proposed attention network. Extensive experiments are conducted to demonstrate the effectiveness of the proposed techniques. Most impressively, we achieve state-of-the-art performance on the BraTS 2015 testing set and BraTS 2017 online validation set. Using these proposed approaches, we also won joint third place in the BraTS 2018 challenge among 64 participating teams. The code is publicly available at https://github.com/chenhong-zhou/OM-Net.},
  archive      = {J_TIP},
  author       = {Chenhong Zhou and Changxing Ding and Xinchao Wang and Zhentai Lu and Dacheng Tao},
  doi          = {10.1109/TIP.2020.2973510},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4516-4529},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {One-pass multi-task networks with cross-task guided attention for brain tumor segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realizing a low-power head-mounted phase-only holographic
display by light-weight compression. <em>TIP</em>, <em>29</em>,
4505–4515. (<a href="https://doi.org/10.1109/TIP.2020.2972112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head-mounted holographic displays (HMHD) are projected to be the first commercial realization of holographic video display systems. HMHDs use liquid crystal on silicon (LCoS) spatial light modulators (SLM), which are best suited to display phase-only holograms (POH). The performance/watt requirement of a monochrome, 60 fps Full HD, 2-eye, POH HMHD system is about 10 TFLOPS/W, which is orders of magnitude higher than that is achievable by commercially available mobile processors. To mitigate this compute power constraint, display-ready POHs shall be generated on a nearby server and sent to the HMHD in compressed form over a wireless link. This paper discusses design of a feasible HMHD-based augmented reality system, focusing on compression requirements and per-pixel rate-distortion trade-off for transmission of display-ready POH from the server to HMHD. Since the decoder in the HMHD needs to operate on low power, only coding methods that have low-power decoder implementation are considered. Effects of 2D phase unwrapping and flat quantization on compression performance are also reported. We next propose a versatile PCM-POH codec with progressive quantization that can adapt to SLM-dynamic-range and available bitrate, and features per-pixel rate-distortion control to achieve acceptable POH quality at target rates of 60-200 Mbit/s that can be reliably achieved by current wireless technologies. Our results demonstrate feasibility of realizing a low-power, quality-ensured, multi-user, interactive HMHD augmented reality system with commercially available components using the proposed adaptive compression of display-ready POH with light-weight decoding.},
  archive      = {J_TIP},
  author       = {Burak Soner and Erdem Ulusoy and A. Murat Tekalp and Hakan Urey},
  doi          = {10.1109/TIP.2020.2972112},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4505-4515},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Realizing a low-power head-mounted phase-only holographic display by light-weight compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel saliency detection algorithm based on adversarial
learning model. <em>TIP</em>, <em>29</em>, 4489–4504. (<a
href="https://doi.org/10.1109/TIP.2020.2972692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional salient object detection models can be divided into several classes based on the low-level features of images and contrast between the pixels. This paper proposes an adversarial learning model (ALM) that includes the generative model and discriminative model. The ALM uses the original image as an input of the generative model to extract the high-level features and forms an initial salient map. Then, the discriminative model is utilized to compare differences in the features between the initial salient map and the ground truth, and the obtained differences are sent to the convolutional layers of the generative model to adjust the parameters for the generative model updating. Due to the serial-iterative adjustment, the salient map of the generative model becomes more similar to the ground truth. Lastly, the ALM forms the salient map fused with the super-pixels by enhancing the color and texture features, so the final salient map is obtained. The ALM is not limited to the color and texture features; on the contrary, it fuses multiple features and achieves good results in the salient target extraction. The experimental results show that ALM performs better than the other ten state-of-the-art models on three different datasets. Thus, the proposed ALM is widely applicable to the salient target extraction.},
  archive      = {J_TIP},
  author       = {Yingfeng Cai and Lei Dai and Hai Wang and Long Chen and Yicheng Li},
  doi          = {10.1109/TIP.2020.2972692},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4489-4504},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel saliency detection algorithm based on adversarial learning model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning a deep dual attention network for video
super-resolution. <em>TIP</em>, <em>29</em>, 4474–4488. (<a
href="https://doi.org/10.1109/TIP.2020.2972118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning based video super-resolution (SR) methods combine the convolutional neural networks (CNN) with motion compensation to estimate a high-resolution (HR) video from its low-resolution (LR) counterpart. However, most previous methods conduct downscaling motion estimation to handle large motions, which can lead to detrimental effects on the accuracy of motion estimation due to the reduction of spatial resolution. Besides, these methods usually treat different types of intermediate features equally, which lack flexibility to emphasize meaningful information for revealing the high-frequency details. In this paper, to solve above issues, we propose a deep dual attention network (DDAN), including a motion compensation network (MCNet) and a SR reconstruction network (ReconNet), to fully exploit the spatio-temporal informative features for accurate video SR. The MCNet progressively learns the optical flow representations to synthesize the motion information across adjacent frames in a pyramid fashion. To decrease the mis-registration errors caused by the optical flow based motion compensation, we extract the detail components of original LR neighboring frames as complementary information for accurate feature extraction. In the ReconNet, we implement dual attention mechanisms on a residual unit and form a residual attention unit to focus on the intermediate informative features for high-frequency details recovery. Extensive experimental results on numerous datasets demonstrate the proposed method can effectively achieve superior performance in terms of quantitative and qualitative assessments compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Feng Li and Huihui Bai and Yao Zhao},
  doi          = {10.1109/TIP.2020.2972118},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4474-4488},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning a deep dual attention network for video super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Multi-scale temporal cues learning for video person
re-identification. <em>TIP</em>, <em>29</em>, 4461–4473. (<a
href="https://doi.org/10.1109/TIP.2020.2972108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal cues embedded in videos provide important clues for person Re-Identification (ReID). To efficiently exploit temporal cues with a compact neural network, this work proposes a novel 3D convolution layer called Multi-scale 3D (M3D) convolution layer. The M3D layer is easy to implement and could be inserted into traditional 2D convolution networks to learn multi-scale temporal cues by end-to-end training. According to its inserted location, the M3D layer has two variants, i.e., local M3D layer and global M3D layer, respectively. The local M3D layer is inserted between 2D convolution layers to learn spatial-temporal cues among adjacent 2D feature maps. The global M3D layer is computed on adjacent frame feature vectors to learn their global temporal relations. The local and global M3D layers hence learn complementary temporal cues. Their combination introduces a fraction of parameters to traditional 2D CNN, but leads to the strong multi-scale temporal feature learning capability. The learned temporal feature is fused with a spatial feature to compose the final spatial-temporal representation for video person ReID. Evaluations on four widely used video person ReID datasets, i.e., MARS, DukeMTMC-VideoReID, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our method over the state-of-the art. For example, it achieves rank1 accuracy of 88.63\% on MARS without re-ranking. Our method also achieves a reasonable trade-off between ReID accuracy and model size, e.g., it saves about 40\% parameters of I3D CNN.},
  archive      = {J_TIP},
  author       = {Jianing Li and Shiliang Zhang and Tiejun Huang},
  doi          = {10.1109/TIP.2020.2972108},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4461-4473},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale temporal cues learning for video person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Geometry guided pose-invariant facial expression
recognition. <em>TIP</em>, <em>29</em>, 4445–4460. (<a
href="https://doi.org/10.1109/TIP.2020.2972114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by recent advances in human-centered computing, Facial Expression Recognition (FER) has attracted significant attention in many applications. However, most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifier for each pose. Different from existing methods, this paper proposes an end-to-end deep learning model that allows to simultaneous facial image synthesis and pose-invariant facial expression recognition by exploiting shape geometry of the face image. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, given an input face and a target pose and expression designated by a set of facial landmarks, an identity-preserving face can be generated through guiding by the target pose and expression. Second, the identity representation is explicitly disentangled from both expression and pose variations through the shape geometry delivered by facial landmarks. Third, our model can automatically generate face images with different expressions and poses in a continuous way to enlarge and enrich the training set for the FER task. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on both controlled and in-the-wild benchmark datasets including Multi-PIE, BU-3DFE, and SFEW. The code is included in the supplementary material.},
  archive      = {J_TIP},
  author       = {Feifei Zhang and Tianzhu Zhang and Qirong Mao and Changsheng Xu},
  doi          = {10.1109/TIP.2020.2972114},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4445-4460},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometry guided pose-invariant facial expression recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color matching images with unknown non-linear encodings.
<em>TIP</em>, <em>29</em>, 4435–4444. (<a
href="https://doi.org/10.1109/TIP.2020.2968766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a color matching method that deals with different non-linear encodings. In particular, given two different views of the same scene taken by two cameras with unknown settings and internal parameters, and encoded with unknown non-linear curves, our method is able to correct the colors of one of the images making it look as if it was captured under the other camera&#39;s settings. Our method is based on treating the in-camera color processing pipeline as a concatenation of a matrix multiplication on the linear image followed by a non-linearity. This allows us to model a color stabilization transformation among the two shots by estimating a single matrix -that will contain information from both of the original images- and an extra parameter that complies with the non-linearity. The method is fast and the results have no spurious colors. It outperforms the state-of-the-art both visually and according to several metrics, and can handle HDR encodings and very challenging real-life examples.},
  archive      = {J_TIP},
  author       = {Raquel Gil Rodríguez and Javier Vazquez-Corral and Marcelo Bertalmío},
  doi          = {10.1109/TIP.2020.2968766},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4435-4444},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color matching images with unknown non-linear encodings},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Light field saliency detection with deep convolutional
networks. <em>TIP</em>, <em>29</em>, 4421–4434. (<a
href="https://doi.org/10.1109/TIP.2020.2970529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field imaging presents an attractive alternative to RGB imaging because of the recording of the direction of the incoming light. The detection of salient regions in a light field image benefits from the additional modeling of angular patterns. For RGB imaging, methods using CNNs have achieved excellent results on a range of tasks, including saliency detection. However, it is not trivial to use CNN-based methods for saliency detection on light field images because these methods are not specifically designed for processing light field inputs. In addition, current light field datasets are not sufficiently large to train CNNs. To overcome these issues, we present a new Lytro Illum dataset, which contains 640 light fields and their corresponding ground-truth saliency maps. Compared to current publicly available light field saliency datasets [1], [2], our new dataset is larger, of higher quality, contains more variation and more types of light field inputs. This makes our dataset suitable for training deeper networks and benchmarking. Furthermore, we propose a novel end-to-end CNN-based framework for light field saliency detection. Specifically, we propose three novel MAC (Model Angular Changes) blocks to process light field micro-lens images. We systematically study the impact of different architecture variants and compare light field saliency with regular 2D saliency. Our extensive comparisons indicate that our novel network significantly outperforms state-of-the-art methods on the proposed dataset and has desired generalization abilities on other existing datasets.},
  archive      = {J_TIP},
  author       = {Jun Zhang and Yamei Liu and Shengping Zhang and Ronald Poppe and Meng Wang},
  doi          = {10.1109/TIP.2020.2970529},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4421-4434},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Light field saliency detection with deep convolutional networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Arc adjacency matrix-based fast ellipse detection.
<em>TIP</em>, <em>29</em>, 4406–4420. (<a
href="https://doi.org/10.1109/TIP.2020.2967601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast and accurate ellipse detection is critical in certain computer vision tasks. In this paper, we propose an arc adjacency matrix-based ellipse detection (AAMED) method to fulfill this requirement. At first, after segmenting the edges into elliptic arcs, the digraph-based arc adjacency matrix (AAM) is constructed to describe their triple sequential adjacency states. Curvature and region constraints are employed to make the AAM sparse. Secondly, through bidirectionally searching the AAM, we can get all arc combinations which are probably true ellipse candidates. The cumulative-factor (CF) based cumulative matrices (CM) are worked out simultaneously. CF is irrelative to the image context and can be pre-calculated. CM is related to the arcs or arc combinations and can be calculated by the addition or subtraction of CF. Then the ellipses are efficiently fitted from these candidates through twice eigendecomposition of CM using Jacobi method. Finally, a comprehensive validation score is proposed to eliminate false ellipses effectively. The score is mainly influenced by the constraints about adaptive shape, tangent similarity, distribution compensation. Experiments show that our method outperforms the 12 state-of-the-art methods on 9 datasets as a whole, with reference to recall, precision, F-measure, and time-consumption.},
  archive      = {J_TIP},
  author       = {Cai Meng and Zhaoxi Li and Xiangzhi Bai and Fugen Zhou},
  doi          = {10.1109/TIP.2020.2967601},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4406-4420},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Arc adjacency matrix-based fast ellipse detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust structural low-rank tracking. <em>TIP</em>,
<em>29</em>, 4390–4405. (<a
href="https://doi.org/10.1109/TIP.2020.2972102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking is an essential task for many computer vision applications. It becomes very challenging when the target appearance changes especially in the presence of occlusion, background clutter, and sudden illumination variations. Methods, that incorporate sparse representation and low-rank assumptions on the target particles have achieved promising results. However, because of the lack of structural constraints, these methods show performance degradation when facing the aforementioned challenges. To alleviate these limitations, we propose a new structural low-rank modeling algorithm for robust object tracking in complex scenarios. In the proposed algorithm, we consider spatial and temporal appearance consistency constraints, among the particles in the low-rank subspace, embedded in four different graphs. The resulting objective function encoding these constraints is novel and it is solved using linearized alternating direction method with adaptive penalty both in batch fashion as well as in online fashion. Our proposed objective function jointly learns the spatial and temporal structure of the target particles in consecutive frames and makes the proposed tracker consistent against many complex tracking scenarios. Results on four challenging datasets demonstrate excellent performance of the proposed algorithm as compared to current state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Sajid Javed and Arif Mahmood and Jorge Dias and Naoufel Werghi},
  doi          = {10.1109/TIP.2020.2972102},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4390-4405},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust structural low-rank tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An underwater image enhancement benchmark dataset and
beyond. <em>TIP</em>, <em>29</em>, 4376–4389. (<a
href="https://doi.org/10.1109/TIP.2019.2955241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world images. In this paper, we construct an Underwater Image Enhancement Benchmark (UIEB) including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory reference images as challenging data. Using this dataset, we conduct a comprehensive study of the state-of-the-art underwater image enhancement algorithms qualitatively and quantitatively. In addition, we propose an underwater image enhancement network (called Water-Net) trained on this benchmark as a baseline, which indicates the generalization of the proposed UIEB for training Convolutional Neural Networks (CNNs). The benchmark evaluations and the proposed Water-Net demonstrate the performance and limitations of state-of-the-art algorithms, which shed light on future research in underwater image enhancement. The dataset and code are available at https://li-chongyi.github.io/proj_benchmark.html.},
  archive      = {J_TIP},
  author       = {Chongyi Li and Chunle Guo and Wenqi Ren and Runmin Cong and Junhui Hou and Sam Kwong and Dacheng Tao},
  doi          = {10.1109/TIP.2019.2955241},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4376-4389},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An underwater image enhancement benchmark dataset and beyond},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatio-temporal multi-scale binary descriptor.
<em>TIP</em>, <em>29</em>, 4362–4375. (<a
href="https://doi.org/10.1109/TIP.2020.2965277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary descriptors are widely used for multi-view matching and robotic navigation. However, their matching performance decreases considerably under severe scale and viewpoint changes in non-planar scenes. To overcome this problem, we propose to encode the varying appearance of selected 3D scene points tracked by a moving camera with compact spatio-temporal descriptors. To this end, we first track interest points and capture their temporal variations at multiple scales. Then, we validate feature tracks through 3D reconstruction and compress the temporal sequence of descriptors by encoding the most frequent and stable binary values. Finally, we determine multi-scale correspondences across views with a matching strategy that handles severe scale differences. The proposed spatio-temporal multi-scale approach is generic and can be used with a variety of binary descriptors. We show the effectiveness of the joint multi-scale extraction and temporal reduction through comparisons of different temporal reduction strategies and the application to several binary descriptors.},
  archive      = {J_TIP},
  author       = {Alessio Xompero and Oswald Lanz and Andrea Cavallaro},
  doi          = {10.1109/TIP.2020.2965277},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4362-4375},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A spatio-temporal multi-scale binary descriptor},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous selective parts-based tracking. <em>TIP</em>,
<em>29</em>, 4349–4361. (<a
href="https://doi.org/10.1109/TIP.2020.2967580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking from videos is still a challenging task due to various changes throughout a video sequence including occlusions, motion blur, scale and other deformation changes. In this paper, we propose a selective parts-based approach, using correlation filters, that makes choices based on a consensus of the parts and global tracking. Moreover, we further enhance our parts-based approach by introducing a segmentation-assisted parts initialization. In addition, we present a genetic algorithm-based method to autonomously select various parameters of the tracking algorithm, as opposed to the common practice of manually tuning those parameters. In contrast to existing part-based methods, the proposed method does not dilute accurate tracking by averaging results over multiple parts at every frame. Instead, we take a selective approach based on the relative weight of the responses across parts. Moreover, we only make location corrections when a part diverges, and rely on these location corrections to maintain an accurate appearance model. In the case of occlusions, which are among the main reasons for using a parts-based approach, our proposed approach consistently achieves the best performance. It is due to the ability to handle occlusion and not dilute decisions with incorrect parts, that our proposed approach enables state-of-the-art performance. The proposed approach was evaluated on videos from three different challenging benchmark datasets. Our approach has resulted in better overall precision and success rates for three different base tracking approaches.},
  archive      = {J_TIP},
  author       = {Maria Cornacchia and Senem Velipasalar},
  doi          = {10.1109/TIP.2020.2967580},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4349-4361},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Autonomous selective parts-based tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving dataset volumes and model accuracy with
semi-supervised iterative self-learning. <em>TIP</em>, <em>29</em>,
4337–4348. (<a href="https://doi.org/10.1109/TIP.2019.2913986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within this paper, a novel semi-supervised learning technique is introduced based on a simple iterative learning cycle together with learned thresholding techniques and an ensemble decision support system. The state-of-the-art model performance and increased training data volume are demonstrated through the use of unlabeled data when training deeply learned classification models. The methods presented work independently from the model architectures or loss functions, making this approach applicable to a wide range of machine learning and classification tasks. Evaluation of the proposed approach is performed on commonly used datasets when evaluating semi-supervised learning techniques and a number of more challenging image classification datasets (CIFAR-100 and a 200 class subset of ImageNet).},
  archive      = {J_TIP},
  author       = {Robert Dupre and Jiri Fajtl and Vasileios Argyriou and Paolo Remagnino},
  doi          = {10.1109/TIP.2019.2913986},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4337-4348},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving dataset volumes and model accuracy with semi-supervised iterative self-learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Deep video super-resolution using HR optical flow
estimation. <em>TIP</em>, <em>29</em>, 4323–4336. (<a
href="https://doi.org/10.1109/TIP.2020.2967596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video super-resolution (SR) aims at generating a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The key challenge for video SR lies in the effective exploitation of temporal dependency between consecutive frames. Existing deep learning based methods commonly estimate optical flows between LR frames to provide temporal dependency. However, the resolution conflict between LR optical flows and HR outputs hinders the recovery of fine details. In this paper, we propose an end-to-end video SR network to super-resolve both optical flows and images. Optical flow SR from LR frames provides accurate temporal dependency and ultimately improves video SR performance. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed using HR optical flows to encode temporal dependency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Extensive experiments have been conducted to demonstrate the effectiveness of HR optical flows for SR performance improvement. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves the state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Longguang Wang and Yulan Guo and Li Liu and Zaiping Lin and Xinpu Deng and Wei An},
  doi          = {10.1109/TIP.2020.2967596},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4323-4336},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep video super-resolution using HR optical flow estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep HDR imaging via a non-local network. <em>TIP</em>,
<em>29</em>, 4308–4322. (<a
href="https://doi.org/10.1109/TIP.2020.2971346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most challenging problems in reconstructing a high dynamic range (HDR) image from multiple low dynamic range (LDR) inputs is the ghosting artifacts caused by the object motion across different inputs. When the object motion is slight, most existing methods can well suppress the ghosting artifacts through aligning LDR inputs based on optical flow or detecting anomalies among them. However, they often fail to produce satisfactory results in practice, since the real object motion can be very large. In this study, we present a novel deep framework, termed NHDRRnet, which adopts an alternative direction and attempts to remove ghosting artifacts by exploiting the non-local correlation in inputs. In NHDRRnet, we first adopt an Unet architecture to fuse all inputs and map the fusion results into a low-dimensional deep feature space. Then, we feed the resultant features into a novel global non-local module which reconstructs each pixel by weighted averaging all the other pixels using the weights determined by their correspondences. By doing this, the proposed NHDRRnet is able to adaptively select the useful information (e.g., which are not corrupted by large motions or adverse lighting conditions) in the whole deep feature space to accurately reconstruct each pixel. In addition, we also incorporate a triple-pass residual module to capture more powerful local features, which proves to be effective in further boosting the performance. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed NDHRnet in terms of suppressing the ghosting artifacts in HDR reconstruction, especially when the objects have large motions.},
  archive      = {J_TIP},
  author       = {Qingsen Yan and Lei Zhang and Yu Liu and Yu Zhu and Jinqiu Sun and Qinfeng Shi and Yanning Zhang},
  doi          = {10.1109/TIP.2020.2971346},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4308-4322},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep HDR imaging via a non-local network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved saliency detection in RGB-d images using two-phase
depth estimation and selective deep fusion. <em>TIP</em>, <em>29</em>,
4296–4307. (<a href="https://doi.org/10.1109/TIP.2020.2968250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the saliency detection problem in RGB-D images, the depth information plays a critical role in distinguishing salient objects or foregrounds from cluttered backgrounds. As the complementary component to color information, the depth quality directly dictates the subsequent saliency detection performance. However, due to artifacts and the limitation of depth acquisition devices, the quality of the obtained depth varies tremendously across different scenarios. Consequently, conventional selective fusion-based RGB-D saliency detection methods may result in a degraded detection performance in cases containing salient objects with low color contrast coupled with a low depth quality. To solve this problem, we make our initial attempt to estimate additional high-quality depth information, which is denoted by Depth + . Serving as a complement to the original depth, Depth + will be fed into our newly designed selective fusion network to boost the detection performance. To achieve this aim, we first retrieve a small group of images that are similar to the given input, and then the inter-image, nonlocal correspondences are built accordingly. Thus, by using these inter-image correspondences, the overall depth can be coarsely estimated by utilizing our newly designed depth-transferring strategy. Next, we build fine-grained, object-level correspondences coupled with a saliency prior to further improve the depth quality of the previous estimation. Compared to the original depth, our newly estimated Depth + is potentially more informative for detection improvement. Finally, we feed both the original depth and the newly estimated Depth + into our selective deep fusion network, whose key novelty is to achieve an optimal complementary balance to make better decisions toward improving saliency boundaries.},
  archive      = {J_TIP},
  author       = {Chenglizhao Chen and Jipeng Wei and Chong Peng and Weizhong Zhang and Hong Qin},
  doi          = {10.1109/TIP.2020.2968250},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4296-4307},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improved saliency detection in RGB-D images using two-phase depth estimation and selective deep fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A context-aware locality measure for inlier pool enrichment
in stepwise image registration. <em>TIP</em>, <em>29</em>, 4281–4295.
(<a href="https://doi.org/10.1109/TIP.2019.2961480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a feature-based image registration method, the stepwise image registration (SIR), with a closed-form solution. Our SIR creates an inlier pool and a candidate pool as the initialization, and then gradually enriches the inlier pool and refines the transformation. In each step, the enriched correspondence exclusively tunes the transformation coefficient within the confirmed inlier pairs, instead of updating the mapping using the complete putative set. In turn, the refined transformation prunes inconsistent mismatches to alleviate the incoming matching ambiguity. The context-aware locality measure (CALM) is designed for dissimilarity measure. The capability of the CALM can be enhanced by the progressive inlier pool enrichment. Finally, a retrieval process is performed based on the finest CALM and alignment, by which the inlier pool is maximized. Extensive experiments of enrichment evaluation, feature matching, image registration, and image retrieval demonstrate the favorable performance of our SIR against state-of-the-art methods. The code and datasets are available at https://github.com/sucv/SIR.},
  archive      = {J_TIP},
  author       = {Su Zhang and Wanjing Zhao and Xuying Hao and Yang Yang and Cuntai Guan},
  doi          = {10.1109/TIP.2019.2961480},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4281-4295},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A context-aware locality measure for inlier pool enrichment in stepwise image registration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shearlet transform-based light field compression under low
bitrates. <em>TIP</em>, <em>29</em>, 4269–4280. (<a
href="https://doi.org/10.1109/TIP.2020.2969087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) acquisition devices capture spatial and angular information of a scene. In contrast with traditional cameras, the additional angular information enables novel post-processing applications, such as 3D scene reconstruction, the ability to refocus at different depth planes, and synthetic aperture. In this paper, we present a novel compression scheme for LF data captured using multiple traditional cameras. The input LF views were divided into two groups: key views and decimated views. The key views were compressed using the multi-view extension of high-efficiency video coding (MV-HEVC) scheme, and decimated views were predicted using the shearlet-transform-based prediction (STBP) scheme. Additionally, the residual information of predicted views was also encoded and sent along with the coded stream of key views. The proposed scheme was evaluated over a benchmark multi-camera based LF datasets, demonstrating that incorporating the residual information into the compression scheme increased the overall peak signal to noise ratio (PSNR) by 2 dB. The proposed compression scheme performed significantly better at low bit rates compared to anchor schemes, which have a better level of compression efficiency in high bit-rate scenarios. The sensitivity of the human vision system towards compression artifacts, specifically at low bit rates, favors the proposed compression scheme over anchor schemes.},
  archive      = {J_TIP},
  author       = {Waqas Ahmad and Suren Vagharshakyan and Mårten Sjöström and Atanas Gotchev and Robert Bregovic and Roger Olsson},
  doi          = {10.1109/TIP.2020.2969087},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4269-4280},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Shearlet transform-based light field compression under low bitrates},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model optimization boosting framework for linear model hash
learning. <em>TIP</em>, <em>29</em>, 4254–4268. (<a
href="https://doi.org/10.1109/TIP.2020.2970577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high-dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.},
  archive      = {J_TIP},
  author       = {Xingbo Liu and Xiushan Nie and Quan Zhou and Liqiang Nie and Yilong Yin},
  doi          = {10.1109/TIP.2020.2970577},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4254-4268},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Model optimization boosting framework for linear model hash learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimized sensing matrix for single pixel multi-resolution
compressive spectral imaging. <em>TIP</em>, <em>29</em>, 4243–4253. (<a
href="https://doi.org/10.1109/TIP.2020.2971150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive spectral imaging (CSI) sensors allow the acquisition of spatial and spectral data using a set of coded projections. The single pixel camera (SPC) is a low-cost CSI architecture capable of sensing high-resolution spectral images, whose potential is diminished by its slow acquisition time due to the large number of required projections. To partially alleviate this issue, dual arm optical systems have been designed such that side information of the scene is captured to guide the reconstruction process. To fully exploit the capabilities of the dual system, the SPC sensing matrix, or equivalently the coding patterns; should be properly designed. Therefore, this work proposes an optimized sensing matrix design for the SPC based on a super-pixel map of the scene, obtained from the side information, such that the number of projections is drastically reduced while the reconstruction quality is improved. Indeed, theoretical analysis based on the restricted isometry property indicates that the error of the reconstruction vanishes when the SPC uses the designed sensing matrix. Simulation and experimental results show that the proposed sensing matrix design improves the reconstruction quality. Specifically, the proposed approach improves image quality in up to 15dB compared with the state of the art sensing matrix designs. Moreover, a fast multi-resolution reconstruction approach is proposed based on the designed matrix, which reduces computation time by two orders of magnitude and does not require an iterative process.},
  archive      = {J_TIP},
  author       = {Hans Garcia and Claudia V. Correa and Henry Arguello},
  doi          = {10.1109/TIP.2020.2971150},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4243-4253},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optimized sensing matrix for single pixel multi-resolution compressive spectral imaging},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast depth estimation for light field cameras. <em>TIP</em>,
<em>29</em>, 4232–4242. (<a
href="https://doi.org/10.1109/TIP.2020.2970814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast depth estimation for light field images is an important task for multiple applications such as image-based rendering and refocusing. Most previous approaches to light field depth estimation involve high computational costs. Therefore, in this study, we propose a fast depth estimation method based on multi-view stereo matching for light field images. Similar to other conventional methods, our method consists of initial depth estimation and refinement. For the initial estimation, we use a one-bit feature for each pixel and calculate matching costs by summing all combinations of viewpoints with a fast algorithm. To reduce computational time, we introduce an offline viewpoint selection strategy and cost volume interpolation. Our refinement process solves the minimization problem in which the objective function consists of ℓ 1 data and smoothness terms. Although this problem can be solved via a graph cuts algorithm, it is computationally expensive; therefore, we propose an approximate solver based on a fast-weighted median filter. Experiments on synthetic and real-world data show that our method achieves competitive accuracy with the shortest computational time of all methods.},
  archive      = {J_TIP},
  author       = {Kazu Mishiba},
  doi          = {10.1109/TIP.2020.2970814},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4232-4242},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast depth estimation for light field cameras},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic receptive field generation for full-reference image
quality assessment. <em>TIP</em>, <em>29</em>, 4219–4231. (<a
href="https://doi.org/10.1109/TIP.2020.2968283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most full-reference image quality assessment (FR-IQA) methods advanced to date have been holistically designed without regard to the type of distortion impairing the image. However, the perception of distortion depends nonlinearly on the distortion type. Here we propose a novel FR-IQA framework that dynamically generates receptive fields responsive to distortion type. Our proposed method- dynamic receptive field generation based image quality assessor (DRF-IQA)-separates the process of FR-IQA into two streams: 1) dynamic error representation and 2) visual sensitivity-based quality pooling. The first stream generates dynamic receptive fields on the input distorted image, implemented by a trained convolutional neural network (CNN), then the generated receptive field profiles are convolved with the distorted and reference images, and differenced to produce spatial error maps. In the second stream, a visual sensitivity map is generated. The visual sensitivity map is used to weight the spatial error map. The experimental results show that the proposed model achieves state-of-the-art prediction accuracy on various open IQA databases.},
  archive      = {J_TIP},
  author       = {Woojae Kim and Anh-Duc Nguyen and Sanghoon Lee and Alan Conrad Bovik},
  doi          = {10.1109/TIP.2020.2968283},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4219-4231},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic receptive field generation for full-reference image quality assessment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward specular removal from natural images based on
statistical reflection models. <em>TIP</em>, <em>29</em>, 4204–4218. (<a
href="https://doi.org/10.1109/TIP.2020.2967857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Removing specular reflections from images is critical for improving the performance of computer vision algorithms. Recently, state-of-the-art methods have demonstrated remarkably good performance at removing specular reflections from chromatic images. These methods are typically based on the chromatic pixels assumption; therefore, they are prone to failure in the achromatic regions. This paper presents a novel method that is applicable to natural images, because it is effective for both chromatic and achromatic regions. The proposed method is based on modeling the general properties of diffuse and specular reflections in a solid convex optimization framework. Considering the physical constraints, we determine the global optimal solution using the split Bregman method. Experimental results demonstrate the effectiveness of the proposed method, particularly for the achromatic regions, and its competence as a state-of-the-art method for removing specular reflections from the chromatic regions.},
  archive      = {J_TIP},
  author       = {Minjung Son and Yunjin Lee and Hyun Sung Chang},
  doi          = {10.1109/TIP.2020.2967857},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4204-4218},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward specular removal from natural images based on statistical reflection models},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High quality light field extraction and post-processing for
raw plenoptic data. <em>TIP</em>, <em>29</em>, 4188–4203. (<a
href="https://doi.org/10.1109/TIP.2020.2967600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field technology has reached a certain level of maturity in recent years, and its applications in both computer vision research and industry are offering new perspectives for cinematography and virtual reality. Several methods of capture exist, each with its own advantages and drawbacks. One of these methods involves the use of handheld plenoptic cameras. While these cameras offer freedom and ease of use, they also suffer from various visual artefacts and inconsistencies. We propose in this paper an advanced pipeline that enhances their output. After extracting sub-aperture images from the RAW images with our demultiplexing method, we perform three correction steps. We first remove hot pixel artefacts, then correct colour inconsistencies between views using a colour transfer method, and finally we apply a state of the art light field denoising technique to ensure a high image quality. An in-depth analysis is provided for every step of the pipeline, as well as their interaction within the system. We compare our approach to existing state of the art sub-aperture image extracting algorithms, using a number of metrics as well as a subjective experiment. Finally, we showcase the positive impact of our system on a number of relevant light field applications.},
  archive      = {J_TIP},
  author       = {Pierre Matysiak and Mairéead Grogan and Mikaël Le Pendu and Martin Alain and Emin Zerman and Aljosa Smolic},
  doi          = {10.1109/TIP.2020.2967600},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4188-4203},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High quality light field extraction and post-processing for raw plenoptic data},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Notice of violation of IEEE publication principles: Tone
mapping beyond the classical receptive field. <em>TIP</em>, <em>29</em>,
4174–4187. (<a href="https://doi.org/10.1109/TIP.2020.2970541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Notice of Violation of IEEE Publication Principles &quot;Tone Mapping Beyond the Classical Receptive Field,&quot; by Shao-Bing Gao, Min-Jie Tan, Zhen He, and Yong-Jie Li in the IEEE Transactions on Image Processing, vol. 29, February 2020, pp. 4174‐4187 After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE&#39;s Publication Principles. This paper contains significant portions of content from the paper cited below. The original content was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission. Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following paper &quot;Colour Constancy Beyond the Classical Receptive Field,&quot; by Arash Akbarinia, and C. Alejandro Parraga, in the IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 9,September 2018, pp. 2081‐2094 Some neurons in the primary visual cortex (V1) of human visual system (HVS) conduct dynamic center-surround computation, which is thought to contribute to compress the high dynamic range (HDR) scene and preserve the details. We simulate this dynamic receptive field (RF) property of V1 neurons to solve the so-called tone mapping (TM) task in this paper. The novelties of our method are as follows. (1) Cortical processing mechanisms of HVS are modeled to build a local TM operation based on two Gaussian functions whose kernels and weights adapt according to the center-surround contrast, thus reducing halo artifacts and effectively enhancing the local details of bright and dark parts of image. (2) Our method uses an adaptive filter that follows the contrast levels of the image, which is computationally very efficient. (3) The local fusion between the center and surround responses returned by a cortical processing flow and the global signals returned by a sub-cortical processing flow according to the local contrast forms a dynamic mechanism that selectively enhances the details. Extensive experiments show that the proposed method can efficiently render the HDR scenes with good contrast, clear details, and high structural fidelity. In addition, the proposed method can also obtain promising performance when applied to enhance the low-light images. Furthermore, by modeling these biological solutions, our technique is simple and robust considering that our results were obtained using the same parameters for all the datasets (e.g., HDR images or low-light images), that is, mimicking how HVS operates.},
  archive      = {J_TIP},
  author       = {Shao-Bing Gao and Min-Jie Tan and Zhen He and Yong-Jie Li},
  doi          = {10.1109/TIP.2020.2970541},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4174-4187},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Notice of violation of IEEE publication principles: Tone mapping beyond the classical receptive field},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Multi-view photometric stereo: A robust solution and
benchmark dataset for spatially varying isotropic materials.
<em>TIP</em>, <em>29</em>, 4159–4173. (<a
href="https://doi.org/10.1109/TIP.2020.2968818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo (MVPS) technique that works for general isotropic materials. Our algorithm is suitable for perspective cameras and nearby point light sources. Our data capture setup is simple, which consists of only a digital camera, some LED lights, and an optional automatic turntable. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distribution function (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. In experiments, we demonstrate our algorithm with two different setups: a studio setup for highest precision and a desktop setup for best usability. According to our experiments, under the studio setting, the captured shapes are accurate to 0.5 millimeters and the captured reflectance has a relative root-mean-square error (RMSE) of 9\%. We also quantitatively evaluate state-of-the-art MVPS on a newly collected benchmark dataset, which is publicly available for inspiring future research.},
  archive      = {J_TIP},
  author       = {Min Li and Zhenglong Zhou and Zhe Wu and Boxin Shi and Changyu Diao and Ping Tan},
  doi          = {10.1109/TIP.2020.2968818},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4159-4173},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view photometric stereo: A robust solution and benchmark dataset for spatially varying isotropic materials},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point cloud denoising via feature graph laplacian
regularization. <em>TIP</em>, <em>29</em>, 4143–4158. (<a
href="https://doi.org/10.1109/TIP.2020.2969052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is a collection of 3D coordinates that are discrete geometric samples of an object&#39;s 2D surfaces. Imperfection in the acquisition process means that point clouds are often corrupted with noise. Building on recent advances in graph signal processing, we design local algorithms for 3D point cloud denoising. Specifically, we design a signal-dependent feature graph Laplacian regularizer (SDFGLR) that assumes surface normals computed from point coordinates are piecewise smooth with respect to a signal-dependent graph Laplacian matrix. Using SDFGLR as a signal prior, we formulate an optimization problem with a general ℓ p -norm fidelity term that can explicitly remove only two types of additive noise: small but non-sparse noise like Gaussian (using ℓ 2 fidelity term) and large but sparser noise like Laplacian (using ℓ 1 fidelity term).To establish a linear relationship between normals and 3D point coordinates, we first perform bipartite graph approximation to divide the point cloud into two disjoint node sets (red and blue). We then optimize the red and blue nodes&#39; coordinates alternately. For ℓ 2 -norm fidelity term, we iteratively solve an unconstrained quadratic programming (QP) problem, efficiently computed using conjugate gradient with a bounded condition number to ensure numerical stability. For ℓ 1 -norm fidelity term, we iteratively minimize an ℓ 1 -ℓ 2 cost function using accelerated proximal gradient (APG), where a good step size is chosen via Lipschitz continuity analysis. Finally, we propose simple mean and median filters for flat patches of a given point cloud to estimate the noise variance given the noise type, which in turn is used to compute a weight parameter trading off the fidelity term and signal prior in the problem formulation. Extensive experiments show state-of-the-art denoising performance among local methods using our proposed algorithms.},
  archive      = {J_TIP},
  author       = {Chinthaka Dinesh and Gene Cheung and Ivan V. Bajić},
  doi          = {10.1109/TIP.2020.2969052},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4143-4158},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Point cloud denoising via feature graph laplacian regularization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial learning for joint optimization of depth and
ego-motion. <em>TIP</em>, <em>29</em>, 4130–4142. (<a
href="https://doi.org/10.1109/TIP.2020.2968751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, supervised deep learning methods have shown a great promise in dense depth estimation. However, massive high-quality training data are expensive and impractical to acquire. Alternatively, self-supervised learning-based depth estimators can learn the latent transformation from monocular or binocular video sequences by minimizing the photometric warp error between consecutive frames, but they suffer from the scale ambiguity problem or have difficulty in estimating precise pose changes between frames. In this paper, we propose a joint self-supervised deep learning pipeline for depth and ego-motion estimation by employing the advantages of adversarial learning and joint optimization with spatial-temporal geometrical constraints. The stereo reconstruction error provides the spatial geometric constraint to estimate the absolute scale depth. Meanwhile, the depth map with an absolute scale and a pre-trained pose network serves as a good starting point for direct visual odometry (DVO). DVO optimization based on spatial geometric constraints can result in a fine-grained ego-motion estimation with the additional backpropagation signals provided to the depth estimation network. Finally, the spatial and temporal domain-based reconstructed views are concatenated, and the iterative coupling optimization process is implemented in combination with the adversarial learning for accurate depth and precise ego-motion estimation. The experimental results show superior performance compared with state-of-the-art methods for monocular depth and ego-motion estimation on the KITTI dataset and a great generalization ability of the proposed approach.},
  archive      = {J_TIP},
  author       = {Anjie Wang and Zhijun Fang and Yongbin Gao and Songchao Tan and Shanshe Wang and Siwei Ma and Jenq-Neng Hwang},
  doi          = {10.1109/TIP.2020.2968751},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4130-4142},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial learning for joint optimization of depth and ego-motion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quasi fourier-mellin transform for affine invariant
features. <em>TIP</em>, <em>29</em>, 4114–4129. (<a
href="https://doi.org/10.1109/TIP.2020.2967578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fourier-Mellin transform (FMT) has been widely used for the extraction of rotation- and scale-invariant features. However, affine transform is a more reasonable approximation model for real viewpoint change. Due to shearing, the integral along the angular direction in the calculation of FMT cannot be used to extract the inherent features of an image undergoing affine transform. To eliminate the effect of shearing, whitening transform should be conducted on the integral along the radial direction. FMT can hardly be modified by conventional whitening-based methods with low computational cost due to additional processes. In this paper, two factors are constructed and embedded into FMT. Quasi Fourier-Mellin transform (QFMT) is proposed. The embedding of these factors is equivalent to whitening transform and can eliminate the effect of shearing in the affine transform. In particular, QFMT can also be calculated by integrating along the radial direction followed by integrating along the angular direction, as in FMT. Based on QFMT, the quasi Fourier-Mellin descriptor (QFMD) is constructed for the extraction of affine invariant features. Some experiments have also been conducted to test the performance of the proposed method.},
  archive      = {J_TIP},
  author       = {Jianwei Yang and Zhengda Lu and Yuan Yan Tang and Zhou Yuan and Yunjie Chen},
  doi          = {10.1109/TIP.2020.2967578},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4114-4129},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Quasi fourier-mellin transform for affine invariant features},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A shape-based statistical inversion method for EIT/URT
dual-modality imaging. <em>TIP</em>, <em>29</em>, 4099–4113. (<a
href="https://doi.org/10.1109/TIP.2020.2969077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A shape-based statistical inversion method is proposed for Electrical Impedance Tomography (EIT) and Ultrasound Reflection Tomography (URT) dual-modality imaging. It is promising to improve the imaging accuracy in inclusion detection problems. The proposed image reconstruction method is based on the statistical shape inversion framework. The likelihood function is derived from EIT and URT forward models. The prior distribution is constructed using the Markov random field (MRF) prior. The measurement uncertainty is modeled by conditional error model method. The statistical shape inversion problem is solved by the Maximum a posterior (MAP) method with conventional error model. A set of numerical and experimental tests are carried out to evaluate the performance of the proposed method. The results show that the proposed EIT/URT dual-modality imaging method has obvious improvement in imaging accuracy compared to the traditional single-modality EIT and URT methods.},
  archive      = {J_TIP},
  author       = {Guanghui Liang and Shangjie Ren and Feng Dong},
  doi          = {10.1109/TIP.2020.2969077},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4099-4113},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A shape-based statistical inversion method for EIT/URT dual-modality imaging},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified framework for generalizable style transfer: Style
and content separation. <em>TIP</em>, <em>29</em>, 4085–4098. (<a
href="https://doi.org/10.1109/TIP.2020.2969081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image style transfer has drawn broad attention recently. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is often not generalizable to new styles. Based on the idea of style and content separation, we here propose a unified style transfer framework that consists of style encoder, content encoder, mixer and decoder. The style encoder and the content encoder are used to extract the style and content representations from the corresponding reference images. The two representations are integrated by the mixer and fed to the decoder, which generates images with the target style and content. Assuming the same encoder could be shared among different styles/contents, the style/content encoder explores a generalizable way to represent style/content information, i.e. the encoders are expected to capture the underlying representation for different styles/contents and generalize to new styles/contents. Training simultaneously with a number of styles and contents, the framework enables building one single transfer network for multiple styles and further leads to a key merit of the framework, i.e. its generalizability to new styles and contents. To evaluate the proposed framework, we apply it to both supervised and unsupervised style transfer, using character typeface transfer and neural style transfer as respective examples. For character typeface transfer, to separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. For neural style transfer, we leverage the statistical information of feature maps in certain layers to represent style. Extensive experimental results have demonstrated the effectiveness and robustness of the proposed methods. Furthermore, models learned under the proposed framework are shown to be better generalizable to new styles and contents.},
  archive      = {J_TIP},
  author       = {Yexun Zhang and Ya Zhang and Wenbin Cai},
  doi          = {10.1109/TIP.2020.2969081},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4085-4098},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A unified framework for generalizable style transfer: Style and content separation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor oriented no-reference light field image quality
assessment. <em>TIP</em>, <em>29</em>, 4070–4084. (<a
href="https://doi.org/10.1109/TIP.2020.2969777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field image (LFI) quality assessment is becoming more and more important, which helps to better guide the acquisition, processing and application of immersive media. However, due to the inherent high dimensional characteristics of LFI, the LFI quality assessment turns into a multi-dimensional problem that requires consideration of the quality degradation in both spatial and angular dimensions. Therefore, we propose a novel Tensor oriented No-reference Light Field image Quality evaluator (Tensor-NLFQ) based on tensor theory. Specifically, since the LFI is regarded as a low-rank 4D tensor, the principal components of four oriented sub-aperture view stacks are obtained via Tucker decomposition. Then, the Principal Component Spatial Characteristic (PCSC) is designed to measure the spatial-dimensional quality of LFI considering its global naturalness and local frequency properties. Finally, the Tensor Angular Variation Index (TAVI) is proposed to measure angular consistency quality by analyzing the structural similarity distribution between the first principal component and each view in the view stack. Extensive experimental results on four publicly available LFI quality databases demonstrate that the proposed Tensor-NLFQ model outperforms state-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.},
  archive      = {J_TIP},
  author       = {Wei Zhou and Likun Shi and Zhibo Chen and Jinglin Zhang},
  doi          = {10.1109/TIP.2020.2969777},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4070-4084},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Tensor oriented no-reference light field image quality assessment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Region attention networks for pose and occlusion robust
facial expression recognition. <em>TIP</em>, <em>29</em>, 4057–4069. (<a
href="https://doi.org/10.1109/TIP.2019.2956143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion and pose variations, which can change facial appearance significantly, are two major obstacles for automatic Facial Expression Recognition (FER). Though automatic FER has made substantial progresses in the past few decades, occlusion-robust and pose-invariant issues of FER have received relatively less attention, especially in real-world scenarios. This paper addresses the real-world pose and occlusion robust FER problem in the following aspects. First, to stimulate the research of FER under real-world occlusions and variant poses, we annotate several in-the-wild FER datasets with pose and occlusion attributes for the community. Second, we propose a novel Region Attention Network (RAN), to adaptively capture the importance of facial regions for occlusion and pose variant FER. The RAN aggregates and embeds varied number of region features produced by a backbone convolutional neural network into a compact fixed-length representation. Last, inspired by the fact that facial expressions are mainly defined by facial action units, we propose a region biased loss to encourage high attention weights for the most important regions. We validate our RAN and region biased loss on both our built test datasets and four popular datasets: FERPlus, AffectNet, RAF-DB, and SFEW. Extensive experiments show that our RAN and region biased loss largely improve the performance of FER with occlusion and variant pose. Our method also achieves state-of-the-art results on FERPlus, AffectNet, RAF-DB, and SFEW. Code and the collected test data will be publicly available.},
  archive      = {J_TIP},
  author       = {Kai Wang and Xiaojiang Peng and Jianfei Yang and Debin Meng and Yu Qiao},
  doi          = {10.1109/TIP.2019.2956143},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4057-4069},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Region attention networks for pose and occlusion robust facial expression recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). KonIQ-10k: An ecologically valid database for deep learning
of blind image quality assessment. <em>TIP</em>, <em>29</em>, 4041–4056.
(<a href="https://doi.org/10.1109/TIP.2020.2967829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods for image quality assessment (IQA) are limited due to the small size of existing datasets. Extensive datasets require substantial resources both for generating publishable content and annotating it accurately. We present a systematic and scalable approach to creating KonIQ-10k, the largest IQA dataset to date, consisting of 10,073 quality scored images. It is the first in-the-wild database aiming for ecological validity, concerning the authenticity of distortions, the diversity of content, and quality-related indicators. Through the use of crowdsourcing, we obtained 1.2 million reliable quality ratings from 1,459 crowd workers, paving the way for more general IQA models. We propose a novel, deep learning model (KonCept512), to show an excellent generalization beyond the test set (0.921 SROCC), to the current state-of-the-art database LIVE-in-the-Wild (0.825 SROCC). The model derives its core performance from the InceptionResNet architecture, being trained at a higher resolution than previous models (512 × 384 ). Correlation analysis shows that KonCept512 performs similar to having 9 subjective scores for each test image.},
  archive      = {J_TIP},
  author       = {Vlad Hosu and Hanhe Lin and Tamas Sziranyi and Dietmar Saupe},
  doi          = {10.1109/TIP.2020.2967829},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4041-4056},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learned image downscaling for upscaling using content
adaptive resampler. <em>TIP</em>, <em>29</em>, 4027–4040. (<a
href="https://doi.org/10.1109/TIP.2020.2970248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural network based image super-resolution (SR) models have shown superior performance in recovering the underlying high resolution (HR) images from low resolution (LR) images obtained from the predefined downscaling methods. In this paper, we propose a learned image downscaling method based on content adaptive resampler (CAR) with consideration on the upscaling process. The proposed resampler network generates content adaptive image resampling kernels that are applied to the original HR input to generate pixels on the downscaled image. Moreover, a differentiable upscaling (SR) module is employed to upscale the LR result into its underlying HR counterpart. By back-propagating the reconstruction error down to the original HR input across the entire framework to adjust model parameters, the proposed framework achieves a new state-of-the-art SR performance through upscaling guided image resamplers which adaptively preserve detailed information that is essential to the upscaling. Experimental results indicate that the quality of the generated LR image is comparable to that of the traditional interpolation based method and the significant SR performance gain is achieved by deep SR models trained jointly with the CAR model. The code is publicly available on: https://github.com/sunwj/CAR.},
  archive      = {J_TIP},
  author       = {Wanjie Sun and Zhenzhong Chen},
  doi          = {10.1109/TIP.2020.2970248},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4027-4040},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learned image downscaling for upscaling using content adaptive resampler},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image captioning with end-to-end attribute detection and
subsequent attributes prediction. <em>TIP</em>, <em>29</em>, 4013–4026.
(<a href="https://doi.org/10.1109/TIP.2020.2969330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic attention has been shown to be effective in improving the performance of image captioning. The core of semantic attention based methods is to drive the model to attend to semantically important words, or attributes. In previous works, the attribute detector and the captioning network are usually independent, leading to the insufficient usage of the semantic information. Also, all the detected attributes, no matter whether they are appropriate for the linguistic context at the current step, are attended to through the whole caption generation process. This may sometimes disrupt the captioning model to attend to incorrect visual concepts. To solve these problems, we introduce two end-to-end trainable modules to closely couple attribute detection with image captioning as well as prompt the effective uses of attributes by predicting appropriate attributes at each time step. The multimodal attribute detector (MAD) module improves the attribute detection accuracy by using not only the image features but also the word embedding of attributes already existing in most captioning models. MAD models the similarity between the semantics of attributes and the image object features to facilitate accurate detection. The subsequent attribute predictor (SAP) module dynamically predicts a concise attribute subset at each time step to mitigate the diversity of image attributes. Compared to previous attribute based methods, our approach enhances the explainability in how the attributes affect the generated words and achieves a state-of-the-art single model performance of 128.8 CIDEr-D on the MSCOCO dataset. Extensive experiments on the MSCOCO dataset show that our proposal actually improves the performances in both image captioning and attribute detection simultaneously. The codes are available at: https://github.com/RubickH/Image-Captioning-with-MAD-and-SAP.},
  archive      = {J_TIP},
  author       = {Yiqing Huang and Jiansheng Chen and Wanli Ouyang and Weitao Wan and Youze Xue},
  doi          = {10.1109/TIP.2020.2969330},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4013-4026},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image captioning with end-to-end attribute detection and subsequent attributes prediction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensing matrix design for compressive spectral imaging via
binary principal component analysis. <em>TIP</em>, <em>29</em>,
4003–4012. (<a href="https://doi.org/10.1109/TIP.2019.2959737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive spectral imaging (CSI) is a framework that captures coded-and-multiplexed low-dimensional projections of spectral data-cubes. In general, the sensing process in many CSI architectures is described using binary matrices, so-called sensing/projection matrices, whose elements can be either random or designed. However, some characteristics of the spectral data, such as the ℓ 2 -norm or the second moment statistics, can be lost when this dimensionality reduction is performed. Similarly, principal component analysis (PCA) is a data dimensionality reduction technique that minimizes the least-squared error between the spectral data and its low-dimensional projection, but preserving its structure or variance. Thus, PCA can be used to guide the CSI acquisition process by designing the binary sensing matrix. Nonetheless, PCA requires to know the spectral image a-priori, and also, its associated projection matrix is not binary, as required by CSI optical architectures. Therefore, in this paper, an algorithm to design CSI sensing matrices by exploiting the structure-preserving property of the PCA projection is proposed. First, a set of compressive measurements obtained with random sensing matrices is used to rapidly estimate the covariance matrix associated with the spectral data. Then, a new sensing matrix is designed by solving a non-convex optimization problem that finds a set of binary vectors that approximate the principal components of the covariance matrix, thus maximizing the explanation of the data variance. Experimental results show an improvement of up to 3 dB in image reconstruction quality, in terms of the peak signal to noise ratio (PSNR), when the binary PCA-based sensing matrices are used and compared with conventional random sensing matrices and state-of-art designed matrices based on PCA.},
  archive      = {J_TIP},
  author       = {Jonathan Monsalve and Hoover Rueda-Chacon and Henry Arguello},
  doi          = {10.1109/TIP.2019.2959737},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4003-4012},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sensing matrix design for compressive spectral imaging via binary principal component analysis},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised multi-target domain adaptation: An information
theoretic approach. <em>TIP</em>, <em>29</em>, 3993–4002. (<a
href="https://doi.org/10.1109/TIP.2019.2963389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (uDA) models focus on pairwise adaptation settings where there is a single, labeled, source and a single target domain. However, in many real-world settings one seeks to adapt to multiple, but somewhat similar, target domains. Applying pairwise adaptation approaches to this setting may be suboptimal, as they fail to leverage shared information among multiple domains. In this work, we propose an information theoretic approach for domain adaptation in the novel context of multiple target domains with unlabeled instances and one source domain with labeled instances. Our model aims to find a shared latent space common to all domains, while simultaneously accounting for the remaining private, domain-specific factors. Disentanglement of shared and private information is accomplished using a unified information-theoretic approach, which also serves to establish a stronger link between the latent representations and the observed data. The resulting model, accompanied by an efficient optimization algorithm, allows simultaneous adaptation from a single source to multiple target domains. We test our approach on three challenging publicly-available datasets, showing that it outperforms several popular domain adaptation methods.},
  archive      = {J_TIP},
  author       = {Behnam Gholami and Pritish Sahu and Ognjen Rudovic and Konstantinos Bousmalis and Vladimir Pavlovic},
  doi          = {10.1109/TIP.2019.2963389},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3993-4002},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised multi-target domain adaptation: An information theoretic approach},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Revisiting EmbodiedQA: A simple baseline and beyond.
<em>TIP</em>, <em>29</em>, 3984–3992. (<a
href="https://doi.org/10.1109/TIP.2020.2967584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Embodied Question Answering (EmbodiedQA), an agent interacts with an environment to gather necessary information for answering user questions. Existing works have laid a solid foundation towards solving this interesting problem. But the current performance, especially in navigation, suggests that EmbodiedQA might be too challenging for the contemporary approaches. In this paper, we empirically study this problem and introduce 1) a simple yet effective baseline that achieves promising performance; 2) an easier and practical setting for EmbodiedQA where an agent has a chance to adapt the trained model to a new environment before it actually answers users questions. In this new setting, we randomly place a few objects in new environments, and upgrade the agent policy by a distillation network to retain the generalization ability from the trained model. On the EmbodiedQA v1 benchmark, under the standard setting, our simple baseline achieves very competitive results to the-state-of-the-art; in the new setting, we found the introduced small change in settings yields a notable gain in navigation.},
  archive      = {J_TIP},
  author       = {Yu Wu and Lu Jiang and Yi Yang},
  doi          = {10.1109/TIP.2020.2967584},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3984-3992},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting EmbodiedQA: A simple baseline and beyond},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph laplacian regularization for robust optical flow
estimation. <em>TIP</em>, <em>29</em>, 3970–3983. (<a
href="https://doi.org/10.1109/TIP.2019.2945653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes graph Laplacian regularization for robust estimation of optical flow. First, we analyze the spectral properties of dense graph Laplacians and show that dense graphs achieve a better trade-off between preserving flow discontinuities and filtering noise, compared with the usual Laplacian. Using this analysis, we then propose a robust optical flow estimation method based on Gaussian graph Laplacians. We revisit the framework of iteratively reweighted least-squares from the perspective of graph edge reweighting, and employ the Welsch loss function to preserve flow discontinuities and handle occlusions. Our experiments using the Middlebury and MPI-Sintel optical flow datasets demonstrate the robustness and the efficiency of our proposed approach.},
  archive      = {J_TIP},
  author       = {Sean I. Young and Aous T. Naman and David Taubman},
  doi          = {10.1109/TIP.2019.2945653},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3970-3983},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph laplacian regularization for robust optical flow estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modality compensation network: Cross-modal adaptation for
action recognition. <em>TIP</em>, <em>29</em>, 3957–3969. (<a
href="https://doi.org/10.1109/TIP.2020.2967577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of RGB-D cameras, multi-modal video data have become more available for human action recognition. One main challenge for this task lies in how to effectively leverage their complementary information. In this work, we propose a Modality Compensation Network (MCN) to explore the relationships of different modalities, and boost the representations for human action recognition. We regard RGB/optical flow videos as source modalities, skeletons as auxiliary modality. Our goal is to extract more discriminative features from source modalities, with the help of auxiliary modality. Built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model bridges data from source and auxiliary modalities by a modality adaptation block to achieve adaptive representation learning, that the network learns to compensate for the loss of skeletons at test time and even at training time. We explore multiple adaptation schemes to narrow the distance between source and auxiliary modal distributions from different levels, according to the alignment of source and auxiliary data in training. In addition, skeletons are only required in the training phase. Our model is able to improve the recognition performance with source data when testing. Experimental results reveal that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.},
  archive      = {J_TIP},
  author       = {Sijie Song and Jiaying Liu and Yanghao Li and Zongming Guo},
  doi          = {10.1109/TIP.2020.2967577},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3957-3969},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Modality compensation network: Cross-modal adaptation for action recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning hybrid representation by robust dictionary learning
in factorized compressed space. <em>TIP</em>, <em>29</em>, 3941–3956.
(<a href="https://doi.org/10.1109/TIP.2020.2965289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the robust dictionary learning (DL) to discover the hybrid salient low-rank and sparse representation in a factorized compressed space. A Joint Robust Factorization and Projective Dictionary Learning (J-RFDL) model is presented. The setting of J-RFDL aims at improving the data representations by enhancing the robustness to outliers and noise in data, encoding the reconstruction error more accurately and obtaining hybrid salient coefficients with accurate reconstruction ability. Specifically, J-RFDL performs the robust representation by DL in a factorized compressed space to eliminate the negative effects of noise and outliers on the results, which can also make the DL process efficient. To make the encoding process robust to noise in data, J-RFDL clearly uses sparse L 2, 1 -norm that can potentially minimize the factorization and reconstruction errors jointly by forcing rows of the reconstruction errors to be zeros. To deliver salient coefficients with good structures to reconstruct given data well, J-RFDL imposes the joint low-rank and sparse constraints on the embedded coefficients with a synthesis dictionary. Based on the hybrid salient coefficients, we also extend J-RFDL for the joint classification and propose a discriminative J-RFDL model, which can improve the discriminating abilities of learnt coefficients by minimizing the classification error jointly. Extensive experiments on public datasets demonstrate that our formulations can deliver superior performance over other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Jiahuan Ren and Zhao Zhang and Sheng Li and Yang Wang and Guangcan Liu and Shuicheng Yan and Meng Wang},
  doi          = {10.1109/TIP.2020.2965289},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3941-3956},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning hybrid representation by robust dictionary learning in factorized compressed space},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient and fast real-world noisy image denoising by
combining pyramid neural network and two-pathway unscented kalman
filter. <em>TIP</em>, <em>29</em>, 3927–3940. (<a
href="https://doi.org/10.1109/TIP.2020.2965294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, image prior learning has emerged as an effective tool for image denoising, which exploits prior knowledge to obtain sparse coding models and utilize them to reconstruct the clean image from the noisy one. Albeit promising, these prior-learning based methods suffer from some limitations such as lack of adaptivity and failed attempts to improve performance and efficiency simultaneously. With the purpose of addressing these problems, in this paper, we propose a Pyramid Guided Filter Network (PGF-Net) integrated with pyramid-based neural network and Two-Pathway Unscented Kalman Filter (TP-UKF). The combination of pyramid network and TP-UKF is based on the consideration that the former enables our model to better exploit hierarchical and multi-scale features, while the latter can guide the network to produce an improved (a posteriori) estimation of the denoising results with fine-scale image details. Through synthesizing the respective advantages of pyramid network and TP-UKF, our proposed architecture, in stark contrast to prior learning methods, is able to decompose the image denoising task into a series of more manageable stages and adaptively eliminate the noise on real images in an efficient manner. We conduct extensive experiments and show that our PGF-Net achieves notable improvement on visual perceptual quality and higher computational efficiency compared to state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Ruijun Ma and Haifeng Hu and Songlong Xing and Zhengming Li},
  doi          = {10.1109/TIP.2020.2965294},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3927-3940},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient and fast real-world noisy image denoising by combining pyramid neural network and two-pathway unscented kalman filter},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An interpretable deep architecture for similarity learning
built upon hierarchical concepts. <em>TIP</em>, <em>29</em>, 3911–3926.
(<a href="https://doi.org/10.1109/TIP.2020.2965275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, development of adequately complex mathematical models, such as deep neural networks, can be an effective way to improve the accuracy of learning models. However, this is achieved at the cost of reduced post-hoc model interpretability, because what is learned by the model can become less intelligible and tractable to humans as the model complexity increases. In this paper, we target a similarity learning task in the context of image retrieval, with a focus on the model interpretability issue. An effective similarity neural network (SNN) is proposed not only to seek robust retrieval performance but also to achieve satisfactory post-hoc interpretability. The network is designed by linking the neuron architecture with the organization of a concept tree and by formulating neuron operations to pass similarity information between concepts. Various ways of understanding and visualizing what is learned by the SNN neurons are proposed. We also exhaustively evaluate the proposed approach using a number of relevant datasets against a number of state-of-the-art approaches to demonstrate the effectiveness of the proposed network. Our results show that the proposed approach can offer superior performance when compared against state-of-the-art approaches. Neuron visualization results are demonstrated to support the understanding of the trained neurons.},
  archive      = {J_TIP},
  author       = {Xinjian Gao and Tingting Mu and John Yannis Goulermas and Jeyarajan Thiyagalingam and Meng Wang},
  doi          = {10.1109/TIP.2020.2965275},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3911-3926},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An interpretable deep architecture for similarity learning built upon hierarchical concepts},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Personality-assisted multi-task learning for generic and
personalized image aesthetics assessment. <em>TIP</em>, <em>29</em>,
3898–3910. (<a href="https://doi.org/10.1109/TIP.2020.2968285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image aesthetics assessment (IAA) approaches mainly predict the average aesthetic score of an image. However, people tend to have different tastes on image aesthetics, which is mainly determined by their subjective preferences. As an important subjective trait, personality is believed to be a key factor in modeling individual&#39;s subjective preference. In this paper, we present a personality-assisted multi-task deep learning framework for both generic and personalized image aesthetics assessment. The proposed framework comprises two stages. In the first stage, a multi-task learning network with shared weights is proposed to predict the aesthetics distribution of an image and Big-Five (BF) personality traits of people who like the image. The generic aesthetics score of the image can be generated based on the predicted aesthetics distribution. In order to capture the common representation of generic image aesthetics and people&#39;s personality traits, a Siamese network is trained using aesthetics data and personality data jointly. In the second stage, based on the predicted personality traits and generic aesthetics of an image, an inter-task fusion is introduced to generate individual&#39;s personalized aesthetic scores on the image. The performance of the proposed method is evaluated using two public image aesthetics databases. The experimental results demonstrate that the proposed method outperforms the state-of-the-arts in both generic and personalized IAA tasks.},
  archive      = {J_TIP},
  author       = {Leida Li and Hancheng Zhu and Sicheng Zhao and Guiguang Ding and Weisi Lin},
  doi          = {10.1109/TIP.2020.2968285},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3898-3910},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Personality-assisted multi-task learning for generic and personalized image aesthetics assessment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OCTRexpert: A feature-based 3D registration method for
retinal OCT images. <em>TIP</em>, <em>29</em>, 3885–3897. (<a
href="https://doi.org/10.1109/TIP.2020.2967589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image registration can be used for studying longitudinal and cross-sectional data, quantitatively monitoring disease progression and guiding computer assisted diagnosis and treatments. However, deformable registration which enables more precise and quantitative comparison has not been well developed for retinal optical coherence tomography (OCT) images. This paper proposes a new 3D registration approach for retinal OCT data called OCTRexpert. To the best of our knowledge, the proposed algorithm is the first full 3D registration approach for retinal OCT images which can be applied to longitudinal OCT images for both normal and serious pathological subjects. In this approach, a pre-processing method is first performed to remove eye motion artifact and then a novel design-detection-deformation strategy is applied for the registration. In the design step, a couple of features are designed for each voxel in the image. In the detection step, active voxels are selected and the point-to-point correspondences between the subject and template images are established. In the deformation step, the image is hierarchically deformed according to the detected correspondences in multi-resolution. The proposed method is evaluated on a dataset with longitudinal OCT images from 20 healthy subjects and 4 subjects diagnosed with serious Choroidal Neovascularization (CNV). Experimental results show that the proposed registration algorithm consistently yields statistically significant improvements in both Dice similarity coefficient and the average unsigned surface error compared with the other registration methods.},
  archive      = {J_TIP},
  author       = {Lingjiao Pan and Fei Shi and Dehui Xiang and Kai Yu and Luwen Duan and Jian Zheng and Xinjian Chen},
  doi          = {10.1109/TIP.2020.2967589},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3885-3897},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {OCTRexpert: A feature-based 3D registration method for retinal OCT images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic random walk for superpixel segmentation.
<em>TIP</em>, <em>29</em>, 3871–3884. (<a
href="https://doi.org/10.1109/TIP.2020.2967583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel random walk model, called Dynamic Random Walk (DRW), which adds a new type of dynamic node to the original RW model and reduces redundant calculation by limiting the walk range. To solve the seed-lacking problem of the proposed DRW, we redefine the energy function of the original RW and use the first arrival probability among each node pair to avoid the interference for each partition. Relaxation of our DRW is performed with the help of a greedy strategy and the Weighted Random Walk Entropy(WRWE) that uses the gradient feature to approximate the stationary distribution. The proposed DRW not only can enhance the boundary adherence but also can run with linear time complexity. To extend our DRW for superpixel segmentation, a seed initialization strategy is proposed. It can evenly distribute seeds in both 2D and 3D space and generate superpixels in only one iteration. The experimental results demonstrate that our DRW is faster than existing RW models and better than the state-of-the-art superpixel segmentation algorithms with respect to both efficiency and segmentation effects.},
  archive      = {J_TIP},
  author       = {Xuejing Kang and Lei Zhu and Anlong Ming},
  doi          = {10.1109/TIP.2020.2967583},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3871-3884},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic random walk for superpixel segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Open-ended video question answering via multi-modal
conditional adversarial networks. <em>TIP</em>, <em>29</em>, 3859–3870.
(<a href="https://doi.org/10.1109/TIP.2020.2963950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging task in visual information retrieval, open-ended long-form video question answering automatically generates the natural language answer from the referenced video content according to the given question. However, the existing video question answering works mainly focus on the short-form video, which may be ineffectively applied for long-form video question answering directly, due to the insufficiency of modeling the semantic representation of long-form video content. In this paper, we study the problem of open-ended long-form video question answering from the viewpoint of hierarchical multi-modal conditional adversarial network learning. We propose the hierarchical attentional encoder network to learn the joint representation of long-form video content and given question with adaptive video segmentation. We then devise the reinforced decoder network to generate the natural language answer for open-ended video question answering with multi-modal conditional adversarial network learning. We construct three large-scale open-ended video question answering datasets. The extensive experiments validate the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Zhou Zhao and Shuwen Xiao and Zehan Song and Chujie Lu and Jun Xiao and Yueting Zhuang},
  doi          = {10.1109/TIP.2020.2963950},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3859-3870},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Open-ended video question answering via multi-modal conditional adversarial networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised deep image fusion with structure tensor
representations. <em>TIP</em>, <em>29</em>, 3845–3858. (<a
href="https://doi.org/10.1109/TIP.2020.2966075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have facilitated substantial progress on various problems in computer vision and image processing. However, applying them to image fusion has remained challenging due to the lack of the labelled data for supervised learning. This paper introduces a deep image fusion network (DIF-Net), an unsupervised deep learning framework for image fusion. The DIF-Net parameterizes the entire processes of image fusion, comprising of feature extraction, feature fusion, and image reconstruction, using a CNN. The purpose of DIF-Net is to generate an output image which has an identical contrast to high-dimensional input images. To realize this, we propose an unsupervised loss function using the structure tensor representation of the multi-channel image contrasts. Different from traditional fusion methods that involve time-consuming optimization or iterative procedures to obtain the results, our loss function is minimized by a stochastic deep learning solver with large-scale examples. Consequently, the proposed method can produce fused images that preserve source image details through a single forward network trained without reference ground-truth labels. The proposed method has broad applicability to various image fusion problems, including multi-spectral, multi-focus, and multi-exposure image fusions. Quantitative and qualitative evaluations show that the proposed technique outperforms existing state-of-the-art approaches for various applications.},
  archive      = {J_TIP},
  author       = {Hyungjoo Jung and Youngjung Kim and Hyunsung Jang and Namkoo Ha and Kwanghoon Sohn},
  doi          = {10.1109/TIP.2020.2966075},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3845-3858},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised deep image fusion with structure tensor representations},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). View-invariant deep architecture for human action
recognition using two-stream motion and shape temporal dynamics.
<em>TIP</em>, <em>29</em>, 3835–3844. (<a
href="https://doi.org/10.1109/TIP.2020.2965299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action Recognition for unknown views, is a challenging task. We propose a deep view-invariant human action recognition framework, which is a novel integration of two important action cues: motion and shape temporal dynamics (STD). The motion stream encapsulates the motion content of action as RGB Dynamic Images (RGB-DIs), which are generated by Approximate Rank Pooling (ARP) and processed by using fine-tuned InceptionV3 model. The STD stream learns long-term view-invariant shape dynamics of action using a sequence of LSTM and Bi-LSTM learning models. Human Pose Model (HPM) generates view-invariant features of structural similarity index matrix (SSIM) based key depth human pose frames. The final prediction of the action is made on the basis of three types of late fusion techniques i.e. maximum (max), average (avg) and multiply (mul), applied on individual stream scores. To validate the performance of the proposed novel framework, the experiments are performed using both cross-subject and cross-view validation schemes on three publically available benchmarks-NUCLA multi-view dataset, UWA3D-II Activity dataset and NTU RGB-D Activity dataset. Our algorithm outperforms existing state-of-the-arts significantly, which is measured in terms of recognition accuracy, receiver operating characteristic (ROC) curve and area under the curve (AUC).},
  archive      = {J_TIP},
  author       = {Chhavi Dhiman and Dinesh Kumar Vishwakarma},
  doi          = {10.1109/TIP.2020.2965299},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3835-3844},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {View-invariant deep architecture for human action recognition using two-stream motion and shape temporal dynamics},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graininess-aware deep feature learning for robust pedestrian
detection. <em>TIP</em>, <em>29</em>, 3820–3834. (<a
href="https://doi.org/10.1109/TIP.2020.2966371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a graininess-aware deep feature learning method for pedestrian detection. Unlike most existing methods which utilize the convolutional features without explicit distinction, we appropriately exploit multiple convolutional layers and dynamically select most informative features. Specifically, we train a multi-scale pedestrian attention via pixel-wise segmentation supervision to efficiently identify the pedestrian of particular scales. We encodes the fine-grained attention map into the feature maps of the detection layers to guide them to highlight the pedestrians of specific scale and avoid the background interference. The graininess-aware feature maps generated with our attention mechanism are more focused on pedestrians, and in particular on the small-scale and occluded targets. We further introduce a zoom-in-zoom-out module to enhances the features by incorporating local details and context information. Extensive experimental results on five challenging pedestrian detection benchmarks show that our method achieves very competitive or even better performance with the state-of-the-arts and is faster than most existing approaches.},
  archive      = {J_TIP},
  author       = {Chunze Lin and Jiwen Lu and Gang Wang and Jie Zhou},
  doi          = {10.1109/TIP.2020.2966371},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3820-3834},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graininess-aware deep feature learning for robust pedestrian detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multimodal saliency model for videos with high
audio-visual correspondence. <em>TIP</em>, <em>29</em>, 3805–3819. (<a
href="https://doi.org/10.1109/TIP.2020.2966082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio information has been bypassed by most of current visual attention prediction studies. However, sound could have influence on visual attention and such influence has been widely investigated and proofed by many psychological studies. In this paper, we propose a novel multi-modal saliency (MMS) model for videos containing scenes with high audio-visual correspondence. In such scenes, humans tend to be attracted by the sound sources and it is also possible to localize the sound sources via cross-modal analysis. Specifically, we first detect the spatial and temporal saliency maps from the visual modality by using a novel free energy principle. Then we propose to detect the audio saliency map from both audio and visual modalities by localizing the moving-sounding objects using cross-modal kernel canonical correlation analysis, which is first of its kind in the literature. Finally we propose a new two-stage adaptive audiovisual saliency fusion method to integrate the spatial, temporal and audio saliency maps to our audio-visual saliency map. The proposed MMS model has captured the influence of audio, which is not considered in the latest deep learning based saliency models. To take advantages of both deep saliency modeling and audio-visual saliency modeling, we propose to combine deep saliency models and the MMS model via a later fusion, and we find that an average of 5\% performance gain is obtained. Experimental results on audio-visual attention databases show that the introduced models incorporating audio cues have significant superiority over state-of-the-art image and video saliency models which utilize a single visual modality.},
  archive      = {J_TIP},
  author       = {Xiongkuo Min and Guangtao Zhai and Jiantao Zhou and Xiao-Ping Zhang and Xiaokang Yang and Xinping Guan},
  doi          = {10.1109/TIP.2020.2966082},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3805-3819},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multimodal saliency model for videos with high audio-visual correspondence},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A metric for light field reconstruction, compression, and
display quality evaluation. <em>TIP</em>, <em>29</em>, 3790–3804. (<a
href="https://doi.org/10.1109/TIP.2020.2966081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owning to the recorded light ray distributions, light field contains much richer information and provides possibilities of some enlightening applications, and it has becoming more and more popular. To facilitate the relevant applications, many light field processing techniques have been proposed recently. These operations also bring the loss of visual quality, and thus there is need of a light field quality metric to quantify the visual quality loss. To reduce the processing complexity and resource consumption, light fields are generally sparsely sampled, compressed, and finally reconstructed and displayed to the users. We consider the distortions introduced in this typical light field processing chain, and propose a full-reference light field quality metric. Specifically, we measure the light field quality from three aspects: global spatial quality based on view structure matching, local spatial quality based on near-edge mean square error, and angular quality based on multi-view quality analysis. These three aspects have captured the most common distortions introduced in light field processing, including global distortions like blur and blocking, local geometric distortions like ghosting and stretching, and angular distortions like flickering and sampling. Experimental results show that the proposed method can estimate light field quality accurately, and it outperforms the state-of-the-art quality metrics which may be effective for light field.},
  archive      = {J_TIP},
  author       = {Xiongkuo Min and Jiantao Zhou and Guangtao Zhai and Patrick Le Callet and Xiaokang Yang and Xinping Guan},
  doi          = {10.1109/TIP.2020.2966081},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3790-3804},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A metric for light field reconstruction, compression, and display quality evaluation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Satisfied-user-ratio modeling for compressed video.
<em>TIP</em>, <em>29</em>, 3777–3789. (<a
href="https://doi.org/10.1109/TIP.2020.2965994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With explosive increase of internet video services, perceptual modeling for video quality has attracted more attentions to provide high quality-of-experience (QoE) for end-users subject to bandwidth constraints, especially for compressed video quality. In this paper, a novel perceptual model for satisfied-user-ratio (SUR) on compressed video quality is proposed by exploiting compressed video bitrate changes and spatial-temporal statistical characteristics extracted from both uncompressed original video and reference video. In the proposed method, an efficient video feature set is explored and established to model SUR curves against bitrate variations by leveraging the Gaussian Processes Regression (GPR) framework. In particular, the proposed model is based on the recently released large-scale video quality dataset, VideoSet, and takes both spatial and temporal masking effects into consideration. To make it more practical, we further optimize the proposed method from three aspects including feature source simplification, computation complexity reduction and video codec adaption. Based on experimental results on VideoSet, the proposed method can accurately model SUR curves for various video contents and predict their required bitrates at given SUR values. Subjective experiments are conducted to further verify the generalization ability of the proposed SUR model.},
  archive      = {J_TIP},
  author       = {Xinfeng Zhang and Chao Yang and Haiqiang Wang and Wei Xu and C.-C. Jay Kuo},
  doi          = {10.1109/TIP.2020.2965994},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3777-3789},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Satisfied-user-ratio modeling for compressed video},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reverse attention-based residual network for salient object
detection. <em>TIP</em>, <em>29</em>, 3763–3776. (<a
href="https://doi.org/10.1109/TIP.2020.2965989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the quick development of deep convolutional neural networks, especially fully convolutional neural networks (FCNs), remarkable progresses have been achieved on salient object detection recently. Nevertheless, these FCNs based methods are still challenging to generate high resolution saliency maps, and also not applicable for subsequent applications due to their heavy model weights. In this paper, we propose a compact and efficient deep network with high accuracy for salient object detection. Firstly, we propose two strategies for initial prediction, one is a new designed multi-scale context module, the other is incorporating hand-crafted saliency priors. Secondly, we employ residual learning to refine it progressively by only learning the residual in each side-output, which can be achieved with few convolutional parameters, therefore leads to high compactness and high efficiency. Finally, we further design a novel top-down reverse attention block to guide the above side-output residual learning. Specifically, the current predicted salient regions are used to erase its side-output feature, thus the missing object parts and details can be efficiently learned from these unerased regions, which results in more complete detection and high accuracy. Extensive experimental results on seven benchmark datasets demonstrate that the proposed network performs favorably against the state-of-the-art approaches, and shows advantages in simplicity, compactness and efficiency.},
  archive      = {J_TIP},
  author       = {Shuhan Chen and Xiuli Tan and Ben Wang and Huchuan Lu and Xuelong Hu and Yun Fu},
  doi          = {10.1109/TIP.2020.2965989},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3763-3776},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reverse attention-based residual network for salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Moment retrieval via cross-modal interaction networks with
query reconstruction. <em>TIP</em>, <em>29</em>, 3750–3762. (<a
href="https://doi.org/10.1109/TIP.2020.2965987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moment retrieval aims to localize the most relevant moment in an untrimmed video according to the given natural language query. Existing works often only focus on one aspect of this emerging task, such as the query representation learning, video context modeling or multi-modal fusion, thus fail to develop a comprehensive system for further performance improvement. In this paper, we introduce a novel Cross-Modal Interaction Network (CMIN) to consider multiple crucial factors for this challenging task, including the syntactic dependencies of natural language queries, long-range semantic dependencies in video context and the sufficient cross-modal interaction. Specifically, we devise a syntactic GCN to leverage the syntactic structure of queries for fine-grained representation learning and propose a multi-head self-attention to capture long-range semantic dependencies from video context. Next, we employ a multi-stage cross-modal interaction to explore the potential relations of video and query contents, and we also consider query reconstruction from the cross-modal representations of target moment as an auxiliary task to strengthen the cross-modal representations. The extensive experiments on ActivityNet Captions and TACoS demonstrate the effectiveness of our proposed method.},
  archive      = {J_TIP},
  author       = {Zhijie Lin and Zhou Zhao and Zhu Zhang and Zijian Zhang and Deng Cai},
  doi          = {10.1109/TIP.2020.2965987},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3750-3762},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Moment retrieval via cross-modal interaction networks with query reconstruction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint coding of local and global deep features in videos for
visual search. <em>TIP</em>, <em>29</em>, 3734–3749. (<a
href="https://doi.org/10.1109/TIP.2020.2965306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practically, it is more feasible to collect compact visual features rather than the video streams from hundreds of thousands of cameras into the cloud for big data analysis and retrieval. Then the problem becomes which kinds of features should be extracted, compressed and transmitted so as to meet the requirements of various visual tasks. Recently, many studies have indicated that the activations from the convolutional layers in convolutional neural networks (CNNs) can be treated as local deep features describing particular details inside an image region, which are then aggregated (e.g., using Fisher Vectors) as a powerful global descriptor. Combination of local and global features can satisfy those various needs effectively. It has also been validated that, if only local deep features are coded and transmitted to the cloud while the global features are recovered using the decoded local features, the aggregated global features should be lossy and consequently would degrade the overall performance. Therefore, this paper proposes a joint coding framework for local and global deep features (DFJC) extracted from videos. In this framework, we introduce a coding scheme for real-valued local and global deep features with intra-frame lossy coding and inter-frame reference coding. The theoretical analysis is performed to understand how the number of inliers varies with the number of local features. Moreover, the inter-feature correlations are exploited in our framework. That is, local feature coding can be accelerated by making use of the frame types determined with global features, while the lossy global features aggregated with the decoded local features can be used as a reference for global feature coding. Extensive experimental results under three metrics show that our DFJC framework can significantly reduce the bitrate of local and global deep features from videos while maintaining the retrieval performance.},
  archive      = {J_TIP},
  author       = {Lin Ding and Yonghong Tian and Hongfei Fan and Changhuai Chen and Tiejun Huang},
  doi          = {10.1109/TIP.2020.2965306},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3734-3749},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint coding of local and global deep features in videos for visual search},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Material based object tracking in hyperspectral videos.
<em>TIP</em>, <em>29</em>, 3719–3733. (<a
href="https://doi.org/10.1109/TIP.2020.2965302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional color images only depict color intensities in red, green and blue channels, often making object trackers fail in challenging scenarios, e.g., background clutter and rapid changes of target appearance. Alternatively, material information of targets contained in large amount of bands of hyperspectral images (HSI) is more robust to these difficult conditions. In this paper, we conduct a comprehensive study on how material information can be utilized to boost object tracking from three aspects: dataset, material feature representation and material based tracking. In terms of dataset, we construct a dataset of fully-annotated videos, which contain both hyperspectral and color sequences of the same scene. Material information is represented by spectral-spatial histogram of multidimensional gradients, which describes the 3D local spectral-spatial structure in an HSI, and fractional abundances of constituted material components which encode the underlying material distribution. These two types of features are embedded into correlation filters, yielding material based tracking. Experimental results on the collected dataset show the potentials and advantages of material based object tracking.},
  archive      = {J_TIP},
  author       = {Fengchao Xiong and Jun Zhou and Yuntao Qian},
  doi          = {10.1109/TIP.2020.2965302},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3719-3733},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Material based object tracking in hyperspectral videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free distortion rectification framework bridged by
distortion distribution map. <em>TIP</em>, <em>29</em>, 3707–3718. (<a
href="https://doi.org/10.1109/TIP.2020.2964523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, learning-based distortion rectification schemes have shown high efficiency. However, most of these methods only focus on a specific camera model with fixed parameters, thus failing to be extended to other models. To avoid such a disadvantage, we propose a model-free distortion rectification framework for the single-shot case, bridged by the distortion distribution map (DDM). Our framework is based on an observation that the pixel-wise distortion information is explicitly regular in a distorted image, despite different models having different types and numbers of distortion parameters. Motivated by this observation, instead of estimating the heterogeneous distortion parameters, we construct a proposed distortion distribution map that intuitively indicates the global distortion features of a distorted image. In addition, we develop a dual-stream feature learning module, benefitting from both the advantages of traditional methods that leverage the local handcrafted feature and learning-based methods that focus on the global semantic feature perception. Due to the sparsity of handcrafted features, we discrete the features into a 2D point map and learn the structure inspired by PointNet. Finally, a multimodal attention fusion module is designed to attentively fuse the local structural and global semantic features, providing the hybrid features for the more reasonable scene recovery. The experimental results demonstrate the excellent generalization ability and more significant performance of our method in both quantitative and qualitative evaluations, compared with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Kang Liao and Chunyu Lin and Yao Zhao and Mai Xu},
  doi          = {10.1109/TIP.2020.2964523},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3707-3718},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Model-free distortion rectification framework bridged by distortion distribution map},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Connecting image denoising and high-level vision tasks via
deep learning. <em>TIP</em>, <em>29</em>, 3695–3706. (<a
href="https://doi.org/10.1109/TIP.2020.2964518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising and high-level vision tasks are usually handled independently in the conventional practice of computer vision, and their connection is fragile. In this paper, we cope with the two jointly and explore the mutual influence between them with the focus on two questions, namely (1) how image denoising can help improving high-level vision tasks, and (2) how the semantic information from high-level vision tasks can be used to guide image denoising. First for image denoising we propose a convolutional neural network in which convolutions are conducted in various spatial resolutions via downsampling and upsampling operations in order to fuse and exploit contextual information on different scales. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and use the joint loss for updating only the denoising network via back-propagation. We experimentally show that on one hand, the proposed denoiser has the generality to overcome the performance degradation of different high-level vision tasks. On the other hand, with the guidance of high-level vision information, the denoising network produces more visually appealing results. Extensive experiments demonstrate the benefit of exploiting image semantics simultaneously for image denoising and high-level vision tasks via deep learning. The code is available online: https://github.com/Ding-Liu/DeepDenoising.},
  archive      = {J_TIP},
  author       = {Ding Liu and Bihan Wen and Jianbo Jiao and Xianming Liu and Zhangyang Wang and Thomas S. Huang},
  doi          = {10.1109/TIP.2020.2964518},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3695-3706},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Connecting image denoising and high-level vision tasks via deep learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A recurrent neural network for particle tracking in
microscopy images using future information, track hypotheses, and
multiple detections. <em>TIP</em>, <em>29</em>, 3681–3694. (<a
href="https://doi.org/10.1109/TIP.2020.2964515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic tracking of particles in time-lapse fluorescence microscopy images is essential for quantifying the dynamic behavior of subcellular structures and virus structures. We introduce a novel particle tracking approach based on a deep recurrent neural network architecture that exploits past and future information in both forward and backward direction. Assignment probabilities are determined jointly across multiple detections, and the probability of missing detections is computed. In addition, existence probabilities are determined by the network to handle track initiation and termination. For correspondence finding, track hypotheses are propagated to future time points so that information at later time points can be used to resolve ambiguities. A handcrafted similarity measure and handcrafted motion features are not necessary. Manually labeled data is not required for network training. We evaluated the performance of our approach using image data of the Particle Tracking Challenge as well as real fluorescence microscopy image sequences of virus structures. It turned out that the proposed approach outperforms previous methods.},
  archive      = {J_TIP},
  author       = {Roman Spilger and Andrea Imle and Ji-Young Lee and Barbara Müller and Oliver T. Fackler and Ralf Bartenschlager and Karl Rohr},
  doi          = {10.1109/TIP.2020.2964515},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3681-3694},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A recurrent neural network for particle tracking in microscopy images using future information, track hypotheses, and multiple detections},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-VAE-GAN: Generating unseen features for generalized and
transductive zero-shot learning. <em>TIP</em>, <em>29</em>, 3665–3680.
(<a href="https://doi.org/10.1109/TIP.2020.2964429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) is a challenging task due to the lack of unseen class data during training. Existing works attempt to establish a mapping between the visual and class spaces through a common intermediate semantic space. The main limitation of existing methods is the strong bias towards seen class, known as the domain shift problem, which leads to unsatisfactory performance in both conventional and generalized ZSL tasks. To tackle this challenge, we propose to convert ZSL to the conventional supervised learning by generating features for unseen classes. To this end, a joint generative model that couples variational autoencoder (VAE) and generative adversarial network (GAN), called Zero-VAE-GAN, is proposed to generate high-quality unseen features. To enhance the class-level discriminability, an adversarial categorization network is incorporated into the joint framework. Besides, we propose two self-training strategies to augment unlabeled unseen features for the transductive extension of our model, addressing the domain shift problem to a large extent. Experimental results on five standard benchmarks and a large-scale dataset demonstrate the superiority of our generative model over the state-of-the-art methods for conventional, especially generalized ZSL tasks. Moreover, the further improvement of the transductive setting demonstrates the effectiveness of the proposed self-training strategies.},
  archive      = {J_TIP},
  author       = {Rui Gao and Xingsong Hou and Jie Qin and Jiaxin Chen and Li Liu and Fan Zhu and Zhao Zhang and Ling Shao},
  doi          = {10.1109/TIP.2020.2964429},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3665-3680},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-VAE-GAN: Generating unseen features for generalized and transductive zero-shot learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Illumination invariant hyperspectral image unmixing based on
a digital surface model. <em>TIP</em>, <em>29</em>, 3652–3664. (<a
href="https://doi.org/10.1109/TIP.2020.2963961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many spectral unmixing models have been developed to address spectral variability caused by variable incident illuminations, the mechanism of the spectral variability is still unclear. This paper proposes an unmixing model, named illumination invariant spectral unmixing (IISU). IISU makes the first attempt to use the radiance hyperspectral data and a LiDAR-derived digital surface model (DSM) in order to physically explain variable illuminations and shadows in the unmixing framework. Incident angles, sky factors, visibility from the sun derived from the LiDAR-derived DSM support the explicit explanation of endmember variability in the unmixing process from radiance perspective. The proposed model was efficiently solved by a straightforward optimization procedure. The unmixing results showed that the other state-of-the-art unmixing models did not work well especially in the shaded pixels. On the other hand, the proposed model estimated more accurate abundances and shadow compensated reflectance than the existing models.},
  archive      = {J_TIP},
  author       = {Tatsumi Uezato and Naoto Yokoya and Wei He},
  doi          = {10.1109/TIP.2020.2963961},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3652-3664},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Illumination invariant hyperspectral image unmixing based on a digital surface model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A data dependent multiscale model for hyperspectral
unmixing with spectral variability. <em>TIP</em>, <em>29</em>,
3638–3651. (<a href="https://doi.org/10.1109/TIP.2020.2963959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral variability in hyperspectral images can result from factors including environmental, illumination, atmospheric and temporal changes. Its occurrence may lead to the propagation of significant estimation errors in the unmixing process. To address this issue, extended linear mixing models have been proposed which lead to large scale nonsmooth ill-posed inverse problems. Furthermore, the regularization strategies used to obtain meaningful results have introduced interdependencies among abundance solutions that further increase the complexity of the resulting optimization problem. In this paper we present a novel data dependent multiscale model for hyperspectral unmixing accounting for spectral variability. The new method incorporates spatial contextual information to the abundances in extended linear mixing models by using a multiscale transform based on superpixels. The proposed method results in a fast algorithm that solves the abundance estimation problem only once in each scale during each iteration. Simulation results using synthetic and real images compare the performances, both in accuracy and execution time, of the proposed algorithm and other state-of-the-art solutions.},
  archive      = {J_TIP},
  author       = {Ricardo Augusto Borsoi and Tales Imbiriba and José Carlos Moreira Bermudez},
  doi          = {10.1109/TIP.2020.2963959},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3638-3651},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A data dependent multiscale model for hyperspectral unmixing with spectral variability},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task consistency-preserving adversarial hashing for
cross-modal retrieval. <em>TIP</em>, <em>29</em>, 3626–3637. (<a
href="https://doi.org/10.1109/TIP.2020.2963957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the advantages of low storage cost and high query efficiency, cross-modal hashing has received increasing attention recently. As failing to bridge the inherent modality gap between modalities, most existing cross-modal hashing methods have limited capability to explore the semantic consistency information between different modality data, leading to unsatisfactory search performance. To address this problem, we propose a novel deep hashing method named Multi-Task Consistency-Preserving Adversarial Hashing (CPAH) to fully explore the semantic consistency and correlation between different modalities for efficient cross-modal retrieval. First, we design a consistency refined module (CR) to divide the representations of different modality into two irrelevant parts, i.e., modality-common and modality-private representations. Then, a multi-task adversarial learning module (MA) is presented, which can make the modality-common representation of different modalities close to each other on feature distribution and semantic consistency. Finally, the compact and powerful hash codes can be generated from modality-common representation. Comprehensive evaluations conducted on three representative cross-modal benchmark datasets illustrate our method is superior to the state-of-the-art cross-modal hashing methods.},
  archive      = {J_TIP},
  author       = {De Xie and Cheng Deng and Chao Li and Xianglong Liu and Dacheng Tao},
  doi          = {10.1109/TIP.2020.2963957},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3626-3637},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-task consistency-preserving adversarial hashing for cross-modal retrieval},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning a single tucker decomposition network for lossy
image compression with multiple bits-per-pixel rates. <em>TIP</em>,
<em>29</em>, 3612–3625. (<a
href="https://doi.org/10.1109/TIP.2020.2963956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lossy image compression (LIC), which aims to utilize inexact approximations to represent an image more compactly, is a classical problem in image processing. Recently, deep convolutional neural networks (CNNs) have achieved interesting results in LIC by learning an encoder-quantizer-decoder network from a large amount of data. However, existing CNN-based LIC methods generally train a network for a specific bits-per-pixel (bpp). Such a “one-network-per-bpp” problem limits the generality and flexibility of CNNs to practical LIC applications. In this paper, we propose to learn a single CNN which can perform LIC at multiple bpp rates. A simple yet effective Tucker Decomposition Network (TDNet) is developed, where there is a novel tucker decomposition layer (TDL) to decompose a latent image representation into a set of projection matrices and a core tensor. By changing the rank of core tensor and its quantization, we can easily adjust the bpp rate of latent image representation within a single CNN. Furthermore, an iterative non-uniform quantization scheme is presented to optimize the quantizer, and a coarse-to-fine training strategy is introduced to reconstruct the decompressed images. Extensive experiments demonstrate the state-of-the-art compression performance of TDNet in terms of both PSNR and MS-SSIM indices.},
  archive      = {J_TIP},
  author       = {Jianrui Cai and Zisheng Cao and Lei Zhang},
  doi          = {10.1109/TIP.2020.2963956},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3612-3625},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning a single tucker decomposition network for lossy image compression with multiple bits-per-pixel rates},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strongly constrained discrete hashing. <em>TIP</em>,
<em>29</em>, 3596–3611. (<a
href="https://doi.org/10.1109/TIP.2020.2963952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH $_{K}$ . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH $_{K}$ achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.},
  archive      = {J_TIP},
  author       = {Yong Chen and Zhibao Tian and Hui Zhang and Jun Wang and Dell Zhang},
  doi          = {10.1109/TIP.2020.2963952},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3596-3611},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Strongly constrained discrete hashing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PWStableNet: Learning pixel-wise warping maps for video
stabilization. <em>TIP</em>, <em>29</em>, 3582–3595. (<a
href="https://doi.org/10.1109/TIP.2019.2963380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the videos captured by hand-held cameras are often perturbed by high-frequency jitters, stabilization of these videos is an essential task. Many video stabilization methods have been proposed to stabilize shaky videos. However, most methods estimate one global homography or several homographies based on fixed meshes to warp the shaky frames into their stabilized views. Due to the existence of parallax, such single or a few homographies can not well handle the depth variation. In contrast to these traditional methods, we propose a novel video stabilization network, called PWStableNet, which comes up pixel-wise warping maps, i.e., potentially different warping for different pixels, and stabilizes each pixel to its stabilized view. To our best knowledge, this is the first deep learning based pixel-wise video stabilization. The proposed method is built upon a multi-stage cascade encoder-decoder architecture and learns pixel-wise warping maps from consecutive unstable frames. Inter-stage connections are also introduced to add feature maps of a former stage to the corresponding feature maps at a latter stage, which enables the latter stage to learn the residual from the feature maps of former stages. This cascade architecture can produce more precise warping maps at latter stages. To ensure the correct learning of pixel-wise warping maps, we use a well-designed loss function to guide the training procedure of the proposed PWStableNet. The proposed stabilization method achieves comparable performance with traditional methods, but stronger robustness and much faster processing speed. Moreover, the proposed stabilization method outperforms some typical CNN-based stabilization methods, especially in videos with strong parallax. Codes will be provided at https://github.com/mindazhao/pix-pix-warping-video-stabilization .},
  archive      = {J_TIP},
  author       = {Minda Zhao and Qiang Ling},
  doi          = {10.1109/TIP.2019.2963380},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3582-3595},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PWStableNet: Learning pixel-wise warping maps for video stabilization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deepzzle: Solving visual jigsaw puzzles with deep learning
and shortest path optimization. <em>TIP</em>, <em>29</em>, 3569–3581.
(<a href="https://doi.org/10.1109/TIP.2019.2963378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the image reassembly problem with wide space between the fragments, in such a way that the patterns and colors continuity is mostly unusable. The spacing emulates the erosion of which the archaeological fragments suffer. We crop-square the fragments borders to compel our algorithm to learn from the content of the fragments. We also complicate the image reassembly by removing fragments and adding pieces from other sources. We use a two-step method to obtain the reassemblies: 1) a neural network predicts the positions of the fragments despite the gaps between them; 2) a graph that leads to the best reassemblies is made from these predictions. In this paper, we notably investigate the effect of branch-cut in the graph of reassemblies. We also provide a comparison with the literature, solve complex images reassemblies, explore at length the dataset, and propose a new metric that suits its specificities.},
  archive      = {J_TIP},
  author       = {Marie-Morgane Paumard and David Picard and Hedi Tabia},
  doi          = {10.1109/TIP.2019.2963378},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3569-3581},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deepzzle: Solving visual jigsaw puzzles with deep learning and shortest path optimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressive spectral light field image reconstruction via
online tensor representation. <em>TIP</em>, <em>29</em>, 3558–3568. (<a
href="https://doi.org/10.1109/TIP.2019.2963376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years there has been an increasing interest in sensing devices that capture multidimensional information such as the spectral light field (SLF) images, which are 5-dimensional (5D) representations of a scene including 2D spatial, 2D angular and 1D spectral information. Spatio-spectral and angular information plays an important role in modern applications spanning from microscopy to computer vision. However, SLF sensors use expensive beam-splitters or cameras arrays placed in tandem, which split the sensing problem in two time consuming and independent tasks: spectral and light field imaging tasks. This work proposes a compressive spectral light field imaging architecture that builds on the principles of the compressive imaging framework, to capture multiplexed representations of the multidimensional information, so that, less measurements are required to capture the SLF data cube. Alongside, we propose a computational algorithm to recover the 5D information from the compressed measurements, exploiting the inherent high correlations within the SLF by treating them as 3D tensors. Furthermore, exploiting the geometry properties of the proposed optical architecture, the Tucker decomposition is applied to the set of compressed measurements, so that, an ad-hoc dictionary-like image representation basis is calculated online. This in turn, entails a more accurate reconstruction of the SLF since the dictionary fits the specific characteristics of the image itself. We demonstrate through simulations over three SLF datasets captured in our laboratory, and an experimental proof-of-concept implementation, that the proposed compressive imaging device together with the proposed computational algorithm represent an efficient alternative to capture SLF, compared to conventional methods that employ either side-information or multiple sensors. Also, we show that the tensor-based proposed algorithm exhibits a lower computational complexity than the matrix-based state of the art counterparts, thus enabling fast processing of multidimensional images.},
  archive      = {J_TIP},
  author       = {Miguel Marquez and Hoover Rueda-Chacon and Henry Arguello},
  doi          = {10.1109/TIP.2019.2963376},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3558-3568},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Compressive spectral light field image reconstruction via online tensor representation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SITUP: Scale invariant tracking using average
peak-to-correlation energy. <em>TIP</em>, <em>29</em>, 3546–3557. (<a
href="https://doi.org/10.1109/TIP.2019.2962694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust and accurate scale estimation of a target object is a challenging task in visual object tracking. Most existing tracking methods cannot accommodate large scale variation in complex image sequences and thus result in inferior performance. In this paper, we propose to incorporate a novel criterion called the average peak-to-correlation energy into the multi-resolution translation filter framework to obtain robust and accurate scale estimation. The resulting system is named SITUP: Scale Invariant Tracking using Average Peak-to-Correlation Energy. SITUP effectively tackles the problem of fixed template size in standard discriminative correlation filter based trackers. Extensive empirical evaluation on the publicly available tracking benchmark datasets demonstrates that the proposed scale searching framework meets the demands of scale variation challenges effectively while providing superior performance over other scale adaptive variants of standard discriminative correlation filter based trackers. Also, SITUP obtains favorable performance compared to state-of-the-art trackers for various scenarios while operating in real-time on a single CPU.},
  archive      = {J_TIP},
  author       = {Haoyi Ma and Scott T. Acton and Zongli Lin},
  doi          = {10.1109/TIP.2019.2962694},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3546-3557},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SITUP: Scale invariant tracking using average peak-to-correlation energy},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A multistage refinement network for salient object
detection. <em>TIP</em>, <em>29</em>, 3534–3545. (<a
href="https://doi.org/10.1109/TIP.2019.2962688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have been successfully applied to a wide variety of problems in computer vision, including salient object detection. To accurately detect and segment salient objects, it is necessary to extract and combine high-level semantic features with low-level fine details simultaneously. This is challenging for CNNs because repeated subsampling operations such as pooling and convolution lead to a significant decrease in the feature resolution, which results in the loss of spatial details and finer structures. Therefore, we propose augmenting feedforward neural networks by using the multistage refinement mechanism. In the first stage, a master net is built to generate a coarse prediction map in which most detailed structures are missing. In the following stages, the refinement net with layerwise recurrent connections to the master net is equipped to progressively combine local context information across stages to refine the preceding saliency maps in a stagewise manner. Furthermore, the pyramid pooling module and channel attention module are applied to aggregate different-region-based global contexts. Extensive evaluations over six benchmark datasets show that the proposed method performs favorably against the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Lihe Zhang and Jie Wu and Tiantian Wang and Ali Borji and Guohua Wei and Huchuan Lu},
  doi          = {10.1109/TIP.2019.2962688},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3534-3545},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multistage refinement network for salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic segmentation with context encoding and multi-path
decoding. <em>TIP</em>, <em>29</em>, 3520–3533. (<a
href="https://doi.org/10.1109/TIP.2019.2962685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image segmentation aims to classify every pixel of a scene image to one of many classes. It implicitly involves object recognition, localization, and boundary delineation. In this paper, we propose a segmentation network called CGBNet to enhance the segmentation performance by context encoding and multi-path decoding. We first propose a context encoding module that generates context-contrasted local feature to make use of the informative context and the discriminative local information. This context encoding module greatly improves the segmentation performance, especially for inconspicuous objects. Furthermore, we propose a scale-selection scheme to selectively fuse the segmentation results from different-scales of features at every spatial position. It adaptively selects appropriate score maps from rich scales of features. To improve the segmentation performance results at boundary, we further propose a boundary delineation module that encourages the location-specific very-low-level features near the boundaries to take part in the final prediction and suppresses them far from the boundaries. The proposed segmentation network achieves very competitive performance in terms of all three different evaluation metrics consistently on the six popular scene segmentation datasets, Pascal Context, SUN-RGBD, Sift Flow, COCO Stuff, ADE20K, and Cityscapes.},
  archive      = {J_TIP},
  author       = {Henghui Ding and Xudong Jiang and Bing Shuai and Ai Qun Liu and Gang Wang},
  doi          = {10.1109/TIP.2019.2962685},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3520-3533},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic segmentation with context encoding and multi-path decoding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image feature correspondence selection: A comparative study
and a new contribution. <em>TIP</em>, <em>29</em>, 3506–3519. (<a
href="https://doi.org/10.1109/TIP.2019.2962678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image feature correspondence selection is pivotal to many computer vision tasks from object recognition to 3D reconstruction. Although many correspondence selection algorithms have been developed in the past decade, there still lacks an in-depth evaluation and comparison in the open literature, which makes it difficult to choose the appropriate algorithm for a specific application. This paper attempts to fill this gap by evaluating eight competing correspondence selection algorithms including both classical methods and current state-of-the-art ones. In addition to preselected correspondences, we have compared different combinations of detector and descriptor on four standard datasets. The diversity of those datasets cover a wide range of uncertainty factors including zoom, rotation, blur, viewpoint change, JPEG compression, light change, different rendering styles and multiple structures. We have measured the quality of competing correspondence selection algorithms in terms of four performance metrics - i.e., precision, recall, F-measure and efficiency. Moreover, we propose to combine the strengths of eight competing methods by combining their correspondence selection results. Extensive experimental results are reported to demonstrate the superiority of several fusion strategies to individual methods, which suggests the possibility of adaptively combining those methods for even better performance.},
  archive      = {J_TIP},
  author       = {Chen Zhao and Zhiguo Cao and Jiaqi Yang and Ke Xian and Xin Li},
  doi          = {10.1109/TIP.2019.2962678},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3506-3519},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image feature correspondence selection: A comparative study and a new contribution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Discriminative multi-view privileged information learning
for image re-ranking. <em>TIP</em>, <em>29</em>, 3490–3505. (<a
href="https://doi.org/10.1109/TIP.2019.2962667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional multi-view re-ranking methods usually perform asymmetrical matching between the region of interest (ROI) in the query image and the whole target image for similarity computation. Due to the inconsistency in the visual appearance, this practice tends to degrade the retrieval accuracy particularly when the image ROI, which is usually interpreted as the image objectness, accounts for a smaller region in the image. Since Privileged Information (PI), which can be viewed as the image prior, is able to characterize well the image objectness, we are aiming at leveraging PI for further improving the performance of multi-view re-ranking in this paper. Towards this end, we propose a discriminative multi-view re-ranking approach in which both the original global image visual contents and the local auxiliary PI features are simultaneously integrated into a unified training framework for generating the latent subspaces with sufficient discriminating power. For the on-the-fly re-ranking, since the multi-view PI features are unavailable, we only project the original multi-view image representations onto the latent subspace, and thus the re-ranking can be achieved by computing and sorting the distances from the multi-view embeddings to the separating hyperplane. Extensive experimental evaluations on the two public benchmarks, Oxford5k and Paris6k, reveal that our approach provides further performance boost for accurate image re-ranking, whilst the comparative study demonstrates the advantage of our method against other multi-view re-ranking methods.},
  archive      = {J_TIP},
  author       = {Jun Li and Chang Xu and Wankou Yang and Changyin Sun and Jianhua Xu and Hong Zhang},
  doi          = {10.1109/TIP.2019.2962667},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3490-3505},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Discriminative multi-view privileged information learning for image re-ranking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D point cloud denoising using graph laplacian
regularization of a low dimensional manifold model. <em>TIP</em>,
<em>29</em>, 3474–3489. (<a
href="https://doi.org/10.1109/TIP.2019.2961429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud—a new signal representation of volumetric objects—is a discrete collection of triples marking exterior object surface locations in 3D space. Conventional imperfect acquisition processes of 3D point cloud—e.g., stereo-matching from multiple viewpoint images or depth data acquired directly from active light sensors—imply non-negligible noise in the data. In this paper, we extend a previously proposed low-dimensional manifold model for the image patches to surface patches in the point cloud, and seek self-similar patches to denoise them simultaneously using the patch manifold prior. Due to discrete observations of the patches on the manifold, we approximate the manifold dimension computation defined in the continuous domain with a patch-based graph Laplacian regularizer, and propose a new discrete patch distance measure to quantify the similarity between two same-sized surface patches for graph construction that is robust to noise. We show that our graph Laplacian regularizer leads to speedy implementation and has desirable numerical stability properties given its natural graph spectral interpretation. Extensive simulation results show that our proposed denoising scheme outperforms state-of-the-art methods in objective metrics and better preserves visually salient structural features like edges.},
  archive      = {J_TIP},
  author       = {Jin Zeng and Gene Cheung and Michael Ng and Jiahao Pang and Cheng Yang},
  doi          = {10.1109/TIP.2019.2961429},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3474-3489},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D point cloud denoising using graph laplacian regularization of a low dimensional manifold model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure and texture-aware image decomposition via training
a neural network. <em>TIP</em>, <em>29</em>, 3458–3473. (<a
href="https://doi.org/10.1109/TIP.2019.2961232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structure-texture image decomposition is a fundamental but challenging topic in computational graphics and image processing. In this paper, we introduce a structure-aware and a texture-aware measures to facilitate the structure-texture decomposition (STD) of images. Edge strengths and spatial scales that have been widely-used in previous STD researches cannot describe the structures and textures of images well. The proposed two measures differentiate image textures from image structures based on their distinctive characteristics. Specifically, the first one aims to measure the anisotropy of local gradients, and the second one is designed to measure the repeatability degree of signal patterns in a neighboring region. Since these two measures describe different properties of image structures and textures, they are complementary to each other. The STD is achieved by optimizing an objective function based on the two new measures. As using traditional optimization methods to solve the optimization problem will require designing different optimizers for different functional spaces, we employ an architecture of deep neural network to optimize the STD cost function in a unified manner. The experimental results demonstrate that, as compared with some state-of-the-art methods, our method can better separate image structure and texture and result in shaper edges in the structural component. Furthermore, to demonstrate the usefulness of the proposed STD method, we have successfully applied it to several applications including detail enhancement, edge detection, and visual quality assessment of super-resolved images.},
  archive      = {J_TIP},
  author       = {Fei Zhou and Qun Chen and Bozhi Liu and Guoping Qiu},
  doi          = {10.1109/TIP.2019.2961232},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3458-3473},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure and texture-aware image decomposition via training a neural network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end optimized ROI image compression. <em>TIP</em>,
<em>29</em>, 3442–3457. (<a
href="https://doi.org/10.1109/TIP.2019.2960869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressing an image with more bits automatically allocated to the region of interest (ROI) than to the background can both protect key information and reduce substantial redundancy. This paper models ROI image compression as an optimization problem of minimizing a weighted sum of the rate of the image and distortion of the ROI. The traditional framework solves this problem by cascading ROI prediction and ROI coding, through which achieving the optimized solution is impossible. To improve coding performance, we propose a novel deep-learning-based unified framework that can achieve rate distortion optimization for ROI compression. Specifically, the proposed framework includes a pair of ROI encoder and decoder convolutional neural networks and a learned entropy codec. The encoder network simultaneously generates multiscale representations that support efficient rate allocation and an implicit ROI mask that guides rate allocation. The proposed framework can automatically complete ROI image compression, and it can be optimized from data in an end-to-end manner. To effectively train the framework by back propagation, we develop a soft-to-hard ROI prediction scheme to make the entire framework differential. To improve visual quality, we propose a hierarchical distortion loss function to protect both pixel-level fidelity for ROI and structural similarity for the entire image. The proposed framework is implemented in two scenarios: salient-target and face-target ROI compression. Comparative experiments demonstrate the advantages of the proposed framework over the traditional framework, including considerably better subjective visual quality, significantly higher objective ROI compression performance and execution efficiency.},
  archive      = {J_TIP},
  author       = {Chunlei Cai and Li Chen and Xiaoyun Zhang and Zhiyong Gao},
  doi          = {10.1109/TIP.2019.2960869},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3442-3457},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {End-to-end optimized ROI image compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HMS-net: Hierarchical multi-scale sparsity-invariant network
for sparse depth completion. <em>TIP</em>, <em>29</em>, 3429–3441. (<a
href="https://doi.org/10.1109/TIP.2019.2960589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense depth cues are important and have wide applications in various computer vision tasks. In autonomous driving, LIDAR sensors are adopted to acquire depth measurements around the vehicle to perceive the surrounding environments. However, depth maps obtained by LIDAR are generally sparse because of its hardware limitation. The task of depth completion attracts increasing attention, which aims at generating a dense depth map from an input sparse depth map. To effectively utilize multi-scale features, we propose three novel sparsity-invariant operations, based on which, a sparsity-invariant multi-scale encoder-decoder network (HMS-Net) for handling sparse inputs and sparse feature maps is also proposed. Additional RGB features could be incorporated to further improve the depth completion performance. Our extensive experiments and component analysis on two public benchmarks, KITTI depth completion benchmark and NYU-depth-v2 dataset, demonstrate the effectiveness of the proposed approach. As of Aug. 12th, 2018, on KITTI depth completion leaderboard, our proposed model without RGB guidance ranks 1st among all peer-reviewed methods without using RGB information, and our model with RGB guidance ranks 2nd among all RGB-guided methods.},
  archive      = {J_TIP},
  author       = {Zixuan Huang and Junming Fan and Shenggan Cheng and Shuai Yi and Xiaogang Wang and Hongsheng Li},
  doi          = {10.1109/TIP.2019.2960589},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3429-3441},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HMS-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CDPM: Convolutional deformable part models for semantically
aligned person re-identification. <em>TIP</em>, <em>29</em>, 3416–3428.
(<a href="https://doi.org/10.1109/TIP.2019.2959923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part-level representations are essential for robust person re-identification. However, common errors that arise during pedestrian detection frequently result in severe misalignment problems for body parts, which degrade the quality of part representations. Accordingly, to deal with this problem, we propose a novel model named Convolutional Deformable Part Models (CDPM). CDPM works by decoupling the complex part alignment procedure into two easier steps: first, a vertical alignment step detects each body part in the vertical direction, with the help of a multi-task learning model; second, a horizontal refinement step based on attention suppresses the background information around each detected body part. Since these two steps are performed orthogonally and sequentially, the difficulty of part alignment is significantly reduced. In the testing stage, CDPM is able to accurately align flexible body parts without any need for outside information. Extensive experimental results demonstrate the effectiveness of the proposed CDPM for part alignment. Most impressively, CDPM achieves state-of-the-art performance on three large-scale datasets: Market-1501, DukeMTMC-ReID, and CUHK03.},
  archive      = {J_TIP},
  author       = {Kan Wang and Changxing Ding and Stephen J. Maybank and Dacheng Tao},
  doi          = {10.1109/TIP.2019.2959923},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3416-3428},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CDPM: Convolutional deformable part models for semantically aligned person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grayscale-thermal tracking via inverse sparse
representation-based collaborative encoding. <em>TIP</em>, <em>29</em>,
3401–3415. (<a href="https://doi.org/10.1109/TIP.2019.2959912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grayscale-thermal tracking has attracted a great deal of attention due to its capability of fusing two different yet complementary target observations. Existing methods often consider extracting the discriminative target information and exploring the target correlation among different images as two separate issues, ignoring their interdependence. This may cause tracking drifts in challenging video pairs. This paper presents a collaborative encoding model called joint correlation and discriminant analysis based inver-sparse representation (JCDA-InvSR) to jointly encode the target candidates in the grayscale and thermal video sequences. In particular, we develop a multi-objective programming to integrate the feature selection and the multi-view correlation analysis into a unified optimization problem in JCDA-InvSR, which can simultaneously highlight the special characters of the grayscale and thermal targets through alternately optimizing two aspects: the target discrimination within a given image and the target correlation across different images. For robust grayscale-thermal tracking, we also incorporate the prior knowledge of target candidate codes into the SVM based target classifier to overcome the overfitting caused by limited training labels. Extensive experiments on GTOT and RGBT234 datasets illustrate the promising performance of our tracking framework.},
  archive      = {J_TIP},
  author       = {Bin Kang and Dong Liang and Wan Ding and Huiyu Zhou and Wei-Ping Zhu},
  doi          = {10.1109/TIP.2019.2959912},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3401-3415},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Grayscale-thermal tracking via inverse sparse representation-based collaborative encoding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multistage GAN for fabric defect detection. <em>TIP</em>,
<em>29</em>, 3388–3400. (<a
href="https://doi.org/10.1109/TIP.2019.2959741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fabric defect detection is an intriguing but challenging topic. Many methods have been proposed for fabric defect detection, but these methods are still suboptimal due to the complex diversity of both fabric textures and defects. In this paper, we propose a generative adversarial network (GAN)-based framework for fabric defect detection. Considering existing challenges in real-world applications, the proposed fabric defect detection system is capable of learning existing fabric defect samples and automatically adapting to different fabric textures during different application periods. Specifically, we customize a deep semantic segmentation network for fabric defect detection that can detect different defect types. Furthermore, we attempted to train a multistage GAN to synthesize reasonable defects in new defect-free samples. First, a texture-conditioned GAN is trained to explore the conditional distribution of defects given different texture backgrounds. Given a novel fabric, we aim to generate reasonable defective patches. Then, a GAN-based fusion network fuses the generated defects to specific locations. Finally, the well-trained multistage GAN continuously updates the existing fabric defect datasets and contributes to the fine-tuning of the semantic segmentation network to better detect defects under different conditions. Comprehensive experiments on various representative fabric samples are conducted to verify the detection performance of our proposed method.},
  archive      = {J_TIP},
  author       = {Juhua Liu and Chaoyue Wang and Hai Su and Bo Du and Dacheng Tao},
  doi          = {10.1109/TIP.2019.2959741},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3388-3400},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multistage GAN for fabric defect detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A grassmannian graph approach to affine invariant feature
matching. <em>TIP</em>, <em>29</em>, 3374–3387. (<a
href="https://doi.org/10.1109/TIP.2019.2959722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a novel, theoretical approach to address one of the longstanding problems in computer vision: 2D and 3D affine invariant feature matching. Our proposed Grassmannian Graph (GrassGraph) framework employs a two stage procedure that is capable of robustly recovering correspondences between two unorganized, affinely related feature (point) sets. In the ideal case, the first stage maps the feature sets to an affine invariant Grassmannian representation, where the features are mapped into the same subspace. It turns out that coordinate representations extracted from the Grassmannian differ by an arbitrary orthonormal matrix. In the second stage, by approximating the Laplace-Beltrami operator (LBO) on these coordinates, this extra orthonormal factor is nullified, providing true affine invariant coordinates which we then utilize to recover correspondences via simple mutual nearest neighbor relations. Our validation benchmarks use large number of experimental trials performed on 2D and 3D datasets. Experimental results show that the proposed Grass-Graph method successfully recovers large affine transformations.},
  archive      = {J_TIP},
  author       = {Mark Moyou and Anand Rangarajan and John Corring and Adrian M. Peter},
  doi          = {10.1109/TIP.2019.2959722},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3374-3387},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A grassmannian graph approach to affine invariant feature matching},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-scale spatial-temporal attention model for person
re-identification in videos. <em>TIP</em>, <em>29</em>, 3365–3373. (<a
href="https://doi.org/10.1109/TIP.2019.2959653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel deep neural network based attention model to learn the representative local regions from a video sequence for person re-identification. Specifically, we propose a multi-scale spatial-temporal attention (MSTA) model to measure the regions of each frame in different scales from the perspective of whole video sequence. Compared to traditional temporal attention models, MSTA focuses on exploiting the importance of local regions of each frame to the whole video representation in both spatial and temporal domains. A new training strategy is designed for the proposed model by incorporating the image-to-image mode with the video-to-video mode. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed model over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Wei Zhang and Xuanyu He and Xiaodong Yu and Weizhi Lu and Zhengjun Zha and Qi Tian},
  doi          = {10.1109/TIP.2019.2959653},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3365-3373},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multi-scale spatial-temporal attention model for person re-identification in videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local semantic siamese networks for fast tracking.
<em>TIP</em>, <em>29</em>, 3351–3364. (<a
href="https://doi.org/10.1109/TIP.2019.2959256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a powerful feature representation is critical for constructing a robust Siamese tracker. However, most existing Siamese trackers learn the global appearance features of the entire object, which usually suffers from drift problems caused by partial occlusion or non-rigid appearance deformation. In this paper, we propose a new Local Semantic Siamese (LSSiam) network to extract more robust features for solving these drift problems, since the local semantic features contain more fine-grained and partial information. We learn the semantic features during offline training by adding a classification branch into the classical Siamese framework. To further enhance the representation of features, we design a generally focal logistic loss to mine the hard negative samples. During the online tracking, we remove the classification branch and propose an efficient template updating strategy to avoid aggressive computing load. Thus, the proposed tracker can run at a high-speed of 100 Frame-per-Second (FPS) far beyond real-time requirement. Extensive experiments on popular benchmarks demonstrate the proposed LSSiam tracker achieves the state-of-the-art performance with a high-speed. Our source code is available at https://github.com/shenjianbing/LSSiam.},
  archive      = {J_TIP},
  author       = {Zhiyuan Liang and Jianbing Shen},
  doi          = {10.1109/TIP.2019.2959256},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3351-3364},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local semantic siamese networks for fast tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage approach to few-shot learning for image
recognition. <em>TIP</em>, <em>29</em>, 3336–3350. (<a
href="https://doi.org/10.1109/TIP.2019.2959254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a multi-layer neural network structure for few-shot image recognition of novel categories. The proposed multi-layer neural network architecture encodes transferable knowledge extracted from a large annotated dataset of base categories. This architecture is then applied to novel categories containing only a few samples. The transfer of knowledge is carried out at the feature-extraction and the classification levels distributed across the two training stages. In the first-training stage, we introduce the relative feature to capture the structure of the data as well as obtain a low-dimensional discriminative space. Secondly, we account for the variable variance of different categories by using a network to predict the variance of each class. Classification is then performed by computing the Mahalanobis distance to the mean-class representation in contrast to previous approaches that used the Euclidean distance. In the second-training stage, a category-agnostic mapping is learned from the mean-sample representation to its corresponding class-prototype representation. This is because the mean-sample representation may not accurately represent the novel category prototype. Finally, we evaluate the proposed network structure on four standard few-shot image recognition datasets, where our proposed few-shot learning system produces competitive performance compared to previous work. We also extensively studied and analyzed the contribution of each component of our proposed framework.},
  archive      = {J_TIP},
  author       = {Debasmit Das and C. S. George Lee},
  doi          = {10.1109/TIP.2019.2959254},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3336-3350},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A two-stage approach to few-shot learning for image recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). RGB-t salient object detection via fusing multi-level CNN
features. <em>TIP</em>, <em>29</em>, 3321–3335. (<a
href="https://doi.org/10.1109/TIP.2019.2959253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-induced salient object detection has recently witnessed substantial progress, which is attributed to the superior feature learning capability of deep convolutional neural networks (CNNs). However, such detections suffer from challenging scenarios characterized by cluttered backgrounds, low-light conditions and variations in illumination. Instead of improving RGB based saliency detection, this paper takes advantage of the complementary benefits of RGB and thermal infrared images. Specifically, we propose a novel end-to-end network for multi-modal salient object detection, which turns the challenge of RGB-T saliency detection to a CNN feature fusion problem. To this end, a backbone network (e.g., VGG-16) is first adopted to extract the coarse features from each RGB or thermal infrared image individually, and then several adjacent-depth feature combination (ADFC) modules are designed to extract multi-level refined features for each single-modal input image, considering that features captured at different depths differ in semantic information and visual details. Subsequently, a multi-branch group fusion (MGF) module is employed to capture the cross-modal features by fusing those features from ADFC modules for a RGB-T image pair at each level. Finally, a joint attention guided bi-directional message passing (JABMP) module undertakes the task of saliency prediction via integrating the multi-level fused features from MGF modules. Experimental results on several public RGB-T salient object detection datasets demonstrate the superiorities of our proposed algorithm over the state-of-the-art approaches, especially under challenging conditions, such as poor illumination, complex background and low contrast.},
  archive      = {J_TIP},
  author       = {Qiang Zhang and Nianchang Huang and Lin Yao and Dingwen Zhang and Caifeng Shan and Jungong Han},
  doi          = {10.1109/TIP.2019.2959253},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3321-3335},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RGB-T salient object detection via fusing multi-level CNN features},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual object tracking via multi-stream deep similarity
learning networks. <em>TIP</em>, <em>29</em>, 3311–3320. (<a
href="https://doi.org/10.1109/TIP.2019.2959249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking remains a challenging research problem because of appearance variations of the object over time, changing cluttered background and requirement for real-time speed. In this paper, we investigate the problem of real-time accurate tracking in a instance-level tracking-by-verification mechanism. We propose a multi-stream deep similarity learning network to learn a similarity comparison model purely off-line. Our loss function encourages the distance between a positive patch and the background patches to be larger than that between the positive patch and the target template. Then, the learned model is directly used to determine the patch in each frame that is most distinctive to the background context and similar to the target template. Within the learned feature space, even if the distance between positive patches becomes large caused by the interference of background clutter, impact from hard distractors from the same class or the appearance change of the target, our method can still distinguish the target robustly using the relative distance. Besides, we also propose a complete framework considering the recovery from failures and the template updating to further improve the tracking performance without taking too much computing resource. Experiments on visual tracking benchmarks show the effectiveness of the proposed tracker when comparing with several recent real-time-speed trackers as well as trackers already included in the benchmarks.},
  archive      = {J_TIP},
  author       = {Kunpeng Li and Yu Kong and Yun Fu},
  doi          = {10.1109/TIP.2019.2959249},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3311-3320},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visual object tracking via multi-stream deep similarity learning networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). RIFT: Multi-modal image matching based on
radiation-variation insensitive feature transform. <em>TIP</em>,
<em>29</em>, 3296–3310. (<a
href="https://doi.org/10.1109/TIP.2019.2959244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional feature matching methods, such as scale-invariant feature transform (SIFT), usually use image intensity or gradient information to detect and describe feature points; however, both intensity and gradient are sensitive to nonlinear radiation distortions (NRD). To solve this problem, this paper proposes a novel feature matching algorithm that is robust to large NRD. The proposed method is called radiation-variation insensitive feature transform (RIFT). There are three main contributions in RIFT. First, RIFT uses phase congruency (PC) instead of image intensity for feature point detection. RIFT considers both the number and repeatability of feature points and detects both corner points and edge points on the PC map. Second, RIFT originally proposes a maximum index map (MIM) for feature description. The MIM is constructed from the log-Gabor convolution sequence and is much more robust to NRD than traditional gradient map. Thus, RIFT not only largely improves the stability of feature detection but also overcomes the limitation of gradient information for feature description. Third, RIFT analyses the inherent influence of rotations on the values of the MIM and realises rotation invariance. We use six different types of multi-modal image datasets to evaluate RIFT, including optical-optical, infrared-optical, synthetic aperture radar (SAR)-optical, depth-optical, map-optical, and day-night datasets. Experimental results show that RIFT is superior to SIFT and SAR-SIFT on multi-modal images. To the best of our knowledge, RIFT is the first feature matching algorithm that can achieve good performance on all the abovementioned types of multi-modal images. The source code of RIFT and the multi-modal image datasets are publicly available.},
  archive      = {J_TIP},
  author       = {Jiayuan Li and Qingwu Hu and Mingyao Ai},
  doi          = {10.1109/TIP.2019.2959244},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3296-3310},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RIFT: Multi-modal image matching based on radiation-variation insensitive feature transform},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction and sampling with local graph transforms for
quasi-lossless light field compression. <em>TIP</em>, <em>29</em>,
3282–3295. (<a href="https://doi.org/10.1109/TIP.2019.2959215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based transforms have been shown to be powerful tools in terms of image energy compaction. However, when the size of the support increases to best capture signal dependencies, the computation of the basis functions becomes rapidly untractable. This problem is in particular compelling for high dimensional imaging data such as light fields. The use of local transforms with limited supports is a way to cope with this computational difficulty. Unfortunately, the locality of the support may not allow us to fully exploit long term signal dependencies present in both the spatial and angular dimensions of light fields. This paper describes sampling and prediction schemes with local graph-based transforms enabling to efficiently compact the signal energy and exploit dependencies beyond the local graph support. The proposed approach is investigated and is shown to be very efficient in the context of spatio-angular transforms for quasi-lossless compression of light fields.},
  archive      = {J_TIP},
  author       = {Mira Rizkallah and Thomas Maugey and Christine Guillemot},
  doi          = {10.1109/TIP.2019.2959215},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3282-3295},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Prediction and sampling with local graph transforms for quasi-lossless light field compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble of deep convolutional neural networks with gabor
face representations for face recognition. <em>TIP</em>, <em>29</em>,
3270–3281. (<a href="https://doi.org/10.1109/TIP.2019.2958404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most DCNN-based FR approaches typically employ grayscale or RGB color images as input representations of DCNN architectures. However, other effective face representation methods have been developed and incorporated into current practical FR systems. In light of this fact, the focus of our study is to employ Gabor face representations in the design of DCNN-based FR frameworks to improve FR performance. To this end, we develop a novel “Gabor DCNN (GDCNN) ensemble” method that effectively applies different and multiple Gabor face representations as inputs during the training and testing phases of a DCNN for FR applications. The proposed GDCNN ensemble method primarily consists of two parts: 1) GDCNN ensemble construction and 2) GDCNN ensemble combination. The goal of the former part is to build an ensemble of GDCNN members (i.e., base models), each learned with a particular type of Gabor face representation. The objective of the latter part is to adaptively combine multiple FR outputs of individual GDCNN members. We perform extensive experiments to evaluate our proposed method on four public face databases (DBs) using the associated standard evaluation protocols. Experimental results demonstrate that our approach exhibits significantly better FR performance than typical DCNN-based approaches that rely only on grayscale or color face images as input representations. In addition, the feasibility of our proposed GDCNN ensemble has been successfully demonstrated by making comparisons with other state-of-the-art DCNN-based FR methods.},
  archive      = {J_TIP},
  author       = {Jae Young Choi and Bumshik Lee},
  doi          = {10.1109/TIP.2019.2958404},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3270-3281},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ensemble of deep convolutional neural networks with gabor face representations for face recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). From rank estimation to rank approximation: Rank residual
constraint for image restoration. <em>TIP</em>, <em>29</em>, 3254–3269.
(<a href="https://doi.org/10.1109/TIP.2019.2958309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel approach to the rank minimization problem, termed rank residual constraint (RRC) model. Different from existing low-rank based approaches, such as the well-known nuclear norm minimization (NNM) and the weighted nuclear norm minimization (WNNM), which estimate the underlying low-rank matrix directly from the corrupted observations, we progressively approximate the underlying low-rank matrix via minimizing the rank residual. Through integrating the image nonlocal self-similarity (NSS) prior with the proposed RRC model, we apply it to image restoration tasks, including image denoising and image compression artifacts reduction. Towards this end, we first obtain a good reference of the original image groups by using the image NSS prior, and then the rank residual of the image groups between this reference and the degraded image is minimized to achieve a better estimate to the desired image. In this manner, both the reference and the estimated image are updated gradually and jointly in each iteration. Based on the group-based sparse representation model, we further provide an analytical investigation on the feasibility of the proposed RRC model. Experimental results demonstrate that the proposed RRC method outperforms many state-of-the-art schemes in both the objective and perceptual quality.},
  archive      = {J_TIP},
  author       = {Zhiyuan Zha and Xin Yuan and Bihan Wen and Jiantao Zhou and Jiachao Zhang and Ce Zhu},
  doi          = {10.1109/TIP.2019.2958309},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3254-3269},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {From rank estimation to rank approximation: Rank residual constraint for image restoration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel retinex-based fractional-order variational model for
images with severely low light. <em>TIP</em>, <em>29</em>, 3239–3253.
(<a href="https://doi.org/10.1109/TIP.2019.2958144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel Retinex-based fractional-order variational model for severely low-light images. The proposed method is more flexible in controlling the regularization extent than the existing integer-order regularization methods. Specifically, we decompose directly in the image domain and perform the fractional-order gradient total variation regularization on both the reflectance component and the illumination component to get more appropriate estimated results. The merits of the proposed method are as follows: 1) small-magnitude details are maintained in the estimated reflectance. 2) illumination components are effectively removed from the estimated reflectance. 3) the estimated illumination is more likely piecewise smooth. We compare the proposed method with other closely related Retinex-based methods. Experimental results demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Zhihao Gu and Fang Li and Faming Fang and Guixu Zhang},
  doi          = {10.1109/TIP.2019.2958144},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3239-3253},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel retinex-based fractional-order variational model for images with severely low light},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter-free gaussian PSF model for extended depth of
field in brightfield microscopy. <em>TIP</em>, <em>29</em>, 3227–3238.
(<a href="https://doi.org/10.1109/TIP.2019.2957941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their limited depth of field, conventional brightfield microscopes cannot image thick specimens entirely in focus. A common way to obtain an all-in-focus image is to acquire a z-stack of images by optically sectioning the specimen and then apply a multi-focus fusion method. Unfortunately, for undersampled image stacks, fusion methods cannot remove the blur in regions where the in-focus position is between two optical sections. In this work, we propose a parameter-free Gaussian PSF model in which the all-in-focus image together with both the depth map and sampling distances in image plane are estimated from the image sequence automatically, without knowledge on the z-stack acquisition. In a maximum a posteriori framework, an iteratively reweighted least squares method is used to estimate the image and an adaptive scaled gradient descent method is utilized to estimate the depth map and sampling distances efficiently. Experiments on synthetic and real data demonstrate that the proposed method outperforms the current state-of-the-art, mitigating fusion artifacts and recovering sharper edges.},
  archive      = {J_TIP},
  author       = {Xu Zhou and Rafael Molina and Yi Ma and Tianfu Wang and Dong Ni},
  doi          = {10.1109/TIP.2019.2957941},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3227-3238},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Parameter-free gaussian PSF model for extended depth of field in brightfield microscopy},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast sparse aperture ISAR autofocusing and imaging via ADMM
based sparse bayesian learning. <em>TIP</em>, <em>29</em>, 3213–3226.
(<a href="https://doi.org/10.1109/TIP.2019.2957939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse aperture ISAR autofocusing and imaging is generally achieved by methods of compressive sensing (CS), or, sparse signal recovery, because non-uniform sampling of sparse aperture disables fast Fourier transform (FFT)-the core of traditional ISAR imaging algorithms. Note that the CS based ISAR autofocusing methods are often computationally heavy to execute, which limits their applications in real-time ISAR systems. The improvement of computational efficiency of sparse aperture ISAR autofocusing is either necessary or at least highly desirable to promote their practical usage. This paper proposes an efficient sparse aperture ISAR autofocusing algorithm. To eliminate the effect of sparse aperture, the ISAR image is reconstructed by sparse Bayesian learning (SBL), and the phase error is estimated by minimum entropy during the reconstruction of ISAR image. However, the computation of expectation in SBL involves a matrix inversion with an intolerable computational complexity of at least O(L 3 ). Here, in the Bayesian inference of SBL, we transform the time-consuming matrix inversion into an element-wise matrix division by the alternating direction method of multipliers (ADMM). An auxiliary variable is introduced to divide the computation of posterior into three simpler subproblems, bringing computational efficiency improvement. Experimental results based on both simulated and measured data validate the effectiveness as well as high efficiency of the proposed algorithm. It is 20-30 times faster than the SBL based sparse aperture ISAR autofocusing approach.},
  archive      = {J_TIP},
  author       = {Shuanghui Zhang and Yongxiang Liu and Xiang Li},
  doi          = {10.1109/TIP.2019.2957939},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3213-3226},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast sparse aperture ISAR autofocusing and imaging via ADMM based sparse bayesian learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Superpixel embedding network. <em>TIP</em>, <em>29</em>,
3199–3212. (<a href="https://doi.org/10.1109/TIP.2019.2957937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixel segmentation is a fundamental computer vision technique that finds application in a multitude of high level computer vision tasks. Most state-of-the-art superpixel segmentation methods are unsupervised in nature and thus cannot fully utilize frequently occurring texture patterns or incorporate multi-scale context. In this paper, we show that superpixel segmentation can be improved by leveraging the superior modeling power of deep convolutional autoencoders in a fully unsupervised manner. We pose the superpixel segmentation problem as one of manifold learning where pixels that belong to similar texture patterns are assigned near identical embedding vectors. The proposed deep network is able to learn image-wide and dataset-wide feature patterns and the relationships between them. This knowledge is used to segment and group pixels in a way that is consistent with a more global definition of pattern coherence. Experiments demonstrate that the superpixels obtained from the embeddings learned by the proposed method outperform the state-of-the-art superpixel segmentation methods for boundary precision and recall values. Additionally, we find that semantic edges obtained from the superpixel embeddings to be significantly better than the contemporary unsupervised approaches.},
  archive      = {J_TIP},
  author       = {Utkarsh Gaur and B. S. Manjunath},
  doi          = {10.1109/TIP.2019.2957937},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3199-3212},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Superpixel embedding network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep unsupervised learning of 3D point clouds via graph
topology inference and filtering. <em>TIP</em>, <em>29</em>, 3183–3198.
(<a href="https://doi.org/10.1109/TIP.2019.2957935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a deep autoencoder with graph topology inference and filtering to achieve compact representations of unorganized 3D point clouds in an unsupervised manner. Many previous works discretize 3D points to voxels and then use lattice-based methods to process and learn 3D spatial information; however, this leads to inevitable discretization errors. In this work, we try to handle raw 3D points without such compromise. The proposed networks follow the autoencoder framework with a focus on designing the decoder. The encoder of the proposed networks adopts similar architectures as in PointNet, which is a well-acknowledged method for supervised learning of 3D point clouds. The decoder of the proposed networks involves three novel modules: the folding module, the graph-topology-inference module, and the graph-filtering module. The folding module folds a canonical 2D lattice to the underlying surface of a 3D point cloud, achieving coarse reconstruction; the graph-topology-inference module learns a graph topology to represent pairwise relationships between 3D points, pushing the latent code to preserve both coordinates and pairwise relationships of points in 3D point clouds; and the graph-filtering module couples the above two modules, refining the coarse reconstruction through a learnt graph topology to obtain the final reconstruction. The proposed decoder leverages a learnable graph topology to push the codeword to preserve representative features and further improve the unsupervised-learning performance. We further provide theoretical analyses of the proposed architecture. We provide an upper bound for the reconstruction loss and further show the superiority of graph smoothness over spatial smoothness as a prior to model 3D point clouds. In the experiments, we validate the proposed networks in three tasks, including 3D point cloud reconstruction, visualization, and transfer classification. The experimental results show that (1) the proposed networks outperform the state-of-the-art methods in various tasks, including reconstruction and transfer classification; (2) a graph topology can be inferred as auxiliary information without specific supervision on graph topology inference; (3) graph filtering refines the reconstruction, leading to better performances; and (4) designing a powerful decoder could improve the unsupervised-learning performance, just like a powerful encoder.},
  archive      = {J_TIP},
  author       = {Siheng Chen and Chaojing Duan and Yaoqing Yang and Duanshun Li and Chen Feng and Dong Tian},
  doi          = {10.1109/TIP.2019.2957935},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3183-3198},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep unsupervised learning of 3D point clouds via graph topology inference and filtering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep image-to-video adaptation and fusion networks for
action recognition. <em>TIP</em>, <em>29</em>, 3168–3182. (<a
href="https://doi.org/10.1109/TIP.2019.2957930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning methods for action recognition in videos require a large number of labeled videos for training, which is labor-intensive and time-consuming. For the same action, the knowledge learned from different media types, e.g., videos and images, may be related and complementary. However, due to the domain shifts and heterogeneous feature representations between videos and images, the performance of classifiers trained on images may be dramatically degraded when directly deployed to videos. In this paper, we propose a novel method, named Deep Image-to-Video Adaptation and Fusion Networks (DIVAFN), to enhance action recognition in videos by transferring knowledge from images using video keyframes as a bridge. The DIVAFN is a unified deep learning model, which integrates domain-invariant representations learning and cross-modal feature fusion into a unified optimization framework. Specifically, we design an efficient cross-modal similarities metric to reduce the modality shift among images, keyframes and videos. Then, we adopt an autoencoder architecture, whose hidden layer is constrained to be the semantic representations of the action class names. In this way, when the autoencoder is adopted to project the learned features from different domains to the same space, more compact, informative and discriminative representations can be obtained. Finally, the concatenation of the learned semantic feature representations from these three autoencoders are used to train the classifier for action recognition in videos. Comprehensive experiments on four real-world datasets show that our method outperforms some state-of-the-art domain adaptation and action recognition methods.},
  archive      = {J_TIP},
  author       = {Yang Liu and Zhaoyang Lu and Jing Li and Tao Yang and Chao Yao},
  doi          = {10.1109/TIP.2019.2957930},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3168-3182},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep image-to-video adaptation and fusion networks for action recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale deep residual learning-based single image haze
removal via image decomposition. <em>TIP</em>, <em>29</em>, 3153–3167.
(<a href="https://doi.org/10.1109/TIP.2019.2957929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images/videos captured from outdoor visual devices are usually degraded by turbid media, such as haze, smoke, fog, rain, and snow. Haze is the most common one in outdoor scenes due to the atmosphere conditions. In this paper, a novel deep learning-based architecture (denoted by MSRL-DehazeNet) for single image haze removal relying on multi-scale residual learning (MSRL) and image decomposition is proposed. Instead of learning an end-to-end mapping between each pair of hazy image and its corresponding haze-free one adopted by most existing learning-based approaches, we reformulate the problem as restoration of the image base component. Based on the decomposition of a hazy image into the base and the detail components, haze removal (or dehazing) can be achieved by both of our multi-scale deep residual learning and our simplified U-Net learning only for mapping between hazy and haze-free base components, while the detail component is further enhanced via the other learned convolutional neural network (CNN). Moreover, benefited by the basic building block of our deep residual CNN architecture and our simplified U-Net structure, the feature maps (produced by extracting structural and statistical features), and each previous layer can be fully preserved and fed into the next layer. Therefore, possible color distortion in the recovered image would be avoided. As a result, the final haze-removed (or dehazed) image is obtained by integrating the haze-removed base and the enhanced detail image components. Experimental results have demonstrated good effectiveness of the proposed framework, compared with state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Chia-Hung Yeh and Chih-Hsiang Huang and Li-Wei Kang},
  doi          = {10.1109/TIP.2019.2957929},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3153-3167},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale deep residual learning-based single image haze removal via image decomposition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Taking a look at small-scale pedestrians and occluded
pedestrians. <em>TIP</em>, <em>29</em>, 3143–3152. (<a
href="https://doi.org/10.1109/TIP.2019.2957927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-scale pedestrian detection and occluded pedestrian detection are two challenging tasks. However, most state-of-the-art methods merely handle one single task each time, thus giving rise to relatively poor performance when the two tasks, in practice, are required simultaneously. In this paper, it is found that small-scale pedestrian detection and occluded pedestrian detection actually have a common problem, i.e., an inaccurate location problem. Therefore, solving this problem enables to improve the performance of both tasks. To this end, we pay more attention to the predicted bounding box with worse location precision and extract more contextual information around objects, where two modules (i.e., location bootstrap and semantic transition) are proposed. The location bootstrap is used to reweight regression loss, where the loss of the predicted bounding box far from the corresponding ground-truth is upweighted and the loss of the predicted bounding box near the corresponding ground-truth is downweighted. Additionally, the semantic transition adds more contextual information and relieves semantic inconsistency of the skip-layer fusion. Since the location bootstrap is not used at the test stage and the semantic transition is lightweight, the proposed method does not add many extra computational costs during inference. Experiments on the challenging CityPersons and Caltech datasets show that the proposed method outperforms the state-of-the-art methods on the small-scale pedestrians and occluded pedestrians (e.g., 5.20\% and 4.73\% improvements on the Caltech).},
  archive      = {J_TIP},
  author       = {Jiale Cao and Yanwei Pang and Jungong Han and Bolin Gao and Xuelong Li},
  doi          = {10.1109/TIP.2019.2957927},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3143-3152},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Taking a look at small-scale pedestrians and occluded pedestrians},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank matrix recovery via modified schatten- <span
class="math inline"><em>p</em></span> norm minimization with convergence
guarantees. <em>TIP</em>, <em>29</em>, 3132–3142. (<a
href="https://doi.org/10.1109/TIP.2019.2957925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, low-rank matrix recovery problems have attracted much attention in computer vision and machine learning. The corresponding rank minimization problems are both combinational and NP-hard in general, which are mainly solved by both nuclear norm and Schatten-p (0 &lt;; p &lt;; 1) norm based optimization algorithms. However, inspired by weighted nuclear norm and Schatten-p norm as the relaxations of rank function, the main merits of this work firstly provide a modified Schatten-p norm in the affine matrix rank minimization problem, denoted as the modified Schatten-p norm minimization (MSpNM). Secondly, its surrogate function is constructed and the equivalence relationship with the MSpNM is further achieved. Thirdly, the iterative singular value thresholding algorithm (ISVTA) is devised to optimize it, and its accelerated version, i.e., AISVTA, is also obtained to reduce the number of iterations through the well-known Nesterov&#39;s acceleration strategy. Most importantly, the convergence guarantees and their relationship with objective function, stationary point and variable sequence generated by the proposed algorithms are established under some specific assumptions, e.g., Kurdyka-Łojasiewicz (KŁ) property. Finally, numerical experiments demonstrate the effectiveness of the proposed algorithms in the matrix completion problem for image inpainting and recommender systems. It should be noted that the accelerated algorithm has a much faster convergence speed and a very close recovery precision when comparing with the proposed non-accelerated one.},
  archive      = {J_TIP},
  author       = {Hengmin Zhang and Jianjun Qian and Bob Zhang and Jian Yang and Chen Gong and Yang Wei},
  doi          = {10.1109/TIP.2019.2957925},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3132-3142},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low-rank matrix recovery via modified schatten- $p$ norm minimization with convergence guarantees},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning non-local spatial correlations to restore sparse 3D
single-photon data. <em>TIP</em>, <em>29</em>, 3119–3131. (<a
href="https://doi.org/10.1109/TIP.2019.2957918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new algorithm for the learning of spatial correlation and non-local restoration of single-photon 3D Lidar images acquired in the photon starved regime (fewer or less than one photon per pixel) or with a reduced number of scanned spatial points (pixels). The algorithm alternates between three steps: (i) extract multi-scale information, (ii) build a robust graph of non-local spatial correlations between pixels, and (iii) the restoration of depth and reflectivity images. A non-uniform sampling approach, which assigns larger patches to homogeneous regions and smaller ones to heterogeneous regions, is adopted to reduce the computational cost associated with the graph. The restoration of the 3D images is achieved by minimizing a cost function accounting for the multi-scale information and the non-local spatial correlation between patches. This minimization problem is efficiently solved using the alternating direction method of multipliers (ADMM) that presents fast convergence properties. Various results based on simulated and real Lidar data show the benefits of the proposed algorithm that improves the quality of the estimated depth and reflectivity images, especially in the photon-starved regime or when containing a reduced number of spatial points.},
  archive      = {J_TIP},
  author       = {Songmao Chen and Abderrahim Halimi and Ximing Ren and Aongus McCarthy and Xiuqin Su and Stephen McLaughlin and Gerald S. Buller},
  doi          = {10.1109/TIP.2019.2957918},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3119-3131},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning non-local spatial correlations to restore sparse 3D single-photon data},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IDGCP: Image dehazing based on gamma correction prior.
<em>TIP</em>, <em>29</em>, 3104–3118. (<a
href="https://doi.org/10.1109/TIP.2019.2957852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel and effective image prior, i.e., gamma correction prior (GCP), which leads to an efficient image dehazing method, i.e., IDGCP. A step-by-step procedure of the proposed IDGCP is as follows. First, an input hazy image is preprocessed by the proposed GCP, resulting in a homogeneous virtual transformation of the hazy image. Then, from the original input hazy image and its virtual transformation, the depth ratio is extracted based on atmospheric scattering theory. Finally, a “global-wise” strategy and a vision indicator are employed to recover the scene albedo, thus restoring the hazy image. Unlike other image dehazing methods, IDGCP is based on the “global-wise” strategy, and it only needs to determine one unknown constant without any refining process to attain a high-quality restoration, thereby leading to significantly reduced processing time and computation cost. Each step of IDGCP is tested experimentally to validate its robustness. Moreover, a series of experiments are conducted on a number of challenging images with IDGCP and other state-of-the-art technologies, demonstrating the superiority of IDGCP over the others in terms of restoration quality and implementation efficiency.},
  archive      = {J_TIP},
  author       = {Mingye Ju and Can Ding and Y. Jay Guo and Dengyin Zhang},
  doi          = {10.1109/TIP.2019.2957852},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3104-3118},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IDGCP: Image dehazing based on gamma correction prior},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HAR-net: Joint learning of hybrid attention for single-stage
object detection. <em>TIP</em>, <em>29</em>, 3092–3103. (<a
href="https://doi.org/10.1109/TIP.2019.2957850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has been a challenging task in computer vision. Although significant progress has been made in object detection with deep neural networks, the attention mechanism has yet to be fully developed. In this paper, we propose a hybrid attention mechanism for single-stage object detection. First, we present the modules of spatial attention, channel attention and aligned attention for single-stage object detection. In particular, dilated convolution layers with symmetrically fixed rates are stacked to learn spatial attention. A channel attention mechanism with the cross-level group normalization and squeeze-and-excitation operation is proposed. Aligned attention is constructed with organized deformable filters. Second, the three types of attention are unified to construct the hybrid attention mechanism. We then plug the hybrid attention into Retina-Net and propose the efficient single-stage HAR-Net for object detection. The attention modules and the proposed HAR-Net are evaluated on the COCO detection dataset. The experiments demonstrate that hybrid attention can significantly improve the detection accuracy and that the HAR-Net can achieve a state-of-the-art 45.8\% mAP, thus outperforming existing single-stage object detectors.},
  archive      = {J_TIP},
  author       = {Ya-Li Li and Shengjin Wang},
  doi          = {10.1109/TIP.2019.2957850},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3092-3103},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HAR-net: Joint learning of hybrid attention for single-stage object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Corrections to “pothole detection based on disparity
transformation and road surface modeling” [aug 19 897-908].
<em>TIP</em>, <em>29</em>, 3091. (<a
href="https://doi.org/10.1109/TIP.2019.2957622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unfortunately, we made two minor mistakes in the above paper. First of all, the first graph on row (c) in Fig. 11 was same as the third graph on row (c) in Fig. 11 . Secondly, “precision” and “recall” in Table III need to be switched. The correct figure and table have no influence on the discussion and conclusions in the above paper, and they are given here.},
  archive      = {J_TIP},
  author       = {Ru Fan and Umar Ozgunalp and Brett Hosking and Ming Liu and Ioannis Pitas},
  doi          = {10.1109/TIP.2019.2957622},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3091},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Corrections to “Pothole detection based on disparity transformation and road surface modeling” [Aug 19 897-908]},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-enhanced convolutional network for facial video
hallucination. <em>TIP</em>, <em>29</em>, 3078–3090. (<a
href="https://doi.org/10.1109/TIP.2019.2955640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a domain-specific super-resolution problem, facial image hallucination has enjoyed a series of breakthroughs thanks to the advances of deep convolutional neural networks. However, the direct migration of existing methods to video is still difficult to achieve good performance due to its lack of alignment and consistency modelling in temporal domain. Taking advantage of high inter-frame dependency in videos, we propose a self-enhanced convolutional network for facial video hallucination. It is implemented by making full usage of preceding super-resolved frames and a temporal window of adjacent low-resolution frames. Specifically, the algorithm first obtains the initial high-resolution inference of each frame by taking into consideration a sequence of consecutive low-resolution inputs through temporal consistency modelling. It further recurrently exploits the reconstructed results and intermediate features of a sequence of preceding frames to improve the initial super-resolution of the current frame by modelling the coherence of structural facial features across frames. Quantitative and qualitative evaluations demonstrate the superiority of the proposed algorithm against state-of-the-art methods. Moreover, our algorithm also achieves excellent performance in the task of general video super-resolution in a single-shot setting.},
  archive      = {J_TIP},
  author       = {Chaowei Fang and Guanbin Li and Xiaoguang Han and Yizhou Yu},
  doi          = {10.1109/TIP.2019.2955640},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3078-3090},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-enhanced convolutional network for facial video hallucination},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time burst photo selection using a light-head
adversarial network. <em>TIP</em>, <em>29</em>, 3065–3077. (<a
href="https://doi.org/10.1109/TIP.2019.2955563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an automatic moment capture system that runs in real-time on mobile cameras. The system is designed to run in the viewfinder mode and capture a burst sequence of frames before and after the shutter is pressed. For each frame, the system predicts in real-time a goodness score, based on which the best moment in the burst can be selected immediately after the shutter is released. We develop a highly efficient deep neural network ranking model, which implicitly learns a latent relative attribute space to capture subtle visual differences within a sequence of burst images. The overall goodness is computed as a linear aggregation of the goodnesses of all the latent attributes. To obtain a compact model which can run on mobile devices in real-time, we have explored and evaluated a wide range of network design choices, taking into account the constraints of model size, computational cost, and accuracy. Extensive studies show that the best frame predicted by our model hit users’ top-1 (out of 11 on average) choice for 64.1\% cases and top-3 choices for 86.2\% cases. Moreover, the model (only 0.47M Bytes) can run in real time on mobile devices, e.g. 13ms on iPhone 7.},
  archive      = {J_TIP},
  author       = {Baoyuan Wang and Noranart Vesdapunt and Utkarsh Sinha and Lei Zhang},
  doi          = {10.1109/TIP.2019.2955563},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3065-3077},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real-time burst photo selection using a light-head adversarial network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tangent fisher vector on matrix manifolds for action
recognition. <em>TIP</em>, <em>29</em>, 3052–3064. (<a
href="https://doi.org/10.1109/TIP.2019.2955561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of representing and recognizing human actions from videos on matrix manifolds. For this purpose, we propose a new vector representation method, named tangent Fisher vector, to describe video sequences in the Fisher kernel framework. We first extract dense curved spatio-temporal cuboids from each video sequence. Compared with the traditional &#39;straight cuboids&#39;, the dense curved spatio-temporal cuboids contain much more local motion information. Each cuboid is then described using a linear dynamical system (LDS) to simultaneously capture the local appearance and dynamics. Furthermore, a simple yet efficient algorithm is proposed to learn the LDS parameters and approximate the observability matrix at the same time. Each video sequence is thus represented by a set of LDSs. Considering that each LDS can be viewed as a point in a Grassmann manifold, we propose to learn an intrinsic GMM on the manifold to cluster the LDS points. Finally a tangent Fisher vector is computed by first accumulating all the tangent vectors in each Gaussian component, and then concatenating the normalized results across all the Gaussian components. A kernel is defined to measure the similarity between tangent Fisher vectors for classification and recognition of a video sequence. This approach is evaluated on the state-of-the-art human action benchmark datasets. The recognition performance is competitive when compared with current state-of-the-art results.},
  archive      = {J_TIP},
  author       = {Guan Luo and Jiutong Wei and Weiming Hu and Stephen J. Maybank},
  doi          = {10.1109/TIP.2019.2955561},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3052-3064},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Tangent fisher vector on matrix manifolds for action recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Bi-directional dermoscopic feature learning and multi-scale
consistent decision fusion for skin lesion segmentation. <em>TIP</em>,
<em>29</em>, 3039–3051. (<a
href="https://doi.org/10.1109/TIP.2019.2955297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of skin lesion from dermoscopic images is a crucial part of computer-aided diagnosis of melanoma. It is challenging due to the fact that dermoscopic images from different patients have non-negligible lesion variation, which causes difficulties in anatomical structure learning and consistent skin lesion delineation. In this paper, we propose a novel bi-directional dermoscopic feature learning (biDFL) framework to model the complex correlation between skin lesions and their informative context. By controlling feature information passing through two complementary directions, a substantially rich and discriminative feature representation is achieved. Specifically, we place biDFL module on the top of a CNN network to enhance high-level parsing performance. Furthermore, we propose a multi-scale consistent decision fusion (mCDF) that is capable of selectively focusing on the informative decisions generated from multiple classification layers. By analysis of the consistency of the decision at each position, mCDF automatically adjusts the reliability of decisions and thus allows a more insightful skin lesion delineation. The comprehensive experimental results show the effectiveness of the proposed method on skin lesion segmentation, achieving state-of-the-art performance consistently on two publicly available dermoscopic image databases.},
  archive      = {J_TIP},
  author       = {Xiaohong Wang and Xudong Jiang and Henghui Ding and Jun Liu},
  doi          = {10.1109/TIP.2019.2955297},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3039-3051},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bi-directional dermoscopic feature learning and multi-scale consistent decision fusion for skin lesion segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New design for compact color screen sets for high-end
digital color press. <em>TIP</em>, <em>29</em>, 3023–3038. (<a
href="https://doi.org/10.1109/TIP.2019.2955295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital halftoning is an essential part of the process for printing color, continuous-tone content. Traditionally, the highest quality has been achieved with analog, offset lithographic presses, using color screen sets that yield periodic, clustered-dot halftone patterns. Increasingly, these systems are being supplanted by digital presses that are based on either electrophotographic or inkjet marking processes. Due to the inherent instability of the electrophotographic marking process, periodic, clustered-dot halftone patterns are also widely used with such presses. However, digital presses have much lower resolution than their analog counterparts. Simply mimicking the traditional screen designs used with commercial, offset presses will result in halftone patterns that are more susceptible to moire due to the interaction between the periodic patterns used to render the different color channels. This causes instability in the printed colors. The moire can be reduced by increasing the frequency of the halftone patterns. But this may make the print appear grainier than its analog counterpart. In this paper, we introduce a principled design procedure that allows one to design color screen sets that generate periodic, clustered-dot halftone patterns that improve color stability without increasing graininess. We present experimental results to support the benefits of our new color screen set design framework.},
  archive      = {J_TIP},
  author       = {Tal Frank and Oren Haik and Altyngul Jumabayeva and Jan P. Allebach and Yitzhak Yitzhaky},
  doi          = {10.1109/TIP.2019.2955295},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3023-3038},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {New design for compact color screen sets for high-end digital color press},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A metric for video blending quality assessment.
<em>TIP</em>, <em>29</em>, 3014–3022. (<a
href="https://doi.org/10.1109/TIP.2019.2955294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an objective approach to assess the quality of video blending. Blending is a fundamental operation in video editing, which can smooth the intensity changes of relevant regions. However blending also generates artefacts such as bleeding and ghosting. To assess the quality of the blended videos, our approach considers the illuminance consistency as a positive aspect while regard the artefacts as a negative aspect. Temporal coherence between frames is also considered. We evaluate our metric on a video blending dataset where the results of subjective evaluation are available. Experimental results validate the effectiveness of our proposed metric, and shows that this metric gives superior performance over existing video quality metrics.},
  archive      = {J_TIP},
  author       = {Zhe Zhu and Hantao Liu and Jiaming Lu and Shi-Min Hu},
  doi          = {10.1109/TIP.2019.2955294},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3014-3022},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A metric for video blending quality assessment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selective spatial regularization by reinforcement learned
decision making for object tracking. <em>TIP</em>, <em>29</em>,
2999–3013. (<a href="https://doi.org/10.1109/TIP.2019.2955292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial regularization (SR) is known as an effective tool to alleviate the boundary effect of correlation filter (CF), a successful visual object tracking scheme, from which a number of state-of-the-art visual object trackers can be stemmed. Nevertheless, SR highly increases the optimization complexity of CF and its target-driven nature makes spatially-regularized CF trackers may easily lose the occluded targets or the targets surrounded by other similar objects. In this paper, we propose selective spatial regularization (SSR) for CF-tracking scheme. It can achieve not only higher accuracy and robustness, but also higher speed compared with spatially-regularized CF trackers. Specifically, rather than simply relying on foreground information, we extend the objective function of CF tracking scheme to learn the target-context-regularized filters using target-context-driven weight maps. We then formulate the online selection of these weight maps as a decision making problem by a Markov Decision Process (MDP), where the learning of weight map selection is equivalent to policy learning of the MDP that is solved by a reinforcement learning strategy. Moreover, by adding a special state, representing not-updating filters, in the MDP, we can learn when to skip unnecessary or erroneous filter updating, thus accelerating the online tracking. Finally, the proposed SSR is used to equip three popular spatially-regularized CF trackers to significantly boost their tracking accuracy, while achieving much faster online tracking speed. Besides, extensive experiments on five benchmarks validate the effectiveness of SSR.},
  archive      = {J_TIP},
  author       = {Qing Guo and Ruize Han and Wei Feng and Zhihao Chen and Liang Wan},
  doi          = {10.1109/TIP.2019.2955292},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2999-3013},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Selective spatial regularization by reinforcement learned decision making for object tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realistic film noise generation based on experimental noise
spectra. <em>TIP</em>, <em>29</em>, 2987–2998. (<a
href="https://doi.org/10.1109/TIP.2019.2955284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating 2D noise with local, space-varying spectral characteristics is vital where random noise fields with spatially heterogeneous statistical properties are observed and need to be simulated. A realistic, non-stationary noise generator relying on experimental data is presented. That generator is desired in areas such as photography and radiography. For example, before performing actual X-ray imaging in practice, output images are simulated to assess and improve setups. For that purpose, realistic film noise modelling is crucial because noise downgrades the detectability of visual signals. The presented film noise synthesiser improves the realism and value of radiographic simulations significantly, allowing more realistic assessments of radiographic test setups. The method respects space-varying spectral characteristics and probability distributions, locally simulating noise with realistic granularity and contrast. The benefits of this approach are to respect the correlation between noise and image as well as internal correlation, the fast generation of any number of unique noise samples, the exploitation of real experimental data, and its statistical non-stationarity. The combination of these benefits is not available in existing work. Validation of the new technique was undertaken in the field of industrial radiography. While applied to that field here, the technique is general and can also be utilised in any other field where the generation of 2D noise with local, space-varying statistical properties is necessary.},
  archive      = {J_TIP},
  author       = {Sebastian Eckel and Peter Huthwaite and Uwe Zscherpel and Andreas Schumm and Nicolas Paul},
  doi          = {10.1109/TIP.2019.2955284},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2987-2998},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Realistic film noise generation based on experimental noise spectra},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). SRHandNet: Real-time 2D hand pose estimation with
simultaneous region localization. <em>TIP</em>, <em>29</em>, 2977–2986.
(<a href="https://doi.org/10.1109/TIP.2019.2955280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel method for real-time 2D hand pose estimation from monocular color images, which is named as SRHandNet. Existing methods can not time efficiently obtain appropriate results for small hand. Our key idea is to simultaneously regress the hand region of interests (RoIs) and hand keypoints for a given color image, and iteratively take the hand RoIs as feedback information for boosting the performance of hand keypoints estimation with a single encoder-decoder network architecture. Different from previous region proposal network (RPN), a new lightweight bounding box representation, which is called region map, is proposed. The proposed bounding box representation map together with hand keypoints heatmaps are combined into the unified multi-channel feature maps, which can be easily acquired with only one forward network inference and thus improve the runtime efficiency of the network. Our proposed SRHandNet can run at 40fps for hand bounding box detection and up to 30fps accurate hand keypoints estimation under the desktop environment without implementation optimization. Experiments demonstrate the effectiveness of the proposed method. State-of-the-art results are also achieved out competing all recent methods.},
  archive      = {J_TIP},
  author       = {Yangang Wang and Baowen Zhang and Cong Peng},
  doi          = {10.1109/TIP.2019.2955280},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2977-2986},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SRHandNet: Real-time 2D hand pose estimation with simultaneous region localization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optical-and-radar image fusion for dynamic estimation of
spin satellites. <em>TIP</em>, <em>29</em>, 2963–2976. (<a
href="https://doi.org/10.1109/TIP.2019.2955248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more and more satellites are launched into the space, dynamic estimation of spin satellites has become a critical component of the space situation awareness application. Some explored studies using exterior measurements from different sensors such as optical device and inverse synthetic aperture radar (ISAR) to estimate dynamic parameters of spin satellites. As a single sensor normally provides two-dimensional observation, three-dimensional estimations resulting from these algorithms are strictly related to the prior knowledge of targets characteristics. As a result, it is difficult to expand these methods to other satellites. In order to support the dynamic estimation of most spin satellites, this paper presents a novel dynamic estimation approach which employs synchronized optical-and-radar images. The optical-and-radar fusion strategy has demonstrated its superiority in image analysis field, and breaks down the dynamic estimation of spin satellites into two sub-problems: target attitude estimation and spin parameters estimation. In this work, the proposed algorithm deduces two explicit expressions of target dynamic parameters under the imaging projection model of the joint optical-and-radar observation. Through the particle swarm optimization (PSO), target dynamic parameters are determined in two stages. This paper presents some experiments illustrating the feasibility of the proposed method and subsequent conclusions, which reflect advantages of the joint optical-and-radar observation mode in image interpretation.},
  archive      = {J_TIP},
  author       = {Yejian Zhou and Lei Zhang and Yunhe Cao and Yan Huang},
  doi          = {10.1109/TIP.2019.2955248},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2963-2976},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optical-and-radar image fusion for dynamic estimation of spin satellites},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Notice of violation of IEEE publication principles: Recent
advances in 3D object detection in the era of deep neural networks: A
survey. <em>TIP</em>, <em>29</em>, 2947–2962. (<a
href="https://doi.org/10.1109/TIP.2019.2955239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Notice of Violation of IEEE Publication Principles &quot;Recent Advances in 3D Object Detection in the Era of Deep Neural Networks: A Survey,&quot; by M. M. Rahman, Y. Tan, J. Xue and K. Lu, in IEEE Transactions on Image Processing, vol. 29, 2020, pp. 2947-2962 After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE’s Publication Principles. This paper contains portions of text from the paper cited below that were paraphrased without attribution. &quot;A Survey on 3D Object Detection Methods for Autonomous Driving Applications,&quot; by E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby and A. Mouzakitis, in IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 10, Oct. 2019, pp. 3782-3795 With the rapid development of deep learning technology and other powerful tools, 3D object detection has made great progress and become one of the fastest growing field in computer vision. Many automated applications such as robotic navigation, autonomous driving, and virtual or augmented reality system require estimation of accurate 3D object location and detection. Under this requirement, many methods have been proposed to improve the performance of 3D object localization and detection. Despite recent efforts, 3D object detection is still a very challenging task due to occlusion, viewpoint variations, scale changes, and limited information in 3D scenes. In this paper, we present a comprehensive review of recent state-of-the-art approaches in 3D object detection technology. We start with some basic concepts, then describe some of the available datasets that are designed to facilitate the performance evaluation of 3D object detection algorithms. Next, we will review the state-of-the-art technologies in this area, highlighting their contributions, importance, and limitations as a guide for future research. Finally, we provide a quantitative comparison of the results of the state-of-the-art methods on the popular public datasets.},
  archive      = {J_TIP},
  author       = {Mohammad Muntasir Rahman and Yanhao Tan and Jian Xue and Ke Lu},
  doi          = {10.1109/TIP.2019.2955239},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2947-2962},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Notice of violation of IEEE publication principles: recent advances in 3D object detection in the era of deep neural networks: a survey},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extended coding unit partitioning for future video coding.
<em>TIP</em>, <em>29</em>, 2931–2946. (<a
href="https://doi.org/10.1109/TIP.2019.2955238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flexible partitioning structure such as quad-tree plus binary-tree (QTBT) plays important roles in the next generation video coding standard. This paper investigates new coding unit (CU) partitioning methods to further improve the compression efficiency. In particular, we propose the extended quad-tree partitioning and asymmetric ternary-tree partitioning to further develop the principle behind the existing partitioning structure. First, a central extended quad-tree (CENTRAL-EQT) partitioning is introduced, which extends the traditional QT partitioning with central pattern and generates four elaborately designed sub-CUs with different sizes. Second, we propose a parallel extended quad-tree (PARALLEL-EQT) partitioning that allows the CU to split along a single direction, leading to four identical size sub-blocks. Third, we present an asymmetric ternary-tree (ATT) method, which splits the CU asymmetrically into three sub-blocks. The proposed new partitioning methods allow the interleaving with binary-tree partitioning for enhanced adaptability, and they can be jointly enabled to capture different characteristics of the local content. Simulation results on the JEM7-QTBT-Only platform show that averagely 4.92\%, 4.81\% and 5.08\% BD-Rate savings are achieved for the luma component by the proposed methods under random access (RA), low-delay P (LDP) and low-delay B (LDB) configurations, respectively, with around 725\% encoding time increase and negligible decoding time increase. Furthermore, experimental results also reveal that the proposed partitioning schemes are effective when cooperating with the upcoming Versatile Video Coding standard.},
  archive      = {J_TIP},
  author       = {Meng Wang and Junru Li and Li Zhang and Kai Zhang and Hongbin Liu and Shiqi Wang and Sam Kwong and Siwei Ma},
  doi          = {10.1109/TIP.2019.2955238},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2931-2946},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Extended coding unit partitioning for future video coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Arbitrarily shaped scene text detection with a mask
tightness text detector. <em>TIP</em>, <em>29</em>, 2918–2930. (<a
href="https://doi.org/10.1109/TIP.2019.2954218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text in the environment is complicated. It can exist in arbitrary text fonts, sizes or shapes. Although scene text detection has witnessed considerable progress in recent years, the detection of text with complex shapes, especially curved text, remains challenging. Datasets with adequate samples to overcome the problem presented by curved text (or other irregularly shaped text) have been introduced only recently; however, the performance of the reported methods on these datasets is unsatisfactory. Therefore, detecting arbitrarily shaped text remains a challenging. This motivated us to propose the Mask Tightness Text Detector (Mask TTD) to improve text detection performance. Mask TTD uses a tightness prior and text frontier learning to enhance pixel-wise mask prediction. In addition, it achieves mutual promotion by integrating a branch for the polygonal boundary of each text region, which significantly improves the detection performance of arbitrarily shaped text. Experiments demonstrate that Mask TTD can achieve state-of-the-art performance on existing curved text datasets (CTW1500, Total-text, and CUTE80) and three common benchmark datasets (RCTW-17, MSRA-TD500, and ICDAR 2015). It is worth mentioning that on CTW1500, our method can outperform previous methods, especially at higher intersection over union (IoU) thresholds (16\% higher than the next-best method with an IoU threshold of 0.8), which demonstrates its potential for tight text detection. Moreover, on the largest Chinese-based dataset RCTW-17, Mask TTD outperforms other methods by a large margin in terms of both the Average Precision and F-measure, showing its powerful generalization ability.},
  archive      = {J_TIP},
  author       = {Yuliang Liu and Lianwen Jin and Chuanming Fang},
  doi          = {10.1109/TIP.2019.2954218},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2918-2930},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Arbitrarily shaped scene text detection with a mask tightness text detector},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Super diffusion for salient object detection. <em>TIP</em>,
<em>29</em>, 2903–2917. (<a
href="https://doi.org/10.1109/TIP.2019.2954209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One major branch of saliency object detection methods are diffusion-based which construct a graph model on a given image and diffuse seed saliency values to the whole graph by a diffusion matrix. While their performance is sensitive to specific feature spaces and scales used for the diffusion matrix definition, little work has been published to systematically promote the robustness and accuracy of salient object detection under the generic mechanism of diffusion. In this work, we firstly present a novel view of the working mechanism of the diffusion process based on mathematical analysis, which reveals that the diffusion process is actually computing the similarity of nodes with respect to the seeds based on diffusion maps. Following this analysis, we propose super diffusion, a novel inclusive learning-based framework for salient object detection, which makes the optimum and robust performance by integrating a large pool of feature spaces, scales and even features originally computed for non-diffusion-based salient object detection. A closed-form solution of the optimal parameters for the integration is determined through supervised learning. At the local level, we propose to promote each individual diffusion before the integration. Our mathematical analysis reveals the close relationship between saliency diffusion and spectral clustering. Based on this, we propose to re-synthesize each individual diffusion matrix from the most discriminative eigenvectors and the constant eigenvector (for saliency normalization). The proposed framework is implemented and experimented on prevalently used benchmark datasets, consistently leading to state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Peng Jiang and Zhiyi Pan and Changhe Tu and Nuno Vasconcelos and Baoquan Chen and Jingliang Peng},
  doi          = {10.1109/TIP.2019.2954209},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2903-2917},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Super diffusion for salient object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The structure transfer machine theory and applications.
<em>TIP</em>, <em>29</em>, 2889–2902. (<a
href="https://doi.org/10.1109/TIP.2019.2954178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning is a fundamental but challenging problem, especially when the distribution of data is unknown. In this paper, we propose a new representation learning method, named Structure Transfer Machine (STM), which enables feature learning process to converge at the representation expectation in a probabilistic way. We theoretically show that such an expected value of the representation (mean) is achievable if the manifold structure can be transferred from the data space to the feature space. The resulting structure regularization term, named manifold loss, is incorporated into the loss function of the typical deep learning pipeline. The STM architecture is constructed to enforce the learned deep representation to satisfy the intrinsic manifold structure from the data, which results in robust features that suit various application scenarios, such as digit recognition, image classification and object tracking. Compared with state-of-the-art CNN architectures, we achieve better results on several commonly used public benchmarks.},
  archive      = {J_TIP},
  author       = {Baochang Zhang and Wankou Yang and Ze Wang and Lian Zhuo and Jungong Han and Xiantong Zhen},
  doi          = {10.1109/TIP.2019.2954178},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2889-2902},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {The structure transfer machine theory and applications},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative residual analysis for image set
classification with posture and age variations. <em>TIP</em>,
<em>29</em>, 2875–2888. (<a
href="https://doi.org/10.1109/TIP.2019.2954176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image set recognition has been widely applied in many practical problems like real-time video retrieval and image caption tasks. Due to its superior performance, it has grown into a significant topic in recent years. However, images with complicated variations, e.g., postures and human ages, are difficult to address, as these variations are continuous and gradual with respect to image appearance. Consequently, the crucial point of image set recognition is to mine the intrinsic connection or structural information from the image batches with variations. In this work, a Discriminant Residual Analysis (DRA) method is proposed to improve the classification performance by discovering discriminant features in related and unrelated groups. Specifically, DRA attempts to obtain a powerful projection which casts the residual representations into a discriminant subspace. Such a projection subspace is expected to magnify the useful information of the input space as much as possible, then the relation between the training set and the test set described by the given metric or distance will be more precise in the discriminant subspace. We also propose a nonfeasance strategy by defining another approach to construct the unrelated groups, which help to reduce furthermore the cost of sampling errors. Two regularization approaches are used to deal with the probable small sample size problem. Extensive experiments are conducted on benchmark databases, and the results show superiority and efficiency of the new methods.},
  archive      = {J_TIP},
  author       = {Chuan-Xian Ren and You-Wei Luo and Xiao-Lin Xu and Dao-Qing Dai and Hong Yan},
  doi          = {10.1109/TIP.2019.2954176},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2875-2888},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Discriminative residual analysis for image set classification with posture and age variations},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Context-interactive CNN for person re-identification.
<em>TIP</em>, <em>29</em>, 2860–2874. (<a
href="https://doi.org/10.1109/TIP.2019.2953587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite growing progresses in recent years, cross-scenario person re-identification remains challenging, mainly due to the pedestrians commonly surrounded by highly-complex environment contexts. In reality, the human perception mechanism could adaptively find proper contextualized spatial-temporal clues towards pedestrian recognition. However, conventional methods fall short in adaptively leveraging the long-term spatial-temporal information due to ever-increasing computational cost. Moreover, CNN-based deep learning methods are hard to conduct optimization due to the non-differentiable property of the built-in context search operation. To ameliorate, this paper proposes a novel Context-Interactive CNN (CI-CNN) to dynamically find both spatial and temporal contexts by embedding multi-task Reinforcement Learning (MTRL). The CI-CNN streamlines the multi-task reinforcement learning by using an actor-critic agent to capture the temporal-spatial context simultaneously, which comprises a context-policy network and a context-critic network. The former network learns policies to determine the optimal spatial context region and temporal sequence range. Based on the inferred temporal-spatial cues, the latter one focuses on the identification task and provides feedback for the policy network. Thus, CI-CNN can simultaneously zoom in/out the perception field in spatial and temporal domain for the context interaction with the environment. By fostering the collaborative interaction between the person and context, our method could achieve outstanding performance on various public benchmarks, which confirms the rationality of our hypothesis, and verifies the effectiveness of our CI-CNN framework.},
  archive      = {J_TIP},
  author       = {Wenfeng Song and Shuai Li and Tao Chang and Aimin Hao and Qinping Zhao and Hong Qin},
  doi          = {10.1109/TIP.2019.2953587},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2860-2874},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Context-interactive CNN for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Phase asymmetry ultrasound despeckling with fractional
anisotropic diffusion and total variation. <em>TIP</em>, <em>29</em>,
2845–2859. (<a href="https://doi.org/10.1109/TIP.2019.2953361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an ultrasound speckle filtering method for not only preserving various edge features but also filtering tissue-dependent complex speckle noises in ultrasound images. The key idea is to detect these various edges using a phase congruence-based edge significance measure called phase asymmetry (PAS), which is invariant to the intensity amplitude of edges and takes 0 in non-edge smooth regions and 1 at the idea step edge, while also taking intermediate values at slowly varying ramp edges. By leveraging the PAS metric in designing weighting coefficients to maintain a balance between fractional-order anisotropic diffusion and total variation (TV) filters in TV cost function, we propose a new fractional TV framework to not only achieve the best despeckling performance with ramp edge preservation but also reduce the staircase effect produced by integral-order filters. Then, we exploit the PAS metric in designing a new fractional-order diffusion coefficient to properly preserve low-contrast edges in diffusion filtering. Finally, different from fixed fractional-order diffusion filters, an adaptive fractional order is introduced based on the PAS metric to enhance various weak edges in the spatially transitional areas between objects. The proposed fractional TV model is minimized using the gradient descent method to obtain the final denoised image. The experimental results and real application of ultrasound breast image segmentation show that the proposed method outperforms other state-of-the-art ultrasound despeckling filters for both speckle reduction and feature preservation in terms of visual evaluation and quantitative indices. The best scores on feature similarity indices have achieved 0.867, 0.844 and 0.834 under three different levels of noise, while the best breast ultrasound segmentation accuracy in terms of the mean and median dice similarity coefficient are 96.25\% and 96.15\%, respectively.},
  archive      = {J_TIP},
  author       = {Kunqiang Mei and Bin Hu and Baowei Fei and Binjie Qin},
  doi          = {10.1109/TIP.2019.2953361},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2845-2859},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Phase asymmetry ultrasound despeckling with fractional anisotropic diffusion and total variation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised deep contrast enhancement with power constraint
for OLED displays. <em>TIP</em>, <em>29</em>, 2834–2844. (<a
href="https://doi.org/10.1109/TIP.2019.2953352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various power-constrained contrast enhancement (PCCE) techniques have been applied to an organic light emitting diode (OLED) display for reducing the power demands of the display while preserving the image quality. In this paper, we propose a new deep learning-based PCCE scheme that constrains the power consumption of the OLED displays while enhancing the contrast of the displayed image. In the proposed method, the power consumption is constrained by simply reducing the brightness a certain ratio, whereas the perceived visual quality is preserved as much as possible by enhancing the contrast of the image using a convolutional neural network (CNN). Furthermore, our CNN can learn the PCCE technique without a reference image by unsupervised learning. Experimental results show that the proposed method is superior to conventional ones in terms of image quality assessment metrics such as a visual saliency-induced index (VSI) and a measure of enhancement (EME).},
  archive      = {J_TIP},
  author       = {Yong-Goo Shin and Seung Park and Yoon-Jae Yeo and Min-Jae Yoo and Sung-Jea Ko},
  doi          = {10.1109/TIP.2019.2953352},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2834-2844},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised deep contrast enhancement with power constraint for OLED displays},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Latent elastic-net transfer learning. <em>TIP</em>,
<em>29</em>, 2820–2833. (<a
href="https://doi.org/10.1109/TIP.2019.2952739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace learning based transfer learning methods commonly find a common subspace where the discrepancy of the source and target domains is reduced. The final classification is also performed in such subspace. However, the minimum discrepancy does not guarantee the best classification performance and thus the common subspace may be not the best discriminative. In this paper, we propose a latent elastic-net transfer learning (LET) method by simultaneously learning a latent subspace and a discriminative subspace. Specifically, the data from different domains can be well interlaced in the latent subspace by minimizing Maximum Mean Discrepancy (MMD). Since the latent subspace decouples inputs and outputs and, thus a more compact data representation is obtained for discriminative subspace learning. Based on the latent subspace, we further propose a low-rank constraint based matrix elastic-net regression to learn another subspace in which the intrinsic intra-class structure correlations of data from different domains is well captured. In doing so, a better discriminative alignment is guaranteed and thus LET finally learns another discriminative subspace for classification. Experiments on visual domains adaptation tasks show the superiority of the proposed LET method.},
  archive      = {J_TIP},
  author       = {Na Han and Jigang Wu and Xiaozhao Fang and Shengli Xie and Shanhua Zhan and Kan Xie and Xuelong Li},
  doi          = {10.1109/TIP.2019.2952739},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2820-2833},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Latent elastic-net transfer learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep guided learning for fast multi-exposure image fusion.
<em>TIP</em>, <em>29</em>, 2808–2819. (<a
href="https://doi.org/10.1109/TIP.2019.2952716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast multi-exposure image fusion (MEF) method, namely MEF-Net, for static image sequences of arbitrary spatial resolution and exposure number. We first feed a low-resolution version of the input sequence to a fully convolutional network for weight map prediction. We then jointly upsample the weight maps using a guided filter. The final image is computed by a weighted fusion. Unlike conventional MEF methods, MEF-Net is trained end-to-end by optimizing the perceptually calibrated MEF structural similarity (MEF-SSIM) index over a database of training sequences at full resolution. Across an independent set of test sequences, we find that the optimized MEF-Net achieves consistent improvement in visual quality for most sequences, and runs 10 to 1000 times faster than state-of-the-art methods. The code is made publicly available at https://github.com/makedede/MEFNet.},
  archive      = {J_TIP},
  author       = {Kede Ma and Zhengfang Duanmu and Hanwei Zhu and Yuming Fang and Zhou Wang},
  doi          = {10.1109/TIP.2019.2952716},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2808-2819},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep guided learning for fast multi-exposure image fusion},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-domain and multi-modal representation disentangler
for cross-domain image manipulation and classification. <em>TIP</em>,
<em>29</em>, 2795–2807. (<a
href="https://doi.org/10.1109/TIP.2019.2952707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning interpretable data representation has been an active research topic in deep learning and computer vision. While representation disentanglement is an effective technique for addressing this task, existing works cannot easily handle the problems in which manipulating and recognizing data across multiple domains are desirable. In this paper, we present a unified network architecture of Multi-domain and Multi-modal Representation Disentangler (M 2 RD), with the goal of learning domain-invariant content representation with the associated domain-specific representation observed. By advancing adversarial learning and disentanglement techniques, the proposed model is able to perform continuous image manipulation across data domains with multiple modalities. More importantly, the resulting domain-invariant feature representation can be applied for unsupervised domain adaptation. Finally, our quantitative and qualitative results would confirm the effectiveness and robustness of the proposed model over state-of-the-art methods on the above tasks.},
  archive      = {J_TIP},
  author       = {Fu-En Yang and Jing-Cheng Chang and Chung-Chi Tsai and Yu-Chiang Frank Wang},
  doi          = {10.1109/TIP.2019.2952707},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2795-2807},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multi-domain and multi-modal representation disentangler for cross-domain image manipulation and classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive sample-level graph combination for partial
multiview clustering. <em>TIP</em>, <em>29</em>, 2780–2794. (<a
href="https://doi.org/10.1109/TIP.2019.2952696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering explores complementary information among distinct views to enhance clustering performance under the assumption that all samples have complete information in all available views. However, this assumption does not hold in many real applications, where the information of some samples in one or more views may be missing, leading to partial multiview clustering problems. In this case, significant performance degeneration is usually observed. A collection of partial multiview clustering algorithms has been proposed to address this issue and most treat all different views equally during clustering. In fact, because different views provide features collected from different angles/feature spaces, they might play different roles in the clustering process. With the diversity of different views considered, in this study, a novel adaptive method is proposed for partial multiview clustering by automatically adjusting the contributions of different views. The samples are divided into complete and incomplete sets, while a joint learning mechanism is established to facilitate the connection between them and thereby improve clustering performance. More specifically, the method is characterized by a joint optimization model comprising two terms. The first term mines the underlying cluster structure from both complete and incomplete samples by adaptively updating their importance in all available views. The second term is designed to group all data with the aid of the cluster structure modeled in the first term. These two terms seamlessly integrate the complementary information among multiple views and enhance the performance of partial multiview clustering. Experimental results on real-world datasets illustrate the effectiveness and efficiency of our proposed method.},
  archive      = {J_TIP},
  author       = {Liu Yang and Chenyang Shen and Qinghua Hu and Liping Jing and Yingbo Li},
  doi          = {10.1109/TIP.2019.2952696},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2780-2794},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive sample-level graph combination for partial multiview clustering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised image dehazing. <em>TIP</em>, <em>29</em>,
2766–2779. (<a href="https://doi.org/10.1109/TIP.2019.2952690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an effective semi-supervised learning algorithm for single image dehazing. The proposed algorithm applies a deep Convolutional Neural Network (CNN) containing a supervised learning branch and an unsupervised learning branch. In the supervised branch, the deep neural network is constrained by the supervised loss functions, which are mean squared, perceptual, and adversarial losses. In the unsupervised branch, we exploit the properties of clean images via sparsity of dark channel and gradient priors to constrain the network. We train the proposed network on both the synthetic data and real-world images in an end-to-end manner. Our analysis shows that the proposed semi-supervised learning algorithm is not limited to synthetic training datasets and can be generalized well to real-world images. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art single image dehazing algorithms on both benchmark datasets and real-world images.},
  archive      = {J_TIP},
  author       = {Lerenhan Li and Yunlong Dong and Wenqi Ren and Jinshan Pan and Changxin Gao and Nong Sang and Ming-Hsuan Yang},
  doi          = {10.1109/TIP.2019.2952690},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2766-2779},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised image dehazing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MonoFENet: Monocular 3D object detection with feature
enhancement networks. <em>TIP</em>, <em>29</em>, 2753–2765. (<a
href="https://doi.org/10.1109/TIP.2019.2952201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D object detection has the merit of low cost and can be served as an auxiliary module for autonomous driving system, becoming a growing concern in recent years. In this paper, we present a monocular 3D object detection method with feature enhancement networks, which we call MonoFENet . Specifically, with the estimated disparity from the input monocular image, the features of both the 2D and 3D streams can be enhanced and utilized for accurate 3D localization. For the 2D stream, the input image is used to generate 2D region proposals as well as to extract appearance features. For the 3D stream, the estimated disparity is transformed into 3D dense point cloud, which is then enhanced by the associated front view maps. With the RoI Mean Pooling layer, 3D geometric features of RoI point clouds are further enhanced by the proposed point feature enhancement ( PointFE ) network. The region-wise features of image and point cloud are fused for the final 2D and 3D bounding boxes regression. The experimental results on the KITTI benchmark reveal that our method can achieve state-of-the-art performance for monocular 3D object detection.},
  archive      = {J_TIP},
  author       = {Wentao Bao and Bin Xu and Zhenzhong Chen},
  doi          = {10.1109/TIP.2019.2952201},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2753-2765},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MonoFENet: Monocular 3D object detection with feature enhancement networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A context knowledge map guided coarse-to-fine action
recognition. <em>TIP</em>, <em>29</em>, 2742–2752. (<a
href="https://doi.org/10.1109/TIP.2019.2952088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human actions involve a wide variety and a large number of categories, which leads to a big challenge in action recognition. However, according to similarities on human body poses, scenes, interactive objects, human actions can be grouped into some semantic groups, i.e. sports, cooking, etc. Therefore, in this paper, we propose a novel approach which recognizes human actions from coarse to fine. Taking full advantage of contributions from high-level semantic contexts, a context knowledge map guided recognition method is designed to realize the coarse-to-fine procedure. In the approach, we define semantic contexts with interactive objects, scenes and body motions in action videos, and build a context knowledge map to automatically define coarse-grained groups. Then fine-grained classifiers are proposed to realize accurate action recognition. The coarse-to-fine procedure narrows action categories in target classifiers, so it is beneficial to improving recognition performance. We evaluate the proposed approach on the CCV, the HMDB-51, and the UCF101 database. Experiments verify its significant effectiveness, on average, improving more than 5\% of recognition precisions than current approaches. Compared with the state-of-the-art, it also obtains outstanding performance. The proposed approach achieves higher accuracies of 93.1\%, 95.4\% and 74.5\% in the CCV, the UCF-101 and the HMDB51 database, respectively.},
  archive      = {J_TIP},
  author       = {Yanli Ji and Yue Zhan and Yang Yang and Xing Xu and Fumin Shen and Heng Tao Shen},
  doi          = {10.1109/TIP.2019.2952088},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2742-2752},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A context knowledge map guided coarse-to-fine action recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MAVA: Multi-level adaptive visual-textual alignment by
cross-media bi-attention mechanism. <em>TIP</em>, <em>29</em>,
2728–2741. (<a href="https://doi.org/10.1109/TIP.2019.2952085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapidly developing information technology leads to a fast growth of visual and textual contents, and it comes with huge challenges to make correlation and perform cross-media retrieval between images and sentences. Existing methods mainly explore cross-media correlation from either global-level instances as the whole images and sentences, or local-level fine-grained patches as the discriminative image regions and key words, which ignore the complementary information from the relation between local-level fine-grained patches. Naturally, relation understanding is highly important for learning cross-media correlation. People focus on not only the alignment between discriminative image regions and key words, but also their relations lying in the visual and textual context. Therefore, in this paper, we propose Multi-level Adaptive Visual-textual Alignment (MAVA) approach with the following contributions. First, we propose cross-media multi-pathway fine-grained network to extract not only the local fine-grained patches as discriminative image regions and key words, but also visual relations between image regions as well as textual relations from the context of sentences, which contain complementary information to exploit fine-grained characteristics within different media types. Second, we propose visual-textual bi-attention mechanism to distinguish the fine-grained information with different saliency from both local and relation levels, which can provide more discriminative hints for correlation learning. Third, we propose cross-media multi-level adaptive alignment to explore global, local and relation alignments. An adaptive alignment strategy is further proposed to enhance the matched pairs of different media types, and discard those misalignments adaptively to learn more precise cross-media correlation. Extensive experiments are conducted to perform image-sentence matching on 2 widely-used cross-media datasets, namely Flickr-30K and MS-COCO, comparing with 10 state-of-the-art methods, which can fully verify the effectiveness of our proposed MAVA approach.},
  archive      = {J_TIP},
  author       = {Yuxin Peng and Jinwei Qi and Yunkan Zhuo},
  doi          = {10.1109/TIP.2019.2952085},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2728-2741},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MAVA: Multi-level adaptive visual-textual alignment by cross-media bi-attention mechanism},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PaDNet: Pan-density crowd counting. <em>TIP</em>,
<em>29</em>, 2714–2727. (<a
href="https://doi.org/10.1109/TIP.2019.2952083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is a highly challenging problem in computer vision and machine learning. Most previous methods have focused on consistent density crowds, i.e., either a sparse or a dense crowd, meaning they performed well in global estimation while neglecting local accuracy. To make crowd counting more useful in the real world, we propose a new perspective, named pan-density crowd counting, which aims to count people in varying density crowds. Specifically, we propose the Pan-Density Network (PaDNet) which is composed of the following critical components. First, the Density-Aware Network (DAN) contains multiple subnetworks pretrained on scenarios with different densities. This module is capable of capturing pan-density information. Second, the Feature Enhancement Layer (FEL) effectively captures the global and local contextual features and generates a weight for each density-specific feature. Third, the Feature Fusion Network (FFN) embeds spatial context and fuses these density-specific features. Further, the metrics Patch MAE (PMAE) and Patch RMSE (PRMSE) are proposed to better evaluate the performance on the global and local estimations. Extensive experiments on four crowd counting benchmark datasets, the ShanghaiTech, the UCF_CC_50, the UCSD, and the UCF-QNRF, indicate that PaDNet achieves state-of-the-art recognition performance and high robustness in pan-density crowd counting.},
  archive      = {J_TIP},
  author       = {Yukun Tian and Yiming Lei and Junping Zhang and James Z. Wang},
  doi          = {10.1109/TIP.2019.2952083},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2714-2727},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PaDNet: Pan-density crowd counting},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-order feature learning for multi-atlas based label
fusion: Application to brain segmentation with MRI. <em>TIP</em>,
<em>29</em>, 2702–2713. (<a
href="https://doi.org/10.1109/TIP.2019.2952079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-atlas based segmentation methods have shown their effectiveness in brain regions-of-interesting (ROIs) segmentation, by propagating labels from multiple atlases to a target image based on the similarity between patches in the target image and multiple atlas images. Most of the existing multi-atlas based methods use image intensity features to calculate the similarity between a pair of image patches for label fusion. In particular, using only low-level image intensity features cannot adequately characterize the complex appearance patterns (e.g., the high-order relationship between voxels within a patch) of brain magnetic resonance (MR) images. To address this issue, this paper develops a high-order feature learning framework for multi-atlas based label fusion, where high-order features of image patches are extracted and fused for segmenting ROIs of structural brain MR images. Specifically, an unsupervised feature learning method (i.e., means-covariances restricted Boltzmann machine, mcRBM) is employed to learn high-order features (i.e., mean and covariance features) of patches in brain MR images. Then, a group-fused sparsity dictionary learning method is proposed to jointly calculate the voting weights for label fusion, based on the learned high-order and the original image intensity features. The proposed method is compared with several state-of-the-art label fusion methods on ADNI, NIREP and LONI-LPBA40 datasets. The Dice ratio achieved by our method is 88.30\%, 88.83\%, 79.54\% and 81.02\% on left and right hippocampus on the ADNI, NIREP and LONI-LPBA40 datasets, respectively, while the best Dice ratio yielded by the other methods are 86.51\%, 87.39\%, 78.48\% and 79.65\% on three datasets, respectively.},
  archive      = {J_TIP},
  author       = {Liang Sun and Wei Shao and Mingliang Wang and Daoqiang Zhang and Mingxia Liu},
  doi          = {10.1109/TIP.2019.2952079},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2702-2713},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-order feature learning for multi-atlas based label fusion: Application to brain segmentation with MRI},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised single image dehazing using dark channel prior
loss. <em>TIP</em>, <em>29</em>, 2692–2701. (<a
href="https://doi.org/10.1109/TIP.2019.2952032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing is a critical stage in many modern-day autonomous vision applications. Early prior-based methods often involved a time-consuming minimization of a hand-crafted energy function. Recent learning-based approaches utilize the representational power of deep neural networks (DNNs) to learn the underlying transformation between hazy and clear images. Due to inherent limitations in collecting matching clear and hazy images, these methods resort to training on synthetic data, constructed from indoor images and corresponding depth information. This may result in a possible domain shift when treating outdoor scenes. We propose a completely unsupervised method of training via minimization of the well-known, Dark Channel Prior (DCP) energy function. Instead of feeding the network with synthetic data, we solely use real-world outdoor images and tune the network&#39;s parameters by directly minimizing the DCP. Although our “Deep DCP” technique can be regarded as a fast approximator of DCP, it actually improves its results significantly. This suggests an additional regularization obtained via the network and learning process. Experiments show that our method performs on par with large-scale supervised methods.},
  archive      = {J_TIP},
  author       = {Alona Golts and Daniel Freedman and Michael Elad},
  doi          = {10.1109/TIP.2019.2952032},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2692-2701},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised single image dehazing using dark channel prior loss},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning no-reference quality assessment of multiply and
singly distorted images with big data. <em>TIP</em>, <em>29</em>,
2676–2691. (<a href="https://doi.org/10.1109/TIP.2019.2952010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research on no-reference (NR) quality assessment of multiply-distorted images focused mainly on three distortion types (white noise, Gaussian blur, and JPEG compression), while in practice images can be contaminated by many other common distortions due to the various stages of processing. Although MUSIQUE (MUltiply- and Singly-distorted Image QUality Estimator) [Zhang et al., TIP 2018] is a successful NR algorithm, this approach is still limited to the three distortion types. In this paper, we extend MUSIQUE to MUSIQUE-II to blindly assess the quality of images corrupted by five distortion types (white noise, Gaussian blur, JPEG compression, JPEG2000 compression, and contrast change) and their combinations. The proposed MUSIQUE-II algorithm builds upon the classification and parameter-estimation framework of its predecessor by using more advanced models and a more comprehensive set of distortion-sensitive features. Specifically, MUSIQUE-II relies on a three-layer classification model to identify 19 distortion types. To predict the five distortion parameter values, MUSIQUE-II extracts an additional 14 contrast features and employs a multi-layer probability-weighting rule. Finally, MUSIQUE-II employs a new most-apparent-distortion strategy to adaptively combine five quality scores based on outputs of three classification models. Experimental results tested on three multiply-distorted and six singly-distorted image quality databases show that MUSIQUE-II yields not only a substantial improvement in quality predictive performance as compared with its predecessor, but also highly competitive performance relative to other state-of-the-art FR/NR IQA algorithms.},
  archive      = {J_TIP},
  author       = {Yi Zhang and Xuanqin Mou and Damon M. Chandler},
  doi          = {10.1109/TIP.2019.2952010},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2676-2691},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning no-reference quality assessment of multiply and singly distorted images with big data},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast online 3D reconstruction of dynamic scenes from
individual single-photon detection events. <em>TIP</em>, <em>29</em>,
2666–2675. (<a href="https://doi.org/10.1109/TIP.2019.2952008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an algorithm for online 3D reconstruction of dynamic scenes using individual times of arrival (ToA) of photons recorded by single-photon detector arrays. One of the main challenges in 3D imaging using single-photon Lidar is the integration time required to build ToA histograms and reconstruct reliably 3D profiles in the presence of non-negligible ambient illumination. This long integration time also prevents the analysis of rapid dynamic scenes using existing techniques. We propose a new method which does not rely on the construction of ToA histograms but allows, for the first time, individual detection events to be processed online, in a parallel manner in different pixels, while accounting for the intrinsic spatiotemporal structure of dynamic scenes. Adopting a Bayesian approach, a Bayesian model is constructed to capture the dynamics of the 3D profile and an approximate inference scheme based on assumed density filtering is proposed, yielding a fast and robust reconstruction algorithm able to process efficiently thousands to millions of frames, as usually recorded using single-photon detectors. The performance of the proposed method, able to process hundreds of frames per second, is assessed using a series of experiments conducted with static and dynamic 3D scenes and the results obtained pave the way to a new family of real-time 3D reconstruction solutions.},
  archive      = {J_TIP},
  author       = {Yoann Altmann and Stephen McLaughlin and Michael E. Davies},
  doi          = {10.1109/TIP.2019.2952008},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2666-2675},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast online 3D reconstruction of dynamic scenes from individual single-photon detection events},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color channel compensation (3C): A fundamental
pre-processing step for image enhancement. <em>TIP</em>, <em>29</em>,
2653–2665. (<a href="https://doi.org/10.1109/TIP.2019.2951304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel solution to improve image enhancement in terms of color appearance. Our approach, called Color Channel Compensation (3C), overcomes artifacts resulting from the severely non-uniform color spectrum distribution encountered in images captured under hazy night-time conditions, underwater, or under non-uniform artificial illumination. Our solution is founded on the observation that, under such adverse conditions, the information contained in at least one color channel is close to completely lost, making the traditional enhancing techniques subject to noise and color shifting. In those cases, our pre-processing method proposes to reconstruct the lost channel based on the opponent color channel. Our algorithm subtracts a local mean from each opponent color pixel. Thereby, it partly recovers the lost color from the two colors (red-green or blue-yellow) involved in the opponent color channel. The proposed approach, whilst simple, is shown to consistently improve the outcome of conventional restoration methods. To prove the utility of our 3C operator, we provide an extensive qualitative and quantitative evaluation for white balancing, image dehazing, and underwater enhancement applications.},
  archive      = {J_TIP},
  author       = {Codruta O. Ancuti and Cosmin Ancuti and Christophe De Vleeschouwer and Mateu Sbert},
  doi          = {10.1109/TIP.2019.2951304},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2653-2665},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color channel compensation (3C): A fundamental pre-processing step for image enhancement},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Group-group loss-based global-regional feature learning for
vehicle re-identification. <em>TIP</em>, <em>29</em>, 2638–2652. (<a
href="https://doi.org/10.1109/TIP.2019.2950796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle Re-Identification (Re-ID) is challenging because vehicles of the same model commonly show similar appearance. We tackle this challenge by proposing a Global-Regional Feature (GRF) that depicts extra local details to enhance discrimination power in addition to the global context. It is motivated by the observation that, vehicles of same color, maker, and model can be distinguished by their regional difference, e.g., the decorations on the windshields. To accelerate the GRF learning and promote its discrimination power, we propose a Group-Group Loss (GGL) to optimize the distance within and across vehicle image groups. Different from the siamese or triplet loss, GGL is directly computed on image groups rather than individual sample pairs or triplets. By avoiding traversing numerous sample combinations, GGL makes the model training easier and more efficient. Those two contributions highlight this work from previous methods on vehicle Re-ID task, which commonly learn global features with triplet loss or its variants. We evaluate our methods on two large-scale vehicle Re-ID datasets, i.e., VeRi and VehicleID. Experimental results show our methods achieve promising performance in comparison with recent works.},
  archive      = {J_TIP},
  author       = {Xiaobin Liu and Shiliang Zhang and Xiaoyu Wang and Richang Hong and Qi Tian},
  doi          = {10.1109/TIP.2019.2950796},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2638-2652},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Group-group loss-based global-regional feature learning for vehicle re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved techniques for adversarial discriminative domain
adaptation. <em>TIP</em>, <em>29</em>, 2622–2637. (<a
href="https://doi.org/10.1109/TIP.2019.2950768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial discriminative domain adaptation (ADDA) is an efficient framework for unsupervised domain adaptation in image classification, where the source and target domains are assumed to have the same classes, but no labels are available for the target domain. While ADDA has already achieved better training efficiency and competitive accuracy on image classification in comparison to other adversarial based methods, we investigate whether we can improve its performance with a new framework and new loss formulations. Following the framework of semi-supervised GANs, we first extend the discriminator output over the source classes, in order to model the joint distribution over domain and task. We thus leverage on the distribution over the source encoder posteriors (which is fixed during adversarial training) and propose maximum mean discrepancy (MMD) and reconstruction-based loss functions for aligning the target encoder distribution to the source domain. We compare and provide a comprehensive analysis of how our framework and loss formulations extend over simple multi-class extensions of ADDA and other discriminative variants of semi-supervised GANs. In addition, we introduce various forms of regularization for stabilizing training, including treating the discriminator as a denoising autoencoder and regularizing the target encoder with source examples to reduce overfitting under a contraction mapping (i.e., when the target per-class distributions are contracting during alignment with the source). Finally, we validate our framework on standard datasets like MNIST, USPS, SVHN, MNIST-M and Office-31. We additionally examine how the proposed framework benefits recognition problems based on sensing modalities that lack training data. This is realized by introducing and evaluating on a neuromorphic vision sensing (NVS) sign language recognition dataset, where the source domain constitutes emulated neuromorphic spike events converted from conventional pixel-based video and the target domain is experimental (real) spike events from an NVS camera. Our results on all datasets show that our proposal is both simple and efficient, as it competes or outperforms the state-of-the-art in unsupervised domain adaptation, such as DIFA and MCDDA, whilst offering lower complexity than other recent adversarial methods.},
  archive      = {J_TIP},
  author       = {Aaron Chadha and Yiannis Andreopoulos},
  doi          = {10.1109/TIP.2019.2950768},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2622-2637},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improved techniques for adversarial discriminative domain adaptation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distilling channels for efficient deep tracking.
<em>TIP</em>, <em>29</em>, 2610–2621. (<a
href="https://doi.org/10.1109/TIP.2019.2950508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep trackers have proven success in visual tracking. Typically, these trackers employ optimally pre-trained deep networks to represent all diverse objects with multi-channel features from some fixed layers. The deep networks employed are usually trained to extract rich knowledge from massive data used in object classification and so they are capable to represent generic objects very well. However, these networks are too complex to represent a specific moving object, leading to poor generalization as well as high computational and memory costs. This paper presents a novel and general framework termed channel distillation to facilitate deep trackers. To validate the effectiveness of channel distillation, we take discriminative correlation filter (DCF) and ECO for example. We demonstrate that an integrated formulation can turn feature compression, response map generation, and model update into a unified energy minimization problem to adaptively select informative feature channels that improve the efficacy of tracking moving objects on the fly. Channel distillation can accurately extract good channels, alleviating the influence of noisy channels and generally reducing the number of channels, as well as adaptively generalizing to different channels and networks. The resulting deep tracker is accurate, fast, and has low memory requirements. Extensive experimental evaluations on popular benchmarks clearly demonstrate the effectiveness and generalizability of our framework.},
  archive      = {J_TIP},
  author       = {Shiming Ge and Zhao Luo and Chunhui Zhang and Yingying Hua and Dacheng Tao},
  doi          = {10.1109/TIP.2019.2950508},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2610-2621},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Distilling channels for efficient deep tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Super-resolution phase retrieval from designed coded
diffraction patterns. <em>TIP</em>, <em>29</em>, 2598–2609. (<a
href="https://doi.org/10.1109/TIP.2019.2949436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution phase retrieval is an inverse problem that appears in diffractive optical imaging (DOI) and consists in estimating a high-resolution image from low-resolution phaseless measurements. DOI has three diffraction zones where the data can be acquired, known as near, middle, and far fields. Recent works have studied super-resolution phase retrieval under a setup that records coded diffraction patterns at the near and far fields. However, the attainable resolution of the image is mainly governed by the sensor characteristics, whose cost increases in proportion to the resolution. Also, these methodologies lack theoretical analysis. Hence, this work derives super-resolution models from low-resolution coded phaseless measurements at any diffraction zone that in contrast to prior contributions, the attainable resolution of the image is determined by the resolution of the coded aperture. For the proposed models, the existence of a unique solution (up to a global unimodular constant) is guaranteed with high probability, which can be increased by designing the coded aperture. Therefore, a strategy that designs the spatial distribution of the coded aperture is developed. Additionally, a super-resolution phase retrieval algorithm that minimizes a smoothed nonconvex least-squares objective function is proposed. The method first approximates the image by a spectral algorithm, which is then refined based upon a sequence of alternate steps. Simulation results show that the proposed algorithm overcomes state-of-the-art methods in reconstructing the high-resolution image. In addition, the reconstruction quality using designed coded apertures is higher than that of the non-designed ensembles.},
  archive      = {J_TIP},
  author       = {Jorge Bacca and Samuel Pinilla and Henry Arguello},
  doi          = {10.1109/TIP.2019.2949436},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2598-2609},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Super-resolution phase retrieval from designed coded diffraction patterns},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate transmission estimation for removing haze and noise
from a single image. <em>TIP</em>, <em>29</em>, 2583–2597. (<a
href="https://doi.org/10.1109/TIP.2019.2949392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image noise usually causes depth-dependent visual artifacts in single image dehazing. Most existing dehazing methods exploit a two-step strategy in the restoration, which inevitably leads to inaccurate transmission maps and low-quality scene radiance for noisy and hazy inputs. To address these problems, we present a novel variational model for joint recovery of the transmission map and the scene radiance from a single image. In the model, we propose a transmission-aware non-local regularization to avoid noise amplification by adaptively suppressing noise and preserving fine details in the recovered image. Meanwhile, to improve the accuracy of transmission estimation, we introduce a semantic-guided regularization to smooth out the transmission map while keeping depth inconsistency at the boundaries of different objects. Furthermore, we design an alternating scheme to jointly optimize the transmission map and the scene radiance as well as the segmentation map. Extensive experiments on synthetic and real-world data demonstrate that the proposed algorithm performs favorably against state-of-the-art dehazing methods on noisy and hazy images.},
  archive      = {J_TIP},
  author       = {Qingbo Wu and Jingang Zhang and Wenqi Ren and Wangmeng Zuo and Xiaochun Cao},
  doi          = {10.1109/TIP.2019.2949392},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2583-2597},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Accurate transmission estimation for removing haze and noise from a single image},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PML-LocNet: Improving object localization with prior-induced
multi-view learning network. <em>TIP</em>, <em>29</em>, 2568–2582. (<a
href="https://doi.org/10.1109/TIP.2019.2947155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new model for Weakly Supervised Object Localization (WSOL) problems where only image-level supervision is provided. The key to solve such problems is to infer the object locations accurately. Previous methods usually model the missing object locations as latent variables, and alternate between updating their estimates and learning a detector accordingly. However, the performance of such alternative optimization is sensitive to the quality of the initial latent variables and the resulted localization model is prone to overfitting to improper localizations. To address these issues, we develop a Prior-induced Multi-view Learning Localization Network (PML-LocNet) which exploits both view diversity and sample diversity to improve object localization. In particular, the view diversity is imposed by a two-phase multi-view learning strategy, with which the complementarity among learned features from different views and the consensus among localized instances from each view are leveraged to benefit localization. The sample diversity is pursued by harnessing coarse-to-fine priors at both image and instance levels. With these priors, more emphasis would go to the reliable samples and the contributions of the unreliable ones would be decreased, such that the intrinsic characteristics of each sample can be exploited to make the model more robust during network learning. PML-LocNet can be easily combined with existing WSOL models to further improve the localization accuracy. Its effectiveness has been proved experimentally. Notably, it achieves 69.3\% CorLoc and 50.4\% mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.},
  archive      = {J_TIP},
  author       = {Xiaopeng Zhang and Yang Yang and Hongkai Xiong and Jiashi Feng},
  doi          = {10.1109/TIP.2019.2947155},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2568-2582},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PML-LocNet: Improving object localization with prior-induced multi-view learning network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep retinal image segmentation with regularization under
geometric priors. <em>TIP</em>, <em>29</em>, 2552–2567. (<a
href="https://doi.org/10.1109/TIP.2019.2946078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vessel segmentation of retinal images is a key diagnostic capability in ophthalmology. This problem faces several challenges including low contrast, variable vessel size and thickness, and presence of interfering pathology such as micro-aneurysms and hemorrhages. Early approaches addressing this problem employed hand-crafted filters to capture vessel structures, accompanied by morphological post-processing. More recently, deep learning techniques have been employed with significantly enhanced segmentation accuracy. We propose a novel domain enriched deep network that consists of two components: 1) a representation network that learns geometric features specific to retinal images, and 2) a custom designed computationally efficient residual task network that utilizes the features obtained from the representation layer to perform pixel-level segmentation. The representation and task networks are jointly learned for any given training set. To obtain physically meaningful and practically effective representation filters, we propose two new constraints that are inspired by expected prior structure on these filters: 1) orientation constraint that promotes geometric diversity of curvilinear features, and 2) a data adaptive noise regularizer that penalizes false positives. Multi-scale extensions are developed to enable accurate detection of thin vessels. Experiments performed on three challenging benchmark databases under a variety of training scenarios show that the proposed prior guided deep network outperforms state of the art alternatives as measured by common evaluation metrics, while being more economical in network size and inference time.},
  archive      = {J_TIP},
  author       = {Venkateswararao Cherukuri and Vijay Kumar B.G. and Raja Bala and Vishal Monga},
  doi          = {10.1109/TIP.2019.2946078},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2552-2567},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep retinal image segmentation with regularization under geometric priors},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quality measurement of images on mobile streaming interfaces
deployed at scale. <em>TIP</em>, <em>29</em>, 2536–2551. (<a
href="https://doi.org/10.1109/TIP.2019.2939733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing use of smart cellular devices for entertainment purposes, audio and video streaming services now offer an increasingly wide variety of popular mobile applications that offer portable and accessible ways to consume content. The user interfaces of these applications have become increasingly visual in nature, and are commonly loaded with dense multimedia content such as thumbnail images, animated GIFs, and short videos. To efficiently render these and to aid rapid download to the client display, it is necessary to compress, scale and color subsample them. These operations introduce distortions, reducing the appeal of the application. It is desirable to be able to automatically monitor and govern the visual qualities of these small images, which are usually small images. However, while there exists a variety of high-performing image quality assessment (IQA) algorithms, none have been designed for this particular use case. This kind of content often has unique characteristics, such as overlaid graphics, intentional brightness, gradients, text, and warping. We describe a study we conducted on the subjective and objective quality of images embedded in the displayed user interfaces of mobile streaming applications. We created a database of typical “billboard” and “thumbnail” images viewed on such services. Using the collected data, we studied the effects of compression, scaling and chroma-subsampling on perceived quality by conducting a subjective study. We also evaluated the performance of leading picture quality prediction models on the new database. We report some surprising results regarding algorithm performance, and find that there remains ample scope for future model development.},
  archive      = {J_TIP},
  author       = {Zeina Sinno and Anush Krishna Moorthy and Jan De Cock and Zhi Li and Alan C. Bovik},
  doi          = {10.1109/TIP.2019.2939733},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2536-2551},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Quality measurement of images on mobile streaming interfaces deployed at scale},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating local geometric feature representations for 3D
rigid data matching. <em>TIP</em>, <em>29</em>, 2522–2535. (<a
href="https://doi.org/10.1109/TIP.2019.2959236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local geometric descriptors act as an essential component for 3D rigid data matching. A rotational invariant local geometric descriptor usually consists of two components: local reference frame (LRF) and feature representation. However, existing evaluation efforts have mainly been paid on the LRF or the overall descriptor and the quantitative comparison of feature representations remains unexplored. This paper fills the gap by comprehensively evaluating nine state-of-the-art local geometric feature representations. In particular, our evaluation assesses feature representations based on ground-truth LRFs such that the ranking of tested methods is more convincing as compared with existing studies. The experiments are deployed on six standard datasets with various application scenarios (shape retrieval, point cloud registration, and object recognition) and data modalities (LiDAR, Kinect, and Space Time) as well as perturbations including Gaussian noise, shot noise, data decimation, clutter, occlusion, and limited overlap. The evaluated terms cover the major concerns for a feature representation, e.g., distinctiveness, robustness, compactness, and efficiency. The outcomes present interesting findings that may shed new light on this community and provide complementary perspectives to existing evaluations on the topic of local geometric feature description. A summary of evaluated methods regarding their peculiarities is finally presented to guide real-world applications and new descriptor crafting.},
  archive      = {J_TIP},
  author       = {Jiaqi Yang and Siwen Quan and Peng Wang and Yanning Zhang},
  doi          = {10.1109/TIP.2019.2959236},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2522-2535},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Evaluating local geometric feature representations for 3D rigid data matching},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive regularization of some inverse problems in image
analysis. <em>TIP</em>, <em>29</em>, 2507–2521. (<a
href="https://doi.org/10.1109/TIP.2019.2960587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an adaptive regularization scheme for optimizing composite energy functionals arising in image analysis problems. The scheme automatically trades off data fidelity and regularization depending on the current data fit during the iterative optimization, so that regularization is strongest initially, and wanes as data fidelity improves, with the weight of the regularizer being minimized at convergence. We also introduce a Huber loss function in both data fidelity and regularization terms, and present an efficient convex optimization algorithm based on the alternating direction method of multipliers (ADMM) using the equivalent relation between the Huber function and the proximal operator of the one-norm. We illustrate and validate our adaptive Huber-Huber model on synthetic and real images in segmentation, motion estimation, and denoising problems.},
  archive      = {J_TIP},
  author       = {Byung-Woo Hong and Jakeoung Koo and Martin Burger and Stefano Soatto},
  doi          = {10.1109/TIP.2019.2960587},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2507-2521},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive regularization of some inverse problems in image analysis},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and accurate depth estimation from sparse light fields.
<em>TIP</em>, <em>29</em>, 2492–2506. (<a
href="https://doi.org/10.1109/TIP.2019.2959233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a fast and accurate method for dense depth reconstruction, which is specifically tailored to process sparse, wide-baseline light field data captured with camera arrays. In our method, the source images are over-segmented into non-overlapping compact superpixels. We model superpixel as planar patches in the image space and use them as basic primitives for depth estimation. Such superpixel-based representation yields desired reduction in both memory and computation requirements while preserving image geometry with respect to the object contours. The initial depth maps, obtained by plane-sweeping independently for each view, are jointly refined via iterative belief-propagation-like optimization in superpixel domain. During the optimization, smoothness between the neighboring superpixels and geometric consistency between the views are enforced. To ensure rapid information propagation into textureless and occluded regions, together with the immediate superpixel neighbors, candidates from larger neighborhoods are sampled. Additionally, in order to make full use of the parallel graphics hardware a synchronous message update schedule is employed allowing to process all the superpixels of all the images at once. This way, the distribution of the scene geometry becomes distinctive already after the first iterations, facilitating stability and fast convergence of the refinement procedure. We demonstrate that a few refinement iterations result in globally consistent dense depth maps even in the presence of wide textureless regions and occlusions. The experiments show that while the depth reconstruction takes about a second per full high-definition view, the accuracy of the obtained depth maps is comparable with the state-of-the-art results, which otherwise require much longer processing time.},
  archive      = {J_TIP},
  author       = {Aleksandra Chuchvara and Attila Barsi and Atanas Gotchev},
  doi          = {10.1109/TIP.2019.2959233},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2492-2506},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast and accurate depth estimation from sparse light fields},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local proximity for enhanced visibility in haze.
<em>TIP</em>, <em>29</em>, 2478–2491. (<a
href="https://doi.org/10.1109/TIP.2019.2957931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atmospheric medium often constrains the visibility of outdoor scenes due to scattering of light rays. This causes attenuation in the irradiance reaching the imaging device along with an additive component to render a hazy effect in the image. The visibility is further reduced for poorly illuminated scenes. The attenuation becomes wavelength dependent in underwater scenario, causing undesired color cast along with hazy effect. In order to suppress the effect of different atmospheric/underwater conditions such as haze and to enhance the contrast of such images, we reformulate local haziness in a generalized manner. The parameters are estimated by harnessing the similarity of patches within a local neighborhood. Unlike existing methods, our approach is developed based on the assumption that for outdoor scenes, the depth of patches changes gradually in a local neighborhood surrounding the patch. This change in depth can be approximated by patch similarity in that neighborhood. As the attenuation in irradiance of an image in presence of atmospheric medium relies on the depth of the scene, the coefficients related to the attenuation are estimated from the weights of patch similarity. The additive haze effect is deduced using non-local mean of the patch. Our experimental results demonstrate the effectiveness of our approach in reducing the haze component as well as in enhancing the image under different conditions of haze (daytime, nighttime, and underwater).},
  archive      = {J_TIP},
  author       = {Srimanta Mandal and A. N. Rajagopalan},
  doi          = {10.1109/TIP.2019.2957931},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2478-2491},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local proximity for enhanced visibility in haze},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor multi-task learning for person re-identification.
<em>TIP</em>, <em>29</em>, 2463–2477. (<a
href="https://doi.org/10.1109/TIP.2019.2949929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a tensor multi-task model for person re-identification (Re-ID). Due to discrepancy among cameras, our approach regards Re-ID from multiple cameras as different but related classification tasks, each task corresponding to a specific camera. In each task, we distinguish the person identity as a one-vs-all linear classification problem, where one classifier is associated with a specific person. By constructing all classifiers into a task-specific projection matrix, the proposed method could utilize all the matrices to form a tensor structure, and jointly train all the tasks in a uniform tensor space. In this space, by assuming the features of the same person under different cameras are generated from a latent subspace, and different identities under the same perspective share similar patterns, the high-order correlations, not only across different tasks but also within a certain task, can be captured by utilizing a new type of low-rank tensor constraint. Therefore, the learned classifiers transform the original feature vector into the latent space, where feature distributions across cameras can be well-aligned. Moreover, this model can be incorporated into multiple visual features to boost the performance, and easily extended to the unsupervised setting. Extensive experiments and comparisons with recent Re-ID methods manifest the competitive performance of our method.},
  archive      = {J_TIP},
  author       = {Zhizhong Zhang and Yuan Xie and Wensheng Zhang and Yongqiang Tang and Qi Tian},
  doi          = {10.1109/TIP.2019.2949929},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2463-2477},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Tensor multi-task learning for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconstruction of binary shapes from blurred images via
hankel-structured low-rank matrix recovery. <em>TIP</em>, <em>29</em>,
2452–2462. (<a href="https://doi.org/10.1109/TIP.2019.2950512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the dominance of digital imaging systems, we are often dealing with discrete-domain samples of an analog image. Due to physical limitations, all imaging devices apply a blurring kernel on the input image before taking samples to form the output pixels. In this paper, we focus on the reconstruction of binary shape images from few blurred samples. This problem has applications in medical imaging, shape processing, and image segmentation. Our method relies on representing the analog shape image in a discrete grid much finer than the sampling grid. We formulate the problem as the recovery of a rank r matrix that is formed by a Hankel structure on the pixels. We further propose efficient ADMM-based algorithms to recover the lowrank matrix in both noiseless and noisy settings. We also analytically investigate the number of required samples for successful recovery in the noiseless case. For this purpose, we study the problem in the random sampling framework, and show that with O(r log 4 (n 1 n 2 )) random samples (where the size of the image is assumed to be n 1 × n 2 ) we can guarantee the perfect reconstruction with high probability under mild conditions. We further prove the robustness of the proposed recovery in the noisy setting by showing that the reconstruction error in the noisy case is bounded when the input noise is bounded. Simulation results confirm that our proposed method outperform the conventional total variation minimization in the noiseless settings.},
  archive      = {J_TIP},
  author       = {Saeed Razavikia and Arash Amini and Sajad Daei},
  doi          = {10.1109/TIP.2019.2950512},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2452-2462},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reconstruction of binary shapes from blurred images via hankel-structured low-rank matrix recovery},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based compensated wavelet lifting for scalable
lossless coding of dynamic medical data. <em>TIP</em>, <em>29</em>,
2439–2451. (<a href="https://doi.org/10.1109/TIP.2019.2947138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lossless compression of dynamic 2D+t and 3D+t medical data is challenging regarding the huge amount of data, the characteristics of the inherent noise, and the high bit depth. Beyond that, a scalable representation is often required in telemedicine applications. Motion Compensated Temporal Filtering works well for lossless compression of medical volume data and additionally provides temporal, spatial, and quality scalability features. To achieve a high quality lowpass subband, which shall be used as a downscaled representative of the original data, graph-based motion compensation was recently introduced to this framework. However, encoding the motion information, which is stored in adjacency matrices, is not well investigated so far. This work focuses on coding these adjacency matrices to make the graph-based motion compensation feasible for data compression. We propose a novel coding scheme based on constructing so-called motion maps. This allows for the first time to compare the performance of graph-based motion compensation to traditional block- and mesh-based approaches. For high quality lowpass subbands our method is able to outperform the block- and mesh-based approaches by increasing the visual quality in terms of PSNR by 0.53dB and 0.28dB for CT data, as well as 1.04dB and 1.90dB for MR data, respectively, while the bit rate is reduced at the same time.},
  archive      = {J_TIP},
  author       = {Daniela Lanz and André Kaup},
  doi          = {10.1109/TIP.2019.2947138},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2439-2451},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph-based compensated wavelet lifting for scalable lossless coding of dynamic medical data},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Class-specific reconstruction transfer learning for visual
recognition across domains. <em>TIP</em>, <em>29</em>, 2424–2438. (<a
href="https://doi.org/10.1109/TIP.2019.2948480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace learning and reconstruction have been widely explored in recent transfer learning work. Generally, a specially designed projection and reconstruction transfer functions bridging multiple domains for heterogeneous knowledge sharing are wanted. However, we argue that the existing subspace reconstruction based domain adaptation algorithms neglect the class prior, such that the learned transfer function is biased, especially when data scarcity of some class is encountered. Different from those previous methods, in this article, we propose a novel class-wise reconstruction-based adaptation method called Class-specific Reconstruction Transfer Learning (CRTL), which optimizes a well modeled transfer loss function by fully exploiting intra-class dependency and inter-class independency. The merits of the CRTL are three-fold. 1) Using a class-specific reconstruction matrix to align the source domain with the target domain fully exploits the class prior in modeling the domain distribution consistency, which benefits the cross-domain classification. 2) Furthermore, to keep the intrinsic relationship between data and labels after feature augmentation, a projected Hilbert-Schmidt Independence Criterion (pHSIC), that measures the dependency between data and label, is first proposed in transfer learning community by mapping the data from raw space to RKHS. 3) In addition, by imposing low-rank and sparse constraints on the class-specific reconstruction coefficient matrix, the global and local data structure that contributes to domain correlation can be effectively preserved. Extensive experiments on challenging benchmark datasets demonstrate the superiority of the proposed method over state-of-the-art representation-based domain adaptation methods. The demo code is available in https://github.com/wangshanshanCQU/CRTL .},
  archive      = {J_TIP},
  author       = {Shanshan Wang and Lei Zhang and Wangmeng Zuo and Bob Zhang},
  doi          = {10.1109/TIP.2019.2948480},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2424-2438},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Class-specific reconstruction transfer learning for visual recognition across domains},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RhythmNet: End-to-end heart rate estimation from face via
spatial-temporal representation. <em>TIP</em>, <em>29</em>, 2409–2423.
(<a href="https://doi.org/10.1109/TIP.2019.2947204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart rate (HR) is an important physiological signal that reflects the physical and emotional status of a person. Traditional HR measurements usually rely on contact monitors, which may cause inconvenience and discomfort. Recently, some methods have been proposed for remote HR estimation from face videos; however, most of them focus on well-controlled scenarios, their generalization ability into less-constrained scenarios (e.g., with head movement, and bad illumination) are not known. At the same time, lacking large-scale HR databases has limited the use of deep models for remote HR estimation. In this paper, we propose an end-to-end RhythmNet for remote HR estimation from the face. In RyhthmNet, we use a spatial-temporal representation encoding the HR signals from multiple ROI volumes as its input. Then the spatial-temporal representations are fed into a convolutional network for HR estimation. We also take into account the relationship of adjacent HR measurements from a video sequence via Gated Recurrent Unit (GRU) and achieves efficient HR measurement. In addition, we build a large-scale multi-modal HR database (named as VIPL-HR 1 ), which contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our VIPL-HR database contains various variations such as head movements, illumination variations, and acquisition device changes, replicating a less-constrained scenario for HR estimation. The proposed approach outperforms the state-of-the-art methods on both the public-domain and our VIPL-HR databases. 1 VIPL-HR is available at: http://vipl.ict.ac.cn/view_database.php?id=15.},
  archive      = {J_TIP},
  author       = {Xuesong Niu and Shiguang Shan and Hu Han and Xilin Chen},
  doi          = {10.1109/TIP.2019.2947204},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2409-2423},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RhythmNet: End-to-end heart rate estimation from face via spatial-temporal representation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BMAN: Bidirectional multi-scale aggregation networks for
abnormal event detection. <em>TIP</em>, <em>29</em>, 2395–2408. (<a
href="https://doi.org/10.1109/TIP.2019.2948286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal event detection is an important task in video surveillance systems. In this paper, we propose novel bidirectional multi-scale aggregation networks (BMAN) for abnormal event detection. The proposed BMAN learns spatio-temporal patterns of normal events to detect deviations from the learned normal patterns as abnormalities. The BMAN consists of two main parts: an inter-frame predictor and an appearance-motion joint detector. The inter-frame predictor is devised to encode normal patterns, which generates an inter-frame using bidirectional multi-scale aggregation based on attention. With the feature aggregation, robustness for object scale variations and complex motions is achieved in normal pattern encoding. Based on the encoded normal patterns, abnormal events are detected by the appearance-motion joint detector in which both appearance and motion characteristics of scenes are considered. Comprehensive experiments are performed, and the results show that the proposed method outperforms the existing state-of-the-art methods. The resulting abnormal event detection is interpretable on the visual basis of where the detected events occur. Further, we validate the effectiveness of the proposed network designs by conducting ablation study and feature visualization.},
  archive      = {J_TIP},
  author       = {Sangmin Lee and Hak Gu Kim and Yong Man Ro},
  doi          = {10.1109/TIP.2019.2948286},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2395-2408},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BMAN: Bidirectional multi-scale aggregation networks for abnormal event detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep active shape model for robust object fitting.
<em>TIP</em>, <em>29</em>, 2380–2394. (<a
href="https://doi.org/10.1109/TIP.2019.2948728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition and localization is still a very challenging problem, despite recent advances in deep learning (DL) approaches, especially for objects with varying shapes and appearances. Statistical models, such as an Active Shape Model (ASM), rely on a parametric model of the object, allowing an easy incorporation of prior knowledge about shape and appearance in a principled way. To take advantage of these benefits, this paper proposes a new ASM framework that addresses two tasks: (i) comparing the performance of several image features used to extract observations from an input image; and (ii) improving the performance of the model fitting by relying on a probabilistic framework that allows the use of multiple observations and is robust to the presence of outliers. The goal in (i) is to maximize the quality of the observations by exploring a wide set of handcrafted features (HOG, SIFT, and texture templates) and more recent DL-based features. Regarding (ii), we use the Generalized Expectation-Maximization algorithm to deal with outliers and to extend the fitting process to multiple observations. The proposed framework is evaluated in the context of facial landmark fitting and the segmentation of the endocardium of the left ventricle in cardiac magnetic resonance volumes. We experimentally observe that the proposed approach is robust not only to outliers, but also to adverse initialization conditions and to large search regions (from where the observations are extracted from the image). Furthermore, the results of the proposed combination of the ASM with DL-based features are competitive with more recent DL approaches (e.g. FCN [1], U-Net [2] and CNN Cascade [3]), showing that it is possible to combine the benefits of statistical models and DL into a new deep ASM probabilistic framework.},
  archive      = {J_TIP},
  author       = {Daniela O. Medley and Carlos Santiago and Jacinto C. Nascimento},
  doi          = {10.1109/TIP.2019.2948728},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2380-2394},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep active shape model for robust object fitting},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Repeated look-up tables. <em>TIP</em>, <em>29</em>,
2370–2379. (<a href="https://doi.org/10.1109/TIP.2019.2949245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient hardware implementations routinely approximate mathematical functions with look-up tables, while keeping the error of the approximation under control. For a certain class of commonly occurring 1D functions, namely monotonically increasing or decreasing functions, we found that it is possible to approximate such functions by repeated application of a very low resolution 1D look-up table. There are many advantages to cascading multiple identical LUTs, including the promise of a very simple hardware design and the use of standard linear interpolation. Further, the complexity associated with unequal bin sizes can be avoided. We show that for realistic applications, including gamma correction, high dynamic range encoding and decoding curves, as well as tone mapping and inverse tone mapping applications, multiple cascaded look-up tables can reduce the approximation error by more than 50\% compared to a single look-up table with the same total memory footprint.},
  archive      = {J_TIP},
  author       = {Erik Reinhard and Elena Garces and Jürgen Stauder},
  doi          = {10.1109/TIP.2019.2949245},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2370-2379},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Repeated look-up tables},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local-adaptive image alignment based on triangular facet
approximation. <em>TIP</em>, <em>29</em>, 2356–2369. (<a
href="https://doi.org/10.1109/TIP.2019.2949424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient image alignment is the core problem in the research of panoramic stitching nowadays. This paper proposes a local-adaptive image alignment method based on triangular facet approximation, which directly manipulates the matching data in the camera coordinates, and therefore rises superior to the imaging model of cameras. A more robust planar transformation model is proposed and extended to be local-adaptive via combining it with two weighting strategies. By approximating the scene as a combination of adjacent triangular facets, the planar and spherical triangulation strategies are introduced to more efficiently align normal and fisheye images respectively. The efficiency of the proposed method are verified through the comparative experiments on several challenging cases both qualitatively and quantitatively.},
  archive      = {J_TIP},
  author       = {Jing Li and Baosong Deng and Rongfu Tang and Zhengming Wang and Ye Yan},
  doi          = {10.1109/TIP.2019.2949424},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2356-2369},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local-adaptive image alignment based on triangular facet approximation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep portrait image completion and extrapolation.
<em>TIP</em>, <em>29</em>, 2344–2355. (<a
href="https://doi.org/10.1109/TIP.2019.2945866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General image completion and extrapolation methods often fail on portrait images where parts of the human body need to be recovered - a task that requires accurate human body structure and appearance synthesis. We present a two-stage deep learning framework for tackling this problem. In the first stage, given a portrait image with an incomplete human body, we extract a complete, coherent human body structure through a human parsing network, which focuses on structure recovery inside the unknown region with the help of full-body pose estimation. In the second stage, we use an image completion network to fill the unknown region, guided by the structure map recovered in the first stage. For realistic synthesis the completion network is trained with both perceptual loss and conditional adversarial loss. We further propose a face refinement network to improve the fidelity of the synthesized face region. We evaluate our method on publicly-available portrait image datasets, and show that it outperforms other state-of-the-art general image completion methods. Our method enables new portrait image editing applications such as occlusion removal and portrait extrapolation. We further show that the proposed general learning framework can be applied to other types of images, e.g. animal images.},
  archive      = {J_TIP},
  author       = {Xian Wu and Rui-Long Li and Fang-Lue Zhang and Jian-Cheng Liu and Jue Wang and Ariel Shamir and Shi-Min Hu},
  doi          = {10.1109/TIP.2019.2945866},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2344-2355},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep portrait image completion and extrapolation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low cost gaze estimation: Knowledge-based solutions.
<em>TIP</em>, <em>29</em>, 2328–2343. (<a
href="https://doi.org/10.1109/TIP.2019.2946452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking technology in low resolution scenarios is not a completely solved issue to date. The possibility of using eye tracking in a mobile gadget is a challenging objective that would permit to spread this technology to non-explored fields. In this paper, a knowledge based approach is presented to solve gaze estimation in low resolution settings. The understanding of the high resolution paradigm permits to propose alternative models to solve gaze estimation. In this manner, three models are presented: a geometrical model, an interpolation model and a compound model, as solutions for gaze estimation for remote low resolution systems. Since this work considers head position essential to improve gaze accuracy, a method for head pose estimation is also proposed. The methods are validated in an optimal framework, I2Head database, which combines head and gaze data. The experimental validation of the models demonstrates their sensitivity to image processing inaccuracies, critical in the case of the geometrical model. Static and extreme movement scenarios are analyzed showing the higher robustness of compound and geometrical models in the presence of user&#39;s displacement. Accuracy values of about 3° have been obtained, increasing to values close to 5° in extreme displacement settings, results fully comparable with the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Ion Martinikorena and Andoni Larumbe-Bergera and Mikel Ariz and Sonia Porta and Rafael Cabeza and Arantxa Villanueva},
  doi          = {10.1109/TIP.2019.2946452},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2328-2343},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low cost gaze estimation: Knowledge-based solutions},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust low-rank tensor minimization via a new tensor
spectral <span class="math inline"><em>k</em></span> -support norm.
<em>TIP</em>, <em>29</em>, 2314–2327. (<a
href="https://doi.org/10.1109/TIP.2019.2946445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, based on a new tensor algebraic framework for third-order tensors, the tensor singular value decomposition (t-SVD) and its associated tubal rank definition have shed new light on low-rank tensor modeling. Its applications to robust image/video recovery and background modeling show promising performance due to its superior capability in modeling cross-channel/frame information. Under the t-SVD framework, we propose a new tensor norm called tensor spectral k-support norm (TSP-k) by an alternative convex relaxation. As an interpolation between the existing tensor nuclear norm (TNN) and tensor Frobenius norm (TFN), it is able to simultaneously drive minor singular values to zero to induce low-rankness, and to capture more global information for better preserving intrinsic structure. We provide the proximal operator and the polar operator for the TSP-k norm as key optimization blocks, along with two showcase optimization algorithms for mediumand large-size tensors. Experiments on synthetic, image and video datasets in medium and large sizes, all verify the superiority of the TSP-k norm and the effectiveness of both optimization methods in comparison with the existing counterparts.},
  archive      = {J_TIP},
  author       = {Jian Lou and Yiu-Ming Cheung},
  doi          = {10.1109/TIP.2019.2946445},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2314-2327},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust low-rank tensor minimization via a new tensor spectral $k$ -support norm},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A wave-shaped deep neural network for smoke density
estimation. <em>TIP</em>, <em>29</em>, 2301–2313. (<a
href="https://doi.org/10.1109/TIP.2019.2946126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoke density estimation from a single image is a totally new but highly ill-posed problem. To solve the problem, we stack several convolutional encoder-decoder structures together to propose a wave-shaped neural network, termed W-Net. Stacking encoder-decoders directly increases the network depth, leading to the enlargement of receptive fields for encoding more semantic information. To maximize the degrees of feature re-usage, we copy and resize the outputs of encoding layers to corresponding decoding layers, and then concatenate them to implement short-cut connections for improving spatial accuracy. The crests and troughs of W-Net are special structures containing abundant localization and semantic information, so we also use short-cut connections between these structures and decoding layers. Estimated smoke density is useful in many applications, such as smoke segmentation, smoke detection, disaster simulation. Experimental results show that our method outperforms existing methods on both smoke density estimation and segmentation. It also achieves satisfying results in visual detection of auto exhausts.},
  archive      = {J_TIP},
  author       = {Feiniu Yuan and Lin Zhang and Xue Xia and Qinghua Huang and Xuelong Li},
  doi          = {10.1109/TIP.2019.2946126},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2301-2313},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A wave-shaped deep neural network for smoke density estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How is gaze influenced by image transformations? Dataset and
model. <em>TIP</em>, <em>29</em>, 2287–2300. (<a
href="https://doi.org/10.1109/TIP.2019.2945857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data size is the bottleneck for developing deep saliency models, because collecting eye-movement data is very time-consuming and expensive. Most of current studies on human attention and saliency modeling have used high-quality stereotype stimuli. In real world, however, captured images undergo various types of transformations. Can we use these transformations to augment existing saliency datasets? Here, we first create a novel saliency dataset including fixations of 10 observers over 1900 images degraded by 19 types of transformations. Second, by analyzing eye movements, we find that observers look at different locations over transformed versus original images. Third, we utilize the new data over transformed images, called data augmentation transformation (DAT), to train deep saliency models. We find that label-preserving DATs with negligible impact on human gaze boost saliency prediction, whereas some other DATs that severely impact human gaze degrade the performance. These label-preserving valid augmentation transformations provide a solution to enlarge existing saliency datasets. Finally, we introduce a novel saliency model based on generative adversarial networks (dubbed GazeGAN). A modified U-Net is utilized as the generator of the GazeGAN, which combines classic “skip connection” with a novel “center-surround connection” (CSC) module. Our proposed CSC module mitigates trivial artifacts while emphasizing semantic salient regions, and increases model nonlinearity, thus demonstrating better robustness against transformations. Extensive experiments and comparisons indicate that GazeGAN achieves state-of-the-art performance over multiple datasets. We also provide a comprehensive comparison of 22 saliency models on various transformed scenes, which contributes a new robustness benchmark to saliency community. Our code and dataset are available at: https://github.com/CZHQuality/Sal-CFS-GAN.},
  archive      = {J_TIP},
  author       = {Zhaohui Che and Ali Borji and Guangtao Zhai and Xiongkuo Min and Guodong Guo and Patrick Le Callet},
  doi          = {10.1109/TIP.2019.2945857},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2287-2300},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {How is gaze influenced by image transformations? dataset and model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 2D quaternion sparse discriminant analysis. <em>TIP</em>,
<em>29</em>, 2271–2286. (<a
href="https://doi.org/10.1109/TIP.2019.2947775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis has been incorporated with various representations and measurements for dimension reduction and feature extraction. In this paper, we propose two-dimensional quaternion sparse discriminant analysis (2D-QSDA) that meets the requirements of representing RGB and RGB-D images. 2D-QSDA advances in three aspects: 1) including sparse regularization, 2D-QSDA relies only on the important variables, and thus shows good generalization ability to the out-of-sample data which are unseen during the training phase; 2) benefited from quaternion representation, 2D-QSDA well preserves the high order correlation among different image channels and provides a unified approach to extract features from RGB and RGB-D images; 3) the spatial structure of the input images is also retained via the matrix-based processing. We tackle the constrained trace ratio problem of 2D-QSDA by solving a corresponding constrained trace difference problem, which is then transformed into a quaternion sparse regression (QSR) model. Afterward, we reformulate the QSR model to an equivalent complex form to avoid the processing of the complicated structure of quaternions. A nested iterative algorithm is designed to learn the solution of 2D-QSDA in the complex space and then we convert this solution back to the quaternion domain. To improve the separability of 2D-QSDA, we further propose 2D-QSDAw using the weighted pairwise between-class distances. Extensive experiments on RGB and RGB-D databases demonstrate the effectiveness of 2D-QSDA and 2D-QSDAw compared with peer competitors.},
  archive      = {J_TIP},
  author       = {Xiaolin Xiao and Yongyong Chen and Yue-Jiao Gong and Yicong Zhou},
  doi          = {10.1109/TIP.2019.2947775},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2271-2286},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {2D quaternion sparse discriminant analysis},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual saliency detection via kernelized subspace ranking
with active learning. <em>TIP</em>, <em>29</em>, 2258–2270. (<a
href="https://doi.org/10.1109/TIP.2019.2945679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection task has witnessed a booming interest for years, due to the growth of the computer vision community. In this paper, we introduce a new saliency model that performs active learning with kernelized subspace ranker (KSR) referred to as KSR-AL. This pool-based active learning algorithm ranks the informativeness of unlabeled data by considering both uncertainty sampling and information density, thereby minimizing the cost of labeling. The informative images are selected to train the KSR iteratively and incrementally. The learning model of this algorithm is designed on object-level proposals and region-based convolutional neural network (R-CNN) features, by jointly learning a Rank-SVM classifier and a subspace projection. When the active learning process meets its stopping criteria, the saliency map of each image is generated by a weight fusion of its top-ranked proposals, whose ranking scores are graded by the learned ranker. We show that the KSR-AL achieves a reduction in annotation, as well as improvement in performance, compared with the supervised learning scheme. Besides, the proposed algorithm also outperforms the state-of-the-art methods. These improvements are demonstrated by extensive experiments on six publicly available benchmark datasets.},
  archive      = {J_TIP},
  author       = {Lihe Zhang and Jiayu Sun and Tiantian Wang and Yifan Min and Huchuan Lu},
  doi          = {10.1109/TIP.2019.2945679},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2258-2270},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visual saliency detection via kernelized subspace ranking with active learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank approximation via generalized reweighted iterative
nuclear and frobenius norms. <em>TIP</em>, <em>29</em>, 2244–2257. (<a
href="https://doi.org/10.1109/TIP.2019.2949383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank approximation problem has recently attracted wide concern due to its excellent performance in realworld applications such as image restoration, traffic monitoring, and face recognition. Compared with the classic nuclear norm, the Schatten-p norm is stated to be a closer approximation to restrain the singular values for practical applications in the real world. However, Schatten-p norm minimization is a challenging non-convex, non-smooth, and non-Lipschitz problem. In this paper, inspired by the reweighted ℓ 1 and ℓ 2 norm for compressive sensing, the generalized iterative reweighted nuclear norm (GIRNN) and the generalized iterative reweighted Frobenius norm (GIRFN) algorithms are proposed to approximate Schatten-p norm minimization. By involving the proposed algorithms, the problem becomes more tractable and the closed solutions are derived from the iteratively reweighted subproblems. In addition, we prove that both proposed algorithms converge at a linear rate to a bounded optimum. Numerical experiments for the practical matrix completion (MC), robust principal component analysis (RPCA), and image decomposition problems are illustrated to validate the superior performance of both algorithms over some common state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yan Huang and Guisheng Liao and Yijian Xiang and Lei Zhang and Jie Li and Arye Nehorai},
  doi          = {10.1109/TIP.2019.2949383},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2244-2257},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low-rank approximation via generalized reweighted iterative nuclear and frobenius norms},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Toward intelligent sensing: Intermediate deep feature
compression. <em>TIP</em>, <em>29</em>, 2230–2243. (<a
href="https://doi.org/10.1109/TIP.2019.2941660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advances of hardware technology have made the intelligent analysis equipped at the front-end with deep learning more prevailing and practical. To better enable the intelligent sensing at the front-end, instead of compressing and transmitting visual signals or the ultimately utilized top-layer deep learning features, we propose to compactly represent and convey the intermediate-layer deep learning features with high generalization capability, to facilitate the collaborating approach between front and cloud ends. This strategy enables a good balance among the computational load, transmission load and the generalization ability for cloud servers when deploying the deep neural networks for large scale cloud based visual analysis. Moreover, the presented strategy also makes the standardization of deep feature coding more feasible and promising, as a series of tasks can simultaneously benefit from the transmitted intermediate layer features. We also present the results for evaluations of both lossless and lossy deep feature compression, which provide meaningful investigations and baselines for future research and standardization activities.},
  archive      = {J_TIP},
  author       = {Zhuo Chen and Kui Fan and Shiqi Wang and Lingyu Duan and Weisi Lin and Alex Chichung Kot},
  doi          = {10.1109/TIP.2019.2941660},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2230-2243},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward intelligent sensing: Intermediate deep feature compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A volumetric approach to point cloud compression–part II:
Geometry compression. <em>TIP</em>, <em>29</em>, 2217–2229. (<a
href="https://doi.org/10.1109/TIP.2019.2957853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compression of point clouds has so far been confined to coding the positions of a discrete set of points in space and the attributes of those discrete points. We introduce an alternative approach based on volumetric functions, which are functions defined not just on a finite set of points, but throughout space. As in regression analysis, volumetric functions are continuous functions that are able to interpolate values on a finite set of points as linear combinations of continuous basis functions. Using a B-spline wavelet basis, we are able to code volumetric functions representing both geometry and attributes. Attribute compression is addressed in Part I of this paper, while geometry compression is addressed in Part II. Geometry is represented implicitly as the level set of a volumetric function (the signed distance function or similar). Experimental results show that geometry compression using volumetric functions improves over the methods used in the emerging MPEG Point Cloud Compression (G-PCC) standard.},
  archive      = {J_TIP},
  author       = {Maja Krivokuća and Philip A. Chou and Maxim Koroteev},
  doi          = {10.1109/TIP.2019.2957853},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2217-2229},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A volumetric approach to point cloud Compression–Part II: Geometry compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A volumetric approach to point cloud compression—part i:
Attribute compression. <em>TIP</em>, <em>29</em>, 2203–2216. (<a
href="https://doi.org/10.1109/TIP.2019.2908095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compression of point clouds has so far been confined to coding the positions of a discrete set of points in space and the attributes of those discrete points. We introduce an alternative approach based on volumetric functions that are functions defined not just on a finite set of points but throughout space. As in regression analysis, volumetric functions are continuous functions that are able to interpolate values on a finite set of points as linear combinations of continuous basis functions. Using a B-spline wavelet basis, we are able to code volumetric functions representing both geometry and attributes. Geometry compression is addressed in Part II of this paper, while attribute compression is addressed in Part I. Attributes are represented by a volumetric function whose coefficients can be regarded as a critically sampled orthonormal transform that generalizes the recent successful Region-Adaptive Hierarchical (or Haar) Transform to higher orders. Experimental results show that attribute compression using higher order volumetric functions is an improvement over the first-order functions used in the emerging MPEG point cloud compression standard.},
  archive      = {J_TIP},
  author       = {Philip A. Chou and Maxim Koroteev and Maja Krivokuća},
  doi          = {10.1109/TIP.2019.2908095},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2203-2216},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A volumetric approach to point cloud Compression—Part i: Attribute compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep learning reconstruction framework for differential
phase-contrast computed tomography with incomplete data. <em>TIP</em>,
<em>29</em>, 2190–2202. (<a
href="https://doi.org/10.1109/TIP.2019.2947790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential phase-contrast computed tomography (DPC-CT) is a powerful analysis tool for soft-tissue and low-atomic-number samples. Limited by the implementation conditions, DPC-CT with incomplete projections happens quite often. Conventional reconstruction algorithms face difficulty when given incomplete data. They usually involve complicated parameter selection operations, which are also sensitive to noise and are time-consuming. In this paper, we report a new deep learning reconstruction framework for incomplete data DPC-CT. It involves the tight coupling of the deep learning neural network and DPC-CT reconstruction algorithm in the domain of DPC projection sinograms. The estimated result is not an artifact caused by the incomplete data, but a complete phase-contrast projection sinogram. After training, this framework is determined and can be used to reconstruct the final DPC-CT images for a given incomplete projection sinogram. Taking the sparse-view, limited-view and missing-view DPC-CT as examples, this framework is validated and demonstrated with synthetic and experimental data sets. Compared with other methods, our framework can achieve the best imaging quality at a faster speed and with fewer parameters. This work supports the application of the state-of-the-art deep learning theory in the field of DPC-CT.},
  archive      = {J_TIP},
  author       = {Jian Fu and Jianbing Dong and Feng Zhao},
  doi          = {10.1109/TIP.2019.2947790},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2190-2202},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A deep learning reconstruction framework for differential phase-contrast computed tomography with incomplete data},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-linearized proximal alternating minimization for a
discrete mumford–shah model. <em>TIP</em>, <em>29</em>, 2176–2189. (<a
href="https://doi.org/10.1109/TIP.2019.2944561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Mumford-Shah model is a standard model in image segmentation, and due to its difficulty, many approximations have been proposed. The major interest of this functional is to enable joint image restoration and contour detection. In this work, we propose a general formulation of the discrete counterpart of the Mumford-Shah functional, adapted to nonsmooth penalizations, fitting the assumptions required by the Proximal Alternating Linearized Minimization (PALM), with convergence guarantees. A second contribution aims to relax some assumptions on the involved functionals and derive a novel Semi-Linearized Proximal Alternated Minimization (SL-PAM) algorithm, with proved convergence. We compare the performances of the algorithm with several nonsmooth penalizations, for Gaussian and Poisson denoising, image restoration and RGB-color denoising. We compare the results with state-of-the-art convex relaxations of the Mumford-Shah functional, and a discrete version of the Ambrosio-Tortorelli functional. We show that the SL-PAM algorithm is faster than the original PALM algorithm, and leads to competitive denoising, restoration and segmentation results.},
  archive      = {J_TIP},
  author       = {Marion Foare and Nelly Pustelnik and Laurent Condat},
  doi          = {10.1109/TIP.2019.2944561},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2176-2189},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-linearized proximal alternating minimization for a discrete Mumford–Shah model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised rotation factorization in restricted boltzmann
machines. <em>TIP</em>, <em>29</em>, 2166–2175. (<a
href="https://doi.org/10.1109/TIP.2019.2946455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding suitable image representations for the task at hand is critical in computer vision. Different approaches extending the original Restricted Boltzmann Machine (RBM) model have recently been proposed to offer rotation-invariant feature learning. In this paper, we present an extended novel RBM that learns rotation invariant features by explicitly factorizing for rotation nuisance in 2D image inputs within an unsupervised framework. While the goal is to learn invariant features, our model infers an orientation per input image during training, using information related to the reconstruction error. The training process is regularised by a Kullback-Leibler divergence, offering stability and consistency. We used the γ-score, a measure that calculates the amount of invariance, to mathematically and experimentally demonstrate that our approach indeed learns rotation invariant features. We show that our method outperforms the current state-of-the-art RBM approaches for rotation invariant feature learning on three different benchmark datasets, by measuring the performance with the test accuracy of an SVM classifier. Our implementation is available at https://bitbucket.org/tuttoweb/rotinvrbm.},
  archive      = {J_TIP},
  author       = {Mario Valerio Giuffrida and Sotirios A. Tsaftaris},
  doi          = {10.1109/TIP.2019.2946455},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2166-2175},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised rotation factorization in restricted boltzmann machines},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Face hallucination using cascaded super-resolution and
identity priors. <em>TIP</em>, <em>29</em>, 2150–2165. (<a
href="https://doi.org/10.1109/TIP.2019.2945835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we address the problem of hallucinating high-resolution facial images from low-resolution inputs at high magnification factors. We approach this task with convolutional neural networks (CNNs) and propose a novel (deep) face hallucination model that incorporates identity priors into the learning procedure. The model consists of two main parts: i) a cascaded super-resolution network that upscales the low-resolution facial images, and ii) an ensemble of face recognition models that act as identity priors for the super-resolution network during training. Different from most competing super-resolution techniques that rely on a single model for upscaling (even with large magnification factors), our network uses a cascade of multiple SR models that progressively upscale the low-resolution images using steps of $2\times $ . This characteristic allows us to apply supervision signals (target appearances) at different resolutions and incorporate identity constraints at multiple-scales. The proposed C-SRIP model (Cascaded Super Resolution with Identity Priors) is able to upscale (tiny) low-resolution images captured in unconstrained conditions and produce visually convincing results for diverse low-resolution inputs. We rigorously evaluate the proposed model on the Labeled Faces in the Wild (LFW), Helen and CelebA datasets and report superior performance compared to the existing state-of-the-art.},
  archive      = {J_TIP},
  author       = {Klemen Grm and Walter J. Scheirer and Vitomir Štruc},
  doi          = {10.1109/TIP.2019.2945835},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2150-2165},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Face hallucination using cascaded super-resolution and identity priors},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Discriminative and uncorrelated feature selection with
constrained spectral analysis in unsupervised learning. <em>TIP</em>,
<em>29</em>, 2139–2149. (<a
href="https://doi.org/10.1109/TIP.2019.2947776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing unsupervised feature extraction methods frequently explore low-redundant features by an uncorrelated constraint. However, the constrained models might incur trivial solutions, due to the singularity of scatter matrix triggered by high-dimensional data. In this paper, we propose a regularized regression model with a generalized uncorrelated constraint for feature selection, which leads to three merits: 1) exploring the low-redundant and discriminative features; 2) avoiding the trivial solutions and 3) simplifying the optimization. Besides that, the local cluster structure is achieved via a novel constrained spectral analysis for the unsupervised learning, where MustLinks and Cannot-Links are transformed into a intrinsic graph and a penalty graph respectively, rather than incorporated into a mixed affinity graph. Accordingly, a discriminative and uncorrelated feature selection with constrained spectral analysis (DUCFS) is proposed with adopting σ-norm regularization for interpolating between F-norm and ℓ 2,1 -norm. Due to the flexible gradient and global differentiability, our model converges fast. Extensive experiments on benchmark datasets among several state-of-the-art approaches verify the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Xuelong Li and Han Zhang and Rui Zhang and Feiping Nie},
  doi          = {10.1109/TIP.2019.2947776},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2139-2149},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Discriminative and uncorrelated feature selection with constrained spectral analysis in unsupervised learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optical-flow based nonlinear weighted prediction for SDR and
backward compatible HDR video coding. <em>TIP</em>, <em>29</em>,
2123–2138. (<a href="https://doi.org/10.1109/TIP.2019.2945685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tone Mapping Operators (TMO) designed for videos can be classified into two categories. In a first approach, TMOs are temporal filtered to reduce temporal artifacts and provide a Standard Dynamic Range (SDR) content with improved temporal consistency. This however does not improve the SDR coding Rate Distortion (RD) performances. A second approach is to design the TMO with the goal of optimizing the SDR coding rate-distortion performances. This second category of methods may lead to SDR videos altering the artistic intent compared with the produced HDR content. In this paper, we combine the benefits of the two approaches by introducing new Weighted Prediction (WP) methods inside the HEVC SDR codec. As a first step, we demonstrate the interest of the WP methods compared to TMO optimized for RD performances. Then we present the newly introduced WP algorithm and WP modes. The WP algorithm consists in performing a global motion compensation between frames using an optical flow, and the new modes are based on non linear functions in contrast with the literature using only linear functions. The contribution of each novelty is studied independently and in a second time they are all put in competition to maximize the RD performances. Tests were made for HDR backward compatible compression but also for SDR compression only. In both cases, the proposed WP methods improve the RD performances while maintaining the SDR temporal coherency.},
  archive      = {J_TIP},
  author       = {David Gommelet and Julien Le Tanou and Aline Roumy and Michaël Ropert and Christine Guillemot},
  doi          = {10.1109/TIP.2019.2945685},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2123-2138},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optical-flow based nonlinear weighted prediction for SDR and backward compatible HDR video coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional analysis operator learning: Acceleration and
convergence. <em>TIP</em>, <em>29</em>, 2108–2122. (<a
href="https://doi.org/10.1109/TIP.2019.2937734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional operator learning is gaining attention in many signal processing and computer vision applications. Learning kernels has mostly relied on so-called patch-domain approaches that extract and store many overlapping patches across training signals. Due to memory demands, patch-domain methods have limitations when learning kernels from large datasets - particularly with multi-layered structures, e.g., convolutional neural networks - or when applying the learned kernels to high-dimensional signal recovery problems. The so-called convolution approach does not store many overlapping patches, and thus overcomes the memory problems particularly with careful algorithmic designs; it has been studied within the “synthesis” signal model, e.g., convolutional dictionary learning. This paper proposes a new convolutional analysis operator learning (CAOL) framework that learns an analysis sparsifying regularizer with the convolution perspective, and develops a new convergent Block Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve the corresponding block multi-nonconvex problems. To learn diverse filters within the CAOL framework, this paper introduces an orthogonality constraint that enforces a tight-frame filter condition, and a regularizer that promotes diversity between filters. Numerical experiments show that, with sharp majorizers, BPEG-M significantly accelerates the CAOL convergence rate compared to the state-of-the-art block proximal gradient (BPG) method. Numerical experiments for sparse-view computational tomography show that a convolutional sparsifying regularizer learned via CAOL significantly improves reconstruction quality compared to a conventional edge-preserving regularizer. Using more and wider kernels in a learned regularizer better preserves edges in reconstructed images.},
  archive      = {J_TIP},
  author       = {Il Yong Chun and Jeffrey A. Fessler},
  doi          = {10.1109/TIP.2019.2937734},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2108-2122},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Convolutional analysis operator learning: Acceleration and convergence},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning latent low-rank and sparse embedding for robust
image feature extraction. <em>TIP</em>, <em>29</em>, 2094–2107. (<a
href="https://doi.org/10.1109/TIP.2019.2938859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To defy the curse of dimensionality, the inputs are always projected from the original high-dimensional space into the target low-dimension space for feature extraction. However, due to the existence of noise and outliers, the feature extraction task for corrupted data is still a challenging problem. Recently, a robust method called low rank embedding (LRE) was proposed. Despite the success of LRE in experimental studies, it also has many disadvantages: 1) The learned projection cannot quantitatively interpret the importance of features. 2) LRE does not perform data reconstruction so that the features may not be capable of holding the main energy of the original “clean” data. 3) LRE explicitly transforms error into the target space. 4) LRE is an unsupervised method, which is only suitable for unsupervised scenarios. To address these problems, in this paper, we propose a novel method to exploit the latent discriminative features. In particular, we first utilize an orthogonal matrix to hold the main energy of the original data. Next, we introduce an 12,1-norm term to encourage the features to be more compact, discriminative and interpretable. Then, we enforce a columnwise 12,1-norm constraint on an error component to resist noise. Finally, we integrate a classification loss term into the objective function to fit supervised scenarios. Our method performs better than several state-of-the-art methods in terms of effectiveness and robustness, as demonstrated on six publicly available datasets.},
  archive      = {J_TIP},
  author       = {Zhenwen Ren and Quansen Sun and Bin Wu and Xiaoqian Zhang and Wenzhu Yan},
  doi          = {10.1109/TIP.2019.2938859},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2094-2107},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning latent low-rank and sparse embedding for robust image feature extraction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Mask SSD: An effective single-stage approach to object
instance segmentation. <em>TIP</em>, <em>29</em>, 2078–2093. (<a
href="https://doi.org/10.1109/TIP.2019.2947806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Mask SSD, an efficient and effective approach to address the challenging instance segmentation task. Based on a single-shot detector, Mask SSD detects all instances in an image and marks the pixels that belong to each instance. It consists of a detection subnetwork that predicts object categories and bounding box locations, and an instance-level segmentation subnetwork that generates the foreground mask for each instance. In the detection subnetwork, multi-scale and feedback features from different layers are used to better represent objects of various sizes and provide high-level semantic information. Then, we adopt an assistant classification network to guide per-class score prediction, which consists of objectness prior and category likelihood. The instance-level segmentation subnetwork outputs pixel-wise segmentation for each detection while providing the multi-scale and feedback features from different layers as input. These two subnetworks are jointly optimized by a multi-task loss function, which renders Mask SSD direct prediction on detection and segmentation results. We conduct extensive experiments on PASCAL VOC, SBD, and MS COCO datasets to evaluate the performance of Mask SSD. Experimental results verify that as compared with state-of-the-art approaches, our proposed method has a comparable precision with less speed overhead.},
  archive      = {J_TIP},
  author       = {Hui Zhang and Yonglin Tian and Kunfeng Wang and Wensheng Zhang and Fei-Yue Wang},
  doi          = {10.1109/TIP.2019.2947806},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2078-2093},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Mask SSD: An effective single-stage approach to object instance segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic image segmentation by scale-adaptive networks.
<em>TIP</em>, <em>29</em>, 2066–2077. (<a
href="https://doi.org/10.1109/TIP.2019.2941644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image segmentation is an important yet unsolved problem. One of the major challenges is the large variability of the object scales. To tackle this scale problem, we propose a Scale-Adaptive Network (SAN) which consists of multiple branches with each one taking charge of the segmentation of the objects of a certain range of scales. Given an image, SAN first computes a dense scale map indicating the scale of each pixel which is automatically determined by the size of the enclosing object. Then the features of different branches are fused according to the scale map to generate the final segmentation map. To ensure that each branch indeed learns the features for a certain scale, we propose a scale-induced ground-truth map and enforce a scale-aware segmentation loss for the corresponding branch in addition to the final loss. Extensive experiments over the PASCAL-Person-Part, the PASCAL VOC 2012, and the Look into Person datasets demonstrate that our SAN can handle the large variability of the object scales and outperforms the state-of-the-art semantic segmentation methods.},
  archive      = {J_TIP},
  author       = {Zilong Huang and Chunyu Wang and Xinggang Wang and Wenyu Liu and Jingdong Wang},
  doi          = {10.1109/TIP.2019.2941644},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2066-2077},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic image segmentation by scale-adaptive networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining faster r-CNN and model-driven clustering for
elongated object detection. <em>TIP</em>, <em>29</em>, 2052–2065. (<a
href="https://doi.org/10.1109/TIP.2019.2947792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While analyzing the performance of state-of-the-art R-CNN based generic object detectors, we find that the detection performance for objects with low object-region-percentages (ORPs) of the bounding boxes are much lower than the overall average. Elongated objects are examples. To address the problem of low ORPs for elongated object detection, we propose a hybrid approach which employs a Faster R-CNN to achieve robust detections of object parts, and a novel model-driven clustering algorithm to group the related partial detections and suppress false detections. First, we train a Faster R-CNN with partial region proposals of suitable and stable ORPs. Next, we introduce a deep CNN (DCNN) for orientation classification on the partial detections. Then, on the outputs of the Faster R-CNN and DCNN, the algorithm of adaptive model-driven clustering first initializes a model of an elongated object with a data-driven process on local partial detections, and refines the model iteratively by model-driven clustering and data-driven model updating. By exploiting Faster R-CNN to produce robust partial detections and model-driven clustering to form a global representation, our method is able to generate a tight oriented bounding box for elongated object detection. We evaluate the effectiveness of our approach on two typical elongated objects in the COCO dataset, and other typical elongated objects, including rigid objects (pens, screwdrivers and wrenches) and non-rigid objects (cracks). Experimental results show that, compared with the state-of-the-art approaches, our method achieves a large margin of improvements for both detection and localization of elongated objects in images.},
  archive      = {J_TIP},
  author       = {Fen Fang and Liyuan Li and Hongyuan Zhu and Joo-Hwee Lim},
  doi          = {10.1109/TIP.2019.2947792},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2052-2065},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Combining faster R-CNN and model-driven clustering for elongated object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep adversarial metric learning. <em>TIP</em>, <em>29</em>,
2037–2051. (<a href="https://doi.org/10.1109/TIP.2019.2948472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning an effective distance measurement between sample pairs plays an important role in visual analysis, where the training procedure largely relies on hard negative samples. However, hard negative samples usually account for the tiny minority in the training set, which may fail to fully describe the data distribution close to the decision boundary. In this paper, we present a deep adversarial metric learning (DAML) framework to generate synthetic hard negatives from the original negative samples, which is widely applicable to existing supervised deep metric learning algorithms. Different from existing sampling strategies which simply ignore numerous easy negatives, our DAML aim to exploit them by generating synthetic hard negatives adversarial to the learned metric as complements. We simultaneously train the feature embedding and hard negative generator in an adversarial manner, so that adequate and targeted synthetic hard negatives are created to learn more precise distance metrics. As a single transformation may not be powerful enough to describe the global input space under the attack of the hard negative generator, we further propose a deep adversarial multi-metric learning (DAMML) method by learning multiple local transformations for more complete description. We simultaneously exploit the collaborative and competitive relationships among multiple metrics, where the metrics display unity against the generator for effective distance measurement as well as compete for more training data through a metric discriminator to avoid overlapping. Extensive experimental results on five benchmark datasets show that our DAML and DAMML effectively boost the performance of existing deep metric learning approaches through adversarial learning.},
  archive      = {J_TIP},
  author       = {Yueqi Duan and Jiwen Lu and Wenzhao Zheng and Jie Zhou},
  doi          = {10.1109/TIP.2019.2948472},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2037-2051},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep adversarial metric learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational bayesian blind color deconvolution of
histopathological images. <em>TIP</em>, <em>29</em>, 2026–2036. (<a
href="https://doi.org/10.1109/TIP.2019.2946442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most whole-slide histological images are stained with two or more chemical dyes. Slide stain separation or color deconvolution is a crucial step within the digital pathology workflow. In this paper, the blind color deconvolution problem is formulated within the Bayesian framework. Starting from a multi-stained histological image, our model takes into account both spatial relations among the concentration image pixels and similarity between a given reference color-vector matrix and the estimated one. Using Variational Bayes inference, three efficient new blind color deconvolution methods are proposed which provide automated procedures to estimate all the model parameters in the problem. A comparison with classical and current state-of-the-art color deconvolution algorithms using real images has been carried out demonstrating the superiority of the proposed approach.},
  archive      = {J_TIP},
  author       = {Natalia Hidalgo-Gavira and Javier Mateos and Miguel Vega and Rafael Molina and Aggelos K. Katsaggelos},
  doi          = {10.1109/TIP.2019.2946442},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2026-2036},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational bayesian blind color deconvolution of histopathological images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Learning sparse and identity-preserved hidden attributes
for person re-identification. <em>TIP</em>, <em>29</em>, 2013–2025. (<a
href="https://doi.org/10.1109/TIP.2019.2946975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) aims at matching person images captured in non-overlapping camera views. To represent person appearance, low-level visual features are sensitive to environmental changes, while high-level semantic attributes, such as “short-hair” or “long-hair”, are relatively stable. Hence, researches have started to design semantic attributes to reduce the visual ambiguity. However, to train a prediction model for semantic attributes, it requires plenty of annotations, which are hard to obtain in practical large-scale applications. To alleviate the reliance on annotation efforts, we propose to incrementally generate Deep Hidden Attribute (DHA) based on baseline deep network for newly uncovered annotations. In particular, we propose an auto-encoder model that can be plugged into any deep network to mine latent information in an unsupervised manner. To optimize the effectiveness of DHA, we reform the auto-encoder model with additional orthogonal generation module, along with identity-preserving and sparsity constraints. 1) Orthogonally generating : In order to make DHAs different from each other, Singular Vector Decomposition (SVD) is introduced to generate DHAs orthogonally. 2) Identity-preserving constraint : The generated DHAs should be distinct for telling different persons, so we associate DHAs with person identities. 3) Sparsity constraint : To enhance the discriminability of DHAs, we also introduce the sparsity constraint to restrict the number of effective DHAs for each person. Experiments conducted on public datasets have validated the effectiveness of the proposed network. On two large-scale datasets, i.e. , Market-1501 and DukeMTMC-reID, the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Zheng Wang and Junjun Jiang and Yang Wu and Mang Ye and Xiang Bai and Shin’ichi Satoh},
  doi          = {10.1109/TIP.2019.2946975},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2013-2025},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning sparse and identity-preserved hidden attributes for person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spaghetti labeling: Directed acyclic graphs for block-based
connected components labeling. <em>TIP</em>, <em>29</em>, 1999–2012. (<a
href="https://doi.org/10.1109/TIP.2019.2946979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected Components Labeling is an essential step of many Image Processing and Computer Vision tasks. Since the first proposal of a labeling algorithm, which dates back to the sixties, many approaches have optimized the computational load needed to label an image. In particular, the use of decision forests and state prediction have recently appeared as valuable strategies to improve performance. However, due to the overhead of the manual construction of prediction states and the size of the resulting machine code, the application of these strategies has been restricted to small masks, thus ignoring the benefit of using a block-based approach. In this paper, we combine a block-based mask with state prediction and code compression: the resulting algorithm is modeled as a Directed Rooted Acyclic Graph with multiple entry points, which is automatically generated without manual intervention. When tested on synthetic and real datasets, in comparison with optimized implementations of state-of-the-art algorithms, the proposed approach shows superior performance, surpassing the results obtained by all compared approaches in all settings.},
  archive      = {J_TIP},
  author       = {Federico Bolelli and Stefano Allegretti and Lorenzo Baraldi and Costantino Grana},
  doi          = {10.1109/TIP.2019.2946979},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1999-2012},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spaghetti labeling: Directed acyclic graphs for block-based connected components labeling},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast single image dehazing using saturation based
transmission map estimation. <em>TIP</em>, <em>29</em>, 1985–1998. (<a
href="https://doi.org/10.1109/TIP.2019.2948279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing has been a challenging problem because of its ill-posed nature. For this reason, numerous efforts have been made in the field of haze removal. This paper proposes a simple, fast, and powerful algorithm for haze removal. The medium transmission is derived as a function of the saturation of the scene radiance only, and the saturation of scene radiance is estimated using a simple stretching method. A different medium transmission can be estimated for each pixel because this method does not assume that transmission is constant in a small patch. Furthermore, this paper presents a color veil removing algorithm, which is useful for an image with fine or yellow dust, using the white balance technique. The proposed algorithm requires no training, prior, and refinement process. The simulation results show that the proposed dehazing scheme outperforms state-of-the-art dehazing approaches in terms of both computational complexity and dehazing efficiency.},
  archive      = {J_TIP},
  author       = {Se Eun Kim and Tae Hee Park and Il Kyu Eom},
  doi          = {10.1109/TIP.2019.2948279},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1985-1998},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast single image dehazing using saturation based transmission map estimation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attended end-to-end architecture for age estimation from
facial expression videos. <em>TIP</em>, <em>29</em>, 1972–1984. (<a
href="https://doi.org/10.1109/TIP.2019.2948288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main challenges of age estimation from facial expression videos lie not only in the modeling of the static facial appearance, but also in the capturing of the temporal facial dynamics. Traditional techniques to this problem focus on constructing handcrafted features to explore the discriminative information contained in facial appearance and dynamics separately. This relies on sophisticated feature-refinement and framework-design. In this paper, we present an end-to-end architecture for age estimation, called Spatially-Indexed Attention Model (SIAM), which is able to simultaneously learn both the appearance and dynamics of age from raw videos of facial expressions. Specifically, we employ convolutional neural networks to extract effective latent appearance representations and feed them into recurrent networks to model the temporal dynamics. More importantly, we propose to leverage attention models for salience detection in both the spatial domain for each single image and the temporal domain for the whole video as well. We design a specific spatially-indexed attention mechanism among the convolutional layers to extract the salient facial regions in each individual image, and a temporal attention layer to assign attention weights to each frame. This two-pronged approach not only improves the performance by allowing the model to focus on informative frames and facial areas, but it also offers an interpretable correspondence between the spatial facial regions as well as temporal frames, and the task of age estimation. We demonstrate the strong performance of our model in experiments on a large, gender-balanced database with 400 subjects with ages spanning from 8 to 76 years. Experiments reveal that our model exhibits significant superiority over the state-of-the-art methods given sufficient training data.},
  archive      = {J_TIP},
  author       = {Wenjie Pei and Hamdi Dibeklioğlu and Tadas Baltrušaitis and David M. J. Tax},
  doi          = {10.1109/TIP.2019.2948288},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1972-1984},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attended end-to-end architecture for age estimation from facial expression videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep unbiased embedding transfer for zero-shot learning.
<em>TIP</em>, <em>29</em>, 1958–1971. (<a
href="https://doi.org/10.1109/TIP.2019.2947780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning aims to recognize objects which do not appear in the training dataset. Previous prevalent mapping-based zero-shot learning methods suffer from the projection domain shift problem due to the lack of image classes in the training stage. In order to alleviate the projection domain shift problem, a deep unbiased embedding transfer (DUET) model is proposed in this paper. The DUET model is composed of a deep embedding transfer (DET) module and an unseen visual feature generation (UVG) module. In the DET module, a novel combined embedding transfer net which integrates the complementary merits of the linear and nonlinear embedding mapping functions is proposed to connect the visual space and semantic space. What&#39;s more, the end-to-end joint training process is implemented to train the visual feature extractor and the combined embedding transfer net simultaneously. In the UVG module, a visual feature generator trained with a conditional generative adversarial framework is used to synthesize the visual features of the unseen classes to ease the disturbance of the projection domain shift problem. Furthermore, a quantitative index, namely the score of resistance on domain shift (ScoreRDS), is proposed to evaluate different models regarding their resistance capability on the projection domain shift problem. The experiments on five zero-shot learning benchmarks verify the effectiveness of the proposed DUET model. As demonstrated by the qualitative and quantitative analysis, the unseen class visual feature generation, the combined embedding transfer net and the end-to-end joint training process all contribute to alleviating projection domain shift in zero-shot learning.},
  archive      = {J_TIP},
  author       = {Zhen Jia and Zhang Zhang and Liang Wang and Caifeng Shan and Tieniu Tan},
  doi          = {10.1109/TIP.2019.2947780},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1958-1971},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep unbiased embedding transfer for zero-shot learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Needles in a haystack: Tracking city-scale moving vehicles
from continuously moving satellite. <em>TIP</em>, <em>29</em>,
1944–1957. (<a href="https://doi.org/10.1109/TIP.2019.2944097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the satellite videos have been captured by moving satellite platforms. In contrast to consumers, movies, and common surveillance videos, satellite videos can record the snapshots of city-scale scenes. In a broad field-of-view of satellite videos, each moving target would be very tiny and usually composed of several pixels in frames. Even worse, the noise signals also exist in the video frames, and the background of the video frames subpixel-level and uneven moving thanks to the motion of satellites. We argue that it is a novel type of computer vision task since previous technologies are unable to detect such tiny moving vehicles efficiently. This paper proposes a novel framework that can identify small moving vehicles in satellite videos. In particular, we offer a novel detecting algorithm based on the local noise modeling. We differentiate the potential vehicle targets from noise patterns by an exponential probability distribution. Subsequently, a multi-morphological-cue based discrimination strategy is designed to distinguish correct vehicle targets from the existing noises further. Another significant contribution is to introduce a series of evaluation protocols to measure the performance of tiny moving vehicle detection systematically. We annotate satellite videos manually to test our algorithms under different evaluation criterions. The proposed algorithm is also compared with the state-of-the-art baselines, which demonstrates the advantages of our framework over the benchmarks. Besides, the dataset would be downloaded from http://first.authour.github.com.},
  archive      = {J_TIP},
  author       = {Wei Ao and Yanwei Fu and Xiyue Hou and Feng Xu},
  doi          = {10.1109/TIP.2019.2944097},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1944-1957},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Needles in a haystack: Tracking city-scale moving vehicles from continuously moving satellite},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallax tolerant light field stitching for hand-held
plenoptic cameras. <em>TIP</em>, <em>29</em>, 1929–1943. (<a
href="https://doi.org/10.1109/TIP.2019.2945687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) stitching is a potential solution to improve the field of view (FOV) for hand-held plenoptic cameras. Existing LF stitching methods cannot provide accurate registration for scenes with large depth variation. In this paper, a novel LF stitching method is proposed to handle parallax in the LFs more flexibly and accurately. First, a depth layer map (DLM) is proposed to guarantee adequate feature points on each depth layer. For the regions of nondeterministic depth, superpixel layer map (SLM) is proposed based on LF spatial correlation analysis to refine the depth layer assignments. Then, DLM-SLM-based LF registration is proposed to derive the location dependent homography transforms accurately and to warp LFs to its corresponding position without parallax interference. 4D graph-cut is further applied to fuse the registration results for higher LF spatial continuity and angular continuity. Horizontal, vertical and multi-LF stitching are tested for different scenes, which demonstrates the superior performance provided by the proposed method in terms of subjective quality of the stitched LFs, epipolar plane image consistency in the stitched LF, and perspective-averaged correlation between the stitched LF and the input LFs.},
  archive      = {J_TIP},
  author       = {Xin Jin and Pei Wang and Qionghai Dai},
  doi          = {10.1109/TIP.2019.2945687},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1929-1943},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Parallax tolerant light field stitching for hand-held plenoptic cameras},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-motion-assisted tensor completion method for background
initialization in complex video sequences. <em>TIP</em>, <em>29</em>,
1915–1928. (<a href="https://doi.org/10.1109/TIP.2019.2946098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The background Initialization (BI) problem has attracted the attention of researchers in different image/video processing fields. Recently, a tensor-based technique called spatiotemporal slice-based singular value decomposition (SS-SVD) has been proposed for background initialization. SS-SVD applies the SVD on the tensor slices and estimates the background from low-rank information. Despite its efficiency in background initialization, the performance of SS-SVD requires further improvement in the case of complex sequences with challenges such as stationary foreground objects (SFOs), illumination changes, low frame-rate, and clutter. In this paper, a self-motion-assisted tensor completion method is proposed to overcome the limitations of SS-SVD in complex video sequences and enhance the visual appearance of the initialized background. With the proposed method, the motion information, extracted from the sparse portion of the tensor slices, is incorporated with the low-rank information of SS-SVD to eliminate existing artifacts in the initiated background. Efficient blending schemes between the low-rank (background) and sparse (foreground) information of the tensor slices is developed for scenarios such as SFO removal, lighting variation processing, low frame-rate processing, crowdedness estimation, and best frame selection. The performance of the proposed method on video sequences with complex scenarios is compared with the top-ranked state-of-the-art techniques in the field of background initialization. The results not only validate the improved performance over the majority of the tested challenges but also demonstrate the capability of the proposed method to initialize the background in less computational time.},
  archive      = {J_TIP},
  author       = {Ibrahim Kajo and Nidal Kamel and Yassine Ruichek},
  doi          = {10.1109/TIP.2019.2946098},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1915-1928},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-motion-assisted tensor completion method for background initialization in complex video sequences},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatiotemporal knowledge distillation for efficient
estimation of aerial video saliency. <em>TIP</em>, <em>29</em>,
1902–1914. (<a href="https://doi.org/10.1109/TIP.2019.2946102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of video saliency estimation techniques has achieved significant advances along with the rapid development of Convolutional Neural Networks (CNNs). However, devices like cameras and drones may have limited computational capability and storage space so that the direct deployment of complex deep saliency models becomes infeasible. To address this problem, this paper proposes a dynamic saliency estimation approach for aerial videos via spatiotemporal knowledge distillation. In this approach, five components are involved, including two teachers, two students and the desired spatiotemporal model. The knowledge of spatial and temporal saliency is first separately transferred from the two complex and redundant teachers to their simple and compact students, while the input scenes are also degraded from high-resolution to low-resolution to remove the probable data redundancy so as to greatly speed up the feature extraction process. After that, the desired spatiotemporal model is further trained by distilling and encoding the spatial and temporal saliency knowledge of two students into a unified network. In this manner, the inter-model redundancy can be removed for the effective estimation of dynamic saliency on aerial videos. Experimental results show that the proposed approach is comparable to 11 state-of-the-art models in estimating visual saliency on aerial videos, while its speed reaches up to 28,738 FPS and 1,490.5 FPS on the GPU and CPU platforms, respectively.},
  archive      = {J_TIP},
  author       = {Jia Li and Kui Fu and Shengwei Zhao and Shiming Ge},
  doi          = {10.1109/TIP.2019.2946102},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1902-1914},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatiotemporal knowledge distillation for efficient estimation of aerial video saliency},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph sequence recurrent neural network for vision-based
freezing of gait detection. <em>TIP</em>, <em>29</em>, 1890–1901. (<a
href="https://doi.org/10.1109/TIP.2019.2946469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of gait (FoG) is one of the most common symptoms of Parkinson&#39;s disease (PD), a neurodegenerative disorder which impacts millions of people around the world. Accurate assessment of FoG is critical for the management of PD and to evaluate the efficacy of treatments. Currently, the assessment of FoG requires well-trained experts to perform time-consuming annotations via vision-based observations. Thus, automatic FoG detection algorithms are needed. In this study, we formulate vision-based FoG detection, as a fine-grained graph sequence modelling task, by representing the anatomic joints in each temporal segment with a directed graph, since FoG events can be observed through the motion patterns of joints. A novel deep learning method is proposed, namely graph sequence recurrent neural network (GS-RNN), to characterize the FoG patterns by devising graph recurrent cells, which take graph sequences of dynamic structures as inputs. For the cases of which prior edge annotations are not available, a data-driven based adjacency estimation method is further proposed. To the best of our knowledge, this is one of the first studies on vision-based FoG detection using deep neural networks designed for graph sequences of dynamic structures. Experimental results on more than 150 videos collected from 45 patients demonstrated promising performance of the proposed GS-RNN for FoG detection with an AUC value of 0.90.},
  archive      = {J_TIP},
  author       = {Kun Hu and Zhiyong Wang and Wei Wang and Kaylena A. Ehgoetz Martens and Liang Wang and Tieniu Tan and Simon J. G. Lewis and David Dagan Feng},
  doi          = {10.1109/TIP.2019.2946469},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1890-1901},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph sequence recurrent neural network for vision-based freezing of gait detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Saliency detection via depth-induced cellular automata on
light field. <em>TIP</em>, <em>29</em>, 1879–1889. (<a
href="https://doi.org/10.1109/TIP.2019.2942434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorrect saliency detection such as false alarms and missed alarms may lead to potentially severe consequences in various application areas. Effective separation of salient objects in complex scenes is a major challenge in saliency detection. In this paper, we propose a new method for saliency detection on light field to improve the saliency detection in challenging scenes. We construct an object-guided depth map, which acts as an inducer to efficiently incorporate the relations among light field cues, by using abundant light field cues. Furthermore, we enforce spatial consistency by constructing an optimization model, named Depth-induced Cellular Automata (DCA), in which the saliency value of each superpixel is updated by exploiting the intrinsic relevance of its similar regions. Additionally, the proposed DCA model enables inaccurate saliency maps to achieve a high level of accuracy. We analyze our approach on one publicly available dataset. Experiments show the proposed method is robust to a wide range of challenging scenes and outperforms the state-of-the-art 2D/3D/4D (light-field) saliency detection approaches.},
  archive      = {J_TIP},
  author       = {Yongri Piao and Xiao Li and Miao Zhang and Jingyi Yu and Huchuan Lu},
  doi          = {10.1109/TIP.2019.2942434},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1879-1889},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Saliency detection via depth-induced cellular automata on light field},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attention-aware multi-task convolutional neural networks.
<em>TIP</em>, <em>29</em>, 1867–1878. (<a
href="https://doi.org/10.1109/TIP.2019.2944522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task deep learning methods learn multiple tasks simultaneously and share representations amongst them, so information from related tasks improves learning within one task. The generalization capabilities of the produced models are substantially enhanced. Typical multi-task deep learning models usually share representations of different tasks in lower layers of the network, and separate representations of different tasks in higher layers. However, different groups of tasks always have different requirements for sharing representations, so the required design criterion does not necessarily guarantee that the obtained network architecture is optimal. In addition, most existing methods ignore the redundancy problem and lack the pre-screening process for representations before they are shared. Here, we propose a model called Attention-aware Multi-task Convolutional Neural Network, which automatically learns appropriate sharing through end-to-end training. The attention mechanism is introduced into our architecture to suppress redundant contents contained in the representations. The shortcut connection is adopted to preserve useful information. We evaluate our model by carrying out experiments on different task groups and different datasets. Our model demonstrates an improvement over existing techniques in many experiments, indicating the effectiveness and the robustness of the model. We also demonstrate the importance of attention mechanism and shortcut connection in our model.},
  archive      = {J_TIP},
  author       = {Kejie Lyu and Yingming Li and Zhongfei Zhang},
  doi          = {10.1109/TIP.2019.2944522},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1867-1878},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attention-aware multi-task convolutional neural networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mumford–shah loss functional for image segmentation with
deep learning. <em>TIP</em>, <em>29</em>, 1856–1866. (<a
href="https://doi.org/10.1109/TIP.2019.2941265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent state-of-the-art image segmentation algorithms are mostly based on deep neural networks, thanks to their high performance and fast computation time. However, these methods are usually trained in a supervised manner, which requires large number of high quality ground-truth segmentation masks. On the other hand, classical image segmentation approaches such as level-set methods are formulated in a self-supervised manner by minimizing energy functions such as Mumford-Shah functional, so they are still useful to help generate segmentation masks without labels. Unfortunately, these algorithms are usually computationally expensive and often have limitation in semantic segmentation. In this paper, we propose a novel loss function based on Mumford-Shah functional that can be used in deep-learning based image segmentation without or with small labeled data. This loss function is based on the observation that the softmax layer of deep neural networks has striking similarity to the characteristic function in the Mumford-Shah functional. We show that the new loss function enables semi-supervised and unsupervised segmentation. In addition, our loss function can also be used as a regularized function to enhance supervised semantic segmentation algorithms. Experimental results on multiple datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Boah Kim and Jong Chul Ye},
  doi          = {10.1109/TIP.2019.2941265},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1856-1866},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Mumford–Shah loss functional for image segmentation with deep learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient evaluation of image quality via deep-learning
approximation of perceptual metrics. <em>TIP</em>, <em>29</em>,
1843–1855. (<a href="https://doi.org/10.1109/TIP.2019.2944079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image metrics based on Human Visual System (HVS) play a remarkable role in the evaluation of complex image processing algorithms. However, mimicking the HVS is known to be complex and computationally expensive (both in terms of time and memory), and its usage is thus limited to a few applications and to small input data. All of this makes such metrics not fully attractive in real-world scenarios. To address these issues, we propose Deep Image Quality Metric (DIQM), a deep-learning approach to learn the global image quality feature (mean-opinion-score). DIQM can emulate existing visual metrics efficiently, reducing the computational costs by more than an order of magnitude with respect to existing implementations.},
  archive      = {J_TIP},
  author       = {Alessandro Artusi and Francesco Banterle and Fabio Carra and Alejandro Moreno},
  doi          = {10.1109/TIP.2019.2944079},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1843-1855},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient evaluation of image quality via deep-learning approximation of perceptual metrics},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stereoscopic image generation from light field with
disparity scaling and super-resolution. <em>TIP</em>, <em>29</em>,
1827–1842. (<a href="https://doi.org/10.1109/TIP.2019.2944519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method to generate stereoscopic images from light-field images with the intended depth range and simultaneously perform image super-resolution. Subject to the small baseline of neighboring subaperture views and low spatial resolution of light-field images captured using compact commercial light-field cameras, the disparity range of any two subaperture views is usually very small. We propose a method to control the disparity range of the target stereoscopic images with linear or nonlinear disparity scaling and properly resolve the disocclusion problem with the aid of a smooth energy term previously used for texture synthesis. The left and right views of the target stereoscopic image are simultaneously generated by a unified optimization framework, which preserves content coherence between the left and right views by a coherence energy term. The disparity range of the target stereoscopic image can be larger than that of the input light field image. This benefits many light field image-based applications, e.g., displaying light field images on various stereo display devices and generating stereoscopic panoramic images from a light field image montage. An extensive experimental evaluation demonstrates the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Tao Yan and Jianbo Jiao and Wenxi Liu and Rynson W. H. Lau},
  doi          = {10.1109/TIP.2019.2944519},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1827-1842},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Stereoscopic image generation from light field with disparity scaling and super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skeleton filter: A self-symmetric filter for skeletonization
in noisy text images. <em>TIP</em>, <em>29</em>, 1815–1826. (<a
href="https://doi.org/10.1109/TIP.2019.2944560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustly computing the skeletons of objects in natural images is difficult due to the large variations in shape boundaries and the large amount of noise in the images. Inspired by recent findings in neuroscience, we propose the Skeleton Filter, which is a novel model for skeleton extraction from natural images. The Skeleton Filter consists of a pair of oppositely oriented Gabor-like filters; by applying the Skeleton Filter in various orientations to an image at multiple resolutions and fusing the results, our system can robustly extract the skeleton even under highly noisy conditions. We evaluate the performance of our approach using challenging noisy text datasets and demonstrate that our pipeline realizes state-of-the-art performance for extracting the text skeleton. Moreover, the presence of Gabor filters in the human visual system and the simple architecture of the Skeleton Filter can help explain the strong capabilities of humans in perceiving skeletons of objects, even under dramatically noisy conditions.},
  archive      = {J_TIP},
  author       = {Xiuxiu Bai and Lele Ye and Jihua Zhu and Li Zhu and Taku Komura},
  doi          = {10.1109/TIP.2019.2944560},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1815-1826},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Skeleton filter: A self-symmetric filter for skeletonization in noisy text images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind quality metric of DIBR-synthesized images in the
discrete wavelet transform domain. <em>TIP</em>, <em>29</em>, 1802–1814.
(<a href="https://doi.org/10.1109/TIP.2019.2945675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free viewpoint video (FVV) has received considerable attention owing to its widespread applications in several areas such as immersive entertainment, remote surveillance and distanced education. Since FVV images are synthesized via a depth image-based rendering (DIBR) procedure in the “blind” environment (without reference images), a real-time and reliable blind quality assessment metric is urgently required. However, the existing image quality assessment metrics are insensitive to the geometric distortions engendered by DIBR. In this research, a novel blind method of DIBR-synthesized images is proposed based on measuring geometric distortion, global sharpness and image complexity. First, a DIBR-synthesized image is decomposed into wavelet subbands by using discrete wavelet transform. Then, the Canny operator is employed to detect the edges of the binarized low-frequency subband and high-frequency subbands. The edge similarities between the binarized low-frequency subband and high-frequency subbands are further computed to quantify geometric distortions in DIBR-synthesized images. Second, the log-energies of wavelet subbands are calculated to evaluate global sharpness in DIBR-synthesized images. Third, a hybrid filter combining the autoregressive and bilateral filters is adopted to compute image complexity. Finally, the overall quality score is derived to normalize geometric distortion and global sharpness by the image complexity. Experiments show that our proposed quality method is superior to the competing reference-free state-of-the-art DIBR-synthesized image quality models.},
  archive      = {J_TIP},
  author       = {Guangcheng Wang and Zhongyuan Wang and Ke Gu and Leida Li and Zhifang Xia and Lifang Wu},
  doi          = {10.1109/TIP.2019.2945675},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1802-1814},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind quality metric of DIBR-synthesized images in the discrete wavelet transform domain},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning interleaved cascade of shrinkage fields for joint
image dehazing and denoising. <em>TIP</em>, <em>29</em>, 1788–1801. (<a
href="https://doi.org/10.1109/TIP.2019.2942504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing image dehazing methods deteriorate to different extents when processing hazy inputs with noise. The main reason is that the commonly adopted two-step strategy tends to amplify noise in the inverse operation of division by the transmission. To address this problem, we learn an interleaved Cascade of Shrinkage Fields (CSF) to reduce noise in jointly recovering the transmission map and the scene radiance from a single hazy image. Specifically, an auxiliary shrinkage field (SF) model is integrated into each cascade of the proposed scheme to reduce undesirable artifacts during the transmission estimation. Different from conventional CSF, our learned SF models have special visual patterns, which facilitate the specific task of noise reduction in haze removal. Furthermore, a numerical algorithm is proposed to efficiently update the scene radiance and the transmission map in each cascade. Extensive experiments on synthetic and real-world data demonstrate that the proposed algorithm performs favorably against state-of-the-art dehazing methods on hazy and noisy images.},
  archive      = {J_TIP},
  author       = {Qingbo Wu and Wenqi Ren and Xiaochun Cao},
  doi          = {10.1109/TIP.2019.2942504},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1788-1801},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning interleaved cascade of shrinkage fields for joint image dehazing and denoising},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hazy image decolorization with color contrast restoration.
<em>TIP</em>, <em>29</em>, 1776–1787. (<a
href="https://doi.org/10.1109/TIP.2019.2939946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to convert a hazy color image into a gray-scale image because the color contrast field of a hazy image is distorted. In this paper, a novel decolorization algorithm is proposed to transfer a hazy image into a distortion-recovered gray-scale image. To recover the color contrast field, the relationship between the restored color contrast and its distorted input is presented in CIELab color space. Based on this restoration, a nonlinear optimization problem is formulated to construct the resultant gray-scale image. A new differentiable approximation solution is introduced to solve this problem with an extension of the Huber loss function. Experimental results show that the proposed algorithm effectively preserves the global luminance consistency while represents the original color contrast in gray-scales, which is very close to the corresponding ground truth gray-scale one.},
  archive      = {J_TIP},
  author       = {Wei Wang and Zhengguo Li and Shiqian Wu and Liangcai Zeng},
  doi          = {10.1109/TIP.2019.2939946},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1776-1787},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hazy image decolorization with color contrast restoration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep spatial and temporal network for robust visual object
tracking. <em>TIP</em>, <em>29</em>, 1762–1775. (<a
href="https://doi.org/10.1109/TIP.2019.2942502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two key components that can be leveraged for visual tracking: (a) object appearances; and (b) object motions. Many existing techniques have recently employed deep learning to enhance visual tracking due to its superior representation power and strong learning ability, where most of them employed object appearances but few of them exploited object motions. In this work, a deep spatial and temporal network (DSTN) is developed for visual tracking by explicitly exploiting both the object representations from each frame and their dynamics along multiple frames in a video, such that it can seamlessly integrate the object appearances with their motions to produce compact object appearances and capture their temporal variations effectively. Our DSTN method, which is deployed into a tracking pipeline in a coarse-to-fine form, can perceive the subtle differences on spatial and temporal variations of the target (object being tracked), and thus it benefits from both off-line training and online fine-tuning. We have also conducted our experiments over four largest tracking benchmarks, including OTB-2013, OTB-2015, VOT2015, and VOT2017, and our experimental results have demonstrated that our DSTN method can achieve competitive performance as compared with the state-of-the-art techniques. The source code, trained models, and all the experimental results of this work will be made public available to facilitate further studies on this problem.},
  archive      = {J_TIP},
  author       = {Zhu Teng and Junliang Xing and Qiang Wang and Baopeng Zhang and Jianping Fan},
  doi          = {10.1109/TIP.2019.2942502},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1762-1775},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep spatial and temporal network for robust visual object tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint stereo video deblurring, scene flow estimation and
moving object segmentation. <em>TIP</em>, <em>29</em>, 1748–1761. (<a
href="https://doi.org/10.1109/TIP.2019.2945867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo videos for the dynamic scenes often show unpleasant blurred effects due to the camera motion and the multiple moving objects with large depth variations. Given consecutive blurred stereo video frames, we aim to recover the latent clean images, estimate the 3D scene flow and segment the multiple moving objects. These three tasks have been previously addressed separately, which fail to exploit the internal connections among these tasks and cannot achieve optimality. In this paper, we propose to jointly solve these three tasks in a unified framework by exploiting their intrinsic connections. To this end, we represent the dynamic scenes with the piece-wise planar model, which exploits the local structure of the scene and expresses various dynamic scenes. Under our model, these three tasks are naturally connected and expressed as the parameter estimation of 3D scene structure and camera motion (structure and motion for the dynamic scenes). By exploiting the blur model constraint, the moving objects and the 3D scene structure, we reach an energy minimization formulation for joint deblurring, scene flow and segmentation. We evaluate our approach extensively on both synthetic datasets and publicly available real datasets with fast-moving objects, camera motion, uncontrolled lighting conditions and shadows. Experimental results demonstrate that our method can achieve significant improvement in stereo video deblurring, scene flow estimation and moving object segmentation, over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Liyuan Pan and Yuchao Dai and Miaomiao Liu and Fatih Porikli and Quan Pan},
  doi          = {10.1109/TIP.2019.2945867},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1748-1761},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint stereo video deblurring, scene flow estimation and moving object segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Aggregation signature for small object tracking.
<em>TIP</em>, <em>29</em>, 1738–1747. (<a
href="https://doi.org/10.1109/TIP.2019.2940477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object tracking becomes an increasingly important task, which however has been largely unexplored in computer vision. The great challenges stem from the facts that: 1) small objects show extreme vague and variable appearances, and 2) they tend to be lost easier as compared to normal-sized ones due to the shaking of lens. In this paper, we propose a novel aggregation signature suitable for small object tracking, especially aiming for the challenge of sudden and large drift. We make three-fold contributions in this work. First, technically, we propose a new descriptor, named aggregation signature, based on saliency, able to represent highly distinctive features for small objects. Second, theoretically, we prove that the proposed signature matches the foreground object more accurately with a high probability. Third, experimentally, the aggregation signature achieves a high performance on multiple datasets, outperforming the state-of-the-art methods by large margins. Moreover, we contribute with two newly collected benchmark datasets, i.e., small90 and small112, for visually small object tracking. The datasets will be available in https://github.com/bczhangbczhang/.},
  archive      = {J_TIP},
  author       = {Chunlei Liu and Wenrui Ding and Jinyu Yang and Vittorio Murino and Baochang Zhang and Jungong Han and Guodong Guo},
  doi          = {10.1109/TIP.2019.2940477},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1738-1747},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Aggregation signature for small object tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep non-local kalman network for video compression artifact
reduction. <em>TIP</em>, <em>29</em>, 1725–1737. (<a
href="https://doi.org/10.1109/TIP.2019.2943214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video compression algorithms are widely used to reduce the huge size of video data, but they also introduce unpleasant visual artifacts due to the lossy compression. In order to improve the quality of the compressed videos, we proposed a deep non-local Kalman network for compression artifact reduction. Specifically, the video restoration is modeled as a Kalman filtering procedure and the decoded frames can be restored from the proposed deep Kalman model. Instead of using the noisy previous decoded frames as temporal information, the less noisy previous restored frame is employed in a recursive way, which provides the potential to generate high quality restored frames. In the proposed framework, several deep neural networks are utilized to estimate the corresponding states in the Kalman filter and integrated together in the deep Kalman filtering network. More importantly, we also exploit the non-local prior information by incorporating the spatial and temporal non-local networks for better restoration. Our approach takes the advantages of both the model-based methods and learning-based methods, by combining the recursive nature of the Kalman model and powerful representation ability of neural networks. Extensive experimental results on the Vimeo-90k and HEVC benchmark datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_TIP},
  author       = {Guo Lu and Xiaoyun Zhang and Wanli Ouyang and Dong Xu and Li Chen and Zhiyong Gao},
  doi          = {10.1109/TIP.2019.2943214},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1725-1737},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep non-local kalman network for video compression artifact reduction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image super-resolution as a defense against adversarial
attacks. <em>TIP</em>, <em>29</em>, 1711–1724. (<a
href="https://doi.org/10.1109/TIP.2019.2940533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks have achieved significant success across multiple computer vision tasks. However, they are vulnerable to carefully crafted, human-imperceptible adversarial noise patterns which constrain their deployment in critical security-sensitive systems. This paper proposes a computationally efficient image enhancement approach that provides a strong defense mechanism to effectively mitigate the effect of such adversarial perturbations. We show that deep image restoration networks learn mapping functions that can bring off-the-manifold adversarial samples onto the natural image manifold, thus restoring classification towards correct classes. A distinguishing feature of our approach is that, in addition to providing robustness against attacks, it simultaneously enhances image quality and retains models performance on clean images. Furthermore, the proposed method does not modify the classifier or requires a separate mechanism to detect adversarial images. The effectiveness of the scheme has been demonstrated through extensive experiments, where it has proven a strong defense in gray-box settings. The proposed scheme is simple and has the following advantages: 1) it does not require any model training or parameter optimization, 2) it complements other existing defense mechanisms, 3) it is agnostic to the attacked model and attack type, and 4) it provides superior performance across all popular attack algorithms. Our codes are publicly available at https://github.com/aamir-mustafa/super-resolution-adversarial-defense.},
  archive      = {J_TIP},
  author       = {Aamir Mustafa and Salman H. Khan and Munawar Hayat and Jianbing Shen and Ling Shao},
  doi          = {10.1109/TIP.2019.2940533},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1711-1724},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image super-resolution as a defense against adversarial attacks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneous multireference alignment for images with
application to 2D classification in single particle reconstruction.
<em>TIP</em>, <em>29</em>, 1699–1710. (<a
href="https://doi.org/10.1109/TIP.2019.2945686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the task of 2D classification in single particle reconstruction by cryo-electron microscopy (cryo-EM), we consider the problem of heterogeneous multireference alignment of images. In this problem, the goal is to estimate a (typically small) set of target images from a (typically large) collection of observations. Each observation is a rotated, noisy version of one of the target images. For each individual observation, neither the rotation nor which target image has been rotated are known. As the noise level in cryo-EM data is high, clustering the observations and estimating individual rotations is challenging. We propose a framework to estimate the target images directly from the observations, completely bypassing the need to cluster or register the images. The framework consists of two steps. First, we estimate rotation-invariant features of the images, such as the bispectrum. These features can be estimated to any desired accuracy, at any noise level, provided sufficiently many observations are collected. Then, we estimate the images from the invariant features. Numerical experiments on synthetic cryo-EM datasets demonstrate the effectiveness of the method. Ultimately, we outline future developments required to apply this method to experimental data.},
  archive      = {J_TIP},
  author       = {Chao Ma and Tamir Bendory and Nicolas Boumal and Fred Sigworth and Amit Singer},
  doi          = {10.1109/TIP.2019.2945686},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1699-1710},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Heterogeneous multireference alignment for images with application to 2D classification in single particle reconstruction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep coupled ISTA network for multi-modal image
super-resolution. <em>TIP</em>, <em>29</em>, 1683–1698. (<a
href="https://doi.org/10.1109/TIP.2019.2944270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a low-resolution (LR) image, multi-modal image super-resolution (MISR) aims to find the high-resolution (HR) version of this image with the guidance of an HR image from another modality. In this paper, we use a model-based approach to design a new deep network architecture for MISR. We first introduce a novel joint multi-modal dictionary learning (JMDL) algorithm to model cross-modality dependency. In JMDL, we simultaneously learn three dictionaries and two transform matrices to combine the modalities. Then, by unfolding the iterative shrinkage and thresholding algorithm (ISTA), we turn the JMDL model into a deep neural network, called deep coupled ISTA network. Since the network initialization plays an important role in deep network training, we further propose a layer-wise optimization algorithm (LOA) to initialize the parameters of the network before running back-propagation strategy. Specifically, we model the network initialization as a multi-layer dictionary learning problem, and solve it through convex optimization. The proposed LOA is demonstrated to effectively decrease the training loss and increase the reconstruction accuracy. Finally, we compare our method with other state-of-the-art methods in the MISR task. The numerical results show that our method consistently outperforms others both quantitatively and qualitatively at different upscaling factors for various multi-modal scenarios.},
  archive      = {J_TIP},
  author       = {Xin Deng and Pier Luigi Dragotti},
  doi          = {10.1109/TIP.2019.2944270},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1683-1698},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep coupled ISTA network for multi-modal image super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Receptive field size versus model depth for single image
super-resolution. <em>TIP</em>, <em>29</em>, 1669–1682. (<a
href="https://doi.org/10.1109/TIP.2019.2941327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of single image super-resolution (SISR) has been largely improved by innovative designs of deep architectures. An important claim raised by these designs is that the deep models have large receptive field size and strong nonlinearity. However, we are concerned about the question that which factor, receptive field size or model depth, is more critical for SISR. Towards revealing the answers, in this paper, we propose a strategy based on dilated convolution to investigate how the two factors affect the performance of SISR. Our findings from exhaustive investigations suggest that SISR is more sensitive to the changes of receptive field size than to the model depth variations, and that the model depth must be congruent with the receptive field size to produce improved performance. These findings inspire us to design a shallower architecture which can save computational and memory cost while preserving comparable effectiveness with respect to a much deeper architecture.},
  archive      = {J_TIP},
  author       = {Ruxin Wang and Mingming Gong and Dacheng Tao},
  doi          = {10.1109/TIP.2019.2941327},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1669-1682},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Receptive field size versus model depth for single image super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting block-sparsity for hyperspectral kronecker
compressive sensing: A tensor-based bayesian method. <em>TIP</em>,
<em>29</em>, 1654–1668. (<a
href="https://doi.org/10.1109/TIP.2019.2944722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian methods are attracting increasing attention in the field of compressive sensing (CS), as they are applicable to recover signals from random measurements. However, these methods have limited use in many tensor-based cases such as hyperspectral Kronecker compressive sensing (HKCS), because they exploit the sparsity in only one dimension. In this paper, we propose a novel Bayesian model for HKCS in an attempt to overcome the above limitation. The model exploits multi-dimensional block-sparsity such that the information redundancies in all dimensions are eliminated. Laplace prior distributions are employed for sparse coefficients in each dimension, and their coupling is consistent with the multi-dimensional block-sparsity model. Based on the proposed model, we develop a tensor-based Bayesian reconstruction algorithm, which decouples the hyperparameters for each dimension via a low-complexity technique. Experimental results demonstrate that the proposed method is able to provide more accurate reconstruction than existing Bayesian methods at a satisfactory speed. Additionally, the proposed method can not only be used for HKCS, it also has the potential to be extended to other multi-dimensional CS applications and to multi-dimensional block-sparse-based data recovery.},
  archive      = {J_TIP},
  author       = {Rongqiang Zhao and Qiang Wang and Jun Fu and Luquan Ren},
  doi          = {10.1109/TIP.2019.2944722},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1654-1668},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploiting block-sparsity for hyperspectral kronecker compressive sensing: A tensor-based bayesian method},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image processing methods for coronal hole segmentation,
matching, and map classification. <em>TIP</em>, <em>29</em>, 1641–1653.
(<a href="https://doi.org/10.1109/TIP.2019.2944057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents the results from a multi-year effort to develop and validate image processing methods for selecting the best physical models based on solar image observations. The approach consists of selecting the physical models based on their agreement with coronal holes extracted from the images. Ultimately, the goal is to use physical models to predict geomagnetic storms. We decompose the problem into three subproblems: (i) coronal hole segmentation based on physical constraints, (ii) matching clusters of coronal holes between different maps, and (iii) physical map classification. For segmenting coronal holes, we develop a multi-modal method that uses segmentation maps from three different methods to initialize a level-set method that evolves the initial coronal hole segmentation to the magnetic boundary. Then, we introduce a new method based on Linear Programming for matching clusters of coronal holes. The final matching is then performed using Random Forests. The methods were carefully validated using consensus maps derived from multiple readers, manual clustering, manual map classification, and method validation for 50 maps. The proposed multi-modal segmentation method significantly outperformed SegNet, U-net, Henney-Harvey, and FCN by providing accurate boundary detection. Overall, the method gave a 95.5\% map classification accuracy.},
  archive      = {J_TIP},
  author       = {Venkatesh Jatla and Marios S. Pattichis and Charles Nick Arge},
  doi          = {10.1109/TIP.2019.2944057},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1641-1653},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image processing methods for coronal hole segmentation, matching, and map classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Binocular light-field: Imaging theory and occlusion-robust
depth perception application. <em>TIP</em>, <em>29</em>, 1628–1640. (<a
href="https://doi.org/10.1109/TIP.2019.2943019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binocular stereo vision (SV) has been widely used to reconstruct the depth information, but it is quite vulnerable to scenes with strong occlusions. As an emerging computational photography technology, light-field (LF) imaging brings about a novel solution to passive depth perception by recording multiple angular views in a single exposure. In this paper, we explore binocular SV and LF imaging to form the binocular-LF imaging system. An imaging theory is derived by modeling the imaging process and analyzing disparity properties based on the geometrical optics theory. Then an accurate occlusion-robust depth estimation algorithm is proposed by exploiting multi-baseline stereo matching cues and defocus cues. The occlusions caused by binocular SV and LF imaging are detected and handled to eliminate the matching ambiguities and outliers. Finally, we develop a binocular-LF database and capture real-world scenes by our binocular-LF system to test the accuracy and robustness. The experimental results demonstrate that the proposed algorithm definitely recovers high quality depth maps with smooth surfaces and precise geometric shapes, which tackles the drawbacks of binocular SV and LF imaging simultaneously.},
  archive      = {J_TIP},
  author       = {Fei Liu and Shubo Zhou and Yunlong Wang and Guangqi Hou and Zhenan Sun and Tieniu Tan},
  doi          = {10.1109/TIP.2019.2943019},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1628-1640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Binocular light-field: Imaging theory and occlusion-robust depth perception application},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LEARNet: Dynamic imaging network for micro expression
recognition. <em>TIP</em>, <em>29</em>, 1618–1627. (<a
href="https://doi.org/10.1109/TIP.2019.2912358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike prevalent facial expressions, micro expressions have subtle, involuntary muscle movements which are short-lived in nature. These minute muscle movements reflect true emotions of a person. Due to the short duration and low intensity, these micro-expressions are very difficult to perceive and interpret correctly. In this paper, we propose the dynamic representation of micro-expressions to preserve facial movement information of a video in a single frame. We also propose a Lateral Accretive Hybrid Network (LEARNet) to capture micro-level features of an expression in the facial region. The LEARNet refines the salient expression features in accretive manner by incorporating accretion layers (AL) in the network. The response of the AL holds the hybrid feature maps generated by prior laterally connected convolution layers. Moreover, LEARNet architecture incorporates the cross decoupled relationship between convolution layers which helps in preserving the tiny but influential facial muscle change information. The visual responses of the proposed LEARNet depict the effectiveness of the system by preserving both high- and micro-level edge features of facial expression. The effectiveness of the proposed LEARNet is evaluated on four benchmark datasets: CASME-I, CASME-II, CAS(ME)^2 and SMIC. The experimental results after investigation show a significant improvement of 4.03\%, 1.90\%, 1.79\% and 2.82\% as compared with ResNet on CASME-I, CASME-II, CAS(ME)^2 and SMIC datasets respectively.},
  archive      = {J_TIP},
  author       = {Monu Verma and Santosh Kumar Vipparthi and Girdhari Singh and Subrahmanyam Murala},
  doi          = {10.1109/TIP.2019.2912358},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1618-1627},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LEARNet: Dynamic imaging network for micro expression recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised monocular depth estimation from light field
image. <em>TIP</em>, <em>29</em>, 1606–1617. (<a
href="https://doi.org/10.1109/TIP.2019.2944343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning based depth estimation from light field has made significant progresses in recent years. However, most existing approaches are under the supervised framework, which requires vast quantities of ground-truth depth data for training. Furthermore, accurate depth maps of light field are hardly available except for a few synthetic datasets. In this paper, we exploit the multi-orientation epipolar geometry of light field and propose an unsupervised monocular depth estimation network. It predicts depth from the central view of light field without any ground-truth information. Inspired by the inherent depth cues and geometry constraints of light field, we then introduce three novel unsupervised loss functions: photometric loss, defocus loss and symmetry loss. We have evaluated our method on a public 4D light field synthetic dataset. As the first unsupervised method published in the 4D Light Field Benchmark website, our method can achieve satisfactory performance in most error metrics. Comparison experiments with two state-of-the-art unsupervised methods demonstrate the superiority of our method. We also prove the effectiveness and generality of our method on real-world light-field images.},
  archive      = {J_TIP},
  author       = {Wenhui Zhou and Enci Zhou and Gaomin Liu and Lili Lin and Andrew Lumsdaine},
  doi          = {10.1109/TIP.2019.2944343},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1606-1617},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised monocular depth estimation from light field image},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate pedestrian detection by human pose regression.
<em>TIP</em>, <em>29</em>, 1591–1605. (<a
href="https://doi.org/10.1109/TIP.2019.2942686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection with high detection and localization accuracy is increasingly important for many practical applications. Due to the flexible structure of the human body, it is hard to train a template-based pedestrian detector that achieves a high detection rate and a good localization accuracy simultaneously. In this paper, we utilize human pose estimation to improve the detection and localization accuracy of pedestrian detection. We design two kinds of pose-indexed features that can considerably improve the discriminability of the detector. In addition to employing a two-stage pipeline to carry out these two tasks, we unify pose estimation and pedestrian detection into a cascaded decision forest in which they can cooperate sufficiently. To prevent irregular positive examples, such as truncated ones, from distracting the pedestrian detection and the pose regression, we clean the positive training data by realigning the bounding boxes and rejecting the wrong positive samples. Experimental results on the Caltech test dataset demonstrate the effectiveness of our proposed method. Our detector achieves 11.1\% MR-2, outperforming all existing detectors without using the convolutional neural network (CNN). Moreover, our method can be assembled with other detectors based on CNNs to improve detection and localization performance. By collaborating with the recent CNN-based method, our detector achieves 5.5\% MR-2 on the Caltech test dataset, outperforming the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yun Zhao and Zejian Yuan and Badong Chen},
  doi          = {10.1109/TIP.2019.2942686},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1591-1605},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Accurate pedestrian detection by human pose regression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical recurrent deep fusion using adaptive clip
summarization for sign language translation. <em>TIP</em>, <em>29</em>,
1575–1590. (<a href="https://doi.org/10.1109/TIP.2019.2941267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based sign language translation (SLT) is a challenging task due to the complicated variations of facial expressions, gestures, and articulated poses involved in sign linguistics. As a weakly supervised sequence-to-sequence learning problem, in SLT there are usually no exact temporal boundaries of actions. To adequately explore temporal hints in videos, we propose a novel framework named Hierarchical deep Recurrent Fusion (HRF). Aiming at modeling discriminative action patterns, in HRF we design an adaptive temporal encoder to capture crucial RGB visemes and skeleton signees. Specifically, RGB visemes and skeleton signees are learned by the same scheme named Adaptive Clip Summarization (ACS), respectively. ACS consists of three key modules, i.e., variable-length clip mining, adaptive temporal pooling, and attention-aware weighting. Besides, based on unaligned action patterns (RGB visemes and skeleton signees), a query-adaptive decoding fusion is proposed to translate the target sentence. Extensive experiments demonstrate the effectiveness of the proposed HRF framework.},
  archive      = {J_TIP},
  author       = {Dan Guo and Wengang Zhou and Anyang Li and Houqiang Li and Meng Wang},
  doi          = {10.1109/TIP.2019.2941267},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1575-1590},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical recurrent deep fusion using adaptive clip summarization for sign language translation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised human detection via region proposal networks
aided by verification. <em>TIP</em>, <em>29</em>, 1562–1574. (<a
href="https://doi.org/10.1109/TIP.2019.2944306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore how to leverage readily available unlabeled data to improve semi-supervised human detection performance. For this purpose, we specifically modify the region proposal network (RPN) for learning on a partially labeled dataset. Based on commonly observed false positive types, a verification module is developed to assess foreground human objects in the candidate regions to provide an important cue for filtering the RPN&#39;s proposals. The remaining proposals with high confidence scores are then used as pseudo annotations for re-training our detection model. To reduce the risk of error propagation in the training process, we adopt a self-paced training strategy to progressively include more pseudo annotations generated by the previous model over multiple training rounds. The resulting detector re-trained on the augmented data can be expected to have better detection performance. The effectiveness of the main components of this framework is verified through extensive experiments, and the proposed approach achieves state-of-the-art detection results on multiple scene-specific human detection benchmarks in the semi-supervised setting.},
  archive      = {J_TIP},
  author       = {Si Wu and Wenhao Wu and Shiyao Lei and Sihao Lin and Rui Li and Zhiwen Yu and Hau-San Wong},
  doi          = {10.1109/TIP.2019.2944306},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1562-1574},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised human detection via region proposal networks aided by verification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified probabilistic formulation of image aesthetic
assessment. <em>TIP</em>, <em>29</em>, 1548–1561. (<a
href="https://doi.org/10.1109/TIP.2019.2941778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image aesthetic assessment (IAA) has been attracting considerable attention in recent years due to the explosive growth of digital photography in Internet and social networks. The IAA problem is inherently challenging, owning to the ineffable nature of the human sense of aesthetics and beauty, and its close relationship to understanding pictorial content. Three different approaches to framing and solving the problem have been posed: binary classification, average score regression and score distribution prediction. Solutions that have been proposed have utilized different types of aesthetic labels and loss functions to train deep IAA models. However, these studies ignore the fact that the three different IAA tasks are inherently related. Here, we reveal that the use of the different types of aesthetic labels can be developed within the same statistical framework, which we use to create a unified probabilistic formulation of all the three IAA tasks. This unified formulation motivates the use of an efficient and effective loss function for training deep IAA models to conduct different tasks. We also discuss the problem of learning from a noisy raw score distribution which hinders network performance. We then show that by fitting the raw score distribution to a more stable and discriminative score distribution, we are able to train a single model which is able to obtain highly competitive performance on all three IAA tasks. Extensive qualitative analysis and experimental results on image aesthetic benchmarks validate the superior performance afforded by the proposed formulation. The source code is available at https://github.com/HuiZeng/Unified_IAA.},
  archive      = {J_TIP},
  author       = {Hui Zeng and Zisheng Cao and Lei Zhang and Alan C. Bovik},
  doi          = {10.1109/TIP.2019.2941778},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1548-1561},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A unified probabilistic formulation of image aesthetic assessment},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local regression ranking for saliency detection.
<em>TIP</em>, <em>29</em>, 1536–1547. (<a
href="https://doi.org/10.1109/TIP.2019.2942796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection is an important and challenging research topic due to the variety and complex of the background and saliency regions. In this paper, we present a novel unsupervised saliency detection approach by exploiting a learning-based ranking framework. First, the local linear regression model is adopted to simulate the local manifold structure of every image element, which is approximately linear. Using the background queries from the boundary prior, we construct a unified objective function to globally minimize all the errors of the local models for the whole image element points. The Laplacian matrix is learned via optimizing the unified objective function. Low-level image features as well as high-level semantic information extracted from deep neural networks are used for the Laplacian matrix learning. Based on the learnt Laplacian matrix, the saliency of the image element is measured as the relevance ranking to the background queries. The foreground queries are obtained from the background-based saliency and the relevance ranking to the foreground queries is calculated in the same way as the background-based saliency. Second, we calculate an enhanced similarity matrix by fusing two different-level deep feature metrics through cross diffusion. A propagation algorithm uses this enhanced similarity matrix to better exploit the intrinsic relevance of similar regions and improve the saliency ranking results effectively. Results on four benchmark datasets with pixel-wise accurate labelling demonstrate that the proposed unsupervised method shows better performance compared with the newest state-of-the-art methods and is competitive with deep learning-based methods.},
  archive      = {J_TIP},
  author       = {Ying-Ying Zhang and Shuo Zhang and Ping Zhang and Hai-Zhen Song and Xin-Gang Zhang},
  doi          = {10.1109/TIP.2019.2942796},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1536-1547},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local regression ranking for saliency detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning for weak human activity
localization. <em>TIP</em>, <em>29</em>, 1522–1535. (<a
href="https://doi.org/10.1109/TIP.2019.2942814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity localization aims at recognizing contents and detecting locations of activities in video sequences. With an increasing number of untrimmed video data, traditional activity localization methods always suffer from two major limitations. First, detailed annotations are needed in most existing methods, i.e., bounding-box annotations in every frame, which are both expensive and time consuming. Second, the search space is too large for 3D activity localization, which requires generating a large number of proposals. In this paper, we propose a unified deep Q-network with weak reward and weak loss (DWRLQN) to address the two problems. Certain weak knowledge and weak constraints involving the temporal dynamics of human activity are incorporated into a deep reinforcement learning framework under sparse spatial supervision, where we assume that only a portion of frames are annotated in each video sequence. Experiments on UCF-Sports, UCF-101 and sub-JHMDB demonstrate that our proposed model achieves promising performance by only utilizing a very small number of proposals. More importantly, our DWRLQN trained with partial annotations and weak information even outperforms fully supervised methods.},
  archive      = {J_TIP},
  author       = {Wanru Xu and Zhenjiang Miao and Jian Yu and Qiang Ji},
  doi          = {10.1109/TIP.2019.2942814},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1522-1535},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep reinforcement learning for weak human activity localization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cascaded face sketch synthesis under various illuminations.
<em>TIP</em>, <em>29</em>, 1507–1521. (<a
href="https://doi.org/10.1109/TIP.2019.2942514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face sketch synthesis from a photo is of significant importance in digital entertainment. An intelligent face sketch synthesis system requires a strong robustness to lighting variations. Under uncontrolled lighting conditions in real-world settings, such a system will perform consistently well and have little restriction on the lighting conditions. However, previous face sketch synthesis methods tend to synthesize sketches under well-controlled lighting conditions. These methods are sensitive to lighting variations and produce unsatisfactory results when the lighting condition varies. In this paper, we propose a novel cascaded face sketch synthesis framework composed of a multiple feature generator and a cascaded low-rank representation. The multiple feature generator not only produces a generated sketch feature consistent with an artist&#39;s drawing style but also extracts a photo feature that is robust to various illuminations. Both features ensure that given a photo patch, the optimal sketch candidates can be selected from the database. The cascaded low-rank representation enables a gradual reduction in the gap between the synthesized face sketch and the corresponding artist-drawn sketch. Experimental results illustrate that the proposed cascaded framework generates realistic sketches on par with the current methods on the Chinese University of Hong Kong face sketch database under well-controlled illuminations. Moreover, this framework exhibits greatly improved performance compared to these methods on the extended Chinese University of Hong Kong face sketch database and Chinese celebrity face photos from the web under different illuminations. We argue that this framework paves a novel way for the implementation of computer-aided optical systems that are of essential importance in both face sketch synthesis and optical imaging.},
  archive      = {J_TIP},
  author       = {Mingjin Zhang and Yunsong Li and Nannan Wang and Yuan Chi and Xinbo Gao},
  doi          = {10.1109/TIP.2019.2942514},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1507-1521},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cascaded face sketch synthesis under various illuminations},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A biological vision inspired framework for image enhancement
in poor visibility conditions. <em>TIP</em>, <em>29</em>, 1493–1506. (<a
href="https://doi.org/10.1109/TIP.2019.2938310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image enhancement is an important pre-processing step for many computer vision applications especially regarding the scenes in poor visibility conditions. In this work, we develop a unified two-pathway model inspired by the biological vision, especially the early visual mechanisms, which contributes to image enhancement tasks including low dynamic range (LDR) image enhancement and high dynamic range (HDR) image tone mapping. Firstly, the input image is separated and sent into two visual pathways: structure-pathway and detail-pathway, corresponding to the M- and P-pathway in the early visual system, which code the low- and high-frequency visual information, respectively. In the structure-pathway, an extended biological normalization model is used to integrate the global and local luminance adaptation, which can handle the visual scenes with varying illuminations. On the other hand, the detail enhancement and local noise suppression are achieved in the detail-pathway based on local energy weighting. Finally, the outputs of structure-and detail-pathway are integrated to achieve the low-light image enhancement. In addition, the proposed model can also be used for tone mapping of HDR images with some fine-tuning steps. Extensive experiments on three datasets (two LDR image datasets and one HDR scene dataset) show that the proposed model can handle the visual enhancement tasks mentioned above efficiently and outperform the related state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Kai-Fu Yang and Xian-Shi Zhang and Yong-Jie Li},
  doi          = {10.1109/TIP.2019.2938310},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1493-1506},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A biological vision inspired framework for image enhancement in poor visibility conditions},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DP-siam: Dynamic policy siamese network for robust object
tracking. <em>TIP</em>, <em>29</em>, 1479–1492. (<a
href="https://doi.org/10.1109/TIP.2019.2942506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balancing the trade-off between real-time performance and accuracy in object tracking is a major challenge. In this paper, a novel dynamic policy gradient Agent-Environment architecture with Siamese network (DP-Siam) is proposed to train the tracker to increase the accuracy and the expected average overlap while performing in real-time. DP-Siam is trained offline with reinforcement learning to produce a continuous action that predicts the optimal object location. DP-Siam has a novel architecture that consists of three networks: an Agent network to predict the optimal state (bounding box) of the object being tracked, an Environment network to get the Q-value during the offline training phase to minimize the error of the loss function, and a Siamese network to produce a heat-map. During online tracking, the Environment network acts as a verifier to the Agent network action. Extensive experiments are performed on six widely used benchmarks: OTB2013, OTB50, OTB100, VOT2015, VOT2016 and VOT2018. The results show that DP-Siam significantly outperforms the current state-of-the-art trackers.},
  archive      = {J_TIP},
  author       = {Mohamed H. Abdelpakey and Mohamed S. Shehata},
  doi          = {10.1109/TIP.2019.2942506},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1479-1492},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DP-siam: Dynamic policy siamese network for robust object tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collision-free video synopsis incorporating object speed and
size changes. <em>TIP</em>, <em>29</em>, 1465–1478. (<a
href="https://doi.org/10.1109/TIP.2019.2942543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new surveillance video synopsis method which performs much better than previous approaches in terms of both compression ratio and artifact. Previously, a surveillance video was usually compressed by shifting the moving objects of that video forward along the time axis, which inevitably yielded serious collision and chronological disorder artifacts between the shifted objects. The main observation of this paper is that these artifacts can be alleviated by changing the speed or size of the objects, since with varied speed and size the objects can move more flexibly to avoid collision points or to keep chronological relationships. Based on this observation, we propose a video synopsis method that performs object shifting, speed changing, and size scaling simultaneously. We show how to integrate the three heterogeneous operations into a single optimization framework and achieve high-quality synopsis results. Unlike previous approaches that usually use alternative optimization strategies to solve synopsis optimizations, we develop a Metropolis sampling algorithm to find the solution for our three-variable optimization problem. A variety of experiments demonstrate the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Yongwei Nie and Zhenkai Li and Zhensong Zhang and Qing Zhang and Tiezheng Ma and Hanqiu Sun},
  doi          = {10.1109/TIP.2019.2942543},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1465-1478},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Collision-free video synopsis incorporating object speed and size changes},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LCSCNet: Linear compressing-based skip-connecting network
for image super-resolution. <em>TIP</em>, <em>29</em>, 1450–1464. (<a
href="https://doi.org/10.1109/TIP.2019.2940679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a concise but efficient network architecture called linear compressing based skip-connecting network (LCSCNet) for image super-resolution. Compared with two representative network architectures with skip connections, ResNet and DenseNet, a linear compressing layer is designed in LCSCNet for skip connection, which connects former feature maps and distinguishes them from newly-explored feature maps. In this way, the proposed LCSCNet enjoys the merits of the distinguish feature treatment of DenseNet and the parameter-economic form of ResNet. Moreover, to better exploit hierarchical information from both low and high levels of various receptive fields in deep models, inspired by gate units in LSTM, we also propose an adaptive element-wise fusion strategy with multi-supervised training. Experimental results in comparison with state-of-the-art algorithms validate the effectiveness of LCSCNet.},
  archive      = {J_TIP},
  author       = {Wenming Yang and Xuechen Zhang and Yapeng Tian and Wei Wang and Jing-Hao Xue and Qingmin Liao},
  doi          = {10.1109/TIP.2019.2940679},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1450-1464},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LCSCNet: Linear compressing-based skip-connecting network for image super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imaging with SPADs and DMDs: Seeing through
diffraction-photons. <em>TIP</em>, <em>29</em>, 1440–1449. (<a
href="https://doi.org/10.1109/TIP.2019.2941315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of imaging in the presence of diffraction-photons. Diffraction-photons arise from the low contrast ratio of DMDs (~1000:1), and very much degrade the quality of images captured by SPAD-based systems. Herein, a joint illumination-deconvolution scheme is designed to overcome diffraction-photons, enabling the acquisition of intensity and depth images. Additionally, a proof-of-concept experiment is conducted to demonstrate the viability of the designed scheme. It is shown that by co-designing the illumination and deconvolution phases of imaging, one can substantially overcome diffraction-photons.},
  archive      = {J_TIP},
  author       = {Ibrahim Alsolami and Wolfgang Heidrich},
  doi          = {10.1109/TIP.2019.2941315},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1440-1449},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Imaging with SPADs and DMDs: Seeing through diffraction-photons},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank quaternion approximation for color image
processing. <em>TIP</em>, <em>29</em>, 1426–1439. (<a
href="https://doi.org/10.1109/TIP.2019.2941319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank matrix approximation (LRMA)-based methods have made a great success for grayscale image processing. When handling color images, LRMA either restores each color channel independently using the monochromatic model or processes the concatenation of three color channels using the concatenation model. However, these two schemes may not make full use of the high correlation among RGB channels. To address this issue, we propose a novel low-rank quaternion approximation (LRQA) model. It contains two major components: first, instead of modeling a color image pixel as a scalar in conventional sparse representation and LRMA-based methods, the color image is encoded as a pure quaternion matrix, such that the cross-channel correlation of color channels can be well exploited; second, LRQA imposes the low-rank constraint on the constructed quaternion matrix. To better estimate the singular values of the underlying low-rank quaternion matrix from its noisy observation, a general model for LRQA is proposed based on several nonconvex functions. Extensive evaluations for color image denoising and inpainting tasks verify that LRQA achieves better performance over several state-of-the-art sparse representation and LRMA-based methods in terms of both quantitative metrics and visual quality.},
  archive      = {J_TIP},
  author       = {Yongyong Chen and Xiaolin Xiao and Yicong Zhou},
  doi          = {10.1109/TIP.2019.2941319},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1426-1439},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low-rank quaternion approximation for color image processing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient single-stage pedestrian detector by asymptotic
localization fitting and multi-scale context encoding. <em>TIP</em>,
<em>29</em>, 1413–1425. (<a
href="https://doi.org/10.1109/TIP.2019.2938877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though Faster R-CNN based two-stage detectors have witnessed significant boost in pedestrian detection accuracy, they are still slow for practical applications. One solution is to simplify this working flow as a single-stage detector. However, current single-stage detectors (e.g. SSD) have not presented competitive accuracy on common pedestrian detection benchmarks. Accordingly, a structurally simple but effective module called Asymptotic Localization Fitting (ALF) is proposed, which stacks a series of predictors to directly evolve the default anchor boxes of SSD step by step to improve detection results. Additionally, combining the advantages from residual learning and multi-scale context encoding, a bottleneck block is proposed to enhance the predictors&#39; discriminative power. On top of the above designs, an efficient single-stage detection architecture is designed, resulting in an attractive pedestrian detector in both accuracy and speed. A comprehensive set of experiments on two of the largest pedestrian detection datasets (i.e. CityPersons and Caltech) demonstrate the superiority of the proposed method, comparing to the state of the arts on both the benchmarks.},
  archive      = {J_TIP},
  author       = {Wei Liu and Shengcai Liao and Weidong Hu},
  doi          = {10.1109/TIP.2019.2938877},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1413-1425},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient single-stage pedestrian detector by asymptotic localization fitting and multi-scale context encoding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anisotropic guided filtering. <em>TIP</em>, <em>29</em>,
1397–1412. (<a href="https://doi.org/10.1109/TIP.2019.2941326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The guided filter and its subsequent derivatives have been widely employed in many image processing and computer vision applications primarily brought about by their low complexity and good edge-preservation properties. Despite this success, the different variants of the guided filter are unable to handle more aggressive filtering strengths leading to the manifestation of “detail halos”. At the same time, these existing filters perform poorly when the input and guide images have structural inconsistencies. In this paper, we demonstrate that these limitations are due to the guided filter operating as a variable-strength locally-isotropic filter that, in effect, acts as a weak anisotropic filter on the image. Our analysis shows that this behaviour stems from the use of unweighted averaging in the final steps of guided filter variants including the adaptive guided filter (AGF), weighted guided image filter (WGIF), and gradient-domain guided image filter (GGIF). We propose a novel filter, the Anisotropic Guided Filter (AnisGF), that utilises weighted averaging to achieve maximum diffusion while preserving strong edges in the image. The proposed weights are optimised based on the local neighbourhood variances to achieve strong anisotropic filtering while preserving the low computational cost of the original guided filter. Synthetic tests show that the proposed method addresses the presence of detail halos and the handling of inconsistent structures found in previous variants of the guided filter. Furthermore, experiments in scale-aware filtering, detail enhancement, texture removal, and chroma upsampling demonstrate the improvements brought about by the technique.},
  archive      = {J_TIP},
  author       = {Carlo Noel Ochotorena and Yukihiko Yamashita},
  doi          = {10.1109/TIP.2019.2941326},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1397-1412},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Anisotropic guided filtering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). 50 FPS object-level saliency detection via maximally stable
region. <em>TIP</em>, <em>29</em>, 1384–1396. (<a
href="https://doi.org/10.1109/TIP.2019.2941663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human visual system tends to consider saliency of an object as a whole. Some object-level saliency detection methods have been proposed by leveraging object proposals in bounding boxes, and regarding the entire bounding box as one candidate salient region. However, the bounding boxes can not provide exact object position and a lot of pixels in bounding boxes belong to the background. Consequently, background pixels in bounding box also show high saliency. Besides, acquiring object proposals needs high time cost. In order to compute object-level saliency, we consider region growing from some seed superpixels, to find one surrounding region which probably represents the whole object. The desired surrounding region has similar appearance inside and obvious difference with the outside, which is proposed as maximally stable region (MSR) in this paper. In addition, one effective seed superpixel selection strategy is presented to improve speed. MSR based saliency detection is more robust than pixel or superpixel level methods and object proposal based methods. The proposed method significantly outperforms the state-of-the-art unsupervised methods at 50 FPS. Compared with deep learning based methods, we show worse performance, but with about 1200-1600 times faster, which means better trade-off between performance and speed.},
  archive      = {J_TIP},
  author       = {Xiaoming Huang and Yin Zheng and Junzhou Huang and Yu-Jin Zhang},
  doi          = {10.1109/TIP.2019.2941663},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1384-1396},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {50 FPS object-level saliency detection via maximally stable region},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep MR brain image super-resolution using spatio-structural
priors. <em>TIP</em>, <em>29</em>, 1368–1383. (<a
href="https://doi.org/10.1109/TIP.2019.2942510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High resolution Magnetic Resonance (MR) images are desired for accurate diagnostics. In practice, image resolution is restricted by factors like hardware and processing constraints. Recently, deep learning methods have been shown to produce compelling state-of-the-art results for image enhancement/super-resolution. Paying particular attention to desired hi-resolution MR image structure, we propose a new regularized network that exploits image priors, namely a low-rank structure and a sharpness prior to enhance deep MR image super-resolution (SR). Our contributions are then incorporating these priors in an analytically tractable fashion as well as towards a novel prior guided network architecture that accomplishes the super-resolution task. This is particularly challenging for the low rank prior since the rank is not a differentiable function of the image matrix (and hence the network parameters), an issue we address by pursuing differentiable approximations of the rank. Sharpness is emphasized by the variance of the Laplacian which we show can be implemented by a fixed feedback layer at the output of the network. As a key extension, we modify the fixed feedback (Laplacian) layer by learning a new set of training data driven filters that are optimized for enhanced sharpness. Experiments performed on publicly available MR brain image databases and comparisons against existing state-of-the-art methods show that the proposed prior guided network offers significant practical gains in terms of improved SNR/image quality measures. Because our priors are on output images, the proposed method is versatile and can be combined with a wide variety of existing network architectures to further enhance their performance.},
  archive      = {J_TIP},
  author       = {Venkateswararao Cherukuri and Tiantong Guo and Steven J. Schiff and Vishal Monga},
  doi          = {10.1109/TIP.2019.2942510},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1368-1383},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep MR brain image super-resolution using spatio-structural priors},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards weakly-supervised focus region detection via
recurrent constraint network. <em>TIP</em>, <em>29</em>, 1356–1367. (<a
href="https://doi.org/10.1109/TIP.2019.2942505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent state-of-the-art methods on focus region detection (FRD) rely on deep convolutional networks trained with costly pixel-level annotations. In this study, we propose a FRD method that achieves competitive accuracies but only uses easily obtained bounding box annotations. Box-level tags provide important cues of focus regions but lose the boundary delineation of the transition area. A recurrent constraint network (RCN) is introduced for this challenge. In our static training, RCN is jointly trained with a fully convolutional network (FCN) through box-level supervision. The RCN can generate a detailed focus map to locate the boundary of the transition area effectively. In our dynamic training, we iterate between fine-tuning FCN and RCN with the generated pixel-level tags and generate finer new pixel-level tags. To boost the performance further, a guided conditional random field is developed to improve the quality of the generated pixel-level tags. To promote further study of the weakly supervised FRD methods, we construct a new dataset called FocusBox, which consists of 5000 challenging images with bounding box-level labels. Experimental results on existing datasets demonstrate that our method not only yields comparable results than fully supervised counterparts but also achieves a faster speed.},
  archive      = {J_TIP},
  author       = {Wenda Zhao and Xueqing Hou and Xiaobing Yu and You He and Huchuan Lu},
  doi          = {10.1109/TIP.2019.2942505},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1356-1367},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards weakly-supervised focus region detection via recurrent constraint network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collective affinity learning for partial cross-modal
hashing. <em>TIP</em>, <em>29</em>, 1344–1355. (<a
href="https://doi.org/10.1109/TIP.2019.2941858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, various unsupervised hashing methods have been developed for cross-modal retrieval. However, in real-world applications, it is often the incomplete case that every modality of data may suffer from some missing samples. Most existing works assume that every object appears in both modalities, hence they may not work well for partial multi-modal data. To address this problem, we propose a novel Collective Affinity Learning Method (CALM), which collectively and adaptively learns an anchor graph for generating binary codes on partial multi-modal data. In CALM, we first construct modality-specific bipartite graphs collectively, and derive a probabilistic model to figure out complete data-to-anchor affinities for each modality. Theoretical analysis reveals its ability to recover missing adjacency information. Moreover, a robust model is proposed to fuse these modality-specific affinities by adaptively learning a unified anchor graph. Then, the neighborhood information from the learned anchor graph acts as feedback, which guides the previous affinity reconstruction procedure. To solve the formulated optimization problem, we further develop an effective algorithm with linear time complexity and fast convergence. Last, Anchor Graph Hashing (AGH) is conducted on the fused affinities for cross-modal retrieval. Experimental results on benchmark datasets show that our proposed CALM consistently outperforms the existing methods.},
  archive      = {J_TIP},
  author       = {Jun Guo and Wenwu Zhu},
  doi          = {10.1109/TIP.2019.2941858},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1344-1355},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Collective affinity learning for partial cross-modal hashing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain-transformable sparse representation for anomaly
detection in moving-camera videos. <em>TIP</em>, <em>29</em>, 1329–1343.
(<a href="https://doi.org/10.1109/TIP.2019.2940686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a special matrix factorization based on sparse representation that detects anomalies in video sequences generated with moving cameras. Such representation is made by associating the frames of the target video, that is a sequence to be tested for the presence of anomalies, with the frames of an anomaly-free reference video, which is a previously validated sequence. This factorization is done by a sparse coefficient matrix, and any target-video anomaly is encapsulated into a residue term. In order to cope with camera trepidations, domain-transformations are incorporated into the sparse representation process. Approximations of the transformed-domain optimization problem are introduced to turn it into a feasible iterative process. Results obtained from a comprehensive video database acquired with moving cameras on a visually cluttered environment indicate that the proposed algorithm provides a better geometric registration between reference and target videos, greatly improving the overall performance of the anomaly-detection system.},
  archive      = {J_TIP},
  author       = {Eric Jardim and Lucas A. Thomaz and Eduardo A. B. da Silva and Sergio L. Netto},
  doi          = {10.1109/TIP.2019.2940686},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1329-1343},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Domain-transformable sparse representation for anomaly detection in moving-camera videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tunable VVC frame partitioning based on lightweight machine
learning. <em>TIP</em>, <em>29</em>, 1313–1328. (<a
href="https://doi.org/10.1109/TIP.2019.2938670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Block partition structure is a critical module in video coding scheme to achieve significant gap of compression performance. Under the exploration of the future video coding standard, named Versatile Video Coding (VVC), a new Quad Tree Binary Tree (QTBT) block partition structure has been introduced. In addition to the QT block partitioning defined in High Efficiency Video Coding (HEVC) standard, new horizontal and vertical BT partitions are enabled, which drastically increases the encoding time compared to HEVC. In this paper, we propose a lightweight and tunable QTBT partitioning scheme based on a Machine Learning (ML) approach. The proposed solution uses Random Forest classifiers to determine for each coding block the most probable partition modes. To minimize the encoding loss induced by misclassification, risk intervals for classifier decisions are introduced in the proposed solution. By varying the size of risk intervals, tunable trade-off between encoding complexity reduction and coding loss is achieved. The proposed solution implemented in the JEM-7.0 software offers encoding complexity reductions ranging from 30\% to 70\% in average for only 0.7\% to 3.0\% Bjøntegaard Delta Rate (BD-BR) increase in Random Access (RA) coding configuration, with very slight overhead induced by Random Forest. The proposed solution based on Random Forest classifiers is also efficient to reduce the complexity of the Multi-Type Tree (MTT) partitioning scheme under the VTM-5.0 software, with complexity reductions ranging from 25\% to 61\% in average for only 0.4\% to 2.2\% BD-BR increase.},
  archive      = {J_TIP},
  author       = {Thomas Amestoy and Alexandre Mercat and Wassim Hamidouche and Daniel Menard and Cyril Bergeron},
  doi          = {10.1109/TIP.2019.2938670},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1313-1328},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Tunable VVC frame partitioning based on lightweight machine learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep heterogeneous hashing for face video retrieval.
<em>TIP</em>, <em>29</em>, 1299–1312. (<a
href="https://doi.org/10.1109/TIP.2019.2940683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieving videos of a particular person with face image as query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space, characterizing face videos with some robust set modeling techniques (e.g. covariance matrices as exploited in this study, which reside on Riemannian manifold), has recently shown appealing advantages. This hence results in a thorny heterogeneous spaces matching problem. Moreover, hashing with handcrafted features as done in many existing works is clearly inadequate to achieve desirable performance for this task. To address such problems, we present an end-to-end Deep Heterogeneous Hashing (DHH) method that integrates three stages including image feature learning, video modeling, and heterogeneous hashing in a single framework, to learn unified binary codes for both face images and videos. To tackle the key challenge of hashing on manifold, a well-studied Riemannian kernel mapping is employed to project data (i.e. covariance matrices) into Euclidean space and thus enables to embed the two heterogeneous representations into a common Hamming space, where both intra-space discriminability and inter-space compatibility are considered. To perform network optimization, the gradient of the kernel mapping is innovatively derived via structured matrix backpropagation in a theoretically principled way. Experiments on three challenging datasets show that our method achieves quite competitive performance compared with existing hashing methods.},
  archive      = {J_TIP},
  author       = {Shishi Qiao and Ruiping Wang and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TIP.2019.2940683},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1299-1312},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep heterogeneous hashing for face video retrieval},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep tone mapping operator for high dynamic range images.
<em>TIP</em>, <em>29</em>, 1285–1298. (<a
href="https://doi.org/10.1109/TIP.2019.2936649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computationally fast tone mapping operator (TMO) that can quickly adapt to a wide spectrum of high dynamic range (HDR) content is quintessential for visualization on varied low dynamic range (LDR) output devices such as movie screens or standard displays. Existing TMOs can successfully tone-map only a limited number of HDR content and require an extensive parameter tuning to yield the best subjective-quality tone-mapped output. In this paper, we address this problem by proposing a fast, parameter-free and scene-adaptable deep tone mapping operator (DeepTMO) that yields a high-resolution and high-subjective quality tone mapped output. Based on conditional generative adversarial network (cGAN), DeepTMO not only learns to adapt to vast scenic-content (e.g., outdoor, indoor, human, structures, etc.) but also tackles the HDR related scene-specific challenges such as contrast and brightness, while preserving the fine-grained details. We explore 4 possible combinations of Generator-Discriminator architectural designs to specifically address some prominent issues in HDR related deep-learning frameworks like blurring, tiling patterns and saturation artifacts. By exploring different influences of scales, loss-functions and normalization layers under a cGAN setting, we conclude with adopting a multi-scale model for our task. To further leverage on the large-scale availability of unlabeled HDR data, we train our network by generating targets using an objective HDR quality metric, namely Tone Mapping Image Quality Index (TMQI). We demonstrate results both quantitatively and qualitatively, and showcase that our DeepTMO generates high-resolution, high-quality output images over a large spectrum of real-world scenes. Finally, we evaluate the perceived quality of our results by conducting a pair-wise subjective study which confirms the versatility of our method.},
  archive      = {J_TIP},
  author       = {Aakanksha Rana and Praveer Singh and Giuseppe Valenzise and Frederic Dufaux and Nikos Komodakis and Aljosa Smolic},
  doi          = {10.1109/TIP.2019.2936649},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1285-1298},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep tone mapping operator for high dynamic range images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable deep hashing for large-scale social image
retrieval. <em>TIP</em>, <em>29</em>, 1271–1284. (<a
href="https://doi.org/10.1109/TIP.2019.2940693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the wide application of hashing for large-scale image retrieval, because of its high computation efficiency and low storage cost. Particularly, benefiting from current advances in deep learning, supervised deep hashing methods have greatly boosted the retrieval performance, under the strong supervision of large amounts of manually annotated semantic labels. However, their performance is highly dependent upon the supervised labels, which significantly limits the scalability. In contrast, unsupervised deep hashing without label dependence enjoys the advantages of well scalability. Nevertheless, due to the relaxed hash optimization, and more importantly, the lack of semantic guidance, existing methods suffer from limited retrieval performance. In this paper, we propose a SCAlable Deep Hashing (SCADH) to learn enhanced hash codes for social image retrieval. We formulate a unified scalable deep hash learning framework which explores the weak but free supervision of discriminative user tags that are commonly accompanied with social images. It jointly learns image representations and hash functions with deep neural networks, and simultaneously enhances the discriminative capability of image hash codes with the refined semantics from the accompanied social tags. Further, instead of simple relaxed hash optimization, we propose a discrete hash optimization method based on Augmented Lagrangian Multiplier to directly solve the hash codes and avoid the binary quantization information loss. Experiments on two standard social image datasets demonstrate the superiority of the proposed approach compared with state-of-the-art shallow and deep hashing techniques.},
  archive      = {J_TIP},
  author       = {Hui Cui and Lei Zhu and Jingjing Li and Yang Yang and Liqiang Nie},
  doi          = {10.1109/TIP.2019.2940693},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1271-1284},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scalable deep hashing for large-scale social image retrieval},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path-based dictionary augmentation: A framework for
improving <span class="math inline"><em>k</em></span> -sparse image
processing. <em>TIP</em>, <em>29</em>, 1259–1270. (<a
href="https://doi.org/10.1109/TIP.2019.2927331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have previously shown that augmenting orthogonal matching pursuit (OMP) with an additional step in the identification stage of each pursuit iteration yields improved k-sparse reconstruction and denoising performance relative to baseline OMP. At each iteration a “path” or geodesic, is generated between the two dictionary atoms that are most correlated with the residual and from this path a new atom that has a greater correlation to the residual than either of the two bracketing atoms is selected. Here, we provide new computational results illustrating improvements in sparse coding and denoising on canonical datasets using both learned and structured dictionaries. The two methods of constructing a path are investigated for each dictionary type: the Euclidean geodesic formed by a linear combination of the two atoms and the 2-Wasserstein geodesic corresponding to the optimal transport map between the atoms. We prove here the existence of a higher-correlation atom in the Euclidean case under assumptions on the two bracketing atoms and introduce algorithmic modifications to improve the likelihood that the bracketing atoms meet those conditions. Although, we demonstrate our augmentation on OMP alone, in general it may be applied to any reconstruction algorithm that relies on the selection and sorting of high-similarity atoms during an analysis or identification phase.},
  archive      = {J_TIP},
  author       = {Tegan H. Emerson and Colin C. Olson and Timothy Doster},
  doi          = {10.1109/TIP.2019.2927331},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1259-1270},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Path-based dictionary augmentation: A framework for improving $k$ -sparse image processing},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational-based mixed noise removal with CNN deep learning
regularization. <em>TIP</em>, <em>29</em>, 1246–1258. (<a
href="https://doi.org/10.1109/TIP.2019.2940496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the traditional model based variational methods and deep learning based algorithms are naturally integrated to address mixed noise removal, specially for Gaussian mixture noise and Gaussian-impulse noise removal problem. To be different from single type noise (e.g. Gaussian) removal, it is a challenge problem to accurately discriminate noise types and levels for each pixel. We propose a variational method to iteratively estimate the noise parameters, and then the algorithm can automatically classify the noise according to the different statistical parameters. The proposed variational problem can be separated into regularization, synthesis, parameters estimation and noise classification four steps with the operator splitting scheme. Each step is related to an optimization subproblem. To enforce the regularization, the deep learning method is employed to learn the natural images prior. Compared with some model based regularizations, the CNN regularizer can significantly improve the quality of the restored images. Compared with some learning based methods, the synthesis step can produce better reconstructions by analyzing the types and levels of the recognized noise. In our method, the convolution neutral network (CNN) can be regarded as an operator which associated to a variational functional. From this viewpoint, the proposed method can be extended to many image reconstruction and inverse problems. Numerical experiments in the paper show that our method can achieve some state-of-the-art results for Gaussian mixture noise and Gaussian-impulse noise removal.},
  archive      = {J_TIP},
  author       = {Faqiang Wang and Haiyang Huang and Jun Liu},
  doi          = {10.1109/TIP.2019.2940496},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1246-1258},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational-based mixed noise removal with CNN deep learning regularization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Few-shot deep adversarial learning for video-based person
re-identification. <em>TIP</em>, <em>29</em>, 1233–1245. (<a
href="https://doi.org/10.1109/TIP.2019.2940684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based person re-identification (re-ID) refers to matching people across camera views from arbitrary unaligned video footages. Existing methods rely on supervision signals to optimise a projected space under which the distances between inter/intra-videos are maximised/minimised. However, this demands exhaustively labelling people across camera views, rendering them unable to be scaled in large networked cameras. Also, it is noticed that learning effective video representations with view invariance is not explicitly addressed for which features exhibit different distributions otherwise. Thus, matching videos for person re-ID demands flexible models to capture the dynamics in time-series observations and learn view-invariant representations with access to limited labeled training samples. In this paper, we propose a novel few-shot deep learning approach to video-based person re-ID, to learn comparable representations that are discriminative and view-invariant. The proposed method is developed on the variational recurrent neural networks (VRNNs) and trained adversarially to produce latent variables with temporal dependencies that are highly discriminative yet view-invariant in matching persons. Through extensive experiments conducted on three benchmark datasets, we empirically show the capability of our method in creating view-invariant temporal features and state-of-the-art performance achieved by our method.},
  archive      = {J_TIP},
  author       = {Lin Wu and Yang Wang and Hongzhi Yin and Meng Wang and Ling Shao},
  doi          = {10.1109/TIP.2019.2940684},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1233-1245},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Few-shot deep adversarial learning for video-based person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noise-robust iterative back-projection. <em>TIP</em>,
<em>29</em>, 1219–1232. (<a
href="https://doi.org/10.1109/TIP.2019.2940414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy image super-resolution (SR) is a significant challenging process due to the smoothness caused by denoising. Iterative back-projection (IBP) can be helpful in further enhancing the reconstructed SR image, but there is no clean reference image available. This paper proposes a novel back-projection algorithm for noisy image SR. Its main goal is to pursuit the consistency between LR and SR images. We aim to estimate the clean reconstruction error to be back-projected, using the noisy and denoised reconstruction errors. We formulate a new cost function on the principal component analysis (PCA) transform domain to estimate the clean reconstruction error. In the data term of the cost function, noisy and denoised reconstruction errors are combined in a region-adaptive manner using texture probability. In addition, the sparsity constraint is incorporated into the regularization term, based on the Laplacian characteristics of the reconstruction error. Finally, we propose an eigenvector estimation method to minimize the effect of noise. The experimental results demonstrate that the proposed method can perform back-projection in a more noise-robust manner than the conventional IBP, and harmoniously work with any other SR methods as a post-processing.},
  archive      = {J_TIP},
  author       = {Jun-Sang Yoo and Jong-Ok Kim},
  doi          = {10.1109/TIP.2019.2940414},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1219-1232},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Noise-robust iterative back-projection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compositional attention networks with two-stream fusion for
video question answering. <em>TIP</em>, <em>29</em>, 1204–1218. (<a
href="https://doi.org/10.1109/TIP.2019.2940677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a video, Video Question Answering (VideoQA) aims at answering arbitrary free-form questions about the video content in natural language. A successful VideoQA framework usually has the following two key components: 1) a discriminative video encoder that learns the effective video representation to maintain as much information as possible about the video and 2) a question-guided decoder that learns to select the most related features to perform spatiotemporal reasoning, as well as outputs the correct answer. We propose compositional attention networks (CAN) with two-stream fusion for VideoQA tasks. For the encoder, we sample video snippets using a two-stream mechanism (i.e., a uniform sampling stream and an action pooling stream) and extract a sequence of visual features for each stream to represent the video semantics with implementation. For the decoder, we propose a compositional attention module to integrate the two-stream features with the attention mechanism. The compositional attention module is the core of CAN and can be seen as a modular combination of a unified attention block. With different fusion strategies, we devise five compositional attention module variants. We evaluate our approach on one long-term VideoQA dataset, ActivityNet-QA, and two short-term VideoQA datasets, MSRVTT-QA and MSVD-QA. Our CAN model achieves new state-of-the-art results on all the datasets.},
  archive      = {J_TIP},
  author       = {Ting Yu and Jun Yu and Zhou Yu and Dacheng Tao},
  doi          = {10.1109/TIP.2019.2940677},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1204-1218},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Compositional attention networks with two-stream fusion for video question answering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning nonclassical receptive field modulation for contour
detection. <em>TIP</em>, <em>29</em>, 1192–1203. (<a
href="https://doi.org/10.1109/TIP.2019.2940690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work develops a biologically inspired neural network for contour detection in natural images by combining the nonclassical receptive field modulation mechanism with a deep learning framework. The input image is first convolved with the local feature detectors to produce the classical receptive field responses, and then a corresponding modulatory kernel is constructed for each feature map to model the nonclassical receptive field modulation behaviors. The modulatory effects can activate a larger cortical area and thus allow cortical neurons to integrate a broader range of visual information to recognize complex cases. Additionally, to characterize spatial structures at various scales, a multiresolution technique is used to represent visual field information from fine to coarse. Different scale responses are combined to estimate the contour probability. Our method achieves state-of-the-art results among all biologically inspired contour detection models. This study provides a method for improving visual modeling of contour detection and inspires new ideas for integrating more brain cognitive mechanisms into deep neural networks.},
  archive      = {J_TIP},
  author       = {Qiling Tang and Nong Sang and Haihua Liu},
  doi          = {10.1109/TIP.2019.2940690},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1192-1203},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning nonclassical receptive field modulation for contour detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Edge-sensitive human cutout with hierarchical granularity
and loopy matting guidance. <em>TIP</em>, <em>29</em>, 1177–1191. (<a
href="https://doi.org/10.1109/TIP.2019.2930146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human parsing and matting play important roles in various applications, such as dress collocation, clothing recommendation, and image editing. In this paper, we propose a lightweight hybrid model that unifies the fully-supervised hierarchical-granularity parsing task and the unsupervised matting one. Our model comprises two parts, the extensible hierarchical semantic segmentation block using CNN and the matting module composed of guided filters. Given a human image, the segmentation block stage-1 first obtains a primitive segmentation map to separate the human and the background. The primitive segmentation is then fed into stage-2 together with the original image to give a rough segmentation of human body. This procedure is repeated in the stage-3 to acquire a refined segmentation. The matting module takes as input the above estimated segmentation maps and produces the matting map, in a fully unsupervised manner. The obtained matting map is then in turn fed back to the CNN in the first block for refining the semantic segmentation results.},
  archive      = {J_TIP},
  author       = {Jingwen Ye and Yongcheng Jing and Xinchao Wang and Kairi Ou and Dacheng Tao and Mingli Song},
  doi          = {10.1109/TIP.2019.2930146},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1177-1191},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Edge-sensitive human cutout with hierarchical granularity and loopy matting guidance},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Color control functions for multiprimary displays—II:
Variational robustness optimization. <em>TIP</em>, <em>29</em>,
1164–1176. (<a href="https://doi.org/10.1109/TIP.2019.2936992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a companion Part I paper, we presented a framework for analyzing robustness of color control functions (CCFs) for multiprimary displays against primary and observer variations and proposed a variational minimization for obtaining robust CCFs. The objective function proposed in the Part I paper combines two nonnegative terms that serve as useful figures of merit for quantitatively characterizing CCFs. The first term measures lack of smoothness of the CCFs and characterizes how well transitions in perceptual color space are preserved in the presence of the primary/observer variations. The second term measures deviation of the CCF, in the vicinity of the gray axis, from a specific axially linear CCF that provides perceptual invariance to the variations along the gray axis. In this paper, using calculus of variations, we develop an algorithm for numerically computing optimal CCFs under the proposed variational formulation. Using the proposed algorithm, we determine optimal CCFs for a several multiprimary display designs and assess and compare their performance against alternative approaches. The variationally optimal CCFs obtained using the proposed approach offer improvements over the alternatives, as assessed visually and via quantitative metrics measuring smoothness and invariance in the presence of primary variations. The relative improvements provided by the proposed CCF increase with increasing number of primaries.},
  archive      = {J_TIP},
  author       = {Carlos Eduardo Rodríguez-Pardo and Gaurav Sharma},
  doi          = {10.1109/TIP.2019.2936992},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1164-1176},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color control functions for multiprimary Displays—II: Variational robustness optimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Color control functions for multiprimary displays—i:
Robustness analysis and optimization formulations. <em>TIP</em>,
<em>29</em>, 1152–1163. (<a
href="https://doi.org/10.1109/TIP.2019.2937067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color management for a multiprimary display requires, as a fundamental step, the determination of a color control function (CCF) that specifies control values for reproducing each color in the display&#39;s gamut. Multiprimary displays offer alternative choices of control values for reproducing a color in the interior of the gamut and accordingly alternative choices of CCFs. Under ideal conditions, alternative CCFs render colors identically. However, deviations in the spectral distributions of the primaries and the diversity of cone sensitivities among observers impact alternative CCFs differently, and, in particular, make some CCFs prone to artifacts in rendered images. We develop a framework for analyzing robustness of CCFs for multiprimary displays against primary and observer variations, incorporating a common model of human color perception. Using the framework, we propose analytical and numerical approaches for determining robust CCFs. First, via analytical development, we: (a) demonstrate that linearity of the CCF in tristimulus space endows it with resilience to variations, particularly, linearity can ensure invariance of the gray axis, (b) construct an axially linear CCF that is defined by the property of linearity over constant chromaticity loci, and (c) obtain an analytical form for the axially linear CCF that demonstrates it is continuous but suffers from the limitation that it does not have continuous derivatives. Second, to overcome the limitation of the axially linear CCF, we motivate and develop two variational objective functions for optimization of multiprimary CCFs, the first aims to preserve color transitions in the presence of primary/observer variations and the second combines this objective with desirable invariance along the gray axis, by incorporating the axially linear CCF. A companion Part II paper, presents an algorithmic approach for numerically computing optimal CCFs for the two alternative variational objective functions proposed here and presents results comparing alternative CCFs for several different 4, 5, and 6 primary designs.},
  archive      = {J_TIP},
  author       = {Carlos Eduardo Rodríguez-Pardo and Gaurav Sharma},
  doi          = {10.1109/TIP.2019.2937067},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1152-1163},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color control functions for multiprimary Displays—I: Robustness analysis and optimization formulations},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From pairwise comparisons and rating to a unified quality
scale. <em>TIP</em>, <em>29</em>, 1139–1151. (<a
href="https://doi.org/10.1109/TIP.2019.2936103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of psychometric scaling is the quantification of perceptual experiences, understanding the relationship between an external stimulus, the internal representation and the response. In this paper, we propose a probabilistic framework to fuse the outcome of different psychophysical experimental protocols, namely rating and pairwise comparisons experiments. Such a method can be used for merging existing datasets of subjective nature and for experiments in which both measurements are collected. We analyze and compare the outcomes of both types of experimental protocols in terms of time and accuracy in a set of simulations and experiments with benchmark and real-world image quality assessment datasets, showing the necessity of scaling and the advantages of each protocol and mixing. Although most of our examples focus on image quality assessment, our findings generalize to any other subjective quality-of-experience task.},
  archive      = {J_TIP},
  author       = {María Pérez-Ortiz and Aliaksei Mikhailiuk and Emin Zerman and Vedad Hulusic and Giuseppe Valenzise and Rafał K. Mantiuk},
  doi          = {10.1109/TIP.2019.2936103},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1139-1151},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {From pairwise comparisons and rating to a unified quality scale},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perceptual evaluation for multi-exposure image fusion of
dynamic scenes. <em>TIP</em>, <em>29</em>, 1127–1138. (<a
href="https://doi.org/10.1109/TIP.2019.2940678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common approach to high dynamic range (HDR) imaging is to capture multiple images of different exposures followed by multi-exposure image fusion (MEF) in either radiance or intensity domain. A predominant problem of this approach is the introduction of the ghosting artifacts in dynamic scenes with camera and object motion. While many MEF methods (often referred to as deghosting algorithms) have been proposed for reduced ghosting artifacts and improved visual quality, little work has been dedicated to perceptual evaluation of their deghosting results. Here we first construct a database that contains 20 multi-exposure sequences of dynamic scenes and their corresponding fused images by nine MEF algorithms. We then carry out a subjective experiment to evaluate fused image quality, and find that none of existing objective quality models for MEF provides accurate quality predictions. Motivated by this, we develop an objective quality model for MEF of dynamic scenes. Specifically, we divide the test image into static and dynamic regions, measure structural similarity between the image and the corresponding sequence in the two regions separately, and combine quality measurements of the two regions into an overall quality score. Experimental results show that the proposed method significantly outperforms the state-of-the-art. In addition, we demonstrate the promise of the proposed model in parameter tuning of MEF methods. The subjective database and the MATLAB code of the proposed model are made publicly available at https://github.com/h4nwei/MEF-SSIMd.},
  archive      = {J_TIP},
  author       = {Yuming Fang and Hanwei Zhu and Kede Ma and Zhou Wang and Shutao Li},
  doi          = {10.1109/TIP.2019.2940678},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1127-1138},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceptual evaluation for multi-exposure image fusion of dynamic scenes},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video saliency prediction using spatiotemporal residual
attentive networks. <em>TIP</em>, <em>29</em>, 1113–1126. (<a
href="https://doi.org/10.1109/TIP.2019.2936112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel residual attentive learning network architecture for predicting dynamic eye-fixation maps. The proposed model emphasizes two essential issues, i.e., effective spatiotemporal feature integration and multi-scale saliency learning. For the first problem, appearance and motion streams are tightly coupled via dense residual cross connections, which integrate appearance information with multi-layer, comprehensive motion features in a residual and dense way. Beyond traditional two-stream models learning appearance and motion features separately, such design allows early, multi-path information exchange between different domains, leading to a unified and powerful spatiotemporal learning architecture. For the second one, we propose a composite attention mechanism that learns multi-scale local attentions and global attention priors end-to-end. It is used for enhancing the fused spatiotemporal features via emphasizing important features in multi-scales. A lightweight convolutional Gated Recurrent Unit (convGRU), which is flexible for small training data situation, is used for long-term temporal characteristics modeling. Extensive experiments over four benchmark datasets clearly demonstrate the advantage of the proposed video saliency model over other competitors and the effectiveness of each component of our network. Our code and all the results will be available at https://github.com/ashleylqx/STRA-Net.},
  archive      = {J_TIP},
  author       = {Qiuxia Lai and Wenguan Wang and Hanqiu Sun and Jianbing Shen},
  doi          = {10.1109/TIP.2019.2936112},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1113-1126},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video saliency prediction using spatiotemporal residual attentive networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Multiple cycle-in-cycle generative adversarial networks for
unsupervised image super-resolution. <em>TIP</em>, <em>29</em>,
1101–1112. (<a href="https://doi.org/10.1109/TIP.2019.2938347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the help of convolutional neural networks (CNN), the single image super-resolution problem has been widely studied. Most of these CNN based methods focus on learning a model to map a low-resolution (LR) image to a highresolution (HR) image, where the LR image is downsampled from the HR image with a known model. However, in a more general case when the process of the down-sampling is unknown and the LR input is degraded by noises and blurring, it is difficult to acquire the LR and HR image pairs for traditional supervised learning. Inspired by the recent unsupervised imagestyle translation applications using unpaired data, we propose a multiple Cycle-in-Cycle network structure to deal with the more general case using multiple generative adversarial networks (GAN) as the basis components. The first network cycle aims at mapping the noisy and blurry LR input to a noise-free LR space, then a new cycle with a well-trained x2 network model is orderly introduced to super-resolve the intermediate output of the former cycle. The number of total cycles depends on the different up-sampling factors (x2, x4, x8). Finally, all modules are trained in an end-to-end manner to get the desired HR output. Quantitative indexes and qualitative results show that our proposed method achieves comparable performance with the state-of-the-art supervised models.},
  archive      = {J_TIP},
  author       = {Yongbing Zhang and Siyuan Liu and Chao Dong and Xinfeng Zhang and Yuan Yuan},
  doi          = {10.1109/TIP.2019.2938347},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1101-1112},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiple cycle-in-cycle generative adversarial networks for unsupervised image super-resolution},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved robust video saliency detection based on long-term
spatial-temporal information. <em>TIP</em>, <em>29</em>, 1090–1100. (<a
href="https://doi.org/10.1109/TIP.2019.2934350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes to utilize supervised deep convolutional neural networks to take full advantage of the long-term spatial-temporal information in order to improve the video saliency detection performance. The conventional methods, which use the temporally neighbored frames solely, could easily encounter transient failure cases when the spatial-temporal saliency clues are less-trustworthy for a long period. To tackle the aforementioned limitation, we plan to identify those beyond-scope frames with trustworthy long-term saliency clues first and then align it with the current problem domain for an improved video saliency detection.},
  archive      = {J_TIP},
  author       = {Chenglizhao Chen and Guotao Wang and Chong Peng and Xiaowei Zhang and Hong Qin},
  doi          = {10.1109/TIP.2019.2934350},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1090-1100},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improved robust video saliency detection based on long-term spatial-temporal information},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Homologous component analysis for domain adaptation.
<em>TIP</em>, <em>29</em>, 1074–1089. (<a
href="https://doi.org/10.1109/TIP.2019.2929421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate shift assumption-based domain adaptation approaches usually utilize only one common transformation to align marginal distributions and make conditional distributions preserved. However, one common transformation may cause loss of useful information, such as variances and neighborhood relationship in both source and target domains. To address this problem, we propose a novel method called homologous component analysis (HCA) where we try to find two totally different but homologous transformations to align distributions with side information and make conditional distributions preserved. As it is hard to find a closed-form solution to the corresponding optimization problem, we solve them by means of the alternating direction minimizing method (ADMM) in the context of Stiefel manifolds. We also provide a generalization error bound for domain adaptation in the semi-supervised case, and two transformations can help to decrease this upper bound more than only one common transformation does. Extensive experiments on synthetic and real data show the effectiveness of the proposed method by comparing its classification accuracy with the state-of-the-art methods, and the numerical evidence on chordal distance and Frobenius distance shows that resulting optimal transformations are different.},
  archive      = {J_TIP},
  author       = {Youfa Liu and Weiping Tu and Bo Du and Lefei Zhang and Dacheng Tao},
  doi          = {10.1109/TIP.2019.2929421},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1074-1089},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Homologous component analysis for domain adaptation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EleAtt-RNN: Adding attentiveness to neurons in recurrent
neural networks. <em>TIP</em>, <em>29</em>, 1061–1073. (<a
href="https://doi.org/10.1109/TIP.2019.2937724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are capable of modeling temporal dependencies of complex sequential data. In general, current available structures of RNNs tend to concentrate on controlling the contributions of current and previous information. However, the exploration of different importance levels of different elements within an input vector is always ignored. We propose a simple yet effective Element-wise-Attention Gate (EleAttG), which can be easily added to an RNN block (e.g. all RNN neurons in an RNN layer), to empower the RNN neurons to have attentiveness capability. For an RNN block, an EleAttG is used for adaptively modulating the input by assigning different levels of importance, i.e., attention, to each element/dimension of the input. We refer to an RNN block equipped with an EleAttG as an EleAtt-RNN block. Instead of modulating the input as a whole, the EleAttG modulates the input at fine granularity, i.e., element-wise, and the modulation is content adaptive. The proposed EleAttG, as an additional fundamental unit, is general and can be applied to any RNN structures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to different tasks including the action recognition, from both skeleton-based data and RGB videos, gesture recognition, and sequential MNIST classification. Experiments show that adding attentiveness through EleAttGs to RNN blocks significantly improves the power of RNNs.},
  archive      = {J_TIP},
  author       = {Pengfei Zhang and Jianru Xue and Cuiling Lan and Wenjun Zeng and Zhanning Gao and Nanning Zheng},
  doi          = {10.1109/TIP.2019.2937724},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1061-1073},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {EleAtt-RNN: Adding attentiveness to neurons in recurrent neural networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ring difference filter for fast and noise robust depth from
focus. <em>TIP</em>, <em>29</em>, 1045–1060. (<a
href="https://doi.org/10.1109/TIP.2019.2937064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth from focus (DfF) is a method of estimating the depth of a scene by using information acquired through changes in the focus of a camera. Within the DfF framework of, the focus measure (FM) forms the foundation which determines the accuracy of the output. With the results from the FM, the role of a DfF pipeline is to determine and recalculate unreliable measurements while enhancing those that are reliable. In this paper, we propose a new FM, which we call the “ring difference filter” (RDF), that can more accurately and robustly measure focus. FMs can usually be categorized as confident local methods or noise robust non-local methods. The RDF&#39;s unique ring-and-disk structure allows it to have the advantages of both local and non-local FMs. We then describe an efficient pipeline that utilizes the RDF&#39;s properties. Part of this pipeline is our proposed RDF-based cost aggregation method, which is able to robustly refine the initial results in the presence of image noise. Our method is able to reproduce results that are on par with or even better than those of state-of-the-art methods, while spending less time in computation.},
  archive      = {J_TIP},
  author       = {Hae-Gon Jeon and Jaeheung Surh and Sunghoon Im and In So Kweon},
  doi          = {10.1109/TIP.2019.2937064},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1045-1060},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ring difference filter for fast and noise robust depth from focus},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ME r-CNN: Multi-expert r-CNN for object detection.
<em>TIP</em>, <em>29</em>, 1030–1044. (<a
href="https://doi.org/10.1109/TIP.2019.2938879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Multi-Expert Region-based Convolutional Neural Network (ME R-CNN) which is equipped with multiple experts (ME) where each expert is learned to process a certain type of regions of interest (RoIs). This architecture better captures the appearance variations of the RoIs caused by different shapes, poses, and viewing angles. In order to direct each RoI to the appropriate expert, we devise a novel “learnable” network, which we call, expert assignment network (EAN). EAN automatically learns the optimal RoI-expert relationship even without any supervision of expert assignment. As the major components of ME R-CNN, ME and EAN, are mutually affecting each other while tied to a shared network, neither an alternating nor a naive end-to-end optimization is likely to fail. To address this problem, we introduce a practical training strategy which is tailored to optimize ME, EAN, and the shared network in an end-to-end fashion. We show that both of the architectures provide considerable performance increase over the baselines on PASCAL VOC 07, 12, and MS COCO datasets.},
  archive      = {J_TIP},
  author       = {Hyungtae Lee and Sungmin Eum and Heesung Kwon},
  doi          = {10.1109/TIP.2019.2938879},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1030-1044},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ME R-CNN: Multi-expert R-CNN for object detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep cascade model-based face recognition: When deep-layered
learning meets small data. <em>TIP</em>, <em>29</em>, 1016–1029. (<a
href="https://doi.org/10.1109/TIP.2019.2938307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation based classification (SRC), nuclear-norm matrix regression (NMR), and deep learning (DL) have achieved a great success in face recognition (FR). However, there still exist some intrinsic limitations among them. SRC and NMR based coding methods belong to one-step model, such that the latent discriminative information of the coding error vector cannot be fully exploited. DL, as a multi-step model, can learn powerful representation, but relies on large-scale data and computation resources for numerous parameters training with complicated back-propagation. Straightforward training of deep neural networks from scratch on small-scale data is almost infeasible. Therefore, in order to develop efficient algorithms that are specifically adapted for small-scale data, we propose to derive the deep models of SRC and NMR. Specifically, in this paper, we propose an end-to-end deep cascade model (DCM) based on SRC and NMR with hierarchical learning, nonlinear transformation and multi-layer structure for corrupted face recognition. The contributions include four aspects. First, an end-to-end deep cascade model for small-scale data without back-propagation is proposed. Second, a multi-level pyramid structure is integrated for local feature representation. Third, for introducing nonlinear transformation in layer-wise learning, softmax vector coding of the errors with class discrimination is proposed. Fourth, the existing representation methods can be easily integrated into our DCM framework. Experiments on a number of small-scale benchmark FR datasets demonstrate the superiority of the proposed model over state-of-the-art counterparts. Additionally, a perspective that deep-layered learning does not have to be convolutional neural network with back-propagation optimization is consolidated. The demo code is available in https://github.com/liuji93/DCM},
  archive      = {J_TIP},
  author       = {Lei Zhang and Ji Liu and Bob Zhang and David Zhang and Ce Zhu},
  doi          = {10.1109/TIP.2019.2938307},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1016-1029},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep cascade model-based face recognition: When deep-layered learning meets small data},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Cross-view gait recognition by discriminative feature
learning. <em>TIP</em>, <em>29</em>, 1001–1015. (<a
href="https://doi.org/10.1109/TIP.2019.2926208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning-based cross-view gait recognition has become popular owing to the strong capacity of convolutional neural networks (CNNs). Current deep learning methods often rely on loss functions used widely in the task of face recognition, e.g., contrastive loss and triplet loss. These loss functions have the problem of hard negative mining. In this paper, a robust, effective, and gait-related loss function, called angle center loss (ACL), is proposed to learn discriminative gait features. The proposed loss function is robust to different local parts and temporal window sizes. Different from center loss which learns a center for each identity, the proposed loss function learns multiple sub-centers for each angle of the same identity. Only the largest distance between the anchor feature and the corresponding cross-view sub-centers is penalized, which achieves better intra-subject compactness. We also propose to extract discriminative spatial–temporal features by local feature extractors and a temporal attention model. A simplified spatial transformer network is proposed to localize the suitable horizontal parts of the human body. Local gait features for each horizontal part are extracted and then concatenated as the descriptor. We introduce long short-term memory (LSTM) units as the temporal attention model to learn the attention score for each frame, e.g., focusing more on discriminative frames and less on frames with bad quality. The temporal attention model shows better performance than the temporal average pooling or gait energy images (GEI). By combing the three aspects, we achieve state-of-the-art results on several cross-view gait recognition benchmarks.},
  archive      = {J_TIP},
  author       = {Yuqi Zhang and Yongzhen Huang and Shiqi Yu and Liang Wang},
  doi          = {10.1109/TIP.2019.2926208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1001-1015},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-view gait recognition by discriminative feature learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Progressive object transfer detection. <em>TIP</em>,
<em>29</em>, 986–1000. (<a
href="https://doi.org/10.1109/TIP.2019.2938680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent development of object detection mainly depends on deep learning with large-scale benchmarks. However, collecting such fully-annotated data is often difficult or expensive for real-world applications, which restricts the power of deep neural networks in practice. Alternatively, humans can detect new objects with little annotation burden, since humans often use the prior knowledge to identify new objects with few elaborately-annotated examples, and subsequently generalize this capacity by exploiting objects from wild images. Inspired by this procedure of learning to detect, we propose a novel Progressive Object Transfer Detection (POTD) framework. Specifically, we make three main contributions in this paper. First, POTD can leverage various object supervision of different domains effectively into a progressive detection procedure. Via such human-like learning, one can boost a target detection task with few annotations. Second, POTD consists of two delicate transfer stages, i.e., Low-Shot Transfer Detection (LSTD), and Weakly-Supervised Transfer Detection (WSTD). In LSTD, we distill the implicit object knowledge of source detector to enhance target detector with few annotations. It can effectively warm up WSTD later on. In WSTD, we design a recurrent object labelling mechanism for learning to annotate weakly-labeled images. More importantly, we exploit the reliable object supervision from LSTD, which can further enhance the robustness of target detector in the WSTD stage. Finally, we perform extensive experiments on a number of challenging detection benchmarks with different settings. The results demonstrate that, our POTD outperforms the recent state-of-the-art approaches. The codes and models are available at https://github.com/Cassie94/LSTD/tree/lstd .},
  archive      = {J_TIP},
  author       = {Hao Chen and Yali Wang and Guoyou Wang and Xiang Bai and Yu Qiao},
  doi          = {10.1109/TIP.2019.2938680},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {986-1000},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive object transfer detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view video synopsis via simultaneous object-shifting
and view-switching optimization. <em>TIP</em>, <em>29</em>, 971–985. (<a
href="https://doi.org/10.1109/TIP.2019.2938086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for synopsizing multiple videos captured by a set of surveillance cameras with some overlapped field-of-views. Currently, object-based approaches that directly shift objects along the time axis are already able to compute compact synopsis results for multiple surveillance videos. The challenge is how to present the multiple synopsis results in a more compact and understandable way. Previous approaches show them side by side on the screen, which however is difficult for user to comprehend. In this paper, we solve the problem by joint object-shifting and camera view-switching. Firstly, we synchronize the input videos, and group the same object in different videos together. Then we shift the groups of objects along the time axis to obtain multiple synopsis videos. Instead of showing them simultaneously, we just show one of them at each time, and allow to switch among the views of different synopsis videos. In this view switching way, we obtain just a single synopsis results consisting of content from all the input videos, which is much easier for user to follow and understand. To obtain the best synopsis result, we construct a simultaneous object-shifting and view-switching optimization framework instead of solving them separately. We also present an alternative optimization strategy composed of graph cuts and dynamic programming to solve the unified optimization. Experiments demonstrate that our single synopsis video generated from multiple input videos is compact, complete, and easy to understand.},
  archive      = {J_TIP},
  author       = {Zhensong Zhang and Yongwei Nie and Hanqiu Sun and Qing Zhang and Qiuxia Lai and Guiqing Li and Mingyu Xiao},
  doi          = {10.1109/TIP.2019.2938086},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {971-985},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view video synopsis via simultaneous object-shifting and view-switching optimization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning latent global network for skeleton-based action
prediction. <em>TIP</em>, <em>29</em>, 959–970. (<a
href="https://doi.org/10.1109/TIP.2019.2937757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human actions represented with 3D skeleton sequences are robust to clustered backgrounds and illumination changes. In this paper, we investigate skeleton-based action prediction, which aims to recognize an action from a partial skeleton sequence that contains incomplete action information. We propose a new Latent Global Network based on adversarial learning for action prediction. We demonstrate that the proposed network provides latent long-term global information that is complementary to the local action information of the partial sequences and helps improve action prediction. We show that action prediction can be improved by combining the latent global information with the local action information. We test the proposed method on three challenging skeleton datasets and report state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Qiuhong Ke and Mohammed Bennamoun and Hossein Rahmani and Senjian An and Ferdous Sohel and Farid Boussaid},
  doi          = {10.1109/TIP.2019.2937757},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {959-970},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning latent global network for skeleton-based action prediction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Burst ranking for blind multi-image deblurring.
<em>TIP</em>, <em>29</em>, 947–958. (<a
href="https://doi.org/10.1109/TIP.2019.2936073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new incremental aggregation algorithm for multi-image deblurring with automatic image selection. The primary motivation is that current burst deblurring methods do not handle well situations in which misalignment or out-of-context frames are present in the burst. These real-life situations result in poor reconstructions or manual selection of the images that are used to deblur. Automatically selecting the best frames within the burst to improve the base reconstruction is challenging because the number of possible images fusions is equal to the power set cardinal. Here, we approach the multi-image deblurring problem as a two steps process. First, we successfully learn a comparison function to rank a burst of images using a deep convolutional neural network. Then, an incremental Fourier burst accumulation with a reconstruction degradation mechanism is applied fusing only less blurred images that are sufficient to maximize the reconstruction quality. Experiments with the proposed algorithm have shown superior results when compared to other similar approaches, outperforming other methods described in the literature in previously described situations. We validate our findings on several synthetic and real datasets.},
  archive      = {J_TIP},
  author       = {Fidel Alejandro Guerrero Peña and Pedro Diamel Marrero Fernández and Tsang Ing Ren and Jorge de Jesus Gomes Leandro and Ricardo Massahiro Nishihara},
  doi          = {10.1109/TIP.2019.2936073},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {947-958},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Burst ranking for blind multi-image deblurring},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online multi-expert learning for visual tracking.
<em>TIP</em>, <em>29</em>, 934–946. (<a
href="https://doi.org/10.1109/TIP.2019.2931082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correlation filters based trackers have achieved an excellent performance for object tracking in recent years. However, most existing methods use only one filter but ignore the information of the previous filters. In this paper, we propose a novel online multi-expert learning algorithm for visual tracking. In our proposed scheme, there are former trackers which retain the previous filters, and those trackers will give their predictions in each frame. The current tracker represents the filter of current frame, and both the current tracker and the former trackers constitute our expert ensemble. We use an adaptive Second-order Quantile strategy to learn the weights of each expert, which can take full advantage of all the experts. To simplify our model and remove some bad experts, we prune our models via a minimum entropy criterion. Finally, we propose a new update strategy to avoid the model corruption problem. Extensive experimental results on both OTB2013 and OTB2015 benchmarks demonstrate that our proposed tracker performs favorably against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Zhetao Li and Wei Wei and Tianzhu Zhang and Meng Wang and Sujuan Hou and Xin Peng},
  doi          = {10.1109/TIP.2019.2931082},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {934-946},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Online multi-expert learning for visual tracking},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient algorithm for the piecewise affine-linear
mumford-shah model based on a taylor jet splitting. <em>TIP</em>,
<em>29</em>, 921–933. (<a
href="https://doi.org/10.1109/TIP.2019.2937040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an algorithm to efficiently compute approximate solutions of the piecewise affine Mumford-Shah model. The algorithm is based on a novel reformulation of the underlying optimization problem in terms of Taylor jets. A splitting approach leads to linewise segmented jet estimation problems for which we propose an exact and efficient solver. The proposed method has the combined advantages of prior algorithms: it directly yields a partition, it does not need an initialization procedure, and it is highly parallelizable. The experiments show that the algorithm has lower computation times and that the solutions often have lower functional values than the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Lukas Kiefer and Martin Storath and Andreas Weinmann},
  doi          = {10.1109/TIP.2019.2937040},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {921-933},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An efficient algorithm for the piecewise affine-linear mumford-shah model based on a taylor jet splitting},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure-preserving neural style transfer. <em>TIP</em>,
<em>29</em>, 909–920. (<a
href="https://doi.org/10.1109/TIP.2019.2936746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art neural style transfer methods have demonstrated amazing results by training feed-forward convolutional neural networks or using an iterative optimization strategy. The image representation used in these methods, which contains two components: style representation and content representation, is typically based on high-level features extracted from pre-trained classification networks. Because the classification networks are originally designed for object recognition, the extracted features often focus on the central object and neglect other details. As a result, the style textures tend to scatter over the stylized outputs and disrupt the content structures. To address this issue, we present a novel image stylization method that involves an additional structure representation. Our structure representation, which considers two factors: i) the global structure represented by the depth map and ii) the local structure details represented by the image edges, effectively reflects the spatial distribution of all the components in an image as well as the structure of dominant objects respectively. Experimental results demonstrate that our method achieves an impressive visual effectiveness, which is particularly significant when processing images sensitive to structure distortion, e.g. images containing multiple objects potentially at different depths, or dominant objects with clear structures.},
  archive      = {J_TIP},
  author       = {Ming-Ming Cheng and Xiao-Chang Liu and Jie Wang and Shao-Ping Lu and Yu-Kun Lai and Paul L. Rosin},
  doi          = {10.1109/TIP.2019.2936746},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {909-920},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure-preserving neural style transfer},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Pothole detection based on disparity transformation and
road surface modeling. <em>TIP</em>, <em>29</em>, 897–908. (<a
href="https://doi.org/10.1109/TIP.2019.2933750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pothole detection is one of the most important tasks for road maintenance. Computer vision approaches are generally based on either 2D road image analysis or 3D road surface modeling. However, these two categories are always used independently. Furthermore, the pothole detection accuracy is still far from satisfactory. Therefore, in this paper, we present a robust pothole detection algorithm that is both accurate and computationally efficient. A dense disparity map is first transformed to better distinguish between damaged and undamaged road areas. To achieve greater disparity transformation efficiency, golden section search and dynamic programming are utilized to estimate the transformation parameters. Otsu&#39;s thresholding method is then used to extract potential undamaged road areas from the transformed disparity map. The disparities in the extracted areas are modeled by a quadratic surface using least squares fitting. To improve disparity map modeling robustness, the surface normal is also integrated into the surface modeling process. Furthermore, random sample consensus is utilized to reduce the effects caused by outliers. By comparing the difference between the actual and modeled disparity maps, the potholes can be detected accurately. Finally, the point clouds of the detected potholes are extracted from the reconstructed 3D road surface. The experimental results show that the successful detection accuracy of the proposed system is around 98.7\% and the overall pixel-level accuracy is approximately 99.6\%.},
  archive      = {J_TIP},
  author       = {Rui Fan and Umar Ozgunalp and Brett Hosking and Ming Liu and Ioannis Pitas},
  doi          = {10.1109/TIP.2019.2933750},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {897-908},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pothole detection based on disparity transformation and road surface modeling},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting related and unrelated tasks for hierarchical
metric learning and image classification. <em>TIP</em>, <em>29</em>,
883–896. (<a href="https://doi.org/10.1109/TIP.2019.2938321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-task learning, multiple interrelated tasks are jointly learned to achieve better performance. In many cases, if we can identify which tasks are related, we can also clearly identify which tasks are unrelated. In the past, most researchers emphasized exploiting correlations among interrelated tasks while completely ignoring the unrelated tasks that may provide valuable prior knowledge for multi-task learning. In this paper, a new approach is developed to hierarchically learn a tree of multi-task metrics by leveraging prior knowledge about both the related tasks and unrelated tasks. First, a visual tree is constructed to hierarchically organize large numbers of image categories in a coarse-to-fine fashion. Over the visual tree, a multi-task metric classifier is learned for each node by exploiting both the related and unrelated tasks, where the learning tasks for training the classifiers for the sibling child nodes under the same parent node are treated as the interrelated tasks, and the others are treated as the unrelated tasks. In addition, the node-specific metric for the parent node is propagated to its sibling child nodes to control inter-level error propagation. Our experimental results demonstrate that our hierarchical metric learning algorithm achieves better results than other state-of-the-art algorithms.},
  archive      = {J_TIP},
  author       = {Yu Zheng and Jianping Fan and Ji Zhang and Xinbo Gao},
  doi          = {10.1109/TIP.2019.2938321},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {883-896},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploiting related and unrelated tasks for hierarchical metric learning and image classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural compatibility modeling with probabilistic knowledge
distillation. <em>TIP</em>, <em>29</em>, 871–882. (<a
href="https://doi.org/10.1109/TIP.2019.2936742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern society, clothing matching plays a pivotal role in people&#39;s daily life, as suitable outfits can beautify their appearance directly. Nevertheless, how to make a suitable outfit has become a daily headache for many people, especially those who do not have much sense of aesthetics. In the light of this, many research efforts have been dedicated to the task of complementary clothing matching and have achieved great success relying on the advanced data-driven neural networks. However, most existing methods overlook the rich valuable knowledge accumulated by our human beings in the fashion domain, especially the rules regarding clothing matching, like “coats go with dresses” and “silk tops cannot go with chiffon bottoms”. Towards this end, in this work, we propose a knowledge-guided neural compatibility modeling scheme, which is able to incorporate the rich fashion domain knowledge to enhance the performance of the compatibility modeling in the context of clothing matching. To better integrate the huge and implicit fashion domain knowledge into the data-driven neural networks, we present a probabilistic knowledge distillation (PKD) method, which is able to encode vast knowledge rules in a probabilistic manner. Extensive experiments on two real-world datasets have verified the guidance of rules from different sources and demonstrated the effectiveness and portability of our model. As a byproduct, we released the codes and involved parameters to benefit the research community.},
  archive      = {J_TIP},
  author       = {Xianjing Han and Xuemeng Song and Yiyang Yao and Xin-Shun Xu and Liqiang Nie},
  doi          = {10.1109/TIP.2019.2936742},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {871-882},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Neural compatibility modeling with probabilistic knowledge distillation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework of reversible color-to-grayscale conversion with
watermarking feature. <em>TIP</em>, <em>29</em>, 859–870. (<a
href="https://doi.org/10.1109/TIP.2019.2936097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible color-to-grayscale conversion (RCGC) is a method that embeds the chromatic information of a full color image into its grayscale version such that the original color image can be reconstructed in the future when necessary. In practical applications, it is required to provide a means to authenticate an information-embedded image such that its integrity can be guaranteed. However, none of the current RCGC algorithms take this factor into account. In this paper, to address this issue, we develop an information-embedding framework based on a vector quantization-based (VQ-based) RCGC algorithm recently proposed by us. Under this framework, we propose a RCGC algorithm that can embed both chromatic information and fragile watermark simultaneously into a grayscale image with the same technique to reduce the complexity and improve the efficiency. Like other VQ-based RCGC algorithms, the performance of the proposed RCGC algorithm highly relies on the palette it uses. We also propose a palette generation algorithm in this paper to support the information embedding process such that the visual quality of the color-embedded grayscale images and the reconstructed color images can be significantly improved.},
  archive      = {J_TIP},
  author       = {Yuk-Hee Chan and Zi-Xin Xu and Daniel Pak-Kong Lun},
  doi          = {10.1109/TIP.2019.2936097},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {859-870},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A framework of reversible color-to-grayscale conversion with watermarking feature},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Category-aware spatial constraint for weakly supervised
detection. <em>TIP</em>, <em>29</em>, 843–858. (<a
href="https://doi.org/10.1109/TIP.2019.2933735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection has attracted increasing research attention recently. To this end, most existing schemes rely on scoring category-independent region proposals, which is formulated as a multiple instance learning problem. During this process, the proposal scores are aggregated and supervised by only image-level labels, which often fails to locate object boundaries precisely. In this paper, we break through such a restriction by taking a deeper look into the score aggregation stage and propose a Category-aware Spatial Constraint (CSC) scheme for proposals, which is integrated into weakly supervised object detection in an end-to-end learning manner. In particular, we incorporate the global shape information of objects as an unsupervised constraint, which is inferred from build-in foreground-and-background cues, termed Category-specific Pixel Gradient (CPG) maps. Specifically, each region proposal is weighted according to how well it covers the estimated shape of objects. For each category, a multi-center regularization is further introduced to penalize the violations between centers cluster and high-score proposals in a given image. Extensive experiments are done on the most widely-used benchmark Pascal VOC and COCO, which shows that our approach significantly improves weakly supervised object detection without adding new learnable parameters to the existing models nor changing the structures of CNNs.},
  archive      = {J_TIP},
  author       = {Yunhang Shen and Rongrong Ji and Kuiyuan Yang and Cheng Deng and Changhu Wang},
  doi          = {10.1109/TIP.2019.2933735},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {843-858},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Category-aware spatial constraint for weakly supervised detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiresolution localization with temporal scanning for
super-resolution diffuse optical imaging of fluorescence. <em>TIP</em>,
<em>29</em>, 830–842. (<a
href="https://doi.org/10.1109/TIP.2019.2931080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A super-resolution optical imaging method is presented that relies on the distinct temporal information associated with each fluorescent optical reporter to determine its spatial position to high precision with measurements of heavily scattered light. This multiple-emitter localization approach uses a diffusion equation forward model in a cost function, and has the potential to achieve micron-scale spatial resolution through centimeters of tissue. Utilizing some degree of temporal separation for the reporter emissions, position and emission strength are determined using a computationally efficient temporal-scanning multiresolution algorithm. The approach circumvents the spatial resolution challenges faced by earlier optical imaging approaches by using a diffusion equation forward model, and is promising for in vivo applications. For example, in principle, the method could be used to localize individual neurons firing throughout a rodent brain, enabling the direct imaging of neural network activity.},
  archive      = {J_TIP},
  author       = {Brian Z. Bentz and Dergan Lin and Justin A. Patel and Kevin J. Webb},
  doi          = {10.1109/TIP.2019.2931080},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {830-842},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiresolution localization with temporal scanning for super-resolution diffuse optical imaging of fluorescence},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust seismic image interpolation with mathematical
morphological constraint. <em>TIP</em>, <em>29</em>, 819–829. (<a
href="https://doi.org/10.1109/TIP.2019.2936744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seismic image interpolation is a currently popular research subject in modern reflection seismology. The interpolation problem is generally treated as a process of inversion. Under the compressed sensing framework, various sparse transformations and low-rank constraints based methods have great performances in recovering irregularly missing traces. However, in the case of regularly missing traces, their applications are limited because of the strong spatial aliasing energies. In addition, the erratic noise always poses a serious impact on the interpolation results obtained by the sparse transformations and low-rank constraints-based methods,. This is because the erratic noise is far from satisfying the statistical assumption behind these methods. In this study, we propose a mathematical morphology-based interpolation technique, which constrains the morphological scale of the model in the inversion process. The inversion problem is solved by the shaping regularization approach. The mathematical morphological constraint (MMC)-based interpolation technique has a satisfactory robustness to the spatial aliasing and erratic energies. We provide a detailed algorithmic framework and discuss the extension from 2D to higher dimensional version and the back operator in the shaping inversion. A group of numerical examples demonstrates the successful performance of the proposed technique.},
  archive      = {J_TIP},
  author       = {Weilin Huang and Jianxin Liu},
  doi          = {10.1109/TIP.2019.2936744},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {819-829},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust seismic image interpolation with mathematical morphological constraint},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmenting cellular retinal images by optimizing
super-pixels, multi-level modularity, and cell boundary representation.
<em>TIP</em>, <em>29</em>, 809–818. (<a
href="https://doi.org/10.1109/TIP.2019.2936743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an interactive method for retina layer segmentation in gray-level and RGB images based on super-pixels, multi-level optimization of modularity, and boundary erosion. Our method produces highly accurate segmentation results and can segment very large images. We have evaluated our method with two datasets of 2D confocal microscopy (CM) images of a mammalian retina. We have obtained average Jaccard index values of 0.948 and 0.942 respectively, confirming the high-quality segmentation performance of our method relative to a known ground truth segmentation. Average processing time was two seconds.},
  archive      = {J_TIP},
  author       = {Oscar Cuadros Linares and Bernd Hamann and João Batista Neto},
  doi          = {10.1109/TIP.2019.2936743},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {809-818},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Segmenting cellular retinal images by optimizing super-pixels, multi-level modularity, and cell boundary representation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D point cloud attribute compression using geometry-guided
sparse representation. <em>TIP</em>, <em>29</em>, 796–808. (<a
href="https://doi.org/10.1109/TIP.2019.2936738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point clouds associated with attributes are considered as a promising paradigm for immersive communication. However, the corresponding compression schemes for this media are still in the infant stage. Moreover, in contrast to conventional image/video compression, it is a more challenging task to compress 3D point cloud data, arising from the irregular structure. In this paper, we propose a novel and effective compression scheme for the attributes of voxelized 3D point clouds. In the first stage, an input voxelized 3D point cloud is divided into blocks of equal size. Then, to deal with the irregular structure of 3D point clouds, a geometry-guided sparse representation (GSR) is proposed to eliminate the redundancy within each block, which is formulated as an ℓ 0 -norm regularized optimization problem. Also, an inter-block prediction scheme is applied to remove the redundancy between blocks. Finally, by quantitatively analyzing the characteristics of the resulting transform coefficients by GSR, an effective entropy coding strategy that is tailored to our GSR is developed to generate the bitstream. Experimental results over various benchmark datasets show that the proposed compression scheme is able to achieve better rate-distortion performance and visual quality, compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Shuai Gu and Junhui Hou and Huanqiang Zeng and Hui Yuan and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2019.2936738},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {796-808},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D point cloud attribute compression using geometry-guided sparse representation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Degraded image semantic segmentation with dense-gram
networks. <em>TIP</em>, <em>29</em>, 782–795. (<a
href="https://doi.org/10.1109/TIP.2019.2936111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Degraded image semantic segmentation is of great importance in autonomous driving, highway navigation systems, and many other safety-related applications and it was not systematically studied before. In general, image degradations increase the difficulty of semantic segmentation, usually leading to decreased semantic segmentation accuracy. Therefore, performance on the underlying clean images can be treated as an upper bound of degraded image semantic segmentation. While the use of supervised deep learning has substantially improved the state of the art of semantic image segmentation, the gap between the feature distribution learned using the clean images and the feature distribution learned using the degraded images poses a major obstacle in improving the degraded image semantic segmentation performance. The conventional strategies for reducing the gap include: 1) Adding image-restoration based pre-processing modules; 2) Using both clean and the degraded images for training; 3) Fine-tuning the network pre-trained on the clean image. In this paper, we propose a novel Dense-Gram Network to more effectively reduce the gap than the conventional strategies and segment degraded images. Extensive experiments demonstrate that the proposed Dense-Gram Network yields state-of-the-art semantic segmentation performance on degraded images synthesized using PASCAL VOC 2012, SUNRGBD, CamVid, and CityScapes datasets.},
  archive      = {J_TIP},
  author       = {Dazhou Guo and Yanting Pei and Kang Zheng and Hongkai Yu and Yuhang Lu and Song Wang},
  doi          = {10.1109/TIP.2019.2936111},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {782-795},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Degraded image semantic segmentation with dense-gram networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Arc-support line segments revisited: An efficient
high-quality ellipse detection. <em>TIP</em>, <em>29</em>, 768–781. (<a
href="https://doi.org/10.1109/TIP.2019.2934352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years many ellipse detection algorithms spring up and are studied broadly, while the critical issue of detecting ellipses accurately and efficiently in real-world images remains a challenge. In this paper, we propose a valuable industry-oriented ellipse detector by arc-support line segments, which simultaneously reaches high detection accuracy and efficiency. To simplify the complicated curves in an image while retaining the general properties including convexity and polarity, the arc-support line segments are extracted, which grounds the successful detection of ellipses. The arc-support groups are formed by iteratively and robustly linking the arc-support line segments that latently belong to a common ellipse. Afterward, two complementary approaches, namely, locally selecting the arc-support group with higher saliency and globally searching all the valid paired groups, are adopted to fit the initial ellipses in a fast way. Then, the ellipse candidate set can be formulated by hierarchical clustering of 5D parameter space of initial ellipses. Finally, the salient ellipse candidates are selected and refined as detections subject to the stringent and effective verification. Extensive experiments on three public datasets are implemented and our method achieves the best F-measure scores compared to the state-of-the-art methods. The source code is available at https://github.com/AlanLuSun/High-quality-ellipse-detection.},
  archive      = {J_TIP},
  author       = {Changsheng Lu and Siyu Xia and Ming Shao and Yun Fu},
  doi          = {10.1109/TIP.2019.2934352},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {768-781},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Arc-support line segments revisited: An efficient high-quality ellipse detection},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal change detection in remote sensing images using
an unsupervised pixel pairwise-based markov random field model.
<em>TIP</em>, <em>29</em>, 757–767. (<a
href="https://doi.org/10.1109/TIP.2019.2933747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a Bayesian statistical approach to the multimodal change detection (CD) problem in remote sensing imagery. More precisely, we formulate the multimodal CD problem in the unsupervised Markovian framework. The main novelty of the proposed Markovian model lies in the use of an observation field built up from a pixel pairwise modeling and on the bitemporal heterogeneous satellite image pair. Such modeling allows us to rely instead on a robust visual cue, with the appealing property of being quasi-invariant to the imaging (multi-) modality. To use this observation cue as part of a stochastic likelihood model, we first rely on a preliminary iterative estimation technique that takes into account the variety of the laws in the distribution mixture and estimates the parameters of the Markovian mixture model. Once this estimation step is completed, the Maximum a posteriori (MAP) solution of the change detection map, based on the previously estimated parameters, is then computed with a stochastic optimization process. Experimental results and comparisons involving a mixture of different types of imaging modalities confirm the robustness of the proposed approach.},
  archive      = {J_TIP},
  author       = {Redha Touati and Max Mignotte and Mohamed Dahmane},
  doi          = {10.1109/TIP.2019.2933747},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {757-767},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multimodal change detection in remote sensing images using an unsupervised pixel pairwise-based markov random field model},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel key-point detector based on sparse coding.
<em>TIP</em>, <em>29</em>, 747–756. (<a
href="https://doi.org/10.1109/TIP.2019.2934891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most popular hand-crafted key-point detectors such as Harris corner, MSER, SIFT, SURF rely on some specific pre-designed structures for detection of corners, blobs, or junctions in an image. The very nature of pre-designed structures can be considered a source of inflexibility for these detectors in different contexts. Additionally, the performance of these detectors is also highly affected by non-uniform change in illumination. To the best of our knowledge, while there are some previous works addressing one of the two aforementioned problems, there currently lacks an efficient method to solve both simultaneously. In this paper, we propose a novel Sparse Coding based Key-point detector (SCK) which is fully invariant to affine intensity change and independent of any particular structure. The proposed detector locates a key-point in an image, based on a complexity measure calculated from the block surrounding its position. A strength measure is also proposed for comparing and selecting the detected key-points when the maximum number of key-points is limited. In this paper, the desirable characteristics of the proposed detector are theoretically confirmed. Experimental results on three public datasets also show that the proposed detector achieves significantly high performance in terms of repeatability and matching score.},
  archive      = {J_TIP},
  author       = {Thanh Hong-Phuoc and Ling Guan},
  doi          = {10.1109/TIP.2019.2934891},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {747-756},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel key-point detector based on sparse coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust feature matching using spatial clustering with heavy
outliers. <em>TIP</em>, <em>29</em>, 736–746. (<a
href="https://doi.org/10.1109/TIP.2019.2934572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on removing mismatches from given putative feature matches created typically based on descriptor similarity. To achieve this goal, existing attempts usually involve estimating the image transformation under a geometrical constraint, where a pre-defined transformation model is demanded. This severely limits the applicability, as the transformation could vary with different data and is complex and hard to model in many real-world tasks. From a novel perspective, this paper casts the feature matching into a spatial clustering problem with outliers. The main idea is to adaptively cluster the putative matches into several motion consistent clusters together with an outlier/mismatch cluster. To implement the spatial clustering, we customize the classic density based spatial clustering method of applications with noise (DBSCAN) in the context of feature matching, which enables our approach to achieve quasi-linear time complexity. We also design an iterative clustering strategy to promote the matching performance in case of severely degraded data. Extensive experiments on several datasets involving different types of image transformations demonstrate the superiority of our approach over state-of-the-art alternatives. Our approach is also applied to near-duplicate image retrieval and co-segmentation and achieves promising performance.},
  archive      = {J_TIP},
  author       = {Xingyu Jiang and Jiayi Ma and Junjun Jiang and Xiaojie Guo},
  doi          = {10.1109/TIP.2019.2934572},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {736-746},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust feature matching using spatial clustering with heavy outliers},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single-perspective warps in natural image stitching.
<em>TIP</em>, <em>29</em>, 724–735. (<a
href="https://doi.org/10.1109/TIP.2019.2934344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Results of image stitching can be perceptually divided into single-perspective and multiple-perspective. Compared to the multiple-perspective result, the single-perspective result excels in perspective consistency but suffers from projective distortion. In this paper, we propose two single-perspective warps for natural image stitching. The first one is a parametric warp, which is an incremental combination of the dual-feature-based as-projective-as-possible warp and the quasi-homography warp. The second one is a mesh-based warp, which is determined by optimizing a total energy function that simultaneously emphasizes different characteristics of the single-perspective warp, including alignment, distortion and saliency. A comprehensive evaluation demonstrates that the proposed warp outperforms some state-of-the-art warps in urban scenes, including APAP, AutoStitch, SPHP and GSP.},
  archive      = {J_TIP},
  author       = {Tianli Liao and Nan Li},
  doi          = {10.1109/TIP.2019.2934344},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {724-735},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Single-perspective warps in natural image stitching},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind deblurring of text images using a text-specific hybrid
dictionary. <em>TIP</em>, <em>29</em>, 710–723. (<a
href="https://doi.org/10.1109/TIP.2019.2933739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a blind text image deblurring algorithm by using a text-specific hybrid dictionary. After careful analysis, we find that the text-specific hybrid dictionary has the great ability of providing powerful contextual information for text image deblurring. Here, it is worth noting that our proposed method is inspired by our observation that an intermediate latent image contains not only sharp regions, but also multiple types of small blurred regions. Based upon our discovery, we propose a prior for text images based on sparse representation, which models the relationship between an intermediate latent image and a desired sharp image. To this end, we carefully collect three different image patch pairs, which are 1) Gaussian blur-sharp, 2) motion blur-sharp, and 3) sharp-sharp, in order to construct the text-specific hybrid dictionary. We also propose a new optimization framework suitable for the task of text image deblurring in this paper. Extensive experiments have been conducted on a challenging dataset of synthetic and real-world text images. Our results demonstrate that the proposed method outperforms the state-of-the-art image deblurring methods both quantitatively and qualitatively.},
  archive      = {J_TIP},
  author       = {Hyukzae Lee and Chanho Jung and Changick Kim},
  doi          = {10.1109/TIP.2019.2933739},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {710-723},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind deblurring of text images using a text-specific hybrid dictionary},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Re-caption: Saliency-enhanced image captioning through
two-phase learning. <em>TIP</em>, <em>29</em>, 694–709. (<a
href="https://doi.org/10.1109/TIP.2019.2928144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual saliency and semantic saliency are important in image captioning. However, a single-phase image captioning model benefits little from limited saliency information without a saliency predictor. In this paper, a novel saliency-enhanced re-captioning framework via two-phase learning is proposed to enhance single-phase image captioning. In the framework, both visual and semantic saliency cues are distilled from the first-phase model and fused with the second-phase model for model self-boosting. The visual saliency mechanism can generate a saliency map and a saliency mask for an image without learning a saliency predictor. The semantic saliency mechanism sheds some lights on the properties of those words with the part-of-speech Noun in a caption. Besides, another type of saliency, sample saliency is proposed to compute the saliency degree of each sample, which is helpful for more robust image captioning. In addition, how to combine the three types of saliency for further performance boost is also examined. Our framework can treat an image captioning model as a saliency extractor, which may benefit other captioning models and the related tasks. The experimental results on both the Flickr30k and MSCOCO datasets show that the saliency-enhanced models can obtain promising performance gains.},
  archive      = {J_TIP},
  author       = {Lian Zhou and Yuejie Zhang and Yu-Gang Jiang and Tao Zhang and Weiguo Fan},
  doi          = {10.1109/TIP.2019.2928144},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {694-709},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Re-caption: Saliency-enhanced image captioning through two-phase learning},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Context-adaptive neural network-based prediction for image
compression. <em>TIP</em>, <em>29</em>, 679–693. (<a
href="https://doi.org/10.1109/TIP.2019.2934565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a set of neural network architectures, called Prediction Neural Networks Set (PNNS), based on both fully-connected and convolutional neural networks, for intra image prediction. The choice of neural network for predicting a given image block depends on the block size, hence does not need to be signalled to the decoder. It is shown that, while fully-connected neural networks give good performance for small block sizes, convolutional neural networks provide better predictions in large blocks with complex textures. Thanks to the use of masks of random sizes during training, the neural networks of PNNS well adapt to the available context that may vary, depending on the position of the image block to be predicted. When integrating PNNS into a H.265 codec, PSNR-rate performance gains going from 1.46\% to 5.20\% are obtained. These gains are on average 0.99\% larger than those of prior neural network based methods. Unlike the H.265 intra prediction modes, which are each specialized in predicting a specific texture, the proposed PNNS can model a large set of complex textures.},
  archive      = {J_TIP},
  author       = {Thierry Dumas and Aline Roumy and Christine Guillemot},
  doi          = {10.1109/TIP.2019.2934565},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {679-693},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Context-adaptive neural network-based prediction for image compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressive color pattern detection using partial orthogonal
circulant sensing matrix. <em>TIP</em>, <em>29</em>, 670–678. (<a
href="https://doi.org/10.1109/TIP.2019.2927334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One key issue in compressive sensing is to design a sensing matrix that is random enough to have a good signal reconstruction quality and that also enjoys some desirable properties, such that orthogonality or being circulant. The classic method to construct such sensing matrices is to, first, generate a full orthogonal circulant matrix and, then, select only a few rows. In this paper, we propose a refined construction of orthogonal circulant sensing matrices that generates a circulant matrix, where only a given subset of its rows are orthogonal. That way, the generation method is a lot less constrained leading to better sensing matrices, and we still have the desired properties. The proposed partial shift-orthogonal sensing matrix is compared to random and learned sensing matrices in the frame of signal reconstruction. This sensing matrix is pattern-dependent and, thus, efficient to detect color patterns and edges from the measurements of a color image.},
  archive      = {J_TIP},
  author       = {Sylvain Rousseau and David Helbert},
  doi          = {10.1109/TIP.2019.2927334},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {670-678},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Compressive color pattern detection using partial orthogonal circulant sensing matrix},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task deep relative attribute learning for visual urban
perception. <em>TIP</em>, <em>29</em>, 657–669. (<a
href="https://doi.org/10.1109/TIP.2019.2932502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual urban perception aims to quantify perceptual attributes (e.g., safe and depressing attributes) of physical urban environment from crowd-sourced street-view images and their pairwise comparisons. It has been receiving more and more attention in computer vision for various applications, such as perceptive attribute learning and urban scene understanding. Most existing methods adopt either 1) a regression model trained using image features and ranked scores converted from pairwise comparisons for perceptual attribute prediction or 2) a pairwise ranking algorithm to independently learn each perceptual attribute. However, the former fails to directly exploit pairwise comparisons while the latter ignores the relationship among different attributes. To address them, we propose a multi-task deep relative attribute learning network (MTDRALN) to learn all the relative attributes simultaneously via multi-task Siamese networks, where each Siamese network will predict one relative attribute. Combined with deep relative attribute learning, we utilize the structured sparsity to exploit the prior from natural attribute grouping, where all the attributes are divided into different groups based on semantic relatedness in advance. As a result, MTDRALN is capable of learning all the perceptual attributes simultaneously via multi-task learning. Besides the ranking sub-network, MTDRALN further introduces the classification sub-network, and these two types of losses from two sub-networks jointly constrain parameters of the deep network to make the network learn more discriminative visual features for relative attribute learning. In addition, our network can be trained in an end-to-end way to make deep feature learning and multi-task relative attribute learning reinforces each other. Extensive experiments on the large-scale Place Pulse 2.0 dataset validate the advantage of our proposed network. Our qualitative results along with visualization of saliency maps also show that the proposed network is able to learn effective features for perceptual attributes.},
  archive      = {J_TIP},
  author       = {Weiqing Min and Shuhuan Mei and Linhu Liu and Yi Wang and Shuqiang Jiang},
  doi          = {10.1109/TIP.2019.2932502},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {657-669},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-task deep relative attribute learning for visual urban perception},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning-based picture-wise just noticeable distortion
prediction model for image compression. <em>TIP</em>, <em>29</em>,
641–656. (<a href="https://doi.org/10.1109/TIP.2019.2933743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Picture Wise Just Noticeable Difference (PW-JND), which accounts for the minimum difference of a picture that human visual system can perceive, can be widely used in perception-oriented image and video processing. However, the conventional Just Noticeable Difference (JND) models calculate the JND threshold for each pixel or sub-band separately, which may not reflect the total masking effect of a picture accurately. In this paper, we propose a deep learning based PW-JND prediction model for image compression. Firstly, we formulate the task of predicting PW-JND as a multi-class classification problem, and propose a framework to transform the multi-class classification problem to a binary classification problem solved by just one binary classifier. Secondly, we construct a deep learning based binary classifier named perceptually lossy/lossless predictor which can predict whether an image is perceptually lossy to another or not. Finally, we propose a sliding window based search strategy to predict PW-JND based on the prediction results of the perceptually lossy/lossless predictor. Experimental results show that the mean accuracy of the perceptually lossy/lossless predictor reaches 92\%, and the absolute prediction error of the proposed PW-JND model is 0.79 dB on average, which show the superiority of the proposed PW-JND model to the conventional JND models.},
  archive      = {J_TIP},
  author       = {Huanhua Liu and Yun Zhang and Huan Zhang and Chunling Fan and Sam Kwong and C.-C. Jay Kuo and Xiaoping Fan},
  doi          = {10.1109/TIP.2019.2933743},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {641-656},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep learning-based picture-wise just noticeable distortion prediction model for image compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RYF-net: Deep fusion network for single image haze removal.
<em>TIP</em>, <em>29</em>, 628–640. (<a
href="https://doi.org/10.1109/TIP.2019.2934360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze removal from a single image is a challenging task. Estimation of accurate scene transmission map (TrMap) is the key to reconstruct the haze-free scene. In this paper, we propose a convolutional neural network based architecture to estimate the TrMap of the hazy scene. The proposed network takes the hazy image as an input and extracts the haze relevant features using proposed RNet and YNet through RGB and YCbCr color spaces respectively and generates two TrMaps. Further, we propose a novel TrMap fusion network (FNet) to integrate two TrMaPs and estimate robust TrMap for the hazy scene. To analyze the robustness of FNet, we tested it on combinations of TrMaps obtained from existing state-of-the-art methods. Performance evaluation of the proposed approach has been carried out using the structural similarity index, mean square error and peak signal to noise ratio. We conduct experiments on five datasets namely: D-HAZY, Imagenet, Indoor SOTS, HazeRD and set of real-world hazy images. Performance analysis shows that the proposed approach outperforms the existing state-of-the-art methods for single image dehazing. Further, we extended our work to address high-level vision task such as object detection in hazy scenes. It is observed that there is a significant improvement in accurate object detection in hazy scenes using proposed approach.},
  archive      = {J_TIP},
  author       = {Akshay Dudhane and Subrahmanyam Murala},
  doi          = {10.1109/TIP.2019.2934360},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {628-640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RYF-net: Deep fusion network for single image haze removal},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view image classification with visual, semantic and
view consistency. <em>TIP</em>, <em>29</em>, 617–627. (<a
href="https://doi.org/10.1109/TIP.2019.2934576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view visual classification methods have been widely applied to use discriminative information of different views. This strategy has been proven very effective by many researchers. On the one hand, images are often treated independently without fully considering their visual and semantic correlations. On the other hand, view consistency is often ignored. To solve these problems, in this paper, we propose a novel multi-view image classification method with visual, semantic and view consistency (VSVC). For each image, we linearly combine multi-view information for image classification. The combination parameters are determined by considering both the classification loss and the visual, semantic and view consistency. Visual consistency is imposed by ensuring that visually similar images of the same view are predicted to have similar values. For semantic consistency, we impose the locality constraint that nearby images should be predicted to have the same class by multi-view combination. View consistency is also used to ensure that similar images have consistent multi-view combination parameters. An alternative optimization strategy is used to learn the combination parameters. To evaluate the effectiveness of VSVC, we perform image classification experiments on several public datasets. The experimental results on these datasets show the effectiveness of the proposed VSVC method.},
  archive      = {J_TIP},
  author       = {Chunjie Zhang and Jian Cheng and Qi Tian},
  doi          = {10.1109/TIP.2019.2934576},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {617-627},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view image classification with visual, semantic and view consistency},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometry-aware graph transforms for light field compact
representation. <em>TIP</em>, <em>29</em>, 602–616. (<a
href="https://doi.org/10.1109/TIP.2019.2928873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of energy compaction of dense 4D light fields by designing geometry-aware local graph-based transforms. Local graphs are constructed on super-rays that can be seen as a grouping of spatially and geometry-dependent angularly correlated pixels. Both non-separable and separable transforms are considered. Despite the local support of limited size defined by the super-rays, the Laplacian matrix of the non-separable graph remains of high dimension and its diagonalization to compute the transform eigenvectors remains computationally expensive. To solve this problem, we then perform the local spatio-angular transform in a separable manner. We show that when the shape of corresponding super-pixels in the different views is not isometric, the basis functions of the spatial transforms are not coherent, resulting in a decreased correlation between spatial transform coefficients. We hence propose a novel transform optimization method that aims at preserving angular correlation even when the shapes of the super-pixels are not isometric. Experimental results show the benefit of the approach in terms of energy compaction. A coding scheme is also described to assess the rate-distortion performances of the proposed transforms and is compared to several state-of-the-art encoders.},
  archive      = {J_TIP},
  author       = {Mira Rizkallah and Xin Su and Thomas Maugey and Christine Guillemot},
  doi          = {10.1109/TIP.2019.2928873},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {602-616},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometry-aware graph transforms for light field compact representation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unambiguous scene text segmentation with referring
expression comprehension. <em>TIP</em>, <em>29</em>, 591–601. (<a
href="https://doi.org/10.1109/TIP.2019.2930176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text instance provides valuable information for the understanding and interpretation of natural scenes. The rich precise high-level semantics embodied in the text could be beneficial for understanding the world around us, and empower a wide range of real-world applications. While most recent visual phrase grounding approaches focus on general objects, this paper explores extracting designated texts and predicting unambiguous scene text segmentation mask, i.e., scene text segmentation from natural language descriptions (referring expressions) like orange text on a little boy in black swinging a bat. The solution of this novel problem enables accurate segmentation of scene text instances from the complex background. In our proposed framework, a unified deep network jointly models visual and linguistic information by encoding both region-level and pixel-level visual features of natural scene images into spatial feature maps, and then decode them into saliency response map of text instances. To conduct quantitative evaluations, we establish a new scene text referring expression segmentation dataset: COCO-CharRef. Experimental results demonstrate the effectiveness of the proposed framework on the text instance segmentation task. By combining image-based visual features with language-based textual explanations, our framework outperforms baselines that are derived from state-of-the-art text localization and natural language object retrieval methods on COCO-CharRef dataset.},
  archive      = {J_TIP},
  author       = {Xuejian Rong and Chucai Yi and Yingli Tian},
  doi          = {10.1109/TIP.2019.2930176},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {591-601},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unambiguous scene text segmentation with referring expression comprehension},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning modality-specific representations for
visible-infrared person re-identification. <em>TIP</em>, <em>29</em>,
579–590. (<a href="https://doi.org/10.1109/TIP.2019.2928126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional person re-identification (re-id) methods perform poorly under changing illuminations. This situation can be addressed by using dual-cameras that capture visible images in a bright environment and infrared images in a dark environment. Yet, this scheme needs to solve the visible-infrared matching issue, which is largely under-studied. Matching pedestrians across heterogeneous modalities is extremely challenging because of different visual characteristics. In this paper, we propose a novel framework that employs modality-specific networks to tackle with the heterogeneous matching problem. The proposed framework utilizes the modality-related information and extracts modality-specific representations (MSR) by constructing an individual network for each modality. In addition, a cross-modality Euclidean constraint is introduced to narrow the gap between different networks. We also integrate the modality-shared layers into modality-specific networks to extract shareable information and use a modality-shared identity loss to facilitate the extraction of modality-invariant features. Then a modality-specific discriminant metric is learned for each domain to strengthen the discriminative power of MSR. Eventually, we use a view classifier to learn view information. The experiments demonstrate that the MSR effectively improves the performance of deep networks on VI-REID and remarkably outperforms the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Zhanxiang Feng and Jianhuang Lai and Xiaohua Xie},
  doi          = {10.1109/TIP.2019.2928126},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {579-590},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning modality-specific representations for visible-infrared person re-identification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral image denoising via matrix factorization and
deep prior regularization. <em>TIP</em>, <em>29</em>, 565–578. (<a
href="https://doi.org/10.1109/TIP.2019.2928627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been successfully introduced for 2D-image denoising, but it is still unsatisfactory for hyperspectral image (HSI) denoising due to the unacceptable computational complexity of the end-to-end training process and the difficulty of building a universal 3D-image training dataset. In this paper, instead of developing an end-to-end deep learning denoising network, we propose an HSI denoising framework for the removal of mixed Gaussian impulse noise, in which the denoising problem is modeled as a convolutional neural network (CNN) constrained non-negative matrix factorization problem. Using the proximal alternating linearized minimization, the optimization can be divided into three steps: the update of the spectral matrix, the update of the abundance matrix, and the estimation of the sparse noise. Then, we design the CNN architecture and proposed two training schemes, which can allow the CNN to be trained with a 2D-image dataset. Compared with the state-of-the-art denoising methods, the proposed method has a relatively good performance on the removal of the Gaussian and mixed Gaussian impulse noises. More importantly, the proposed model can be only trained once by a 2D-image dataset but can be used to denoise HSIs with different numbers of channel bands.},
  archive      = {J_TIP},
  author       = {Baihong Lin and Xiaoming Tao and Jianhua Lu},
  doi          = {10.1109/TIP.2019.2928627},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {565-578},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral image denoising via matrix factorization and deep prior regularization},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Summit navigator: A novel approach for local maxima
extraction. <em>TIP</em>, <em>29</em>, 551–564. (<a
href="https://doi.org/10.1109/TIP.2019.2932501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method, called the Summit Navigator, to effectively extract local maxima of an image histogram for multi-object segmentation of images. After smoothing with a moving average filter, the obtained histogram is analyzed, based on the data density and distribution to find the best observing location. An observability index for each initial peak is proposed to evaluate if it can be considered as dominant by using the calculated observing location. Recursive algorithms are then developed for peak searching and merging to remove any false detection of peaks that are located on one side of each mode. Experimental results demonstrated the advantages of the proposed approach in terms of accuracy and consistency in different reputable datasets.},
  archive      = {J_TIP},
  author       = {Tran Hiep Dinh and Manh Duong Phung and Quang Phuc Ha},
  doi          = {10.1109/TIP.2019.2932501},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {551-564},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Summit navigator: A novel approach for local maxima extraction},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Semi-supervised deep coupled ensemble learning with
classification landmark exploration. <em>TIP</em>, <em>29</em>, 538–550.
(<a href="https://doi.org/10.1109/TIP.2019.2933724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using an ensemble of neural networks with consistency regularization is effective for improving performance and stability of deep learning, compared to the case of a single network. In this paper, we present a semi-supervised Deep Coupled Ensemble (DCE) model, which contributes to ensemble learning and classification landmark exploration for better locating the final decision boundaries in the learnt latent space. First, multiple complementary consistency regularizations are integrated into our DCE model to enable the ensemble members to learn from each other and themselves, such that training experience from different sources can be shared and utilized during training. Second, in view of the possibility of producing incorrect predictions on a number of difficult instances, we adopt class-wise mean feature matching to explore important unlabeled instances as classification landmarks, on which the model predictions are more reliable. Minimizing the weighted conditional entropy on unlabeled data is able to force the final decision boundaries to move away from important training data points, which facilitates semi-supervised learning. Ensemble members could eventually have similar performance due to consistency regularization, and thus only one of these members is needed during the test stage, such that the efficiency of our model is the same as the non-ensemble case. Extensive experimental results demonstrate the superiority of our proposed DCE model over existing state-of-the-art semi-supervised learning methods.},
  archive      = {J_TIP},
  author       = {Jichang Li and Si Wu and Cheng Liu and Zhiwen Yu and Hau-San Wong},
  doi          = {10.1109/TIP.2019.2933724},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {538-550},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised deep coupled ensemble learning with classification landmark exploration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image representations with spatial object-to-object
relations for RGB-d scene recognition. <em>TIP</em>, <em>29</em>,
525–537. (<a href="https://doi.org/10.1109/TIP.2019.2933728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene recognition is challenging due to the intra-class diversity and inter-class similarity. Previous works recognize scenes either with global representations or with the intermediate representations of objects. In contrast, we investigate more discriminative image representations of object-to-object relations for scene recognition, which are based on the triplets of obtained with detection techniques. Particularly, two types of representations, including co-occurring frequency of object-to-object relation (denoted as COOR) and sequential representation of object-to-object relation (denoted as SOOR), are proposed to describe objects and their relative relations in different forms. COOR is represented as the intermediate representation of co-occurring frequency of objects and their relations, with a three order tensor that can be fed to scene classifier without further embedding. SOOR is represented in a more explicit and freer form that sequentially describe image contents with local captions. And a sequence encoding model (e.g., recurrent neural network (RNN)) is implemented to encode SOOR to the features for feeding the classifiers. In order to better capture the spatial information, the proposed COOR and SOOR are adapted to RGB-D data, where a RGB-D proposal fusion method is proposed for RGB-D object detection. With the proposed approaches COOR and SOOR, we obtain the state-of-the-art results of RGB-D scene recognition on SUN RGB-D and NYUD2 datasets.},
  archive      = {J_TIP},
  author       = {Xinhang Song and Shuqiang Jiang and Bohan Wang and Chengpeng Chen and Gongwei Chen},
  doi          = {10.1109/TIP.2019.2933728},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {525-537},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image representations with spatial object-to-object relations for RGB-D scene recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse representation-based video quality assessment for
synthesized 3D videos. <em>TIP</em>, <em>29</em>, 509–524. (<a
href="https://doi.org/10.1109/TIP.2019.2929433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The temporal flicker distortion is one of the most annoying noises in synthesized virtual view videos when they are rendered by compressed multi-view video plus depth in Three Dimensional (3D) video system. To assess the synthesized view video quality and further optimize the compression techniques in 3D video system, objective video quality assessment which can accurately measure the flicker distortion is highly needed. In this paper, we propose a full reference sparse representation-based video quality assessment method toward synthesized 3D videos. First, a synthesized video, treated as a 3D volume data with spatial (X-Y) and temporal (T) domains, is reformed and decomposed as a number of spatially neighboring temporal layers, i.e., X-T or Y-T planes. Gradient features in temporal layers of the synthesized video and strong edges of depth maps are used as key features in detecting the location of flicker distortions. Second, the dictionary learning and sparse representation for the temporal layers are then derived and applied to effectively represent the temporal flicker distortion. Third, a rank pooling method is used to pool all the temporal layer scores and obtain the score for the flicker distortion. Finally, the temporal flicker distortion measurement is combined with the conventional spatial distortion measurement to assess the quality of synthesized 3D videos. Experimental results on synthesized video quality database demonstrate our proposed method is significantly superior to the other state-of-the-art methods, especially on the view synthesis distortions induced from depth videos.},
  archive      = {J_TIP},
  author       = {Yun Zhang and Huan Zhang and Mei Yu and Sam Kwong and Yo-Sung Ho},
  doi          = {10.1109/TIP.2019.2929433},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {509-524},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sparse representation-based video quality assessment for synthesized 3D videos},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted guided image filtering with steering kernel.
<em>TIP</em>, <em>29</em>, 500–508. (<a
href="https://doi.org/10.1109/TIP.2019.2928631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its local property, guided image filter (GIF) generally suffers from halo artifacts near edges. To make up for the deficiency, a weighted guided image filter (WGIF) was proposed recently by incorporating an edge-aware weighting into the filtering process. It takes the advantages of local and global operations, and achieves better performance in edge-preserving. However, edge direction, a vital property of the guidance image, is not considered fully in these guided filters. In order to overcome the drawback, we propose a novel version of GIF, which can leverage the edge direction more sufficiently. In particular, we utilize the steering kernel to adaptively learn the direction and incorporate the learning results into the filtering process to improve the filter&#39;s behavior. Theoretical analysis shows that the proposed method can get more powerful performance with preserving edges and reducing halo artifacts effectively. Similar conclusions are also reached through the thorough experiments including edge-aware smoothing, detail enhancement, denoising, and dehazing.},
  archive      = {J_TIP},
  author       = {Zhonggui Sun and Bo Han and Jie Li and Jin Zhang and Xinbo Gao},
  doi          = {10.1109/TIP.2019.2928631},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {500-508},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weighted guided image filtering with steering kernel},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Holistic multi-modal memory network for movie question
answering. <em>TIP</em>, <em>29</em>, 489–499. (<a
href="https://doi.org/10.1109/TIP.2019.2931534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answering questions using multi-modal context is a challenging problem, as it requires a deep integration of diverse data sources. Existing approaches only consider a subset of all possible interactions among data sources during one attention hop. In this paper, we present a holistic multi-modal memory network (HMMN) framework that fully considers interactions between different input sources (multi-modal context and question) at each hop. In addition, to hone in on relevant information, our framework takes answer choices into consideration during the context retrieval stage. Our HMMN framework effectively integrates information from the multi-modal context, question, and answer choices, enabling more informative context to be retrieved for question answering. Experimental results on the Movie QA and TVQA datasets validate the effectiveness of our HMMN framework. Extensive ablation studies show the importance of holistic reasoning and reveal the contributions of different attention strategies to model performance.},
  archive      = {J_TIP},
  author       = {Anran Wang and Anh Tuan Luu and Chuan-Sheng Foo and Hongyuan Zhu and Yi Tay and Vijay Chandrasekhar},
  doi          = {10.1109/TIP.2019.2931534},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {489-499},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Holistic multi-modal memory network for movie question answering},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning rich part hierarchies with progressive attention
networks for fine-grained image recognition. <em>TIP</em>, <em>29</em>,
476–488. (<a href="https://doi.org/10.1109/TIP.2019.2921876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the localization of subtle yet discriminative parts for fine-grained image recognition. Based on the observation that such parts typically exist within a hierarchical structure (e.g., from a coarse-scale “head” to a fine-scale “eye” when recognizing bird species), we propose a novel progressive-attention convolutional neural network (PA-CNN) to progressively localize parts at multiple scales. The PA-CNN localizes parts in two steps, where a part proposal network (PPN) generates multiple local attention maps, and a part rectification network (PRN) learns part-specific features from each proposal and provides the PPN with refined part locations. This coupling of the PPN and PRN allows them to be optimized in a mutually reinforcing manner, leading to improved pinpointing of fine-grained parts. Moreover, the convolutional parameters for a PPN at a finer scale can be inherited from the PRN at a coarser scale, enabling a rich part hierarchy (e.g., eye and beak in a bird&#39;s head) to be learned in a stacked fashion. Case studies show that PA-CNN can precisely identify parts without using bounding box/part annotations. In addition, quantitative evaluations demonstrate that PA-CNN yields state-of-the-art performance in three challenging fine-grained recognition tasks. i.e., CUB-2000-2011, FGVC-Aircraft, and Stanford Cars.},
  archive      = {J_TIP},
  author       = {Heliang Zheng and Jianlong Fu and Zheng-Jun Zha and Jiebo Luo and Tao Mei},
  doi          = {10.1109/TIP.2019.2921876},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {476-488},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning rich part hierarchies with progressive attention networks for fine-grained image recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-resolution encoder–decoder networks for low-contrast
medical image segmentation. <em>TIP</em>, <em>29</em>, 461–475. (<a
href="https://doi.org/10.1109/TIP.2019.2919937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image segmentation is an essential step for many medical image analysis applications, include computer-aided radiation therapy, disease diagnosis, and treatment effect evaluation. One of the major challenges for this task is the blurry nature of medical images (e.g., CT, MR, and microscopic images), which can often result in low-contrast and vanishing boundaries. With the recent advances in convolutional neural networks, vast improvements have been made for image segmentation, mainly based on the skip-connection-linked encoder-decoder deep architectures. However, in many applications (with adjacent targets in blurry images), these models often fail to accurately locate complex boundaries and properly segment tiny isolated parts. In this paper, we aim to provide a method for blurry medical image segmentation and argue that skip connections are not enough to help accurately locate indistinct boundaries. Accordingly, we propose a novel high-resolution multi-scale encoder-decoder network (HMEDN), in which multi-scale dense connections are introduced for the encoder-decoder structure to finely exploit comprehensive semantic information. Besides skip connections, extra deeply supervised high-resolution pathways (comprised of densely connected dilated convolutions) are integrated to collect high-resolution semantic information for accurate boundary localization. These pathways are paired with a difficulty-guided cross-entropy loss function and a contour regression task to enhance the quality of boundary detection. The extensive experiments on a pelvic CT image dataset, a multi-modal brain tumor dataset, and a cell segmentation dataset show the effectiveness of our method for 2D/3D semantic segmentation and 2D instance segmentation, respectively. Our experimental results also show that besides increasing the network complexity, raising the resolution of semantic feature maps can largely affect the overall model performance. For different tasks, finding a balance between these two factors can further improve the performance of the corresponding network.},
  archive      = {J_TIP},
  author       = {Sihang Zhou and Dong Nie and Ehsan Adeli and Jianping Yin and Jun Lian and Dinggang Shen},
  doi          = {10.1109/TIP.2019.2919937},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {461-475},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-resolution Encoder–Decoder networks for low-contrast medical image segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced fuzzy-based local information algorithm for sonar
image segmentation. <em>TIP</em>, <em>29</em>, 445–460. (<a
href="https://doi.org/10.1109/TIP.2019.2930148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent boost in undersea operations has led to the development of high-resolution sonar systems mounted on autonomous vehicles. These vehicles are used to scan the seafloor in search of different objects such as sunken ships, archaeological sites, and submerged mines. An important part of the detection operation is the segmentation of sonar images, where the object&#39;s highlight and shadow are distinguished from the seabed background. In this paper, we focus on the automatic segmentation of sonar images. We present our enhanced fuzzy-based with Kernel metric (EnFK) algorithm for the segmentation of sonar images which, in an attempt to improve segmentation accuracy, introduces two new fuzzy terms of local spatial and statistical information. Our algorithm includes a preliminary de-noising algorithm which, together with the original image, feeds into the segmentation procedure to avoid trapping to local minima and to improve convergence. The result is a segmentation procedure that specifically suits the intensity inhomogeneity and the complex seabed texture of sonar images. We tested our approach using simulated images, real sonar images, and sonar images that were created in two different sea experiments, using multibeam sonar and synthetic aperture sonar. The results show accurate segmentation performance that is far beyond the state-of-the-art results.},
  archive      = {J_TIP},
  author       = {Avi Abu and Roee Diamant},
  doi          = {10.1109/TIP.2019.2930148},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {445-460},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhanced fuzzy-based local information algorithm for sonar image segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wavelet-based spectral–spatial transforms for CFA-sampled
raw camera image compression. <em>TIP</em>, <em>29</em>, 433–444. (<a
href="https://doi.org/10.1109/TIP.2019.2928124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral-spatial transforms (SSTs) change a raw camera image captured using a color filter array (CFA-sampled image) from an RGB color space composed of red, green, and blue components into a decorrelated color space, such as YDgCbCr or YDgCoCg color space composed of luma, difference green, and two chroma components. This paper describes three types of wavelet-based SST (WSST) obtained by reorganizing all of the existing SSTs covered in this paper. First, we introduce three types of macropixel SST (MSST) implemented within each 2 × 2 macropixel. Next, we focus on two-channel Haar wavelet transforms, which are simple wavelet transforms, and three-channel Haar-like wavelet transforms in each MSST and replace the Haar and Haar-like wavelet transforms with Cohen-Daubechies-Feauveau (CDF) 5/3 and 9/7 wavelet transforms, which are customized on the basis of the original pixel positions in 2D space. Although the test data set is not big, in lossless CFA-sampled image compression based on JPEG 2000, the WSSTs improve the bitrates by about 1.67\%-3.17\% compared with not using a transform, and the WSSTs that use 5/3 wavelet transforms improve the bitrates by about 0.31\%-0.71\% compared with the best existing SST. Moreover, in lossy CFA-sampled image compression based on JPEG 2000, the WSSTs show about 2.25-4.40 dB and 26.04\%-49.35\% in the Bjøntegaard metrics (BD-PSNRs and BD-rates) compared with not using a transform, and the WSSTs that use 9/7 wavelet transforms improve the metrics by about 0.13-0.40 dB and 2.27\%-4.80\% compared with the best existing SST.},
  archive      = {J_TIP},
  author       = {Taizo Suzuki},
  doi          = {10.1109/TIP.2019.2928124},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {433-444},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Wavelet-based Spectral–Spatial transforms for CFA-sampled raw camera image compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph transform optimization with application to image
compression. <em>TIP</em>, <em>29</em>, 419–432. (<a
href="https://doi.org/10.1109/TIP.2019.2932853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new graph-based transform and illustrate its potential application to signal compression. Our approach relies on the careful design of a graph that optimizes the overall rate-distortion performance through an effective graph-based transform. We introduce a novel graph estimation algorithm, which uncovers the connectivities between the graph signal values by taking into consideration the coding of both the signal and the graph topology in rate-distortion terms. In particular, we introduce a novel coding solution for the graph by treating the edge weights as another graph signal that lies on the dual graph. Then, the cost of the graph description is introduced in the optimization problem by minimizing the sparsity of the coefficients of its graph Fourier transform (GFT) on the dual graph. In this way, we obtain a convex optimization problem whose solution defines an efficient transform coding strategy. The proposed technique is a general framework that can be applied to different types of signals, and we show two possible application fields, namely natural image coding and piecewise smooth image coding. Experimental results show that the proposed graph-based transform outperforms classical fixed transforms, such as DCT for both natural and piecewise smooth images. In the case of depth map coding, the obtained results are even comparable to the state-of-the-art graph-based coding method that is specifically designed for depth map images.},
  archive      = {J_TIP},
  author       = {Giulia Fracastoro and Dorina Thanou and Pascal Frossard},
  doi          = {10.1109/TIP.2019.2932853},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {419-432},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph transform optimization with application to image compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised deep sparse coding networks for image
classification. <em>TIP</em>, <em>29</em>, 405–418. (<a
href="https://doi.org/10.1109/TIP.2019.2928121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel deep sparse coding network (SCN) capable of efficiently adapting its own regularization parameters for a given application. The network is trained end-to-end with a supervised task-driven learning algorithm via error backpropagation. During training, the network learns both the dictionaries and the regularization parameters of each sparse coding layer so that the reconstructive dictionaries are smoothly transformed into increasingly discriminative representations. In addition, the adaptive regularization also offers the network more flexibility to adjust sparsity levels. Furthermore, we have devised a sparse coding layer utilizing a “skinny” dictionary. Integral to computational efficiency, these skinny dictionaries compress the high-dimensional sparse codes into lower dimensional structures. The adaptivity and discriminability of our 15-layer SCN are demonstrated on six benchmark datasets, namely Cifar-10, Cifar-100, STL-10, SVHN, MNIST, and ImageNet, most of which are considered difficult for sparse coding models. Experimental results show that our architecture overwhelmingly outperforms traditional one-layer sparse coding architectures while using much fewer parameters. Moreover, our multilayer architecture exploits the benefits of depth with sparse coding&#39;s characteristic ability to operate on smaller datasets. In such data-constrained scenarios, our technique demonstrates a highly competitive performance compared with the deep neural networks.},
  archive      = {J_TIP},
  author       = {Xiaoxia Sun and Nasser M. Nasrabadi and Trac D. Tran},
  doi          = {10.1109/TIP.2019.2928121},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {405-418},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Supervised deep sparse coding networks for image classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exemplar-based recursive instance segmentation with
application to plant image analysis. <em>TIP</em>, <em>29</em>, 389–404.
(<a href="https://doi.org/10.1109/TIP.2019.2923571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation is a challenging computer vision problem which lies at the intersection of object detection and semantic segmentation. Motivated by plant image analysis in the context of plant phenotyping, a recently emerging application field of computer vision, this paper presents the exemplar-based recursive instance segmentation (ERIS) framework. A three-layer probabilistic model is first introduced to jointly represent hypotheses, voting elements, instance labels, and their connections. Afterward, a recursive optimization algorithm is developed to infer the maximum a posteriori (MAP) solution, which handles one instance at a time by alternating among the three steps of detection, segmentation, and update. The proposed ERIS framework departs from previous works mainly in two respects. First, it is exemplar-based and model-free, which can achieve instance-level segmentation of a specific object class given only a handful of (typically less than 10) annotated exemplars. Such a merit enables its use in case that no massive manually-labeled data is available for training strong classification models, as required by most existing methods. Second, instead of attempting to infer the solution in a single shot, which suffers from extremely high computational complexity, our recursive optimization strategy allows for reasonably efficient MAP-inference in full hypothesis space. The ERIS framework is substantialized for the specific application of plant leaf segmentation in this work. Experiments are conducted on public benchmarks to demonstrate the superiority of our method in both effectiveness and efficiency in comparison with the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Jin-Gang Yu and Yansheng Li and Changxin Gao and Hongxia Gao and Gui-Song Xia and Zhu Liang Yu and Yuanqing Li},
  doi          = {10.1109/TIP.2019.2923571},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {389-404},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exemplar-based recursive instance segmentation with application to plant image analysis},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image compressed sensing using convolutional neural network.
<em>TIP</em>, <em>29</em>, 375–388. (<a
href="https://doi.org/10.1109/TIP.2019.2928136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of compressed sensing (CS), the two main challenges are the design of sampling matrix and the development of reconstruction method. On the one hand, the usually used random sampling matrices (e.g., GRM) are signal independent, which ignore the characteristics of the signal. On the other hand, the state-of-the-art image CS methods (e.g., GSR and MH) achieve quite good performance, however with much higher computational complexity. To deal with the two challenges, we propose an image CS framework using convolutional neural network (dubbed CSNet) that includes a sampling network and a reconstruction network, which are optimized jointly. The sampling network adaptively learns the sampling matrix from the training images, which makes the CS measurements retain more image structural information for better reconstruction. Specifically, three types of sampling matrices are learned, i.e., floating-point matrix, {0, 1}-binary matrix, and {-1, +1}-bipolar matrix. The last two matrices are specially designed for easy storage and hardware implementation. The reconstruction network, which contains a linear initial reconstruction network and a non-linear deep reconstruction network, learns an end-to-end mapping between the CS measurements and the reconstructed images. Experimental results demonstrate that CSNet offers state-of-the-art reconstruction quality, while achieving fast running speed. In addition, CSNet with {0, 1}-binary matrix, and {-1, +1}-bipolar matrix gets comparable performance with the existing deep learning-based CS methods, outperforms the traditional CS methods. Experimental results further suggest that the learned sampling matrices can improve the traditional image CS reconstruction methods significantly.},
  archive      = {J_TIP},
  author       = {Wuzhen Shi and Feng Jiang and Shaohui Liu and Debin Zhao},
  doi          = {10.1109/TIP.2019.2928136},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {375-388},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image compressed sensing using convolutional neural network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep salient object detection with contextual information
guidance. <em>TIP</em>, <em>29</em>, 360–374. (<a
href="https://doi.org/10.1109/TIP.2019.2930906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integration of multi-level contextual information, such as feature maps and side outputs, is crucial for Convolutional Neural Networks (CNNs)-based salient object detection. However, most existing methods either simply concatenate multi-level feature maps or calculate element-wise addition of multi-level side outputs, thus failing to take full advantages of them. In this paper, we propose a new strategy for guiding multi-level contextual information integration, where feature maps and side outputs across layers are fully engaged. Specifically, shallower-level feature maps are guided by the deeper-level side outputs to learn more accurate properties of the salient object. In turn, the deeper-level side outputs can be propagated to high-resolution versions with spatial details complemented by means of shallower-level feature maps. Moreover, a group convolution module is proposed with the aim to achieve high-discriminative feature maps, in which the backbone feature maps are divided into a number of groups and then the convolution is applied to the channels of backbone feature maps within each group. Eventually, the group convolution module is incorporated in the guidance module to further promote the guidance role. Experiments on three public benchmark datasets verify the effectiveness and superiority of the proposed method over the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yi Liu and Jungong Han and Qiang Zhang and Caifeng Shan},
  doi          = {10.1109/TIP.2019.2930906},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {360-374},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep salient object detection with contextual information guidance},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inpainting versus denoising for dose reduction in
scanning-beam microscopies. <em>TIP</em>, <em>29</em>, 351–359. (<a
href="https://doi.org/10.1109/TIP.2019.2928133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider sampling strategies for reducing the radiation dose during image acquisition in scanning-beam microscopies, such as SEM, STEM, and STXM. Our basic assumption is that we may acquire subsampled image data (with some pixels missing) and then inpaint the missing data using a compressed-sensing approach. Our noise model consists of Poisson noise plus random Gaussian noise. We include the possibility of acquiring fully sampled image data, in which case the inpainting approach reduces to a denoising procedure. We use numerical simulations to compare the accuracy of reconstructed images with the “ground truths.” The results generally indicate that, for sufficiently high radiation doses, higher sampling rates achieve greater accuracy, commensurate with the well-established literature. However, for very low radiation doses, where the Poisson noise and/or random Gaussian noise begins to dominate, then our results indicate that subsampling/inpainting can result in smaller reconstruction errors. We also present an information-theoretic analysis, which allows us to quantify the amount of information gained through the different sampling strategies and enables some broader discussion of the main results.},
  archive      = {J_TIP},
  author       = {Toby Sanders and Christian Dwyer},
  doi          = {10.1109/TIP.2019.2928133},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {351-359},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Inpainting versus denoising for dose reduction in scanning-beam microscopies},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Morphology-based noise reduction: Structural variation and
thresholding in the bitonic filter. <em>TIP</em>, <em>29</em>, 336–350.
(<a href="https://doi.org/10.1109/TIP.2019.2932572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bitonic filter was recently developed to embody the novel concept of signal bitonicity (one local extremum within a set range) to differentiate from noise, by use of data ranking and linear operators. For processing images, the spatial extent was locally constrained to a fixed circular mask. Since structure in natural images varies, a novel structurally varying bitonic filter is presented, which locally adapts the mask, without following patterns in the noise. This new filter includes novel robust structurally varying morphological operations, with efficient implementations, and a novel formulation of non-iterative directional Gaussian filtering. Data thresholds are also integrated with the morphological operations, increasing noise reduction for low noise, and enabling a multi-resolution framework for high noise levels. The structurally varying bitonic filter is presented without presuming prior knowledge of morphological filtering, and compared to high-performance linear noise-reduction filters, to set this novel concept in context. These are tested over a wide range of noise levels, on a fairly broad set of images. The new filter is a considerable improvement on the fixed-mask bitonic, outperforms anisotropic diffusion and image-guided filtering in all but extremely low noise, non-local means at all noise levels, but not the block-matching 3D filter, though results are promising for very high noise. The structurally varying bitonic tends to have less characteristic residual noise in regions of smooth signal, and very good preservation of signal edges, though with some loss of small scale detail when compared to the block-matching 3D filter. The efficient implementation means that processing time, though slower than the fixed-mask bitonic filter, remains competitive.},
  archive      = {J_TIP},
  author       = {Graham Treece},
  doi          = {10.1109/TIP.2019.2932572},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {336-350},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Morphology-based noise reduction: Structural variation and thresholding in the bitonic filter},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HA-CCN: Hierarchical attention-based crowd counting network.
<em>TIP</em>, <em>29</em>, 323–335. (<a
href="https://doi.org/10.1109/TIP.2019.2928634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image-based crowd counting has recently witnessed increased focus, but many leading methods are far from optimal, especially in highly congested scenes. In this paper, we present the Hierarchical Attention-based Crowd Counting Network (HA-CCN) that employs attention mechanisms at various levels to selectively enhance the features of the network. The proposed method, which is based on the VGG16 network, consists of a spatial attention module (SAM) and a set of global attention modules (GAM). SAM enhances low-level features in the network by infusing spatial segmentation information, whereas the GAM focuses on enhancing channel-wise information in the higher level layers. The proposed method is a single-step training framework, simple to implement and achieves the state-of-the-art results on different datasets. Furthermore, we extend the proposed counting network by introducing a novel set-up to adapt the network to different scenes and datasets via weak supervision using image-level labels. This new set up reduces the burden of acquiring labor intensive point-wise annotations for new datasets while improving the cross-dataset performance.},
  archive      = {J_TIP},
  author       = {Vishwanath A. Sindagi and Vishal M. Patel},
  doi          = {10.1109/TIP.2019.2928634},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {323-335},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HA-CCN: Hierarchical attention-based crowd counting network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometry coding for dynamic voxelized point clouds using
octrees and multiple contexts. <em>TIP</em>, <em>29</em>, 313–322. (<a
href="https://doi.org/10.1109/TIP.2019.2931466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to compress geometry information of point clouds that explores redundancies across consecutive frames of a sequence. It uses octrees and works by progressively increasing the resolution of the octree. At each branch of the tree, we generate an approximation of the child nodes by a number of methods which are used as contexts to drive an arithmetic coder. The best approximation, i.e., the context that yields the least amount of encoding bits, is selected and the chosen method is indicated as side information for replication at the decoder. The core of our method is a context-based arithmetic coder in which a reference octree is used as a reference to encode the current octree, thus providing 255 contexts for each output octet. The 255 × 255 frequency histogram is viewed as a discrete 3D surface and is conveyed to the decoder using another octree. We present two methods to generate the predictions (contexts) which use adjacent frames in the sequence (inter-frame) and one method that works purely intra-frame. The encoder continuously switches the best mode among the three and conveys such information to the decoder. Since an intra-frame prediction is present, our coder can also work in purely intra-frame mode, as well. Extensive results are presented to show the method&#39;s potential against many compression alternatives for the geometry information in dynamic voxelized point clouds.},
  archive      = {J_TIP},
  author       = {Diogo C. Garcia and Tiago A. Fonseca and Renan U. Ferreira and Ricardo L. de Queiroz},
  doi          = {10.1109/TIP.2019.2931466},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {313-322},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometry coding for dynamic voxelized point clouds using octrees and multiple contexts},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural network regression for automated retinal layer
segmentation in optical coherence tomography images. <em>TIP</em>,
<em>29</em>, 303–312. (<a
href="https://doi.org/10.1109/TIP.2019.2931461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting the retinal layers in optical coherence tomography (OCT) images helps to quantify the layer information in early diagnosis of retinal diseases, which are the main cause of permanent blindness. Thus, the segmentation process plays a critical role in preventing vision impairment. However, because there is a lack of practical automated techniques, expert ophthalmologists still have to manually segment the retinal layers. In this paper, we propose an automated segmentation method for OCT images based on a feature-learning regression network without human bias. The proposed deep neural network regression takes the intensity, gradient, and adaptive normalized intensity score (ANIS) of an image segment as features for learning, and then predicts the corresponding retinal boundary pixel. Reformulating the segmentation as a regression problem obviates the need for a huge dataset and reduces the complexity significantly, as shown in the analysis of computational complexity given here. In addition, assisted by ANIS, the method operates robustly on OCT images containing intensity variances, low-contrast regions, speckle noise, and blood vessels, yet remains accurate and time-efficient. In the evaluation of the method conducted using 114 images, the processing time was approximately 10.596 s per image for identifying eight boundaries, and the training phase for each boundary line took only 30 s. Further, the Dice similarity coefficient used for assessing accuracy gave a computed value of approximately 0.966. The absolute pixel distance of manual and automatic segmentation using the proposed scheme was 0.612, which is less than a one-pixel difference, on average.},
  archive      = {J_TIP},
  author       = {Lua Ngo and Jaepyeong Cha and Jae-Ho Han},
  doi          = {10.1109/TIP.2019.2931461},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {303-312},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep neural network regression for automated retinal layer segmentation in optical coherence tomography images},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advanced 3D motion prediction for video-based dynamic point
cloud compression. <em>TIP</em>, <em>29</em>, 289–302. (<a
href="https://doi.org/10.1109/TIP.2019.2931621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud-based immersive media representation format has provided many opportunities for extended reality applications and has become widely used in volumetric content capturing scenarios. The high data rate of the point cloud is one of the key problems preventing the adoption of this media format. MPEG Immersive media working group (MPEG-I) aims to create a point cloud compression methodology relying on the existing video coding hardware implementations to solve this problem. However, in the scope of the state-of-the-art video-based dynamic point cloud compression (V-PCC) standard under MPEG-I, the intrinsic 3D object&#39;s motion continuity is destroyed by the 2D projections, resulting in a significant loss of inter prediction coding efficiency. In this paper, we first propose a general model utilizing the 3D motion and 3D to 2D correspondence to calculate the 2D motion vector (MV). Then, under the V-PCC, we propose a geometry-based method using the accurate 3D reconstructed geometry from the 2D geometry video to estimate the 2D MV in the 2D attribute video. In addition, we propose an auxiliary-information-based method using the coarse 3D reconstructed geometry provided by the auxiliary information to estimate the 2D MV in both the 2D geometry and attribute videos. Furthermore, we provide the following two ways to use the estimated 2D MV to improve coding efficiency. The first one is normative. We propose adding the estimated MV into the advanced MV candidate list and find a better MV predictor for each prediction unit (PU). The second one is non-normative. We propose applying the estimated MV as an additional candidate of the centers for motion estimation. We implement the proposed algorithms in the V-PCC reference software. Experimental results show that the proposed methods present significant coding gains compared with the current state-of-the-art motion prediction algorithm.},
  archive      = {J_TIP},
  author       = {Li Li and Zhu Li and Vladyslav Zakharchenko and Jianle Chen and Houqiang Li},
  doi          = {10.1109/TIP.2019.2931621},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {289-302},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advanced 3D motion prediction for video-based dynamic point cloud compression},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free tracker for multiple objects using joint
appearance and motion inference. <em>TIP</em>, <em>29</em>, 277–288. (<a
href="https://doi.org/10.1109/TIP.2019.2928123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-free tracking is a widely accepted approach to track an arbitrary object in a video using a single frame annotation with no further prior knowledge about the object of interest. Extending this problem to track multiple objects is really challenging because: 1) the tracker is not aware of the objects&#39; type while trying to distinguish them from background (detection task) and 2) the tracker needs to distinguish one object from other potentially similar objects (data association task) to generate stable trajectories. In order to track multiple arbitrary objects, most existing model-free tracking approaches rely on tracking each target individually by updating their appearance model independently. Therefore, in this scenario they often fail to perform well due to confusion between the appearance of similar objects, their sudden appearance changes and occlusion. To tackle this problem, we propose to use both appearance and motion models, and to learn those jointly using graphical models and the deep neural networks features. We introduce an indicator variable to predict sudden appearance change and/or occlusion. When these happen, our model does not update the appearance model thus avoiding using the background and/or incorrect object to update the appearance of the object of interest mistakenly, and relies on our motion model to track. Moreover, we consider the correlation among all targets, and seek the joint optimal locations for all targets simultaneously as a graphical model inference problem. We learn the joint parameters for both appearance model and motion model in an online fashion under the framework of LaRank. Experiment results show that our method achieved superior performance compared to the competitive methods.},
  archive      = {J_TIP},
  author       = {Chongyu Liu and Rui Yao and S. Hamid Rezatofighi and Ian Reid and Qinfeng Shi},
  doi          = {10.1109/TIP.2019.2928123},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {277-288},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Model-free tracker for multiple objects using joint appearance and motion inference},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale multi-view deep feature aggregation for food
recognition. <em>TIP</em>, <em>29</em>, 265–276. (<a
href="https://doi.org/10.1109/TIP.2019.2929447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, food recognition has received more and more attention in image processing and computer vision for its great potential applications in human health. Most of the existing methods directly extracted deep visual features via convolutional neural networks (CNNs) for food recognition. Such methods ignore the characteristics of food images and are, thus, hard to achieve optimal recognition performance. In contrast to general object recognition, food images typically do not exhibit distinctive spatial arrangement and common semantic patterns. In this paper, we propose a multi-scale multi-view feature aggregation (MSMVFA) scheme for food recognition. MSMVFA can aggregate high-level semantic features, mid-level attribute features, and deep visual features into a unified representation. These three types of features describe the food image from different granularity. Therefore, the aggregated features can capture the semantics of food images with the greatest probability. For that solution, we utilize additional ingredient knowledge to obtain mid-level attribute representation via ingredient-supervised CNNs. High-level semantic features and deep visual features are extracted from class-supervised CNNs. Considering food images do not exhibit distinctive spatial layout in many cases, MSMVFA fuses multi-scale CNN activations for each type of features to make aggregated features more discriminative and invariable to geometrical deformation. Finally, the aggregated features are more robust, comprehensive, and discriminative via two-level fusion, namely multi-scale fusion for each type of features and multi-view aggregation for different types of features. In addition, MSMVFA is general and different deep networks can be easily applied into this scheme. Extensive experiments and evaluations demonstrate that our method achieves state-of-the-art recognition performance on three popular large-scale food benchmark datasets in Top-1 recognition accuracy. Furthermore, we expect this paper will further the agenda of food recognition in the community of image processing and computer vision.},
  archive      = {J_TIP},
  author       = {Shuqiang Jiang and Weiqing Min and Linhu Liu and Zhengdong Luo},
  doi          = {10.1109/TIP.2019.2929447},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {265-276},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale multi-view deep feature aggregation for food recognition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional deblurring for natural imaging. <em>TIP</em>,
<em>29</em>, 250–264. (<a
href="https://doi.org/10.1109/TIP.2019.2929865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel design of image deblurring in the form of one-shot convolution filtering that can directly convolve with naturally blurred images for restoration. The problem of optical blurring is a common disadvantage to many imaging applications that suffer from optical imperfections. Despite numerous deconvolution methods that blindly estimate blurring in either inclusive or exclusive forms, they are practically challenging due to high computational cost and low image reconstruction quality. Both conditions of high accuracy and high speed are prerequisites for high-throughput imaging platforms in digital archiving. In such platforms, deblurring is required after image acquisition before being stored, previewed, or processed for high-level interpretation. Therefore, on-the-fly correction of such images is important to avoid possible time delays, mitigate computational expenses, and increase image perception quality. We bridge this gap by synthesizing a deconvolution kernel as a linear combination of finite impulse response (FIR) even-derivative filters that can be directly convolved with blurry input images to boost the frequency fall-off of the point spread function (PSF) associated with the optical blur. We employ a Gaussian low-pass filter to decouple the image denoising problem for image edge deblurring. Furthermore, we propose a blind approach to estimate the PSF statistics for two Gaussian and Laplacian models that are common in many imaging pipelines. Thorough experiments are designed to test and validate the efficiency of the proposed method using 2054 naturally blurred images across six imaging applications and seven state-of-the-art deconvolution methods.},
  archive      = {J_TIP},
  author       = {Mahdi S. Hosseini and Konstantinos N. Plataniotis},
  doi          = {10.1109/TIP.2019.2929865},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {250-264},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Convolutional deblurring for natural imaging},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised online video object segmentation with motion
property understanding. <em>TIP</em>, <em>29</em>, 237–249. (<a
href="https://doi.org/10.1109/TIP.2019.2930152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised video object segmentation aims to automatically segment moving objects over an unconstrained video without any user annotation. So far, only few unsupervised online methods have been reported in the literature, and their performance is still far from satisfactory because the complementary information from future frames cannot be processed under online setting. To solve this challenging problem, in this paper, we propose a novel unsupervised online video object segmentation (UOVOS) framework by construing the motion property to mean moving in concurrence with a generic object for segmented regions. By incorporating the salient motion detection and the object proposal, a pixel-wise fusion strategy is developed to effectively remove detection noises, such as dynamic background and stationary objects. Furthermore, by leveraging the obtained segmentation from immediately preceding frames, a forward propagation algorithm is employed to deal with unreliable motion detection and object proposals. Experimental results on several benchmark datasets demonstrate the efficacy of the proposed method. Compared to state-of-the-art unsupervised online segmentation algorithms, the proposed method achieves an absolute gain of 6.2\%. Moreover, our method achieves better performance than the best unsupervised offline algorithm on the DAVIS-2016 benchmark dataset. Our code is available on the project website: https://www.github.com/visiontao/uovos .},
  archive      = {J_TIP},
  author       = {Tao Zhuo and Zhiyong Cheng and Peng Zhang and Yongkang Wong and Mohan Kankanhalli},
  doi          = {10.1109/TIP.2019.2930152},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {237-249},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised online video object segmentation with motion property understanding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coarse-to-fine semantic segmentation from image-level
labels. <em>TIP</em>, <em>29</em>, 225–236. (<a
href="https://doi.org/10.1109/TIP.2019.2926748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network-based semantic segmentation generally requires large-scale cost extensive annotations for training to obtain better performance. To avoid pixel-wise segmentation annotations that are needed for most methods, recently some researchers attempted to use object-level labels (e.g., bounding boxes) or image-level labels (e.g., image categories). In this paper, we propose a novel recursive coarse-to-fine semantic segmentation framework based on only image-level category labels. For each image, an initial coarse mask is first generated by a convolutional neural network-based unsupervised foreground segmentation model and then is enhanced by a graph model. The enhanced coarse mask is fed to a fully convolutional neural network to be recursively refined. Unlike the existing image-level label-based semantic segmentation methods, which require labeling of all categories for images that contain multiple types of objects, our framework only needs one label for each image and can handle images that contain multi-category objects. Only trained on ImageNet, our framework achieves comparable performance on the PASCAL VOC dataset with other image-level label-based state-of-the-art methods of semantic segmentation. Furthermore, our framework can be easily extended to foreground object segmentation task and achieves comparable performance with the state-of-the-art supervised methods on the Internet object dataset.},
  archive      = {J_TIP},
  author       = {Longlong Jing and Yucheng Chen and Yingli Tian},
  doi          = {10.1109/TIP.2019.2926748},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {225-236},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Coarse-to-fine semantic segmentation from image-level labels},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Temporally coherent video harmonization using adversarial
networks. <em>TIP</em>, <em>29</em>, 214–224. (<a
href="https://doi.org/10.1109/TIP.2019.2925550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositing is one of the most important editing operations for images and videos. The process of improving the realism of composite results is often called harmonization. Previous approaches for harmonization mainly focus on images. In this paper, we take one step further to attack the problem of video harmonization. Specifically, we train a convolutional neural network in an adversarial way, exploiting a pixel-wise disharmony discriminator to achieve more realistic harmonized results and introducing a temporal loss to increase temporal consistency between consecutive harmonized frames. Thanks to the pixel-wise disharmony discriminator, we are also able to relieve the need of input foreground masks. Since existing video datasets which have ground-truth foreground masks and optical flows are not sufficiently large, we propose a simple yet efficient method to build up a synthetic dataset supporting supervised training of the proposed adversarial network. The experiments show that training on our synthetic dataset generalizes well to the real-world composite dataset. In addition, our method successfully incorporates temporal consistency during training and achieves more harmonious visual results than previous methods.},
  archive      = {J_TIP},
  author       = {Hao-Zhi Huang and Sen-Zhe Xu and Jun-Xiong Cai and Wei Liu and Shi-Min Hu},
  doi          = {10.1109/TIP.2019.2925550},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {214-224},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Temporally coherent video harmonization using adversarial networks},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). A graph embedding framework for maximum mean
discrepancy-based domain adaptation algorithms. <em>TIP</em>,
<em>29</em>, 199–213. (<a
href="https://doi.org/10.1109/TIP.2019.2928630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to deal with learning problems in which the labeled training data and unlabeled testing data are differently distributed. Maximum mean discrepancy (MMD), as a distribution distance measure, is minimized in various domain adaptation algorithms for eliminating domain divergence. We analyze empirical MMD from the point of view of graph embedding. It is discovered from the MMD intrinsic graph that, when the empirical MMD is minimized, the compactness within each domain and each class is simultaneously reduced. Therefore, points from different classes may mutually overlap, leading to unsatisfactory classification results. To deal with this issue, we present a graph embedding framework with intrinsic and penalty graphs for MMD-based domain adaptation algorithms. In the framework, we revise the intrinsic graph of MMD-based algorithms such that the within-class scatter is minimized, and thus, the new features are discriminative. Two strategies are proposed. Based on the strategies, we instantiate the framework by exploiting four models. Each model has a penalty graph characterizing certain similarity property that should be avoided. Comprehensive experiments on visual cross-domain benchmark datasets demonstrate that the proposed models can greatly enhance the classification performance compared with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yiming Chen and Shiji Song and Shuang Li and Cheng Wu},
  doi          = {10.1109/TIP.2019.2928630},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {199-213},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A graph embedding framework for maximum mean discrepancy-based domain adaptation algorithms},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constrained discriminative projection learning for image
classification. <em>TIP</em>, <em>29</em>, 186–198. (<a
href="https://doi.org/10.1109/TIP.2019.2926774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection learning is widely used in extracting discriminative features for classification. Although numerous methods have already been proposed for this goal, they barely explore the label information during projection learning and fail to obtain satisfactory performance. Besides, many existing methods can learn only a limited number of projections for feature extraction which may degrade the performance in recognition. To address these problems, we propose a novel constrained discriminative projection learning (CDPL) method for image classification. Specifically, CDPL can be formulated as a joint optimization problem over subspace learning and classification. The proposed method incorporates the low-rank constraint to learn a robust subspace which can be used as a bridge to seamlessly connect the original visual features and objective outputs. A regression function is adopted to explicitly exploit the class label information so as to enhance the discriminability of subspace. Unlike existing methods, we use two matrices to perform feature learning and regression, respectively, such that the proposed approach can obtain more projections and achieve superior performance in classification tasks. The experiments on several datasets show clearly the advantages of our method against other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Min Meng and Mengcheng Lan and Jun Yu and Jigang Wu and Dapeng Tao},
  doi          = {10.1109/TIP.2019.2926774},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {186-198},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Constrained discriminative projection learning for image classification},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online-learning-based bayesian decision rule for fast intra
mode and CU partitioning algorithm in HEVC screen content coding.
<em>TIP</em>, <em>29</em>, 170–185. (<a
href="https://doi.org/10.1109/TIP.2019.2924810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screen content coding (SCC) is an extension of high efficiency video coding by adopting new coding modes to improve the coding efficiency of SCC at the expense of increased complexity. This paper proposes an online-learning approach for fast mode decision and coding unit (CU) size decision in SCC. To make a fast mode decision, the corner point is first extracted as a unique feature in screen content, which is an essential pre-processing step to guide Bayesian decision modeling. Second, the distinct color number in a CU is derived as another unique feature in screen content to build the precise model using online-learning for skipping unnecessary modes. Third, the correlation of the modes among spatial neighboring CUs is analyzed to further eliminate unnecessary mode candidates. Finally, the Bayesian decision rule using online-learning is applied again to make a fast CU size decision. To ensure the accuracy of the Bayesian decision models, new scene change detection is designed to update the models. Results show that the proposed algorithm achieves 36.69\% encoding time reduction with 1.08\% Bjøntegaard delta bitrate (BDBR) increment under all intra configuration. By integrating into the existing fast SCC approach, the proposed algorithm reduces 48.83\% encoding time with a 1.78\% increase in BDBR.},
  archive      = {J_TIP},
  author       = {Wei Kuang and Yui-Lam Chan and Sik-Ho Tsang and Wan-Chi Siu},
  doi          = {10.1109/TIP.2019.2924810},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {170-185},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Online-learning-based bayesian decision rule for fast intra mode and CU partitioning algorithm in HEVC screen content coding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Multipatch unbiased distance non-local adaptive means with
wavelet shrinkage. <em>TIP</em>, <em>29</em>, 157–169. (<a
href="https://doi.org/10.1109/TIP.2019.2928644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing non-local means (NLM) methods either use Euclidean distance to measure the similarity between patches, or compute weight ω ij only once and keep it unchanged during the subsequent denoising iterations, or use only the structure information of the denoised image to update weight ω ij . These may lead to the limited denoising performance. To address these issues, this paper proposes the non-local adaptive means (NLAM) for image denoising. NLAM treats weight ω ij as an optimization variable and iteratively updates its value. We then introduce three unbiased distances, namely, pixel-pixel, patch- patch, and coupled unbiased distances. These unbiased distances are more robust to measure the image pixel/patch similarity than Euclidean distance. Using the coupled unbiased distance, we propose the unbiased distance non-local adaptive means (UD-NLAM). Because UD-NLAM uses only a single patch size to compute weight ω ij , we introduce multipatch UD-NLAM (MUD-NLAM) to adapt different noise levels. To further improve denoising performance, we then propose a new denoising method called MUD-NLAM with wavelet shrinkage (MUD-NLAM-WS). Experimental results show that the proposed NLAM, UD-NLAM, and MUD-NLAM outperform existing NLM methods, and MUDNLAM-WS achieves a better performance than the state-of-theart denoising methods.},
  archive      = {J_TIP},
  author       = {Xiaoyao Li and Yicong Zhou and Jing Zhang and Lianhong Wang},
  doi          = {10.1109/TIP.2019.2928644},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {157-169},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multipatch unbiased distance non-local adaptive means with wavelet shrinkage},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-channel and multi-model-based autoencoding prior for
grayscale image restoration. <em>TIP</em>, <em>29</em>, 142–156. (<a
href="https://doi.org/10.1109/TIP.2019.2931240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration (IR) is a long-standing challenging problem in low-level image processing. It is of utmost importance to learn good image priors for pursuing visually pleasing results. In this paper, we develop a multi-channel and multi-model-based denoising autoencoder network as image prior for solving IR problem. Specifically, the network that trained on RGB-channel images is used to construct a prior at first, and then the learned prior is incorporated into single-channel grayscale IR tasks. To achieve the goal, we employ the auxiliary variable technique to integrate the higher-dimensional network-driven prior information into the iterative restoration procedure. In addition, according to the weighted aggregation idea, a multi-model strategy is put forward to enhance the network stability that favors to avoid getting trapped in local optima. Extensive experiments on image deblurring and deblocking tasks show that the proposed algorithm is efficient, robust, and yields state-of-the-art restoration quality on grayscale images.},
  archive      = {J_TIP},
  author       = {Sanqian Li and Binjie Qin and Jing Xiao and Qiegen Liu and Yuhao Wang and Dong Liang},
  doi          = {10.1109/TIP.2019.2931240},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {142-156},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-channel and multi-model-based autoencoding prior for grayscale image restoration},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Weaklier supervised semantic segmentation with only one
image level annotation per category. <em>TIP</em>, <em>29</em>, 128–141.
(<a href="https://doi.org/10.1109/TIP.2019.2930874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image semantic segmentation tasks and methods based on weakly supervised conditions have been proposed and achieve better and better performance in recent years. However, the purpose of these tasks is mainly to simplify the labeling work. In this paper, we establish a new and more challenging task condition: weaklier supervision with one image level annotation per category, which only provides prior knowledge that humans need to recognize new objects, and aims to achieve pixel-level object semantic understanding. In order to solve this problem, a three-stage semantic segmentation framework is put forward, which realizes image level, pixel level, and object common features learning from coarse to fine grade, and finally obtains semantic segmentation results with accurate and complete object regions. Researches on PASCAL VOC 2012 dataset demonstrates the effectiveness of the proposed method, which makes an obvious improvement compared to baselines. Based on fewer supervised information, the method also provides satisfactory performance compared to weakly supervised learning-based methods with complete image-level annotations.},
  archive      = {J_TIP},
  author       = {Xi Li and Huimin Ma and Xiong Luo},
  doi          = {10.1109/TIP.2019.2930874},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {128-141},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weaklier supervised semantic segmentation with only one image level annotation per category},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Super-resolution for hyperspectral and multispectral image
fusion accounting for seasonal spectral variability. <em>TIP</em>,
<em>29</em>, 116–127. (<a
href="https://doi.org/10.1109/TIP.2019.2928895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion combines data from different heterogeneous sources to obtain more precise information about an underlying scene. Hyperspectral-multispectral (HS-MS) image fusion is currently attracting great interest in remote sensing since it allows the generation of high spatial resolution HS images and circumventing the main limitation of this imaging modality. Existing HS-MS fusion algorithms, however, neglect the spectral variability often existing between images acquired at different time instants. This time difference causes variations in spectral signatures of the underlying constituent materials due to the different acquisition and seasonal conditions. This paper introduces a novel HS-MS image fusion strategy that combines an unmixing-based formulation with an explicit parametric model for typical spectral variability between the two images. Simulations with synthetic and real data show that the proposed strategy leads to a significant performance improvement under spectral variability and state-of-the-art performance otherwise.},
  archive      = {J_TIP},
  author       = {Ricardo Augusto Borsoi and Tales Imbiriba and José Carlos Moreira Bermudez},
  doi          = {10.1109/TIP.2019.2928895},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {116-127},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Super-resolution for hyperspectral and multispectral image fusion accounting for seasonal spectral variability},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Jointly using low-rank and sparsity priors for sparse
inverse synthetic aperture radar imaging. <em>TIP</em>, <em>29</em>,
100–115. (<a href="https://doi.org/10.1109/TIP.2019.2927458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inverse synthetic aperture radar (ISAR) imaging technique of a moving target with sparse sampling data has attracted wide attention due to its ability to reduce the data collection burden. However, traditional low-rank or 2D compressive sensing (CS)-based ISAR imaging methods can handle the random sampling or the separable sampling data only. When the specific data collection condition cannot be satisfied, low-rank or 2D CS-based methods cannot provide satisfactory imaging results any more. To remedy this problem, in this paper, we proposed a joint low-rank and sparsity priors&#39; constrained model for ISAR imaging with various sparse data patterns. This model is inspired by the facts that the received radar data have a low-rank property and the ISAR image is sparse on the specific dictionary. Two reconstruction algorithms to solve the double priors&#39; constrained optimization problem are developed under the alternative direction method of multipliers (ADMM) framework with the help of augmented Lagrange multipliers (ALM). Results on simulation data and real data show that the proposed methods are quite effective in recovering missing samples and focused image and perform better than the matrix completion-based method and the sparse representation-based method when dealing with the various kinds of sparse sampling data.},
  archive      = {J_TIP},
  author       = {Wei Qiu and Jianxiong Zhou and Qiang Fu},
  doi          = {10.1109/TIP.2019.2927458},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {100-115},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Jointly using low-rank and sparsity priors for sparse inverse synthetic aperture radar imaging},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 4D light field superpixel and segmentation. <em>TIP</em>,
<em>29</em>, 85–99. (<a
href="https://doi.org/10.1109/TIP.2019.2927330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixel segmentation of 2D images has been widely used in many computer vision tasks. Previous algorithms model the color, position, or higher spectral information for segmenting a 2D image. However, limited to the Gaussian imaging principle in a traditional camera, where each pixel is formed by summing lots of light rays from different angles, there is not a thorough segmentation solution to eliminate the ambiguity in defocus and occlusion boundary areas. In this paper, we consider the essential element of image pixel, i.e., rays in light space, and propose light field superpixel (LFSP) to eliminate the ambiguity. The LFSP is first defined mathematically and then two evaluation metrics, named LFSP self-similarity and effective label ratio, are proposed to evaluate the refocus-invariant and full-sliced properties of segmentation. By building a clique system containing 80 neighbors in light field, a robust refocus-invariant LFSP segmentation algorithm is developed. Experimental results on both synthetic and real light field datasets demonstrate the advantages over the current state of the art in terms of traditional evaluation metrics. Additionally, the LFSP self-similarity evaluations under different light field refocus levels show the refocus-invariance of the proposed algorithm. The full-sliced property of the proposed LFSP algorithm is verified by comparing it with the classical supervoxel algorithms. Finally, an LFSP-based application is demonstrated to show the effectiveness of LFSP in light field editing.},
  archive      = {J_TIP},
  author       = {Hao Zhu and Qi Zhang and Qing Wang and Hongdong Li},
  doi          = {10.1109/TIP.2019.2927330},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {85-99},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {4D light field superpixel and segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FAMED-net: A fast and accurate multi-scale end-to-end
dehazing network. <em>TIP</em>, <em>29</em>, 72–84. (<a
href="https://doi.org/10.1109/TIP.2019.2922837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing is a critical image pre-processing step for subsequent high-level computer vision tasks. However, it remains challenging due to its ill-posed nature. Existing dehazing models tend to suffer from model overcomplexity and computational inefficiency or have limited representation capacity. To tackle these challenges, here, we propose a fast and accurate multi-scale end-to-end dehazing network, called FAMED-Net, which comprises encoders at three scales and a fusion module to efficiently and directly learn the haze-free image. Each encoder consists of cascaded and densely connected point-wise convolutional layers and pooling layers. Since no larger convolutional kernels are used and features are reused layer-by-layer, FAMED-Net is lightweight and computationally efficient. Thorough empirical studies on public synthetic datasets (including RESIDE) and real-world hazy images demonstrate the superiority of FAMED-Net over other representative state-of-the-art models with respect to model complexity, computational efficiency, restoration accuracy, and cross-set generalization. The code will be made publicly available.},
  archive      = {J_TIP},
  author       = {Jing Zhang and Dacheng Tao},
  doi          = {10.1109/TIP.2019.2922837},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {72-84},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FAMED-net: A fast and accurate multi-scale end-to-end dehazing network},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RESLS: Region and edge synergetic level set framework for
image segmentation. <em>TIP</em>, <em>29</em>, 57–71. (<a
href="https://doi.org/10.1109/TIP.2019.2928134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The active contour models with level set evolution have been visited with a vast number of methods for image segmentation. They can be mainly classified into region-based and edge-based models, and it has been validated that the hybrid variants combining both region and edge information can improve the segmentation performance. However, to the best of our knowledge, the theoretical foundation of collaboration mechanism between the region and the edge information is limited. Specifically, most existing hybrid models are just combining all the energy terms together, resulting in great challenges of choosing an appropriate weight coefficient for each term and accommodating different modalities of imaging. To overcome these difficulties, this paper proposes a region and edge synergetic level set framework named RESLS. It provides an approach to construct new hybrid level set models using a normalized intensity indicator function that allows the region information easily embedding into the edge-based model. In this case, the energy weights of region and edge terms can be constrained by the global optimization condition deduced from the framework. Some representative as well as state-of-the-art models are taken as examples to demonstrate the generality of our method. The experiments validate that under the guidance of the optimization condition, the weighting parameter of each term can be reliably chosen. Meanwhile, the segmentation accuracy, robustness, and computational efficiency of RESLS can be improved compared with its component models.},
  archive      = {J_TIP},
  author       = {Weihang Zhang and Xue Wang and Wei You and Junfeng Chen and Peng Dai and Pengbo Zhang},
  doi          = {10.1109/TIP.2019.2928134},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {57-71},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RESLS: Region and edge synergetic level set framework for image segmentation},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral images denoising via nonconvex regularized
low-rank and sparse matrix decomposition. <em>TIP</em>, <em>29</em>,
44–56. (<a href="https://doi.org/10.1109/TIP.2019.2926736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) are often degraded by a mixture of various types of noise during the imaging process, including Gaussian noise, impulse noise, and stripes. Such complex noise could plague the subsequent HSIs processing. Generally, most HSI denoising methods formulate sparsity optimization problems with convex norm constraints, which over-penalize large entries of vectors, and may result in a biased solution. In this paper, a nonconvex regularized low-rank and sparse matrix decomposition (NonRLRS) method is proposed for HSI denoising, which can simultaneously remove the Gaussian noise, impulse noise, dead lines, and stripes. The NonRLRS aims to decompose the degraded HSI, expressed in a matrix form, into low-rank and sparse components with a robust formulation. To enhance the sparsity in both the intrinsic low-rank structure and the sparse corruptions, a novel nonconvex regularizer named as normalized ε-penalty, is presented, which can adaptively shrink each entry. In addition, an effective algorithm based on the majorization minimization (MM) is developed to solve the resulting nonconvex optimization problem. Specifically, the MM algorithm first substitutes the nonconvex objective function with the surrogate upper-bound in each iteration, and then minimizes the constructed surrogate function, which enables the nonconvex problem to be solved in the framework of reweighted technique. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Ting Xie and Shutao Li and Bin Sun},
  doi          = {10.1109/TIP.2019.2926736},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {44-56},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral images denoising via nonconvex regularized low-rank and sparse matrix decomposition},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast collective activity recognition under weak supervision.
<em>TIP</em>, <em>29</em>, 29–43. (<a
href="https://doi.org/10.1109/TIP.2019.2918725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collective activity recognition, which tells what activity a group of people is performing, is a cutting-edge research topic in computer vision. Different from action performed by individuals, collective activity needs to consider the complex interactions among different people. However, most previous works require exhaustive annotations such as accurate label information of individual actions, pairwise interactions, and poses, which could not be easily available in practice. Moreover, most of them treat human detection as a decoupled task before collective activity recognition and leverage all detected persons. This not only ignores the mutual relation between the two tasks, which makes it hard for filtering out irrelevant people, but also probably increases the computation burden when reasoning the collective activities. In this paper, we propose a fast weakly supervised deep learning architecture for collective activity recognition. For fast inference, we propose to make the actor detection and weakly supervised collective activity reasoning collaborate in an end-to-end framework by sharing convolutional layers between them. The joint learning makes the two tasks united and reinforced each other, so that it is more effective to filter out the outliers who are not involved in the activity. For the weakly supervised learning, we propose a latent embedding scheme for mining person-group interactive relationship to get rid of the use of any pairwise relation between people and the individual action labels as well. The experimental results show that the proposed framework achieves comparable or even better performance as compared to the state-of-the-art on three datasets. Our joint modelling reasons collective activities at the speed of 22.65 fps, which is the fastest ever known and substantially makes collective activity recognition more towards real-time applications.},
  archive      = {J_TIP},
  author       = {Peizhen Zhang and Yongyi Tang and Jian-Fang Hu and Wei-Shi Zheng},
  doi          = {10.1109/TIP.2019.2918725},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {29-43},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast collective activity recognition under weak supervision},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparative review of recent kinect-based action
recognition algorithms. <em>TIP</em>, <em>29</em>, 15–28. (<a
href="https://doi.org/10.1109/TIP.2019.2925285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based human action recognition is currently one of the most active research areas in computer vision. Various research studies indicate that the performance of action recognition is highly dependent on the type of features being extracted and how the actions are represented. Since the release of the Kinect camera, a large number of Kinect-based human action recognition techniques have been proposed in the literature. However, there still does not exist a thorough comparison of these Kinect-based techniques under the grouping of feature types, such as handcrafted versus deep learning features and depth-based versus skeleton-based features. In this paper, we analyze and compare 10 recent Kinect-based algorithms for both cross-subject action recognition and cross-view action recognition using six benchmark datasets. In addition, we have implemented and improved some of these techniques and included their variants in the comparison. Our experiments show that the majority of methods perform better on cross-subject action recognition than cross-view action recognition, that the skeleton-based features are more robust for cross-view recognition than the depth-based features, and that the deep learning features are suitable for large datasets.},
  archive      = {J_TIP},
  author       = {Lei Wang and Du Q. Huynh and Piotr Koniusz},
  doi          = {10.1109/TIP.2019.2925285},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {15-28},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A comparative review of recent kinect-based action recognition algorithms},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural multimodal cooperative learning toward micro-video
understanding. <em>TIP</em>, <em>29</em>, 1–14. (<a
href="https://doi.org/10.1109/TIP.2019.2923608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevailing characteristics of micro-videos result in the less descriptive power of each modality. The micro-video representations, several pioneer efforts proposed, are limited in implicitly exploring the consistency between different modality information but ignore the complementarity. In this paper, we focus on how to explicitly separate the consistent features and the complementary features from the mixed information and harness their combination to improve the expressiveness of each modality. Toward this end, we present a neural multimodal cooperative learning (NMCL) model to split the consistent component and the complementary component by a novel relation-aware attention mechanism. Specifically, the computed attention score can be used to measure the correlation between the features extracted from different modalities. Then, a threshold is learned for each modality to distinguish the consistent and complementary features according to the score. Thereafter, we integrate the consistent parts to enhance the representations and supplement the complementary ones to reinforce the information in each modality. As to the problem of redundant information, which may cause overfitting and is hard to distinguish, we devise an attention network to dynamically capture the features which closely related the category and output a discriminative representation for prediction. The experimental results on a real-world micro-video dataset show that the NMCL outperforms the state-of-the-art methods. Further studies verify the effectiveness and cooperative effects brought by the attentive mechanism.},
  archive      = {J_TIP},
  author       = {Yinwei Wei and Xiang Wang and Weili Guan and Liqiang Nie and Zhouchen Lin and Baoquan Chen},
  doi          = {10.1109/TIP.2019.2923608},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Neural multimodal cooperative learning toward micro-video understanding},
  volume       = {29},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
