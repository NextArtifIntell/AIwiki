<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tai---22">TAI - 22</h2>
<ul>
<li><details>
<summary>
(2020). Fake profile detection on social networking websites: A
comprehensive review. <em>TAI</em>, <em>1</em>(3), 271–285. (<a
href="https://doi.org/10.1109/TAI.2021.3064901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to summarize the recent advancement in the fake account detection methodology on social networking websites. Over the past decade, social networking websites have received huge attention from users all around the world. As a result, popular websites such as Facebook, Twitter, LinkedIn, Instagram, and others saw an unexpected rise in registered users. However, researchers claim that all registered accounts are not real; many of them are fake and created for specific purposes. The primary purpose of fake accounts is to spread spam content, rumor, and other unauthentic messages on the platform. Hence, it is needed to filter out the fake accounts, but it has many challenges. In the past few years, researchers applied many advanced technologies to identify fake accounts. In the survey presented in this article, we summarize the recent development of fake account detection technologies. We discuss the challenges and limitations of the existing models in brief. The survey may help future researchers to identify the gaps in the current literature and develop a generalized framework for fake profile detection on social networking websites.},
  archive      = {J_TAI},
  author       = {Pradeep Kumar Roy and Shivam Chahar},
  doi          = {10.1109/TAI.2021.3064901},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {3},
  pages        = {271-285},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fake profile detection on social networking websites: A comprehensive review},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A systematic review on the use of AI and ML for fighting the
COVID-19 pandemic. <em>TAI</em>, <em>1</em>(3), 258–270. (<a
href="https://doi.org/10.1109/TAI.2021.3062771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) and machine learning (ML) have caused a paradigm shift in healthcare that can be used for decision support and forecasting by exploring medical data. Recent studies have shown that AI and ML can be used to fight COVID-19. The objective of this article is to summarize the recent AI- and ML-based studies that have addressed the pandemic. From an initial set of 634 articles, a total of 49 articles were finally selected through an inclusion-exclusion process. In this article, we have explored the objectives of the existing studies (i.e., the role of AI/ML in fighting the COVID-19 pandemic); the context of the studies (i.e., whether it was focused on a specific country-context or with a global perspective; the type and volume of the dataset; and the methodology, algorithms, and techniques adopted in the prediction or diagnosis processes). We have mapped the algorithms and techniques with the data type by highlighting their prediction/classification accuracy. From our analysis, we categorized the objectives of the studies into four groups: disease detection, epidemic forecasting, sustainable development, and disease diagnosis. We observed that most of these studies used deep learning algorithms on image-data, more specifically on chest X-rays and CT scans. We have identified six future research opportunities that we have summarized in this paper. Impact Statement: Artificial intelligence (AI) and machine learning(ML) methods have been widely used to assist in the fight against COVID-19 pandemic. A very few in-depth literature reviews have been conducted to synthesize the knowledge and identify future research agenda including a previously published review on data science for COVID-19 in this article. In this article, we synthesized reviewed recent literature that focuses on the usages and applications of AI and ML to fight against COVID-19. We have identified seven future research directions that would guide researchers to conduct future research. The most significant of these are: develop new treatment options, explore the contextual effect and variation in research outcomes, support the health care workforce, and explore the effect and variation in research outcomes based on different types of data.},
  archive      = {J_TAI},
  author       = {Muhammad Nazrul Islam and Toki Tahmid Inan and Suzzana Rafi and Syeda Sabrina Akter and Iqbal H. Sarker and A. K. M. Najmul Islam},
  doi          = {10.1109/TAI.2021.3062771},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {3},
  pages        = {258-270},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A systematic review on the use of AI and ML for fighting the COVID-19 pandemic},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SleepPrintNet: A multivariate multimodal neural network
based on physiological time-series for automatic sleep staging.
<em>TAI</em>, <em>1</em>(3), 248–257. (<a
href="https://doi.org/10.1109/TAI.2021.3060350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sleep is one of the most fundamental physiological activities of human beings. Sleep assessment based on physiological time-series can efficiently assist human experts to diagnose the sleep health of people. However, most of the existing methods only considered one or two kinds of time-domain, frequency-domain, and spatial-domain information from electroencephalogram (EEG). Besides, existing deep learning methods share the feature extraction module of EEG with other modalities, which ignore the discriminative features of electrooculogram (EOG) and electromyography (EMG). Therefore, how to make full use of the complementarity of different features of EEG and capture the discriminative features from other modalities is challenging. To tackle this challenge, we design SleepPrintNet to capture the SleepPrint in physiological time-series, which represents the complementarity among different features of EEG and discriminative features from other modalities in different sleep stages. SleepPrintNet consists of an EEG temporal feature extraction module, an EEG spectral-spatial feature extraction module for the temporal-spectral-spatial representation of EEG signals, and two multimodal feature extraction modules including EOG and EMG feature extraction module. To the best of our knowledge, it is the first attempt to integrate EEG temporal-spectral-spatial as well as the multimodal features simultaneously in a unified model for sleep staging. Experiments on the benchmark dataset MASS-SS3 demonstrate that SleepPrintNet outperforms all baseline models. The implementation code of SleepPrintNet is available at https://github.com/xiyangcai/SleepPrintNet . Impact Statement– Sleep staging helps sleep experts assess sleep quality and diagnose sleep health. The polysomnography, which contains the physiological signal recordings during sleep, is a kind of multimodal multivariant physiological time-series for sleep staging. However, existing methods treat physiological time-series from different parts of the body equally, which ignore the abundant information in multimodal signals. The SleepPrintNet proposed in this article has improved the accuracy of sleep staging with the help of the discriminative characteristics from different physiological time-series, which is made up of several independent modules for the extraction of different modalities signals. SleepPrintNet is also a universal framework for the classification of multivariate and multimodal signals. It can be applied in the diagnosis and treatment of other diseases based on physiological signals. The high-accuracy classification of physiological signals has great significance in the field of intelligent medical diagnostics.},
  archive      = {J_TAI},
  author       = {Ziyu Jia and Xiyang Cai and Gaoxing Zheng and Jing Wang and Youfang Lin},
  doi          = {10.1109/TAI.2021.3060350},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {3},
  pages        = {248-257},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SleepPrintNet: A multivariate multimodal neural network based on physiological time-series for automatic sleep staging},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). V&lt;inline-formula&gt;&lt;tex-math
notation=“LaTeX”&gt;<span
class="math inline"><sup>3</sup></span>&lt;/tex-math&gt;&lt;/inline-formula&gt;h:
View variation and view heredity for incomplete multiview clustering.
<em>TAI</em>, <em>1</em>(3), 233–247. (<a
href="https://doi.org/10.1109/TAI.2021.3052425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real data often appear in the form of multiple incomplete views. Incomplete multiview clustering is an effective method to integrate these incomplete views. Previous methods only learn the consistent information between different views and ignore the unique information of each view, which limits their clustering performance and generalizations. To overcome this limitation, we propose a novel V iew V ariation and V iew H eredity approach (V &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt; H). Inspired by the variation and the heredity in genetics, V &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt; H first decomposes each subspace into a variation matrix for the corresponding view and a heredity matrix for all the views to represent the unique information and the consistent information respectively. Then, by aligning different views based on their cluster indicator matrices, V &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt; H integrates the unique information from different views to improve the clustering performance. Finally, with the help of the adjustable low-rank representation based on the heredity matrix, V &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt; H recovers the underlying true data structure to reduce the influence of the large incompleteness. More importantly, V &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt; H presents possibly the first work to introduce genetics to clustering algorithms for learning simultaneously the consistent information and the unique information from incomplete multiview data. Extensive experimental results on fifteen benchmark datasets validate its superiority over other state-of-the-arts. Impact Statement—Incomplete multiview clustering is a popular technology to cluster incomplete datasets from multiple sources. The technology is becoming more significant due to the absence of the expensive requirement of labeling these datasets. However, previous algorithms cannot fully learn the information of each view. Inspired by variation and heredity in genetics, our proposed algorithm V&lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt;H fully learns the information of each view. Compared with the state-of-the-art algorithms, V&lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt;H improves clustering performance by more than 20% in representative cases. With the large improvement on multiple datasets, V&lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt;H has wide potential applications including the analysis of pandemic, financial and election datasets. The DOI of our codes is 10.24 433/CO.2 119 636.v1.},
  archive      = {J_TAI},
  author       = {Xiang Fang and Yuchong Hu and Pan Zhou and Dapeng Oliver Wu},
  doi          = {10.1109/TAI.2021.3052425},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {3},
  pages        = {233-247},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {V&lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^3$&lt;/tex-math&gt;&lt;/inline-formula&gt;H: View variation and view heredity for incomplete multiview clustering},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ZJU-leaper: A benchmark dataset for fabric defect detection
and a comparative study. <em>TAI</em>, <em>1</em>(3), 219–232. (<a
href="https://doi.org/10.1109/TAI.2021.3057027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fabric inspection plays an important role in the process of quality control in textile manufacturing. There is a growing demand in the textile industry to leverage computer vision technology for more efficient quality control in the hope that it will replace the traditional labor-intensive inspection by naked eyes. However, there is an underlying viewpoint in most existing fabric datasets that automatic defect detection is a traditional image classification problem, thus more samples help better, which lacks enough consideration about the problem itself and real application environments. After deep communication with users, we find these facts that an assembly line usually has only a few fixed texture fabrics for a long period, users prefer fast deployment and easily upgradable model to a general model and long-time tuning, and users hope the process of collecting samples, annotating, and deployment affects assembly lines as little as possible. This implies that defect detection is different from popular deep learning problems. Multiple-stage models and fast training become more attractive since users are able to train and deploy models by themselves according to the real conditions of samples that can be obtained. Based on this analysis, we propose a new fabric dataset “ZJU-Leaper”. It provides a series of task settings in accordance with the progressive strategy dealing with the problem, from only normal samples to many defective samples with precise annotations, to facilitate real-world application. To avoid misleading information and inconsistency issues associated with the prior evaluation metrics, we propose a new evaluation protocol by experimental analysis of task-specific indexes, which can tell a truthful comparison between different inspection methods. We also offer some novel solutions to address the new challenges of our dataset, as part of the baseline experiments. It is our hope that ZJU-Leaper can accelerate the research of automated visual inspection and empower the practitioners with more opportunities for manufacturing automation in the textile industry. Impact Statement—Automatic defect inspection is very important in quality control of the fabric industry by helping manufacturers to identify production problems early, hence improving product quality and production efficiency. Meanwhile, it is able to reduce the high labor cost of manual inspection and boost the productivity of the textile industry. To develop effective mathematical inspection algorithms, the fabric dataset serves as an indispensable component to present a practical application environment and enable fair evaluation for algorithms. This paper proposes a new dataset, called “ZJU-Leaper” designed from a viewpoint of multiple-stage models and fast training, containing threefold novelty: 1) the data collection and organization consider the actual requirements and special characteristics of assembly lines in textile factories; 2) it has several designed task settings in order to meet the different levels of requirements in the practical inspection task; 3) it provides a reasonable evaluation protocol for comprehensive comparisons between different inspection algorithms. The preliminary experiments show that some existing algorithms still cannot reach the satisfying performance by this benchmark, which implies more effort should be made to develop new methods for the real use of automatic defect inspection.},
  archive      = {J_TAI},
  author       = {Chenkai Zhang and Shaozhe Feng and Xulongqi Wang and Yueming Wang},
  doi          = {10.1109/TAI.2021.3057027},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {3},
  pages        = {219-232},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ZJU-leaper: A benchmark dataset for fabric defect detection and a comparative study},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast real-time reinforcement learning for
partially-observable large-scale systems. <em>TAI</em>, <em>1</em>(3),
206–218. (<a href="https://doi.org/10.1109/TAI.2021.3058228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast real-time reinforcement learning (RL) control algorithm for large-scale partially-observable linear dynamic systems. We first develop a one-shot RL method for designing model-free optimal controllers based on a finite-time history of the inputs and the outputs. However, when the system dimension is large, this method may suffer from a long learning time. To overcome this problem, in the second half of the paper we introduce a new notion of approximation to the design, where the original set of input-output history is replaced by a much shorter set. We show that this approximation can lead to a nearly optimal controller that is based on a lower-dimensional approximant of the original system in terms of reachability and observability. We provide a guideline for determining an appropriate length of the input-output history to reduce the suboptimality gap. The dimension of the resulting suboptimal controller is far less than that of the optimal controller, thereby speeding up learning time. The learned controller, however may cause instability when implemented in the original high-dimensional system by adversely exciting the approximation error. We theoretically establish the conditions for closed-loop stability using robust control theory, followed by numerical investigations of the trade-offs between learning time, length of input/output history, and closed-loop performance. The effectiveness of the method is illustrated using examples from electric power systems, modeled by partially-observable nonlinear differential-algebraic equations.},
  archive      = {J_TAI},
  author       = {Tomonori Sadamoto and Aranya Chakrabortty},
  doi          = {10.1109/TAI.2021.3058228},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {3},
  pages        = {206-218},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fast real-time reinforcement learning for partially-observable large-scale systems},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neural state pushdown automata. <em>TAI</em>,
<em>1</em>(3), 193–205. (<a
href="https://doi.org/10.1109/TAI.2021.3055167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To learn complex formal grammars, recurrent neural networks (RNNs) require sufficient computational resources to ensure correct grammar recognition. One approach to expand model capacity is to couple an RNN to an external stack memory. Here, we introduce a “neural state” pushdown automaton (NSPDA), which consists of a discrete stack instead of an continuous one and is coupled to a neural network state machine. We empirically show its effectiveness in recognizing various context-free grammars (CFGs). First, we develop the underlying mechanics of the proposed higher order recurrent network and its manipulation of a stack as well as how to stably program its underlying pushdown automaton (PDA). We also introduce a noise regularization scheme for higher-order (tensor) networks and design an algorithm for improved incremental learning. Finally, we design a method for inserting grammar rules into a NSPDA and empirically show that this prior knowledge improves its training convergence time by an order of magnitude and, in some cases, leads to better generalization. The NSPDA is also compared to a classical analog stack neural network pushdown automaton (NNPDA) as well as a wide array of first and second-order RNNs with and without external memory, trained using different learning algorithms. Our results show that for the Dyck languages, prior rule-based knowledge is critical for optimization convergence and for ensuring generalization to longer sequences at test time. We observe that many RNNs with and without memory, but no prior knowledge, struggle to converge and generalize on complex and longer CFGs.},
  archive      = {J_TAI},
  author       = {Ankur Arjun Mali and Alexander G. Ororbia II and C. Lee Giles},
  doi          = {10.1109/TAI.2021.3055167},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {3},
  pages        = {193-205},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A neural state pushdown automata},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color channel perturbation attacks for fooling convolutional
neural networks and a defense against such attacks. <em>TAI</em>,
<em>1</em>(2), 181–191. (<a
href="https://doi.org/10.1109/TAI.2020.3046167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convolutional neural networks (CNNs) have emerged as a very powerful data dependent hierarchical feature extraction method. It is widely used in several computer vision problems. The CNNs learn the important visual features from training samples automatically. It is observed that the network overfits the training samples very easily. Several regularization methods have been proposed to avoid the overfitting. In spite of this, the network is sensitive to the color distribution within the images which is ignored by the existing approaches. In this paper, we discover the color robustness problem of CNN by proposing a color channel perturbation (CCP) attack to fool the CNNs. In CCP attack new images are generated with new channels created by combining the original channels with the stochastic weights. Experiments were carried out over widely used CIFAR10, Caltech256 and TinyImageNet datasets in the image classification framework. The VGG, ResNet and DenseNet models are used to test the impact of the proposed attack. It is observed that the performance of the CNNs degrades drastically under the proposed CCP attack. Result show the effect of the proposed simple CCP attack over the robustness of the CNN trained model. The results are also compared with existing CNN fooling approaches to evaluate the accuracy drop. We also propose a primary defense mechanism to this problem by augmenting the training dataset with the proposed CCP attack. The state-of-the-art performance using the proposed solution in terms of the CNN robustness under CCP attack is observed in the experiments. The code is made publicly available at https://github.com/jayendrakantipudi/Color-Channel-Perturbation-Attack .},
  archive      = {J_TAI},
  author       = {Jayendra Kantipudi and Shiv Ram Dubey and Soumendu Chakraborty},
  doi          = {10.1109/TAI.2020.3046167},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {2},
  pages        = {181-191},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Color channel perturbation attacks for fooling convolutional neural networks and a defense against such attacks},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural network-based heterogeneous domain adaptation
using ensemble decision making in land cover classification.
<em>TAI</em>, <em>1</em>(2), 167–180. (<a
href="https://doi.org/10.1109/TAI.2020.3043724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A domain adaptation (DA) problem is investigated for land cover classification by utilizing the ensemble decision approach of deep neural networks to address the extra and missing class problem. Two different pretrained models followed by two autoencoders are used to extract the two sets of cross-domain features for samples in the reduced shape. Thereafter, the primary labels and probability scores of target samples are obtained using two different classifiers trained over the source samples with these different features. Moreover, unlike other open set DA techniques, the proposed framework is capable of identifying extra classes separately by exploring the agreement between these two classifiers. Experiments were carried out using three aerial image datasets, and the results are found to be encouraging for the proposed scheme in comparison with other state-of-the-art techniques.},
  archive      = {J_TAI},
  author       = {Indrajit Kalita and Moumita Roy},
  doi          = {10.1109/TAI.2020.3043724},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {2},
  pages        = {167-180},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep neural network-based heterogeneous domain adaptation using ensemble decision making in land cover classification},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decade survey of transfer learning (2010–2020).
<em>TAI</em>, <em>1</em>(2), 151–166. (<a
href="https://doi.org/10.1109/TAI.2021.3054609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning (TL) has been successfully applied to many real-world problems that traditional machine learning (ML) cannot handle, such as image processing, speech recognition, and natural language processing (NLP). Commonly, TL tends to address three main problems of traditional machine learning: (1) insufficient labeled data, (2) incompatible computation power, and (3) distribution mismatch. In general, TL can be organized into four categories: transductive learning, inductive learning, unsupervised learning, and negative learning. Furthermore, each category can be organized into four learning types: learning on instances, learning on features, learning on parameters, and learning on relations. This article presents a comprehensive survey on TL. In addition, this article presents the state of the art, current trends, applications, and open challenges.},
  archive      = {J_TAI},
  author       = {Shuteng Niu and Yongxin Liu and Jian Wang and Houbing Song},
  doi          = {10.1109/TAI.2021.3054609},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {2},
  pages        = {151-166},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A decade survey of transfer learning (2010–2020)},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recursive multi-signal temporal fusions with attention
mechanism improves EMG feature extraction. <em>TAI</em>, <em>1</em>(2),
139–150. (<a href="https://doi.org/10.1109/TAI.2020.3046160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of pattern recognition-based myoelectric interfaces has been heavily explored and contested in the research literature. A considerable proportion of the performance of these interfaces has been linked to the quality of the feature extraction (FE) stage used to describe the underlying myoelectric signal. In this paper, we address two important factors of FE that have not been fully exploited; 1) Spatial focus - traditional FE methods focus mainly on the concatenation of features extracted from individual channels and 2) Temporal focus - available FE methods are cross-sectional in nature, largely ignoring the temporal information that may exist between feature windows. To overcome these limitations, several spatiotemporal FE methods have been proposed including hand-crafted and deep learning (DL) models, with the latter showing significant performance enhancements at the cost of increased computational burden. This paper tackles the aforementioned limitations by 1) proposing novel extensions to simple time-domain FE methods, including the waveform length, zero crossings and root mean square, that can capture the relation between any number of channels, and 2) leveraging the long short-term memory concepts from deep neural networks to build a recursive framework that overcomes the cross-sectional nature of traditional methods. The advantages offered by the proposed Recursive Multi-Signal Temporal Fusion (RMTF) features include improved performance, competing with state-of-the-art FE methods without the computational costs associated with leading DL models, and the simplicity of the concepts making them suitable for real-time implementations. Experiments on 65 intact-limbed and amputee subjects reveal an approximate average of 15% reduction in classification errors as compared to models built with other feature sets.},
  archive      = {J_TAI},
  author       = {Rami N. Khushaba and Angkoon Phinyomark and Ali H. Al-Timemy and Erik Scheme},
  doi          = {10.1109/TAI.2020.3046160},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {2},
  pages        = {139-150},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Recursive multi-signal temporal fusions with attention mechanism improves EMG feature extraction},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning-based fault localization in video networks
using only client-side QoE. <em>TAI</em>, <em>1</em>(2), 130–138. (<a
href="https://doi.org/10.1109/TAI.2020.3041816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining a satisfactory customer Quality of Experience (QoE) is of vital importance for video service providers such as Netflix or Amazon Prime Video. Network faults degrade QoE and must therefore be detected, isolated, and fixed. However, this is difficult because each part of the end-to-end path belongs to a different autonomous system (AS) that is typically owned by a different entity, such as the video streaming provider, the internet service provider (ISP), and the client&#39;s local network operator. Although the video service provider (VSP) is usually blamed by the customer when there is poor QoE, the VSP does not have access to many parts of the network to localize the issue. In this paper, we show that with the aid of AI, it is possible for the VSP to localize the network fault without having access to the faulty part and using only QoE metrics. We collected a dataset from an actual video streaming testbed, where multiple videos are streamed from a video server through a simplified ISP network to a client network. Actual faults were generated in both the ISP and the client networks. Using only the QoE metrics measured at the client side, we use the deep learning methods of multi-layer perceptron (MLP) and long-short-term memory (LSTM) to detect and localize the fault with an accuracy of 93–97%, depending on the situation. Impact Statement—Technologically, our work impacts video/game streaming service providers such as Netflix, YouTube, Amazon Prime, Google Stadia, Sony PlayStation Now, Nvidia GeForce Now, and videoconferencing providers such as Zoom and Skype. Our work enables these providers to train similar AI systems that can localize network problems using only the video quality of experience (QoE) recorded by their client software. They can then take an appropriate action, such as rerouting traffic using Open Connect Appliances (OCA) if available, using another network provider if they have contracts with more than one, or informing the owner of the network segment with the fault, so they can fix the problem and maintain their customers’ QoE at a satisfactory level. Economically, our work can contribute to the market expansion of any video streaming solution because it will lead to better QoE, which is synonymous with more customers.},
  archive      = {J_TAI},
  author       = {Hossein Ebrahimi Dinaki and Shervin Shirmohammadi and Emil Janulewicz and David Côté},
  doi          = {10.1109/TAI.2020.3041816},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {2},
  pages        = {130-138},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep learning-based fault localization in video networks using only client-side QoE},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can shallow neural networks beat the curse of
dimensionality? A mean field training perspective. <em>TAI</em>,
<em>1</em>(2), 121–129. (<a
href="https://doi.org/10.1109/TAI.2021.3051357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that the gradient descent training of a two-layer neural network on empirical or population risk may not decrease population risk at an order faster than &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$t^{-4/(d-2)}$&lt;/tex-math&gt;&lt;/inline-formula&gt; under mean field scaling. The loss functional is mean squared error with a Lipschitz-continuous target function and data distributed uniformly on the &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$d$&lt;/tex-math&gt;&lt;/inline-formula&gt; -dimensional unit cube. Thus gradient descent training for fitting reasonably smooth, but truly high-dimensional data may be subject to the curse of dimensionality. We present numerical evidence that gradient descent training with general Lipschitz target functions becomes slower and slower as the dimension increases, but converges at approximately the same rate in all dimensions when the target function lies in the natural function space for two-layer ReLU networks. Impact Statement–Artificial neural networks perform well in many real life applications, but may suffer from the curse of dimensionality on certain problems. We provide theoretical and numerical evidence that this may be related to whether a target function lies in the hypothesis class described by infinitely wide networks. The training dynamics are considered in the fully non-linear regime and not reduced to neural tangent kernels. We believe that it will be essential to study these hypothesis classes in detail to choose an appropriate machine learning models for a given problem. The goal of the article is to illustrate this in a mathematically sound and numerically convincing fashion.},
  archive      = {J_TAI},
  author       = {Stephan Wojtowytsch and Weinan E},
  doi          = {10.1109/TAI.2021.3051357},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {2},
  pages        = {121-129},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Can shallow neural networks beat the curse of dimensionality? a mean field training perspective},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Procedural memory augmented deep reinforcement learning.
<em>TAI</em>, <em>1</em>(2), 105–120. (<a
href="https://doi.org/10.1109/TAI.2021.3054722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the human brain, we propose an external memory-augmented decision-making architecture for video processing. A self-organizing object detector is employed as a frontend to deconstruct the environment. This is done by extracting events from the flow of time and detecting objects within the frames. By employing an extra working memory where objects are temporarily stored, the system can extract properties of the stored objects related to the task. We propose a deep reinforcement learning (RL) neural network to learn affordances, i.e., a sequence of actions to manipulate these objects. The RL network and object detector are trained alternatively. After both the network and detector are trained, the objects and their affordances are transferred to an external memory. They are then utilized when the same objects are detected in input frames. Here, we use a combination of a dictionary and a linked list for the external memory that can be accessed by either content or temporal order. This dual access is motivated by the temporal property of human procedural memory. The proposed memory-augmented RL framework brings advantages of transferability, explainability and computational efficiency with respect to conventional deep learning architectures. We validate the framework on the video game Super Mario Brothers to show superiority to some classical deep RL architectures and exemplify these three advantages.},
  archive      = {J_TAI},
  author       = {Ying Ma and Joseph Brooks and Hongming Li and Jose C. Principe},
  doi          = {10.1109/TAI.2021.3054722},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {2},
  pages        = {105-120},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Procedural memory augmented deep reinforcement learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging data science to combat COVID-19: A comprehensive
review. <em>TAI</em>, <em>1</em>(1), 85–103. (<a
href="https://doi.org/10.1109/TAI.2020.3020521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.},
  archive      = {J_TAI},
  author       = {Siddique Latif and Muhammad Usman and Sanaullah Manzoor and Waleed Iqbal and Junaid Qadir and Gareth Tyson and Ignacio Castro and Adeel Razi and Maged N. Kamel Boulos and Adrian Weller and Jon Crowcroft},
  doi          = {10.1109/TAI.2020.3020521},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {1},
  pages        = {85-103},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Leveraging data science to combat COVID-19: A comprehensive review},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised deep metric learning via orthogonality based
probabilistic loss. <em>TAI</em>, <em>1</em>(1), 74–84. (<a
href="https://doi.org/10.1109/TAI.2020.3026982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning is an important problem in machine learning. It aims to group similar observations together. Existing state-of-the-art metric learning approaches require class labels to induce a metric. Sometimes it is expensive or not possible to collect these labels. In this paper, we propose an unsupervised learning approach that learns a metric without making use of class labels. The lack of class labels is compensated by obtaining pseudo-labels of data using a graph-based clustering approach. The pseudo-labels are used to form triplets of examples, which guide the metric learning process. We propose a probabilistic loss function that minimizes the chances of each triplet violating an angular constraint. A weight function and an orthogonality constraint in the objective speed up convergence and avoid a model collapse. We also provide a stochastic formulation of our method to scale up to large-scale datasets. Our studies demonstrate the competitiveness of our approach against state-of-the-art methods. Impact Statement —Adequately detecting the similarity among two observations is the essence of many Artificial Intelligence (AI) algorithms, and to an extent, impacts their success. Similarity, hence distance, depends on the manner we represent observations. Existing AI algorithms that automatically learn a good representation of data, require huge manual intervention and effort in the form of annotations. However, it is not possible in many crucial applications to obtain a large amount of manually annotated data. For example, some applications of significant technological and economic impacts, such as invasive medical imaging, insurance, and computer security, produce a huge amount of unlabeled data. We present an algorithm for this problem that when compared to a rival, increased recall between 1.4% to 6.4%.},
  archive      = {J_TAI},
  author       = {Ujjal Kr Dutta and Mehrtash Harandi and Chellu Chandra Sekhar},
  doi          = {10.1109/TAI.2020.3026982},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {1},
  pages        = {74-84},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsupervised deep metric learning via orthogonality based probabilistic loss},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Result-based re-computation for error-tolerant
classification by a support vector machine. <em>TAI</em>, <em>1</em>(1),
62–73. (<a href="https://doi.org/10.1109/TAI.2020.3028321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Machines (SVMs) are widely used in Machine Learning (ML) to perform classification. For a given element, an SVM computes a value using a kernel function and several support vectors to determine its class. Unfortunately, the computational units can be affected by externally induced phenomena such as soft errors; therefore, an erroneous result can be obtained. This is an issue when an SVM is used in safety-critical applications in which a change of the classification result is not acceptable. To ensure that errors do not change the classification result, traditional protection schemes can be used. For example, if the SVM computation is done on a processor, calculations can be executed twice, and the results get compared. If they are different, the error is detected, and the calculation can be done for a third time, voting can then be utilized to obtain the most likely result. However, this approach incurs in a large cost for computing resources and may not be acceptable when an SVM is used in resource constrained platforms such as Internet of Things (IoT) devices. In this paper, Result-based Re-computation (RBR) is proposed; RBR is an efficient technique to protect SVMs from errors in the kernel function, which is the most complex part in the SVM implementation. RBR is based on the observation that the SVM result is a sum of kernel terms to detect the terms that can modify the classification result and only these terms must be re-computed. The evaluation results using several publicly available datasets show that compared to a traditional protection scheme, the proposed RBR reduces up to 95.58% of the re-computation needed to protect an SVM against errors. Impact Statement-Error tolerance is critical in safety-critical Artificial Intelligence (AI) applications due to their dramatic consequences and safety implications. However, conventional protection solutions always incur in a large computational cost that could become prohibitive in resource-constrained platforms and application domains such as Internet of Things devices. The Algorithm-based Error Tolerance (ABET) approach for Support Vector Machines (SVMs) proposed in this paper has a significant advantage in terms of low computational demands. This advantage makes it very attractive in practice. The proposed algorithm reduces computational time with up to 95.58%, when compared to a classic protection scheme. This saving will promote the use of ABET approaches in industry, especially in embedded low-power systems.},
  archive      = {J_TAI},
  author       = {Shanshan Liu and Pedro Reviriego and Xiaochen Tang and Wei Tang and Fabrizio Lombardi},
  doi          = {10.1109/TAI.2020.3028321},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {1},
  pages        = {62-73},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Result-based re-computation for error-tolerant classification by a support vector machine},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approaches and applications of early classification of time
series: A review. <em>TAI</em>, <em>1</em>(1), 47–61. (<a
href="https://doi.org/10.1109/TAI.2020.3027279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early classification of time series has been extensively studied for minimizing class prediction delay in time-sensitive applications such as medical diagnostic and industrial process monitoring. A primary task of an early classification approach is to classify an incomplete time series as soon as possible with some desired level of accuracy. Recent years have witnessed several approaches for early classification of time series. As most approaches have solved the early classification problem using a diverse set of strategies, it becomes very important to make a thorough review of existing solutions. These solutions have demonstrated reasonable performance on a wide range of applications including human activity recognition, gene expression based health diagnostic, and industrial monitoring. In this article, we presenta-systematic review of the current literature on early classification approaches for both univariate and multivariate time series. We divide various existing approaches into four exclusive categories based on their proposed solution strategies. The four categories include prefix based, shapelet based, model based, and miscellaneous approaches. We discuss the applications of early classification and provide a quick summary of the current literature with future research directions. Impact Statement-Early classification is mainly an extension of classification with an ability to classify a time series using limited data points. It is true that one can achieve better accuracy if one waits for more data points, but opportunities for early interventions could equally be missed. In a pandemic situation such as COVID-19, early detection of an infected person becomes more desirable to curb the spread of the virus and possibly save lives. Early classification of gas (e.g., methyl isocyanate) leakage can help to avoid life-threatening consequences on human beings. Early classification techniques have been successfully applied to solve many time-critical problems related to medical diagnostic and industrial monitoring. This article provides a systematic review of the current literature on these early classification approaches for time series data, along with their potential applications. It also suggests some promising directions for further work in this area.},
  archive      = {J_TAI},
  author       = {Ashish Gupta and Hari Prabhat Gupta and Bhaskar Biswas and Tanima Dutta},
  doi          = {10.1109/TAI.2020.3027279},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {1},
  pages        = {47-61},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Approaches and applications of early classification of time series: A review},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised pose adaptation for cross-domain image
animation. <em>TAI</em>, <em>1</em>(1), 34–46. (<a
href="https://doi.org/10.1109/TAI.2020.3031581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image animation is to animate a still image of the object of interest using poses extracted from another video sequence. Through training on a large-scale video dataset, most existing approaches aim to explore disentangled appearance and pose representations of training frames. Then, the desired output with a specific appearance and pose can be synthesized via recombining learned representations. However, in some real-world applications, test images may lack the corresponding video ground-truth or follow a different distribution than the distribution of the training video frames (i.e., different domains), which largely limit the performance of existing methods. In this paper, we propose domain-independent pose representations that are compatible with and accessible by still images from a different domain. Specifically, we devise a two-stage self-supervised pose adaptation framework for general image animation tasks. A domain-independent pose adaptation generative adversarial network (DIPA-GAN) and a shuffle-patch generative adversarial network (Shuffle-patch GAN) are proposed to penalize the rationality of the synthesized frame&#39;s pose and appearance, respectively. Finally, experiments evaluated on various image animation tasks, which include same/cross-domain moving objects, facial expression transfer and human pose retargeting, demonstrate the superiority of the proposed framework over prior literature. Impact Statement-Image animation is a popular technology in video production. Benefiting from the rapid development of artificial intelligence (AI), recent image animation algorithms have been widely used in real-world applications, such as virtual AI news anchor, virtual try-on, and face swapping. However, most existing methods are designed for specific cases. To animate a new portrait, users are asked to collect hundreds of images of the same person and train a new model. The technology proposed in this paper overcomes these training limitations and generalizes image animations. In the challenging cross-domain facial expression transfer task, the user study demonstrated that our technology achieved more than 20% increase in animation success rate. The proposed technology could benefit users in a wide variety of industries including movie production, virtual reality, social media and online retail.},
  archive      = {J_TAI},
  author       = {Chaoyue Wang and Chang Xu and Dacheng Tao},
  doi          = {10.1109/TAI.2020.3031581},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {1},
  pages        = {34-46},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-supervised pose adaptation for cross-domain image animation},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Delegating decisions in strategic settings. <em>TAI</em>,
<em>1</em>(1), 19–33. (<a
href="https://doi.org/10.1109/TAI.2020.3031545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we formalise and investigate the following problem. A number of decisions must be delegated to a collection of agents; once the decisions are delegated, the agents to whom the decisions are delegated will then make these decisions rationally and independently, in pursuit of their own preferences. A principal is able to determine how decisions will be delegated, and seeks to do so in such a way that, when the decisions are ultimately made, some overall goal is satisfied. The principal delegation problem, is then, given such a setting, whether it is possible for the principal to delegate decisions in such a way that, if all the agents to whom decisions have been delegated then make their respective decisions rationally, the principal&#39;s goal will be achieved in equilibrium. We also distinguish the distributed allocation problem where the agents can delegate decisions among themselves. Here we not only require that the principal&#39;s goal will be achieved in equilibrium, but moreover that the allocation to the agents is stable, in the sense that no coalition of agents can redistribute the decisions delegated to them among themselves so as to be better positioned to satisfy their individual goals in equilibrium. We formalise these problems using Boolean games, which provides a very natural framework within which to capture the delegation problem: decisions are directly represented as Boolean variables, which the principal assigns to agents. After motivating and formally defining the principal and distributed delegation problems, we investigate these computational complexity of several varieties of these problems, along with some issues surrounding it. Impact Statement: Our research concerns the ubiquitous problem of how to incentivise agents to decide on courses of action that are simultaneously desirable from the global perspective and rational from a local game-theoretic perspective. The design of mechanisms so as to achieve this is through steering the strategic capabilities of interested agents by delegating decisions to them in specific ways. We distinguish the cases in which the delegation is effectuated by a principal and where the agents are delegating decisions among one another. Our reliance on the mathematical framework of Boolean games to formalise the principal and distributed delegation problems offers valuable insights into the computational complexity of these problems. The delegation problem touches on many important applications, not only in everyday life, but also in Artificial Intelligence, in particular Multi-Agent Systems.},
  archive      = {J_TAI},
  author       = {Paul E. Dunne and Paul Harrenstein and Sarit Kraus and Michael Wooldridge},
  doi          = {10.1109/TAI.2020.3031545},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {1},
  pages        = {19-33},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Delegating decisions in strategic settings},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A recommender system for metaheuristic algorithms for
continuous optimization based on deep recurrent neural networks.
<em>TAI</em>, <em>1</em>(1), 5–18. (<a
href="https://doi.org/10.1109/TAI.2020.3022339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As revealed by the no free lunch theorem, no single algorithm can outperform any others on all classes of optimization problems. To tackle this issue, methods for recommending an existing algorithm for solving given problems have been proposed. However, existing recommendation methods for continuous optimization suffer from low practicability and transferability, mainly due to the difficulty in extracting features that can effectively describe the problem structure and lack of data for training a recommendation model. This work proposes a generic recommender system to address the above two challenges. First, a novel method is proposed to represent an analytic objective function of a continuous optimization problem as a tree, which is directly used as the features of the problem. For black-box optimization problems whose objective function is unknown, a symbolic regressor is adopted to estimate the tree structure. Second, a large number of benchmark problems are randomly created based on the proposed tree representation, providing an abundant amount of training data with various levels of difficulty. By employing a deep recurrent neural network, a recommendation model is trained to recommend a most suitable metaheuristic algorithm for white- or black-box optimization, making a significant step forward towards fully automated algorithm recommendation for continuous optimization. Experimental results on 100 000 benchmark problems show that the proposed recommendation model achieves considerably better performance than existing ones, and exhibits high transferability to real-world problems.},
  archive      = {J_TAI},
  author       = {Ye Tian and Shichen Peng and Xingyi Zhang and Tobias Rodemann and Kay Chen Tan and Yaochu Jin},
  doi          = {10.1109/TAI.2020.3022339},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {1},
  pages        = {5-18},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A recommender system for metaheuristic algorithms for continuous optimization based on deep recurrent neural networks},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial: Artificial intelligence: A new transactions.
<em>TAI</em>, <em>1</em>(1), 2–4. (<a
href="https://doi.org/10.1109/TAI.2020.3035827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TAI},
  author       = {Hussein Abbass},
  doi          = {10.1109/TAI.2020.3035827},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {1},
  pages        = {2-4},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Editorial: artificial intelligence: a new transactions},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
