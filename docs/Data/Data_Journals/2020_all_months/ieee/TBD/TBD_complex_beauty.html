<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TBD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tbd---68">TBD - 68</h2>
<ul>
<li><details>
<summary>
(2020). GraphMP: I/o-efficient big graph analytics on a single
commodity machine. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(4), 816–829. (<a
href="https://doi.org/10.1109/TBDATA.2019.2908384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent studies showed that single-machine graph processing systems can be as highly competitive as cluster-based approaches on large-scale problems. While several out-of-core graph processing systems and computation models have been proposed, the high disk I/O overhead could significantly reduce performance in many practical cases. In this paper, we propose GraphMP to tackle big graph analytics on a single machine. GraphMP achieves low disk I/O overhead with three techniques. First, we design a vertex-centric sliding window (VSW) computation model to avoid reading and writing vertices on disk. Second, we propose a selective scheduling method to skip loading and processing unnecessary edge shards on disk. Third, we use a compressed edge cache mechanism to fully utilize the available memory of a machine to reduce the amount of disk accesses for edges. Extensive evaluations have shown that GraphMP could outperform existing single-machine out-of-core systems such as GraphChi, X-Stream and GridGraph by up to 30, and can be as highly competitive as distributed graph engines like Pregel+, PowerGraph and Chaos.},
  archive  = {J},
  author   = {Peng Sun and Yonggang Wen and Ta Nguyen Binh Duong and Xiaokui Xiao},
  doi      = {10.1109/TBDATA.2019.2908384},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {816-829},
  title    = {GraphMP: I/O-efficient big graph analytics on a single commodity machine},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). P-MOD: Secure privilege-based multilevel organizational
data-sharing in cloud computing. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(4), 804–815. (<a
href="https://doi.org/10.1109/TBDATA.2019.2907133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cloud computing has changed the way enterprises store, access and share data. Big data sets are constantly being uploaded to the cloud and shared within a hierarchy of many different individuals with different access privileges. With more data storage needs turning over to the cloud, finding a secure and efficient data access structure has become a major research issue. In this paper, a Privilege-based Multilevel Organizational Data-sharing scheme (P-MOD) is proposed that incorporates a privilege-based access structure into an attribute-based encryption mechanism to handle the management and sharing of big data sets. Our proposed privilege-based access structure helps reduce the complexity of defining hierarchies as the number of users grows, which makes managing healthcare records using mobile healthcare devices feasible. It can also facilitate organizations in applying big data analytics to understand populations in a holistic way. Security analysis shows that P-MOD is secure against adaptively chosen plaintext attack assuming the DBDH assumption holds. The comprehensive performance and simulation analyses using the real U.S. Census Income data set demonstrate that P-MOD is more efficient in computational complexity and storage space than the existing schemes.},
  archive  = {J},
  author   = {Ehab Zaghloul and Kai Zhou and Jian Ren},
  doi      = {10.1109/TBDATA.2019.2907133},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {804-815},
  title    = {P-MOD: Secure privilege-based multilevel organizational data-sharing in cloud computing},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient provenance management via clustering and hybrid
storage in big data environments. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(4), 792–803. (<a
href="https://doi.org/10.1109/TBDATA.2019.2907116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Provenance is a type of metadata that records the creation and transformation of data objects. It has been applied to a wide variety of areas such as security, search, and experimental documentation. However, provenance usually has a vast amount of data with its rapid growth rate which hinders the effective extraction and application of provenance. This paper proposes an efficient provenance management system via clustering and hybrid storage. Specifically, we propose a Provenance-Based Label Propagation Algorithm which is able to regularize and cluster a large number of irregular provenance. Then, we use separate physical storage mediums, such as SSD and HDD, to store hot and cold data separately, and implement a hot/cold scheduling scheme which can update and schedule data between them automatically. Besides, we implement a feedback mechanism which can locate and compress the rarely used cold data according to the query request. The experimental test shows that the system can significantly improve provenance query performance with a small run-time overhead.},
  archive  = {J},
  author   = {Die Hu and Dan Feng and Yulai Xie and Gongming Xu and Xinrui Gu and Darrell Long},
  doi      = {10.1109/TBDATA.2019.2907116},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {792-803},
  title    = {Efficient provenance management via clustering and hybrid storage in big data environments},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning a unified blind image quality metric via on-line
and off-line big training instances. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(4), 780–791. (<a
href="https://doi.org/10.1109/TBDATA.2019.2895605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we resolve a big challenge that most current image quality metrics (IQMs) are unavailable across different image contents, especially simultaneously coping with natural scene (NS) images or screen content (SC) images. By comparison with existing works, this paper deploys on-line and off-line data for proposing a unified no-reference (NR) IQM, not only applied to different distortion types and intensities but also to various image contents including classical NS images and prevailing SC images. Our proposed NR IQM is developed with two data-driven learning processes following feature extraction, which is based on scene statistic models, free-energy brain principle, and human visual system (HVS) characteristics. In the first process, the scene statistic models and an image retrieve technique are combined, based on on-line and off-line training instances, to derive a novel loose classifier for retrieving clean images and helping to infer the image content. In the second process, the features extracted by incorporating the inferred image content, free-energy and low-level perceptual characteristics of the HVS are learned by utilizing off-line training samples to analyze the distortion types and intensities and thereby to predict the image quality. The two processes mentioned above depend on a gigantic quantity of training data, much exceeding the number of images applied to performance validation, and thus make our model&#39;s performance more reliable. Through extensive experiments, it has been validated that the proposed blind IQM is capable of simultaneously inferring the quality of NS and SC images, and it has attained superior performance as compared with popular and state-of-the-art IQMs on the subjective NS and SC image quality databases. The source code of our model will be released with the publication of the paper at https://kegu.netlify.com.},
  archive  = {J},
  author   = {Ke Gu and Xin Xu and Junfei Qiao and Qiuping Jiang and Weisi Lin and Daniel Thalmann},
  doi      = {10.1109/TBDATA.2019.2895605},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {780-791},
  title    = {Learning a unified blind image quality metric via on-line and off-line big training instances},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer to rank for top-n recommendation. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(4), 770–779. (<a
href="https://doi.org/10.1109/TBDATA.2019.2892478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we study top-N recommendation by exploiting users&#39; explicit feedback such as 5-star numerical ratings, which has been overlooked to some extent in the past decade. As a response, we design a novel and generic transfer learning based recommendation framework coarse-to-fine transfer to rank (CoFiToR), which is a significant extension of a very recent work called transfer to rank (ToR). The key idea of our solution is modeling users&#39; behaviors by simulating users&#39; shopping processes. Therefore, we convert the studied ranking problem to three subtasks corresponding to three specific questions, including (i) whether an item will be examined by a user, (ii) how an item will be scored by a user, and (iii) whether an item will finally be purchased by a user. Based on this new conversion, we then develop a three-staged solution that progressively models users&#39; preferences from a coarse granularity to a fine granularity. In each stage, we adopt an appropriate recommendation algorithm with pointwise or pairwise preference assumption to answer each question in order to seek an effective and efficient overall solution. Empirical studies on two large and public datasets showcase the merits of our solution in comparison with the state-of-the-art methods.},
  archive  = {J},
  author   = {Wei Dai and Qing Zhang and Weike Pan and Zhong Ming},
  doi      = {10.1109/TBDATA.2019.2892478},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {770-779},
  title    = {Transfer to rank for top-N recommendation},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable triangle discovery algorithm for large scale-free
network with limited internal memory. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(4), 757–769. (<a
href="https://doi.org/10.1109/TBDATA.2018.2889120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In complex network, the links among the nodes fabricate thousands of polygons, where triangles are the most foundamental ones. As more links are added to the network, more polygons are split to be triangles. Discovering the triangles can help understand the current status of the network. Due to the limitations of internal memory, discovering all the triangles in a large-scale real-world network, e.g., online social network (OSN), is challenging and has intrigued researchers for years. Currently, parallel computation frameworks, e.g., MapReduce, have been employed to address this issue. The last reducer is required to load all the neighbors of each node into internal memory, which causes “the curse of the last reducer.” The scale-free characteristic in real-world networks furthur exacerbate the applicability of the parallel triangle discovery algorithms. The current efforts on this issue mainly focus on dividing a large network into several small subnetworks and later employ internal memory algorithms. In this paper, we address this issue from a different perspective by employing the power-law degree distribution in scale-free networks. We theoretically and empirically prove that one node in scale-free network has a limited number of neighbors with a higher degree. Employing this finding, an novel parallel triangle discovery algorithm called degree partition (DePart) is designed. Since only the portion of neighbors with higher degrees are loaded into internal memory in the reducers, DePart solves “the curse of the last reducer” problem and works efficiently in clusters with limited internal memory. We develop DePart using MapReduce framework and examine the performance of DePart on five large real-world networks. The empirical testing results demonstrate that DePart has a favorable tradeoff between internal memory and total disk space and outperforms the current state-of-the-art MapReduce-based triangle discovery algorithms.},
  archive  = {J},
  author   = {Xiaoping Zhou and Xun Liang and Zhi Tang},
  doi      = {10.1109/TBDATA.2018.2889120},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {757-769},
  title    = {Scalable triangle discovery algorithm for large scale-free network with limited internal memory},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Product supply optimization for crowdfunding campaigns.
<em>IEEE Transactions on Big Data</em>, <em>6</em>(4), 741–756. (<a
href="https://doi.org/10.1109/TBDATA.2018.2889479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent years have witnessed the rapid development of Finance Internet platforms, specifically, crowdfunding, which is for creators designing campaigns (projects) to collect funds from the public. Usually, the limited budget of a creator is manually divided into several perks (reward options), which should fit various market demand and further bring different monetary contributions for the campaign. Therefore, it is very challenging for each creator to design an effective allocation of perks when launching a campaign. Indeed, our aim is to enhance the funding performance of newly proposed campaigns, with a focus on optimizing the product supply of perks. In this paper, given the expected budget and the perks of a campaign, we propose a novel solution to automatically recommend the optimal product supply for balancing the expected return of this campaign against the risk. Along this line, we define it as a constrained portfolio selection problem, where the investment volume is measured by a multi-task learning method, and we adopt two kinds of methods for task splitting. Furthermore, we extend the investment volume prediction model with inner competition for capturing the competitive relationship between the perks in one campaign. Finally, extensive experiments on real-world crowdfunding data clearly prove the performance significantly.},
  archive  = {J},
  author   = {Guifeng Wang and Qi Liu and Hongke Zhao and Chuanren Liu and Tong Xu and Enhong Chen},
  doi      = {10.1109/TBDATA.2018.2889479},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {741-756},
  title    = {Product supply optimization for crowdfunding campaigns},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A data-driven unified framework for predicting citation
dynamics. <em>IEEE Transactions on Big Data</em>, <em>6</em>(4),
727–740. (<a href="https://doi.org/10.1109/TBDATA.2018.2884505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the rising interest in predicting the scientific output, various efforts have been made to predict a scientist&#39;s h-index or the citation trajectory of a publication. In this work, we employ a dynamic categorization for scientists to ensure at each stage of their careers a comparison amongst their peers and combine this grouping with predictive models to estimate a scientist&#39;s future impact, as expressed by citation counts. Moreover, we investigate a wide range of factors identifying their importance in determining the future of science for different performance and academic levels with particular emphasis on features describing a scholar&#39;s position in multi-layered collaboration and citation networks. The robustness of the approach is examined on a longitudinal dataset centered around 700,302 data points representing Computer Scientists in various time periods with their complete networks of over 18 million collaboration links and 36 million citations. Our results indicate up to 30 percent improvement in prediction performance compared to baseline methods along with an average R 2 =0.96 for short term and R 2 =0.91 for long term predictions.},
  archive  = {J},
  author   = {Antonia Gogoglou and Yannis Manolopoulos},
  doi      = {10.1109/TBDATA.2018.2884505},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {727-740},
  title    = {A data-driven unified framework for predicting citation dynamics},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards matching user mobility traces in large-scale
datasets. <em>IEEE Transactions on Big Data</em>, <em>6</em>(4),
714–726. (<a href="https://doi.org/10.1109/TBDATA.2018.2871693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The problem of unicity and reidentifiability of records in large-scale databases has been studied in different contexts and approaches, with focus on preserving privacy or matching records from different data sources. With an increasing number of service providers nowadays routinely collecting location traces of their users on unprecedented scales, there is a pronounced interest in the possibility of matching records and datasets based on spatial trajectories. Extending previous work on reidentifiability of spatial data and trajectory matching, we present the first large-scale analysis of user matchability in real mobility datasets on realistic scales, i.e. among two datasets that consist of several million people&#39;s mobility traces, coming from a mobile network operator and transportation smart card usage. We extract the relevant statistical properties which influence the matching process and analyze their impact on the matchability of users. We show that for individuals with typical activity in the transportation system (those making 3-4 trips per day on average), a matching algorithm based on the co-occurrence of their activities is expected to achieve a 16.8 percent success only after a one-week long observation of their mobility traces, and over 55 percent after four weeks. We show that the main determinant of matchability is the expected number of co-occurring records in the two datasets. Finally, we discuss different scenarios in terms of data collection frequency and give estimates of matchability over time. We show that with higher frequency data collection becoming more common, we can expect much higher success rates in even shorter intervals.},
  archive  = {J},
  author   = {Dániel Kondor and Behrooz Hashemian and Yves-Alexandre de Montjoye and Carlo Ratti},
  doi      = {10.1109/TBDATA.2018.2871693},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {714-726},
  title    = {Towards matching user mobility traces in large-scale datasets},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable uncertainty-aware truth discovery in big data
social sensing applications for cyber-physical systems. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(4), 702–713. (<a
href="https://doi.org/10.1109/TBDATA.2017.2669308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social sensing is a new big data application paradigm for Cyber-Physical Systems (CPS), where a group of individuals volunteer (or are recruited) to report measurements or observations about the physical world at scale. A fundamental challenge in social sensing applications lies in discovering the correctness of reported observations and reliability of data sources without prior knowledge on either of them. We refer to this problem as truth discovery. While prior studies have made progress on addressing this challenge, two important limitations exist: (i) current solutions did not fully explore the uncertainty aspect of human reported data, which leads to sub-optimal truth discovery results; (ii) current truth discovery solutions are mostly designed as sequential algorithms that do not scale well to large-scale social sensing events. In this paper, we develop a Scalable Uncertainty-Aware Truth Discovery (SUTD) scheme to address the above limitations. The SUTD scheme solves a constraint estimation problem to jointly estimate the correctness of reported data and the reliability of data sources while explicitly considering the uncertainty on the reported data. To address the scalability challenge, the SUTD is designed to run a Graphic Processing Unit (GPU) with thousands of cores, which is shown to run two to three orders of magnitude faster than the sequential truth discovery solutions. In evaluation, we compare our SUTD scheme to the state-of-the-art solutions using three real world datasets collected from Twitter: Paris Attack, Oregon Shooting, and Baltimore Riots, all in 2015. The evaluation results show that our new scheme significantly outperforms the baselines in terms of both truth discovery accuracy and execution time.},
  archive  = {J},
  author   = {Chao Huang and Dong Wang and Nitesh V. Chawla},
  doi      = {10.1109/TBDATA.2017.2669308},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {702-713},
  title    = {Scalable uncertainty-aware truth discovery in big data social sensing applications for cyber-physical systems},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Control of large-scale cyber-physical systems with agents
having various dynamics. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(4), 691–701. (<a
href="https://doi.org/10.1109/TBDATA.2017.2664892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, a controller design methodology is developed for large-scale cyber-physical systems (CPSs) with agents having various dynamics. Especially, we propose a representative model of much lower order than the scale of the original CPS. This model facilitates designing controllers for large-scale CPSs. We derive necessary and sufficient conditions such that controllers designed for the proposed representative model stabilize the original CPS with given performance. The representative model is in the form of a nominal model with a variation of the agent models. This result shows that bad behavior of a few agents does not affect the stability or performance of the original system very much. Finally, the effectiveness of the proposed method is illustrated by simulation for a power grid.},
  archive  = {J},
  author   = {Kazunori Sakurama},
  doi      = {10.1109/TBDATA.2017.2664892},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {691-701},
  title    = {Control of large-scale cyber-physical systems with agents having various dynamics},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On physical-social-aware localness inference by exploring
big data from location-based services. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(4), 679–690. (<a
href="https://doi.org/10.1109/TBDATA.2017.2726551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A user&#39;s localness (i.e., whether a user is a local resident in a city or not) and a venue&#39;s local attractiveness (i.e., the likelihood of a venue to attract local people) are important information for many location-based applications related with Cyber-Physical Systems (CPS), such as participatory sensing, urban planning, traffic control and localized travel recommendations. Previous effort has been devoted to geo-locating users in a city using supervised learning approaches, which depend on the availability of high quality training datasets. However, it is difficult to obtain such training datasets in the real-world CPS applications due to the issue of privacy. In this work, we develop an unsupervised approach, called a Physical-Social-Aware Inference (PSAI) scheme, to jointly infer a user&#39;s localness and a venue&#39;s local attractiveness by exploring both the physical and social information embedded in the location-based social networks (LBSN). We further implement a parallel PSAI framework on the platform of a Graphic Processing Unit (GPU) to enhance its ability to process large-scale data. Our extensive experiments on the real-world LBSN datasets demonstrate the effectiveness and efficiency of the PSAI scheme compared to the state-of-the-art baselines.},
  archive  = {J},
  author   = {Chao Huang and Dong Wang and Jun Tao and Brian Mann},
  doi      = {10.1109/TBDATA.2017.2726551},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {679-690},
  title    = {On physical-social-aware localness inference by exploring big data from location-based services},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-order distributed HOSVD with its incremental
computing for big services in cyber-physical-social systems. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(4), 666–678. (<a
href="https://doi.org/10.1109/TBDATA.2018.2824303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Big service is an extremely important application of service computing to provide predictive and needed services to humans. To operationalize big services, the heterogeneous data collected from Cyber-Physical-Social Systems (CPSS) must be processed efficiently. However, because of the rapid rise in the volume of data, faster and more efficient computational techniques are required. Therefore, in this paper, we propose a multi-order distributed high-order singular value decomposition method (MDHOSVD) with its incremental computational algorithm. To realize the MDHOSVD, a tensor blocks unfolding integration regulation is proposed. This method allows for the efficient analysis of large-scale heterogeneous data in blocks in an incremental fashion. Using simulation and experimental results from real-life, the high-efficiency of the proposed data processing and computational method, is demonstrated. Further, a case study about cyber-physical-social system data processing is illustrated. The proposed MDHOSVD method speeds up data processing, scales with data volume, improves the adaptability and extensibility over data diversity and converts low-level data into actionable knowledge.},
  archive  = {J},
  author   = {Xiaokang Wang and Laurence T. Yang and Xingyu Chen and Lizhe Wang and Rajiv Ranjan and Xiaodao Chen and M. Jamal Deen},
  doi      = {10.1109/TBDATA.2018.2824303},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {666-678},
  title    = {A multi-order distributed HOSVD with its incremental computing for big services in cyber-physical-social systems},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event detection through differential pattern mining in
cyber-physical systems. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(4), 652–665. (<a
href="https://doi.org/10.1109/TBDATA.2017.2731838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Extracting knowledge from sensor data for various purposes has received a great deal of attention by the data mining community. For the purpose of event detection in cyber-physical systems (CPS), e.g., damage in building or aerospace vehicles from the continuous arriving data is challenging due to the detection quality. Traditional data mining schemes are used to reduce data that often use metrics, association rules, and binary values for frequent patterns as indicators for finding interesting knowledge about an event. However, these may not be directly applicable to the network due to certain constraints (communication, computation, bandwidth). We discover that, the indicators may not reveal meaningful information for event detection in practice. In this paper, we propose a comprehensive data mining framework for event detection in the CPS named DPminer, which functions in a distributed and parallel manner (data in a partitioned database processed by one or more sensor processors) and is able to extract a pattern of sensors that may have event information with a low communication cost. To achieve this, we introduce a new sensor behavioral pattern mining technique called differential sensor pattern (DSP) which considers different frequencies and values (non-binary) with a set of sensors, instead of traditional binary patterns. We present an algorithm for data preparation and then use a highly-compact data tree structure (called DP-Tree) for generating the DSP. An important tradeoff between the communication and computation costs for the event detection via data mining is made. Evaluation results show that DPminer can be very useful for networked sensing with a superior performance in terms of communication cost and event detection quality compared to existing data mining schemes.},
  archive  = {J},
  author   = {Md Zakirul Alam Bhuiyan and Jie Wu and Gary M. Weiss and Thaier Hayajneh and Tian Wang and Guojun Wang},
  doi      = {10.1109/TBDATA.2017.2731838},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {652-665},
  title    = {Event detection through differential pattern mining in cyber-physical systems},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mathematical programming formulation for optimal load
shifting of electricity demand for the smart grid. <em>IEEE Transactions
on Big Data</em>, <em>6</em>(4), 638–651. (<a
href="https://doi.org/10.1109/TBDATA.2016.2639528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We describe the background and an analytical framework for a mathematical optimization model for home energy management systems (HEMS) to manage electricity demand on the smart grid by efficiently shifting electricity loads of households from peak times to off-peak times. We illustrate the flexibility of the model by modularizing various available technologies such as plug-in electric vehicles, battery storage, and automatic windows. First, the analysis shows that the end-user can accrue economic benefits by shifting consumer loads away from higher-priced periods. Specifically, we assessed the most likely sources of value to be derived from demand response technologies. Therefore, wide adoption of such modeling could create significant cost savings for consumers. Second, the findings are promising for the further development of more intelligent HEMS in the residential sector. Third, we formulated a smart grid valuation framework that is helpful for interpreting the model&#39;s results concerning the efficiency of current smart appliances and their respective prices. Finally, we explain the model&#39;s benefits, the major concerns when the model is applied in the real world, and the possible future areas that can be explored.},
  archive  = {J},
  author   = {R. Lily Hu and Ryan Skorupski and Robert Entriken and Yinyu Ye},
  doi      = {10.1109/TBDATA.2016.2639528},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {638-651},
  title    = {A mathematical programming formulation for optimal load shifting of electricity demand for the smart grid},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatiotemporal data summarization approach for real-time
operation of smart grid. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(4), 624–637. (<a
href="https://doi.org/10.1109/TBDATA.2017.2691350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In a smart grid distribution management system, operation, planning, forecasting and decision making relies on demand-side management functions, which require real-time smart grid data. This data has significant dollar value because it is extremely useful for efficient control and intelligent prediction of the energy consumption, and expert management of residential and commercial load. However, the huge amount of (smart grid) data generated at a very high velocity poses a number of challenges. Utility companies have a huge demand for efficient summarization techniques to mine interesting patterns and extracting useful and actionable intelligence. Research from various domains has shown that data summarization can significantly improve the scalability and efficiency of various data analytic tasks (e.g., transactional database mining, data streams mining, network monitoring). This paper proposes a summarization approach (i.e., a set of algorithms, data structures, and query mechanisms) that enables the utility company to accurately infer various energy consumption patterns in real-time by automatic monitoring of smart grid data using significantly less computational resources. The proposed summarization approach is suitable for processing spatiotemporal streams, and it can also provide answers in real-time to various smart grid applications (e.g., demand-side management, direct load control, smart pricing and Volt-VAr control). Both theoretical bound and experimental evaluation are presented in this paper, which shows that the memory required for the proposed data structure grows linearly for the first 52 weeks; but interestingly, after the first year, the memory growth is negligible. The experimental results show that the proposed approach can process around 4 million smart meter readings every second or 120 million readings every minute. The proposed approach outperforms widely commercially used Database Management Systems (DBMSs) in terms of update and query costs: it is about 200 times faster than DBMSs in terms of update time, and about 340 times faster than DBMSs in terms of query time.},
  archive  = {J},
  author   = {Zubair Shah and Adnan Anwar and Abdun Naser Mahmood and Zahir Tari and Albert Y. Zomaya},
  doi      = {10.1109/TBDATA.2017.2691350},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {624-637},
  title    = {A spatiotemporal data summarization approach for real-time operation of smart grid},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BioCyBig: A cyberphysical system for integrative
microfluidics-driven analysis of genomic association studies. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(4), 609–623. (<a
href="https://doi.org/10.1109/TBDATA.2016.2643683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a research vision to design a large-scale cyberphysical systems (CPS) experimental framework to enable collaborative and coordinated molecular biology studies. This framework will be based on the integration of CPS with microfluidic biochips and cloud computing. It has the potential to drastically advance personalized medicine through knowledge fusion among many research groups, and synchronization of research planning. This framework therefore leads to a better understanding of diseases such as cancer, and helps researchers in identifying effective treatments. A case study from cancer research is discussed to explain the significance of our framework in promoting coordinated genomic studies.},
  archive  = {J},
  author   = {Mohamed Ibrahim and Krishnendu Chakrabarty and Jun Zeng},
  doi      = {10.1109/TBDATA.2016.2643683},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {609-623},
  title    = {BioCyBig: A cyberphysical system for integrative microfluidics-driven analysis of genomic association studies},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Big data for cyber-physical systems. <em>IEEE Transactions
on Big Data</em>, <em>6</em>(4), 606–608. (<a
href="https://doi.org/10.1109/TBDATA.2020.3033101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cyber-physical systems (CPS) are characterized by deep and complex intertwining among cyber components and physical components. Due to the fast increase in system complexities, the operations of CPS involve sensing, processing and storage of massive amount of data. This nature of “big data” imposes fundamental challenges on the design and management of CPS in multiple aspects such as performance, energy efficiency, security, privacy, reliability, sustainability, fault tolerance, scalability and flexibility. Tackling these challenges necessitates innovative big data techniques for handling massive data in CPS. The articles in this special section include a few selected state-of-the-art research results on the topic of big data sensing, processing and storage for CPS, and stimulates a broad range of researchers to participate in the interdisciplinary CPS research in the future. This special issue has received a significant number of submissions while only a small portion of them are selected for publications. The selected papers showcase how interesting data analytics techniques can be leveraged to optimize different metrics in CPS, such as timing, efficiency, schedulability, power, reliability, and security, etc.},
  archive  = {J},
  author   = {Shiyan Hu and Xin Li and Haibo He and Shuguang Cui and Manish Parashar},
  doi      = {10.1109/TBDATA.2020.3033101},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {4},
  pages    = {606-608},
  title    = {Big data for cyber-physical systems},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural network-based impacts analysis of multimodal
factors on heat demand prediction. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(3), 594–605. (<a
href="https://doi.org/10.1109/TBDATA.2019.2907127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Prediction of heat demand using artificial neural networks has attracted enormous research attention. Weather conditions, such as direct solar irradiance and wind speed, have been identified as key parameters affecting heat demand. This paper employs an Elman neural network to investigate the impacts of direct solar irradiance and wind speed on the heat demand from the perspective of the entire district heating network. Results of the overall mean absolute percentage error (MAPE) show that direct solar irradiance and wind speed have quite similar impacts. However, the involvement of direct solar irradiance can clearly reduce the maximum absolute deviation when only involving direct solar irradiance and wind speed, respectively. In addition, the simultaneous involvement of both wind speed and direct solar irradiance does not show an obvious improvement of MAPE. Moreover, the prediction accuracy can also be affected by other factors like data discontinuity and outliers.},
  archive  = {J},
  author   = {Zhanyu Ma and Jiyang Xie and Hailong Li and Qie Sun and Fredrik Wallin and Zhongwei Si and Jun Guo},
  doi      = {10.1109/TBDATA.2019.2907127},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {594-605},
  title    = {Deep neural network-based impacts analysis of multimodal factors on heat demand prediction},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring spring onset at continental scales: Mapping
phenoregions and correlating temperature and satellite-based
phenometrics. <em>IEEE Transactions on Big Data</em>, <em>6</em>(3),
583–593. (<a href="https://doi.org/10.1109/TBDATA.2019.2926292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Each spring many plants put on new leaves and/or open their flowers creating a “green-wave” that can be tracked using phenological data. Various phenological datasets can be used to study spring onset at continental to global scales. Here we present a novel exploratory analysis where we link two multi-decadal and high-spatial resolution datasets: temperature-based phenological indices and land surface phenological metrics derived from satellite images. Our exploratory analysis, illustrated with data for the conterminous US, focuses on identifying regions with similar spring onset, and on mapping the coherence between these phenological products. Our results show that the spring onset patterns captured by the satellite are more complex than the ones identified using temperature-based phenological indices. They also highlight areas with stable and unstable spring onsets (i.e., areas that tend to remain or change of phenoregion from year to year). Finally, our results reveal that temperature-based indices are both positively and negatively correlated with the phenological information that can be derived from satellites. This opens the door to the definition of rules to integrate multi-source phenological data. To cope with the computational challenges of analyzing big geospatial rasters, we executed our analysis on a cloud platform running Apache Spark and various of its extensions (e.g., Geotrellis, SparkMLlib). This platform performed well and allowed the execution of user-tailored analyses. Hence, we believe that our computational platform paves the path towards the efficient analysis of global vegetation phenology at very high spatial resolution and, more generally, to the analysis of the ever-increasing collections of geospatial data about our planet.},
  archive  = {J},
  author   = {Raul Zurita-Milla and Romulo Goncalves and Emma Izquierdo-Verdiguier and Frank O. Ostermann},
  doi      = {10.1109/TBDATA.2019.2926292},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {583-593},
  title    = {Exploring spring onset at continental scales: Mapping phenoregions and correlating temperature and satellite-based phenometrics},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cloud approach to automated crop classification using
sentinel-1 imagery. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(3), 572–582. (<a
href="https://doi.org/10.1109/TBDATA.2019.2940237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For accurate crop classification, it is necessary to use time-series of high-resolution satellite data to better discriminate among certain crop types. This task brings the following challenges: a large amount of satellite data for download, Big data processing and computational resources for utilization of state-of-the-art classification approaches. For solving these problems, we have developed an automated crop classification workflow, which is based on machine-learning techniques. By deployment of the workflow on the cloud platform, we can overcome challenges of Big data downloading and processing. In this paper, we present the system architecture and describe the experiments on structural and parametric identification of machine learning models utilized in the system.},
  archive  = {J},
  author   = {Andrii Shelestov and Mykola Lavreniuk and Vladimir Vasiliev and Leonid Shumilo and Andrii Kolotii and Bohdan Yailymov and Nataliia Kussul and Hanna Yailymova},
  doi      = {10.1109/TBDATA.2019.2940237},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {572-582},
  title    = {Cloud approach to automated crop classification using sentinel-1 imagery},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). National scale surface deformation time series generation
through advanced DInSAR processing of sentinel-1 data within a cloud
computing environment. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(3), 558–571. (<a
href="https://doi.org/10.1109/TBDATA.2018.2863558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an automatic pipeline implemented within the Amazon Web Services (AWS) Cloud Computing platform for the interferometric processing of large Sentinel-1 (S1) multi-temporal SAR datasets, aimed at analyzing Earth surface deformation phenomena at wide spatial scale. The developed processing chain is based on the advanced DInSAR approach referred to as Small BAseline Subset (SBAS) technique, which allows producing, with centimeter to millimeter accuracy, surface deformation time series and the corresponding mean velocity maps from a temporal sequence of SAR images. The implemented solution addresses the aspects relevant to i) S1 input data archiving; ii) interferometric processing of S1 data sequences, performed in parallel on the AWS computing nodes through both multi-node and multi-core programming techniques; iii) storage of the generated interferometric products. The experimental results are focused on a national scale DInSAR analysis performed over the whole Italian territory by processing 18 S1 slices acquired from descending orbits between March 2015 and April 2017, corresponding to 2612 S1 acquisitions. Our analysis clearly shows that an effective integration of advanced remote sensing methods and new ICT technologies can successfully contribute to deeply investigate the Earth System processes and to address new challenges within the Big Data EO scenario.},
  archive  = {J},
  author   = {I. Zinno and M. Bonano and S. Buonanno and F. Casu and C. De Luca and M. Manunta and M. Manzo and R. Lanari},
  doi      = {10.1109/TBDATA.2018.2863558},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {558-571},
  title    = {National scale surface deformation time series generation through advanced DInSAR processing of sentinel-1 data within a cloud computing environment},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mosaicking copernicus sentinel-1 data at global scale.
<em>IEEE Transactions on Big Data</em>, <em>6</em>(3), 547–557. (<a
href="https://doi.org/10.1109/TBDATA.2018.2846265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a processing chain for handling big volume of remotely sensing data for generating wide extent mosaics. More specifically, the data under consideration are level-1 ground range detected Sentinel-1 products with dual polarisation (VV+VH or HH+HV). Two approaches for a) distribution discretization accompanied by false color composition and b) image rendering and mosaicking are proposed. While these two components are necessary constituents of the presented mosaicking workflow, they can operate independently of each other. The design of the processing chain satisfies three objectives: i) contrasting derivative products of the input Sentinel-1 imagery such as the Global Human Settlement Layer, ii) adapting on a high-throughput computing system for fast execution, and iii) allowing potential extensions to more complex applications such as the image classification. Fast processing, process automation, incremental adjustment and information distinction are the main advantages of the proposed method. Elaboration and focus on these features are carried out during the presentation of the results.},
  archive  = {J},
  author   = {Vasileios Syrris and Christina Corbane and Martino Pesaresi and Pierre Soille},
  doi      = {10.1109/TBDATA.2018.2846265},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {547-557},
  title    = {Mosaicking copernicus sentinel-1 data at global scale},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A time series mining approach for agricultural area
detection. <em>IEEE Transactions on Big Data</em>, <em>6</em>(3),
537–546. (<a href="https://doi.org/10.1109/TBDATA.2019.2913402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Acquiring meaningful data to be employed in building training sets for classification models is a costly task, both in terms of difficult to find suitable samples as well as their quantity. In this sense, Active Learning (AL) improves the training set building by providing an efficient way to select only essential data to be attached to the training set, consequently reducing its size and even enhancing model&#39;s accuracy, when compared to random sample selection. In this paper, we proposed a framework for time series classification in order to monitor sugarcane area in São Paulo, Brazil. The AL approach consisted of selecting seasonal time series information from less than 1 percent of each class&#39; pixels to build the training set and evaluate this selection by an expert user supported by distance measurements, repeating this process until both distance measurement thresholds were satisfied. In most years, the classification results presented about 90 percent of correlation with official estimates based on both traditional and satellite image analysis methods. This framework can then help Land Use Change (LUC) monitoring as it produced similar results compared to other methods that demands more human and financial resources to be adopted.},
  archive  = {J},
  author   = {João Paulo Da Silva and Jurandir Zullo and Luciana Alvim Santos Romani},
  doi      = {10.1109/TBDATA.2019.2913402},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {537-546},
  title    = {A time series mining approach for agricultural area detection},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining deep semantic representations for scene
classification of high-resolution remote sensing imagery. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(3), 522–536. (<a
href="https://doi.org/10.1109/TBDATA.2019.2916880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Scene classification is one of the most fundamental task in interpretation of high-resolution remote sensing (HRRS) images. Many recent works show that the probabilistic topic models which are capable of mining latent semantics of images can be effectively applied to HRRS scene classification. However, the existing approaches based on topic models simply utilize low-level hand-crafted features to form semantic features, which severely limit the representative capability of the semantic features derived from topic models. To alleviate this problem, this paper propose to build powerful semantic features using the probabilistic latent semantic analysis (pLSA) model, by employing the pre-trained deep convolutional neural networks (CNNs) as feature extractors rather than relying on the hand-crafted features. Specifically, we develop two methods to generate semantic features, called multi-scale deep semantic representation (MSDS) and multi-level deep semantic representation (MLDS), by extracting CNN features from different layers: (1) in MSDS, the final semantic features are learned by the pLSA with multi-scale features extracted from the convolutional layer of a pre-trained CNN; (2) in MLDS, we extract CNN features for densely sampled image patches at different size level from the fully-connected layer of a pre-trained CNN, and concatenate the sematic features learned by the pLSA at each level. We comprehensively evaluate the two methods on two public HRRS scene datasets, and achieve significant performance improvement over the state-of-the-art. The outstanding results demonstrate that the pLSA model is capable of discovering considerably discriminative semantic features from the deep CNN features.},
  archive  = {J},
  author   = {Fan Hu and Gui-Song Xia and Wen Yang and Liangpei Zhang},
  doi      = {10.1109/TBDATA.2019.2916880},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {522-536},
  title    = {Mining deep semantic representations for scene classification of high-resolution remote sensing imagery},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting deep features for remote sensing image retrieval:
A systematic investigation. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(3), 507–521. (<a
href="https://doi.org/10.1109/TBDATA.2019.2948924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Remote sensing (RS) image retrieval is of great significant for geological information mining. Over the past two decades, a large amount of research on this task has been carried out, which mainly focuses on the following three core issues: feature extraction, similarity metric, and relevance feedback. Due to the complexity and multiformity of ground objects in high-resolution remote sensing (HRRS) images, there is still room for improvement in the current retrieval approaches. In this article, we analyze the three core issues of RS image retrieval and provide a comprehensive review on existing methods. Furthermore, for the goal to advance the state-of-the-art in HRRS image retrieval, we focus on the feature extraction issue and delve how to use powerful deep representations to address this task. We conduct systematic investigation on evaluating correlative factors that may affect the performance of deep features. By optimizing each factor, we acquire remarkable retrieval results on publicly available HRRS datasets. Finally, we explain the experimental phenomenon in detail and draw conclusions according to our analysis. Our work can serve as a guiding role for the research of content-based RS image retrieval.},
  archive  = {J},
  author   = {Xin-Yi Tong and Gui-Song Xia and Fan Hu and Yanfei Zhong and Mihai Datcu and Liangpei Zhang},
  doi      = {10.1109/TBDATA.2019.2948924},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {507-521},
  title    = {Exploiting deep features for remote sensing image retrieval: A systematic investigation},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Beyond the patchwise classification: Spectral-spatial fully
convolutional networks for hyperspectral image classification. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(3), 492–506. (<a
href="https://doi.org/10.1109/TBDATA.2019.2923243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, patchwise classification methods are commonly adopted when dealing with the hyperspectral image (HSI) classification. Despite their promising results from the perspective of accuracy, the efficiency of these methods can hardly be ensured since there are redundant computations between adjacent patches. In this paper, we propose a spectral-spatial fully convolutional network for HSI classification with an end-to-end, pixel-to-pixel architecture. Compared with patchwise methods, the proposed framework can avoid the patch extraction and is more efficient. Since the training samples in HSIs are highly sparse, the training strategy in original fully convolutional networks is no longer feasible for HSIs. To solve this problem, we propose a novel mask matrix to assist the back-propagation in the training stage. Considering the importance of spectral and spatial features may vary for different objects and scenes, we combine both features with two weighting factors which can be adaptively learned during the network training. Besides, the dense conditional random field (CRF) is introduced into the framework to further balance the local and global information. Experiments on three benchmark HSI data sets demonstrate that the proposed method can yield competitive results with less time costs compared with patchwise methods.},
  archive  = {J},
  author   = {Yonghao Xu and Bo Du and Liangpei Zhang},
  doi      = {10.1109/TBDATA.2019.2923243},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {492-506},
  title    = {Beyond the patchwise classification: Spectral-spatial fully convolutional networks for hyperspectral image classification},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ASTROIDE: A unified astronomical big data processing engine
over spark. <em>IEEE Transactions on Big Data</em>, <em>6</em>(3),
477–491. (<a href="https://doi.org/10.1109/TBDATA.2018.2873749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The next decade promises to be an exciting time for astronomers. Large volumes of astronomical data are continuously collected from highly productive space missions. This data has to be efficiently stored and analyzed in such a way that astronomers maximize their scientific return from these missions. Recognizing the need to better handle astronomical datasets, we designed ASTROIDE, a distributed data server for astronomical data. We analyze the peculiarities of the data and the queries in cosmological applications and design a new framework where astronomers can explore and manage vast amounts of data. ASTROIDE introduces effective methods for efficient astronomical query execution on Spark through data partitioning with HEALPix and customized optimizer. ASTROIDE offers a simple, expressive and unified interface through ADQL, a standard language for querying databases in astronomy. Experiments have shown that ASTROIDE is effective in processing astronomical data, scalable and outperforms the state-of-the-art.},
  archive  = {J},
  author   = {Mariem Brahem and Karine Zeitouni and Laurent Yeh},
  doi      = {10.1109/TBDATA.2018.2873749},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {477-491},
  title    = {ASTROIDE: A unified astronomical big data processing engine over spark},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional neural networks for spectroscopic redshift
estimation on euclid data. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(3), 460–476. (<a
href="https://doi.org/10.1109/TBDATA.2019.2934475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we address the problem of spectroscopic redshift estimation in Astronomy. Due to the expansion of the Universe, galaxies recede from each other on average. This movement causes the emitted electromagnetic waves to shift from the blue part of the spectrum to the red part, due to the Doppler effect. Redshift is one of the most important observables in Astronomy, allowing the measurement of galaxy distances. Several sources of noise render the estimation process far from trivial, especially in the low signal-to-noise regime of many astrophysical observations. In recent years, new approaches for a reliable and automated estimation methodology have been sought out, in order to minimize our reliance on currently popular techniques that heavily involve human intervention. The fulfilment of this task has evolved into a grave necessity, in conjunction with the insatiable generation of immense amounts of astronomical data. In our work, we introduce a novel approach based on Deep Convolutional Neural Networks. The proposed methodology is extensively evaluated on a spectroscopic dataset of full spectral energy galaxy distributions, modelled after the upcoming Euclid satellite galaxy survey. Experimental analysis on observations of idealistic and realistic conditions demonstrate the potent capabilities of the proposed scheme.},
  archive  = {J},
  author   = {Radamanthys Stivaktakis and Grigorios Tsagkatakis and Bruno Moraes and Filipe Abdalla and Jean-Luc Starck and Panagiotis Tsakalides},
  doi      = {10.1109/TBDATA.2019.2934475},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {460-476},
  title    = {Convolutional neural networks for spectroscopic redshift estimation on euclid data},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Innovative approach for PMM data processing and analytics.
<em>IEEE Transactions on Big Data</em>, <em>6</em>(3), 452–459. (<a
href="https://doi.org/10.1109/TBDATA.2020.2995242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {ALTEC defined and developed a framework with the main aim to process a big amount of data allowing a seamless connection between the collected information and the analyses performed by end users. This is the ASDP environment, that allows to organize data in the most adapt domain data store in order to have data ready for complex analyses. In particular, the PMM module of the ISS is a reference case for the survey on framework capabilities for telemetry data management. The main objective is to demonstrate the advantages achievable through the application of new data analysis methodologies and tools after data organization through ASDP capabilities.},
  archive  = {J},
  author   = {R. De March and C. Leuzzi and M. Deffacis and F. Caronte and A. F. Mulone and R. Messineo},
  doi      = {10.1109/TBDATA.2020.2995242},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {452-459},
  title    = {Innovative approach for PMM data processing and analytics},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple testing for outlier detection in space telemetries.
<em>IEEE Transactions on Big Data</em>, <em>6</em>(3), 443–451. (<a
href="https://doi.org/10.1109/TBDATA.2019.2954831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a novel procedure for outlier detection in space telemetries, in a semi-supervised framework. As the data is functional, we reduce its dimension by considering the coefficients obtained after projecting the observations onto orthonormal bases. A multiple testing procedure based on the two-sample test is defined in order to highlight the levels of the coefficients on which the outliers appear as significantly different from the nominal data. The Local Outlier Factor is computed on the selected coefficients to highlight the outliers. This procedure for selecting the features is applied on simulated data that mimic the behavior of space telemetries and on a real telemetry and then compared with existing dimension reduction techniques.},
  archive  = {J},
  author   = {Clémentine Barreyre and Béatrice Laurent and Jean-Michel Loubes and Loic Boussouf and Bertrand Cabon},
  doi      = {10.1109/TBDATA.2019.2954831},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {443-451},
  title    = {Multiple testing for outlier detection in space telemetries},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optical compressive imaging technologies for space big data.
<em>IEEE Transactions on Big Data</em>, <em>6</em>(3), 430–442. (<a
href="https://doi.org/10.1109/TBDATA.2019.2907135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The increasing amount of data generated by space applications poses several challenges due to limited resources available onboard: power, memory, computation, data rate. In this paper, we propose Compressed Sensing (CS) as the key tool to face those challenges via compressive imaging. This signal processing technique, only recently applied to space applications, dramatically simplifies the image acquisition featuring native compression/encryption and enabling onboard image analysis, allowing to design simpler and lighter optical systems. In this paper, we try to answer the following question: To what extent are the potential benefits of CS going to materialize in a realistic “space big data” application scenario? To this purpose, we first review compressive imaging techniques and already existing prototypes and concepts, critically discussing the technological issues involved. Then, we propose a set of instrument concepts in the application domains of space science, planetary exploration and earth observation, most suitable for a CS-based application. For the most promising of them, we go deeper into the analysis showing preliminary reconstruction performance tests.},
  archive  = {J},
  author   = {Giulio Coluccia and Cinzia Lastri and Donatella Guzzi and Enrico Magli and Vanni Nardino and Lorenzo Palombi and Ivan Pippi and Valentina Raimondi and Chiara Ravazzi and Florin Garoi and Daniela Coltuc and Raffaele Vitulli and Alessandro Zuccaro Marchi},
  doi      = {10.1109/TBDATA.2019.2907135},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {430-442},
  title    = {Optical compressive imaging technologies for space big data},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on big data from space. <em>IEEE Transactions
on Big Data</em>, <em>6</em>(3), 427–429. (<a
href="https://doi.org/10.1109/TBDATA.2020.3015536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The recent multiplication of open access initiatives to Big Data from Space is giving momentum to the field by widening substantially the spectrum of scientific communities and users, as well as awareness among the public, while offering new benefits at all levels from individual citizens to the whole society. Following a detailed and rigorous review process, 14 articles have been selected out of 48 submissions for this special issue. These are briefly summarized.},
  archive  = {J},
  author   = {Mihai Datcu and Jacqueline Le Moigne and Sveinung Loekken and Pierre Soille and Gui-Song Xia},
  doi      = {10.1109/TBDATA.2020.3015536},
  journal  = {IEEE Transactions on Big Data},
  month    = {9},
  number   = {3},
  pages    = {427-429},
  title    = {Special issue on big data from space},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On power-peak-aware scheduling for large-scale shared
clusters. <em>IEEE Transactions on Big Data</em>, <em>6</em>(2),
412–426. (<a href="https://doi.org/10.1109/TBDATA.2018.2874663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent studies have reported that big data analytics clusters, such as Hadoop, can create substantial power peaks, bringing instability and inflexibility issues to the power grid. Substantial power peaks also lead to high penalty charges from electric utility companies, accounting for more than 30 percent of the electricity bill for a cluster operator according to empirical studies. To this end, in this paper, we present a framework that schedules computing jobs in large-scale data analytics clusters to mitigate power peaks. The scheduling model captures important properties of modern distributed data analytics clusters, including bundled resource provisioning and job-to-task decomposition with distributed processing. The scheduling problem is formulated as a nonlinear integer program. Its solution is derived by decomposing it into two classes of sub-problems and solving each class with an exact and efficient solution method. As a direct application, we detail the implementation of our proposed scheduling framework on a Hadoop cluster, and demonstrate its efficacy by extensive trace-driven simulations based on the CloudSim simulator.},
  archive  = {J},
  author   = {Yuxuan Jiang and Zhe Huang and Danny H.K. Tsang},
  doi      = {10.1109/TBDATA.2018.2874663},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {412-426},
  title    = {On power-peak-aware scheduling for large-scale shared clusters},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large-scale data pollution with apache spark. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(2), 396–411. (<a
href="https://doi.org/10.1109/TBDATA.2016.2637378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Because of the increasing volume of autonomously collected data objects, duplicate detection is an important challenge in today&#39;s data management. To evaluate the efficiency of duplicate detection algorithms with respect to big data, large test data sets are required. Existing test data generation tools, however, are either not able to produce large test data sets or are domain-dependent which limits their usefulness to a few cases. In this paper, we describe a new framework that can be used to pollute a clean, homogeneous and large data set from an arbitrary domain with duplicates, errors and inhomogeneities. To prove its concept, we implemented a prototype which is built upon the cluster computing framework Apache Spark and evaluate its performance in several experiments.},
  archive  = {J},
  author   = {Kai Hildebrandt and Fabian Panse and Niklas Wilcke and Norbert Ritter},
  doi      = {10.1109/TBDATA.2016.2637378},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {396-411},
  title    = {Large-scale data pollution with apache spark},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Benchmarking blocking algorithms for web entities. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(2), 382–395. (<a
href="https://doi.org/10.1109/TBDATA.2016.2576463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An increasing number of entities are described by interlinked data rather than documents on the Web. Entity Resolution (ER) aims to identify descriptions of the same real-world entity within one or across knowledge bases in the Web of data. To reduce the required number of pairwise comparisons among descriptions, ER methods typically perform a pre-processing step, called blocking, which places similar entity descriptions into blocks and thus only compare descriptions within the same block. We experimentally evaluate several blocking methods proposed for the Web of data using real datasets, whose characteristics significantly impact their effectiveness and efficiency. The proposed experimental evaluation framework allows us to better understand the characteristics of the missed matching entity descriptions and contrast them with ground truth obtained from different kinds of relatedness links.},
  archive  = {J},
  author   = {Vasilis Efthymiou and Kostas Stefanidis and Vassilis Christophides},
  doi      = {10.1109/TBDATA.2016.2576463},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {382-395},
  title    = {Benchmarking blocking algorithms for web entities},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kira: Processing astronomy imagery using big data
technology. <em>IEEE Transactions on Big Data</em>, <em>6</em>(2),
369–381. (<a href="https://doi.org/10.1109/TBDATA.2016.2599926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Scientific analyses commonly compose multiple single-process programs into a dataflow. An end-to-end dataflow of single-process programs is known as a many-task application. Typically, HPC tools are used to parallelize these analyses. In this work, we investigate an alternate approach that uses Apache Spark-a modern platform for data intensive computing-to parallelize many-task applications. We implement Kira, a flexible and distributed astronomy image processing toolkit, and its Source Extractor (Kira SE) application. Using Kira SE as a case study, we examine the programming flexibility, dataflow richness, scheduling capacity and performance of Apache Spark running on the Amazon EC2 cloud. By exploiting data locality, Kira SE achieves a 4.1× speedup over an equivalent C program when analyzing a 1TB dataset using 512 cores on the Amazon EC2 cloud. Furthermore, Kira SE on the Amazon EC2 cloud achieves a 1.8× speedup over the C program on the NERSC Edison supercomputer. A 128-core Amazon EC2 cloud deployment of Kira SE using Spark Streaming can achieve a second-scale latency with a sustained throughput of 800 MB/s. Our experience with Kira demonstrates that data intensive computing platforms like Apache Spark are a performant alternative for many-task scientific applications.},
  archive  = {J},
  author   = {Zhao Zhang and Kyle Barbary and Frank Austin Nothaft and Evan R. Sparks and Oliver Zahn and Michael J. Franklin and David A. Patterson and Saul Perlmutter},
  doi      = {10.1109/TBDATA.2016.2599926},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {369-381},
  title    = {Kira: Processing astronomy imagery using big data technology},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comorbidity patterns and its impact on health outcomes:
Two-way clustering analysis. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(2), 359–368. (<a
href="https://doi.org/10.1109/TBDATA.2016.2623323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Comorbidity greatly increases the complexity of managing disease in patients. Approximately 27 percent of the US population have two or more concurrent comorbid conditions. Traditional models for assessing the impact of patient demographic and comorbidity burden on patient health outcomes, represented comorbidity conditions by Charlson Comorbidity Index. In this paper, we develop a novel two-way clustering approach combining model-based and weighted K-means clustering methods for characterizing and summarizing a patient&#39;s comorbid conditions. Our two-way approach helps reduce the size of the data to a manageable size, thus being practical for big data applications. Another novel aspect of our approach is the ability to handle weighted observations. Assigning weights to observations helps reduce the size of the dataset, thus addressing the scalability challenge of algorithms when dealing with big data. Using the National Inpatient Sample database for 2008-2013, we evaluate the performance of our approach by the use of logistic regression and support vector machine models by applying them to patients whose primary diagnosis is cardiovascular disease. In addition to evaluating our proposed method using empirical test data, we use asymptotic statistics. Both evaluation methods show that the proposed approach improves the prediction of patient health outcomes; specifically, hospital length of stay.},
  archive  = {J},
  author   = {Debopriya Ghosh and Javier Cabrera and Tarek N. Adam and Petros Levounis and Nabil R. Adam},
  doi      = {10.1109/TBDATA.2016.2623323},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {359-368},
  title    = {Comorbidity patterns and its impact on health outcomes: Two-way clustering analysis},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CloudFinder: A system for processing big data workloads on
volunteered federated clouds. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(2), 347–358. (<a
href="https://doi.org/10.1109/TBDATA.2017.2703830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The proliferation of private clouds that are often underutilized and the tremendous computational potential of these clouds when combined has recently brought forth the idea of volunteer cloud computing (VCC), a computing model where cloud owners contribute underutilized computing and/or storage resources on their clouds to support the execution of applications of other members in the community. This model is particularly suitable to solve big data scientific problems. Scientists in data-intensive scientific fields increasingly recognize that sharing volunteered resources from several clouds is a cost-effective alternative to solve many complex, data- and/or compute-intensive science problems. Despite the promise of the idea of VCC, it still remains at the vision stage at best. Challenges include the heterogeneity and autonomy of member clouds, access control and security, complex inter-cloud virtual machine scheduling, etc. In this paper, we present CloudFinder, a system that supports the efficient execution of big data workloads on volunteered federated clouds (VFCs). Our evaluation of the system indicates that VFCs are a promising cost-effective approach to enable big data science.},
  archive  = {J},
  author   = {Abdelmounaam Rezgui and Nickolas Davis and Zaki Malik and Brahim Medjahed and Hamdy S. Soliman},
  doi      = {10.1109/TBDATA.2017.2703830},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {347-358},
  title    = {CloudFinder: A system for processing big data workloads on volunteered federated clouds},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of different machine learning approaches to
predict small for gestational age infants. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(2), 334–346. (<a
href="https://doi.org/10.1109/TBDATA.2016.2620981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Diagnosing infants who are small for gestational age (SGA) at early stages could help physicians to introduce interventions for SGA infants earlier. Machine learning (ML) is envisioned as a tool to identify SGA infants. However, ML has not been widely studied in this field. To develop effective SGA prediction models, we conducted four groups of experiments that considered basic ML methods, imbalanced data, feature selection and the time characteristics of variables, respectively. Infants with SGA data collected from 2010 to 2013 with gestational weeks between 24 and 42 were detected. Support vector machine (SVM), random forest (RF), logistic regression (LR) and Sparse LR models were trained on 10-fold cross validation. Precision and the area under the curve (AUC) of the receiver operator characteristic curve were evaluated. For each group, the performance of SVM and Sparse LR was similarly well. LR without any sparsity penalties performed worst, possibly caused by the overfitting problem. With the combination of handling imbalanced data and feature selection, the RF ensemble classifier performed best, which even obtained the highest AUC value (0.8547) with the help of expert knowledge. In other cases, RF performed worse than Sparse LR and SVM, possibly because of fully grown trees.},
  archive  = {J},
  author   = {Jianqiang Li and Lu Liu and Jingchao Sun and Haowen Mo and Ji-Jiang Yang and Shi Chen and Huiting Liu and Qing Wang and Hui Pan},
  doi      = {10.1109/TBDATA.2016.2620981},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {334-346},
  title    = {Comparison of different machine learning approaches to predict small for gestational age infants},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep model based transfer and multi-task learning for
biological image analysis. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(2), 322–333. (<a
href="https://doi.org/10.1109/TBDATA.2016.2573280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A central theme in learning from image data is to develop appropriate representations for the specific task at hand. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of gene expression patterns in Drosophila , texture features were particularly effective for determining the developmental stages from in situ hybridization images. Such image representation is however not suitable for controlled vocabulary term annotation. Here, we developed feature extraction methods to generate hierarchical representations for ISH images. Our approach is based on the deep convolutional neural networks that can act on image pixels directly. To make the extracted features generic, the models were trained using a natural image set with millions of labeled examples. These models were transferred to the ISH image domain. To account for the differences between the source and target domains, we proposed a partial transfer learning scheme in which only part of the source model is transferred. We employed multi-task learning method to fine-tune the pre-trained models with labeled ISH images. Results showed that feature representations computed by deep models based on transfer and multi-task learning significantly outperformed other methods for annotating gene expression patterns at different stage ranges.},
  archive  = {J},
  author   = {Wenlu Zhang and Rongjian Li and Tao Zeng and Qian Sun and Sudhir Kumar and Jieping Ye and Shuiwang Ji},
  doi      = {10.1109/TBDATA.2016.2573280},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {322-333},
  title    = {Deep model based transfer and multi-task learning for biological image analysis},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extracting medical knowledge from crowdsourced question
answering website. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(2), 309–321. (<a
href="https://doi.org/10.1109/TBDATA.2016.2612236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The medical crowdsourced question answering (Q&amp;A) websites are booming in recent years, and an increasingly large amount of patients and doctors are involved. The valuable information from these medical crowdsourced Q&amp;A websites can benefit patients, doctors and the society. One key to unleash the power of these Q&amp;A websites is to extract medical knowledge from the noisy question-answer pairs and filter out unrelated or even incorrect information. Facing the daunting scale of information generated on medical Q&amp;A websites everyday, it is unrealistic to fulfill this task via supervised method due to the expensive annotation cost. In this paper, we propose a Medical Knowledge Extraction (MKE) system that can automatically provide high-quality knowledge triples extracted from the noisy question-answer pairs, and at the same time, estimate expertise for the doctors who give answers on these Q&amp;A websites. The MKE system is built upon a truth discovery framework, where we jointly estimate trustworthiness of answers and doctor expertise from the data without any supervision. We further tackle three unique challenges in the medical knowledge extraction task, namely representation of noisy input, multiple linked truths, and the long-tail phenomenon in the data. The MKE system is applied to real-world datasets crawled from xywy.com , one of the most popular medical crowdsourced Q&amp;A websites. Both quantitative evaluation and case studies demonstrate that the proposed MKE system can successfully provide useful medical knowledge and accurate doctor expertise. We further demonstrate a real-world application, Ask A Doctor , which can automatically give patients suggestions to their questions.},
  archive  = {J},
  author   = {Yaliang Li and Chaochun Liu and Nan Du and Wei Fan and Qi Li and Jing Gao and Chenwei Zhang and Hao Wu},
  doi      = {10.1109/TBDATA.2016.2612236},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {309-321},
  title    = {Extracting medical knowledge from crowdsourced question answering website},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Big data privacy in biomedical research. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(2), 296–308. (<a
href="https://doi.org/10.1109/TBDATA.2016.2608848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Biomedical research often involves studying patient data that contain personal information. Inappropriate use of these data might lead to leakage of sensitive information, which can put patient privacy at risk. The problem of preserving patient privacy has received increasing attentions in the era of big data. Many privacy methods have been developed to protect against various attack models. This paper reviews relevant topics in the context of biomedical research. We discuss privacy preserving technologies related to (1) record linkage, (2) synthetic data generation, and (3) genomic data privacy. We also discuss the ethical implications of big data privacy in biomedicine and present challenges in future research directions for improving data privacy in biomedical research.},
  archive  = {J},
  author   = {Shuang Wang and Luca Bonomi and Wenrui Dai and Feng Chen and Cynthia Cheung and Cinnamon S. Bloss and Samuel Cheng and Xiaoqian Jiang},
  doi      = {10.1109/TBDATA.2016.2608848},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {296-308},
  title    = {Big data privacy in biomedical research},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differential privacy preserving of training model in
wireless big data with edge computing. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(2), 283–295. (<a
href="https://doi.org/10.1109/TBDATA.2018.2829886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the popularity of smart devices and the widespread use of machine learning methods, smart edges have become the mainstream of dealing with wireless big data. When smart edges use machine learning models to analyze wireless big data, nevertheless, some models may unintentionally store a small portion of the training data with sensitive records. Thus, intruders can expose sensitive information by careful analysis of this model. To solve this privacy issue, in this paper, we propose and implement a machine learning strategy for smart edges using differential privacy. We focus our attention on privacy protection in training datasets in wireless big data scenario. Moreover, we guarantee privacy protection by adding Laplace mechanisms, and design two different algorithms Output Perturbation (OPP) and Objective Perturbation (OJP), which satisfy differential privacy. In addition, we consider the privacy preserving issues presented in the existing literatures for differential privacy in the correlated datasets, and further provided differential privacy preserving methods for correlated datasets, guaranteeing privacy by theoretical deduction. Finally, we implement the experiments on the TensorFlow, and evaluate our strategy on four datasets, i.e., MNIST, SVHN, CIFAR-10 and STL-10. The experiment results show that our methods can efficiently protect the privacy of training datasets and guarantee the accuracy on benchmark datasets.},
  archive  = {J},
  author   = {Miao Du and Kun Wang and Zhuoqun Xia and Yan Zhang},
  doi      = {10.1109/TBDATA.2018.2829886},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {283-295},
  title    = {Differential privacy preserving of training model in wireless big data with edge computing},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WiFind: Driver fatigue detection with fine-grained wi-fi
signal features. <em>IEEE Transactions on Big Data</em>, <em>6</em>(2),
269–282. (<a href="https://doi.org/10.1109/TBDATA.2018.2848969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Driver fatigue is a leading factor in road accidents that can cause severe fatalities. Existing fatigue detection works focus on vision and electroencephalography (EEG) based means of detection. However, vision-based approaches suffer from view-blocking or vision distortion problems and EEG-based systems are intrusive, and the drivers have to use/wear the devices with inconvenience or additional costs. In our work, we propose a novel Wi-Fi signals based fatigue detection approach, called WiFind to overcome the drawbacks as associated with the current works. WiFind is simple and (wearable) device-free. It can detect the fatigue symptoms in the vehicle without relying on any visual image or video. By applying self-adaptive method, it can recognize the body features of drivers in multiple modes. It applies Hilbert-Huang transform (HHT) based pattern extract method results in accuracy increase in motion detection mode. WiFind can be easily deployed in a commodity Wi-Fi infrastructure, and we have evaluated its performance in real driving environments. The experimental results have shown that WiFind can achieve the recognition accuracy of 89.6 percent in a single driver scenario.},
  archive  = {J},
  author   = {Weijia Jia and Hongjian Peng and Na Ruan and Zhiqing Tang and Wei Zhao},
  doi      = {10.1109/TBDATA.2018.2848969},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {269-282},
  title    = {WiFind: Driver fatigue detection with fine-grained wi-fi signal features},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sleepy: Wireless channel data driven sleep monitoring via
commodity WiFi devices. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(2), 258–268. (<a
href="https://doi.org/10.1109/TBDATA.2018.2851201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sleep is a major event of our daily lives. Its quality constitutes a critical indicator of people&#39;s health conditions, both mentally and physically. Existing sensor-based or vision-based sleep monitoring systems either are obstructive to use or fail to provide adequate coverage. With the fast expansion of wireless infrastructures nowadays, channel data, which is pervasive and transparent, emerges as another alternative. To this end, we propose Sleepy, a wireless channel data driven sleep monitoring system leveraging commercial WiFi devices. The key idea of Sleepy is that the energy feature of the wireless channel follows a Gaussian Mixture Model (GMM) derived from the accumulated channel data over a long period. Therefore, a GMM based foreground extraction method has been designed to adaptively distinguish motions like rollovers (foreground) from background (stationary postures), leading to certain major merits, e.g., no calibrations or target-dependent training needed. We prototype Sleepy and evaluate it in two real environments. In the short-term controlled experiments, Sleepy achieves 95.65 percent detection accuracy (DA) and 2.16 percent false negative rate (FNR) on average. In the 60-minute real sleep studies, Sleepy demonstrates strong stability, i.e., 0 percent FNR and 98.22 percent DA. Considering that Sleepy is compatible with existing WiFi infrastructure, it constitutes a low-cost yet promising solution for sleep monitoring.},
  archive  = {J},
  author   = {Yu Gu and Yifan Zhang and Jie Li and Yusheng Ji and Xin An and Fuji Ren},
  doi      = {10.1109/TBDATA.2018.2851201},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {258-268},
  title    = {Sleepy: Wireless channel data driven sleep monitoring via commodity WiFi devices},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental deep computation model for wireless big data
feature learning. <em>IEEE Transactions on Big Data</em>, <em>6</em>(2),
248–257. (<a href="https://doi.org/10.1109/TBDATA.2019.2903092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Big data feature learning is a crucial issue for the service management for Internet of Things. However, big data collected from Internet of Things is of dynamic nature at a high speed, which poses an important challenge on wireless big data learning models, especially the deep computation model. In this paper, an incremental deep computation model is proposed for wireless big data feature learning in Internet of Things. First, two incremental tensor auto-encoders (ITAE) are developed by devising two incremental learning algorithms, namely parameter-based incremental learning algorithm (PI-TAE) and structure-based incremental learning algorithm (SI-TAE), when new wireless samples are available. PI-TAE only updates the network parameters while SI-TAE simultaneously adjusts the structure and updates the parameters to adapt to the new arriving wireless big data. Furthermore, an incremental deep computation model is constructed by stacking several ITAEs. Experiments are conducted to evaluate the performance of the proposed model by comparing with the conventional deep computation model and other two representative incremental learning algorithms, i.e., OANN and PIE. Results demonstrate that the presented model can modify the network in an incremental manner for new arriving data learning efficiently with preserving the prior knowledge for the previous data learning, proving its potential for dynamic wireless big data learning in Internet of Things.},
  archive  = {J},
  author   = {Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li},
  doi      = {10.1109/TBDATA.2019.2903092},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {248-257},
  title    = {Incremental deep computation model for wireless big data feature learning},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MWBS: An efficient many-to-many wireless big data delivery
scheme. <em>IEEE Transactions on Big Data</em>, <em>6</em>(2), 233–247.
(<a href="https://doi.org/10.1109/TBDATA.2018.2878584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Wireless big data raises the demands on the networking schemes to support the efficient group data sharing over heterogeneous wireless technologies, which take many-to-many data delivery as the foundation. Information-centric networking (ICN) approach is a promising networking technology to support big data delivery, which has the potential to establish the harmony between networking and wireless big data sharing. However, the existing ICN schemes have not carefully addressed the many-to-many communications. To address this issue, we propose an efficient and secure many-to-many wireless big data delivery scheme (MWBS) to provide group-based data dissemination and retrieval with name-integrated forwarding. In MWBS, a bi-directional tree is securely constructed for each group through the procedures of group initiation, join, leave, publication, and multi-level inter-zone routing. Especially, Designated Forwarding and Cacheable Nodes (DFCNs) are introduced to act as the roots for the construction of such bi-directional trees. The implementation details of MWBS are provided for function verifications. To effectively deploy MWBS, we investigate the impacts to the MWBS performance from the number and locations of DFCNs, which show that the optimized number of DFCNs can reduce the total traffic cost and the DFCN close to users is preferred to be selected for a group. Finally, simulations are performed to evaluate the performance of MWBS, which show that MWBS can reduce the control packet overhead and the state storage overhead compared to the existing ICN schemes.},
  archive  = {J},
  author   = {Ruidong Li and Hitoshi Asaeda},
  doi      = {10.1109/TBDATA.2018.2878584},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {233-247},
  title    = {MWBS: An efficient many-to-many wireless big data delivery scheme},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering analysis in the wireless propagation channel with
a variational gaussian mixture model. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(2), 223–232. (<a
href="https://doi.org/10.1109/TBDATA.2018.2840696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, the Gaussian mixture model (GMM) is introduced to implement channel multipath clustering. The GMM incorporates the covariance structure and the mean information of the channel multipaths, thus it can effectively reveal the similarity of the channel multipaths. First, the expectation-maximization (EM) algorithm is utilized to search for the posterior estimation of the GMM parameters. Then, the variational Bayesian (VB) algorithm is employed to optimize the GMM parameters to enhance the searching ability of EM and further to determine the optimal number of Gaussian distributions without resorting to cross-validation. Finally, a compact index (CI) is proposed to validate the clustering results reasonably. Thanks to the proposed CI, it is possible to find a close relationship among the GMM clustering mechanism, the multipath propagation characteristics and the CI evaluation index. Experiments with synthetic data and outdoor-to-indoor (O2I) channel data are presented to demonstrate the effectiveness of the proposed method.},
  archive  = {J},
  author   = {Yupeng Li and Jianhua Zhang and Zhanyu Ma and Yu Zhang},
  doi      = {10.1109/TBDATA.2018.2840696},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {223-232},
  title    = {Clustering analysis in the wireless propagation channel with a variational gaussian mixture model},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A big data enabled channel model for 5G wireless
communication systems. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(2), 211–222. (<a
href="https://doi.org/10.1109/TBDATA.2018.2884489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The standardization process of the fifth generation (5G) wireless communications has recently been accelerated and the first commercial 5G services would be provided as early as in 2018. The increasing of enormous smartphones, new complex scenarios, large frequency bands, massive antenna elements, and dense small cells will generate big datasets and bring 5G communications to the era of big data. This paper investigates various applications of big data analytics, especially machine learning algorithms in wireless communications and channel modeling. We propose a big data and machine learning enabled wireless channel model framework. The proposed channel model is based on artificial neural networks (ANNs), including feed-forward neural network (FNN) and radial basis function neural network (RBF-NN). The input parameters are transmitter (Tx) and receiver (Rx) coordinates, Tx–Rx distance, and carrier frequency, while the output parameters are channel statistical properties, including the received power, root mean square (RMS) delay spread (DS), and RMS angle spreads (ASs). Datasets used to train and test the ANNs are collected from both real channel measurements and a geometry based stochastic model (GBSM). Simulation results show good performance and indicate that machine learning algorithms can be powerful analytical tools for future measurement-based wireless channel modeling.},
  archive  = {J},
  author   = {Jie Huang and Cheng-Xiang Wang and Lu Bai and Jian Sun and Yang Yang and Jie Li and Olav Tirkkonen and Ming-Tuo Zhou},
  doi      = {10.1109/TBDATA.2018.2884489},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {211-222},
  title    = {A big data enabled channel model for 5G wireless communication systems},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on wireless big data. <em>IEEE Transactions on
Big Data</em>, <em>6</em>(2), 209–210. (<a
href="https://doi.org/10.1109/TBDATA.2020.2966806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The papers in this special section focus on wireless big data. Big data, which has been following the exponential growth rates in different commercial areas, has profoundly changed the way we live. It has received considerable attention in both academic and industrial communities, in contexts such as mobile communications, distributed computing, e-health, intelligent transportation systems, wireless sensor networks, etc. In the meantime, the Internet of Things (IoT) scenarios considered in the Fifth Generation (5G) wireless communication systems are expected to create many novel applications and services with various requirements [1]. These new directions bring a dramatic increase and change in the amount and types of wireless data, thus driving wireless communications into a new era. Therefore, an in-depth analysis and understanding of wireless big data can greatly facilitate better system design and performance optimization, which will certainly benefit equipment vendors, network operators and service providers.},
  archive  = {J},
  author   = {Yang Yang and Jie Li and Cheng-Xiang Wang and Olav Tirkkonen and Ming-Tuo Zhou},
  doi      = {10.1109/TBDATA.2020.2966806},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {209-210},
  title    = {Special issue on wireless big data},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast tensor factorization for large-scale context-aware
recommendation from implicit feedback. <em>IEEE Transactions on Big
Data</em>, <em>6</em>(1), 201–208. (<a
href="https://doi.org/10.1109/TBDATA.2018.2889121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a fast Tensor Factorization (TF) algorithm for context-aware recommendation from implicit feedback. For such a recommendation problem, the observed data indicate the (positive) association between users and items in some given contexts. For better accuracy, it has been shown essential to include unobserved data that indicate the negative user-item-context associations. As such unobserved data greatly outnumber the observed ones, for efficiency existing algorithms usually use only a small part of the unobserved data for model training. We show in this paper that it is possible, and beneficial, to use all the unobserved data in training a TF based context-aware recommender system. This is achieved by two technical innovations. First, we scrutinize the matrix computation of the closed-form solution and accelerate the computation by memorizing the repetitive computation. Second, we further boost the generalization and scalability by dropping out some pairwise interactions when updating user, item or context factors to prevent overfitting and to reduce the training time. The resulting whole-data based learning algorithm, referred to as DropTF in the paper, is efficient and scale well. Our evaluation on two small benchmark datasets and a million-scale large dataset demonstrates improved accuracy over some existing algorithms for context-aware recommendation.},
  archive  = {J},
  author   = {Szu-Yu Chou and Jyh-Shing Roger Jang and Yi-Hsuan Yang},
  doi      = {10.1109/TBDATA.2018.2889121},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {201-208},
  title    = {Fast tensor factorization for large-scale context-aware recommendation from implicit feedback},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Overlapping community change-point detection in an evolving
network. <em>IEEE Transactions on Big Data</em>, <em>6</em>(1), 189–200.
(<a href="https://doi.org/10.1109/TBDATA.2018.2880780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Change-point detection is a task that looks for specific moments across which a network changes fundamentally. Change-point detection is one of the most important challenges for overlapping community evolution analysis, and its aim is to identify the moment, type, and degree of change of a specific dynamic event when an overlapping community is evolving. In contrast to overlapping community detection, change-point detection addresses the evolution of an overlapping community rather than a network topology. In this paper, we propose such a method by reformulating an overlapping community in the form of a one-dimensional stream constrained by gentle degree fluctuation and the heterogeneous size distribution of the overlapping communities. According to the number of interacting overlapping communities involved in a specific change event, overlapping community change-points are classified as unary or binary. Based on a signal processing framework and a decision function-based strategy, our proposed method finds the change-points for both unary and binary cases. The experimental results from a synthetic dataset show that our proposed approach can ensure higher accuracy and a lower false positive rate than the traditional two-stage approach.},
  archive  = {J},
  author   = {Jiujun Cheng and Minjun Chen and MengChu Zhou and Shangce Gao and Chunmei Liu and Cong Liu},
  doi      = {10.1109/TBDATA.2018.2880780},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {189-200},
  title    = {Overlapping community change-point detection in an evolving network},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the opportunistic topology of taxi networks in urban
mobility environment. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(1), 171–188. (<a
href="https://doi.org/10.1109/TBDATA.2018.2878577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Understanding and characterizing the connectivity of vehicular networks has become increasingly important because of their wide applications and fast development. To address the dynamical links in vehicular networks, time-varying graph (TVG) is one of the most important models. Nowadays, due to the fact that lots of vehicular applications can tolerate a small amount of latency in communication, opportunistic reachability graph (ORG) characterizes the connectivity better by introducing delay tolerance to the model. However, people still do not have a high-level summarization, i.e., the topology, of the vehicular network on how nodes are clustered and isolated. In this paper, based on ORG model, we analyze the opportunistic topology of taxi networks in urban mobility environment by mainly focusing on the number, location and evolution of connected components and the size of the largest components to reveal the unique properties of the taxi networks instead of just links and hops. Our analysis is based on the real taxi traces of big cities and reflects the real urban mobility environment. We find that the opportunistic topology of the networks with delay tolerance is substantially different from the instantaneous topology without considering the delay. Moreover, we unveil the fundamental relationships and trade-offs between the dynamical topology and the key network parameters related to mobility, e.g., delay tolerance, transmission distance, etc. To the best of our knowledge, our study is the first work to reveal the characteristics of opportunistic topology models in the large-scale urban mobility environment with real traces.},
  archive  = {J},
  author   = {Ran Xu and Yong Li and Sheng Chen},
  doi      = {10.1109/TBDATA.2018.2878577},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {171-188},
  title    = {On the opportunistic topology of taxi networks in urban mobility environment},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geospatial event detection by grouping emotion contagion in
social media. <em>IEEE Transactions on Big Data</em>, <em>6</em>(1),
159–170. (<a href="https://doi.org/10.1109/TBDATA.2018.2876405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Twitter has a significant user base, with reportedly over 300 million active user accounts. Twitter, a micro blog service, limits the length of each tweet, keeping them short and concise. The contents of tweets include news, trending topics, emotions, and opinions. This makes Twitter a (popular) source of data for social science, marketing, psychology and news. Twitter users tend to use emojis, slang, and acronyms in order to fit more content within the character limit. The use of emojis in tweets complicates efforts in text mining and emotion analysis, as such emojis can also be used to express sarcasm when used in different contexts. In this paper, we use Twitter API to mine tweets that were geotagged by users and apply text analytics to the tweets. We also develop a system to detect events using the geospatial emotion vector in the area we are monitoring. Combining graph theory, machine learning semantics, and statistics with the geospatial emotion vectors, we track trending topics during times of extreme emotion. Our system correctly detected ten out of 11 events that we used in our study. Our findings suggest that Robert Parks theory on Expressive Groups and Gustave Le Bons Theory on Social Contagion hold true in the Twittersphere.},
  archive  = {J},
  author   = {Brandon Lwowski and Paul Rad and Kim-Kwang Raymond Choo},
  doi      = {10.1109/TBDATA.2018.2876405},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {159-170},
  title    = {Geospatial event detection by grouping emotion contagion in social media},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing taxi driver profit efficiency: A spatial
network-based markov decision process approach. <em>IEEE Transactions on
Big Data</em>, <em>6</em>(1), 145–158. (<a
href="https://doi.org/10.1109/TBDATA.2018.2875524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Taxi services play an important role in the public transportation system of large cities. Improving taxi business efficiency is an important societal problem. Most of the recent analytical approaches on this topic only considered how to maximize the pickup chance, energy efficiency, or profit for the immediate next trip when recommending seeking routes, therefore may not be optimal for the overall profit over an extended period of time due to ignoring the destination choice of potential passengers. To tackle this issue, we propose a novel Spatial Network-based Markov Decision Process (SN-MDP) with a rolling horizon configuration to recommend better driving directions. Given a set of historical taxi records and the current status (e.g., road segment and time) of a vacant taxi, we find the best move for this taxi to maximize the profit in the near future. We propose statistical models to estimate the necessary time-variant parameters of SN-MDP from data to avoid competition between drivers. In addition, we take into account fuel cost to assess profit, rather than only income. A case study and several experimental evaluations on a real taxi dataset from a major city in China show that our proposed approach improves the profit efficiency by up to 13.7 percent and outperforms baseline methods in all the time slots.},
  archive  = {J},
  author   = {Xun Zhou and Huigui Rong and Chang Yang and Qun Zhang and Amin Vahedian Khezerlou and Hui Zheng and Zubair Shafiq and Alex X. Liu},
  doi      = {10.1109/TBDATA.2018.2875524},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {145-158},
  title    = {Optimizing taxi driver profit efficiency: A spatial network-based markov decision process approach},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Big data processing workflows oriented real-time scheduling
algorithm using task-duplication in geo-distributed clouds. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(1), 131–144. (<a
href="https://doi.org/10.1109/TBDATA.2018.2874469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Scheduling big data processing workflows involves both large-scale tasks and transmission of massive intermediate data among tasks, thus optimizing their completion time and monetary cost becomes a challenging issue. Besides, data streams are continuously generated, and dynamically submitted to clouds for real-time or near real-time processing. Naturally, responsive schedules are required to keep pace with such dynamic environments and this further aggravates the difficulty of the workflow scheduling problem. To address these issues, we first derive two theorems to minimize the completion time of a set of parallel workflow tasks and the start time of each workflow task, and then define the latest finish time for workflow tasks, which is also proved its advantage in reducing costs without delaying the completion of workflows. On the basis of these theorems, we propose a novel real-time scheduling algorithm using task-duplication, RTSATD, such that minimizing both the completion time and monetary cost of processing big data workflows in clouds. The performance of RTSATD is analyzed by using both synthesized and real-world workflows. The experimental results demonstrate the superiority of the proposed algorithm with respect to completion time (up to 28.73 percent) and resource utilization (up to 46.31 percent) over two existing approaches.},
  archive  = {J},
  author   = {Huangke Chen and Jinming Wen and Witold Pedrycz and Guohua Wu},
  doi      = {10.1109/TBDATA.2018.2874469},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {131-144},
  title    = {Big data processing workflows oriented real-time scheduling algorithm using task-duplication in geo-distributed clouds},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matrix completion via sparse factorization solved by
accelerated proximal alternating linearized minimization. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(1), 119–130. (<a
href="https://doi.org/10.1109/TBDATA.2018.2871476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Classical matrix completion methods are not effective in recovering missing entries of data drawn from multiple subspaces because the matrices are often of high-rank. Recently a few advanced matrix completion methods were proposed to solve the problem but they are not scalable to large matrices and big data problems. This paper proposes a sparse factorization method for matrix completion on multiple-subspace data. The method factorizes the given incomplete matrix into a dense matrix and a sparse matrix, while the factorization errors of the observed entries are minimized. To solve the optimization problem, an accelerated proximal alternating linearized minimization (APALM) algorithm is proposed. As a non-trivial task owing to the alternation, linearization, nonconvexity, and extrapolation, the convergence of APALM is proved. APALM can solve a large class of optimization problems such as matrix factorization with nonsmooth regularizations. In addition, we show that, to recover an m × n matrix consisting of data drawn from k subspaces of dimension r 0 , the number of observed entries required in our matrix completion method is O(nr 0 logklog n) while that in conventional methods is O(nr 0 klog n), which theoretically proves the superiority of our method on multiple-subspace data and high-rank matrices. The proposed matrix completion method is compared with state-of-the-art on synthetic data and real collaborative filtering problems. The experimental results corroborate that the proposed method can handle large matrices efficiently and provide high recovery accuracy.},
  archive  = {J},
  author   = {Jicong Fan and Mingbo Zhao and Tommy W.S. Chow},
  doi      = {10.1109/TBDATA.2018.2871476},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {119-130},
  title    = {Matrix completion via sparse factorization solved by accelerated proximal alternating linearized minimization},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel grid-based colocation mining algorithms on GPUs for
big spatial event data. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(1), 107–118. (<a
href="https://doi.org/10.1109/TBDATA.2018.2871062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Colocation patterns refer to subsets of spatial features whose instances are frequently located together. Mining colocation patterns is important in many applications such as identifying relationships between diseases and environmental factors, but is computationally challenging due to the large number of instances and candidate patterns. Existing algorithms are mostly sequential, and thus can be insufficient for big spatial event data. Recently, parallel colocation mining algorithms have been developed based on the Map-reduce framework, which is economically expensive. Another work proposed a GPU algorithm based on iCPI tree, but assumes that the number of neighbors for each instance is within a small constant, and thus cannot be used when instances are dense and unevenly distributed. To address these limitations, we recently proposed grid-based GPU colocation mining algorithms that include a novel cell-aggregate-based upper bound filter, and two refinement algorithms. In this paper, we provide theoretical analysis of running time. Furthermore using GPU profiling, we identify our recent GPU implementation, GPU-grid-join, as a memory bound problem and to address its bottlenecks, we proposes GPU-grid-join+, an optimized GPU algorithm. Our experimental results on real world data shows that GPU-grid-join+ achieves 4 to 12-fold speedup over GPU-grid-join both running on Nvidia P100 GPU as well as 56 to 126-fold speedup over OpenMP implementation over Intel(R) Xeon(R) CPU with 12 cores. Also for synthetic data, the speedup is in ranges 3 to 7-fold and 9 to 42-fold respectively.},
  archive  = {J},
  author   = {Arpan Man Sainju and Danial Aghajarian and Zhe Jiang and Sushil Prasad},
  doi      = {10.1109/TBDATA.2018.2871062},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {107-118},
  title    = {Parallel grid-based colocation mining algorithms on GPUs for big spatial event data},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Short-term rainfall forecasting using multi-layer
perceptron. <em>IEEE Transactions on Big Data</em>, <em>6</em>(1),
93–106. (<a href="https://doi.org/10.1109/TBDATA.2018.2871151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Rainfall forecasting is crucial in the field of meteorology and hydrology. However, existing solutions always achieve low prediction accuracy for short-term rainfall forecasting. Atmospheric forecasting models perform worse in many conditions. Machine learning approaches neglect the influences of physical factors in upstream or downstream regions, which make forecasting accuracy fluctuate in different areas. To improve the overall forecasting accuracy for short-term rainfall, this paper proposes a novel solution called Dynamic Regional Combined short-term rainfall Forecasting approach (DRCF) using Multi-layer Perceptron (MLP). First, Principal Component Analysis (PCA) is used to reduce the dimension of thirteen physical factors, which serves as the input of MLP. Second, a greedy algorithm is applied to determine the structure of MLP. The surrounding sites are perceived based on the forecasting site. Finally, to solve the clutter interference which is caused by the extension of the perception range, DRCF is enhanced with several dynamic strategies. Experiments are conducted on data from 56 real-world meteorology sites in China, and we compare DRCF with atmospheric models and other machine learning approaches. The experimental results show that DRCF outperforms existing approaches in both threat score (TS) and root mean square error (RMSE).},
  archive  = {J},
  author   = {Pengcheng Zhang and Yangyang Jia and Jerry Gao and Wei Song and Hareton Leung},
  doi      = {10.1109/TBDATA.2018.2871151},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {93-106},
  title    = {Short-term rainfall forecasting using multi-layer perceptron},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cornac: Tackling huge graph visualization with big data
infrastructure. <em>IEEE Transactions on Big Data</em>, <em>6</em>(1),
80–92. (<a href="https://doi.org/10.1109/TBDATA.2018.2869165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The size of available graphs has drastically increased in recent years. The real-time visualization of graphs with millions of edges is a challenge but is necessary to grasp information hidden in huge datasets. This article presents an end-to-end technique to visualize huge graphs using an established Big Data ecosystem and a lightweight client running in a Web browser. For that purpose, levels of abstraction and graph tiles are generated by a batch layer and the interactive visualization is provided using a serving layer and client-side real-time computation of edge bundling and graph splatting. A major challenge is to create techniques that work without moving data to an ad hoc system and that take advantage of the horizontal scalability of these infrastructures. We introduce two novel scalable algorithms that enable to generate a canopy clustering and to aggregate graph edges. These two algorithms are both used to produce levels of abstraction and graph tiles. We prove that our technique guarantee a quality of visualization by controlling both the necessary bandwidth required for data transfer and the quality of the produced visualization. Furthermore, we demonstrate the usability of our technique by providing a complete prototype. We present benchmarks on graphs with millions of elements and we compare our results to those obtained by state of the art techniques. Our results show that new Big Data technologies can be incorporated into visualization pipeline to push out the size limits of graphs one can visually analyze.},
  archive  = {J},
  author   = {Alexandre Perrot and David Auber},
  doi      = {10.1109/TBDATA.2018.2869165},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {80-92},
  title    = {Cornac: Tackling huge graph visualization with big data infrastructure},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient path query processing over massive trajectories on
the cloud. <em>IEEE Transactions on Big Data</em>, <em>6</em>(1), 66–79.
(<a href="https://doi.org/10.1109/TBDATA.2018.2868936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A path query aims to find trajectories passing a given sequence of connected road segments within a time period. It is very useful in many urban applications: 1) traffic modeling, 2) frequent path mining, 3) intersection coordination, and 4) traffic anomaly detection. Existing solutions for path query processing are implemented based on single machines, which are not efficient for the following tasks: 1) indexing large-scale historical data; 2) handling real-time trajectory updates; and 3) processing concurrent path queries from urban data mining applications. In this paper, we design and implement a cloud-based path query processing framework based on Microsoft Azure. We modify existing suffix tree structure to index trajectories using Azure Table. The proposed system consists of two main parts: 1) back-end processing , which performs pre-processing (i.e., parsing and map-matching) and index building tasks with a distributed computing platform (i.e., Storm) used to efficiently handle massive real-time trajectory updates; and 2) query processing , which answers path queries using Azure Storm to improve efficiency and overcome I/O bottleneck. Extensive experiments are performed based on the real-time taxi trajectories from Guiyang City, the capital of Guizhou Province, China to confirm the system efficiency. We also demonstrate a real deployed traffic analysis system based on our query processing framework.},
  archive  = {J},
  author   = {Ruiyuan Li and Sijie Ruan and Jie Bao and Yanhua Li and Yingcai Wu and Liang Hong and Yu Zheng},
  doi      = {10.1109/TBDATA.2018.2868936},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {66-79},
  title    = {Efficient path query processing over massive trajectories on the cloud},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). E-optimal sensor selection for compressive sensing-based
purposes. <em>IEEE Transactions on Big Data</em>, <em>6</em>(1), 51–65.
(<a href="https://doi.org/10.1109/TBDATA.2018.2868120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Collaborative estimation of a sparse vector &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathbf {x}$&lt;/tex-math&gt;&lt;/inline-formula&gt; by &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$M$&lt;/tex-math&gt;&lt;/inline-formula&gt; potential measurements is considered. Each measurement is the projection of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathbf {x}$&lt;/tex-math&gt;&lt;/inline-formula&gt; obtained by a regressor, i.e., &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$y_m=\mathbf {a}_m^T \mathbf {x}$&lt;/tex-math&gt;&lt;/inline-formula&gt; . The problem of selecting &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$K$&lt;/tex-math&gt;&lt;/inline-formula&gt; sensor measurements from a set of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$M$&lt;/tex-math&gt;&lt;/inline-formula&gt; potential sensors is studied where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$K\ll M$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$K$&lt;/tex-math&gt;&lt;/inline-formula&gt; is less than the dimension of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathbf {x}$&lt;/tex-math&gt;&lt;/inline-formula&gt; . In other words, we aim to reduce the problem to an under-determined system of equations in which a sparse solution is desired. This paper suggests selecting sensors in a way that the reduced matrix construct a well conditioned measurement matrix. Our criterion is based on E-optimality, which is highly related to the restricted isometry property that provides some guarantees for sparse solution obtained by &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _1$&lt;/tex-math&gt;&lt;/inline-formula&gt; minimization. The proposed basic E-optimal selection is vulnerable to outlier and noisy data. The robust version of the algorithm is presented for distributed selection for big data sets. Moreover, an online implementation is proposed that involves partially observed measurements in a sequential manner. Our simulation results show the proposed method outperforms the other criteria for collaborative spectrum sensing in cognitive radio networks (CRNs). Our suggested selection method is evaluated in machine learning applications. It is used to pick up the most informative features/data. Specifically, the proposed method is exploited for face recognition with partial training data.},
  archive  = {J},
  author   = {Mohsen Joneidi and Alireza Zaeemzadeh and Behzad Shahrasbi and Guo-Jun Qi and Nazanin Rahnavard},
  doi      = {10.1109/TBDATA.2018.2868120},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {51-65},
  title    = {E-optimal sensor selection for compressive sensing-based purposes},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From megabits to CPU ticks: Enriching a demand trace in the
age of MEC. <em>IEEE Transactions on Big Data</em>, <em>6</em>(1),
43–50. (<a href="https://doi.org/10.1109/TBDATA.2018.2867025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {All the content consumed by mobile users, be it a web page or a live stream, undergoes some processing along the way; as an example, web pages and videos are transcoded to fit each device&#39;s screen. The recent multi-access edge computing (MEC) paradigm envisions performing such processing within the cellular network, as opposed to resorting to a cloud server on the Internet. Designing a MEC network, i.e., placing and dimensioning the computational facilities therein, requires information on how much computational power is required to produce the contents needed by the users. However, real-world demand traces only contain information on how much data is downloaded. In this paper, we demonstrate how to enrich demand traces with information about the computational power needed to process the different types of content, and we show the substantial benefit that can be obtained from using such enriched traces for the design of MEC-based networks.},
  archive  = {J},
  author   = {Francesco Malandrino and Carla-Fabiana Chiasserini and Giuseppe Avino and Marco Malinverno and Scott Kirkpatrick},
  doi      = {10.1109/TBDATA.2018.2867025},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {43-50},
  title    = {From megabits to CPU ticks: Enriching a demand trace in the age of MEC},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Closed walk sampler: An efficient method for estimating
eigenvalues of large graphs. <em>IEEE Transactions on Big Data</em>,
<em>6</em>(1), 29–42. (<a
href="https://doi.org/10.1109/TBDATA.2018.2865805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Eigenvalues of a graph are of high interest in graph analytics for Big Data due to their relevance to many important properties of the graph including network resilience, community detection and the speed of viral propagation. Accurate computation of eigenvalues of extremely large graphs is usually not feasible due to the prohibitive computational and storage costs and also because full access to many social network graphs is often restricted to most researchers. In this paper, we present a series of new sampling algorithms which solve both of the above-mentioned problems and estimate the two largest eigenvalues of a large graph efficiently and with high accuracy. Unlike previous methods which try to extract a subgraph with the most influential nodes, our algorithms sample only a small portion of the large graph via a simple random walk, and arrive at estimates of the two largest eigenvalues by estimating the number of closed walks of a certain length. Our experimental results using real graphs show that our algorithms are substantially faster while also achieving significantly better accuracy on most graphs than the current state-of-the-art algorithms.},
  archive  = {J},
  author   = {Guyue Han and Harish Sethu},
  doi      = {10.1109/TBDATA.2018.2865805},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {29-42},
  title    = {Closed walk sampler: An efficient method for estimating eigenvalues of large graphs},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Network representation learning: A survey. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(1), 3–28. (<a
href="https://doi.org/10.1109/TBDATA.2018.2850013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the widespread use of information technologies, information networks are becoming increasingly popular to capture complex relationships across various disciplines, such as social networks, citation networks, telecommunication networks, and biological networks. Analyzing these networks sheds light on different aspects of social life such as the structure of societies, information diffusion, and communication patterns. In reality, however, the large scale of information networks often makes network analytic tasks computationally expensive or intractable. Network representation learning has been recently proposed as a new learning paradigm to embed network vertices into a low-dimensional vector space, by preserving network topology structure, vertex content, and other side information. This facilitates the original network to be easily handled in the new vector space for further analysis. In this survey, we perform a comprehensive review of the current literature on network representation learning in the data mining and machine learning field. We propose new taxonomies to categorize and summarize the state-of-the-art network representation learning techniques according to the underlying learning mechanisms, the network information intended to preserve, as well as the algorithmic designs and methodologies. We summarize evaluation protocols used for validating network representation learning including published benchmark datasets, evaluation methods, and open source algorithms. We also perform empirical studies to compare the performance of representative algorithms on common datasets, and analyze their computational complexity. Finally, we suggest promising research directions to facilitate future study.},
  archive  = {J},
  author   = {Daokun Zhang and Jie Yin and Xingquan Zhu and Chengqi Zhang},
  doi      = {10.1109/TBDATA.2018.2850013},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {3-28},
  title    = {Network representation learning: A survey},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Message from the incoming editor-in-chief. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(1), 2. (<a
href="https://doi.org/10.1109/TBDATA.2020.2971766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Presents the introductory editorial for this issue of the publication.},
  archive  = {J},
  author   = {Jie Tang},
  doi      = {10.1109/TBDATA.2020.2971766},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {2},
  title    = {Message from the incoming editor-in-chief},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Message from the outgoing editor-in-chief. <em>IEEE
Transactions on Big Data</em>, <em>6</em>(1), 1. (<a
href="https://doi.org/10.1109/TBDATA.2020.2971791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Presents the editorial for this issue of the publication.},
  archive  = {J},
  author   = {Qiang Yang},
  doi      = {10.1109/TBDATA.2020.2971791},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {1},
  title    = {Message from the outgoing editor-in-chief},
  volume   = {6},
  year     = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
