<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms---67">THMS - 67</h2>
<ul>
<li><details>
<summary>
(2020a). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>50</em>(6), C3. (<a
href="https://doi.org/10.1109/THMS.2020.3033908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.3033908},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Organizing audible alarm sounds in the hospital: A
card-sorting study. <em>THMS</em>, <em>50</em>(6), 623–627. (<a
href="https://doi.org/10.1109/THMS.2020.3019363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hospitals, clinicians are presented with varied and disorganized alarm sounds from disparate devices. While there has been attention to reducing inactionable alarms to address alarm overload, little effort has been focused on organizing, simplifying, or improving the informativeness of alarms. In this article, we sought to elicit nurses&#39; tacit interpretation of alarm events to create an organizational structure to inform the design of advanced alarm sounds or integrated alert systems. We used open card sorting to evaluate nurses&#39; perception of the relatedness of different alarm events. A total of 70 hospital nurses sorted 89 alarm events into groups they believed could or should be indicated by the same sound. We conducted a factor analysis on a similarity matrix of the frequency of alarm event pairings to interpret how strongly alarm events loaded on different alarm groups (factors). We interpreted participants&#39; grouping rationale from their group labels and comments. The urgency of response was the most common grouping rationale. Participants also grouped monitoring-related events, device-related events, and events related to calls and patients. Our findings support the standardization and integration of alarm sounds across devices toward a simpler and more informative hospital alarm environment.},
  archive      = {J_THMS},
  author       = {Melanie C. Wright and Sydney Radcliffe and Suzanne Janzen and Judy Edworthy and Thomas J. Reese and Noa Segall},
  doi          = {10.1109/THMS.2020.3019363},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {623-627},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Organizing audible alarm sounds in the hospital: A card-sorting study},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inherent kinematic features of dynamic bimanual path
following tasks. <em>THMS</em>, <em>50</em>(6), 613–622. (<a
href="https://doi.org/10.1109/THMS.2020.3016084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bimanual coordination is critical in many robotic and haptic systems, such as surgical robots and rehabilitation robots. While these systems often incorporate two robotic manipulators for each limb, there may be a missed opportunity to leverage overarching models of human bimanual coordination to improve the way in which the robotic manipulators are controlled and respond to the dynamic human operator. In this article, we study the influences of several bimanual motion factors (e.g., symmetry and direction) on kinematic human joint-space features and performance outcome task-space features in a user study with 11 subjects and two haptic devices. Additionally, we evaluated the ability to use joint-space features to classify types of bimanual movement, showing the potential for a robotic system to predict how users coordinate their limbs. Three classifiers, namely, likelihood ratio, k-nearest neighbor, and support vector machine, are evaluated for classification accuracy with regard to the factor of number of targets. Likelihood ratio resulted in an accuracy of 79.6% with the majority of correct predictions occurring immediately at the start of movement. The task-space performance results reveal that despite the relative direction of both hands, reaching two targets results in lower performance than a single target, and symmetry alone does not contribute to performance disparity. In addition, dimensionless integrated absolute jerk is an indicator of superior performance for this particular task. Furthermore, these results align with current bimanual coordination theory by showing that manual performance disparities are a consequence of task constraints and conceptualization.},
  archive      = {J_THMS},
  author       = {Jacob R. Boehm and Nicholas P. Fey and Ann Majewicz},
  doi          = {10.1109/THMS.2020.3016084},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {613-622},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Inherent kinematic features of dynamic bimanual path following tasks},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3-d interface for the p300 speller BCI. <em>THMS</em>,
<em>50</em>(6), 604–612. (<a
href="https://doi.org/10.1109/THMS.2020.3016079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain-computer interface (BCI) is a common device for communication between the human brain and a computer. This article investigates the efficiency of using a 3-D interface for BCI machines. For this purpose, the P300 speller, which is a BCI device that enables the user to spell characters on a screen using brain waves, is modified. The classical virtual keyboard of the P300 speller is replaced with 3-D stereoscopic images, which enhances the ergonomic features of the device. Moreover, the flashing paradigm on a 3-D interface can affect the performance of the device in three ways: accuracy, speed, and capacity. This article proposes two different flashing paradigms called the natural 3-D and parallel 2-D interfaces and studied their effects regarding the three mentioned measures. The former flashes the planes in 3-D space, and the latter comprises flashes of parallel keyboards at different 3-D depths. The theoretical analysis of these effects is presented. The results are validated by experimental data obtained from real subjects and were compared with the classical 2-D interface. Both presented keyboards increase the speed of the device, while parallel 2-D has a better total performance than natural 3-D.},
  archive      = {J_THMS},
  author       = {Saman Noorzadeh and Bertrand Rivet and Christian Jutten},
  doi          = {10.1109/THMS.2020.3016079},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {604-612},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {3-D interface for the p300 speller BCI},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-dimensional fingertip force training with improved
haptic sensation via stochastic resonance. <em>THMS</em>,
<em>50</em>(6), 593–603. (<a
href="https://doi.org/10.1109/THMS.2020.3022859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To learn motor skills using a finger in many haptic training systems, a user places his or her finger in a holder to communicate with a haptic interface. It is known that the force-detection capability at the fingertip is reduced when the user&#39;s finger is enclosed in a holder. The learning performance might therefore be impaired in training. Stochastic resonance is known to improve sensitivity at the fingertip. We thus propose a fingertip force learning method using stochastic resonance. We first examine the effect of stochastic resonance on the user haptic performance in a two-dimensional task when the fingertip is within the finger holder. The outcomes indicate that the user fingertip sensitivity increases even when the finger is placed within a holder. We next perform force learning tasks in one- and two-dimensional space. Our proposed method that combines haptic feedback and stochastic resonance is compared with the method in which only haptic feedback is provided to the user in force leaning tasks. Results obtained using the proposed method indicate that the performance outcome of the proposed method is higher than that of the comparison method. This study demonstrates the potential of the proposed method for motor learning tasks.},
  archive      = {J_THMS},
  author       = {Komi Chamnongthai and Takahiro Endo and Fumitoshi Matsuno and Kenta Fujimoto and Marina Kosaka},
  doi          = {10.1109/THMS.2020.3022859},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {593-603},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Two-dimensional fingertip force training with improved haptic sensation via stochastic resonance},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The HoloLens augmented reality system provides valid
measures of gait performance in healthy adults. <em>THMS</em>,
<em>50</em>(6), 584–592. (<a
href="https://doi.org/10.1109/THMS.2020.3016082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomechanical measures are the gold standard in the assessment of gait in healthy and chronic disease populations. Augmented reality (AR) systems represent an opportunity to evaluate human movement under more realistic and interactive conditions. A barrier to integrating AR into healthcare is the unknown accuracy of systems in the quantification of human movement. This project aimed to determine the accuracy of the HoloLens relative to three-dimensional motion capture (MoCap) in quantifying gait. Ten healthy adults completed nine walking trials (n = 3 for slow, medium, and fast speed, respectively). Outcome measures included: cumulative walking distance, number of steps, step length, and speed. Statistical equivalence testing, using an a priori threshold of five percent, confirmed biomechanical measures derived from the HoloLens was equivalent to MoCap. Cumulative walking distance from the HoloLens was within 1.5-2.1% of the MoCap system for all walking speeds. Difference between systems in terms of movement accuracy was less than 3.7 cm across trials. Equivalence in outcomes makes the HoloLens appropriate for the quantification of frequently used gait variables to characterize walking performance. Future AR applications have the potential to deliver digital therapeutics to patient populations under more real-world conditions and monitor performance using objective and quantitative outcomes.},
  archive      = {J_THMS},
  author       = {Mandy Miller Koop and Anson B. Rosenfeldt and Joshua D. Johnston and Matthew C. Streicher and Jingan Qu and Jay L. Alberts},
  doi          = {10.1109/THMS.2020.3016082},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {584-592},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The HoloLens augmented reality system provides valid measures of gait performance in healthy adults},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An empirical approach to modeling user-system interaction
conflicts in smart homes. <em>THMS</em>, <em>50</em>(6), 573–583. (<a
href="https://doi.org/10.1109/THMS.2020.3017784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conflict is one of the important factors affecting user satisfaction and trust in smart environments, yet conflict modeling in mixed initiative smart environments has not been sufficiently explored. Most of the existing literature on conflict in smart homes are centered on conflicts between users. Although research has shown that about 75% of conflicts are between users and system [1] , only a few studies have considered user-system conflicts in smart homes. The aim of this article is to empirically propose both a definition and a run-time detection method for conflicts between users and smart home systems. Our empirical study is based on conflict sample scenarios collected from 163 users. Using clustering on these scenarios, we form an empirical definition of user-system conflict in smart homes. We also propose two functions that characterize each class of the collected scenarios, and we detect conflicts from this characterization. Our conflict detection model could help users achieve a more satisfactory experience in smart homes. Moreover, the model can offer benefits for system developers to design and deploy more reliable smart homes.},
  archive      = {J_THMS},
  author       = {Fereshteh Jadidi Miandashti and Mohammad Izadi and Ali Asghar Nazari Shirehjini and Shervin Shirmohammadi},
  doi          = {10.1109/THMS.2020.3017784},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {573-583},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An empirical approach to modeling user-system interaction conflicts in smart homes},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of a predictor-based framework in high-speed
teleoperated military UGVs. <em>THMS</em>, <em>50</em>(6), 561–572. (<a
href="https://doi.org/10.1109/THMS.2020.3018684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobility of teleoperated unmanned ground vehicles can be significantly compromised under large communication delays, if the delays are not compensated. This article considers a recently developed delay compensation theory and presents its first empirical evaluation in improving mobility and drivability of a high-speed teleoperated vehicle under large delays. The said delay compensation theory is a predictor-based framework. Two realizations of this framework are considered: a model-free realization that relies only on model-free predictors, and a blended realization, where the heading predictions from the model-free predictor are blended with those from a steering-model-based feedforward predictor for a more accurate prediction of the vehicle heading. A teleoperated track-following task is designed in a human-in-the-loop simulation platform. This platform is used to compare the teleoperation performance with and without the predictor-based framework under both constant and varying delays. Through repeated measurement analysis of variance, it is concluded that the predictor-based framework is effective in achieving a higher vehicle speed, more accurate lateral control, and better drivability as indicated by the three performance metrics of track completion time, track keeping error, and steering control effort, respectively. In addition, it is shown that the blended architecture can lead to further improvements in these metrics compared to using the model-free predictors alone. The analysis also shows that there is no statistically significant difference between constant and varying delay cases in the designed experiment, nor there is any direct relation between drivers&#39; skill level and level of improvement in metrics.},
  archive      = {J_THMS},
  author       = {Yingshi Zheng and Mark J. Brudnak and Paramsothy Jayakumar and Jeffrey L. Stein and Tulga Ersal},
  doi          = {10.1109/THMS.2020.3018684},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {561-572},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Evaluation of a predictor-based framework in high-speed teleoperated military UGVs},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated analysis of the origin of movement: An approach
based on cooperative games on graphs. <em>THMS</em>, <em>50</em>(6),
550–560. (<a href="https://doi.org/10.1109/THMS.2020.3016085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a computational method is proposed to automatically investigate the perception of the origin of full-body human movement and its propagation. The method is based on a mathematical game built over a suitably defined graph structure representing the human body. The players of this game are the graph vertices, which form a subset of body joints. Since each vertex contributes to a shared goal (i.e., to the way in which a specific movement-related feature is transferred among the joints), a cooperative game-theoretical model (specifically a transferable-utility game) is adopted, which is able (via the Shapley value) to measure the relevance of the various joints in human movement when performing full-body movement analysis. The method is theoretically investigated and applied to a motion capture dataset obtained from subjects who performed expressive movements. Finally, the method is validated through an online survey, in which several dancers/nondancers participated. The results show the capability of the proposed approach to represent the evolution of the most important joint responsible for originating each dancer&#39;s movement.},
  archive      = {J_THMS},
  author       = {Ksenia Kolykhalova and Giorgio Gnecco and Marcello Sanguineti and Gualtiero Volpe and Antonio Camurri},
  doi          = {10.1109/THMS.2020.3016085},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {550-560},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Automated analysis of the origin of movement: An approach based on cooperative games on graphs},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Facial expression imitation method for humanoid robot based
on smooth-constraint reversed mechanical model (SRMM). <em>THMS</em>,
<em>50</em>(6), 538–549. (<a
href="https://doi.org/10.1109/THMS.2020.3017781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the space-time similarity and motion smoothness of facial expression imitation (FEI), a real-time FEI method for a humanoid robot is proposed based on smooth-constraint reversed mechanical model (SRMM) by combining a sequence-to-sequence deep learning model and a motion-smoothing constraint. First, on the basis of facial data from a Kinect capture device, a facial feature vector is characterized based on 3 head postures, 17 facial animation units, and facial geometric deformation cascaded by Laplace coordinates. Second, a reversed mechanical model is constructed via a multilayer long short-term memory neural network to accomplish direct mapping from facial feature sequences to motor position sequences. Additionally, to overcome the motor chattering phenomenon during real-time FEI, a high-order polynomial is constructed to fit the position sequence of motors, and an SRMM is proposed and designed based on the deviation of position, velocity, and acceleration. Finally, aiming to imitate the real-time facial feature sequences of a performer captured from Kinect, the optimal position sequences generated based on the SRMM is sent to the hardware system to keep the space-time characteristics consistent with those of the performer. The experimental results demonstrate that the motor position deviation of the SRMM is less than 8%. The space-time similarity between the robot and the performer is greater than 85%, and the motion smoothness of the online FEI exceeded 90%. Compared with other related methods, the proposed method achieves a remarkable improvement in motor position deviation, space-time similarity, and motion smoothness.},
  archive      = {J_THMS},
  author       = {Zhong Huang and Fuji Ren and Min Hu and Sugen Chen},
  doi          = {10.1109/THMS.2020.3017781},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {538-549},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Facial expression imitation method for humanoid robot based on smooth-constraint reversed mechanical model (SRMM)},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of driving patterns and on-board feedback-based
training for proactive road safety monitoring. <em>THMS</em>,
<em>50</em>(6), 529–537. (<a
href="https://doi.org/10.1109/THMS.2020.3027525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road accidents and safe driving are one of the main concerns of transportation systems and the companies that explore different solutions to reduce the accident rate. The most interesting option to achieve this goal is through an on-board training of professional drivers to apply safe driving techniques during their work activity. The purpose of this study is to analyze a monitoring system that is not limited to the real-time vehicle tracking but is also capable of monitoring and providing real-time feedback and in-vehicle training. We analyze the influence of different sociodemographic factors on driving behavior. The analyzed data correspond to an urban public transport company, obtained from a study performed on 246 drivers. The drivers received training based on a blended learning system with an on-board feedback device, accompanied by both theoretical and practical sessions. The driving behavior of each driver is obtained from the data gathered from the vehicles that allow us to characterize their driving patterns. The information related to safe driving is completed with a list of the records of road accidents. The results of the sociodemographic influence on driving behavior provide significant information, giving an elaborated classification of safety driving patterns in order to apply intelligent transportation systems.},
  archive      = {J_THMS},
  author       = {Laura Pozueco and Nishu Gupta and Xabiel G. Pañeda and Roberto García and Alejandro G. Tuero and David Melendi and Abel Rionda and Víctor Corcoba},
  doi          = {10.1109/THMS.2020.3027525},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {529-537},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Analysis of driving patterns and on-board feedback-based training for proactive road safety monitoring},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Characteristics of recovery motion resulting from side
contact with a physical assistant robot worn during gait. <em>THMS</em>,
<em>50</em>(6), 518–528. (<a
href="https://doi.org/10.1109/THMS.2020.3016098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although a wearable assist robot helps to enhance the gait ability of the wearer, it can cause gait instability in an emergency. A collision with an environmental object is one source of such instability. Insufficient adaptation to change by the physical frame, and restriction of joint motion, increases the risk of collision and fall of the wearer owing to the robot. In this article, the reaction motion owing to side contact, which applies a spin moment to the body, is investigated for the wearers of a physical assistant robot. In particular, reaction motions in the horizontal plane, such as body rotation, foot direction, and center-of-mass position, are investigated. Factor analysis and cluster analysis are performed, and two reaction patterns—the rotation group and the straight group—are determined. The motion of these groups suggests an approach for navigation through an obstacle while using an assist robot. Furthermore, the gait phase at collision time is considered as a determinant of the reaction pattern and the reaction pattern is found to affect the fall mode. The results of the article suggest that the joints of the physical assistant robot should be equipped with sufficient degrees of freedom, at least in hip rotation, such that it is capable of rotating the body and feet in the horizontal plane to reduce fall risk.},
  archive      = {J_THMS},
  author       = {Yasuhiro Akiyama and Ryota Kushida and Shogo Okamoto and Yoji Yamada},
  doi          = {10.1109/THMS.2020.3016098},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {518-528},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Characteristics of recovery motion resulting from side contact with a physical assistant robot worn during gait},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An AI-based visual aid with integrated reading assistant for
the completely blind. <em>THMS</em>, <em>50</em>(6), 507–517. (<a
href="https://doi.org/10.1109/THMS.2020.3027534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blindness prevents a person from gaining knowledge of the surrounding environment and makes unassisted navigation, object recognition, obstacle avoidance, and reading tasks a major challenge. In this work, we propose a novel visual aid system for the completely blind. Because of its low cost, compact size, and ease-of-integration, Raspberry Pi 3 Model B+ has been used to demonstrate the functionality of the proposed prototype. The design incorporates a camera and sensors for obstacle avoidance and advanced image processing algorithms for object detection. The distance between the user and the obstacle is measured by the camera as well as ultrasonic sensors. The system includes an integrated reading assistant, in the form of the image-to-text converter, followed by an auditory feedback. The entire setup is lightweight and portable and can be mounted onto a regular pair of eyeglasses, without any additional cost and complexity. Experiments are carried out with 60 completely blind individuals to evaluate the performance of the proposed device with respect to the traditional white cane. The evaluations are performed in controlled environments that mimic real-world scenarios encountered by a blind person. Results show that the proposed device, as compared with the white cane, enables greater accessibility, comfort, and ease of navigation for the visually impaired.},
  archive      = {J_THMS},
  author       = {Muiz Ahmed Khan and Pias Paul and Mahmudur Rashid and Mainul Hossain and Md Atiqur Rahman Ahad},
  doi          = {10.1109/THMS.2020.3027534},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {507-517},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An AI-based visual aid with integrated reading assistant for the completely blind},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Techniques for constructing indoor navigation systems for
the visually impaired: A review. <em>THMS</em>, <em>50</em>(6), 492–506.
(<a href="https://doi.org/10.1109/THMS.2020.3016051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sight is an important sensory input for humans to navigate their surrounding environment independently. Tasks that are simple for the sighted are often difficult for a visually impaired person. Throughout the years, many researchers dedicated their time and efforts to design and implement technologies and devices that can help a visually impaired person to navigate in unknown areas independently. An assistive navigation system integrates multiple advanced technologies such as wayfinding, obstacle avoidance, and human-machine interface to assist the visually impaired. Individual components have been explored extensively in the literature and it is a daunting task to review this literature in its entirety. In this article, a compact but comprehensive collection of available methods to build each component is summarized. The discussed methods in this article may have not been implemented for a navigation assistive technology for visually impaired directly but they have great potential to be integrated as an individual building block for this task. Given that every approach has its own challenges, methods to combat those challenges have been presented. The purpose of this article is to present researchers with a comprehensive review of the available components and methods by recapping the essential knowledge to understand the nuances in each component and give them a baseline to start with.},
  archive      = {J_THMS},
  author       = {Roya Norouzi Kandalan and Kamesh Namuduri},
  doi          = {10.1109/THMS.2020.3016051},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {492-506},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Techniques for constructing indoor navigation systems for the visually impaired: A review},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review of shared control for automated vehicles: Theory
and applications. <em>THMS</em>, <em>50</em>(6), 475–491. (<a
href="https://doi.org/10.1109/THMS.2020.3017748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The last decade has shown an increasing interest on advanced driver assistance systems (ADAS) based on shared control, where automation is continuously supporting the driver at the control level with an adaptive authority. A first look at the literature offers two main research directions: 1) an ongoing effort to advance the theoretical comprehension of shared control, and 2) a diversity of automotive system applications with an increasing number of works in recent years. Yet, a global synthesis on these efforts is not available. To this end, this article covers the complete field of shared control in automated vehicles with an emphasis on these aspects: 1) concept, 2) categories, 3) algorithms, and 4) status of technology. Articles from the literature are classified in theory- and application-oriented contributions. From these, a clear distinction is found between coupled and uncoupled shared control. Also, model-based and model-free algorithms from these two categories are evaluated separately with a focus on systems using the steering wheel as the control interface. Model-based controllers tested by at least one real driver are tabulated to evaluate the performance of such systems. Results show that the inclusion of a driver model helps to reduce the conflicts at the steering. Also, variables such as driver state, driver effort, and safety indicators have a high impact on the calculation of the authority. Concerning the evaluation, driver-in-the-loop simulators are the most common platforms, with few works performed in real vehicles. Implementation in experimental vehicles is expected in the upcoming years.},
  archive      = {J_THMS},
  author       = {Mauricio Marcano and Sergio Díaz and Joshué Pérez and Eloy Irigoyen},
  doi          = {10.1109/THMS.2020.3017748},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {475-491},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A review of shared control for automated vehicles: Theory and applications},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>50</em>(5), C3. (<a
href="https://doi.org/10.1109/THMS.2020.3021783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.3021783},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical eye-tracking data analytics for human fatigue
detection at a traffic control center. <em>THMS</em>, <em>50</em>(5),
465–474. (<a href="https://doi.org/10.1109/THMS.2020.3016088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye-tracking-based human fatigue detection at traffic control centers suffers from an unavoidable problem of low-quality eye-tracking data caused by noisy and missing gaze points. In this article, the authors conducted pioneering work by investigating the effects of data quality on eye-tracking-based fatigue indicators and by proposing a hierarchical-based interpolation approach to extract the eye-tracking-based fatigue indicators from low-quality eye-tracking data. This approach adaptively classified the missing gaze points and hierarchically interpolated them based on the temporal-spatial characteristics of the gaze points. In addition, the definitions of applicable fixations and saccades for human fatigue detection is proposed. Two experiments are conducted to verify the effectiveness and efficiency of the method in extracting eye-tracking-based fatigue indicators and detecting human fatigue. The results indicate that most eye-tracking parameters are significantly affected by the quality of the eye-tracking data. In addition, the proposed approach can achieve much better performance than the classic velocity threshold identification algorithm (I-VT) and a state-of-the-art method (U&#39;n&#39;Eye) in parsing low-quality eye-tracking data. Specifically, the proposed method attained relatively stable eye-tracking-based fatigue indicators and reported the highest accuracy in human fatigue detection. These results are expected to facilitate the application of eye movement-based human fatigue detection in practice.},
  archive      = {J_THMS},
  author       = {Fan Li and Chun-Hsien Chen and Gangyan Xu and Li-Pheng Khoo},
  doi          = {10.1109/THMS.2020.3016088},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {465-474},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Hierarchical eye-tracking data analytics for human fatigue detection at a traffic control center},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating an LPV model of driver neuromuscular admittance
using grip force as scheduling variable. <em>THMS</em>, <em>50</em>(5),
454–464. (<a href="https://doi.org/10.1109/THMS.2020.2989685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can rapidly change their low-frequency arm dynamics to resist forces or give way to them. Quantifying driver time-varying arm dynamics is important to develop steer-by-wire and haptic support systems. Conventional linear time-invariant (LTI) identification, and even time-varying techniques such as wavelets, fail to capture fast changing dynamics. Moreover, such techniques require perturbation signals on the steering wheel (SW), which may affect steering feel and control behavior. We propose a novel two-step method to estimate time-varying driver admittance, using unobtrusive grip-force measurements of the hands on the wheel to schedule a linear parameter-varying (LPV) model that captures the full admittance range. A total of 18 subjects participated in two experiments in a simulator with an actuated SW. In a sensorimotor control experiment, we first establish the grip force and admittance relationship, requiring subjects to perform a boundary tracking task where perturbations on the wheel enabled local LTI identification. Six boundary widths is used to evoke admittance changes, after which a global LPV model is obtained through interpolation between the local models. Results show an inverse relationship between grip force and admittance and that the LPV model accurately captures the admittance settings (fit percentage &gt; 90%). Second, a driving experiment is followed that aims to evoke differences in grip force and admittance in response to varying road widths, offering more realistic data to evaluate the LPV model predictions. Results show that the LPV model accurately describes adaptations in admittance to road width. Our method allows for online estimation of time-varying admittance during driving, without applying force perturbations.},
  archive      = {J_THMS},
  author       = {Anne J. Pronker and David A. Abbink and Marinus M. van Paassen and Max Mulder},
  doi          = {10.1109/THMS.2020.2989685},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {454-464},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Estimating an LPV model of driver neuromuscular admittance using grip force as scheduling variable},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human threshold model for perceiving changes in system
dynamics. <em>THMS</em>, <em>50</em>(5), 444–453. (<a
href="https://doi.org/10.1109/THMS.2020.2989383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limitations of a haptic device can cause distortions of the force feedback it presents. Just-noticeable difference (JND) in system dynamics is important for creating transparent haptic interaction. Based on the previous work, this article presents a unified model that extends the existing JND rule. Our approach projects the JNDs in the mechanical properties of a second-order mass-spring-damper system onto the real and imaginary components of the system&#39;s frequency response function (FRF). We discuss the results of two experiments and show that the JNDs obtained for both the real and imaginary components can be expressed as the same fraction of, and thus are proportional to, the magnitude of the total system&#39;s FRF. Furthermore, the findings are generalized to cases where the system&#39;s dynamics order is different than two. What results is a unified model that accurately describes the threshold for changes in human perception of any linear system dynamics with only two dimensions: the real and imaginary axes in the complex plane.},
  archive      = {J_THMS},
  author       = {Wei Fu and M. M. van Paassen and Max Mulder},
  doi          = {10.1109/THMS.2020.2989383},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {444-453},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human threshold model for perceiving changes in system dynamics},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agreement study using gesture description analysis.
<em>THMS</em>, <em>50</em>(5), 434–443. (<a
href="https://doi.org/10.1109/THMS.2020.2992216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing adequate gestures for touchless interfaces is a challenging task that has a direct impact on human-computer interaction. Such gestures are commonly determined by the designer, ad-hoc, rule-based, or agreement-based methods. Previous approaches to assess agreement grouped the gestures into equivalence classes and ignored the integral properties that are shared between them. In this article, we propose a generalized framework that inherently incorporates the gesture descriptors into the agreement analysis. In contrast to previous approaches, we represent gestures using binary description vectors and allow them to be partially similar. In this context, we introduce a new metric referred to as soft agreement rate (SAR) to measure the level of agreement and provide a mathematical justification for this metric. Furthermore, we perform computational experiments to study the behavior of SAR and demonstrate that existing agreement metrics are a special case of our approach. Our method is evaluated and tested through a guessability study conducted with a group of neurosurgeons. Nevertheless, our formulation can be applied to any other user-elicitation study. Results show that the level of agreement obtained by SAR is 2.64 times higher than the previous metrics. Finally, we show that our approach complements the existing agreement techniques by generating an artificial lexicon based on the most agreed properties.},
  archive      = {J_THMS},
  author       = {Naveen Madapana and Glebys Gonzalez and Lingsong Zhang and Richard Rodgers and Juan Wachs},
  doi          = {10.1109/THMS.2020.2992216},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {434-443},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Agreement study using gesture description analysis},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding a secure place: A map-based crowdsourcing system for
people with autism. <em>THMS</em>, <em>50</em>(5), 424–433. (<a
href="https://doi.org/10.1109/THMS.2020.2984743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People with autism have idiosyncratic sensory experiences, which may impact on how they live the “spaces” of their everyday life. Starting from an investigation of their conception and experience of “secure places,” we defined a series of user requirements for designing technology that supports their everyday movements in the urban environment. On the basis of such requirements, we developed an interactive system that leverages crowdsourcing mechanisms to map places that are perceived as secure by the population with autism.},
  archive      = {J_THMS},
  author       = {Amon Rapp and Federica Cena and Claudio Schifanella and Guido Boella},
  doi          = {10.1109/THMS.2020.2984743},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {424-433},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Finding a secure place: A map-based crowdsourcing system for people with autism},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A smartphone-based adaptive recognition and real-time
monitoring system for human activities. <em>THMS</em>, <em>50</em>(5),
414–423. (<a href="https://doi.org/10.1109/THMS.2020.2984181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) using smartphones provides significant healthcare guidance for telemedicine and long-term treatment. Machine learning and deep learning (DL) techniques are widely utilized for the scientific study of the statistical models of human behaviors. However, the performance of existing HAR platforms is limited by complex physical activity. In this article, we proposed an adaptive recognition and real-time monitoring system for human activities (Ada-HAR), which is expected to identify more human motions in dynamic situations. The Ada-HAR framework introduces an unsupervised online learning algorithm that is independent of the number of class constraints. Furthermore, the adopted hierarchical clustering and classification algorithms label and classify 12 activities (five dynamics, six statics, and a series of transitions) autonomously. Finally, practical experiments have been performed to validate the effectiveness and robustness of the proposed algorithms. Compared with the methods mentioned in the literature, the results show that the DL-based classifier obtains a higher recognition rate (95.15%, waist, and 92.20%, pocket). The decision-tree-based classifier is the fastest method for modal evolution. Finally, the Ada-HAR system can monitor human activity in real time, regardless of the direction of the smartphone.},
  archive      = {J_THMS},
  author       = {Wen Qi and Hang Su and Andrea Aliverti},
  doi          = {10.1109/THMS.2020.2984181},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {414-423},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A smartphone-based adaptive recognition and real-time monitoring system for human activities},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multiviewpoint outdoor dataset for human action
recognition. <em>THMS</em>, <em>50</em>(5), 405–413. (<a
href="https://doi.org/10.1109/THMS.2020.2971958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in deep neural networks have contributed to near-perfect results for many computer vision problems, such as object recognition, face recognition, and pose estimation. However, human action recognition is still far from human-level performance. Owing to the articulated nature of the human body, it is challenging to detect an action from multiple viewpoints, particularly from an aerial viewpoint. This is further compounded by a scarcity of datasets that cover multiple viewpoints of actions. To fill this gap and enable research in wider application areas, in this article we present a multiviewpoint outdoor action recognition dataset collected from YouTube and our own drone. The dataset consists of 20 dynamic human action classes, 2324 video clips, and 503 086 frames. All videos are cropped and resized to 720 × 720 without distorting the original aspect ratio of the human subjects in videos. This dataset should be useful to many research areas, including action recognition, surveillance, and situational awareness. We evaluate the dataset with a two-stream convolutional neural network architecture coupled with a recently proposed temporal pooling scheme called kernelized rank pooling that produces nonlinear feature subspace representations. The overall baseline action recognition accuracy is 74.0%.},
  archive      = {J_THMS},
  author       = {Asanka G. Perera and Yee Wei Law and Titilayo T. Ogunwa and Javaan Chahl},
  doi          = {10.1109/THMS.2020.2971958},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {405-413},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A multiviewpoint outdoor dataset for human action recognition},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Physiological synchrony revealed by delayed coincidence
count: Application to a cooperative complex environment. <em>THMS</em>,
<em>50</em>(5), 395–404. (<a
href="https://doi.org/10.1109/THMS.2020.2986417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchrony at the physiological level is an objective measure that can be used to investigate cooperation between human agents. This physiological synchrony has been experimentally observed in different dyadic contexts through measures of the autonomous system such as cardiac measures. Various metrics are used to characterize synchrony between participants such as crosscorrelation, weighted coherence, or cross recurrence quantification analysis and with a wide variety of paradigms. We propose the delayed coincidence count as a new method for assessing cardiac synchrony. Delayed coincidence count has already been used to characterize synchrony in firing neurons populations. While being straightforward and computationally light, this method has already been formally proven to be statistically robust. A complex dynamic microworld is designed with two difficulty levels and two cooperation conditions. A total of 40 participants, i.e., 20 teams, voluntarily has conducted the experiment. The delayed coincidence count method (with a coincidence threshold δ of 20 ms) reveals a significant synchrony (p &lt;; .01) during the cooperative and high difficulty condition only, while the other methods did not. The results are interpreted in terms of interaction intensity in accordance with recent literature.},
  archive      = {J_THMS},
  author       = {Kevin J. Verdière and Mélisande Albert and Frédéric Dehais and Raphaëlle N. Roy},
  doi          = {10.1109/THMS.2020.2986417},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {395-404},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Physiological synchrony revealed by delayed coincidence count: Application to a cooperative complex environment},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reciprocity and its neurological correlates in human-agent
cooperation. <em>THMS</em>, <em>50</em>(5), 384–394. (<a
href="https://doi.org/10.1109/THMS.2020.2992224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reciprocal cooperation is prevalent in human society. Understanding human reciprocal cooperation in human-agent interaction can help design human-agent systems that promote cooperation and joint performance. Studies have found that people reciprocate cooperative behavior when interacting with computer agents in social dilemma games. However, few studies have investigated human reciprocal cooperation with agents in complex dynamic environments. This article examines the behavioral and neurological patterns of reciprocal cooperation in a hospital management microworld experiment. The participants (n = 30) work with both high- and low-cooperation computer agents to share resources to cope with dynamic demands. Participants&#39; resource sharing behaviors were recorded and their prefrontal cortex (PFC) activation was measured using functional near-infrared spectroscopy (fNIRS) technology. Similar to previous studies conducted with participants in the United States, results demonstrate that participants in China showed reciprocal cooperation behaviors with the agents. Specifically, participants share more resources and achieve higher performance when working with a high-cooperation agent than with a low-cooperation agent. A high activation level is detected in the right dorsolateral PFC when working with a high-cooperation agent. Other PFC activation patterns imply that cooperation could be unnecessarily mentally taxing in certain situations. These findings suggest that human cooperativeness in human-agent systems can be calibrated by an agent&#39;s cooperation behavior. System designers should design for appropriate cooperativeness and avoid the inefficient use of system resources. Neurological measures could be a useful tool to investigate the mental process in human-agent cooperation.},
  archive      = {J_THMS},
  author       = {Jiahao Li and Shen Dong and Erin K. Chiou and Jie Xu},
  doi          = {10.1109/THMS.2020.2992224},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {384-394},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Reciprocity and its neurological correlates in human-agent cooperation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Musculoskeletal model for path generation and modification
of an ankle rehabilitation robot. <em>THMS</em>, <em>50</em>(5),
373–383. (<a href="https://doi.org/10.1109/THMS.2020.2989688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While newer designs and control approaches are being proposed for rehabilitation robots, vital information from the human musculoskeletal system should also be considered. Incorporating knowledge about joint biomechanics during the development of robot controllers can enhance the safety and performance of robot-aided treatments. In this article, the optimal path or trajectories of a parallel ankle rehabilitation robot were generated by minimizing joint reaction moments and the tension along ligaments and muscle-tendon units. The simulations showed that using optimized robot paths, user efforts could be reduced to 80%, thereby ensuring less strain on weaker or stiffer ligaments, etc. Additionally, to limit the moments applied by the robot in stiff or constrained directions, the intended robot path was modified to move the commanded position in the direction opposite to that of the position error. Such online modification of the robot path can lead to a reduction in forces applied by a robot to the subject. Simulation results and experimental findings with healthy subjects using an ankle rehabilitation robot prototype and subsequent statistical analysis further validated that path modification based on ankle joint biomechanics results in a reduction in undesired forces experienced by human users during treatment.},
  archive      = {J_THMS},
  author       = {Prashant K. Jamwal and Shahid Hussain and Yun H. Tsoi and Sheng Q. Xie},
  doi          = {10.1109/THMS.2020.2989688},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {373-383},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Musculoskeletal model for path generation and modification of an ankle rehabilitation robot},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). State-of-the-art robotic devices for wrist rehabilitation:
Design and control aspects. <em>THMS</em>, <em>50</em>(5), 361–372. (<a
href="https://doi.org/10.1109/THMS.2020.2976905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot-assisted physical therapy of the upper limb is becoming popular among the rehabilitation community. The wrist is the second most complicated joint in the upper limb after shoulder in terms of degrees of freedom. Several robotic devices have been developed during the past three decades for wrist joint rehabilitation. Intensive physical therapy and repetitive self-practice, with objective measurement of performance, could be provided by using these wrist rehabilitation robots at a low cost. There has been an increasing trend in the development of wrist rehabilitation robots to provide safe and customized therapy according to the disability level of patients. The mechanical design and control paradigms are two active fields of research undergoing rapid developments in the field of robot-assisted wrist rehabilitation. The mechanical design of these robots could be divided into the categories of end-effector based robots and wearable robotic orthoses. The control for these wrist rehabilitation robots could also be divided into the conventional trajectory tracking control mode and the assist-as-needed control mode for providing customized robotic assistance. This article presents a review of the mechanical design and control aspects of wrist rehabilitation robots. Experimental evaluations of these robots with healthy and neurologically impaired are also discussed along with the future directions of research in the design and control domains of wrist rehabilitation robots.},
  archive      = {J_THMS},
  author       = {Shahid Hussain and Prashant K. Jamwal and Paulette Van Vliet and Mergen H. Ghayesh},
  doi          = {10.1109/THMS.2020.2976905},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {361-372},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {State-of-the-art robotic devices for wrist rehabilitation: Design and control aspects},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>50</em>(4), C3. (<a
href="https://doi.org/10.1109/THMS.2020.3004104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.3004104},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). IEEE foundation realize the full potential of IEEE.
<em>THMS</em>, <em>50</em>(4), 359. (<a
href="https://doi.org/10.1109/THMS.2020.3004108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement: The world&#39;s most daunting challenges require innovations in engineering, and IEEE is committed to finding the solutions. The IEEE Foundation is leading a special campaign to raise awareness, create partnerships, and generate financial resources needed to combat these global challenges.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.3004108},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {359},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE foundation realize the full potential of IEEE},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effect of room complexity on physical object selection
performance in 3-d mobile user interfaces. <em>THMS</em>,
<em>50</em>(4), 349–357. (<a
href="https://doi.org/10.1109/THMS.2020.2984750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important challenge in smart environments is how to manipulate the smart objects. Although mobile applications are typically used for controlling a smart environment, no previous study has evaluated the users performance in manipulating smart objects under different environmental complexities. This article presents an experimental comparison between three different selection techniques 3-D, 2-D, and physical user interfaces (UIs). We evaluate these techniques across two levels of environment complexity measuring 51 participants timing data and errors. Our results indicate that the 3-D UI is superior for task completion time and error, and the 2-D UI is not a better solution than the physical UI when the environment is not complex. The results also show the importance of considering the environment complexity in choosing the proper UI.},
  archive      = {J_THMS},
  author       = {Maryam Rezaie and Morteza Malekmakan and Ali Asghar Nazari Shirehjini and Shervin Shirmohammadi},
  doi          = {10.1109/THMS.2020.2984750},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {349-357},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The effect of room complexity on physical object selection performance in 3-D mobile user interfaces},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparative evaluation of a virtual reality table and a
HoloLens-based augmented reality system for anatomy training.
<em>THMS</em>, <em>50</em>(4), 337–348. (<a
href="https://doi.org/10.1109/THMS.2020.2984746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anatomy training with real cadavers poses many practical problems for which new training and educational solutions have been developed making use of technologies based on real-time 3-D graphics. Although virtual reality (VR) and augmented reality (AR) have been previously used in the medical field, it is not easy to select the right 3-D technology or setup for each particular problem. For this reason, this article presents a comprehensive comparative study with 82 participants between two different 3-D interactive setups: an optical-based AR setup, implemented with a Microsoft HoloLens device, and a semi-immersive setup based on a VR Table. Both setups are tested using an anatomy training software application. Our primary hypothesis is that there would be statistically significant differences between the use of the AR application and the use of the VR Table. Our secondary hypothesis is that user preference and recommendation for the VR setup would be higher than for the HoloLens-based system. After completing two different tasks with both setups, the participants filled two questionnaires about the use of the anatomy training application. Three objective measures are also recorded (time, number of movements, and a score). The results of the experiments show that more than two-thirds of the users prefer, recommend, and find more useful the VR setup. The results also show that there are statistically significant differences in the use of both systems in favor of the VR Table.},
  archive      = {J_THMS},
  author       = {Ramiro Serrano Vergel and Pedro Morillo Tena and Sergio Casas Yrurzum and Carolina Cruz-Neira},
  doi          = {10.1109/THMS.2020.2984746},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {337-348},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A comparative evaluation of a virtual reality table and a HoloLens-based augmented reality system for anatomy training},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development and validation of a scenario-based drilling
simulator for training and evaluating human factors. <em>THMS</em>,
<em>50</em>(4), 327–336. (<a
href="https://doi.org/10.1109/THMS.2020.2969014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drilling and completing an oil/gas well is a time-sensitive and high-value operation, in which environment/system parameters change in unseen, unpredictable environments. Safety issues arise at every stage. Drilling principles can be taught using traditional methods, but safety and event response are difficult to teach in such formats. Here, in this article, we integrate a hardware-in-the-loop simulator, downhole physics, and auxiliary touchscreen interfaces (similar to a rig&#39;s add-on equipment) to develop a realistic, real-time drilling simulator for well control operation training. Realistic operational data are supplied to the simulator representative of downhole operations, including unplanned well events. The well plan accounts for drilling parameter changes, the pore-pressure fracture-gradient drilling window, mud weights, etc., which occur in response to the unplanned events. The developed simulator is used for hands-on training, human factor studies, model verification, and evaluating new auxiliary equipment and/or operational procedures. A critical research objective was evaluating the accuracy/realism of the developed system. To do so, eight petroleum engineering students and 11 certified drillers were trained and asked to complete a comprehensive (&gt;6 h) drilling operation. System accuracy was measured by comparing how new versus experienced operators learned to operate the simulator, execute mission-critical tasks, and respond to unplanned events. The results validate the realism of the developed simulator and scenarios, since personnel with prior drilling experience took significantly less time to master the system.},
  archive      = {J_THMS},
  author       = {Hong-Chih Chan and Melissa M. Lee and Gurtej Singh Saini and Mitch Pryor and Eric van Oort},
  doi          = {10.1109/THMS.2020.2969014},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {327-336},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Development and validation of a scenario-based drilling simulator for training and evaluating human factors},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical modeling of visual attention of junior and
senior anesthesiologists during the induction of general anesthesia in
real and simulated cases. <em>THMS</em>, <em>50</em>(4), 317–326. (<a
href="https://doi.org/10.1109/THMS.2020.2983817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visually rich working environments, it is important for operators to distribute their visual attention in an optimal fashion in order to operate safely. Computational models can provide a systematic method of investigating the attention distribution of humans. In this article, we reanalyze eye tracking data from anesthesiologists when inducing general anesthesia to test whether the so-called expectancy value version of the salience, effort, expectancy, value (SEEV) model can accommodate the visual attention distribution of anesthesiologists, and to investigate the effect of case (real versus simulated cases) and experience (junior versus senior) on the expectancy value model fit. The overall model fit is good (predicted-observed percentage dwell time correlation of 0.810, R2 = 0.656). We observe that the model fit is better in simulated cases than real ones. In addition, the model fit is good for junior anesthesiologists independent of the case, but that there is an even better model fit in simulated cases than in real ones for senior anesthesiologists (case × experience interaction). Overall, the expectancy value model can be validated. However, at least within the context of anesthesiology, the full SEEV model may be needed to capture the large and distractive visual work environment of anesthesiologists. From a practical point of view, previous research suggested that anesthesiologists pay more attention to monitoring in simulated cases. However, the SEEV analysis suggests that anesthesiologists do not pay extra attention to monitoring equipment in simulated cases, but may not be able to pay enough attention to monitoring equipment in real cases.},
  archive      = {J_THMS},
  author       = {Tobias Grundgeiger and Thomas Wurmb and Oliver Happel},
  doi          = {10.1109/THMS.2020.2983817},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {317-326},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Statistical modeling of visual attention of junior and senior anesthesiologists during the induction of general anesthesia in real and simulated cases},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An ego-vision system for discovering human joint attention.
<em>THMS</em>, <em>50</em>(4), 306–316. (<a
href="https://doi.org/10.1109/THMS.2020.2965429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint attention often happens during social interactions, in which individuals share focus on the same object. This article proposes an egocentric vision-based system (ego-vision system) that aims to discover the objects looked at jointly by a group of persons engaged in interactive activities. The proposed system relies on a collection of wearable eye-tracking cameras that provide an egocentric view of the interaction scenes as well as points-of-gaze measurement of each participant. Technically in our system, we develop a hierarchical conditional random field (CRF) based graphical model that can temporally localize joint attention periods and spatially segment objects of joint attention. By solving these two coupled tasks together in an iterative optimization procedure, we show that human joint attention can be reliably discovered from videos even with cluttered background and noisy gaze measurement. A new dataset of joint attention is collected and annotated for evaluating the two tasks of joint attention where two to four persons are involved. Experimental results demonstrate that our approach achieves state-of-the-art performance on both tasks of spatial segmentation and temporal localization of joint attention.},
  archive      = {J_THMS},
  author       = {Yifei Huang and Minjie Cai and Yoichi Sato},
  doi          = {10.1109/THMS.2020.2965429},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {306-316},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An ego-vision system for discovering human joint attention},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-modality matching for evaluating user experience of
emerging mobile EEG technology. <em>THMS</em>, <em>50</em>(4), 298–305.
(<a href="https://doi.org/10.1109/THMS.2020.2989380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging technology for brain-state monitoring offers the possibility to conduct measurements outside the laboratory. However, user-experience research is lacking. In this article, we present and test an approach for determining the development of user experience in the course of time using the so-called cross-modality matching (CMM). We conducted experiments with 24 subjects and evaluated seven mobile electroencephalography (EEG) devices. Using the CMM method, we registered the headset pressure of the EEG devices and subject&#39;s mood. We are able to identify a correlation between headset pressure and mood and to observe time trends. Subjects rated the heaviest, pin-based device as less comfortable in the course of time. The gel-based EEG cap is the most comfortable device regarding its long-time properties. The CMM approach for user-experience evaluation of new EEG technologies is direct, rapid, and easy to perform. This fact creates new opportunities for future studies in the field of user experience and human factors.},
  archive      = {J_THMS},
  author       = {Thea Radüntz and Beate Meffert},
  doi          = {10.1109/THMS.2020.2989380},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {298-305},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cross-modality matching for evaluating user experience of emerging mobile EEG technology},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A usability study of low-cost wireless brain-computer
interface for cursor control using online linear model. <em>THMS</em>,
<em>50</em>(4), 287–297. (<a
href="https://doi.org/10.1109/THMS.2020.2983848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer cursor control using electroencephalogram (EEG) signals is a common and well-studied brain-computer interface (BCI). The emphasis of the literature has been primarily on evaluation of the objective measures of assistive BCIs such as accuracy of the neural decoder whereas the subjective measures such as user&#39;s satisfaction play an essential role for the overall success of a BCI. As far as we know, the BCI literature lacks a comprehensive evaluation of the usability of the mind-controlled computer cursor in terms of decoder efficiency (accuracy), user experience, and relevant confounding variables concerning the platform for the public use. To fill this gap, we conducted a 2-D EEG-based cursor control experiment among 28 healthy participants. The computer cursor velocity was controlled by the imagery of hand movement using a paradigm presented in the literature named imagined body kinematics with a low-cost wireless EEG headset. In this article, we evaluated the usability of the platform for different objective and subjective measures while we investigated the extent to which the training phase may influence the ultimate BCI outcome. We conducted pre- and post-BCI experiment interview questionnaires to evaluate the usability. Analyzing the questionnaires and the testing phase outcome shows a positive correlation between the individuals&#39; ability of visualization and their level of mental controllability of the cursor. Despite individual differences, analyzing training data shows the significance of electrooculogram on the predictability of the linear model. The results of this work may provide useful insights towards designing a personalized user-centered assistive BCI.},
  archive      = {J_THMS},
  author       = {Reza Abiri and Soheil Borhani and Justin Kilmarx and Connor Esterwood and Yang Jiang and Xiaopeng Zhao},
  doi          = {10.1109/THMS.2020.2983848},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {287-297},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A usability study of low-cost wireless brain-computer interface for cursor control using online linear model},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance of a steady-state visual evoked potential and
eye gaze hybrid brain-computer interface on participants with and
without a brain injury. <em>THMS</em>, <em>50</em>(4), 277–286. (<a
href="https://doi.org/10.1109/THMS.2020.2983661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain-computer interface (BCI) and the tracking of eye gaze provide modalities for human-machine communication and control. In this article, we provide the evaluation of a collaborative BCI and eye gaze approach, known as a hybrid BCI. The combined inputs interact with a virtual environment to provide actuation according to a four-way menu system. The following two approaches are evaluated: first, steady-state visual evoked potential (SSVEP) BCI with on-screen stimulation; second, hybrid BCI, which combined eye gaze and SSVEP for navigation and selection. A study comprises participants without known brain injury (non-BI, N = 30) and participants with known brain injury (BI, N = 14). A total of 29 out of 30 non-BI participants can successfully control the hybrid BCI, while nine out of the 14 BI participants are able to achieve control, as evidenced by task completion. The hybrid BCI provides a mean accuracy of 99.84% in the cohort of non-BI participants and 99.14% in the cohort of BI participants. Information transfer rates are 24.41 bpm in non-BI participants and 15.87 bpm in BI participants. The research goal is to quantify usage of SSVEP and ET approaches in cohorts of non-BI and BI participants. The hybrid is the preferred interaction modality for most participants for both cohorts. When compared to non-BI participants, it is encouraging that nine out of 14 participants with known BI can use the hBCI technology with equivalent accuracy and efficiency, albeit with slower transfer rates.},
  archive      = {J_THMS},
  author       = {Chris Brennan and Paul McCullagh and Gaye Lightbody and Leo Galway and Sally McClean and Piotr Stawicki and Felix Gembler and Ivan Volosyak and Elaine Armstrong and Eileen Thompson},
  doi          = {10.1109/THMS.2020.2983661},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {277-286},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Performance of a steady-state visual evoked potential and eye gaze hybrid brain-computer interface on participants with and without a brain injury},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>50</em>(3), C3. (<a
href="https://doi.org/10.1109/THMS.2020.2993394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.2993394},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Demand-driven transparency for monitoring intelligent
agents. <em>THMS</em>, <em>50</em>(3), 264–275. (<a
href="https://doi.org/10.1109/THMS.2020.2988859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous multiagent or multirobotic systems, the ability to quickly and accurately respond to threats and uncertainties is important for both mission outcomes and survivability. Such systems are never truly autonomous, often operating as part of a human-agent team. Artificial intelligent agents (IAs) have been proposed as tools to help manage such teams; e.g., proposing potential courses of action to human operators. However, they are often underutilized due to a lack of trust. Designing transparent agents, who can convey at least some information regarding their internal reasoning processes, is considered an effective method of increasing trust. How people interact with such transparency information to gain situation awareness while avoiding information overload is currently an unexplored topic. In this article, we go part way to answering this question, by investigating two forms of transparency: sequential transparency, which requires people to step through the IA&#39;s explanation in a fixed order; and demand-driven transparency, which allows people to request information as needed. In an experiment using a multivehicle simulation, our results show that demand-driven interaction improves the operators&#39; trust in the system while maintaining, and at times improving, performance and usability.},
  archive      = {J_THMS},
  author       = {Mor Vered and Piers Howe and Tim Miller and Liz Sonenberg and Eduardo Velloso},
  doi          = {10.1109/THMS.2020.2988859},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {264-275},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Demand-driven transparency for monitoring intelligent agents},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agent transparency and reliability in human–robot
interaction: The influence on user confidence and perceived reliability.
<em>THMS</em>, <em>50</em>(3), 254–263. (<a
href="https://doi.org/10.1109/THMS.2019.2925717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent transparency is an important contributor to human performance, situation awareness (SA), and trust in human-agent teaming. However, agent transparency&#39;s effects on human performance when the agent is unreliable have yet to be examined. This paper examined how the transparency and reliability of an autonomous robotic squad member (ASM) affected a human observer&#39;s task performance, workload, SA, trust in the robot, and perceptions of the robot. In a 2 (ASM transparency) × 2 (ASM reliability) within-subject design experiment, participants monitored a simulated soldier squad that included an ASM as it traversed a simulated training environment, while concurrently monitoring the environment for targets. There was no difference in participants&#39; performance on the target detection task, workload, or SA due to either ASM transparency or reliability. ASM reliability influenced participant trust and perceptions of the robot. Results suggest that reliability may be a stronger influence on the human&#39;s perceptions of the robot than transparency. Robot errors had a profound and lasting effect on the participants&#39; perception of the robot&#39;s future reliability and resulted in reduced confidence in their assessments of the robot&#39;s reliability. These findings could have important implications for the continued use of automated systems when the user is aware of system errors.},
  archive      = {J_THMS},
  author       = {Julia L. Wright and Jessie Y. C. Chen and Shan G. Lakhmani},
  doi          = {10.1109/THMS.2019.2925717},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {254-263},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Agent transparency and reliability in Human–Robot interaction: The influence on user confidence and perceived reliability},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The IMPACT of agent transparency on human performance.
<em>THMS</em>, <em>50</em>(3), 245–253. (<a
href="https://doi.org/10.1109/THMS.2020.2978041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary purpose of this article is to determine the impact of a simulated agent&#39;s transparency on human performance and related variables, such as response time, workload, and trust calibration. The agent supports participants as they complete a base defense task by managing a team of heterogeneous unmanned vehicles and serves as a decision aid to the human. Three conditions of transparency are explored. In condition 1, the agent displays only the basic information (map of vehicle location and proposed routes). In condition 2, the agent displays the basic information and an explanation of its reasoning. In condition 3, the agent displays the basic information, reasoning, and uncertainties involved in the plans. Results show that participants exhibit better performance and trust calibration in the high-transparency conditions without perceiving a significant increase in workload. However, response time also increased, likely due to the additional processing time needed for conditions with more information. Overall, our findings indicate that the increased agent transparency can improve human-agent decision making and performance, but with a small cost of the efficiency (timeliness) of task completion.},
  archive      = {J_THMS},
  author       = {Kimberly Stowers and Nicholas Kasdaglis and Michael A. Rupp and Olivia B. Newton and Jessie Y. C. Chen and Michael J. Barnes},
  doi          = {10.1109/THMS.2020.2978041},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {245-253},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The IMPACT of agent transparency on human performance},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Individual differences in trust in autonomous robots:
Implications for transparency. <em>THMS</em>, <em>50</em>(3), 234–244.
(<a href="https://doi.org/10.1109/THMS.2019.2947592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of increasingly intelligent and autonomous systems raises novel human factors challenges for human-machine teaming. People utilize differing mental models in understanding the functioning of complex systems that may be capable of social agency. Operators may perceive the machine as either a complex tool or a humanlike teammate. When the “advanced tool” mental model is adopted, operator trust may reflect individual differences in expectations of automation. By contrast, when the “teammate” mental model is activated, trust may depend on evaluative attitudes to robots. This article investigates predictors of trust in an autonomous robot detecting threat on either a physics-based or psychological basis. Distinct dimensions of physics-based and psychological trust are identified, corresponding to advanced tool and team mental models, respectively. Dispositional perceptions of automation, measured with the perfect automation schema scale, are associated with both aspects of trust. By contrast, the negative attitudes toward robots scale is specifically associated with lower psychological trust. The findings suggest that transparency information should be designed for compatibility with the operator&#39;s mental model in order to support accurate trust calibration and situation awareness. Transparency may be personalized to emphasize either the machine&#39;s data-analytic capabilities (advanced tool) or its humanlike social functioning (teammate).},
  archive      = {J_THMS},
  author       = {Gerald Matthews and Jinchao Lin and April Rose Panganiban and Michael D. Long},
  doi          = {10.1109/THMS.2019.2947592},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {234-244},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Individual differences in trust in autonomous robots: Implications for transparency},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transparency for a workload-adaptive cognitive agent in a
manned–unmanned teaming application. <em>THMS</em>, <em>50</em>(3),
225–233. (<a href="https://doi.org/10.1109/THMS.2019.2914667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on the transparent design of a cognitive agent to enhance situation awareness in two aspects of a human-agent teaming application: assisted system management and mixed-initiative mission planning. Adaptive and complex agent behavior might result in the failure to comprehend resulting interventions, a decrease in trust, and a loss of overall situation awareness. This study describes and validates a concept for transparent agent design by adopting the transparency strategies proposed by the “situation awareness-based agent transparency model.” The overall objective was to improve the human operator&#39;s perception, comprehension, and projection of the agent&#39;s support. The concept was applied to the prototype of a workload-adaptive cognitive agent, which supports a helicopter crew during mission planning and execution in complex and dynamically changing multi-vehicle missions. A human-in-the-loop experiment revealed enhancements in situation awareness and performance. Subjective trust measures implied an increase in human-like characteristics of the cognitive agent. The results and the potential for further research are discussed.},
  archive      = {J_THMS},
  author       = {Gunar Roth and Axel Schulte and Fabian Schmitt and Yannick Brand},
  doi          = {10.1109/THMS.2019.2914667},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {225-233},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Transparency for a workload-adaptive cognitive agent in a Manned–Unmanned teaming application},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agent transparency: A review of current theory and evidence.
<em>THMS</em>, <em>50</em>(3), 215–224. (<a
href="https://doi.org/10.1109/THMS.2020.2965529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As machines and agents become more autonomous, it has been increasingly clear to human factors/ergonomics researchers and practitioners that agent transparency is a critical issue for effective human-agent teaming. Transparency methods can provide the foundation for establishing shared awareness and shared intent between humans and intelligent machines. However, to date, the existing body of research on agent transparency has not been systematically documented. The purpose of this article is to summarize and evaluate current psychological theories and empirical evidence regarding effective agent transparency in human-autonomy teaming. We start by examining how transparency has been operationalized in the literature by discussing the two prominent theoretical frameworks of human-autonomy teaming. We then present a review of the empirical findings concerning how transparency affects key human-autonomy teaming variables, such as operator accuracy, decision time, situation awareness, perceived usability, and workload. This article includes an overview of the experimental tasks, scenarios, and interfaces that have been used in past studies and synthesizes how transparency has been operationalized and manipulated by prior studies. We then summarize the results and conclude by providing key recommendations for future research.},
  archive      = {J_THMS},
  author       = {Adella Bhaskara and Michael Skinner and Shayne Loft},
  doi          = {10.1109/THMS.2020.2965529},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {215-224},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Agent transparency: A review of current theory and evidence},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Influence of culture, transparency, trust, and degree of
automation on automation use. <em>THMS</em>, <em>50</em>(3), 205–214.
(<a href="https://doi.org/10.1109/THMS.2019.2931755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reported study compares groups of 120 participants each, from the United States (U.S.), Taiwan (TW), and Turkey (TK), interacting with versions of an automated path planner that vary in transparency and degree of automation. The nationalities were selected in accordance with the theory of cultural syndromes as representatives of Dignity (U.S.), Face (TW), and Honor (TK) cultures, and were predicted to differ in readiness to trust automation, degree of transparency required to use automation, and willingness to use systems with high degrees of automation. Three experimental conditions were tested. In the first, highlight, path conflicts were highlighted leaving rerouting to the participant. In the second, replanner made requests for permission to reroute when a path conflict was detected. The third combined condition increased transparency of the replanner by combining highlighting with rerouting to make the conflict on which decision was based visible to the user. A novel framework relating transparency, stages of automation, and trust in automation is proposed in which transparency plays a primary role in decisions to use automation but is supplemented by trust where there is insufficient information otherwise. Hypothesized cultural effects and framework predictions were confirmed.},
  archive      = {J_THMS},
  author       = {Shih-Yi Chien and Michael Lewis and Katia Sycara and Asiye Kumru and Jyi-Shane Liu},
  doi          = {10.1109/THMS.2019.2931755},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {205-214},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Influence of culture, transparency, trust, and degree of automation on automation use},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Models of trust in human control of swarms with varied
levels of autonomy. <em>THMS</em>, <em>50</em>(3), 194–204. (<a
href="https://doi.org/10.1109/THMS.2019.2896845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study human trust and its computational models in supervisory control of swarm robots with varied levels of autonomy (LOA) in a target foraging task. We implement three LOAs: manual, mixed-initiative (MI), and fully autonomous LOA. While the swarm in the MI LOA is controlled by a human operator and an autonomous search algorithm collaboratively, the swarms in the manual and autonomous LOAs are fully directed by the human and the search algorithm, respectively. From user studies, we find that humans tend to make their decisions based on physical characteristics of the swarm rather than its performance since the task performance of swarms is not clearly perceivable by humans. Based on the analysis, we formulate trust as a Markov decision process whose state space includes the factors affecting trust. We develop variations of the trust model for different LOAs. We employ an inverse reinforcement learning algorithm to learn behaviors of the operator from demonstrations where the learned behaviors are used to predict human trust. Compared to an existing model, our models reduce the prediction error by at most 39.6%, 36.5%, and 28.8% in the manual, MI, and auto-LOA, respectively.},
  archive      = {J_THMS},
  author       = {Changjoo Nam and Phillip Walker and Huao Li and Michael Lewis and Katia Sycara},
  doi          = {10.1109/THMS.2019.2896845},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {194-204},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Models of trust in human control of swarms with varied levels of autonomy},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial: Agent and system transparency.
<em>THMS</em>, <em>50</em>(3), 189–193. (<a
href="https://doi.org/10.1109/THMS.2020.2988835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The eight papers in this special issue focus on agent and system transparency in human-machine systems. The concept of transparency is investigated in a variety of contexts of human agent interaction—from a single robot to multiple heterogeneous agents and swarms. Two studies examine the effects of individual and cultural differences and, based on the compelling results, provide design recommendations related to transparent interfaces. One of the papers provides a thorough review on theoretical aspects of agent transparency and empirical findings on operator performance, situation awareness, trust and workload, among other outcomes. These measures are also among some of the most common metrics reported in studies in this special issue.},
  archive      = {J_THMS},
  author       = {Jessie Y. C. Chen and Frank Ole Flemisch and Joseph B. Lyons and Mark A. Neerincx},
  doi          = {10.1109/THMS.2020.2988835},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {189-193},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Guest editorial: Agent and system transparency},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>50</em>(2), C3. (<a
href="https://doi.org/10.1109/THMS.2020.2976020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.2976020},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). IEEE foundation realize the full potential of IEEE.
<em>THMS</em>, <em>50</em>(2), 187. (<a
href="https://doi.org/10.1109/THMS.2020.2976027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement: The world&#39;s most daunting challenges require innovations in engineering, and IEEE is committed to finding the solutions. The IEEE Foundation is leading a special campaign to raise awareness, create partnerships, and generate financial resources needed to combat these global challenges.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.2976027},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {187},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE foundation realize the full potential of IEEE},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How communicating features can help pedestrian safety in the
presence of self-driving vehicles: Virtual reality experiment.
<em>THMS</em>, <em>50</em>(2), 176–186. (<a
href="https://doi.org/10.1109/THMS.2019.2960517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) has proven to be a useful tool for conducting human factors research in interface design. With the development and promotion of autonomous vehicle (AV) technology, researchers are focusing on designing different types of interfaces to induce trust in road users toward this new technology. In this article, VR is used to investigate pedestrians&#39; understanding of proposed designs for external features on AVs. We are also interested in investigating how the presence of an operator inside the vehicle influences pedestrians&#39; preference for features. VR headset tracking, survey-based responses, and video-recorded body movements are used to collect data on pedestrian responses to three operator statuses and seven feature types. Pedestrians prefer both “walk” in text and verbal message saying safe to cross as clear and comforting features on an AV. They perceive the distracted operator condition as to be an inconvenient situation even for the AV equipped with visual or audible features. Older people find the features more helpful and people with higher innovativeness rate the feature ideas with higher ratings.},
  archive      = {J_THMS},
  author       = {Shuchisnigdha Deb and Daniel W. Carruth and Christopher R. Hudson},
  doi          = {10.1109/THMS.2019.2960517},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {176-186},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {How communicating features can help pedestrian safety in the presence of self-driving vehicles: Virtual reality experiment},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A unifying theory of driver perception and steering control
on straight and winding roads. <em>THMS</em>, <em>50</em>(2), 165–175.
(<a href="https://doi.org/10.1109/THMS.2019.2947551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel driver support systems potentially enhance road safety by cooperating with the human driver. To optimize the design of emerging steering support systems, a profound understanding of driver steering behavior is required. This article proposes a new theory of driver steering, which unifies visual perception and control models. The theory is derived directly from measured steering data, without any a priori assumptions on driver inputs or control dynamics. Results of a human-in-the-loop simulator experiment are presented, in which drivers tracked the centerline of straight and winding roads. Multiloop frequency response function (FRF) estimates reveal how drivers use visual preview, lateral position feedback, and heading feedback for control. Classical control theory is used to model all three FRF estimates. The model has physically interpretable parameters, which indicate that drivers minimize the bearing angle to an “aim point” (located 0.25-0.75 s ahead) through simple compensatory control, both on straight and winding roads. The resulting unifying perception and control theory provides a new tool for rationalizing driver steering behavior, and for optimizing modern steering support systems.},
  archive      = {J_THMS},
  author       = {Kasper van der El and Daan M. Pool and Marinus René M. van Paassen and Max Mulder},
  doi          = {10.1109/THMS.2019.2947551},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {165-175},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A unifying theory of driver perception and steering control on straight and winding roads},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Determination of the gain for a walking speed amplifying
belt using brain activity. <em>THMS</em>, <em>50</em>(2), 154–164. (<a
href="https://doi.org/10.1109/THMS.2019.2961974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Movement/walking assistance devices have the great advantage of supporting quality of life for the elderly in an aging society. To strike a balance between efficiency of movement and employment of the elderly user&#39;s own body, we developed a smart mobility system called Tread-walk, which is controlled by the user walking on a treadmill and amplifies the user&#39;s walking speed. Since the user&#39;s walking speed is different from the speed at which the Tread-walk moves, users experience a mismatch between their visual optical flow and somatic sense. In this article, we validate the feasibility of an amplifying gain decision method that analyzes user brain activity. To control Tread-walk, the visual sense is integrated with somatosensation in the parietal area of the brain and controlled in the medial prefrontal cortex. Therefore, first, we measure the parietal area when the participants walk while looking at their virtual optical flow. Second, we measure the medical prefrontal cortex when the participants control Tread-walk 2. These experiments are carried out for a variety of speed amplifying gains. We find that the brain activates significantly at amplification gain K = 1.1–1.7 in the virtual optical flow experiment and K = 1.5–2.0 in the Tread-walk experiment; this brain activation represents the amplification gain at which the visual and somatosensory senses seem to receive similar input. In conclusion, the brain would activate the most significantly at the most appropriate amplification gain.},
  archive      = {J_THMS},
  author       = {Satoshi Miura and Yuki Yokoo and Yasutaka Nakashima and Yoshikazu Ogaya and Misato Nihei and Takeshi Ando and Yo Kobayashi and Masakatsu G. Fujie},
  doi          = {10.1109/THMS.2019.2961974},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {154-164},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Determination of the gain for a walking speed amplifying belt using brain activity},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active compliance control reduces upper body effort in
exoskeleton-supported walking. <em>THMS</em>, <em>50</em>(2), 144–153.
(<a href="https://doi.org/10.1109/THMS.2019.2961969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a locomotion controller for lower limb exoskeletons so as to enable the combined robot and user system to exhibit compliant walking characteristics when interacting with the environment. This is of critical importance to reduce the excessive ground reaction forces during the walking task execution with the aim of improved environmental interaction capabilities. In robot-aided walking support for paraplegics, the user has to actively use his/her upper limbs via crutches to ensure overall balance. By virtue of this requisite, several issues may particularly arise during touchdown instants, e.g., upper body orientation fluctuates, shoulder joints are subject to excessive loading, and arms may need to exert extra forces to counterbalance these effects. In order to reduce the upper body effort via compliant locomotion, the controller is designed to manage the force/position tradeoff by using an admittance controller in each joint. For proof of concept, a series of exoskeleton-aided walking experiments were conducted with the participation of nine healthy volunteers, four of whom additionally walked on an irregular surface for further performance evaluation. The results suggest that the proposed locomotion controller is advantageous over conventional high-gain position tracking in decreasing undesired oscillatory torso motion and total arm force, adequately reducing the required upper body effort.},
  archive      = {J_THMS},
  author       = {Barkan Ugurlu and Hironori Oshima and Emre Sariyildiz and Tatsuo Narikiyo and Jan Babic},
  doi          = {10.1109/THMS.2019.2961969},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {144-153},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Active compliance control reduces upper body effort in exoskeleton-supported walking},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Review of human–machine interfaces for small unmanned
systems with robotic manipulators. <em>THMS</em>, <em>50</em>(2),
131–143. (<a href="https://doi.org/10.1109/THMS.2020.2969380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reviews the human-machine interaction (HMI) technologies used for telemanipulation by small unmanned systems (SUS) with remote manipulators. SUS, including land, air, and sea vehicles, can perform a wide range of reconnaissance and manipulation tasks with varying levels of autonomy. SUS operations involving physical interactions with the environment require some level of operator involvement, ranging from direct control to goal-oriented supervision. Telemanipulation remains a challenging task for all levels of human interaction because the operator and the vehicle are not colocated, and operators require HMI technologies that facilitate manipulation from a remote location. This article surveys the human operator interfacing for over 70 teleoperated systems, summarizes the effects of physical and visual interface factors on user performance, and discusses these findings in the context of telemanipulating SUS. This article is of importance to SUS researchers and practitioners who will directly benefit from HMI implementations that improve telemanipulation performance.},
  archive      = {J_THMS},
  author       = {Sierra N. Young and Joshua M. Peschel},
  doi          = {10.1109/THMS.2020.2969380},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {131-143},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Review of Human–Machine interfaces for small unmanned systems with robotic manipulators},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). State of the art in bionic hands. <em>THMS</em>,
<em>50</em>(2), 116–130. (<a
href="https://doi.org/10.1109/THMS.2020.2970740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prosthetic hands have made a significant influence on the quality of life of people with upper arm amputation. Research on prosthetic hands today is focused on replicating the functionalities of the biological hands. The present article provides a bibliometric survey on bionic hands, done through a compilation of a scientific publications database on the field of prosthetic hands spanning the last two decades. Through network-based information analysis, meaningful patterns are inferred, and several key questions significant to bionic prosthetic hands are answered. The article gives an insight into the growth, progress, and future trend in bionic hand prostheses, including identifying subject areas and discovering distinct research communities within the subject. The analysis clearly reveals the rapid expansion of bionic hand research over the last two decades, which is expected to rise considerably in near future.},
  archive      = {J_THMS},
  author       = {Hirakjyoti Basumatary and Shyamanta M. Hazarika},
  doi          = {10.1109/THMS.2020.2970740},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {116-130},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {State of the art in bionic hands},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Brain–computer interface software: A review and discussion.
<em>THMS</em>, <em>50</em>(2), 101–115. (<a
href="https://doi.org/10.1109/THMS.2020.2968411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software is a critical component of brain-computer interfaces (BCIs). While BCI hardware enables the retrieval of brain signals, BCI software is required to analyze these signals, produce output, and provide feedback. Users from multiple research areas have adopted BCI software platforms to investigate various concepts. Recently, interest in web-based BCI software has also emerged. The system design and control signal techniques of state-of-the-art BCI software platforms have been previously investigated. However, there is limited literature discussing user adoption of BCI software platforms. Additionally, there is a lack of work discussing the recent emergence of web tools relevant to BCI applications. This article aims to address these gaps by presenting a bibliometric review of the state-of-the-art BCI software. Furthermore, we discuss web-based BCIs and present tools that may be used to develop future web-based BCI applications.},
  archive      = {J_THMS},
  author       = {Pierce Stegman and Chris S. Crawford and Marvin Andujar and Anton Nijholt and Juan E. Gilbert},
  doi          = {10.1109/THMS.2020.2968411},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {101-115},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Brain–Computer interface software: A review and discussion},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>50</em>(1), C3. (<a
href="https://doi.org/10.1109/THMS.2020.2966133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.2966133},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). IEEE foundation realize the full potential of IEEE.
<em>THMS</em>, <em>50</em>(1), 98. (<a
href="https://doi.org/10.1109/THMS.2020.2966138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement: The world&#39;s most daunting challenges require innovations in engineering, and IEEE is committed to finding the solutions. The IEEE Foundation is leading a special campaign to raise awareness, create partnerships, and generate financial resources needed to combat these global challenges.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.2966138},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {98},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE foundation realize the full potential of IEEE},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficacy of incentive structures for boundedly rational
dispatchers in large-scale queueing networks. <em>THMS</em>,
<em>50</em>(1), 89–97. (<a
href="https://doi.org/10.1109/THMS.2019.2906618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper employs computational approaches to model and explore the efficacy of different incentive structures on the decision making behavior of dispatchers in complex queueing networks, and the subsequent effects of these decisions on teams working within the network and on network performance itself. Computational models that express network structure and function, as well as the decision making process of dispatchers operating within the network and the effect these decisions have on team performance, are presented. Performance of the network under status quo and other incentive structures and decision making processes is illustrated via simulation, validated against data from a large-scale debris removal mission that followed a series of tornadoes in the U.S. state of Alabama in 2011. Results of the simulation experiments suggest that the optimal incentive structure assuming a rational decision maker remains optimal under lower levels of rationality. Furthermore, a simple uniform reward structure is likely to produce performance improvements over the status quo incentive structure under most scenarios.},
  archive      = {J_THMS},
  author       = {James D. Brooks and David Mendonça and Xin Zhang},
  doi          = {10.1109/THMS.2019.2906618},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {89-97},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Efficacy of incentive structures for boundedly rational dispatchers in large-scale queueing networks},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supporting multitracking performance with novel visual,
auditory, and tactile displays. <em>THMS</em>, <em>50</em>(1), 79–88.
(<a href="https://doi.org/10.1109/THMS.2019.2947580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates performance in multiple concurrent tracking tasks with multisensory displays in a driving context. In many work domains, such as driving, aviation, process control, and medicine, humans perform “tracking” tasks that involve observing continuous variables and providing a control input to achieve and maintain satisfactory levels in those variables. Performance in multiple concurrent tracking tasks (“multitracking”) was studied in driving-like scenarios that challenged participants to track established targets for lateral (lane position) and longitudinal (speed) variables. Novel speed displays were developed to engage nontraditional sensory modalities (e.g., ambient-visual, auditory, or tactile) with relative speed conveyed through simple or multidimensional signal encoding methods. Participants&#39; speed-tracking and lane-tracking performances were measured concurrently and compared across display configurations within-subjects. Results showed lane tracking performance to be unaffected by display configuration; however, speed tracking and overall performance were significantly improved with novel displays, compared to the baseline configuration. Redundantly encoded auditory displays best supported multitracking performance, but redundantly encoded tactile displays were not as beneficial as were simple encodings. These results provide insight into the human information processing of semicontinuous multisensory displays and can inform display design in driving and other visually demanding work contexts.},
  archive      = {J_THMS},
  author       = {Shiyan Yang and Thomas K. Ferris},
  doi          = {10.1109/THMS.2019.2947580},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {79-88},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Supporting multitracking performance with novel visual, auditory, and tactile displays},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Effects of target trajectory bandwidth on manual control
behavior in pursuit and preview tracking. <em>THMS</em>, <em>50</em>(1),
68–78. (<a href="https://doi.org/10.1109/THMS.2019.2947577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 1960s crossover model is widely applied to quantitatively predict a human controller&#39;s (HC&#39;s) manual control behavior. Unfortunately, the theory captures only compensatory tracking behavior and, as such, a limited range of real-world manual control tasks. This article finalizes recent advances in manual control theory toward more general pursuit and preview tracking tasks. It is quantified how HCs adapt their control behavior to a final crucial task variable: the target trajectory bandwidth. Beneficial adaptation strategies are first explored offline with computer simulations, using an extended crossover model theory for pursuit and preview tracking. The predictions are then verified with data from a human-in-the-loop experiment, in which participants tracked a target trajectory with bandwidths of 1.5, 2.5, and 4 rad/s, using compensatory, as well as pursuit and preview displays. In stark contrast to the crossover regression found in compensatory tasks, humans attenuate only their feedforward response when tracking higher-bandwidth trajectories in pursuit tasks, while their behavior is generally invariant in preview tasks. A full quantitative theory is now available to predict HC manual control behavior in tracking tasks, which includes HC adaptation to all key task variables.},
  archive      = {J_THMS},
  author       = {Kasper van der El and Daan M. Pool and Marinus René M. van Paassen and Max Mulder},
  doi          = {10.1109/THMS.2019.2947577},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {68-78},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Effects of target trajectory bandwidth on manual control behavior in pursuit and preview tracking},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new mixed-reality-based teleoperation system for
telepresence and maneuverability enhancement. <em>THMS</em>,
<em>50</em>(1), 55–67. (<a
href="https://doi.org/10.1109/THMS.2019.2960676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) is regarded as a useful tool for teleoperation systems and provides operators with immersive visual feedback on the robot and the environment. However, without any haptic feedback or physical constructions, VR-based teleoperation systems normally suffer from poor maneuverability, and operational faults may be caused in some fine movements. In this article, we employ mixed reality (MR), which combines real and virtual worlds, to develop a novel teleoperation system. A new system design and control algorithms are proposed. For the system design, an MR interface is developed based on a virtual environment augmented with real-time data from the task space with the goal of enhancing the operator&#39;s visual perception. To allow the operator to be freely decoupled from the control loop and offload the operator&#39;s burden, a new interaction proxy is proposed to control the robot. For the control algorithms, two control modes are introduced to improve the long-distance movements and fine movements of the MR-based teleoperation system. In addition, a set of fuzzy-logic-based methods are proposed to regulate the orientation, position, velocity, and force of the robot to enhance the system&#39;s maneuverability and address potential operational faults. A barrier Lyapunov function and a backstepping method are leveraged to design the control laws and simultaneously guarantee the system&#39;s stability under state constraints. Experiments conducted using a six-degree-of-freedom robotic arm prove the feasibility of the system.},
  archive      = {J_THMS},
  author       = {Da Sun and Andrey Kiselev and Qianfang Liao and Todor Stoyanov and Amy Loutfi},
  doi          = {10.1109/THMS.2019.2960676},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {55-67},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A new mixed-reality-based teleoperation system for telepresence and maneuverability enhancement},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Natural human–robot interface using adaptive tracking system
with the unscented kalman filter. <em>THMS</em>, <em>50</em>(1), 42–54.
(<a href="https://doi.org/10.1109/THMS.2019.2947576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional human-robot interfaces usually have limitations in accuracy and/or operational space. This article proposes a natural human-robot interface using an adaptive tracking method, which can effectively expand the operational space while ensuring high accuracy. The natural interface allows the robot to directly reproduce the user&#39;s hand movement, making the interaction more intuitive and natural. The leap motion is fixed on the Cartesian platform to capture the movement of the user&#39;s hand. Because the Cartesian platform follows the hand and keeps the hand in the center of the detection area, the measurement accuracy is improved and the measurement space can be extended. During the process of acquiring gesture data, the measurement errors were found to increase over time because of the inherent noise of the sensor. To deal with this problem, the unscented Kalman filter is applied to estimate the position of the hand. Moreover, an adaptive velocity control method is proposed to improve the operation accuracy and reduce the task execution time with the consideration of users&#39; habits and easiness of usage. The effectiveness of this interface is verified by a series of experiments, and the results show that the proposed interface can be used by nonprofessional users for object operation tasks and can provide users with superior interactive experiences.},
  archive      = {J_THMS},
  author       = {Guanglong Du and Gengcheng Yao and Chunquan Li and Peter X. Liu},
  doi          = {10.1109/THMS.2019.2947576},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {42-54},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Natural Human–Robot interface using adaptive tracking system with the unscented kalman filter},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel postprocessing method for robust myoelectric
pattern-recognition control through movement pattern transition
detection. <em>THMS</em>, <em>50</em>(1), 32–41. (<a
href="https://doi.org/10.1109/THMS.2019.2953262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern-recognition-based myoelectric control systems are not yet widely available due to their limited robustness in real-life situations. Some postprocessing methods were introduced to improve the robustness in previous studies, but there is lack of investigation into movement transition phases. This article presents a novel postprocessing method based on movement pattern transition (MPT) detection. An image-based index is used to quantify the similarity of adjacent feature matrices from high-density surface electromyogram (EMG) signals. MPT detection is implemented by applying a double threshold to the calculated index. The proposed postprocessing method is used to rectify the EMG pattern recognition decisions from the classifier by incorporating the detected information. Two representative testing schemes are used to verify the robustness of the proposed method against force level variation and consecutive nonstop task performance. The proposed method achieved mean classification accuracy improvements of 7.33% and 10.91% with respect to the baseline performance of a raw classifier (without any postprocessing) in the two testing schemes. It also outperformed other common postprocessing methods ( p &lt; 0.05). Considering both the accuracy improvement and time efficiency for rapid responses to MPT, the proposed method could be a potential option for postprocessing to enhance the robustness of myoelectric control.},
  archive      = {J_THMS},
  author       = {Bin Yu and Xu Zhang and Le Wu and Xiang Chen and Xun Chen},
  doi          = {10.1109/THMS.2019.2953262},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {32-41},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A novel postprocessing method for robust myoelectric pattern-recognition control through movement pattern transition detection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grasp prediction toward naturalistic exoskeleton glove
control. <em>THMS</em>, <em>50</em>(1), 22–31. (<a
href="https://doi.org/10.1109/THMS.2019.2938139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents accurate grasp prediction algorithms that can be used for naturalistic, synergistic control of exoskeleton gloves with minimal user input. Recent research in exoskeleton systems has focused mainly on the development of novel soft or hard mechanical designs and actuation systems for rehabilitative and assistive applications. On the other hand, estimating user intent for intelligent grasp assistance is a problem that has remained largely unaddressed. As demonstrated by existing studies, the complex motions of human hand can be mapped to a latent space, thereby reducing perceived noise in individual joint angles as well as the number of variables upon which the prediction must be performed. To this extent, we present two latent space grasp prediction algorithms for intelligent exoskeleton glove control. The first presented algorithm is based on a linear regression to determine the slope and prediction horizon. The second algorithm is based on a Gaussian process trajectory matching where the trajectory of the grasping motion is probabilistically compared to existing data in order to form a prediction. Both algorithms were tested on published motion data collected from healthy subjects. In addition, the experimental validation of the algorithms was done using the RML glove (Robotics and Mechatronics Lab), which yielded similar prediction accuracy as compared to the simulation results. The proposed prediction algorithm can act as the backbone for a shifting authority controller that simultaneously amplifies the user&#39;s motion while guiding them toward their desired grasp. Preliminary work in this direction is also described in the paper, with directions for future research.},
  archive      = {J_THMS},
  author       = {Raghuraj Chauhan and Bijo Sebastian and Pinhas Ben-Tzvi},
  doi          = {10.1109/THMS.2019.2938139},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {22-31},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Grasp prediction toward naturalistic exoskeleton glove control},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Annotation generation from IMU-based human whole-body
motions in daily life behavior. <em>THMS</em>, <em>50</em>(1), 13–21.
(<a href="https://doi.org/10.1109/THMS.2019.2960630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes a stochastic framework for integrating human whole-body motions with natural language. Human whole-body motions in daily life are measured by inertial measurement units (IMU) and subsequently encoded into motion primitives. Sentences are manually attached to the human motion primitives for their descriptions. Two aspects of semantics and syntactics are represented by stochastic modules. One stochastic module trains the linking of motion primitives to words, and the other module represents word order in the sentence structure. These two modules are helpful toward converting human whole-body motions into descriptions, where multiple words are generated from the human motions by the first module, and the second module searches for syntactically consistent sentences consisting of the generated words. The proposed framework is tested on a large dataset of human whole-body motions and their descriptive sentences. The linking of human motions to natural language enables robots to understand observations of human behavior as sentences.},
  archive      = {J_THMS},
  author       = {Wataru Takano},
  doi          = {10.1109/THMS.2019.2960630},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {13-21},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Annotation generation from IMU-based human whole-body motions in daily life behavior},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). User recruitment system for efficient photo collection in
mobile crowdsensing. <em>THMS</em>, <em>50</em>(1), 1–12. (<a
href="https://doi.org/10.1109/THMS.2019.2912509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsensing recruits a group of mobile users to cooperatively perform a common sensing job with their smart devices. As a special issue, photo crowdsensing allows users to utilize the built-in cameras of mobile devices to take photos for an event or a target. Then, the photos can be used in numerous application areas, such as target reconstruction, scenario reduction, and so on. Therefore, photo crowdsensing has attracted considerable attention recently due to the rich information that can be provided by images. In this paper, we focus on using the photos to make reconstructions for specific targets. Furthermore, we develop a user recruitment system for efficient photo collecting in mobile crowdsensing (RSMC), where the task requesters publish a sensing task to the users, and the map is gridded according to the locations of the sensing targets. Then, we use a semi-Markov model to calculate the user&#39;s utility for the sensing task. Finally, a user recruitment strategy is devised to recruit the optimal k users for finishing the sensing task. We conduct extensive simulations based on three widely used real-world traces: roma/taxi, epfl, and geolife. The results show that, compared with other recruitment strategies, RSMC takes the largest number of efficient photos for the sensing task.},
  archive      = {J_THMS},
  author       = {En Wang and Yongjian Yang and Jie Wu and Kaihao Lou and Dongming Luan and Hengzhi Wang},
  doi          = {10.1109/THMS.2019.2912509},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {User recruitment system for efficient photo collection in mobile crowdsensing},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
