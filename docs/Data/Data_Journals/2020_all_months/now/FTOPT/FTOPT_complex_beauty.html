<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FTOPT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ftopt---2">FTOPT - 2</h2>
<ul>
<li><details>
<summary>
(2020). Distributionally robust learning. <em>FTOPT</em>,
<em>4</em>(1-2), 1–243. (<a
href="https://doi.org/10.1561/2400000026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This monograph develops a comprehensive statistical learning framework that is robust to (distributional) perturbations in the data using under the Wasserstein metric. Beginning with fundamental properties of the Wasserstein metric and the DRO formulation, we explore duality to arrive at tractable formulations and develop finite-sample, as well as asymptotic, performance guarantees. We consider a series of learning problems, including (i) distributionally robust linear regression; (ii) distributionally robust regression with group structure in the predictors; (iii) distributionally robust multi-output regression and multiclass classification, (iv) optimal decision making that combines distributionally robust regression with nearest-neighbor estimation; (v) distributionally robust semi-supervised learning, and (vi) distributionally robust reinforcement learning. A tractable DRO relaxation for each problem is being derived, establishing a connection between robustness and regularization, and obtaining bounds on the prediction and estimation errors of the solution. Beyond theory, we include numerical experiments and case studies using synthetic and real data. The real data experiments are all associated with various health informatics problems, an application area which provided the initial impetus for this work.},
  archive      = {J_FTOPT},
  author       = {Ruidi Chen and Ioannis Ch. Paschalidis},
  doi          = {10.1561/2400000026},
  journal      = {Foundations and Trends® in Optimization},
  number       = {1-2},
  pages        = {1-243},
  shortjournal = {Found. Trends Optim.},
  title        = {Distributionally robust learning},
  volume       = {4},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Atomic decomposition via polar alignment: The geometry of
structured optimization. <em>FTOPT</em>, <em>3</em>(4), 280–366. (<a
href="https://doi.org/10.1561/2400000028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured optimization uses a prescribed set of atoms to assemble a solution that fits a model to data. Polarity, which extends the familiar notion of orthogonality from linear sets to general convex sets, plays a special role in a simple and geometric form of convex duality. This duality correspondence yields a general notion of alignment that leads to an intuitive and complete description of how atoms participate in the final decomposition of the solution. The resulting geometric perspective leads to variations of existing algorithms effective for large-scale problems. We illustrate these ideas with many examples, including applications in matrix completion and morphological component analysis for the separation of mixtures of signals.},
  archive      = {J_FTOPT},
  author       = {Zhenan Fan and Halyun Jeong and Yifan Sun and Michael P. Friedlander},
  doi          = {10.1561/2400000028},
  journal      = {Foundations and Trends® in Optimization},
  number       = {4},
  pages        = {280-366},
  shortjournal = {Found. Trends Optim.},
  title        = {Atomic decomposition via polar alignment: The geometry of structured optimization},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
