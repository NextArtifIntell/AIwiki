<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac---99">SAC - 99</h2>
<ul>
<li><details>
<summary>
(2020). Convergence rates of gaussian ODE filters. <em>SAC</em>,
<em>30</em>(6), 1791–1816. (<a
href="https://doi.org/10.1007/s11222-020-09972-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recently introduced class of probabilistic (uncertainty-aware) solvers for ordinary differential equations (ODEs) applies Gaussian (Kalman) filtering to initial value problems. These methods model the true solution x and its first q derivatives a priori as a Gauss–Markov process $${\varvec{X}}$$ , which is then iteratively conditioned on information about $${\dot{x}}$$ . This article establishes worst-case local convergence rates of order $$q+1$$ for a wide range of versions of this Gaussian ODE filter, as well as global convergence rates of order q in the case of $$q=1$$ and an integrated Brownian motion prior, and analyses how inaccurate information on $${\dot{x}}$$ coming from approximate evaluations of f affects these rates. Moreover, we show that, in the globally convergent case, the posterior credible intervals are well calibrated in the sense that they globally contract at the same rate as the truncation error. We illustrate these theoretical results by numerical experiments which might indicate their generalizability to $$q \in {2,3,\ldots }$$ .},
  archive      = {J_SAC},
  author       = {Kersting, Hans and Sullivan, T. J. and Hennig, Philipp},
  doi          = {10.1007/s11222-020-09972-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1791-1816},
  shortjournal = {Stat. Comput.},
  title        = {Convergence rates of gaussian ODE filters},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An apparent paradox: A classifier based on a partially
classified sample may have smaller expected error rate than that if the
sample were completely classified. <em>SAC</em>, <em>30</em>(6),
1779–1790. (<a
href="https://doi.org/10.1007/s11222-020-09971-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been increasing interest in using semi-supervised learning to form a classifier. As is well known, the (Fisher) information in an unclassified feature with unknown class label is less (considerably less for weakly separated classes) than that of a classified feature which has known class label. Hence in the case where the absence of class labels does not depend on the data, the expected error rate of a classifier formed from the classified and unclassified features in a partially classified sample is greater than that if the sample were completely classified. We propose to treat the labels of the unclassified features as missing data and to introduce a framework for their missingness as in the pioneering work of Rubin (Biometrika 63:581–592, 1976) for missingness in incomplete data analysis. An examination of several partially classified data sets in the literature suggests that the unclassified features are not occurring at random in the feature space, but rather tend to be concentrated in regions of relatively high entropy. It suggests that the missingness of the labels of the features can be modelled by representing the conditional probability of a missing label for a feature via the logistic model with covariate depending on the entropy of the feature or an appropriate proxy for it. We consider here the case of two normal classes with a common covariance matrix where for computational convenience the square of the discriminant function is used as the covariate in the logistic model in place of the negative log entropy. Rather paradoxically, we show that the classifier so formed from the partially classified sample may have smaller expected error rate than that if the sample were completely classified.},
  archive      = {J_SAC},
  author       = {Ahfock, Daniel and McLachlan, Geoffrey J.},
  doi          = {10.1007/s11222-020-09971-5},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1779-1790},
  shortjournal = {Stat. Comput.},
  title        = {An apparent paradox: A classifier based on a partially classified sample may have smaller expected error rate than that if the sample were completely classified},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-automated simultaneous predictor selection for
regression-SARIMA models. <em>SAC</em>, <em>30</em>(6), 1759–1778. (<a
href="https://doi.org/10.1007/s11222-020-09970-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deciding which predictors to use plays an integral role in deriving statistical models in a wide range of applications. Motivated by the challenges of predicting events across a telecommunications network, we propose a semi-automated, joint model-fitting and predictor selection procedure for linear regression models. Our approach can model and account for serial correlation in the regression residuals, produces sparse and interpretable models and can be used to jointly select models for a group of related responses. This is achieved through fitting linear models under constraints on the number of nonzero coefficients using a generalisation of a recently developed mixed integer quadratic optimisation approach. The resultant models from our approach achieve better predictive performance on the motivating telecommunications data than methods currently used by industry.},
  archive      = {J_SAC},
  author       = {Lowther, Aaron P. and Fearnhead, Paul and Nunes, Matthew A. and Jensen, Kjeld},
  doi          = {10.1007/s11222-020-09970-6},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1759-1778},
  shortjournal = {Stat. Comput.},
  title        = {Semi-automated simultaneous predictor selection for regression-SARIMA models},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subsampling sequential monte carlo for static bayesian
models. <em>SAC</em>, <em>30</em>(6), 1741–1758. (<a
href="https://doi.org/10.1007/s11222-020-09969-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show how to speed up sequential Monte Carlo (SMC) for Bayesian inference in large data problems by data subsampling. SMC sequentially updates a cloud of particles through a sequence of distributions, beginning with a distribution that is easy to sample from such as the prior and ending with the posterior distribution. Each update of the particle cloud consists of three steps: reweighting, resampling, and moving. In the move step, each particle is moved using a Markov kernel; this is typically the most computationally expensive part, particularly when the dataset is large. It is crucial to have an efficient move step to ensure particle diversity. Our article makes two important contributions. First, in order to speed up the SMC computation, we use an approximately unbiased and efficient annealed likelihood estimator based on data subsampling. The subsampling approach is more memory efficient than the corresponding full data SMC, which is an advantage for parallel computation. Second, we use a Metropolis within Gibbs kernel with two conditional updates. A Hamiltonian Monte Carlo update makes distant moves for the model parameters, and a block pseudo-marginal proposal is used for the particles corresponding to the auxiliary variables for the data subsampling. We demonstrate both the usefulness and limitations of the methodology for estimating four generalized linear models and a generalized additive model with large datasets.},
  archive      = {J_SAC},
  author       = {Gunawan, David and Dang, Khue-Dung and Quiroz, Matias and Kohn, Robert and Tran, Minh-Ngoc},
  doi          = {10.1007/s11222-020-09969-z},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1741-1758},
  shortjournal = {Stat. Comput.},
  title        = {Subsampling sequential monte carlo for static bayesian models},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Properties of the stochastic approximation EM algorithm with
mini-batch sampling. <em>SAC</em>, <em>30</em>(6), 1725–1739. (<a
href="https://doi.org/10.1007/s11222-020-09968-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To deal with very large datasets a mini-batch version of the Monte Carlo Markov Chain Stochastic Approximation Expectation–Maximization algorithm for general latent variable models is proposed. For exponential models the algorithm is shown to be convergent under classical conditions as the number of iterations increases. Numerical experiments illustrate the performance of the mini-batch algorithm in various models. In particular, we highlight that mini-batch sampling results in an important speed-up of the convergence of the sequence of estimators generated by the algorithm. Moreover, insights on the effect of the mini-batch size on the limit distribution are presented. Finally, we illustrate how to use mini-batch sampling in practice to improve results when a constraint on the computing time is given.},
  archive      = {J_SAC},
  author       = {Kuhn, Estelle and Matias, Catherine and Rebafka, Tabea},
  doi          = {10.1007/s11222-020-09968-0},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1725-1739},
  shortjournal = {Stat. Comput.},
  title        = {Properties of the stochastic approximation EM algorithm with mini-batch sampling},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On some consistent tests of mutual independence among
several random vectors of arbitrary dimensions. <em>SAC</em>,
<em>30</em>(6), 1707–1723. (<a
href="https://doi.org/10.1007/s11222-020-09967-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing for mutual independence among several random vectors is a challenging problem, and in recent years, it has gained significant attention in statistics and machine learning literature. Most of the existing tests of independence deal with only two random vectors, and they do not have straightforward generalizations for testing mutual independence among more than two random vectors of arbitrary dimensions. On the other hand, there are various tests for mutual independence among several random variables, but these univariate tests do not have natural multivariate extensions. In this article, we propose two general recipes, one based on inter-point distances and the other based on linear projections, for multivariate extensions of these univariate tests. Under appropriate regularity conditions, these resulting tests turn out to be consistent whenever we have consistency for the corresponding univariate tests. We carry out extensive numerical studies to compare the empirical performance of these proposed methods with the state-of-the-art methods.},
  archive      = {J_SAC},
  author       = {Roy, Angshuman and Sarkar, Soham and Ghosh, Anil K. and Goswami, Alok},
  doi          = {10.1007/s11222-020-09967-1},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1707-1723},
  shortjournal = {Stat. Comput.},
  title        = {On some consistent tests of mutual independence among several random vectors of arbitrary dimensions},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BayesProject: Fast computation of a projection direction for
multivariate changepoint detection. <em>SAC</em>, <em>30</em>(6),
1691–1705. (<a
href="https://doi.org/10.1007/s11222-020-09966-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the challenging problem of efficiently detecting changes in mean within multivariate data sequences. Multivariate changepoints can be detected by projecting a multivariate series to a univariate one using a suitable projection direction that preserves a maximal proportion of signal information. However, for some existing approaches the computation of such a projection direction can scale unfavourably with the number of series and might rely on additional assumptions on the data sequences, thus limiting their generality. We introduce BayesProject, a computationally inexpensive Bayesian approach to compute a projection direction in such a setting. The proposed approach allows the incorporation of prior knowledge of the changepoint scenario, when such information is available, which can help to increase the accuracy of the method. A simulation study shows that BayesProject is robust, yields projections close to the oracle projection direction and, moreover, that its accuracy in detecting changepoints is comparable to, or better than, existing algorithms while scaling linearly with the number of series.},
  archive      = {J_SAC},
  author       = {Hahn, Georg and Fearnhead, Paul and Eckley, Idris A.},
  doi          = {10.1007/s11222-020-09966-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1691-1705},
  shortjournal = {Stat. Comput.},
  title        = {BayesProject: Fast computation of a projection direction for multivariate changepoint detection},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Importance sampling for a robust and efficient multilevel
monte carlo estimator for stochastic reaction networks. <em>SAC</em>,
<em>30</em>(6), 1665–1689. (<a
href="https://doi.org/10.1007/s11222-020-09965-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multilevel Monte Carlo (MLMC) method for continuous-time Markov chains, first introduced by Anderson and Higham (SIAM Multiscal Model Simul 10(1):146–179, 2012), is a highly efficient simulation technique that can be used to estimate various statistical quantities for stochastic reaction networks, in particular for stochastic biological systems. Unfortunately, the robustness and performance of the multilevel method can be affected by the high kurtosis, a phenomenon observed at the deep levels of MLMC, which leads to inaccurate estimates of the sample variance. In this work, we address cases where the high-kurtosis phenomenon is due to catastrophic coupling (characteristic of pure jump processes where coupled consecutive paths are identical in most of the simulations, while differences only appear in a tiny proportion) and introduce a pathwise-dependent importance sampling (IS) technique that improves the robustness and efficiency of the multilevel method. Our theoretical results, along with the conducted numerical experiments, demonstrate that our proposed method significantly reduces the kurtosis of the deep levels of MLMC, and also improves the strong convergence rate from $$\beta =1$$ for the standard case (without IS), to $$\beta =1+\delta $$ , where $$0&lt;\delta &lt;1$$ is a user-selected parameter in our IS algorithm. Due to the complexity theorem of MLMC, and given a pre-selected tolerance, $$\text {TOL}$$ , this results in an improvement of the complexity from $${\mathcal {O}}\left( \text {TOL}^{-2} \log (\text {TOL})^2\right) $$ in the standard case to $${\mathcal {O}}\left( \text {TOL}^{-2}\right) $$ , which is the optimal complexity of the MLMC estimator. We achieve all these improvements with a negligible additional cost since our IS algorithm is only applied a few times across each simulated path.},
  archive      = {J_SAC},
  author       = {Ben Hammouda, Chiheb and Ben Rached, Nadhir and Tempone, Raúl},
  doi          = {10.1007/s11222-020-09965-3},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1665-1689},
  shortjournal = {Stat. Comput.},
  title        = {Importance sampling for a robust and efficient multilevel monte carlo estimator for stochastic reaction networks},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel sequential monte carlo for stochastic gradient-free
nonconvex optimization. <em>SAC</em>, <em>30</em>(6), 1645–1663. (<a
href="https://doi.org/10.1007/s11222-020-09964-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and analyze a parallel sequential Monte Carlo methodology for the numerical solution of optimization problems that involve the minimization of a cost function that consists of the sum of many individual components. The proposed scheme is a stochastic zeroth-order optimization algorithm which demands only the capability to evaluate small subsets of components of the cost function. It can be depicted as a bank of samplers that generate particle approximations of several sequences of probability measures. These measures are constructed in such a way that they have associated probability density functions whose global maxima coincide with the global minima of the original cost function. The algorithm selects the best performing sampler and uses it to approximate a global minimum of the cost function. We prove analytically that the resulting estimator converges to a global minimum of the cost function almost surely and provide explicit convergence rates in terms of the number of generated Monte Carlo samples and the dimension of the search space. We show, by way of numerical examples, that the algorithm can tackle cost functions with multiple minima or with broad “flat” regions which are hard to minimize using gradient-based techniques.},
  archive      = {J_SAC},
  author       = {Akyildiz, Ömer Deniz and Crisan, Dan and Míguez, Joaquín},
  doi          = {10.1007/s11222-020-09964-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1645-1663},
  shortjournal = {Stat. Comput.},
  title        = {Parallel sequential monte carlo for stochastic gradient-free nonconvex optimization},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imputation and low-rank estimation with missing not at
random data. <em>SAC</em>, <em>30</em>(6), 1629–1643. (<a
href="https://doi.org/10.1007/s11222-020-09963-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values challenge data analysis because many supervised and unsupervised learning methods cannot be applied directly to incomplete data. Matrix completion based on low-rank assumptions are very powerful solution for dealing with missing values. However, existing methods do not consider the case of informative missing values which are widely encountered in practice. This paper proposes matrix completion methods to recover Missing Not At Random (MNAR) data. Our first contribution is to suggest a model-based estimation strategy by modelling the missing mechanism distribution. An EM algorithm is then implemented, involving a Fast Iterative Soft-Thresholding Algorithm (FISTA). Our second contribution is to suggest a computationally efficient surrogate estimation by implicitly taking into account the joint distribution of the data and the missing mechanism: the data matrix is concatenated with the mask coding for the missing values; a low-rank structure for exponential family is assumed on this new matrix, in order to encode links between variables and missing mechanisms. The methodology that has the great advantage of handling different missing value mechanisms is robust to model specification errors. The performances of our methods are assessed on the real data collected from a trauma registry (TraumaBase $$^{\textregistered }$$ ) containing clinical information about over twenty thousand severely traumatized patients in France. The aim is then to predict if the doctors should administrate tranexomic acid to patients with traumatic brain injury, that would limit excessive bleeding.},
  archive      = {J_SAC},
  author       = {Sportisse, Aude and Boyer, Claire and Josse, Julie},
  doi          = {10.1007/s11222-020-09963-5},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1629-1643},
  shortjournal = {Stat. Comput.},
  title        = {Imputation and low-rank estimation with missing not at random data},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale process modelling and distributed computation
for spatial data. <em>SAC</em>, <em>30</em>(6), 1609–1627. (<a
href="https://doi.org/10.1007/s11222-020-09962-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen a huge development in spatial modelling and prediction methodology, driven by the increased availability of remote-sensing data and the reduced cost of distributed-processing technology. It is well known that modelling and prediction using infinite-dimensional process models is not possible with large data sets, and that both approximate models and, often, approximate-inference methods, are needed. The problem of fitting simple global spatial models to large data sets has been solved through the likes of multi-resolution approximations and nearest-neighbour techniques. Here we tackle the next challenge, that of fitting complex, nonstationary, multi-scale models to large data sets. We propose doing this through the use of superpositions of spatial processes with increasing spatial scale and increasing degrees of nonstationarity. Computation is facilitated through the use of Gaussian Markov random fields and parallel Markov chain Monte Carlo based on graph colouring. The resulting model allows for both distributed computing and distributed data. Importantly, it provides opportunities for genuine model and data scalability and yet is still able to borrow strength across large spatial scales. We illustrate a two-scale version on a data set of sea-surface temperature containing on the order of one million observations, and compare our approach to state-of-the-art spatial modelling and prediction methods.},
  archive      = {J_SAC},
  author       = {Zammit-Mangion, Andrew and Rougier, Jonathan},
  doi          = {10.1007/s11222-020-09962-6},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1609-1627},
  shortjournal = {Stat. Comput.},
  title        = {Multi-scale process modelling and distributed computation for spatial data},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling hierarchies of discrete random structures.
<em>SAC</em>, <em>30</em>(6), 1591–1607. (<a
href="https://doi.org/10.1007/s11222-020-09961-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical normalized discrete random measures identify a general class of priors that is suited to flexibly learn how the distribution of a response variable changes across groups of observations. A special case widely used in practice is the hierarchical Dirichlet process. Although current theory on hierarchies of nonparametric priors yields all relevant tools for drawing posterior inference, their implementation comes at a high computational cost. We fill this gap by proposing an approximation for a general class of hierarchical processes, which leads to an efficient conditional Gibbs sampling algorithm. The key idea consists of a deterministic truncation of the underlying random probability measures leading to a finite dimensional approximation of the original prior law. We provide both empirical and theoretical support for such a procedure.},
  archive      = {J_SAC},
  author       = {Lijoi, Antonio and Prünster, Igor and Rigon, Tommaso},
  doi          = {10.1007/s11222-020-09961-7},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1591-1607},
  shortjournal = {Stat. Comput.},
  title        = {Sampling hierarchies of discrete random structures},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for cluster point processes with over- or
under-dispersed cluster sizes. <em>SAC</em>, <em>30</em>(6), 1573–1590.
(<a href="https://doi.org/10.1007/s11222-020-09960-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster point processes comprise a class of models that have been used for a wide range of applications. While several models have been studied for the probability density function of the offspring displacements and the parent point process, there are few examples of non-Poisson distributed cluster sizes. In this paper, we introduce a generalization of the Thomas process, which allows for the cluster sizes to have a variance that is greater or less than the expected value. We refer to this as the cluster sizes being over- and under-dispersed, respectively. To fit the model, we introduce minimum contrast methods and a Bayesian MCMC algorithm. These are evaluated in a simulation study. It is found that using the Bayesian MCMC method, we are in most cases able to detect over- and under-dispersion in the cluster sizes. We use the MCMC method to fit the model to nerve fiber data, and contrast the results to those of a fitted Thomas process.},
  archive      = {J_SAC},
  author       = {Andersson, Claes and Mrkvička, Tomáš},
  doi          = {10.1007/s11222-020-09960-8},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1573-1590},
  shortjournal = {Stat. Comput.},
  title        = {Inference for cluster point processes with over- or under-dispersed cluster sizes},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anomaly and novelty detection for robust semi-supervised
learning. <em>SAC</em>, <em>30</em>(5), 1545–1571. (<a
href="https://doi.org/10.1007/s11222-020-09959-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three important issues are often encountered in Supervised and Semi-Supervised Classification: class memberships are unreliable for some training units (label noise), a proportion of observations might depart from the main structure of the data (outliers) and new groups in the test set may have not been encountered earlier in the learning phase (unobserved classes). The present work introduces a robust and adaptive Discriminant Analysis rule, capable of handling situations in which one or more of the aforementioned problems occur. Two EM-based classifiers are proposed: the first one that jointly exploits the training and test sets (transductive approach), and the second one that expands the parameter estimation using the test set, to complete the group structure learned from the training set (inductive approach). Experiments on synthetic and real data, artificially adulterated, are provided to underline the benefits of the proposed method.},
  archive      = {J_SAC},
  author       = {Cappozzo, Andrea and Greselin, Francesca and Murphy, Thomas Brendan},
  doi          = {10.1007/s11222-020-09959-1},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1545-1571},
  shortjournal = {Stat. Comput.},
  title        = {Anomaly and novelty detection for robust semi-supervised learning},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing clusterings and numbers of clusters by aggregation
of calibrated clustering validity indexes. <em>SAC</em>, <em>30</em>(5),
1523–1544. (<a
href="https://doi.org/10.1007/s11222-020-09958-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key issue in cluster analysis is the choice of an appropriate clustering method and the determination of the best number of clusters. Different clusterings are optimal on the same data set according to different criteria, and the choice of such criteria depends on the context and aim of clustering. Therefore, researchers need to consider what data analytic characteristics the clusters they are aiming at are supposed to have, among others within-cluster homogeneity, between-clusters separation, and stability. Here, a set of internal clustering validity indexes measuring different aspects of clustering quality is proposed, including some indexes from the literature. Users can choose the indexes that are relevant in the application at hand. In order to measure the overall quality of a clustering (for comparing clusterings from different methods and/or different numbers of clusters), the index values are calibrated for aggregation. Calibration is relative to a set of random clusterings on the same data. Two specific aggregated indexes are proposed and compared with existing indexes on simulated and real data.},
  archive      = {J_SAC},
  author       = {Akhanli, Serhat Emre and Hennig, Christian},
  doi          = {10.1007/s11222-020-09958-2},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1523-1544},
  shortjournal = {Stat. Comput.},
  title        = {Comparing clusterings and numbers of clusters by aggregation of calibrated clustering validity indexes},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference on high-dimensional implicit dynamic models using
a guided intermediate resampling filter. <em>SAC</em>, <em>30</em>(5),
1497–1522. (<a
href="https://doi.org/10.1007/s11222-020-09957-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for inference on moderately high-dimensional, nonlinear, non-Gaussian, partially observed Markov process models for which the transition density is not analytically tractable. Markov processes with intractable transition densities arise in models defined implicitly by simulation algorithms. Widely used particle filter methods are applicable to nonlinear, non-Gaussian models but suffer from the curse of dimensionality. Improved scalability is provided by ensemble Kalman filter methods, but these are inappropriate for highly nonlinear and non-Gaussian models. We propose a particle filter method having improved practical and theoretical scalability with respect to the model dimension. This method is applicable to implicitly defined models having analytically intractable transition densities. Our method is developed based on the assumption that the latent process is defined in continuous time and that a simulator of this latent process is available. In this method, particles are propagated at intermediate time intervals between observations and are resampled based on a forecast likelihood of future observations. We combine this particle filter with parameter estimation methodology to enable likelihood-based inference for highly nonlinear spatiotemporal systems. We demonstrate our methodology on a stochastic Lorenz 96 model and a model for the population dynamics of infectious diseases in a network of linked regions.},
  archive      = {J_SAC},
  author       = {Park, Joonha and Ionides, Edward L.},
  doi          = {10.1007/s11222-020-09957-3},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1497-1522},
  shortjournal = {Stat. Comput.},
  title        = {Inference on high-dimensional implicit dynamic models using a guided intermediate resampling filter},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simulating space-time random fields with nonseparable
gneiting-type covariance functions. <em>SAC</em>, <em>30</em>(5),
1479–1495. (<a
href="https://doi.org/10.1007/s11222-020-09956-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two algorithms are proposed to simulate space-time Gaussian random fields with a covariance function belonging to an extended Gneiting class, the definition of which depends on a completely monotone function associated with the spatial structure and a conditionally negative definite function associated with the temporal structure. In both cases, the simulated random field is constructed as a weighted sum of cosine waves, with a Gaussian spatial frequency vector and a uniform phase. The difference lies in the way to handle the temporal component. The first algorithm relies on a spectral decomposition in order to simulate a temporal frequency conditional upon the spatial one, while in the second algorithm the temporal frequency is replaced by an intrinsic random field whose variogram is proportional to the conditionally negative definite function associated with the temporal structure. Both algorithms are scalable as their computational cost is proportional to the number of space-time locations that may be irregular in space and time. They are illustrated and validated through synthetic examples.},
  archive      = {J_SAC},
  author       = {Allard, Denis and Emery, Xavier and Lacaux, Céline and Lantuéjoul, Christian},
  doi          = {10.1007/s11222-020-09956-4},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1479-1495},
  shortjournal = {Stat. Comput.},
  title        = {Simulating space-time random fields with nonseparable gneiting-type covariance functions},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Composite likelihood methods for histogram-valued random
variables. <em>SAC</em>, <em>30</em>(5), 1459–1477. (<a
href="https://doi.org/10.1007/s11222-020-09955-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic data analysis has been proposed as a technique for summarising large and complex datasets into a much smaller and tractable number of distributions—such as random rectangles or histograms—each describing a portion of the larger dataset. Recent work has developed likelihood-based methods that permit fitting models for the underlying data while only observing the distributional summaries. However, while powerful, when working with random histograms this approach rapidly becomes computationally intractable as the dimension of the underlying data increases. We introduce a composite-likelihood variation of this likelihood-based approach for the analysis of random histograms in K dimensions, through the construction of lower-dimensional marginal histograms. The performance of this approach is examined through simulated and real data analysis of max-stable models for spatial extremes using millions of observed datapoints in more than $$K=100$$ dimensions. Large computational savings are available compared to existing model fitting approaches.},
  archive      = {J_SAC},
  author       = {Whitaker, T. and Beranger, B. and Sisson, S. A.},
  doi          = {10.1007/s11222-020-09955-5},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1459-1477},
  shortjournal = {Stat. Comput.},
  title        = {Composite likelihood methods for histogram-valued random variables},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling of bayesian posteriors with a non-gaussian
probabilistic learning on manifolds from a small dataset. <em>SAC</em>,
<em>30</em>(5), 1433–1457. (<a
href="https://doi.org/10.1007/s11222-020-09954-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the challenge presented by small-data to the task of Bayesian inference. A novel methodology, based on manifold learning and manifold sampling, is proposed for solving this computational statistics problem under the following assumptions: (1) neither the prior model nor the likelihood function are Gaussian and neither can be approximated by a Gaussian measure; (2) the number of functional input (system parameters) and functional output (quantity of interest) can be large; (3) the number of available realizations of the prior model is small, leading to the small-data challenge typically associated with expensive numerical simulations; the number of experimental realizations is also small; (4) the number of the posterior realizations required for decision is much larger than the available initial dataset. The method and its mathematical aspects are detailed. Three applications are presented for validation: The first two involve mathematical constructions aimed to develop intuition around the method and to explore its performance. The third example aims to demonstrate the operational value of the method using a more complex application related to the statistical inverse identification of the non-Gaussian matrix-valued random elasticity field of a damaged biological tissue (osteoporosis in a cortical bone) using ultrasonic waves.},
  archive      = {J_SAC},
  author       = {Soize, Christian and Ghanem, Roger G. and Desceliers, Christophe},
  doi          = {10.1007/s11222-020-09954-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1433-1457},
  shortjournal = {Stat. Comput.},
  title        = {Sampling of bayesian posteriors with a non-gaussian probabilistic learning on manifolds from a small dataset},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalised joint regression for count data: A penalty
extension for competitive settings. <em>SAC</em>, <em>30</em>(5),
1419–1432. (<a
href="https://doi.org/10.1007/s11222-020-09953-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a versatile joint regression framework for count responses. The method is implemented in the R add-on package GJRM and allows for modelling linear and non-linear dependence through the use of several copulae. Moreover, the parameters of the marginal distributions of the count responses and of the copula can be specified as flexible functions of covariates. Motivated by competitive settings, we also discuss an extension which forces the regression coefficients of the marginal (linear) predictors to be equal via a suitable penalisation. Model fitting is based on a trust region algorithm which estimates simultaneously all the parameters of the joint models. We investigate the proposal’s empirical performance in two simulation studies, the first one designed for arbitrary count data, the other one reflecting competitive settings. Finally, the method is applied to football data, showing its benefits compared to the standard approach with regard to predictive performance.},
  archive      = {J_SAC},
  author       = {van der Wurp, Hendrik and Groll, Andreas and Kneib, Thomas and Marra, Giampiero and Radice, Rosalba},
  doi          = {10.1007/s11222-020-09953-7},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1419-1432},
  shortjournal = {Stat. Comput.},
  title        = {Generalised joint regression for count data: A penalty extension for competitive settings},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The turning arcs: A computationally efficient algorithm to
simulate isotropic vector-valued gaussian random fields on the d-sphere.
<em>SAC</em>, <em>30</em>(5), 1403–1418. (<a
href="https://doi.org/10.1007/s11222-020-09952-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random fields on the sphere play a fundamental role in the natural sciences. This paper presents a simulation algorithm parenthetical to the spectral turning bands method used in Euclidean spaces, for simulating scalar- or vector-valued Gaussian random fields on the d-dimensional unit sphere. The simulated random field is obtained by a sum of Gegenbauer waves, each of which is variable along a randomly oriented arc and constant along the parallels orthogonal to the arc. Convergence criteria based on the Berry-Esséen inequality are proposed to choose suitable parameters for the implementation of the algorithm, which is illustrated through numerical experiments. A by-product of this work is a closed-form expression of the Schoenberg coefficients associated with the Chentsov and exponential covariance models on spheres of dimensions greater than or equal to 2.},
  archive      = {J_SAC},
  author       = {Alegría, Alfredo and Emery, Xavier and Lantuéjoul, Christian},
  doi          = {10.1007/s11222-020-09952-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1403-1418},
  shortjournal = {Stat. Comput.},
  title        = {The turning arcs: A computationally efficient algorithm to simulate isotropic vector-valued gaussian random fields on the d-sphere},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multilevel particle filters for the non-linear filtering
problem in continuous time. <em>SAC</em>, <em>30</em>(5), 1381–1402. (<a
href="https://doi.org/10.1007/s11222-020-09951-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the following article we consider the numerical approximation of the non-linear filter in continuous-time, where the observations and signal follow diffusion processes. Given access to high-frequency, but discrete-time observations, we resort to a first order time discretization of the non-linear filter, followed by an Euler discretization of the signal dynamics. In order to approximate the associated discretized non-linear filter, one can use a particle filter. Under assumptions, this can achieve a mean square error of $$\mathcal {O}(\epsilon ^2)$$ , for $$\epsilon &gt;0$$ arbitrary, such that the associated cost is $$\mathcal {O}(\epsilon ^{-4})$$ . We prove, under assumptions, that the multilevel particle filter of Jasra et al. (SIAM J Numer Anal 55:3068–3096, 2017) can achieve a mean square error of $$\mathcal {O}(\epsilon ^2)$$ , for cost $$\mathcal {O}(\epsilon ^{-3})$$ . This is supported by numerical simulations in several examples.},
  archive      = {J_SAC},
  author       = {Jasra, Ajay and Yu, Fangyuan and Heng, Jeremy},
  doi          = {10.1007/s11222-020-09951-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1381-1402},
  shortjournal = {Stat. Comput.},
  title        = {Multilevel particle filters for the non-linear filtering problem in continuous time},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based clustering with determinant-and-shape
constraint. <em>SAC</em>, <em>30</em>(5), 1363–1380. (<a
href="https://doi.org/10.1007/s11222-020-09950-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based approaches to cluster analysis and mixture modeling often involve maximizing classification and mixture likelihoods. Without appropriate constrains on the scatter matrices of the components, these maximizations result in ill-posed problems. Moreover, without constrains, non-interesting or “spurious” clusters are often detected by the EM and CEM algorithms traditionally used for the maximization of the likelihood criteria. Considering an upper bound on the maximal ratio between the determinants of the scatter matrices seems to be a sensible way to overcome these problems by affine equivariant constraints. Unfortunately, problems still arise without also controlling the elements of the “shape” matrices. A new methodology is proposed that allows both control of the scatter matrices determinants and also the shape matrices elements. Some theoretical justification is given. A fast algorithm is proposed for this doubly constrained maximization. The methodology is also extended to robust model-based clustering problems.},
  archive      = {J_SAC},
  author       = {García-Escudero, Luis Angel and Mayo-Iscar, Agustín and Riani, Marco},
  doi          = {10.1007/s11222-020-09950-w},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1363-1380},
  shortjournal = {Stat. Comput.},
  title        = {Model-based clustering with determinant-and-shape constraint},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An information theoretic approach to post randomization
methods under differential privacy. <em>SAC</em>, <em>30</em>(5),
1347–1361. (<a
href="https://doi.org/10.1007/s11222-020-09949-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post randomization methods are among the most popular disclosure limitation techniques for both categorical and continuous data. In the categorical case, given a stochastic matrix M and a specified variable, an individual belonging to category i is changed to category j with probability $$M_{i,j}$$ . Every approach to choose the randomization matrix M has to balance between two desiderata: (1) preserving as much statistical information from the raw data as possible; (2) guaranteeing the privacy of individuals in the dataset. This trade-off has generally been shown to be very challenging to solve. In this work, we use recent tools from the computer science literature and propose to choose M as the solution of a constrained maximization problems. Specifically, M is chosen as the solution of a constrained maximization problem, where we maximize the mutual information between raw and transformed data, given the constraint that the transformation satisfies the notion of differential privacy. For the general categorical model, it is shown how this maximization problem reduces to a convex linear programming and can be therefore solved with known optimization algorithms.},
  archive      = {J_SAC},
  author       = {Ayed, Fadhel and Battiston, Marco and Camerlenghi, Federico},
  doi          = {10.1007/s11222-020-09949-3},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1347-1361},
  shortjournal = {Stat. Comput.},
  title        = {An information theoretic approach to post randomization methods under differential privacy},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Markov chain monte carlo algorithms with sequential
proposals. <em>SAC</em>, <em>30</em>(5), 1325–1345. (<a
href="https://doi.org/10.1007/s11222-020-09948-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore a general framework in Markov chain Monte Carlo (MCMC) sampling where sequential proposals are tried as a candidate for the next state of the Markov chain. This sequential-proposal framework can be applied to various existing MCMC methods, including Metropolis–Hastings algorithms using random proposals and methods that use deterministic proposals such as Hamiltonian Monte Carlo (HMC) or the bouncy particle sampler. Sequential-proposal MCMC methods construct the same Markov chains as those constructed by the delayed rejection method under certain circumstances. In the context of HMC, the sequential-proposal approach has been proposed as extra chance generalized hybrid Monte Carlo (XCGHMC). We develop two novel methods in which the trajectories leading to proposals in HMC are automatically tuned to avoid doubling back, as in the No-U-Turn sampler (NUTS). The numerical efficiency of these new methods compare favorably to the NUTS. We additionally show that the sequential-proposal bouncy particle sampler enables the constructed Markov chain to pass through regions of low target density and thus facilitates better mixing of the chain when the target density is multimodal.},
  archive      = {J_SAC},
  author       = {Park, Joonha and Atchadé, Yves},
  doi          = {10.1007/s11222-020-09948-4},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1325-1345},
  shortjournal = {Stat. Comput.},
  title        = {Markov chain monte carlo algorithms with sequential proposals},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Characterization of topic-based online communities by
combining network data and user generated content. <em>SAC</em>,
<em>30</em>(5), 1309–1324. (<a
href="https://doi.org/10.1007/s11222-020-09947-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a model for characterizing online communities by combining two types of data: network data and user-generated-content (UGC). The existing models for detecting the community structure of a network employ only network information. However, not all people connected in a network share the same interests. For instance, even if students belong to the same community of “school,” they may have various hobbies such as music, books, or sports. Hence, it is more realistic and beneficial for companies to identify communities according to their interests uncovered by their communications on social media. In addition, people may belong to multiple communities such as family, work, and online friends. Our model explores multiple overlapping communities according to their topics identified using two types of data jointly. By way of validating the main features of the proposed model, our simulation study shows that the model correctly identifies the community structure that could not be found without considering both network data and UGC. Furthermore, an empirical analysis using Twitter data clarifies that our model can find realistic and meaningful community structures from large online networks and has a good predictive performance.},
  archive      = {J_SAC},
  author       = {Igarashi, Mirai and Terui, Nobuhiko},
  doi          = {10.1007/s11222-020-09947-5},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1309-1324},
  shortjournal = {Stat. Comput.},
  title        = {Characterization of topic-based online communities by combining network data and user generated content},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian estimation of the latent dimension and communities
in stochastic blockmodels. <em>SAC</em>, <em>30</em>(5), 1291–1307. (<a
href="https://doi.org/10.1007/s11222-020-09946-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral embedding of adjacency or Laplacian matrices of undirected graphs is a common technique for representing a network in a lower dimensional latent space, with optimal theoretical guarantees. The embedding can be used to estimate the community structure of the network, with strong consistency results in the stochastic blockmodel framework. One of the main practical limitations of standard algorithms for community detection from spectral embeddings is that the number of communities and the latent dimension of the embedding must be specified in advance. In this article, a novel Bayesian model for simultaneous and automatic selection of the appropriate dimension of the latent space and the number of blocks is proposed. Extensions to directed and bipartite graphs are discussed. The model is tested on simulated and real world network data, showing promising performance for recovering latent community structure.},
  archive      = {J_SAC},
  author       = {Passino, Francesco Sanna and Heard, Nicholas A.},
  doi          = {10.1007/s11222-020-09946-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1291-1307},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian estimation of the latent dimension and communities in stochastic blockmodels},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The conditional censored graphical lasso estimator.
<em>SAC</em>, <em>30</em>(5), 1273–1289. (<a
href="https://doi.org/10.1007/s11222-020-09945-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applied fields, such as genomics, different types of data are collected on the same system, and it is not uncommon that some of these datasets are subject to censoring as a result of the measurement technologies used, such as data generated by polymerase chain reactions and flow cytometer. When the overall objective is that of network inference, at possibly different levels of a system, information coming from different sources and/or different steps of the analysis can be integrated into one model with the use of conditional graphical models. In this paper, we develop a doubly penalized inferential procedure for a conditional Gaussian graphical model when data can be subject to censoring. The computational challenges of handling censored data in high dimensionality are met with the development of an efficient expectation-maximization algorithm, based on approximate calculations of the moments of truncated Gaussian distributions and on a suitably derived two-step procedure alternating graphical lasso with a novel block-coordinate multivariate lasso approach. We evaluate the performance of this approach on an extensive simulation study and on gene expression data generated by RT-qPCR technologies, where we are able to integrate network inference, differential expression detection and data normalization into one model.},
  archive      = {J_SAC},
  author       = {Augugliaro, Luigi and Sottile, Gianluca and Vinciotti, Veronica},
  doi          = {10.1007/s11222-020-09945-7},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1273-1289},
  shortjournal = {Stat. Comput.},
  title        = {The conditional censored graphical lasso estimator},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditionally structured variational gaussian approximation
with importance weights. <em>SAC</em>, <em>30</em>(5), 1255–1272. (<a
href="https://doi.org/10.1007/s11222-020-09944-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop flexible methods of deriving variational inference for models with complex latent variable structure. By splitting the variables in these models into “global” parameters and “local” latent variables, we define a class of variational approximations that exploit this partitioning and go beyond Gaussian variational approximation. This approximation is motivated by the fact that in many hierarchical models, there are global variance parameters which determine the scale of local latent variables in their posterior conditional on the global parameters. We also consider parsimonious parametrizations by using conditional independence structure and improved estimation of the log marginal likelihood and variational density using importance weights. These methods are shown to improve significantly on Gaussian variational approximation methods for a similar computational cost. Application of the methodology is illustrated using generalized linear mixed models and state space models.},
  archive      = {J_SAC},
  author       = {Tan, Linda S. L. and Bhaskaran, Aishwarya and Nott, David J.},
  doi          = {10.1007/s11222-020-09944-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1255-1272},
  shortjournal = {Stat. Comput.},
  title        = {Conditionally structured variational gaussian approximation with importance weights},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification of periodic arrivals in event time data for
filtering computer network traffic. <em>SAC</em>, <em>30</em>(5),
1241–1254. (<a
href="https://doi.org/10.1007/s11222-020-09943-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodic patterns can often be observed in real-world event time data, possibly mixed with non-periodic arrival times. For modelling purposes, it is necessary to correctly distinguish the two types of events. This task has particularly important implications in computer network security; there, separating automated polling traffic and human-generated activity in a computer network is important for building realistic statistical models for normal activity, which in turn can be used for anomaly detection. Since automated events commonly occur at a fixed periodicity, statistical tests using Fourier analysis can efficiently detect whether the arrival times present an automated component. In this article, sequences of arrival times which contain automated events are further examined, to separate polling and non-periodic activity. This is first achieved using a simple mixture model on the unit circle based on the angular positions of each event time on the p-clock, where p represents the main periodicity associated with the automated activity; this model is then extended by combining a second source of information, the time of day of each event. Efficient implementations exploiting conjugate Bayesian models are discussed, and performance is assessed on real network flow data collected at Imperial College London.},
  archive      = {J_SAC},
  author       = {Sanna Passino, Francesco and Heard, Nicholas A.},
  doi          = {10.1007/s11222-020-09943-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1241-1254},
  shortjournal = {Stat. Comput.},
  title        = {Classification of periodic arrivals in event time data for filtering computer network traffic},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inhomogeneous higher-order summary statistics for point
processes on linear networks. <em>SAC</em>, <em>30</em>(5), 1221–1239.
(<a href="https://doi.org/10.1007/s11222-020-09942-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a workaround for the lack of transitive transformations on linear network structures, which are required to consider different notions of distributional invariance, including stationarity, we introduce the notions of pseudostationarity and intensity reweighted moment pseudostationarity for point processes on linear networks. Moreover, using arbitrary so-called regular linear network distances, e.g. the Euclidean and the shortest-path distance, we further propose geometrically corrected versions of different higher-order summary statistics, including the inhomogeneous empty space function, the inhomogeneous nearest neighbour distance distribution function and the inhomogeneous J-function. Such summary statistics detect interactions of order higher than two. We also discuss their nonparametric estimators and through a simulation study, considering models with different types of spatial interaction and different networks, we study the performance of our proposed summary statistics by means of envelopes. Our summary statistic estimators manage to capture clustering, regularity as well as Poisson process independence. Finally, we make use of our new summary statistics to analyse two different datasets: motor vehicle traffic accidents and spiderwebs.},
  archive      = {J_SAC},
  author       = {Cronie, Ottmar and Moradi, Mehdi and Mateu, Jorge},
  doi          = {10.1007/s11222-020-09942-w},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1221-1239},
  shortjournal = {Stat. Comput.},
  title        = {Inhomogeneous higher-order summary statistics for point processes on linear networks},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating time-varying directed neural networks.
<em>SAC</em>, <em>30</em>(5), 1209–1220. (<a
href="https://doi.org/10.1007/s11222-020-09941-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing the functional network of a neuron cluster is a fundamental step to reveal the complex interactions among neural systems of the brain. Current approaches to reconstruct a network of neurons or neural systems focus on establishing a static network by assuming the neural network structure does not change over time. To the best of our knowledge, this is the first attempt to build a time-varying directed network of neurons by using an ordinary differential equation model, which allows us to describe the underlying dynamical mechanism of network connections. The proposed method is demonstrated by estimating a network of wide dynamic range neurons located in the dorsal horn of the rats’ spinal cord in response to pain stimuli applied to the Zusanli acupoint on the right leg. The finite sample performance of the proposed method is also investigated with a simulation study.},
  archive      = {J_SAC},
  author       = {Wang, Haixu and Cao, Jiguo},
  doi          = {10.1007/s11222-020-09941-x},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1209-1220},
  shortjournal = {Stat. Comput.},
  title        = {Estimating time-varying directed neural networks},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A laplace-based algorithm for bayesian adaptive design.
<em>SAC</em>, <em>30</em>(5), 1183–1208. (<a
href="https://doi.org/10.1007/s11222-020-09938-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel Laplace-based algorithm that can be used to find Bayesian adaptive designs under model and parameter uncertainty. Our algorithm uses Laplace importance sampling to provide a computationally efficient approach to undertake adaptive design and inference when compared to standard approaches such as those based on the sequential Monte Carlo (SMC) algorithm. Like the SMC approach, our new algorithm requires very little problem-specific tuning and provides an efficient estimate of utility functions for parameter estimation and/or model choice. Further, within our algorithm, we adopt methods from Pareto smoothing to improve the robustness of the algorithm in forming particle approximations to posterior distributions. To evaluate our new adaptive design algorithm, three motivating examples from the literature are considered including examples where binary, multiple response and count data are observed under considerable model and parameter uncertainty. We benchmark the performance of our new algorithm against: (1) the standard SMC algorithm and (2) a standard implementation of the Laplace approximation in adaptive design. We assess the performance of each algorithm through comparing computational efficiency and design selection. The results show that our new algorithm is computationally efficient and selects designs that can perform as well as or better than the other two approaches. As such, we propose our Laplace-based algorithm as an efficient approach for designing adaptive experiments.},
  archive      = {J_SAC},
  author       = {Senarathne, S. G. J. and Drovandi, C. C. and McGree, J. M.},
  doi          = {10.1007/s11222-020-09938-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1183-1208},
  shortjournal = {Stat. Comput.},
  title        = {A laplace-based algorithm for bayesian adaptive design},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal non-negative forecast reconciliation. <em>SAC</em>,
<em>30</em>(5), 1167–1182. (<a
href="https://doi.org/10.1007/s11222-020-09930-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sum of forecasts of disaggregated time series is often required to equal the forecast of the aggregate, giving a set of coherent forecasts. The least squares solution for finding coherent forecasts uses a reconciliation approach known as MinT, proposed by Wickramasuriya, Athanasopoulos, and Hyndman (2019). The MinT approach and its variants do not guarantee that the coherent forecasts are non-negative, even when all of the original forecasts are non-negative in nature. This has become a serious issue in applications that are inherently non-negative such as with sales data or tourism numbers. While overcoming this difficulty, we reconsider the least squares minimization problem with non-negativity constraints to ensure that the coherent forecasts are strictly non-negative. The constrained quadratic programming problem is solved using three algorithms. They are the block principal pivoting (BPV) algorithm, projected conjugate gradient (PCG) algorithm, and scaled gradient projection algorithm. A Monte Carlo simulation is performed to evaluate the computational performances of these algorithms as the number of time series increases. The results demonstrate that the BPV algorithm clearly outperforms the rest, and PCG is the second best. The superior performance of the BPV algorithm can be partially attributed to the alternative representation of the weight matrix in the MinT approach. An empirical investigation is carried out to assess the impact of imposing non-negativity constraints on forecast reconciliation over the unconstrained method. It is observed that slight gains in forecast accuracy have occurred at the most disaggregated level. At the aggregated level, slight losses are also observed. Although the gains or losses are negligible, the procedure plays an important role in decision and policy implementation processes.},
  archive      = {J_SAC},
  author       = {Wickramasuriya, Shanika L. and Turlach, Berwin A. and Hyndman, Rob J.},
  doi          = {10.1007/s11222-020-09930-0},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1167-1182},
  shortjournal = {Stat. Comput.},
  title        = {Optimal non-negative forecast reconciliation},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional changepoint detection via a geometrically
inspired mapping. <em>SAC</em>, <em>30</em>(4), 1155–1166. (<a
href="https://doi.org/10.1007/s11222-020-09940-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional changepoint analysis is a growing area of research and has applications in a wide range of fields. The aim is to accurately and efficiently detect changepoints in time series data when both the number of time points and dimensions grow large. Existing methods typically aggregate or project the data to a smaller number of dimensions, usually one. We present a high-dimensional changepoint detection method that takes inspiration from geometry to map a high-dimensional time series to two dimensions. We show theoretically and through simulation that if the input series is Gaussian, then the mappings preserve the Gaussianity of the data. Applying univariate changepoint detection methods to both mapped series allows the detection of changepoints that correspond to changes in the mean and variance of the original time series. We demonstrate that this approach outperforms the current state-of-the-art multivariate changepoint methods in terms of accuracy of detected changepoints and computational efficiency. We conclude with applications from genetics and finance.},
  archive      = {J_SAC},
  author       = {Grundy, Thomas and Killick, Rebecca and Mihaylov, Gueorgui},
  doi          = {10.1007/s11222-020-09940-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1155-1166},
  shortjournal = {Stat. Comput.},
  title        = {High-dimensional changepoint detection via a geometrically inspired mapping},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional VAR with low-rank transition. <em>SAC</em>,
<em>30</em>(4), 1139–1153. (<a
href="https://doi.org/10.1007/s11222-020-09929-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a vector auto-regressive model with a low-rank constraint on the transition matrix. This model is well suited to predict high-dimensional series that are highly correlated, or that are driven by a small number of hidden factors. While our model has formal similarities with factor models, its structure is more a way to reduce the dimension in order to improve the predictions, rather than a way to define interpretable factors. We provide an estimator for the transition matrix in a very general setting and study its performances in terms of prediction and adaptation to the unknown rank. Our method obtains good result on simulated data, in particular when the rank of the underlying process is small. On macroeconomic data from Giannone et al. (Rev Econ Stat 97(2):436–451, 2015), our method is competitive with state-of-the-art methods in small dimension and even improves on them in high dimension.},
  archive      = {J_SAC},
  author       = {Alquier, Pierre and Bertin, Karine and Doukhan, Paul and Garnier, Rémy},
  doi          = {10.1007/s11222-020-09929-7},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1139-1153},
  shortjournal = {Stat. Comput.},
  title        = {High-dimensional VAR with low-rank transition},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matrix completion with nonconvex regularization: Spectral
operators and scalable algorithms. <em>SAC</em>, <em>30</em>(4),
1113–1138. (<a
href="https://doi.org/10.1007/s11222-020-09939-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the popularly dubbed matrix completion problem, where the task is to “fill in” the unobserved entries of a matrix from a small subset of observed entries, under the assumption that the underlying matrix is of low rank. Our contributions herein enhance our prior work on nuclear norm regularized problems for matrix completion (Mazumder et al. in J Mach Learn Res 1532(11):2287–2322, 2010) by incorporating a continuum of nonconvex penalty functions between the convex nuclear norm and nonconvex rank functions. Inspired by Soft-Impute (Mazumder et al. 2010; Hastie et al. in J Mach Learn Res, 2016), we propose NC-Impute—an EM-flavored algorithmic framework for computing a family of nonconvex penalized matrix completion problems with warm starts. We present a systematic study of the associated spectral thresholding operators, which play an important role in the overall algorithm. We study convergence properties of the algorithm. Using structured low-rank SVD computations, we demonstrate the computational scalability of our proposal for problems up to the Netflix size (approximately, a 500,000 $$\times $$ 20,000 matrix with $$10^8$$ observed entries). We demonstrate that on a wide range of synthetic and real data instances, our proposed nonconvex regularization framework leads to low-rank solutions with better predictive performance when compared to those obtained from nuclear norm problems. Implementations of algorithms proposed herein, written in the R language, are made available on github.},
  archive      = {J_SAC},
  author       = {Mazumder, Rahul and Saldana, Diego and Weng, Haolei},
  doi          = {10.1007/s11222-020-09939-5},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1113-1138},
  shortjournal = {Stat. Comput.},
  title        = {Matrix completion with nonconvex regularization: Spectral operators and scalable algorithms},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal classification of gaussian processes in homo- and
heteroscedastic settings. <em>SAC</em>, <em>30</em>(4), 1091–1111. (<a
href="https://doi.org/10.1007/s11222-020-09937-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A procedure to derive optimal discrimination rules is formulated for binary functional classification problems in which the instances available for induction are characterized by random trajectories sampled from different Gaussian processes, depending on the class label. Specifically, these optimal rules are derived as the asymptotic form of the quadratic discriminant for the discretely monitored trajectories in the limit that the set of monitoring points becomes dense in the interval on which the processes are defined. The main goal of this work is to provide a detailed analysis of such optimal rules in the dense monitoring limit, with a particular focus on elucidating the mechanisms by which near-perfect classification arises. In the general case, the quadratic discriminant includes terms that are singular in this limit. If such singularities do not cancel out, one obtains near-perfect classification, which means that the error approaches zero asymptotically, for infinite sample sizes. This singular limit is a consequence of the orthogonality of the probability measures associated with the stochastic processes from which the trajectories are sampled. As a further novel result of this analysis, we formulate rules to determine whether two Gaussian processes are equivalent or mutually singular (orthogonal).},
  archive      = {J_SAC},
  author       = {Torrecilla, José L. and Ramos-Carreño, Carlos and Sánchez-Montañés, Manuel and Suárez, Alberto},
  doi          = {10.1007/s11222-020-09937-7},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1091-1111},
  shortjournal = {Stat. Comput.},
  title        = {Optimal classification of gaussian processes in homo- and heteroscedastic settings},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive iterative hessian sketch via a-optimal subsampling.
<em>SAC</em>, <em>30</em>(4), 1075–1090. (<a
href="https://doi.org/10.1007/s11222-020-09936-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iterative Hessian sketch (IHS) is an effective sketching method for modeling large-scale data. It was originally proposed by Pilanci and Wainwright (J Mach Learn Res 17(1):1842–1879, 2016) based on randomized sketching matrices. However, it is computationally intensive due to the iterative sketch process. In this paper, we analyze the IHS algorithm under the unconstrained least squares problem setting and then propose a deterministic approach for improving IHS via A-optimal subsampling. Our contributions are threefold: (1) a good initial estimator based on the A-optimal design is suggested; (2) a novel ridged preconditioner is developed for repeated sketching; and (3) an exact line search method is proposed for determining the optimal step length adaptively. Extensive experimental results demonstrate that our proposed A-optimal IHS algorithm outperforms the existing accelerated IHS methods.},
  archive      = {J_SAC},
  author       = {Zhang, Aijun and Zhang, Hengtao and Yin, Guosheng},
  doi          = {10.1007/s11222-020-09936-8},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1075-1090},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive iterative hessian sketch via A-optimal subsampling},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Likelihood-free approximate gibbs sampling. <em>SAC</em>,
<em>30</em>(4), 1057–1073. (<a
href="https://doi.org/10.1007/s11222-020-09933-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likelihood-free methods such as approximate Bayesian computation (ABC) have extended the reach of statistical inference to problems with computationally intractable likelihoods. Such approaches perform well for small-to-moderate dimensional problems, but suffer a curse of dimensionality in the number of model parameters. We introduce a likelihood-free approximate Gibbs sampler that naturally circumvents the dimensionality issue by focusing on lower-dimensional conditional distributions. These distributions are estimated by flexible regression models either before the sampler is run, or adaptively during sampler implementation. As a result, and in comparison to Metropolis-Hastings-based approaches, we are able to fit substantially more challenging statistical models than would otherwise be possible. We demonstrate the sampler’s performance via two simulated examples, and a real analysis of Airbnb rental prices using a intractable high-dimensional multivariate nonlinear state-space model with a 36-dimensional latent state observed on 365 time points, which presents a real challenge to standard ABC techniques.},
  archive      = {J_SAC},
  author       = {Rodrigues, G. S. and Nott, David J. and Sisson, S. A.},
  doi          = {10.1007/s11222-020-09933-x},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1057-1073},
  shortjournal = {Stat. Comput.},
  title        = {Likelihood-free approximate gibbs sampling},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating metropolis-within-gibbs sampler with localized
computations of differential equations. <em>SAC</em>, <em>30</em>(4),
1037–1056. (<a
href="https://doi.org/10.1007/s11222-020-09934-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse problem is ubiquitous in science and engineering, and Bayesian methodologies are often used to infer the underlying parameters. For high-dimensional temporal-spatial models, classical Markov chain Monte Carlo methods are often slow to converge, and it is necessary to apply Metropolis-within-Gibbs (MwG) sampling on parameter blocks. However, the computation cost of each MwG iteration is typically $$O(n^2)$$, where n is the model dimension. This can be too expensive in practice. This paper introduces a new reduced computation method to bring down the computation cost to O(n), for the inverse initial value problem of a stochastic differential equation (SDE) with local interactions. The key observation is that each MwG proposal is only different from the original iterate at one parameter block, and this difference will only propagate within a local domain in the SDE computations. Therefore, we can approximate the global SDE computation with a surrogate updated only within the local domain for reduced computation cost. Both theoretically and numerically, we show that the approximation errors can be controlled by the local domain size. We discuss how to implement the local computation scheme using Euler–Maruyama and fourth-order Runge–Kutta methods. We numerically demonstrate the performance of the proposed method with the Lorenz 96 model and a linear stochastic flow model.},
  archive      = {J_SAC},
  author       = {Liu, Qiang and Tong, Xin T.},
  doi          = {10.1007/s11222-020-09934-w},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1037-1056},
  shortjournal = {Stat. Comput.},
  title        = {Accelerating metropolis-within-gibbs sampler with localized computations of differential equations},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian nonparametric priors for hidden markov random
fields. <em>SAC</em>, <em>30</em>(4), 1015–1035. (<a
href="https://doi.org/10.1007/s11222-020-09935-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the central issues in statistics and machine learning is how to select an adequate model that can automatically adapt its complexity to the observed data. In the present paper, we focus on the issue of determining the structure of clustered data, both in terms of finding the appropriate number of clusters and of modeling the right dependence structure between the observations. Bayesian nonparametric (BNP) models, which do not impose an upper limit on the number of clusters, are appropriate to avoid the required guess on the number of clusters but have been mainly developed for independent data. In contrast, Markov random fields (MRF) have been extensively used to model dependencies in a tractable manner but usually reduce to finite cluster numbers when clustering tasks are addressed. Our main contribution is to propose a general scheme to design tractable BNP–MRF priors that combine both features: no commitment to an arbitrary number of clusters and a dependence modeling. A key ingredient in this construction is the availability of a stick-breaking representation which has the threefold advantage to allowing us to extend standard discrete MRFs to infinite state space, to design a tractable estimation algorithm using variational approximation and to derive theoretical properties on the predictive distribution and the number of clusters of the proposed model. This approach is illustrated on a challenging natural image segmentation task for which it shows good performance with respect to the literature.},
  archive      = {J_SAC},
  author       = {Lü, Hongliang and Arbel, Julyan and Forbes, Florence},
  doi          = {10.1007/s11222-020-09935-9},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1015-1035},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian nonparametric priors for hidden markov random fields},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint models with multiple longitudinal outcomes and a
time-to-event outcome: A corrected two-stage approach. <em>SAC</em>,
<em>30</em>(4), 999–1014. (<a
href="https://doi.org/10.1007/s11222-020-09927-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for longitudinal and survival data have gained a lot of attention in recent years, with the development of myriad extensions to the basic model, including those which allow for multivariate longitudinal data, competing risks and recurrent events. Several software packages are now also available for their implementation. Although mathematically straightforward, the inclusion of multiple longitudinal outcomes in the joint model remains computationally difficult due to the large number of random effects required, which hampers the practical application of this extension. We present a novel approach that enables the fitting of such models with more realistic computational times. The idea behind the approach is to split the estimation of the joint model in two steps: estimating a multivariate mixed model for the longitudinal outcomes and then using the output from this model to fit the survival submodel. So-called two-stage approaches have previously been proposed and shown to be biased. Our approach differs from the standard version, in that we additionally propose the application of a correction factor, adjusting the estimates obtained such that they more closely resemble those we would expect to find with the multivariate joint model. This correction is based on importance sampling ideas. Simulation studies show that this corrected two-stage approach works satisfactorily, eliminating the bias while maintaining substantial improvement in computational time, even in more difficult settings.},
  archive      = {J_SAC},
  author       = {Mauff, Katya and Steyerberg, Ewout and Kardys, Isabella and Boersma, Eric and Rizopoulos, Dimitris},
  doi          = {10.1007/s11222-020-09927-9},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {999-1014},
  shortjournal = {Stat. Comput.},
  title        = {Joint models with multiple longitudinal outcomes and a time-to-event outcome: A corrected two-stage approach},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance reduction for markov chains with application to
MCMC. <em>SAC</em>, <em>30</em>(4), 973–997. (<a
href="https://doi.org/10.1007/s11222-020-09931-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel variance reduction approach for additive functionals of Markov chains based on minimization of an estimate for the asymptotic variance of these functionals over suitable classes of control variates. A distinctive feature of the proposed approach is its ability to significantly reduce the overall finite sample variance. This feature is theoretically demonstrated by means of a deep non-asymptotic analysis of a variance reduced functional as well as by a thorough simulation study. In particular, we apply our method to various MCMC Bayesian estimation problems where it favorably compares to the existing variance reduction approaches.},
  archive      = {J_SAC},
  author       = {Belomestny, D. and Iosipoi, L. and Moulines, E. and Naumov, A. and Samsonov, S.},
  doi          = {10.1007/s11222-020-09931-z},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {973-997},
  shortjournal = {Stat. Comput.},
  title        = {Variance reduction for markov chains with application to MCMC},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Metrics and barycenters for point pattern data.
<em>SAC</em>, <em>30</em>(4), 953–972. (<a
href="https://doi.org/10.1007/s11222-020-09932-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the transport–transform and the relative transport–transform metrics between finite point patterns on a general space, which provide a unified framework for earlier point pattern metrics, in particular the generalized spike time and the normalized and unnormalized optimal subpattern assignment metrics. Our main focus is on barycenters, i.e., minimizers of a q-th-order Fréchet functional with respect to these metrics. We present a heuristic algorithm that terminates in a local minimum and is shown to be fast and reliable in a simulation study. The algorithm serves as a general plug-in method that can be applied to point patterns on any state space where an appropriate algorithm for solving the location problem for individual points is available. We present applications to geocoded data of crimes in Euclidean space and on a street network, illustrating that barycenters serve as informative summary statistics. Our work is a first step toward statistical inference in covariate-based models of repeated point pattern observations.},
  archive      = {J_SAC},
  author       = {Müller, Raoul and Schuhmacher, Dominic and Mateu, Jorge},
  doi          = {10.1007/s11222-020-09932-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {953-972},
  shortjournal = {Stat. Comput.},
  title        = {Metrics and barycenters for point pattern data},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational discriminant analysis with variable selection.
<em>SAC</em>, <em>30</em>(4), 933–951. (<a
href="https://doi.org/10.1007/s11222-020-09928-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fast Bayesian method that seamlessly fuses classification and hypothesis testing via discriminant analysis is developed. Building upon the original discriminant analysis classifier, modelling components are added to identify discriminative variables. A combination of cake priors and a novel form of variational Bayes we call reverse collapsed variational Bayes gives rise to variable selection that can be directly posed as a multiple hypothesis testing approach using likelihood ratio statistics. Some theoretical arguments are presented showing that Chernoff-consistency (asymptotically zero type I and type II error) is maintained across all hypotheses. We apply our method on some publicly available genomics datasets and show that our method performs well in practice for its computational cost. An R package VaDA has also been made available on Github.},
  archive      = {J_SAC},
  author       = {Yu, Weichang and Ormerod, John T. and Stewart, Michael},
  doi          = {10.1007/s11222-020-09928-8},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {933-951},
  shortjournal = {Stat. Comput.},
  title        = {Variational discriminant analysis with variable selection},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Random time step probabilistic methods for uncertainty
quantification in chaotic and geometric numerical integration.
<em>SAC</em>, <em>30</em>(4), 907–932. (<a
href="https://doi.org/10.1007/s11222-020-09926-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel probabilistic numerical method for quantifying the uncertainty induced by the time integration of ordinary differential equations (ODEs) is introduced. Departing from the classical strategy to randomise ODE solvers by adding a random forcing term, we show that a probability measure over the numerical solution of ODEs can be obtained by introducing suitable random time steps in a classical time integrator. This intrinsic randomisation allows for the conservation of geometric properties of the underlying deterministic integrator such as mass conservation, symplecticity or conservation of first integrals. Weak and mean square convergence analysis is derived. We also analyse the convergence of the Monte Carlo estimator for the proposed random time step method and show that the measure obtained with repeated sampling converges in the mean square sense independently of the number of samples. Numerical examples including chaotic Hamiltonian systems, chemical reactions and Bayesian inferential problems illustrate the accuracy, robustness and versatility of our probabilistic numerical method.},
  archive      = {J_SAC},
  author       = {Abdulle, Assyr and Garegnani, Giacomo},
  doi          = {10.1007/s11222-020-09926-w},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {907-932},
  shortjournal = {Stat. Comput.},
  title        = {Random time step probabilistic methods for uncertainty quantification in chaotic and geometric numerical integration},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monte carlo co-ordinate ascent variational inference.
<em>SAC</em>, <em>30</em>(4), 887–905. (<a
href="https://doi.org/10.1007/s11222-020-09924-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In variational inference (VI), coordinate-ascent and gradient-based approaches are two major types of algorithms for approximating difficult-to-compute probability densities. In real-world implementations of complex models, Monte Carlo methods are widely used to estimate expectations in coordinate-ascent approaches and gradients in derivative-driven ones. We discuss a Monte Carlo co-ordinate ascent VI (MC-CAVI) algorithm that makes use of Markov chain Monte Carlo (MCMC) methods in the calculation of expectations required within co-ordinate ascent VI (CAVI). We show that, under regularity conditions, an MC-CAVI recursion will get arbitrarily close to a maximiser of the evidence lower bound with any given high probability. In numerical examples, the performance of MC-CAVI algorithm is compared with that of MCMC and—as a representative of derivative-based VI methods—of Black Box VI (BBVI). We discuss and demonstrate MC-CAVI’s suitability for models with hard constraints in simulated and real examples. We compare MC-CAVI’s performance with that of MCMC in an important complex model used in nuclear magnetic resonance spectroscopy data analysis—BBVI is nearly impossible to be employed in this setting due to the hard constraints involved in the model.},
  archive      = {J_SAC},
  author       = {Ye, Lifeng and Beskos, Alexandros and De Iorio, Maria and Hao, Jie},
  doi          = {10.1007/s11222-020-09924-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {887-905},
  shortjournal = {Stat. Comput.},
  title        = {Monte carlo co-ordinate ascent variational inference},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incomplete-data fisher scoring method with steplength
adjustment. <em>SAC</em>, <em>30</em>(4), 871–886. (<a
href="https://doi.org/10.1007/s11222-020-09923-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An incomplete-data Fisher scoring method is proposed for parameter estimation in models where data are missing and in latent-variable models that can be formulated as a missing data problem. The convergence properties of the proposed method and an accelerated variant of this method are provided. The main features of this method are its ability to accelerate the rate of convergence by adjusting the steplength, to provide a second derivative of the observed-data log-likelihood function using only the functions used in the proposed method, and the ability to avoid having to explicitly solve the first derivative of the object function. Four examples are presented to demonstrate how the proposed method converges compared with the EM algorithm and its variants. The computing time is also compared.},
  archive      = {J_SAC},
  author       = {Takai, Keiji},
  doi          = {10.1007/s11222-020-09923-z},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {871-886},
  shortjournal = {Stat. Comput.},
  title        = {Incomplete-data fisher scoring method with steplength adjustment},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noncrossing structured additive multiple-output bayesian
quantile regression models. <em>SAC</em>, <em>30</em>(4), 855–869. (<a
href="https://doi.org/10.1007/s11222-020-09925-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression models are a powerful tool for studying different points of the conditional distribution of univariate response variables. Their multivariate counterpart extension though is not straightforward, starting with the definition of multivariate quantiles. We propose here a flexible Bayesian quantile regression model when the response variable is multivariate, where we are able to define a structured additive framework for all predictor variables. We build on previous ideas considering a directional approach to define the quantiles of a response variable with multiple outputs, and we define noncrossing quantiles in every directional quantile model. We define a Markov chain Monte Carlo (MCMC) procedure for model estimation, where the noncrossing property is obtained considering a Gaussian process design to model the correlation between several quantile regression models. We illustrate the results of these models using two datasets: one on dimensions of inequality in the population, such as income and health; the second on scores of students in the Brazilian High School National Exam, considering three dimensions for the response variable.},
  archive      = {J_SAC},
  author       = {Santos, Bruno and Kneib, Thomas},
  doi          = {10.1007/s11222-020-09925-x},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {855-869},
  shortjournal = {Stat. Comput.},
  title        = {Noncrossing structured additive multiple-output bayesian quantile regression models},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient bayesian shape-restricted function estimation with
constrained gaussian process priors. <em>SAC</em>, <em>30</em>(4),
839–853. (<a href="https://doi.org/10.1007/s11222-020-09922-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article revisits the problem of Bayesian shape-restricted inference in the light of a recently developed approximate Gaussian process that admits an equivalent formulation of the shape constraints in terms of the basis coefficients. We propose a strategy to efficiently sample from the resulting constrained posterior by absorbing a smooth relaxation of the constraint in the likelihood and using circulant embedding techniques to sample from the unconstrained modified prior. We additionally pay careful attention to mitigate the computational complexity arising from updating hyperparameters within the covariance kernel of the Gaussian process. The developed algorithm is shown to be accurate and highly efficient in simulated and real data examples.},
  archive      = {J_SAC},
  author       = {Ray, Pallavi and Pati, Debdeep and Bhattacharya, Anirban},
  doi          = {10.1007/s11222-020-09922-0},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {839-853},
  shortjournal = {Stat. Comput.},
  title        = {Efficient bayesian shape-restricted function estimation with constrained gaussian process priors},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting anomalies in fibre systems using 3-dimensional
image data. <em>SAC</em>, <em>30</em>(4), 817–837. (<a
href="https://doi.org/10.1007/s11222-020-09921-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of detecting anomalies in the directional distribution of fibre materials observed in 3D images. We divide the image into a set of scanning windows and classify them into two clusters: homogeneous material and anomaly. Based on a sample of estimated local fibre directions, for each scanning window we compute several classification attributes, namely the coordinate wise means of local fibre directions, the entropy of the directional distribution, and a combination of them. We also propose a new spatial modification of the Stochastic Approximation Expectation-Maximization (SAEM) algorithm. Besides the clustering we also consider testing the significance of anomalies. To this end, we apply a change point technique for random fields and derive the exact inequalities for tail probabilities of a test statistic. The proposed methodology is first validated on simulated images. Finally, it is applied to a 3D image of a fibre reinforced polymer.},
  archive      = {J_SAC},
  author       = {Dresvyanskiy, Denis and Karaseva, Tatiana and Makogin, Vitalii and Mitrofanov, Sergei and Redenbach, Claudia and Spodarev, Evgeny},
  doi          = {10.1007/s11222-020-09921-1},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {817-837},
  shortjournal = {Stat. Comput.},
  title        = {Detecting anomalies in fibre systems using 3-dimensional image data},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Properties of the bridge sampler with a focus on splitting
the MCMC sample. <em>SAC</em>, <em>30</em>(4), 799–816. (<a
href="https://doi.org/10.1007/s11222-019-09918-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation of normalizing constants is a fundamental mathematical problem in various disciplines, particularly in Bayesian model selection problems. A sampling-based technique known as bridge sampling (Meng and Wong in Stat Sin 6(4):831–860, 1996) has been found to produce accurate estimates of normalizing constants and is shown to possess good asymptotic properties. For small to moderate sample sizes (as in situations with limited computational resources), we demonstrate that the (optimal) bridge sampler produces biased estimates. Specifically, when one density (we denote as $$p_2$$) is constructed to be close to the target density (we denote as $$p_1$$) using method of moments, our simulation-based results indicate that the correlation-induced bias through the moment-matching procedure is non-negligible. More crucially, the bias amplifies as the dimensionality of the problem increases. Thus, a series of theoretical as well as empirical investigations is carried out to identify the nature and origin of the bias. We then examine the effect of sample size allocation on the accuracy of bridge sampling estimates and discovered that one possibility of reducing both the bias and standard error with a small increase in computational effort is by drawing extra samples from the moment-matched density $$p_2$$ (which we assume easy to sample from), provided that the evaluation of $$p_1$$ is not too expensive. We proceed to show how the simple adaptive approach we termed “splitting” manages to alleviate the correlation-induced bias at the expense of a higher standard error, irrespective of the dimensionality involved. We also slightly modified the strategy suggested by Wang et al. (Warp bridge sampling: the next generation, Preprint, 2019. arXiv:1609.07690) to address the issue of the increase in standard error due to splitting, which is later generalized to further improve the efficiency. We conclude the paper by offering our insights of the application of a combination of these adaptive methods to improve the accuracy of bridge sampling estimates in Bayesian applications (where posterior samples are typically expensive to generate) based on the preceding investigations, with an application to a practical example.},
  archive      = {J_SAC},
  author       = {Wong, Jackie S. T. and Forster, Jonathan J. and Smith, Peter W. F.},
  doi          = {10.1007/s11222-019-09918-5},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {799-816},
  shortjournal = {Stat. Comput.},
  title        = {Properties of the bridge sampler with a focus on splitting the MCMC sample},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible particle markov chain monte carlo method.
<em>SAC</em>, <em>30</em>(4), 783–798. (<a
href="https://doi.org/10.1007/s11222-019-09916-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle Markov Chain Monte Carlo methods are used to carry out inference in nonlinear and non-Gaussian state space models, where the posterior density of the states is approximated using particles. Current approaches usually perform Bayesian inference using either a particle marginal Metropolis–Hastings (PMMH) algorithm or a particle Gibbs (PG) sampler. This paper shows how the two ways of generating variables mentioned above can be combined in a flexible manner to give sampling schemes that converge to a desired target distribution. The advantage of our approach is that the sampling scheme can be tailored to obtain good results for different applications. For example, when some parameters and the states are highly correlated, such parameters can be generated using PMMH, while all other parameters are generated using PG because it is easier to obtain good proposals for the parameters within the PG framework. We derive some convergence properties of our sampling scheme and also investigate its performance empirically by applying it to univariate and multivariate stochastic volatility models and comparing it to other PMCMC methods proposed in the literature.},
  archive      = {J_SAC},
  author       = {Mendes, Eduardo F. and Carter, Christopher K. and Gunawan, David and Kohn, Robert},
  doi          = {10.1007/s11222-019-09916-7},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {783-798},
  shortjournal = {Stat. Comput.},
  title        = {A flexible particle markov chain monte carlo method},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functional single-index quantile regression models.
<em>SAC</em>, <em>30</em>(4), 771–781. (<a
href="https://doi.org/10.1007/s11222-019-09917-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that functional single-index regression models can achieve better prediction accuracy than functional linear models or fully nonparametric models, when the target is to predict a scalar response using a function-valued covariate. However, the performance of these models may be adversely affected by extremely large values or skewness in the response. In addition, they are not able to offer a full picture of the conditional distribution of the response. Motivated by using trajectories of $$\hbox {PM}_{{10}}$$ concentrations of last day to predict the maximum $$\hbox {PM}_{{10}}$$ concentration of the current day, a functional single-index quantile regression model is proposed to address those issues. A generalized profiling method is employed to estimate the model. Simulation studies are conducted to investigate the finite sample performance of the proposed estimator. We apply the proposed framework to predict the maximal value of $$\hbox {PM}_{{10}}$$ concentrations based on the intraday $$\hbox {PM}_{{10}}$$ concentrations of the previous day.},
  archive      = {J_SAC},
  author       = {Sang, Peijun and Cao, Jiguo},
  doi          = {10.1007/s11222-019-09917-6},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {771-781},
  shortjournal = {Stat. Comput.},
  title        = {Functional single-index quantile regression models},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new mixture model on the simplex. <em>SAC</em>,
<em>30</em>(4), 749–770. (<a
href="https://doi.org/10.1007/s11222-019-09920-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is meant to introduce a significant extension of the flexible Dirichlet (FD) distribution, which is a quite tractable special mixture model for compositional data, i.e. data representing vectors of proportions of a whole. The FD model displays several theoretical properties which make it suitable for inference, and fairly easy to handle from a computational viewpoint. However, the rigid type of mixture structure implied by the FD makes it unsuitable to describe many compositional datasets. Furthermore, the FD only allows for negative correlations. The new extended model, by considerably relaxing the strict constraints among clusters entailed by the FD, allows for a more general dependence structure (including positive correlations) and greatly expands its applicative potential. At the same time, it retains, to a large extent, its good properties. EM-type estimation procedures can be developed for this more complex model, including ad hoc reliable initialization methods, which permit to keep the computational issues at a rather uncomplicated level. Accurate evaluation of standard error estimates can be provided as well.},
  archive      = {J_SAC},
  author       = {Ongaro, Andrea and Migliorati, Sonia and Ascari, Roberto},
  doi          = {10.1007/s11222-019-09920-x},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {749-770},
  shortjournal = {Stat. Comput.},
  title        = {A new mixture model on the simplex},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mini-batch learning of exponential family finite mixture
models. <em>SAC</em>, <em>30</em>(4), 731–748. (<a
href="https://doi.org/10.1007/s11222-019-09919-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mini-batch algorithms have become increasingly popular due to the requirement for solving optimization problems, based on large-scale data sets. Using an existing online expectation–maximization (EM) algorithm framework, we demonstrate how mini-batch (MB) algorithms may be constructed, and propose a scheme for the stochastic stabilization of the constructed mini-batch algorithms. Theoretical results regarding the convergence of the mini-batch EM algorithms are presented. We then demonstrate how the mini-batch framework may be applied to conduct maximum likelihood (ML) estimation of mixtures of exponential family distributions, with emphasis on ML estimation for mixtures of normal distributions. Via a simulation study, we demonstrate that the mini-batch algorithm for mixtures of normal distributions can outperform the standard EM algorithm. Further evidence of the performance of the mini-batch framework is provided via an application to the famous MNIST data set.},
  archive      = {J_SAC},
  author       = {Nguyen, Hien D. and Forbes, Florence and McLachlan, Geoffrey J.},
  doi          = {10.1007/s11222-019-09919-4},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {731-748},
  shortjournal = {Stat. Comput.},
  title        = {Mini-batch learning of exponential family finite mixture models},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coordinate sampler: A non-reversible gibbs-like MCMC
sampler. <em>SAC</em>, <em>30</em>(3), 721–730. (<a
href="https://doi.org/10.1007/s11222-019-09913-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a novel non-reversible, continuous-time Markov chain Monte Carlo sampler, called Coordinate Sampler, based on a piecewise deterministic Markov process, which is a variant of the Zigzag sampler of Bierkens et al. (Ann Stat 47(3):1288–1320, 2019). In addition to providing a theoretical validation for this new simulation algorithm, we show that the Markov chain it induces exhibits geometrical ergodicity convergence, for distributions whose tails decay at least as fast as an exponential distribution and at most as fast as a Gaussian distribution. Several numerical examples highlight that our coordinate sampler is more efficient than the Zigzag sampler, in terms of effective sample size.},
  archive      = {J_SAC},
  author       = {Wu, Changye and Robert, Christian P.},
  doi          = {10.1007/s11222-019-09913-w},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {721-730},
  shortjournal = {Stat. Comput.},
  title        = {Coordinate sampler: A non-reversible gibbs-like MCMC sampler},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional regression in practice: An empirical study
of finite-sample prediction, variable selection and ranking.
<em>SAC</em>, <em>30</em>(3), 697–719. (<a
href="https://doi.org/10.1007/s11222-019-09914-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Penalized likelihood approaches are widely used for high-dimensional regression. Although many methods have been proposed and the associated theory is now well developed, the relative efficacy of different approaches in finite-sample settings, as encountered in practice, remains incompletely understood. There is therefore a need for empirical investigations in this area that can offer practical insight and guidance to users. In this paper, we present a large-scale comparison of penalized regression methods. We distinguish between three related goals: prediction, variable selection and variable ranking. Our results span more than 2300 data-generating scenarios, including both synthetic and semisynthetic data (real covariates and simulated responses), allowing us to systematically consider the influence of various factors (sample size, dimensionality, sparsity, signal strength and multicollinearity). We consider several widely used approaches (Lasso, Adaptive Lasso, Elastic Net, Ridge Regression, SCAD, the Dantzig Selector and Stability Selection). We find considerable variation in performance between methods. Our results support a “no panacea” view, with no unambiguous winner across all scenarios or goals, even in this restricted setting where all data align well with the assumptions underlying the methods. The study allows us to make some recommendations as to which approaches may be most (or least) suitable given the goal and some data characteristics. Our empirical results complement existing theory and provide a resource to compare methods across a range of scenarios and metrics.},
  archive      = {J_SAC},
  author       = {Wang, Fan and Mukherjee, Sach and Richardson, Sylvia and Hill, Steven M.},
  doi          = {10.1007/s11222-019-09914-9},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {697-719},
  shortjournal = {Stat. Comput.},
  title        = {High-dimensional regression in practice: An empirical study of finite-sample prediction, variable selection and ranking},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Central quantile subspace. <em>SAC</em>, <em>30</em>(3),
677–695. (<a href="https://doi.org/10.1007/s11222-019-09915-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression (QR) is becoming increasingly popular due to its relevance in many scientific investigations. There is a great amount of work about linear and nonlinear QR models. Specifically, nonparametric estimation of the conditional quantiles received particular attention, due to its model flexibility. However, nonparametric QR techniques are limited in the number of covariates. Dimension reduction offers a solution to this problem by considering low-dimensional smoothing without specifying any parametric or nonparametric regression relation. The existing dimension reduction techniques focus on the entire conditional distribution. We, on the other hand, turn our attention to dimension reduction techniques for conditional quantiles and introduce a new method for reducing the dimension of the predictor $$\mathbf {X}$$. The novelty of this paper is threefold. We start by considering a single index quantile regression model, which assumes that the conditional quantile depends on $$\mathbf {X}$$ through a single linear combination of the predictors, then extend to a multi-index quantile regression model, and finally, generalize the proposed methodology to any statistical functional of the conditional distribution. The performance of the methodology is demonstrated through simulation examples and real data applications. Our results suggest that this method has a good finite sample performance and often outperforms the existing methods.},
  archive      = {J_SAC},
  author       = {Christou, Eliana},
  doi          = {10.1007/s11222-019-09915-8},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {677-695},
  shortjournal = {Stat. Comput.},
  title        = {Central quantile subspace},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential monte carlo with transformations. <em>SAC</em>,
<em>30</em>(3), 663–676. (<a
href="https://doi.org/10.1007/s11222-019-09903-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines methodology for performing Bayesian inference sequentially on a sequence of posteriors on spaces of different dimensions. For this, we use sequential Monte Carlo samplers, introducing the innovation of using deterministic transformations to move particles effectively between target distributions with different dimensions. This approach, combined with adaptive methods, yields an extremely flexible and general algorithm for Bayesian model comparison that is suitable for use in applications where the acceptance rate in reversible jump Markov chain Monte Carlo is low. We use this approach on model comparison for mixture models, and for inferring coalescent trees sequentially, as data arrives.},
  archive      = {J_SAC},
  author       = {Everitt, Richard G. and Culliford, Richard and Medina-Aguayo, Felipe and Wilson, Daniel J.},
  doi          = {10.1007/s11222-019-09903-y},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {663-676},
  shortjournal = {Stat. Comput.},
  title        = {Sequential monte carlo with transformations},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularized estimation for highly multivariate log gaussian
cox processes. <em>SAC</em>, <em>30</em>(3), 649–662. (<a
href="https://doi.org/10.1007/s11222-019-09911-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inference for highly multivariate point pattern data is challenging due to complex models with large numbers of parameters. In this paper, we develop numerically stable and efficient parameter estimation and model selection algorithms for a class of multivariate log Gaussian Cox processes. The methodology is applied to a highly multivariate point pattern data set from tropical rain forest ecology.},
  archive      = {J_SAC},
  author       = {Choiruddin, Achmad and Cuevas-Pacheco, Francisco and Coeurjolly, Jean-François and Waagepetersen, Rasmus},
  doi          = {10.1007/s11222-019-09911-y},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {649-662},
  shortjournal = {Stat. Comput.},
  title        = {Regularized estimation for highly multivariate log gaussian cox processes},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral density-based and measure-preserving ABC for
partially observed diffusion processes. An illustration on hamiltonian
SDEs. <em>SAC</em>, <em>30</em>(3), 627–648. (<a
href="https://doi.org/10.1007/s11222-019-09909-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Bayesian computation (ABC) has become one of the major tools of likelihood-free statistical inference in complex mathematical models. Simultaneously, stochastic differential equations (SDEs) have developed to an established tool for modelling time-dependent, real-world phenomena with underlying random effects. When applying ABC to stochastic models, two major difficulties arise: First, the derivation of effective summary statistics and proper distances is particularly challenging, since simulations from the stochastic process under the same parameter configuration result in different trajectories. Second, exact simulation schemes to generate trajectories from the stochastic model are rarely available, requiring the derivation of suitable numerical methods for the synthetic data generation. To obtain summaries that are less sensitive to the intrinsic stochasticity of the model, we propose to build up the statistical method (e.g. the choice of the summary statistics) on the underlying structural properties of the model. Here, we focus on the existence of an invariant measure and we map the data to their estimated invariant density and invariant spectral density. Then, to ensure that these model properties are kept in the synthetic data generation, we adopt measure-preserving numerical splitting schemes. The derived property-based and measure-preserving ABC method is illustrated on the broad class of partially observed Hamiltonian type SDEs, both with simulated data and with real electroencephalography data. The derived summaries are particularly robust to the model simulation, and this fact, combined with the proposed reliable numerical scheme, yields accurate ABC inference. In contrast, the inference returned using standard numerical methods (Euler–Maruyama discretisation) fails. The proposed ingredients can be incorporated into any type of ABC algorithm and directly applied to all SDEs that are characterised by an invariant distribution and for which a measure-preserving numerical method can be derived.},
  archive      = {J_SAC},
  author       = {Buckwar, Evelyn and Tamborrino, Massimiliano and Tubikanec, Irene},
  doi          = {10.1007/s11222-019-09909-6},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {627-648},
  shortjournal = {Stat. Comput.},
  title        = {Spectral density-based and measure-preserving ABC for partially observed diffusion processes. an illustration on hamiltonian SDEs},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximation and sampling of multivariate probability
distributions in the tensor train decomposition. <em>SAC</em>,
<em>30</em>(3), 603–625. (<a
href="https://doi.org/10.1007/s11222-019-09910-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General multivariate distributions are notoriously expensive to sample from, particularly the high-dimensional posterior distributions in PDE-constrained inverse problems. This paper develops a sampler for arbitrary continuous multivariate distributions that is based on low-rank surrogates in the tensor train format, a methodology that has been exploited for many years for scalable, high-dimensional density function approximation in quantum physics and chemistry. We build upon recent developments of the cross approximation algorithms in linear algebra to construct a tensor train approximation to the target probability density function using a small number of function evaluations. For sufficiently smooth distributions, the storage required for accurate tensor train approximations is moderate, scaling linearly with dimension. In turn, the structure of the tensor train surrogate allows sampling by an efficient conditional distribution method since marginal distributions are computable with linear complexity in dimension. Expected values of non-smooth quantities of interest, with respect to the surrogate distribution, can be estimated using transformed independent uniformly-random seeds that provide Monte Carlo quadrature or transformed points from a quasi-Monte Carlo lattice to give more efficient quasi-Monte Carlo quadrature. Unbiased estimates may be calculated by correcting the transformed random seeds using a Metropolis–Hastings accept/reject step, while the quasi-Monte Carlo quadrature may be corrected either by a control-variate strategy or by importance weighting. We show that the error in the tensor train approximation propagates linearly into the Metropolis–Hastings rejection rate and the integrated autocorrelation time of the resulting Markov chain; thus, the integrated autocorrelation time may be made arbitrarily close to 1, implying that, asymptotic in sample size, the cost per effectively independent sample is one target density evaluation plus the cheap tensor train surrogate proposal that has linear cost with dimension. These methods are demonstrated in three computed examples: fitting failure time of shock absorbers; a PDE-constrained inverse diffusion problem; and sampling from the Rosenbrock distribution. The delayed rejection adaptive Metropolis (DRAM) algorithm is used as a benchmark. In all computed examples, the importance weight-corrected quasi-Monte Carlo quadrature performs best and is more efficient than DRAM by orders of magnitude across a wide range of approximation accuracies and sample sizes. Indeed, all the methods developed here significantly outperform DRAM in all computed examples.},
  archive      = {J_SAC},
  author       = {Dolgov, Sergey and Anaya-Izquierdo, Karim and Fox, Colin and Scheichl, Robert},
  doi          = {10.1007/s11222-019-09910-z},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {603-625},
  shortjournal = {Stat. Comput.},
  title        = {Approximation and sampling of multivariate probability distributions in the tensor train decomposition},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling from manifold-restricted distributions using
tangent bundle projections. <em>SAC</em>, <em>30</em>(3), 587–602. (<a
href="https://doi.org/10.1007/s11222-019-09907-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common problem in Bayesian inference is the sampling of target probability distributions at sufficient resolution and accuracy to estimate the probability density and to compute credible regions. Often by construction, many target distributions can be expressed as some higher-dimensional closed-form distribution with parametrically constrained variables, i.e., one that is restricted to a smooth submanifold of Euclidean space. I propose a derivative-based importance sampling framework for such distributions. A base set of n samples from the target distribution is used to map out the tangent bundle of the manifold, and to seed nm additional points that are projected onto the tangent bundle and weighted appropriately. The method essentially acts as an upsampling complement to any standard algorithm. It is designed for the efficient production of approximate high-resolution histograms from manifold-restricted Gaussian distributions and can provide large computational savings when sampling directly from the target distribution is expensive.},
  archive      = {J_SAC},
  author       = {Chua, Alvin J. K.},
  doi          = {10.1007/s11222-019-09907-8},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {587-602},
  shortjournal = {Stat. Comput.},
  title        = {Sampling from manifold-restricted distributions using tangent bundle projections},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal allocation of monte carlo simulations to multiple
hypothesis tests. <em>SAC</em>, <em>30</em>(3), 571–586. (<a
href="https://doi.org/10.1007/s11222-019-09906-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple hypothesis tests are often carried out in practice using p-value estimates obtained with bootstrap or permutation tests since the analytical p-values underlying all hypotheses are usually unknown. This article considers the allocation of a pre-specified total number of Monte Carlo simulations $$K \in \mathbb {N}$$ (i.e., permutations or draws from a bootstrap distribution) to a given number of $$m \in \mathbb {N}$$ hypotheses in order to approximate their p-values $$p \in [0,1]^m$$ in an optimal way, in the sense that the allocation minimises the total expected number of misclassified hypotheses. A misclassification occurs if a decision on a single hypothesis, obtained with an approximated p-value, differs from the one obtained if its p-value was known analytically. The contribution of this article is threefold: under the assumption that p is known and $$K \in \mathbb {R}$$, and using a normal approximation of the Binomial distribution, the optimal real-valued allocation of K simulations to m hypotheses is derived when correcting for multiplicity with the Bonferroni correction, both when computing the p-value estimates with or without a pseudo-count. Computational subtleties arising in the former case will be discussed. Second, with the help of an algorithm based on simulated annealing, empirical evidence is given that the optimal integer allocation is likely of the same form as the optimal real-valued allocation, and that both seem to coincide asympotically. Third, an empirical study on simulated and real data demonstrates that a recently proposed sampling algorithm based on Thompson sampling asympotically mimics the optimal (real-valued) allocation when the p-values are unknown and thus estimated at runtime.},
  archive      = {J_SAC},
  author       = {Hahn, Georg},
  doi          = {10.1007/s11222-019-09906-9},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {571-586},
  shortjournal = {Stat. Comput.},
  title        = {Optimal allocation of monte carlo simulations to multiple hypothesis tests},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local dimension reduction of summary statistics for
likelihood-free inference. <em>SAC</em>, <em>30</em>(3), 559–570. (<a
href="https://doi.org/10.1007/s11222-019-09905-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Bayesian computation (ABC) and other likelihood-free inference methods have gained popularity in the last decade, as they allow rigorous statistical inference for complex models without analytically tractable likelihood functions. A key component for accurate inference with ABC is the choice of summary statistics, which summarize the information in the data, but at the same time should be low-dimensional for efficiency. Several dimension reduction techniques have been introduced to automatically construct informative and low-dimensional summaries from a possibly large pool of candidate summaries. Projection-based methods, which are based on learning simple functional relationships from the summaries to parameters, are widely used and usually perform well, but might fail when the assumptions behind the transformation are not satisfied. We introduce a localization strategy for any projection-based dimension reduction method, in which the transformation is estimated in the neighborhood of the observed data instead of the whole space. Localization strategies have been suggested before, but the performance of the transformed summaries outside the local neighborhood has not been guaranteed. In our localization approach the transformation is validated and optimized over validation datasets, ensuring reliable performance. We demonstrate the improvement in the estimation accuracy for localized versions of linear regression and partial least squares, for three different models of varying complexity.},
  archive      = {J_SAC},
  author       = {Sirén, Jukka and Kaski, Samuel},
  doi          = {10.1007/s11222-019-09905-w},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {559-570},
  shortjournal = {Stat. Comput.},
  title        = {Local dimension reduction of summary statistics for likelihood-free inference},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust bayesian synthetic likelihood via a semi-parametric
approach. <em>SAC</em>, <em>30</em>(3), 543–557. (<a
href="https://doi.org/10.1007/s11222-019-09904-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian synthetic likelihood (BSL) is now a well-established method for performing approximate Bayesian parameter estimation for simulation-based models that do not possess a tractable likelihood function. BSL approximates an intractable likelihood function of a carefully chosen summary statistic at a parameter value with a multivariate normal distribution. The mean and covariance matrix of this normal distribution are estimated from independent simulations of the model. Due to the parametric assumption implicit in BSL, it can be preferred to its nonparametric competitor, approximate Bayesian computation, in certain applications where a high-dimensional summary statistic is of interest. However, despite several successful applications of BSL, its widespread use in scientific fields may be hindered by the strong normality assumption. In this paper, we develop a semi-parametric approach to relax this assumption to an extent and maintain the computational advantages of BSL without any additional tuning. We test our new method, semiBSL, on several challenging examples involving simulated and real data and demonstrate that semiBSL can be significantly more robust than BSL and another approach in the literature.},
  archive      = {J_SAC},
  author       = {An, Ziwen and Nott, David J. and Drovandi, Christopher},
  doi          = {10.1007/s11222-019-09904-x},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {543-557},
  shortjournal = {Stat. Comput.},
  title        = {Robust bayesian synthetic likelihood via a semi-parametric approach},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven stochastic inversion via functional
quantization. <em>SAC</em>, <em>30</em>(3), 525–541. (<a
href="https://doi.org/10.1007/s11222-019-09888-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new methodology for solving stochastic inversion problems through computer experiments, the stochasticity being driven by a functional random variables. This study is motivated by an automotive application. In this context, the simulator code takes a double set of simulation inputs: deterministic control variables and functional uncertain variables. This framework is characterized by two features. The first one is the high computational cost of simulations. The second is that the probability distribution of the functional input is only known through a finite set of realizations. In our context, the inversion problem is formulated by considering the expectation over the functional random variable. We aim at solving this problem by evaluating the model on a design, whose adaptive construction combines the so-called stepwise uncertainty reduction methodology with a strategy for an efficient expectation estimation. Two greedy strategies are introduced to sequentially estimate the expectation over the functional uncertain variable by adaptively selecting curves from the initial set of realizations. Both of these strategies consider functional principal component analysis as a dimensionality reduction technique assuming that the realizations of the functional input are independent realizations of the same continuous stochastic process. The first strategy is based on a greedy approach for functional data-driven quantization, while the second one is linked to the notion of space-filling design. Functional PCA is used as an intermediate step. For each point of the design built in the reduced space, we select the corresponding curve from the sample of available curves, thus guaranteeing the robustness of the procedure to dimension reduction. The whole methodology is illustrated and calibrated on an analytical example. It is then applied on the automotive industrial test case where we aim at identifying the set of control parameters leading to meet the pollutant emission standards of a vehicle.},
  archive      = {J_SAC},
  author       = {El Amri, Mohamed Reda and Helbert, Céline and Lepreux, Olivier and Zuniga, Miguel Munoz and Prieur, Clémentine and Sinoquet, Delphine},
  doi          = {10.1007/s11222-019-09888-8},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {525-541},
  shortjournal = {Stat. Comput.},
  title        = {Data-driven stochastic inversion via functional quantization},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level monte carlo methods for the approximation of
invariant measures of stochastic differential equations. <em>SAC</em>,
<em>30</em>(3), 507–524. (<a
href="https://doi.org/10.1007/s11222-019-09890-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a framework that allows the use of the multi-level Monte Carlo (MLMC) methodology (Giles in Acta Numer. 24:259–328, 2015. https://doi.org/10.1017/S096249291500001X) to calculate expectations with respect to the invariant measure of an ergodic SDE. In that context, we study the (over-damped) Langevin equations with a strongly concave potential. We show that when appropriate contracting couplings for the numerical integrators are available, one can obtain a uniform-in-time estimate of the MLMC variance in contrast to the majority of the results in the MLMC literature. As a consequence, a root mean square error of $$\mathcal {O}(\varepsilon )$$ is achieved with $$\mathcal {O}(\varepsilon ^{-2})$$ complexity on par with Markov Chain Monte Carlo (MCMC) methods, which, however, can be computationally intensive when applied to large datasets. Finally, we present a multi-level version of the recently introduced stochastic gradient Langevin dynamics method (Welling and Teh, in: Proceedings of the 28th ICML, 2011) built for large datasets applications. We show that this is the first stochastic gradient MCMC method with complexity $$\mathcal {O}(\varepsilon ^{-2}|\log {\varepsilon }|^{3})$$, in contrast to the complexity $$\mathcal {O}(\varepsilon ^{-3})$$ of currently available methods. Numerical experiments confirm our theoretical findings.},
  archive      = {J_SAC},
  author       = {Giles, Michael B. and Majka, Mateusz B. and Szpruch, Lukasz and Vollmer, Sebastian J. and Zygalakis, Konstantinos C.},
  doi          = {10.1007/s11222-019-09890-0},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {507-524},
  shortjournal = {Stat. Comput.},
  title        = {Multi-level monte carlo methods for the approximation of invariant measures of stochastic differential equations},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering multivariate data using factor analytic bayesian
mixtures with an unknown number of components. <em>SAC</em>,
<em>30</em>(3), 485–506. (<a
href="https://doi.org/10.1007/s11222-019-09891-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work on overfitting Bayesian mixtures of distributions offers a powerful framework for clustering multivariate data using a latent Gaussian model which resembles the factor analysis model. The flexibility provided by overfitting mixture models yields a simple and efficient way in order to estimate the unknown number of clusters and model parameters by Markov chain Monte Carlo sampling. The present study extends this approach by considering a set of eight parameterizations, giving rise to parsimonious representations of the covariance matrix per cluster. A Gibbs sampler combined with a prior parallel tempering scheme is implemented in order to approximately sample from the posterior distribution of the overfitting mixture. The parameterization and number of factors are selected according to the Bayesian information criterion. Identifiability issues related to label switching are dealt by post-processing the simulated output with the Equivalence Classes Representatives algorithm. The contributed method and software are demonstrated and compared to similar models estimated using the expectation–maximization algorithm on simulated and real datasets. The software is available online at https://CRAN.R-project.org/package=fabMix.},
  archive      = {J_SAC},
  author       = {Papastamoulis, Panagiotis},
  doi          = {10.1007/s11222-019-09891-z},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {485-506},
  shortjournal = {Stat. Comput.},
  title        = {Clustering multivariate data using factor analytic bayesian mixtures with an unknown number of components},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of relative risk for events on a linear network.
<em>SAC</em>, <em>30</em>(2), 469–484. (<a
href="https://doi.org/10.1007/s11222-019-09889-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the study of traffic accidents on a road network, we discuss the estimation of the relative risk, the ratio of rates of occurrence of different types of events occurring on a network of lines. Methods developed for two-dimensional spatial point patterns can be adapted to a linear network, but their requirements and performance are very different on a network. Computation is slow and we introduce new techniques to accelerate it. Intensities (occurrence rates) are estimated by kernel smoothing using the heat kernel on the network. The main methodological problem is bandwidth selection. Binary regression methods, such as likelihood cross-validation and least squares cross-validation, perform tolerably well in our simulation experiments, but the Kelsall–Diggle density-ratio cross-validation method does not. We find a theoretical explanation, and propose a modification of the Kelsall–Diggle method which has better performance. The methods are applied to traffic accidents in a regional city, and to protrusions on the dendritic tree of a neuron.},
  archive      = {J_SAC},
  author       = {McSwiggan, Greg and Baddeley, Adrian and Nair, Gopalan},
  doi          = {10.1007/s11222-019-09889-7},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {469-484},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of relative risk for events on a linear network},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric estimation of probabilistic sensitivity
measures. <em>SAC</em>, <em>30</em>(2), 447–467. (<a
href="https://doi.org/10.1007/s11222-019-09887-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer experiments are becoming increasingly important in scientific investigations. In the presence of uncertainty, analysts employ probabilistic sensitivity methods to identify the key-drivers of change in the quantities of interest. Simulation complexity, large dimensionality and long running times may force analysts to make statistical inference at small sample sizes. Methods designed to estimate probabilistic sensitivity measures at relatively low computational costs are attracting increasing interest. We first, propose new estimators based on a one-sample design and building on the idea of placing piecewise constant Bayesian priors on the conditional distributions of the output given each input, after partitioning the input space. We then present two alternatives, based on Bayesian non-parametric density estimation, which bypass the need for predefined partitions. Quantification of uncertainty in the estimation process through is possible without requiring additional simulator evaluations via Bootstrap in the simplest proposal, or from the posterior distribution over the sensitivity measures, when the entire inferential procedure is Bayesian. The performance of the proposed methods is compared to that of traditional point estimators in a series of numerical experiments comprising synthetic but challenging simulators, as well as a realistic application.},
  archive      = {J_SAC},
  author       = {Antoniano-Villalobos, Isadora and Borgonovo, Emanuele and Lu, Xuefei},
  doi          = {10.1007/s11222-019-09887-9},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {447-467},
  shortjournal = {Stat. Comput.},
  title        = {Nonparametric estimation of probabilistic sensitivity measures},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hilbert space methods for reduced-rank gaussian process
regression. <em>SAC</em>, <em>30</em>(2), 419–446. (<a
href="https://doi.org/10.1007/s11222-019-09886-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel scheme for reduced-rank Gaussian process regression. The method is based on an approximate series expansion of the covariance function in terms of an eigenfunction expansion of the Laplace operator in a compact subset of $$\mathbb {R}^d$$. On this approximate eigenbasis, the eigenvalues of the covariance function can be expressed as simple functions of the spectral density of the Gaussian process, which allows the GP inference to be solved under a computational cost scaling as $$\mathcal {O}(nm^2)$$ (initial) and $$\mathcal {O}(m^3)$$ (hyperparameter learning) with m basis functions and n data points. Furthermore, the basis functions are independent of the parameters of the covariance function, which allows for very fast hyperparameter learning. The approach also allows for rigorous error analysis with Hilbert space theory, and we show that the approximation becomes exact when the size of the compact subset and the number of eigenfunctions go to infinity. We also show that the convergence rate of the truncation error is independent of the input dimensionality provided that the differentiability order of the covariance function increases appropriately, and for the squared exponential covariance function it is always bounded by $${\sim }1/m$$ regardless of the input dimensionality. The expansion generalizes to Hilbert spaces with an inner product which is defined as an integral over a specified input density. The method is compared to previously proposed methods theoretically and through empirical tests with simulated and real data.},
  archive      = {J_SAC},
  author       = {Solin, Arno and Särkkä, Simo},
  doi          = {10.1007/s11222-019-09886-w},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {419-446},
  shortjournal = {Stat. Comput.},
  title        = {Hilbert space methods for reduced-rank gaussian process regression},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Timing observations of diffusions. <em>SAC</em>,
<em>30</em>(2), 405–417. (<a
href="https://doi.org/10.1007/s11222-019-09883-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses when to observe systems that evolve over time. Many processes in biology and chemistry have been modeled by nonlinear stochastic differential equations (SDEs). Obtaining measurements from these systems can be expensive, so experimenters may wish to choose observation times that are as informative as possible. In this paper, we consider Itô diffusions (a class of SDEs) specified by some $$\theta \in {\mathbb {R}}$$, and we assume that we are permitted to observe their sample paths only n times before a terminal time $$\tau &lt; \infty $$. We propose a policy for timing these observations in order to optimally estimate $$\theta $$. Our policy maximizes the expected Fisher information for $$\theta $$ carried by the observations, and it is adaptive, meaning the policy uses data obtained from earlier observations. In numerical studies, this design reduces the variation of estimated parameters by as much as 75\% relative to observations spaced uniformly in time. Our policy depends on the value of the parameter being estimated, so we also discuss strategies for incorporating Bayesian priors over $$\theta $$. These methods are illustrated by example problems from pharmacokinetics and experimental ecology.},
  archive      = {J_SAC},
  author       = {Javeed, Aurya and Hooker, Giles},
  doi          = {10.1007/s11222-019-09883-z},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {405-417},
  shortjournal = {Stat. Comput.},
  title        = {Timing observations of diffusions},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified hamiltonian monte carlo for bayesian inference.
<em>SAC</em>, <em>30</em>(2), 377–404. (<a
href="https://doi.org/10.1007/s11222-019-09885-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hamiltonian Monte Carlo (HMC) method has been recognized as a powerful sampling tool in computational statistics. We show that performance of HMC can be significantly improved by incorporating importance sampling and an irreversible part of the dynamics into a chain. This is achieved by replacing Hamiltonians in the Metropolis test with modified Hamiltonians and a complete momentum update with a partial momentum refreshment. We call the resulting generalized HMC importance sampler Mix &amp; Match Hamiltonian Monte Carlo (MMHMC). The method is irreversible by construction and further benefits from (i) the efficient algorithms for computation of modified Hamiltonians; (ii) the implicit momentum update procedure and (iii) the multistage splitting integrators specially derived for the methods sampling with modified Hamiltonians. MMHMC has been implemented, tested on the popular statistical models and compared in sampling efficiency with HMC, Riemann Manifold Hamiltonian Monte Carlo, Generalized Hybrid Monte Carlo, Generalized Shadow Hybrid Monte Carlo, Metropolis Adjusted Langevin Algorithm and Random Walk Metropolis–Hastings. To make a fair comparison, we propose a metric that accounts for correlations among samples and weights and can be readily used for all methods which generate such samples. The experiments reveal the superiority of MMHMC over popular sampling techniques, especially in solving high-dimensional problems.},
  archive      = {J_SAC},
  author       = {Radivojević, Tijana and Akhmatskaya, Elena},
  doi          = {10.1007/s11222-019-09885-x},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {377-404},
  shortjournal = {Stat. Comput.},
  title        = {Modified hamiltonian monte carlo for bayesian inference},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust bayesian model selection for variable clustering with
the gaussian graphical model. <em>SAC</em>, <em>30</em>(2), 351–376. (<a
href="https://doi.org/10.1007/s11222-019-09879-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable clustering is important for explanatory analysis. However, only few dedicated methods for variable clustering with the Gaussian graphical model have been proposed. Even more severe, small insignificant partial correlations due to noise can dramatically change the clustering result when evaluating for example with the Bayesian information criteria (BIC). In this work, we try to address this issue by proposing a Bayesian model that accounts for negligible small, but not necessarily zero, partial correlations. Based on our model, we propose to evaluate a variable clustering result using the marginal likelihood. To address the intractable calculation of the marginal likelihood, we propose two solutions: one based on a variational approximation and another based on MCMC. Experiments on simulated data show that the proposed method is similarly accurate as BIC in the no noise setting, but considerably more accurate when there are noisy partial correlations. Furthermore, on real data the proposed method provides clustering results that are intuitively sensible, which is not always the case when using BIC or its extensions.},
  archive      = {J_SAC},
  author       = {Andrade, Daniel and Takeda, Akiko and Fukumizu, Kenji},
  doi          = {10.1007/s11222-019-09879-9},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {351-376},
  shortjournal = {Stat. Comput.},
  title        = {Robust bayesian model selection for variable clustering with the gaussian graphical model},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Marginal information for structure learning. <em>SAC</em>,
<em>30</em>(2), 331–349. (<a
href="https://doi.org/10.1007/s11222-019-09877-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structure learning for Bayesian networks has been made in a heuristic mode in search of an optimal model to avoid an explosive computational burden. In the learning process, a structural error which occurred at a point of learning may deteriorate its subsequent learning. We proposed a remedial approach to this error-for-error process by using marginal model structures. The remedy is made by fixing local errors in structure in reference to the marginal structures. In this sense, we call the remedy a marginally corrective procedure. We devised a new score function for the procedure which consists of two components, the likelihood function of a model and a discrepancy measure in marginal structures. The proposed method compares favourably with a couple of the most popular algorithms as shown in experiments with benchmark data sets.},
  archive      = {J_SAC},
  author       = {Kim, Gang-Hoo and Kim, Sung-Ho},
  doi          = {10.1007/s11222-019-09877-x},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {331-349},
  shortjournal = {Stat. Comput.},
  title        = {Marginal information for structure learning},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nudging the particle filter. <em>SAC</em>, <em>30</em>(2),
305–330. (<a href="https://doi.org/10.1007/s11222-019-09884-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a new sampling scheme aimed at improving the performance of particle filters whenever (a) there is a significant mismatch between the assumed model dynamics and the actual system, or (b) the posterior probability tends to concentrate in relatively small regions of the state space. The proposed scheme pushes some particles toward specific regions where the likelihood is expected to be high, an operation known as nudging in the geophysics literature. We reinterpret nudging in a form applicable to any particle filtering scheme, as it does not involve any changes in the rest of the algorithm. Since the particles are modified, but the importance weights do not account for this modification, the use of nudging leads to additional bias in the resulting estimators. However, we prove analytically that nudged particle filters can still attain asymptotic convergence with the same error rates as conventional particle methods. Simple analysis also yields an alternative interpretation of the nudging operation that explains its robustness to model errors. Finally, we show numerical results that illustrate the improvements that can be attained using the proposed scheme. In particular, we present nonlinear tracking examples with synthetic data and a model inference example using real-world financial data.},
  archive      = {J_SAC},
  author       = {Akyildiz, Ömer Deniz and Míguez, Joaquín},
  doi          = {10.1007/s11222-019-09884-y},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {305-330},
  shortjournal = {Stat. Comput.},
  title        = {Nudging the particle filter},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MCEN: A method of simultaneous variable selection and
clustering for high-dimensional multinomial regression. <em>SAC</em>,
<em>30</em>(2), 291–304. (<a
href="https://doi.org/10.1007/s11222-019-09880-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multinomial regression is often used to investigate the association between potential independent variables and multi-class nominal responses such as multiple disease subtypes. However, it cannot identify groups of variables that have similar effects on predicting the same subtypes of diseases, which is an important problem in biomedical research. Clustering variables in this problem is not trivial, since correlated variables may have distinct predictive effects on the multi-class nominal responses. For example, a group of moderately to highly correlated expressed genes may be associated with different subtypes of a disease. This paper presents a new data-driven simultaneous variable selection and clustering method for high-dimensional multinomial regression. By using a novel penalty function that incorporates both regression coefficients and pairwise correlation to define clusters of variables, the proposed method provides a one-stop solution to select and group important variables associated with different classes of multinomial response at the same time. An alternating minimization algorithm is developed to solve the resulting optimizing problem, which incorporates both convex optimization and clustering steps. The proposed method is compared with the state of the art in terms of prediction and variable clustering performance through extensive simulation studies. In addition, three real data examples are presented to demonstrate how to apply our method and further verify the findings in our simulation studies. The results of simulation and real data studies also shed light on the strength and weakness of several different penalized regression methods with respect to variable clustering and prediction in different scenarios.},
  archive      = {J_SAC},
  author       = {Ren, Sheng and Kang, Emily L. and Lu, Jason L.},
  doi          = {10.1007/s11222-019-09880-2},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {291-304},
  shortjournal = {Stat. Comput.},
  title        = {MCEN: A method of simultaneous variable selection and clustering for high-dimensional multinomial regression},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for <span
class="math display"><em>L</em><sub>2</sub></span>-boosting.
<em>SAC</em>, <em>30</em>(2), 279–289. (<a
href="https://doi.org/10.1007/s11222-019-09882-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a statistical inference framework for the component-wise functional gradient descent algorithm (CFGD) under normality assumption for model errors, also known as $$L_2$$-Boosting. The CFGD is one of the most versatile tools to analyze data, because it scales well to high-dimensional data sets, allows for a very flexible definition of additive regression models and incorporates inbuilt variable selection. Due to the variable selection, we build on recent proposals for post-selection inference. However, the iterative nature of component-wise boosting, which can repeatedly select the same component to update, necessitates adaptations and extensions to existing approaches. We propose tests and confidence intervals for linear, grouped and penalized additive model components selected by $$L_2$$-Boosting. Our concepts also transfer to slow-learning algorithms more generally, and to other selection techniques which restrict the response space to more complex sets than polyhedra. We apply our framework to an additive model for sales prices of residential apartments and investigate the properties of our concepts in simulation studies.},
  archive      = {J_SAC},
  author       = {Rügamer, David and Greven, Sonja},
  doi          = {10.1007/s11222-019-09882-0},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {279-289},
  shortjournal = {Stat. Comput.},
  title        = {Inference for $$L_2$$-boosting},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted likelihood mixture modeling and model-based
clustering. <em>SAC</em>, <em>30</em>(2), 255–277. (<a
href="https://doi.org/10.1007/s11222-019-09881-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A weighted likelihood approach for robust fitting of a mixture of multivariate Gaussian components is developed in this work. Two approaches have been proposed that are driven by a suitable modification of the standard EM and CEM algorithms, respectively. In both techniques, the M-step is enhanced by the computation of weights aimed at downweighting outliers. The weights are based on Pearson residuals stemming from robust Mahalanobis-type distances. Formal rules for robust clustering and outlier detection can be also defined based on the fitted mixture model. The behavior of the proposed methodologies has been investigated by numerical studies and real data examples in terms of both fitting and classification accuracy and outlier detection.},
  archive      = {J_SAC},
  author       = {Greco, Luca and Agostinelli, Claudio},
  doi          = {10.1007/s11222-019-09881-1},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {255-277},
  shortjournal = {Stat. Comput.},
  title        = {Weighted likelihood mixture modeling and model-based clustering},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inverse regression for ridge recovery: A data-driven
approach for parameter reduction in computer experiments. <em>SAC</em>,
<em>30</em>(2), 237–253. (<a
href="https://doi.org/10.1007/s11222-019-09876-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter reduction can enable otherwise infeasible design and uncertainty studies with modern computational science models that contain several input parameters. In statistical regression, techniques for sufficient dimension reduction (SDR) use data to reduce the predictor dimension of a regression problem. A computational scientist hoping to use SDR for parameter reduction encounters a problem: a computer prediction is best represented by a deterministic function of the inputs, so data comprised of computer simulation queries fail to satisfy the SDR assumptions. To address this problem, we interpret SDR methods sliced inverse regression (SIR) and sliced average variance estimation (SAVE) as estimating the directions of a ridge function, which is a composition of a low-dimensional linear transformation with a nonlinear function. Within this interpretation, SIR and SAVE estimate matrices of integrals whose column spaces are contained in the ridge directions’ span; we analyze and numerically verify convergence of these column spaces as the number of computer model queries increases. Moreover, we show example functions that are not ridge functions but whose inverse conditional moment matrices are low-rank. Consequently, the computational scientist should beware when using SIR and SAVE for parameter reduction, since SIR and SAVE may mistakenly suggest that truly important directions are unimportant.},
  archive      = {J_SAC},
  author       = {Glaws, Andrew and Constantine, Paul G. and Cook, R. Dennis},
  doi          = {10.1007/s11222-019-09876-y},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {237-253},
  shortjournal = {Stat. Comput.},
  title        = {Inverse regression for ridge recovery: A data-driven approach for parameter reduction in computer experiments},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric bivariate modelling with flexible extremal
dependence. <em>SAC</em>, <em>30</em>(2), 221–236. (<a
href="https://doi.org/10.1007/s11222-019-09878-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference over multivariate tails often requires a number of assumptions which may affect the assessment of the extreme dependence structure. Models are usually constructed in such a way that extreme components can either be asymptotically dependent or be independent of each other. Recently, there has been an increasing interest on modelling multivariate extremes more flexibly, by allowing models to bridge both asymptotic dependence regimes. Here we propose a novel semiparametric approach which allows for a variety of dependence patterns, be them extremal or not, by using in a model-based fashion the full dataset. We build on previous work for inference on marginal exceedances over a high, unknown threshold, by combining it with flexible, semiparametric copula specifications to investigate extreme dependence, thus separately modelling marginals and dependence structure. Because of the generality of our approach, bivariate problems are investigated here due to computational challenges, but multivariate extensions are readily available. Empirical results suggest that our approach can provide sound uncertainty statements about the possibility of asymptotic independence, and we propose a criterion to quantify the presence of either extreme regime which performs well in our applications when compared to others available. Estimation of functions of interest for extremes is performed via MCMC algorithms. Attention is also devoted to the prediction of new extreme observations. Our approach is evaluated through simulations, applied to real data and assessed against competing approaches. Evidence demonstrates that the bulk of the data do not bias and improve the inferential process for extremal dependence in our applications.},
  archive      = {J_SAC},
  author       = {Leonelli, Manuele and Gamerman, Dani},
  doi          = {10.1007/s11222-019-09878-w},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {221-236},
  shortjournal = {Stat. Comput.},
  title        = {Semiparametric bivariate modelling with flexible extremal dependence},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric self-exciting models for computer network
traffic. <em>SAC</em>, <em>30</em>(2), 209–220. (<a
href="https://doi.org/10.1007/s11222-019-09875-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connectivity patterns between nodes in a computer network can be interpreted and modelled as point processes where events in a process indicate connections being established for data to be sent along that edge. A model of normal connectivity behaviour can be constructed for each edge in a network by identifying key network user features such as seasonality or self-exciting behaviour, since events typically arise in bursts at particular times of day which may be peculiar to that edge. When monitoring a computer network in real time, unusual patterns of activity against the model of normality could indicate the presence of a malicious actor. A flexible, novel, nonparametric model for the excitation function of a Wold process is proposed for modelling the conditional intensities of network edges. This approach is shown to outperform standard seasonality and self-excitation models in predicting network connections, achieving well-calibrated predictions for event data collected from the computer networks of both Imperial College and Los Alamos National Laboratory.},
  archive      = {J_SAC},
  author       = {Price-Williams, Matthew and Heard, Nicholas A.},
  doi          = {10.1007/s11222-019-09875-z},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {209-220},
  shortjournal = {Stat. Comput.},
  title        = {Nonparametric self-exciting models for computer network traffic},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A breakpoint detection in the mean model with heterogeneous
variance on fixed time intervals. <em>SAC</em>, <em>30</em>(1), 195–207.
(<a href="https://doi.org/10.1007/s11222-019-09853-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is motivated by an application for the homogenization of global navigation satellite system (GNSS)-derived integrated water vapour series. Indeed, these series are affected by abrupt changes due to equipment changes or environmental effects. The detection and correction of the series from these changes are a crucial step before any use for climate studies. In addition to these abrupt changes, it has been observed in the series a non-stationary of the variability. We propose in this paper a new segmentation model that is a breakpoint detection in the mean model of a Gaussian process with heterogeneous variance on known time intervals. In this segmentation case, the dynamic programming algorithm used classically to infer the breakpoints cannot be applied anymore. We propose a procedure in two steps: we first estimate robustly the variances and then apply the classical inference by plugging these estimators. The performance of our proposed procedure is assessed through simulation experiments. An application to real GNSS data is presented.},
  archive      = {J_SAC},
  author       = {Bock, Olivier and Collilieux, Xavier and Guillamon, François and Lebarbier, Emilie and Pascal, Claire},
  doi          = {10.1007/s11222-019-09853-5},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {195-207},
  shortjournal = {Stat. Comput.},
  title        = {A breakpoint detection in the mean model with heterogeneous variance on fixed time intervals},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and exact simulation of gaussian random fields defined
on the sphere cross time. <em>SAC</em>, <em>30</em>(1), 187–194. (<a
href="https://doi.org/10.1007/s11222-019-09873-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a method for fast and exact simulation of Gaussian random fields on the sphere having isotropic covariance functions. The method proposed is then extended to Gaussian random fields defined over the sphere cross time and having covariance functions that depend on geodesic distance in space and on temporal separation. The crux of the method is in the use of block circulant matrices obtained working on regular grids defined over longitude and latitude.},
  archive      = {J_SAC},
  author       = {Cuevas, Francisco and Allard, Denis and Porcu, Emilio},
  doi          = {10.1007/s11222-019-09873-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {187-194},
  shortjournal = {Stat. Comput.},
  title        = {Fast and exact simulation of gaussian random fields defined on the sphere cross time},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of long-range dependence in gappy gaussian time
series. <em>SAC</em>, <em>30</em>(1), 167–185. (<a
href="https://doi.org/10.1007/s11222-019-09874-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge of the long-range dependence (LRD) parameter is critical to studies of self-similar behavior. However, statistical estimation of the LRD parameter becomes difficult when the observed data are masked by short-range dependence and other noises or are gappy in nature (i.e., some values are missing in an otherwise regular sampling). Currently there is a lack of theory for spectral- and wavelet-based estimators of the LRD parameter for gappy data. To address this, we estimate the LRD parameter for gappy Gaussian semiparametric time series based upon undecimated wavelet variances. We develop estimation methods by using novel estimators of the wavelet variances, providing asymptotic theory for the joint distribution of the wavelet variances and our estimator of the LRD parameter. We introduce sandwich estimators to compute standard errors for our estimates. We demonstrate the efficacy of our methods using Monte Carlo simulations and provide guidance on practical issues such as how to select the range of wavelet scales. We demonstrate the methodology using two applications: one for gappy Arctic sea-ice draft data and another for gap-free and gappy daily average temperature data collected at 17 locations in south central Sweden.},
  archive      = {J_SAC},
  author       = {Craigmile, Peter F. and Mondal, Debashis},
  doi          = {10.1007/s11222-019-09874-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {167-185},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of long-range dependence in gappy gaussian time series},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spherical regression models with general covariates and
anisotropic errors. <em>SAC</em>, <em>30</em>(1), 153–165. (<a
href="https://doi.org/10.1007/s11222-019-09872-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing parametric regression models in the literature for response data on the unit sphere assume that the covariates have particularly simple structure, for example that they are either scalar or are themselves on the unit sphere, and/or that the error distribution is isotropic. In many practical situations, such models are too inflexible. Here, we develop richer parametric spherical regression models in which the covariates can have quite general structure (for example, they may be on the unit sphere, in Euclidean space, categorical or some combination of these) and in which the errors are anisotropic. We consider two anisotropic error distributions—the Kent distribution and the elliptically symmetric angular Gaussian distribution—and two parametrisations of each which enable distinct ways to model how the response depends on the covariates. Various hypotheses of interest, such as the significance of particular covariates, or anisotropy of the errors, are easy to test, for example by classical likelihood ratio tests. We also introduce new model-based residuals for evaluating the fitted models. In the examples we consider, the hypothesis tests indicate strong evidence to favour the novel models over simpler existing ones.},
  archive      = {J_SAC},
  author       = {Paine, P. J. and Preston, S. P. and Tsagris, M. and Wood, Andrew T. A.},
  doi          = {10.1007/s11222-019-09872-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {153-165},
  shortjournal = {Stat. Comput.},
  title        = {Spherical regression models with general covariates and anisotropic errors},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transformation boosting machines. <em>SAC</em>,
<em>30</em>(1), 141–152. (<a
href="https://doi.org/10.1007/s11222-019-09870-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The broad class of conditional transformation models includes interpretable and simple as well as potentially very complex models for conditional distributions. This makes conditional transformation models attractive for predictive distribution modelling, especially because models featuring interpretable parameters and black-box machines can be understood as extremes in a whole cascade of models. So far, algorithms and corresponding theory was developed for special forms of conditional transformation models only: maximum likelihood inference is available for rather simple models, there exists a tailored boosting algorithm for the estimation of additive conditional transformation models, and a special form of random forests targets the estimation of interaction models. Here, I propose boosting algorithms capable of estimating conditional transformation models of arbitrary complexity, starting from simple shift transformation models featuring linear predictors to essentially unstructured conditional transformation models allowing complex nonlinear interaction functions. A generic form of the likelihood is maximized. Thus, the novel boosting algorithms for conditional transformation models are applicable to all types of univariate response variables, including randomly censored or truncated observations.},
  archive      = {J_SAC},
  author       = {Hothorn, Torsten},
  doi          = {10.1007/s11222-019-09870-4},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {141-152},
  shortjournal = {Stat. Comput.},
  title        = {Transformation boosting machines},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear mixed-effects scalar-on-function models and
variable selection. <em>SAC</em>, <em>30</em>(1), 129–140. (<a
href="https://doi.org/10.1007/s11222-019-09871-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by our collaborative research and the aim is to model clinical assessments of upper limb function after stroke using 3D-position and 4D-orientation movement data. We present a new nonlinear mixed-effects scalar-on-function regression model with a Gaussian process prior focusing on the variable selection from a large number of candidates including both scalar and function variables. A novel variable selection algorithm has been developed, namely functional least angle regression. As it is essential for this algorithm, we studied the representation of functional variables with different methods and the correlation between a scalar and a group of mixed scalar and functional variables. We also propose a new stopping rule for practical use. This algorithm is efficient and accurate for both variable selection and parameter estimation even when the number of functional variables is very large and the variables are correlated. And thus the prediction provided by the algorithm is accurate. Our comprehensive simulation study showed that the method is superior to other existing variable selection methods. When the algorithm was applied to the analysis of the movement data, the use of the nonlinear random-effect model and the function variables significantly improved the prediction accuracy for the clinical assessment.},
  archive      = {J_SAC},
  author       = {Cheng, Yafeng and Shi, Jian Qing and Eyre, Janet},
  doi          = {10.1007/s11222-019-09871-3},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {129-140},
  shortjournal = {Stat. Comput.},
  title        = {Nonlinear mixed-effects scalar-on-function models and variable selection},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The minimum regularized covariance determinant estimator.
<em>SAC</em>, <em>30</em>(1), 113–128. (<a
href="https://doi.org/10.1007/s11222-019-09869-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum covariance determinant (MCD) approach estimates the location and scatter matrix using the subset of given size with lowest sample covariance determinant. Its main drawback is that it cannot be applied when the dimension exceeds the subset size. We propose the minimum regularized covariance determinant (MRCD) approach, which differs from the MCD in that the scatter matrix is a convex combination of a target matrix and the sample covariance matrix of the subset. A data-driven procedure sets the weight of the target matrix, so that the regularization is only used when needed. The MRCD estimator is defined in any dimension, is well-conditioned by construction and preserves the good robustness properties of the MCD. We prove that so-called concentration steps can be performed to reduce the MRCD objective function, and we exploit this fact to construct a fast algorithm. We verify the accuracy and robustness of the MRCD estimator in a simulation study and illustrate its practical use for outlier detection and regression analysis on real-life high-dimensional data sets in chemistry and criminology.},
  archive      = {J_SAC},
  author       = {Boudt, Kris and Rousseeuw, Peter J. and Vanduffel, Steven and Verdonck, Tim},
  doi          = {10.1007/s11222-019-09869-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {113-128},
  shortjournal = {Stat. Comput.},
  title        = {The minimum regularized covariance determinant estimator},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal exact designs of experiments via mixed integer
nonlinear programming. <em>SAC</em>, <em>30</em>(1), 93–112. (<a
href="https://doi.org/10.1007/s11222-019-09867-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal exact designs are problematic to find and study because there is no unified theory for determining them and studying their properties. Each has its own challenges and when a method exists to confirm the design optimality, it is invariably applicable to the particular problem only. We propose a systematic approach to construct optimal exact designs by incorporating the Cholesky decomposition of the Fisher Information Matrix in a Mixed Integer Nonlinear Programming formulation. As examples, we apply the methodology to find D- and A-optimal exact designs for linear and nonlinear models using global or local optimizers. Our examples include design problems with constraints on the locations or the number of replicates at the optimal design points.},
  archive      = {J_SAC},
  author       = {Duarte, Belmiro P. M. and Granjo, José F. O. and Wong, Weng Kee},
  doi          = {10.1007/s11222-019-09867-z},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {93-112},
  shortjournal = {Stat. Comput.},
  title        = {Optimal exact designs of experiments via mixed integer nonlinear programming},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parsimonious periodic autoregressive models for time series
with evolving trend and seasonality. <em>SAC</em>, <em>30</em>(1),
77–91. (<a href="https://doi.org/10.1007/s11222-019-09866-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an extension of Periodic AutoRegressive (PAR) modelling for time series with evolving features. The large scale of modern datasets, in fact, implies that the time span may subtend several evolving patterns of the underlying series, affecting also seasonality. The proposed model allows several regimes in time and a possibly different PAR process with a trend term in each regime. The means, autocorrelations and residual variances may change both with the regime and the season, resulting in a very large number of parameters. Therefore as a second step we propose a grouping procedure on the PAR parameters, in order to obtain a more parsimonious and concise model. The model selection procedure is a complex combinatorial problem, and it is solved basing on genetic algorithms that optimize an information criterion. The model is tested in both simulation studies and real data analysis from different fields, proving to be effective for a wide range of series with evolving features, and competitive with respect to more specific models.},
  archive      = {J_SAC},
  author       = {Battaglia, Francesco and Cucina, Domenico and Rizzo, Manuel},
  doi          = {10.1007/s11222-019-09866-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {77-91},
  shortjournal = {Stat. Comput.},
  title        = {Parsimonious periodic autoregressive models for time series with evolving trend and seasonality},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Epidemiologic network inference. <em>SAC</em>,
<em>30</em>(1), 61–75. (<a
href="https://doi.org/10.1007/s11222-019-09865-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many epidemiologic models, a disease is assumed to spread along a contact network. We aim to infer this network, in addition to the epidemiologic model parameters, from the binary status of individuals observed throughout time. We perform an exact evaluation of the probability for each edge to be part of the network by using the matrix-tree theorem on the set of vertices made of the individual status at all times. This leads to a computational complexity of order $${\mathcal {O}}(mn^2)$$, where n is the number of individuals and m the length of the time series. Simulations are provided to demonstrate the efficiency of the proposed method, and it is applied on data concerning seed choices by farmers in India and on data on a measles outbreak.},
  archive      = {J_SAC},
  author       = {Barbillon, Pierre and Schwaller, Loïc and Robin, Stéphane and Flachs, Andrew and Stone, Glenn Davis},
  doi          = {10.1007/s11222-019-09865-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {61-75},
  shortjournal = {Stat. Comput.},
  title        = {Epidemiologic network inference},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mean and median bias reduction in generalized linear models.
<em>SAC</em>, <em>30</em>(1), 43–59. (<a
href="https://doi.org/10.1007/s11222-019-09860-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an integrated framework for estimation and inference from generalized linear models using adjusted score equations that result in mean and median bias reduction. The framework unifies theoretical and methodological aspects of past research on mean bias reduction and accommodates, in a natural way, new advances on median bias reduction. General expressions for the adjusted score functions are derived in terms of quantities that are readily available in standard software for fitting generalized linear models. The resulting estimating equations are solved using a unifying quasi-Fisher scoring algorithm that is shown to be equivalent to iteratively reweighted least squares with appropriately adjusted working variates. Formal links between the iterations for mean and median bias reduction are established. Core model invariance properties are used to develop a novel mixed adjustment strategy when the estimation of a dispersion parameter is necessary. It is also shown how median bias reduction in multinomial logistic regression can be done using the equivalent Poisson log-linear model. The estimates coming out from mean and median bias reduction are found to overcome practical issues related to infinite estimates that can occur with positive probability in generalized linear models with multinomial or discrete responses, and can result in valid inferences even in the presence of a high-dimensional nuisance parameter.},
  archive      = {J_SAC},
  author       = {Kosmidis, Ioannis and Kenne Pagui, Euloge Clovis and Sartori, Nicola},
  doi          = {10.1007/s11222-019-09860-6},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {43-59},
  shortjournal = {Stat. Comput.},
  title        = {Mean and median bias reduction in generalized linear models},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weight-preserving simulated tempering. <em>SAC</em>,
<em>30</em>(1), 27–41. (<a
href="https://doi.org/10.1007/s11222-019-09863-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulated tempering is a popular method of allowing MCMC algorithms to move between modes of a multimodal target density $$\pi $$. One problem with simulated tempering for multimodal targets is that the weights of the various modes change for different inverse-temperature values, sometimes dramatically so. In this paper, we provide a fix to overcome this problem, by adjusting the mode weights to be preserved (i.e. constant) over different inverse-temperature settings. We then apply simulated tempering algorithms to multimodal targets using our mode weight correction. We present simulations in which our weight-preserving algorithm mixes between modes much more successfully than traditional tempering algorithms. We also prove a diffusion limit for an version of our algorithm, which shows that under appropriate assumptions, our algorithm mixes in time $$O(d [\log d]^2)$$.},
  archive      = {J_SAC},
  author       = {Tawn, Nicholas G. and Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  doi          = {10.1007/s11222-019-09863-3},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {27-41},
  shortjournal = {Stat. Comput.},
  title        = {Weight-preserving simulated tempering},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster model matrix crossproducts for large generalized
linear models with discretized covariates. <em>SAC</em>, <em>30</em>(1),
19–25. (<a href="https://doi.org/10.1007/s11222-019-09864-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wood et al. (J Am Stat Assoc 112(519):1199–1210, 2017) developed methods for fitting penalized regression spline based generalized additive models, with of the order of $$10^4$$ coefficients, to up to $$10^8$$ data. The methods offered two to three orders of magnitude reduction in computational cost relative to the most efficient previous methods. Part of the gain resulted from the development of a set of methods for efficiently computing model matrix products when model covariates each take only a discrete set of values substantially smaller than the sample size [generalizing an idea first appearing in Lang et al. (Stat Comput 24(2):223–238, 2014)]. Covariates can always be rounded to achieve such discretization, and it should be noted that the covariate discretization is marginal. That is we do not rely on discretizing covariates jointly, which would typically require the use of very coarse discretization. The most expensive computation in model estimation is the formation of the matrix cross product $$\mathbf{X}^{\mathsf{T}}{\mathbf{WX}}$$ where $$\mathbf{X}$$ is a model matrix and $${\mathbf{W}}$$ a diagonal or tri-diagonal matrix. The purpose of this paper is to present a simple, novel and substantially more efficient approach to the computation of this cross product. The new method offers, for example, a 30 fold reduction in cross product computation time for the Black Smoke model dataset motivating Wood et al. (2017). Given this reduction in computational cost, the subsequent Cholesky decomposition of $$\mathbf{X}^{\mathsf{T}}{\mathbf{WX}}$$ and follow on computation of $$(\mathbf{X}^{\mathsf{T}}{\mathbf{WX}})^{-1}$$ become a more significant part of the computational burden, and we also discuss the choice of methods for improving their speed.},
  archive      = {J_SAC},
  author       = {Li, Zheyuan and Wood, Simon N.},
  doi          = {10.1007/s11222-019-09864-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {19-25},
  shortjournal = {Stat. Comput.},
  title        = {Faster model matrix crossproducts for large generalized linear models with discretized covariates},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical geodesic graphs and CAT(k) metrics for data
analysis. <em>SAC</em>, <em>30</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11222-019-09855-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A methodology is developed for data analysis based on empirically constructed geodesic metric spaces. For a probability distribution, the length along a path between two points can be defined as the amount of probability mass accumulated along the path. The geodesic, then, is the shortest such path and defines a geodesic metric. Such metrics are transformed in a number of ways to produce parametrised families of geodesic metric spaces, empirical versions of which allow computation of intrinsic means and associated measures of dispersion. These reveal properties of the data, based on geometry, such as those that are difficult to see from the raw Euclidean distances. Examples of application include clustering and classification. For certain parameter ranges, the spaces become CAT(0) spaces and the intrinsic means are unique. In one case, a minimal spanning tree of a graph based on the data becomes CAT(0). In another, a so-called “metric cone” construction allows extension to CAT(k) spaces. It is shown how to empirically tune the parameters of the metrics, making it possible to apply them to a number of real cases.},
  archive      = {J_SAC},
  author       = {Kobayashi, Kei and Wynn, Henry P.},
  doi          = {10.1007/s11222-019-09855-3},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Empirical geodesic graphs and CAT(k) metrics for data analysis},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
