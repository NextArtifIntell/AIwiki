<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDAR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijdar---18">IJDAR - 18</h2>
<ul>
<li><details>
<summary>
(2020). Optical character recognition with neural networks and
post-correction with finite state methods. <em>IJDAR</em>,
<em>23</em>(4), 279–295. (<a
href="https://doi.org/10.1007/s10032-020-00359-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optical character recognition (OCR) quality of the historical part of the Finnish newspaper and journal corpus is rather low for reliable search and scientific research on the OCRed data. The estimated character error rate (CER) of the corpus, achieved with commercial software, is between 8 and 13%. There have been earlier attempts to train high-quality OCR models with open-source software, like Ocropy ( https://github.com/tmbdev/ocropy ) and Tesseract ( https://github.com/tesseract-ocr/tesseract ), but so far, none of the methods have managed to successfully train a mixed model that recognizes all of the data in the corpus, which would be essential for an efficient re-OCRing of the corpus. The difficulty lies in the fact that the corpus is printed in the two main languages of Finland (Finnish and Swedish) and in two font families (Blackletter and Antiqua). In this paper, we explore the training of a variety of OCR models with deep neural networks (DNN). First, we find an optimal DNN for our data and, with additional training data, successfully train high-quality mixed-language models. Furthermore, we revisit the effect of confidence voting on the OCR results with different model combinations. Finally, we perform post-correction on the new OCR results and perform error analysis. The results show a significant boost in accuracy, resulting in 1.7% CER on the Finnish and 2.7% CER on the Swedish test set. The greatest accomplishment of the study is the successful training of one mixed language model for the entire corpus and finding a voting setup that further improves the results.},
  archive      = {J_IJDAR},
  author       = {Drobac, Senka and Lindén, Krister},
  doi          = {10.1007/s10032-020-00359-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {279-295},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Optical character recognition with neural networks and post-correction with finite state methods},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DetectGAN: GAN-based text detector for camera-captured
document images. <em>IJDAR</em>, <em>23</em>(4), 267–277. (<a
href="https://doi.org/10.1007/s10032-020-00358-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, with the development of electronic devices, more and more attention has been paid to camera-based text processing. Different from scene image, the recognition system of document image needs to sort out the recognition results and store them in the structured document for the subsequent data processing. However, in document images, the fusion of text lines largely depends on their semantic information rather than just the distance between the characters, which causes the problem of learning confusion in training. At the same time, for multi-directional printed characters in document images, it is necessary to use additional directional information to guide subsequent recognition tasks. In order to avoid learning confusion and get recognition-friendly detection results, we propose a character-level text detection framework, DetectGAN, based on the conditional generative adversarial networks (abbreviation cGAN used in the text). In the proposed framework, position regression and NMS process are removed, and the problem of text detection is directly transformed into an image-to-image generation problem. Experimental results show that our method has an excellent effect on text detection of camera-captured document images and outperforms the classical and state-of-the-art algorithms.},
  archive      = {J_IJDAR},
  author       = {Zhao, Jinyuan and Wang, Yanna and Xiao, Baihua and Shi, Cunzhao and Jia, Fuxi and Wang, Chunheng},
  doi          = {10.1007/s10032-020-00358-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {267-277},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {DetectGAN: GAN-based text detector for camera-captured document images},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic room information retrieval and classification from
floor plan using linear regression model. <em>IJDAR</em>,
<em>23</em>(4), 253–266. (<a
href="https://doi.org/10.1007/s10032-020-00357-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic creation of a repository of the building’s floor plan helps a lot to the architects to reuse them. The basic approach is to extract and recognize texts, symbols or graphics to retrieve the information of the floor plan from the images. This paper proposes a floor plan information retrieval algorithm. The proposed algorithm is based on shape extraction and room identification. $$\alpha $$ -shape is used for finding an accurate shape. From the detected shapes, actual areas of rooms are calculated. Later, a regression model-based binary room classification model is proposed to classify them into room-type, i.e., bedroom, drawing room, kitchen, and non-room-type, i.e., parking porch, bathroom, study room and prayer room. The proposed model is tested on the CVC-FP dataset with an average room detection accuracy of 85.71% and room recognition accuracy of 88%.},
  archive      = {J_IJDAR},
  author       = {Mewada, Hiren K. and Patel, Amit V. and Chaudhari, Jitendra and Mahant, Keyur and Vala, Alpesh},
  doi          = {10.1007/s10032-020-00357-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {253-266},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Automatic room information retrieval and classification from floor plan using linear regression model},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single shot multi-oriented text detection based on local and
non-local features. <em>IJDAR</em>, <em>23</em>(4), 241–252. (<a
href="https://doi.org/10.1007/s10032-020-00356-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the robustness of text detector on scene text of various scales, a single shot text detector that combines local and non-local features is proposed in this paper. A dilated inception module for local feature extraction and a text self-attention module for non-local feature extraction are presented, and these two kinds of modules are integrated into single shot detector (SSD) of generic object detection so as to perform multi-oriented text detection in natural scene. The proposed modules make a contribution to richer and wider receptive field and enhance feature representation. Furthermore, the performance of our text detector is improved. In addition, compared with previous text detectors based on SSD which classify positive and negative samples depending on default boxes, we exploit pixels as reference for more accurate matching with ground truth which avoids complex anchor design. Furthermore, to evaluate the effectiveness of the proposed method, we carry out several comparative experiments on public standard benchmarks and analyze the experimental results in detail. The experimental results illustrate that the proposed text detector can compete with the state-of-the-art methods.},
  archive      = {J_IJDAR},
  author       = {Li, XiaoQian and Liu, Jie and Zhang, ShuWu and Zhang, GuiXuan and Zheng, Yang},
  doi          = {10.1007/s10032-020-00356-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {241-252},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Single shot multi-oriented text detection based on local and non-local features},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust watermarking approach for security issue of binary
documents using fully convolutional networks. <em>IJDAR</em>,
<em>23</em>(3), 219–239. (<a
href="https://doi.org/10.1007/s10032-020-00355-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by increasing possibility of the tampering of genuine documents during a transmission over digital channels, we focus on developing a watermarking framework for determining whether a received document is genuine or falsified, which is performed by hiding a security feature or secret information within it. To begin with, the input document is transformed into a standard form to minimize geometric distortion. Fully convolutional network (FCN) is utilized to detect document’s watermarking regions. Next, we construct hiding patterns used for hiding secret information. Modifying pixel values of these patterns for carrying secret bits depends on the edge and corner features of document content and the connectivity of their neighboring pixels. Lastly, the watermarking process is conducted by either changing the center pixel of the hiding patterns or changing the ratio between the number of edge features and the number of corner features of subregions within the watermarking regions. The experiments are performed on various binary documents, and our approach gives competitive performance compared to state-of-the-art approaches.},
  archive      = {J_IJDAR},
  author       = {Cu, Vinh Loc and Nguyen, Trac and Burie, Jean-Christophe and Ogier, Jean-Marc},
  doi          = {10.1007/s10032-020-00355-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {219-239},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A robust watermarking approach for security issue of binary documents using fully convolutional networks},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A benchmark for unconstrained online handwritten uyghur word
recognition. <em>IJDAR</em>, <em>23</em>(3), 205–218. (<a
href="https://doi.org/10.1007/s10032-020-00354-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite some interesting results from different research groups, a public database for Uyghur online handwriting recognition and a baseline study are not yet available for comparison purpose. In order to fill this void, we present a database of Uyghur online handwritten words and carry out the first benchmark experiments using it. This database contains 125,020 samples of 2030 words collected from 393 writers. According to Uyghur lexicon characteristics, two out-of-vocabulary datasets are especially provided for evaluation. We carry out some unconstrained handwritten word recognition experiments on the database using recurrent neural networks as base model. Recognition results are acquired using connectionist temporal classification without lexicon search and external language model. Concatenated and averaged bidirectional recurrent layers are compared for better generalization. Based on Uyghur unicode representation, we are interested in comparing the models using different alphabets, based both on character types and character forms. To improve generalization, we propose 1D convolutional model which implements 1D convolutional layers for sequence feature extraction. In our experiments, the proposed 1D convolutional model and its variations surpassed the base recurrent layered model on the out-of-vocabulary words by clear margin. 83.23% CAR (character accurate rate) was resulted when out-of-vocabulary samples are used for testing. The highest recognition rate is as high as 94.95% CAR when the test set shares the same lexicon to the training set. The experiments in this paper can be the baseline references for the future study using this database.},
  archive      = {J_IJDAR},
  author       = {Simayi, Wujiahemaiti and Ibrahim, Mayire and Zhang, Xu-Yao and Liu, Cheng-Lin and Hamdulla, Askar},
  doi          = {10.1007/s10032-020-00354-0},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {205-218},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A benchmark for unconstrained online handwritten uyghur word recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based persian calligraphy synthesis via learning to
transfer templates to personal styles. <em>IJDAR</em>, <em>23</em>(3),
183–203. (<a href="https://doi.org/10.1007/s10032-020-00353-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current software tools for computer generation of Persian calligraphy can be mostly described as conventional fonts and typesetting software, which basically neglect the ‘variations’ of real calligraphy performed by hand, in terms of personalization to different calligraphers’ styles, as well as their statistical characteristics. In this paper, we address the problem of natural-looking Persian calligraphy synthesis via a machine learning based approach, at the level of subwords. Given images of samples written by a calligrapher, we train a parametric model to imitate the style. The core idea is to make use of templates (fonts) as a source of background knowledge, and learn a probabilistic mapping from them to personal styles of calligraphers, which is posed as transformation of attributed graphs using neural networks with sliding windows. This can be understood as adding ‘naturalness’ to a Persian calligraphy font, in essence. We report both objective and subjective evaluations, including the model performance in writer (calligrapher) identification task and Visual Turing Test. The results of the latter suggest that humans are unable to distinguish the calligraphy synthesized by our approach from real calligraphy in many cases.},
  archive      = {J_IJDAR},
  author       = {Ahmadian, Amirhossein and Fouladi, Kazim and Araabi, Babak Nadjar},
  doi          = {10.1007/s10032-020-00353-1},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {183-203},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Model-based persian calligraphy synthesis via learning to transfer templates to personal styles},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperkernel-based intuitionistic fuzzy c-means for denoising
color archival document images. <em>IJDAR</em>, <em>23</em>(3), 161–181.
(<a href="https://doi.org/10.1007/s10032-020-00352-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we have addressed the problem of denoising and enhancement of color archival handwritten document images by separating noise from text and background. Indeed, archival document images that originated from scanning or photographing paper documents are mainly digitized in full color mode. Thus, it is necessary to preserve and exploit color information when applying an enhancement method or a denoising technique. Thus, the focus of our work has been to model a color image using a hyperspace. The defined hyperspace formed by the image pixels is obtained by using both topological and color spaces. The novelty of our work lies in exploiting the obtained hyperspace to cluster the extracted low-level features (topological and color) and, thereafter, to separate noise from text and background. Indeed, based on combining the obtained hyperspace with an adapted kernel-based intuitionistic fuzzy c-means (KIFCM) algorithm we have proposed a novel hyper-KIFCM (HKIFCM) method for denoising color historical document images. To illustrate the effectiveness of the HKIFCM method, a thorough experimental study has been firstly conducted with qualitative and quantitative observations obtained from color archival handwritten document images collected from both the Tunisian national archives and two datasets provided in the context of open competitions at ICDAR and ICFHR conferences. Then, we have compared the results achieved with those obtained using the state-of-the-art methods.},
  archive      = {J_IJDAR},
  author       = {Elhedda, Walid and Mehri, Maroua and Mahjoub, Mohamed Ali},
  doi          = {10.1007/s10032-020-00352-2},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {161-181},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Hyperkernel-based intuitionistic fuzzy c-means for denoising color archival document images},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general framework for the recognition of online
handwritten graphics. <em>IJDAR</em>, <em>23</em>(2), 143–160. (<a
href="https://doi.org/10.1007/s10032-019-00349-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit graph grammar and graph parsing as tools for recognizing graphics. A top-down approach for parsing families of handwritten graphics containing different kinds of symbols and of structural relations is proposed. It has been tested on two distinct domains, namely the recognition of handwritten mathematical expressions and of handwritten flowcharts. In the proposed approach, a graphic is considered as a labeled graph generated by a graph grammar. The recognition problem is translated into a graph parsing problem: Given a set of strokes (input data), a parse tree which represents the best interpretation is extracted. The graph parsing algorithm generates multiple interpretations (consistent with the grammar) that can be ranked according to a global cost function that takes into account the likelihood of symbols and structures. The parsing algorithm consists in recursively partitioning the stroke set according to rules defined in the graph grammar. To constrain the number of partitions to be evaluated, we propose the use of a hypothesis graph, built from data-driven machine learning techniques, to encode the most likely symbol and relation hypotheses. Within this approach, it is easy to relax the stroke ordering constraint allowing interspersed symbols, as opposed to some previous works. Experiments show that our method obtains accuracy comparable to methods specifically developed to recognize domain-dependent data.},
  archive      = {J_IJDAR},
  author       = {Julca-Aguilar, Frank and Mouchère, Harold and Viard-Gaudin, Christian and Hirata, Nina S. T.},
  doi          = {10.1007/s10032-019-00349-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {143-160},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A general framework for the recognition of online handwritten graphics},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting complexity in pen- and touch-based signature
biometrics. <em>IJDAR</em>, <em>23</em>(2), 129–141. (<a
href="https://doi.org/10.1007/s10032-020-00351-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric signature verification has been traditionally performed in pen-based office-like scenarios using devices specifically designed for acquiring handwriting. However, the high deployment of devices such as smartphones and tablets has given rise to new and thriving scenarios for signature biometrics where handwriting can be performed using not only a pen stylus but also the finger via touch interaction. Some preliminary studies have highlighted the challenge of this new scenario and the necessity of further research on the topic. The main contribution of this work is to propose a new on-line signature verification architecture adapted to the signature complexity in order to tackle this new and challenging scenario. Additionally, an exhaustive comparative analysis of both pen- and touch-based scenarios using our proposed methodology is carried out along with a review of the most relevant and recent studies in the field. Significant improvements of biometric verification performance and practical insights are extracted for the application of signature verification in real scenarios.},
  archive      = {J_IJDAR},
  author       = {Tolosana, Ruben and Vera-Rodriguez, Ruben and Guest, Richard and Fierrez, Julian and Ortega-Garcia, Javier},
  doi          = {10.1007/s10032-020-00351-3},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {129-141},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Exploiting complexity in pen- and touch-based signature biometrics},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive document recognition system for lettrines.
<em>IJDAR</em>, <em>23</em>(2), 115–128. (<a
href="https://doi.org/10.1007/s10032-019-00346-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an approach to interactively propagate annotations representing the historians’ knowledge on a database of lettrine images manually populated by historians (with annotations). Based on a novel document indexing processing scheme which combines the use of the Zipf law and the use of bag of patterns, our approach extends the bag-of-words model to represent the knowledge by visual features through relevance feedback. Then, annotation propagation is automatically performed to propagate knowledge to the lettrine database. Our approach is presented together with preliminary experimental results and an illustrative example.},
  archive      = {J_IJDAR},
  author       = {Nguyen, Nhu-Van and Coustaty, Mickael and Ogier, Jean-Marc},
  doi          = {10.1007/s10032-019-00346-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {115-128},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {An adaptive document recognition system for lettrines},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MA-CRNN: A multi-scale attention CRNN for chinese text line
recognition in natural scenes. <em>IJDAR</em>, <em>23</em>(2), 103–114.
(<a href="https://doi.org/10.1007/s10032-019-00348-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition methods for Chinese text lines, as an important component of optical character recognition, have been widely applied in many specific tasks. However, there are still some potential challenges: (1) lack of open Chinese text recognition dataset; (2) challenges caused by the characteristics of Chinese characters, e.g., diverse types, complex structure and various sizes; (3) difficulties brought by text images in different scenes, e.g., blur, illumination and distortion. In order to address these challenges, we propose an end-to-end recognition method based on convolutional recurrent neural networks (CRNNs), i.e., multi-scale attention CRNN, which adds three components on the basis of a CRNN: asymmetric convolution, feature reuse network and attention mechanism. The proposed model is mainly aimed at scene text recognition including Chinese characters. Then the model is trained and tested on two Chinese text recognition datasets, i.e., the open dataset MTWI and our constructed large-scale Chinese text line dataset collected from various scenes. The experimental results demonstrate that the proposed method achieves better performance than other methods.},
  archive      = {J_IJDAR},
  author       = {Tong, Guofeng and Li, Yong and Gao, Huashuai and Chen, Huairong and Wang, Hao and Yang, Xiang},
  doi          = {10.1007/s10032-019-00348-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {103-114},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {MA-CRNN: A multi-scale attention CRNN for chinese text line recognition in natural scenes},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast multi-language LSTM-based online handwriting
recognition. <em>IJDAR</em>, <em>23</em>(2), 89–102. (<a
href="https://doi.org/10.1007/s10032-020-00350-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe an online handwriting system that is able to support 102 languages using a deep neural network architecture. This new system has completely replaced our previous segment-and-decode-based system and reduced the error rate by 20–40% relative for most languages. Further, we report new state-of-the-art results on IAM-OnDB for both the open and closed dataset setting. The system combines methods from sequence recognition with a new input encoding using Bézier curves. This leads to up to $$10\times $$ faster recognition times compared to our previous system. Through a series of experiments, we determine the optimal configuration of our models and report the results of our setup on a number of additional public datasets.},
  archive      = {J_IJDAR},
  author       = {Carbune, Victor and Gonnet, Pedro and Deselaers, Thomas and Rowley, Henry A. and Daryin, Alexander and Calvo, Marcos and Wang, Li-Lun and Keysers, Daniel and Feuz, Sandro and Gervais, Philippe},
  doi          = {10.1007/s10032-020-00350-4},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {89-102},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Fast multi-language LSTM-based online handwriting recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient and effective OCR engine training. <em>IJDAR</em>,
<em>23</em>(1), 73–88. (<a
href="https://doi.org/10.1007/s10032-019-00347-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an efficient and effective approach to train OCR engines using the Aletheia document analysis system. All components required for training are seamlessly integrated into Aletheia: training data preparation, the OCR engine’s training processes themselves, text recognition, and quantitative evaluation of the trained engine. Such a comprehensive training and evaluation system, guided through a GUI, allows for iterative incremental training to achieve best results. The widely used Tesseract OCR engine is used as a case study to demonstrate the efficiency and effectiveness of the proposed approach. Experimental results are presented validating the training approach with two different historical datasets, representative of recent significant digitisation projects. The impact of different training strategies and training data requirements is presented in detail.},
  archive      = {J_IJDAR},
  author       = {Clausner, Christian and Antonacopoulos, Apostolos and Pletschacher, Stefan},
  doi          = {10.1007/s10032-019-00347-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {73-88},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Efficient and effective OCR engine training},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified method for augmented incremental recognition of
online handwritten japanese and english text. <em>IJDAR</em>,
<em>23</em>(1), 53–72. (<a
href="https://doi.org/10.1007/s10032-019-00343-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a unified method to augmented incremental recognition for online handwritten Japanese and English text, which is used for busy or on-the-fly recognition while writing, and lazy or delayed recognition after writing, without incurring long waiting times. It extends the local context for segmentation and recognition to a range of recent strokes called “segmentation scope” and “recognition scope,” respectively. The recognition scope is inside of the segmentation scope. The augmented incremental recognition triggers recognition at every several recent strokes, updates the segmentation and recognition candidate lattice, and searches over the lattice for the best result incrementally. It also incorporates three techniques. The first is to reuse the segmentation and recognition candidate lattice in the previous recognition scope for the current recognition scope. The second is to fix undecided segmentation points if they are stable between character/word patterns. The third is to skip recognition of partial candidate character/word patterns. The augmented incremental method includes the case of triggering recognition at every new stroke with the above-mentioned techniques. Experiments conducted on TUAT-Kondate and IAM online database show its superiority to batch recognition (recognizing text at one time) and pure incremental recognition (recognizing text at every input stroke) in processing time, waiting time, and recognition accuracy.},
  archive      = {J_IJDAR},
  author       = {Nguyen, Cuong Tuan and Indurkhya, Bipin and Nakagawa, Masaki},
  doi          = {10.1007/s10032-019-00343-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {53-72},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A unified method for augmented incremental recognition of online handwritten japanese and english text},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Total-text: Toward orientation robustness in scene text
detection. <em>IJDAR</em>, <em>23</em>(1), 31–52. (<a
href="https://doi.org/10.1007/s10032-019-00334-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, text orientation is not diverse enough in the existing scene text datasets. Specifically, curve-orientated text is largely out-numbered by horizontal and multi-oriented text, hence, it has received minimal attention from the community so far. Motivated by this phenomenon, we collected a new scene text dataset, Total-Text, which emphasized on text orientations diversity. It is the first relatively large scale scene text dataset that features three different text orientations: horizontal, multi-oriented, and curve-oriented. In addition, we also study several other important elements such as the practicality and quality of ground truth, evaluation protocol, and the annotation process. We believe that these elements are as important as the images and ground truth to facilitate a new research direction. Secondly, we propose a new scene text detection model as the baseline for Total-Text, namely Polygon-Faster-RCNN, and demonstrated its ability to detect text of all orientations. Images of Total-Text and its annotation are available at https://github.com/cs-chan/Total-Text-Dataset.},
  archive      = {J_IJDAR},
  author       = {Ch’ng, Chee-Kheng and Chan, Chee Seng and Liu, Cheng-Lin},
  doi          = {10.1007/s10032-019-00334-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {31-52},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Total-text: Toward orientation robustness in scene text detection},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Document analysis systems that improve with use.
<em>IJDAR</em>, <em>23</em>(1), 13–29. (<a
href="https://doi.org/10.1007/s10032-019-00344-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document analysis tasks for which representative labeled training samples are available have been largely solved. The next frontier is coping with hitherto unseen formats, unusual typefaces, idiosyncratic handwriting and imperfect image acquisition. Adaptive and style-constrained classification methods can overcome some expected variability, but human intervention will remain necessary in many tasks. Interactive pattern recognition includes data exploration and active learning as well as access to stored documents. The principle of “green interaction” is to make use of every intervention to reduce the likelihood that the automated system will make the same mistake again and again. Some of these techniques may pop up in forthcoming personal camera-based memex-like applications that will have a far broader range of input documents and scene text than the current, successful but highly specialized, systems for patents, postal addresses, bank checks and books.},
  archive      = {J_IJDAR},
  author       = {Nagy, George},
  doi          = {10.1007/s10032-019-00344-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {13-29},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Document analysis systems that improve with use},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Even big data is not enough: Need for a novel reference
modelling for forensic document authentication. <em>IJDAR</em>,
<em>23</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s10032-019-00345-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of big data, deep learning (DL) approaches are becoming quite popular in many branches of science. Forensic science is no longer an exception. However, there are certain problems in forensic science where the solutions would hardly benefit from the recent advances in DL algorithms. Document authentication is one such problem where we can have many reference samples, and with the big data scenario probably we would have even more number of reference samples but number of defective or forged samples will remain an issue. Experts often encounter situations where there is no or hardly a scanty number of forged samples available. In such situation, employment of data-hungry algorithms would be inefficient as they will not be able to learn the forged samples properly. This paper addresses this problem and proposes a novel reference modelling framework for forensic document authentication. The approach is based on Mahalanobis space. Two questioned document examination problems have been studied to show the effectiveness of our reference modelling algorithm which has also been compared to a commonly used learning approach, namely neural network-based classification.},
  archive      = {J_IJDAR},
  author       = {Garain, Utpal and Halder, Biswajit},
  doi          = {10.1007/s10032-019-00345-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Even big data is not enough: Need for a novel reference modelling for forensic document authentication},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
